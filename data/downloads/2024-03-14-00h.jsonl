{"created":"2024-03-12 17:59:48","title":"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension","abstract":"Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards.","sentences":["Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment.","This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement.","Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios.","In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge.","Each LLM receives an Elo rating thereafter.","This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquiries.","Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and MT-Bench.","Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards."],"url":"http://arxiv.org/abs/2403.07872v1","category":"cs.CL"}
{"created":"2024-03-12 17:58:01","title":"TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation","abstract":"A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonstrations collected with TeleMoMa by training imitation learning policies for mobile manipulation tasks involving synchronized whole-body motion. Finally, we also show that TeleMoMa's teleoperation channel enables teleoperation on site, looking at the robot, or remote, sending commands and observations through a computer network, and perform user studies to evaluate how easy it is for novice users to learn to collect demonstrations with different combinations of human interfaces enabled by our system. We hope TeleMoMa becomes a helpful tool for the community enabling researchers to collect whole-body mobile manipulation demonstrations. For more information and video results, https://robin-lab.cs.utexas.edu/telemoma-web.","sentences":["A critical bottleneck limiting imitation learning in robotics is the lack of data.","This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces.","In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators.","TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof.","In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations.","We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world.","We demonstrate the quality of the demonstrations collected with TeleMoMa by training imitation learning policies for mobile manipulation tasks involving synchronized whole-body motion.","Finally, we also show that TeleMoMa's teleoperation channel enables teleoperation on site, looking at the robot, or remote, sending commands and observations through a computer network, and perform user studies to evaluate how easy it is for novice users to learn to collect demonstrations with different combinations of human interfaces enabled by our system.","We hope TeleMoMa becomes a helpful tool for the community enabling researchers to collect whole-body mobile manipulation demonstrations.","For more information and video results, https://robin-lab.cs.utexas.edu/telemoma-web."],"url":"http://arxiv.org/abs/2403.07869v1","category":"cs.RO"}
{"created":"2024-03-12 17:55:38","title":"Exploring Safety Generalization Challenges of Large Language Models via Code","abstract":"The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.","sentences":["The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse.","While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains.","This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs.","Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\\% of the time.","Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages.","These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs."],"url":"http://arxiv.org/abs/2403.07865v1","category":"cs.CL"}
{"created":"2024-03-12 17:52:35","title":"Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing","abstract":"We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures. These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization. Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable. By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently \"dilute\" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels. We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations. These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps.","sentences":["We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures.","These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization.","Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable.","By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently \"dilute\" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families.","As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels.","We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations.","These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps."],"url":"http://arxiv.org/abs/2403.07862v1","category":"math.ST"}
{"created":"2024-03-12 17:50:11","title":"Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation","abstract":"Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at https://github.com/ShihaoZhaoZSH/LaVi-Bridge.","sentences":["Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models.","These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images.","As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts.","A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation.","In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation.","By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models.","Our pipeline is compatible with various language models and generative vision models, accommodating different structures.","Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality.","Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge.","Code is available at https://github.com/ShihaoZhaoZSH/LaVi-Bridge."],"url":"http://arxiv.org/abs/2403.07860v1","category":"cs.CV"}
{"created":"2024-03-12 17:48:08","title":"Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias","abstract":"Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.","sentences":["Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models.","This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models.","When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem.","We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets.","Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR).","We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS.","Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective."],"url":"http://arxiv.org/abs/2403.07857v1","category":"cs.LG"}
{"created":"2024-03-12 17:46:14","title":"On t-structures adjacent and orthogonal to weight structures","abstract":"We study $t$-structures (on triangulated categories) that are closely related to weight structures.   A $t$-structure couple $t=(C_{t\\le 0},C_{t\\ge 0})$ is said to be adjacent to a weight structure $w=(C_{w\\le 0}, C_{w\\ge 0})$ if $C_{t\\ge 0}=C_{w\\ge 0}$.   For a category $C$ that satisfies the Brown representability property we prove that $t$ that is adjacent to $w$ exists if and only if $w$ is smashing (that is, \"respects C-coproducts\"). The heart $Ht$ of this $t$ is the category of those functors $Hw^{op}\\to Ab$ that respect products (here $Hw$ is the heart of $w$); the result has important applications.   We prove several more statements on constructing $t$-structures starting from weight structures; we look for a strictly orthogonal $t$-structure $t$ on some $C'$ (where $C,C'$ are triangulated subcategories of a common $D$) such that $C'_{t\\le 0}$ (resp. $C'_{t\\ge 0}$) is characterized by the vanishing of morphisms from $C_{w\\ge 1}$ (resp. $C_{w\\le -1}$). Some of these results generalize properties of semi-orthogonal decompositions proved in the previous paper, and can be applied to various derived categories of (quasi)coherent sheaves on a scheme $X$ that is projective over an affine noetherian one. We also study hearts of orthogonal $t$-structures and their restrictions, and prove some statements on \"reconstructing\" weight structures from orthogonal $t$-structures.","sentences":["We study $t$-structures (on triangulated categories) that are closely related to weight structures.   ","A $t$-structure couple $t=(C_{t\\le 0},C_{t\\ge 0})$ is said to be adjacent to a weight structure $w=(C_{w\\le 0}, C_{w\\ge 0})$ if $C_{t\\ge 0}=C_{w\\ge 0}$.   For a category $C$ that satisfies the Brown representability property we prove that $t$ that is adjacent to $w$ exists if and only if $w$ is smashing (that is, \"respects C-coproducts\").","The heart $Ht$ of this $t$ is the category of those functors $Hw^{op}\\to Ab$ that respect products (here $Hw$ is the heart of $w$); the result has important applications.   ","We prove several more statements on constructing $t$-structures starting from weight structures; we look for a strictly orthogonal $t$-structure $t$ on some $C'$ (where $C,C'$ are triangulated subcategories of a common $D$) such that $C'_{t\\le 0}$ (resp.","$C'_{t\\ge 0}$) is characterized by the vanishing of morphisms from $C_{w\\ge 1}$ (resp.","$C_{w\\le -1}$).","Some of these results generalize properties of semi-orthogonal decompositions proved in the previous paper, and can be applied to various derived categories of (quasi)coherent sheaves on a scheme $X$ that is projective over an affine noetherian one.","We also study hearts of orthogonal $t$-structures and their restrictions, and prove some statements on \"reconstructing\" weight structures from orthogonal $t$-structures."],"url":"http://arxiv.org/abs/2403.07855v1","category":"math.KT"}
{"created":"2024-03-12 17:44:26","title":"Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks","abstract":"In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising fairness concerns. Existing fairness-aware control schemes tackle this problem by incorporating fairness objectives into the cost function. However, such schemes result in increased overall curtailments. This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing fairness. We illustrate that implementing this approach enhances overall fairness without significantly increasing overall curtailments. The optimization problem involves two stages. The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for fairness based on curtailments from prior days. The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants. Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation. The proposed scheme is numerically validated on several benchmark test cases. Results are compared using the Jain Fairness Index, considering fairness and reconfiguration scenarios.","sentences":["In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising fairness concerns.","Existing fairness-aware control schemes tackle this problem by incorporating fairness objectives into the cost function.","However, such schemes result in increased overall curtailments.","This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing fairness.","We illustrate that implementing this approach enhances overall fairness without significantly increasing overall curtailments.","The optimization problem involves two stages.","The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for fairness based on curtailments from prior days.","The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants.","Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation.","The proposed scheme is numerically validated on several benchmark test cases.","Results are compared using the Jain Fairness Index, considering fairness and reconfiguration scenarios."],"url":"http://arxiv.org/abs/2403.07853v1","category":"eess.SY"}
{"created":"2024-03-12 17:39:13","title":"Infinite tower of higher-curvature corrections: Quasinormal modes and late-time behavior of D-dimensional regular black holes","abstract":"Recently, Bueno, Cano, and Hennigar [arXiv: 2403.04827] proposed a generic approach for incorporating an infinite tower of higher curvature corrections into the Einstein theory. In this study, we compute quasinormal modes for certain regular D-dimensional black holes resulting from this infinite series of higher curvature corrections, specifically focusing on the D-dimensional extensions of the Bardeen and Hayward black holes. We demonstrate that while the fundamental mode is minimally affected by moderate coupling constants, the higher overtones exhibit significant sensitivity even to small coupling values, yielding unconventional modes characterized by vanishing real oscillation frequencies.","sentences":["Recently, Bueno, Cano, and Hennigar [arXiv: 2403.04827] proposed a generic approach for incorporating an infinite tower of higher curvature corrections into the Einstein theory.","In this study, we compute quasinormal modes for certain regular D-dimensional black holes resulting from this infinite series of higher curvature corrections, specifically focusing on the D-dimensional extensions of the Bardeen and Hayward black holes.","We demonstrate that while the fundamental mode is minimally affected by moderate coupling constants, the higher overtones exhibit significant sensitivity even to small coupling values, yielding unconventional modes characterized by vanishing real oscillation frequencies."],"url":"http://arxiv.org/abs/2403.07848v1","category":"gr-qc"}
{"created":"2024-03-12 17:27:49","title":"Quantifying and Mitigating Privacy Risks for Tabular Generative Models","abstract":"Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk.","sentences":["Synthetic data from generative models emerges as the privacy-preserving data-sharing solution.","Such a synthetic data set shall resemble the original data without revealing identifiable private information.","The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models.","Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data.","We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks.","Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables.","Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms.","Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data.","Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk."],"url":"http://arxiv.org/abs/2403.07842v1","category":"cs.LG"}
{"created":"2024-03-12 17:25:30","title":"Programming droplet motion using metamaterials","abstract":"Motion control of droplets has generated much attention for its applications to microfluidics, where precise control of small fluid volumes is an imperative requirement. Mechanical vibrations have been shown to be effective at inducing controllable depinning, and activation of different drop motion regimes. However, existing vibration-based strategies involve establishing homogeneous rigid-body dynamics on the substrate, and therefore lack any form of spatial heterogeneity and tuning. Addressing this limitation, metamaterials provide an ideal platform to achieve spectrally and spatially selective drop motion control, which leverages their ability to attenuate vibrations in selected frequency bands and in selected regions of a substrate. In this work, we illustrate the potential of metamaterials-based drop control by experimentally demonstrating a variety of drop motion capabilities on the surface of metaplates endowed with locally resonant stubs. The experiments leverage the design versatility of a LEGO component-enabled reconfigurable design platform and laser vibrometry measurements with high spatial resolution.","sentences":["Motion control of droplets has generated much attention for its applications to microfluidics, where precise control of small fluid volumes is an imperative requirement.","Mechanical vibrations have been shown to be effective at inducing controllable depinning, and activation of different drop motion regimes.","However, existing vibration-based strategies involve establishing homogeneous rigid-body dynamics on the substrate, and therefore lack any form of spatial heterogeneity and tuning.","Addressing this limitation, metamaterials provide an ideal platform to achieve spectrally and spatially selective drop motion control, which leverages their ability to attenuate vibrations in selected frequency bands and in selected regions of a substrate.","In this work, we illustrate the potential of metamaterials-based drop control by experimentally demonstrating a variety of drop motion capabilities on the surface of metaplates endowed with locally resonant stubs.","The experiments leverage the design versatility of a LEGO component-enabled reconfigurable design platform and laser vibrometry measurements with high spatial resolution."],"url":"http://arxiv.org/abs/2403.07840v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 17:24:26","title":"MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric","abstract":"Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning from width to depth yields highly competitive task-specific models. Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.","sentences":["Vision-language pre-trained models have achieved impressive performance on various downstream tasks.","However, their large model sizes hinder their utilization on platforms with limited computational resources.","We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance.","Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks.","In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks.","Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages.","For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong zero-shot capabilities.","For fine-tuning, consecutive pruning from width to depth yields highly competitive task-specific models.","Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods."],"url":"http://arxiv.org/abs/2403.07839v1","category":"cs.CV"}
{"created":"2024-03-12 17:21:46","title":"MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model","abstract":"Privacy-preserving computing is crucial for multi-center machine learning in many applications such as healthcare and finance. In this paper a Multi-center Privacy Computing framework with Predictions Aggregation (MPCPA) based on denoising diffusion probabilistic model (DDPM) is proposed, in which conditional diffusion model training, DDPM data generation, a classifier, and strategy of prediction aggregation are included. Compared to federated learning, this framework necessitates fewer communications and leverages high-quality generated data to support robust privacy computing. Experimental validation across multiple datasets demonstrates that the proposed framework outperforms classic federated learning and approaches the performance of centralized learning with original data. Moreover, our approach demonstrates robust security, effectively addressing challenges such as image memorization and membership inference attacks. Our experiments underscore the efficacy of the proposed framework in the realm of privacy computing, with the code set to be released soon.","sentences":["Privacy-preserving computing is crucial for multi-center machine learning in many applications such as healthcare and finance.","In this paper a Multi-center Privacy Computing framework with Predictions Aggregation (MPCPA) based on denoising diffusion probabilistic model (DDPM) is proposed, in which conditional diffusion model training, DDPM data generation, a classifier, and strategy of prediction aggregation are included.","Compared to federated learning, this framework necessitates fewer communications and leverages high-quality generated data to support robust privacy computing.","Experimental validation across multiple datasets demonstrates that the proposed framework outperforms classic federated learning and approaches the performance of centralized learning with original data.","Moreover, our approach demonstrates robust security, effectively addressing challenges such as image memorization and membership inference attacks.","Our experiments underscore the efficacy of the proposed framework in the realm of privacy computing, with the code set to be released soon."],"url":"http://arxiv.org/abs/2403.07838v1","category":"cs.DC"}
{"created":"2024-03-12 17:21:10","title":"Topological Protection of Optical Skyrmions through Complex Media","abstract":"Recent experimental realizations of optical Skyrmions through the techniques of structured light have opened the doors to a completely new way of representing data in electromagnetic fields, namely its topology. Apart from potentially enhancing the bandwidth of optical systems, the intrinsically discrete nature of the topological number allows Skyrmions to naturally interface with the digital world. However, investigations into the topological protection of optical Skyrmions through various media remain limited to date. Here, we rigorously define the optical Skyrmion and establish a framework that can be used to analyze the effects of complex media on the topology of Skyrmion fields. Using this framework, we provide simple criteria for spatially varying retarders, diattenuators, depolarizers, and combinations of the former under which topological protection is guaranteed. We then present experimental results validating the robustness of the Skyrmion number against corrupting media and discuss ways of extending the optical Skyrmion to more general settings. We believe that the work presented in this paper provides a theoretical underpinning for the use of Skyrmions in practical applications ranging from optical communications to photonic computing.","sentences":["Recent experimental realizations of optical Skyrmions through the techniques of structured light have opened the doors to a completely new way of representing data in electromagnetic fields, namely its topology.","Apart from potentially enhancing the bandwidth of optical systems, the intrinsically discrete nature of the topological number allows Skyrmions to naturally interface with the digital world.","However, investigations into the topological protection of optical Skyrmions through various media remain limited to date.","Here, we rigorously define the optical Skyrmion and establish a framework that can be used to analyze the effects of complex media on the topology of Skyrmion fields.","Using this framework, we provide simple criteria for spatially varying retarders, diattenuators, depolarizers, and combinations of the former under which topological protection is guaranteed.","We then present experimental results validating the robustness of the Skyrmion number against corrupting media and discuss ways of extending the optical Skyrmion to more general settings.","We believe that the work presented in this paper provides a theoretical underpinning for the use of Skyrmions in practical applications ranging from optical communications to photonic computing."],"url":"http://arxiv.org/abs/2403.07837v1","category":"physics.optics"}
{"created":"2024-03-12 17:18:33","title":"On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems","abstract":"Time-varying phasor-based analysis of subsynchronous oscillations (SSOs) involving grid-following converters (GFLCs) and its benchmarking with electromagnetic transient (EMT) models have so far been restricted to highly simplified grid models with constant voltage sources behind series R-L circuits. In this paper, modeling adequacy of bulk power systems with synchronous generators (SGs), transmission systems, loads, and GFLCs are considered. To this end, we revisit the notions of time-varying phasor calculus, highlighting the distinction between space-phasor-calculus (SPC) and two often interchangeably used frameworks namely baseband-abc and generalized averaging. We present the models of grids in SPC framework that include transmission line dynamics, load dynamics, and SG stator transients. Next, we propose a generic approach to study modeling adequacy in small-signal sense by (a) identifying critical modes through eigenvalue and singular value analysis followed by (b) using weighted maximum singular value error magnitudes as metrics, and (c) further cross-validation. Using a modified 4-machine IEEE benchmark model with up to 3 GFLCs we show that SPC framework can be used for analysis of SSOs. Further, we consider the quasistationary phasor calculus (QPC) framework that neglects transmission line, load, and SG stator dynamics to show its adequacy in SSO modeling and analysis. Time-domain and frequency-domain results with EMT models are also presented.","sentences":["Time-varying phasor-based analysis of subsynchronous oscillations (SSOs) involving grid-following converters (GFLCs) and its benchmarking with electromagnetic transient (EMT) models have so far been restricted to highly simplified grid models with constant voltage sources behind series R-L circuits.","In this paper, modeling adequacy of bulk power systems with synchronous generators (SGs), transmission systems, loads, and GFLCs are considered.","To this end, we revisit the notions of time-varying phasor calculus, highlighting the distinction between space-phasor-calculus (SPC) and two often interchangeably used frameworks namely baseband-abc and generalized averaging.","We present the models of grids in SPC framework that include transmission line dynamics, load dynamics, and SG stator transients.","Next, we propose a generic approach to study modeling adequacy in small-signal sense by (a) identifying critical modes through eigenvalue and singular value analysis followed by (b) using weighted maximum singular value error magnitudes as metrics, and (c) further cross-validation.","Using a modified 4-machine IEEE benchmark model with up to 3 GFLCs we show that SPC framework can be used for analysis of SSOs.","Further, we consider the quasistationary phasor calculus (QPC) framework that neglects transmission line, load, and SG stator dynamics to show its adequacy in SSO modeling and analysis.","Time-domain and frequency-domain results with EMT models are also presented."],"url":"http://arxiv.org/abs/2403.07835v1","category":"eess.SY"}
{"created":"2024-03-12 17:17:20","title":"When Eye-Tracking Meets Machine Learning: A Systematic Review on Applications in Medical Image Analysis","abstract":"Eye-gaze tracking research offers significant promise in enhancing various healthcare-related tasks, above all in medical image analysis and interpretation. Eye tracking, a technology that monitors and records the movement of the eyes, provides valuable insights into human visual attention patterns. This technology can transform how healthcare professionals and medical specialists engage with and analyze diagnostic images, offering a more insightful and efficient approach to medical diagnostics. Hence, extracting meaningful features and insights from medical images by leveraging eye-gaze data improves our understanding of how radiologists and other medical experts monitor, interpret, and understand images for diagnostic purposes. Eye-tracking data, with intricate human visual attention patterns embedded, provides a bridge to integrating artificial intelligence (AI) development and human cognition. This integration allows novel methods to incorporate domain knowledge into machine learning (ML) and deep learning (DL) approaches to enhance their alignment with human-like perception and decision-making. Moreover, extensive collections of eye-tracking data have also enabled novel ML/DL methods to analyze human visual patterns, paving the way to a better understanding of human vision, attention, and cognition. This systematic review investigates eye-gaze tracking applications and methodologies for enhancing ML/DL algorithms for medical image analysis in depth.","sentences":["Eye-gaze tracking research offers significant promise in enhancing various healthcare-related tasks, above all in medical image analysis and interpretation.","Eye tracking, a technology that monitors and records the movement of the eyes, provides valuable insights into human visual attention patterns.","This technology can transform how healthcare professionals and medical specialists engage with and analyze diagnostic images, offering a more insightful and efficient approach to medical diagnostics.","Hence, extracting meaningful features and insights from medical images by leveraging eye-gaze data improves our understanding of how radiologists and other medical experts monitor, interpret, and understand images for diagnostic purposes.","Eye-tracking data, with intricate human visual attention patterns embedded, provides a bridge to integrating artificial intelligence (AI) development and human cognition.","This integration allows novel methods to incorporate domain knowledge into machine learning (ML) and deep learning (DL) approaches to enhance their alignment with human-like perception and decision-making.","Moreover, extensive collections of eye-tracking data have also enabled novel ML/DL methods to analyze human visual patterns, paving the way to a better understanding of human vision, attention, and cognition.","This systematic review investigates eye-gaze tracking applications and methodologies for enhancing ML/DL algorithms for medical image analysis in depth."],"url":"http://arxiv.org/abs/2403.07834v1","category":"eess.IV"}
{"created":"2024-03-12 17:14:12","title":"DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies","abstract":"Large language models (LLMs) can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping. We leverage LLMs' common sense physical reasoning and code-writing abilities to infer an object's physical characteristics--mass $m$, friction coefficient $\\mu$, and spring constant $k$--from a semantic description, and then translate those characteristics into an executable adaptive grasp policy. Using a current-controllable, two-finger gripper with a built-in depth camera, we demonstrate that LLM-generated, physically-grounded grasp policies outperform traditional grasp policies on a custom benchmark of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force. We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness. Our code and videos are available at: https://deligrasp.github.io","sentences":["Large language models (LLMs) can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping.","We leverage LLMs' common sense physical reasoning and code-writing abilities to infer an object's physical characteristics--mass $m$, friction coefficient $\\mu$, and spring constant $k$--from a semantic description, and then translate those characteristics into an executable adaptive grasp policy.","Using a current-controllable, two-finger gripper with a built-in depth camera, we demonstrate that LLM-generated, physically-grounded grasp policies outperform traditional grasp policies on a custom benchmark of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force.","We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness.","Our code and videos are available at: https://deligrasp.github.io"],"url":"http://arxiv.org/abs/2403.07832v1","category":"cs.RO"}
{"created":"2024-03-12 17:13:43","title":"Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration","abstract":"The ubiquity and energy needs of industrial refrigeration has prompted several research studies investigating various control opportunities for reducing energy demand. This work focuses on one such opportunity, termed compressor sequencing, which entails intelligently selecting the operational state of the compressors to service the required refrigeration load with the least possible work. We first study the static compressor sequencing problem and observe that deriving the optimal compressor operational state is computationally challenging and can vary dramatically based on the refrigeration load. Thus we introduce load shifting in conjunction with compressor sequencing, which entails strategically precooling the facility to allow for more efficient compressor operation. Interestingly, we show that load shifting not only provides benefits in computing the optimal compressor operational state, but also can lead to significant energy savings. Our results are based on and compared to real-world sensor data from an operating industrial refrigeration site of Butterball LLC located in Huntsville, AR, which demonstrated that without load shifting, even optimal compressor operation results in compressors often running at intermediate capacity levels, which can lead to inefficiencies. Through collected data, we demonstrate that a load shifting approach for compressor sequencing has the potential to reduce energy use of the compressors up to 20% compared to optimal sequencing without load shifting.","sentences":["The ubiquity and energy needs of industrial refrigeration has prompted several research studies investigating various control opportunities for reducing energy demand.","This work focuses on one such opportunity, termed compressor sequencing, which entails intelligently selecting the operational state of the compressors to service the required refrigeration load with the least possible work.","We first study the static compressor sequencing problem and observe that deriving the optimal compressor operational state is computationally challenging and can vary dramatically based on the refrigeration load.","Thus we introduce load shifting in conjunction with compressor sequencing, which entails strategically precooling the facility to allow for more efficient compressor operation.","Interestingly, we show that load shifting not only provides benefits in computing the optimal compressor operational state, but also can lead to significant energy savings.","Our results are based on and compared to real-world sensor data from an operating industrial refrigeration site of Butterball LLC located in Huntsville, AR, which demonstrated that without load shifting, even optimal compressor operation results in compressors often running at intermediate capacity levels, which can lead to inefficiencies.","Through collected data, we demonstrate that a load shifting approach for compressor sequencing has the potential to reduce energy use of the compressors up to 20% compared to optimal sequencing without load shifting."],"url":"http://arxiv.org/abs/2403.07831v1","category":"eess.SY"}
{"created":"2024-03-12 17:11:36","title":"Convex cones, assessment functions, balanced attributes","abstract":"We investigate a class of polyhedral convex cones, with $R^k_+$ (the nonegative orthant in $\\mathbb{R}^k$) as a special case. We start with the observation that for convex cones contained in $\\mathbb{R}^k$, the respective cone efficiency is inconsistent with the Pareto efficiency, the latter being deeply rooted in economics, the decision theory, and the multiobjective optimization theory. Despite that, we argue that convex cones contained in $\\mathbb{R}^k$ and the respective cone efficiency are also relevant to these domains.   To demonstrate this, we interpret polyhedral convex cones of the investigated class in terms of assessment functions, i.e., functions that aggregate multiple numerical attributes into single numbers.   Further, we observe that all assessment functions in the current use share the same limitation; that is, they do not take explicitly into account attribute proportionality. In consequence, the issue of {\\em attribute balance} (meaning {\\it the balance of attribute values}) escapes them. In contrast, assessment functions defined by polyhedral convex cones of the investigated class, contained in $\\mathbb{R}^k$, enforce the attribute balance. However, enforcing the attribute balance is, in general, inconsistent with the well-established paradigm of Pareto efficiency. We give a practical example where such inconsistency is meaningful.","sentences":["We investigate a class of polyhedral convex cones, with $R^k_+$ (the nonegative orthant in $\\mathbb{R}^k$) as a special case.","We start with the observation that for convex cones contained in $\\mathbb{R}^k$, the respective cone efficiency is inconsistent with the Pareto efficiency, the latter being deeply rooted in economics, the decision theory, and the multiobjective optimization theory.","Despite that, we argue that convex cones contained in $\\mathbb{R}^k$ and the respective cone efficiency are also relevant to these domains.   ","To demonstrate this, we interpret polyhedral convex cones of the investigated class in terms of assessment functions, i.e., functions that aggregate multiple numerical attributes into single numbers.   ","Further, we observe that all assessment functions in the current use share the same limitation; that is, they do not take explicitly into account attribute proportionality.","In consequence, the issue of {\\em attribute balance} (meaning {\\it the balance of attribute values}) escapes them.","In contrast, assessment functions defined by polyhedral convex cones of the investigated class, contained in $\\mathbb{R}^k$, enforce the attribute balance.","However, enforcing the attribute balance is, in general, inconsistent with the well-established paradigm of Pareto efficiency.","We give a practical example where such inconsistency is meaningful."],"url":"http://arxiv.org/abs/2403.07829v1","category":"math.OC"}
{"created":"2024-03-12 17:07:35","title":"Affine Gateaux Differentials and the von Mises Statistical Calculus","abstract":"This paper presents a general study of one-dimensional differentiability for functionals on convex domains that are not necessarily open. The local approximation is carried out by affine functionals, rather than linear ones as in standard Gateaux differentiability. This affine notion of differentiability naturally arises in many applications and, here and there, it appeared in the literature. Our systematic analysis aims to give a general perspective on it.","sentences":["This paper presents a general study of one-dimensional differentiability for functionals on convex domains that are not necessarily open.","The local approximation is carried out by affine functionals, rather than linear ones as in standard Gateaux differentiability.","This affine notion of differentiability naturally arises in many applications and, here and there, it appeared in the literature.","Our systematic analysis aims to give a general perspective on it."],"url":"http://arxiv.org/abs/2403.07827v1","category":"math.FA"}
{"created":"2024-03-12 17:03:07","title":"Fusing Climate Data Products using a Spatially Varying Autoencoder","abstract":"Autoencoders are powerful machine learning models used to compress information from multiple data sources. However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products. The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia.","sentences":["Autoencoders are powerful machine learning models used to compress information from multiple data sources.","However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable.","This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products.","The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products.","Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input.","We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia."],"url":"http://arxiv.org/abs/2403.07822v1","category":"stat.AP"}
{"created":"2024-03-12 17:02:53","title":"Augmenting Interpolation-Based Model Checking with Auxiliary Invariants (Extended Version)","abstract":"Software model checking is a challenging problem, and generating relevant invariants is a key factor in proving the safety properties of a program. Program invariants can be obtained by various approaches, including lightweight procedures based on data-flow analysis and intensive techniques using Craig interpolation. Although data-flow analysis runs efficiently, it often produces invariants that are too weak to prove the properties. By contrast, interpolation-based approaches build strong invariants from interpolants, but they might not scale well due to expensive interpolation procedures. Invariants can also be injected into model-checking algorithms to assist the analysis. Invariant injection has been studied for many well-known approaches, including k-induction, predicate abstraction, and symbolic execution. We propose an augmented interpolation-based verification algorithm that injects external invariants into interpolation-based model checking (McMillan, 2003), a hardware model-checking algorithm recently adopted for software verification. The auxiliary invariants help prune unreachable states in Craig interpolants and confine the analysis to the reachable parts of a program. We implemented the proposed technique in the verification framework CPAchecker and evaluated it against mature SMT-based methods in CPAchecker as well as other state-of-the-art software verifiers. We found that injecting invariants reduces the number of interpolation queries needed to prove safety properties and improves the run-time efficiency. Consequently, the proposed invariant-injection approach verified difficult tasks that none of its plain version (i.e., without invariants), the invariant generator, or any compared tools could solve.","sentences":["Software model checking is a challenging problem, and generating relevant invariants is a key factor in proving the safety properties of a program.","Program invariants can be obtained by various approaches, including lightweight procedures based on data-flow analysis and intensive techniques using Craig interpolation.","Although data-flow analysis runs efficiently, it often produces invariants that are too weak to prove the properties.","By contrast, interpolation-based approaches build strong invariants from interpolants, but they might not scale well due to expensive interpolation procedures.","Invariants can also be injected into model-checking algorithms to assist the analysis.","Invariant injection has been studied for many well-known approaches, including k-induction, predicate abstraction, and symbolic execution.","We propose an augmented interpolation-based verification algorithm that injects external invariants into interpolation-based model checking (McMillan, 2003), a hardware model-checking algorithm recently adopted for software verification.","The auxiliary invariants help prune unreachable states in Craig interpolants and confine the analysis to the reachable parts of a program.","We implemented the proposed technique in the verification framework CPAchecker and evaluated it against mature SMT-based methods in CPAchecker as well as other state-of-the-art software verifiers.","We found that injecting invariants reduces the number of interpolation queries needed to prove safety properties and improves the run-time efficiency.","Consequently, the proposed invariant-injection approach verified difficult tasks that none of its plain version (i.e., without invariants), the invariant generator, or any compared tools could solve."],"url":"http://arxiv.org/abs/2403.07821v1","category":"cs.SE"}
{"created":"2024-03-12 17:01:38","title":"NPCoronaPredict: A computational pipeline for the prediction of the nanoparticle-biomolecule corona","abstract":"The corona of a nanoparticle immersed in a biological fluid is of key importance to its eventual fate and bioactivity in the environment or inside live tissues. It is critical to have insight into both the underlying bionano interactions and the corona composition to ensure biocompatibility of novel engineered nanomaterials. A prediction of these properties in silico requires the successful spanning of multiple orders of magnitude of both time and physical dimensions to produce results in a reasonable amount of time, necessitating the development of a multiscale modelling approach. Here, we present the NPCoronaPredict open-source software package: a suite of software tools to enable this prediction for complex multi-component nanomaterials in essentially arbitrary biological fluids, or more generally any medium containing organic molecules. The package integrates several recent physics-based computational models and a library of both physics-based and data-driven parameterisations for nanomaterials and organic molecules. We describe the underlying theoretical background and the package functionality from the design of multi-component NPs through to the evaluation of the corona.","sentences":["The corona of a nanoparticle immersed in a biological fluid is of key importance to its eventual fate and bioactivity in the environment or inside live tissues.","It is critical to have insight into both the underlying bionano interactions and the corona composition to ensure biocompatibility of novel engineered nanomaterials.","A prediction of these properties in silico requires the successful spanning of multiple orders of magnitude of both time and physical dimensions to produce results in a reasonable amount of time, necessitating the development of a multiscale modelling approach.","Here, we present the NPCoronaPredict open-source software package: a suite of software tools to enable this prediction for complex multi-component nanomaterials in essentially arbitrary biological fluids, or more generally any medium containing organic molecules.","The package integrates several recent physics-based computational models and a library of both physics-based and data-driven parameterisations for nanomaterials and organic molecules.","We describe the underlying theoretical background and the package functionality from the design of multi-component NPs through to the evaluation of the corona."],"url":"http://arxiv.org/abs/2403.07819v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 16:57:56","title":"Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling","abstract":"Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively with such a loss function and multiple diverse datasets can lead to a form of shortcut learning, where the model associates label presence with domain characteristics, leading to a drop in performance. To address this problem, we propose a novel label dropout scheme to break the link between domain characteristics and the presence or absence of labels. We demonstrate that label dropout improves echo segmentation Dice score by 62% and 25% on two cardiac structures when training using multiple diverse partially labelled datasets.","sentences":["Echocardiography (echo) is the first imaging modality used when assessing cardiac function.","The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process.","However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.).","To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets.","A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled.","Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data.","In this paper we show that training naively with such a loss function and multiple diverse datasets can lead to a form of shortcut learning, where the model associates label presence with domain characteristics, leading to a drop in performance.","To address this problem, we propose a novel label dropout scheme to break the link between domain characteristics and the presence or absence of labels.","We demonstrate that label dropout improves echo segmentation Dice score by 62% and 25% on two cardiac structures when training using multiple diverse partially labelled datasets."],"url":"http://arxiv.org/abs/2403.07818v1","category":"cs.CV"}
{"created":"2024-03-12 16:56:31","title":"UniHand: Privacy-preserving Universal Handover for Small-Cell Networks in 5G-enabled Mobile Communication with KCI Resilience","abstract":"Introducing Small Cell Networks (SCN) has significantly improved wireless link quality, spectrum efficiency and network capacity, which has been viewed as one of the key technologies in the fifth-generation (5G) mobile network. However, this technology increases the frequency of handover (HO) procedures caused by the dense deployment of cells in the network with reduced cell coverage, bringing new security and privacy issues. The current 5G-AKA and HO protocols are vulnerable to security weaknesses, such as the lack of forward secrecy and identity confusion attacks. The high HO frequency of HOs might magnify these security and privacy concerns in the 5G mobile network. This work addresses these issues by proposing a secure privacy-preserving universal HO scheme ($\\UniHand$) for SCNs in 5G mobile communication. $\\UniHand$ can achieve mutual authentication, strong anonymity, perfect forward secrecy, key-escrow-free and key compromise impersonation (KCI) resilience. To the best of our knowledge, this is the \\textit{first} scheme to achieve secure, privacy-preserving universal HO with \\textit{KCI} resilience for roaming users in 5G environment. We demonstrate that our proposed scheme is resilient against all the essential security threats by performing a comprehensive formal security analysis and conducting relevant experiments to show the cost-effectiveness of the proposed scheme.","sentences":["Introducing Small Cell Networks (SCN) has significantly improved wireless link quality, spectrum efficiency and network capacity, which has been viewed as one of the key technologies in the fifth-generation (5G) mobile network.","However, this technology increases the frequency of handover (HO) procedures caused by the dense deployment of cells in the network with reduced cell coverage, bringing new security and privacy issues.","The current 5G-AKA and HO protocols are vulnerable to security weaknesses, such as the lack of forward secrecy and identity confusion attacks.","The high HO frequency of HOs might magnify these security and privacy concerns in the 5G mobile network.","This work addresses these issues by proposing a secure privacy-preserving universal HO scheme ($\\UniHand$) for SCNs in 5G mobile communication.","$\\UniHand$ can achieve mutual authentication, strong anonymity, perfect forward secrecy, key-escrow-free and key compromise impersonation (KCI) resilience.","To the best of our knowledge, this is the \\textit{first} scheme to achieve secure, privacy-preserving universal HO with \\textit{KCI} resilience for roaming users in 5G environment.","We demonstrate that our proposed scheme is resilient against all the essential security threats by performing a comprehensive formal security analysis and conducting relevant experiments to show the cost-effectiveness of the proposed scheme."],"url":"http://arxiv.org/abs/2403.07817v1","category":"cs.CR"}
{"created":"2024-03-12 16:54:58","title":"Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM","abstract":"We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.","sentences":["We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge.","Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost.","After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing.","BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously.","Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff."],"url":"http://arxiv.org/abs/2403.07816v1","category":"cs.CL"}
{"created":"2024-03-12 16:53:54","title":"Chronos: Learning the Language of Time Series","abstract":"We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.","sentences":["We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models.","Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss.","We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization.","In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them.","Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines."],"url":"http://arxiv.org/abs/2403.07815v1","category":"cs.LG"}
{"created":"2024-03-12 16:52:36","title":"Higher condensation theory","abstract":"We develop a unified theory of defect condensations for topological orders in all dimensions based on higher categories, higher algebras and higher representations. We show that condensing a $k$-codimensional topological defect $A$ in an $n$+1D (potentially anomalous) topological order $\\mathsf C^{n+1}$ amounts to a $k$-step process. In the first step, we condense $A$ along one of the transversal directions, thus obtaining a $(k-1)$-codimensional defect $\\Sigma A$, which can be further condensed as the second step, so on and so forth. In the $k$-th step, condensing $\\Sigma^{k-1}A$ along the only transversal direction defines a phase transition to a new phase $\\mathsf D^{n+1}$. Mathematically, a $k$-codimensional defect $A$ is condensable if it is equipped with the structure of a condensable $E_k$-algebra. In this case, $\\Sigma A$ is naturally a condensable $E_{k-1}$-algebra, thus it can be further condensed. The condensed phase $\\mathsf D^{n+1}$ consists of all deconfined topological defects in $\\mathsf C^{n+1}$. A $k$-codimensional topological defect is deconfined if and only if it is equipped with a $k$-dimensional $A$-action, which defines an $E_k$-module over $A$. When $\\mathsf C^{n+1}$ is anomaly-free, the same condensation can be alternatively defined by replacing the last two steps by a single step of condensing the $E_2$-algebra $\\Sigma^{k-2}A$ directly. The condensed phase $\\mathsf D^{n+1}$ is determined by the category of $E_2$-modules over $\\Sigma^{k-2}A$. When $n=2$, this modified last step is precisely a usual anyon condensation in a 2+1D topological order. The proofs of the most mathematical results will appear in a mathematical companion of this paper. We also briefly discuss some generalizations and applications that naturally arise from our condensation theory such as higher Morita theory, factorization homology and the condensation theory of non-topological defects.","sentences":["We develop a unified theory of defect condensations for topological orders in all dimensions based on higher categories, higher algebras and higher representations.","We show that condensing a $k$-codimensional topological defect $A$ in an $n$+1D (potentially anomalous) topological order $\\mathsf C^{n+1}$ amounts to a $k$-step process.","In the first step, we condense $A$ along one of the transversal directions, thus obtaining a $(k-1)$-codimensional defect $\\Sigma A$, which can be further condensed as the second step, so on and so forth.","In the $k$-th step, condensing $\\Sigma^{k-1}A$ along the only transversal direction defines a phase transition to a new phase $\\mathsf D^{n+1}$. Mathematically, a $k$-codimensional defect $A$ is condensable if it is equipped with the structure of a condensable $E_k$-algebra.","In this case, $\\Sigma A$ is naturally a condensable $E_{k-1}$-algebra, thus it can be further condensed.","The condensed phase $\\mathsf D^{n+1}$ consists of all deconfined topological defects in $\\mathsf C^{n+1}$.","A $k$-codimensional topological defect is deconfined if and only if it is equipped with a $k$-dimensional $A$-action, which defines an $E_k$-module over $A$.","When $\\mathsf C^{n+1}$ is anomaly-free, the same condensation can be alternatively defined by replacing the last two steps by a single step of condensing the $E_2$-algebra $\\Sigma^{k-2}A$ directly.","The condensed phase $\\mathsf D^{n+1}$ is determined by the category of $E_2$-modules over $\\Sigma^{k-2}A$.","When $n=2$, this modified last step is precisely a usual anyon condensation in a 2+1D topological order.","The proofs of the most mathematical results will appear in a mathematical companion of this paper.","We also briefly discuss some generalizations and applications that naturally arise from our condensation theory such as higher Morita theory, factorization homology and the condensation theory of non-topological defects."],"url":"http://arxiv.org/abs/2403.07813v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 16:51:16","title":"Mesh Refinement with Early Termination for Dynamic Feasibility Problems","abstract":"We propose a novel early-terminating mesh refinement strategy using an integrated residual method to solve dynamic feasibility problems. As a generalization of direct collocation, the integrated residual method is used to approximate an infinite-dimensional problem into a sequence of finite-dimensional optimization subproblems. Each subproblem in the sequence is a finer approximation of the previous. It is shown that these subproblems need not be solved to a high precision; instead, an early termination procedure can determine when mesh refinement should be performed. The new refinement strategy, applied to an inverted pendulum swing-up problem, outperforms a conventional refinement method by up to a factor of three in function evaluations.","sentences":["We propose a novel early-terminating mesh refinement strategy using an integrated residual method to solve dynamic feasibility problems.","As a generalization of direct collocation, the integrated residual method is used to approximate an infinite-dimensional problem into a sequence of finite-dimensional optimization subproblems.","Each subproblem in the sequence is a finer approximation of the previous.","It is shown that these subproblems need not be solved to a high precision; instead, an early termination procedure can determine when mesh refinement should be performed.","The new refinement strategy, applied to an inverted pendulum swing-up problem, outperforms a conventional refinement method by up to a factor of three in function evaluations."],"url":"http://arxiv.org/abs/2403.07811v1","category":"math.OC"}
{"created":"2024-03-12 16:42:44","title":"Beyond Memorization: The Challenge of Random Memory Access in Language Models","abstract":"Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github. com/sail-sg/lm-random-memory-access.","sentences":["Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.","However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive.","In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly.","Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content.","We find that techniques including recitation and permutation improve the random memory access capability of LMs.","Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering.","The code to reproduce our experiments can be found at https://github. com/sail-sg/lm-random-memory-access."],"url":"http://arxiv.org/abs/2403.07805v1","category":"cs.CL"}
{"created":"2024-03-12 16:41:51","title":"Type IV-like Solar Radio Burst Consisting of a Series of Spikes Observed by PSP","abstract":"Solar and interplanetary radio bursts can reflect the existence and motion of energetic electrons and are therefore a kind of vital phenomenon in solar activities. The present study reported a solar radio burst (SRB) event observed by Parker Solar Probe (PSP) in its 8th orbital encounter phase, and it lasted about 20 hours in a frequency range of 0.5-15 MHz, called the type IV-like SRB. This type IV-like SRB consists of a series of numerous spikes with the center-frequency drifting slowly from ~5 MHz to ~1 MHz, and each individual spike appears a much faster frequency drifting and has a narrow frequency range of a few MHz and short duration of a few minutes. Based on the empirical models of the solar atmosphere adopted commonly, combining the in-situ measurement by PSP, we propose that these small-scale spikes were generated by a group of solitary kinetic Alfv\\'en waves (SKAWs) in a magnetic loop accompanying coronal mass ejection (CME) and moving outwards, in which the frequency drifting of individual spike is caused by the SKAW's propagation and the center-frequency drifting may be attributed to the motion of the magnetic loop.","sentences":["Solar and interplanetary radio bursts can reflect the existence and motion of energetic electrons and are therefore a kind of vital phenomenon in solar activities.","The present study reported a solar radio burst (SRB) event observed by Parker Solar Probe (PSP) in its 8th orbital encounter phase, and it lasted about 20 hours in a frequency range of 0.5-15 MHz, called the type IV-like SRB.","This type IV-like SRB consists of a series of numerous spikes with the center-frequency drifting slowly from ~5 MHz to ~1 MHz, and each individual spike appears a much faster frequency drifting and has a narrow frequency range of a few MHz and short duration of a few minutes.","Based on the empirical models of the solar atmosphere adopted commonly, combining the in-situ measurement by PSP, we propose that these small-scale spikes were generated by a group of solitary kinetic Alfv\\'en waves (SKAWs) in a magnetic loop accompanying coronal mass ejection (CME) and moving outwards, in which the frequency drifting of individual spike is caused by the SKAW's propagation and the center-frequency drifting may be attributed to the motion of the magnetic loop."],"url":"http://arxiv.org/abs/2403.07804v1","category":"astro-ph.SR"}
{"created":"2024-03-12 16:41:35","title":"Variational structures for the Fokker--Planck equation with general Dirichlet boundary conditions","abstract":"We prove convergence of a modified Jordan--Kinderlehrer--Otto scheme to a solution to the Fokker--Planck equation in $\\Omega \\Subset \\mathbb{R}^d$ with spatially nonconstant Dirichlet boundary conditions. We work under mild assumptions on the domain, on the drift, and on the initial datum. In the special case where $\\Omega$ is an interval in $\\mathbb{R}^1$, we prove that such a solution is a gradient flow -- curve of maximal slope -- within a suitable space of measures, endowed with a modified Wasserstein distance. Our discrete scheme and modified distance draw inspiration from contributions by A. Figalli and N. Gigli [J. Math. Pures Appl. 94, (2010), pp. 107--130], and J. Morales [J. Math. Pures Appl. 112, (2018), pp. 41--88] on an optimal-transport approach to evolution equations with Dirichlet boundary conditions.","sentences":["We prove convergence of a modified Jordan--Kinderlehrer--Otto scheme to a solution to the Fokker--Planck equation in $\\Omega \\Subset \\mathbb{R}^d$ with spatially nonconstant Dirichlet boundary conditions.","We work under mild assumptions on the domain, on the drift, and on the initial datum.","In the special case where $\\Omega$ is an interval in $\\mathbb{R}^1$, we prove that such a solution is a gradient flow -- curve of maximal slope -- within a suitable space of measures, endowed with a modified Wasserstein distance.","Our discrete scheme and modified distance draw inspiration from contributions by A. Figalli and N. Gigli [J. Math.","Pures Appl.","94, (2010), pp.","107--130], and J. Morales [J. Math.","Pures Appl.","112, (2018), pp.","41--88] on an optimal-transport approach to evolution equations with Dirichlet boundary conditions."],"url":"http://arxiv.org/abs/2403.07803v1","category":"math.AP"}
{"created":"2024-03-12 16:41:31","title":"Boosting keyword spotting through on-device learnable user speech characteristics","abstract":"Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions. Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios. Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices. In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics. The so-generated features are fused and used to classify the input utterance. For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensive update of the user projections. We moreover demonstrate the few-shot learning capabilities of our proposed architecture in sample- and class-scarce learning conditions. With 23.7 kparameters and 1 MFLOP per epoch required for on-device training, our system is feasible for TinyML applications aimed at battery-powered microcontrollers.","sentences":["Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions.","Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios.","Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices.","In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics.","The so-generated features are fused and used to classify the input utterance.","For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensive update of the user projections.","We moreover demonstrate the few-shot learning capabilities of our proposed architecture in sample- and class-scarce learning conditions.","With 23.7 kparameters and 1 MFLOP per epoch required for on-device training, our system is feasible for TinyML applications aimed at battery-powered microcontrollers."],"url":"http://arxiv.org/abs/2403.07802v1","category":"cs.SD"}
{"created":"2024-03-12 16:35:32","title":"A Fourier Transform Framework for Domain Adaptation","abstract":"By using unsupervised domain adaptation (UDA), knowledge can be transferred from a label-rich source domain to a target domain that contains relevant information but lacks labels. Many existing UDA algorithms suffer from directly using raw images as input, resulting in models that overly focus on redundant information and exhibit poor generalization capability. To address this issue, we attempt to improve the performance of unsupervised domain adaptation by employing the Fourier method (FTF).Specifically, FTF is inspired by the amplitude of Fourier spectra, which primarily preserves low-level statistical information. In FTF, we effectively incorporate low-level information from the target domain into the source domain by fusing the amplitudes of both domains in the Fourier domain. Additionally, we observe that extracting features from batches of images can eliminate redundant information while retaining class-specific features relevant to the task. Building upon this observation, we apply the Fourier Transform at the data stream level for the first time. To further align multiple sources of data, we introduce the concept of correlation alignment. To evaluate the effectiveness of our FTF method, we conducted evaluations on four benchmark datasets for domain adaptation, including Office-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results demonstrate superior performance.","sentences":["By using unsupervised domain adaptation (UDA), knowledge can be transferred from a label-rich source domain to a target domain that contains relevant information but lacks labels.","Many existing UDA algorithms suffer from directly using raw images as input, resulting in models that overly focus on redundant information and exhibit poor generalization capability.","To address this issue, we attempt to improve the performance of unsupervised domain adaptation by employing the Fourier method (FTF).Specifically, FTF is inspired by the amplitude of Fourier spectra, which primarily preserves low-level statistical information.","In FTF, we effectively incorporate low-level information from the target domain into the source domain by fusing the amplitudes of both domains in the Fourier domain.","Additionally, we observe that extracting features from batches of images can eliminate redundant information while retaining class-specific features relevant to the task.","Building upon this observation, we apply the Fourier Transform at the data stream level for the first time.","To further align multiple sources of data, we introduce the concept of correlation alignment.","To evaluate the effectiveness of our FTF method, we conducted evaluations on four benchmark datasets for domain adaptation, including Office-31, Office-Home, ImageCLEF-DA, and Office-Caltech.","Our results demonstrate superior performance."],"url":"http://arxiv.org/abs/2403.07798v1","category":"cs.CV"}
{"created":"2024-03-12 16:34:07","title":"Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data","abstract":"Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.","sentences":["Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings.","However, one limitation of these methods is their inability to incorporate public data.","Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori.","We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data.","This technique allows for public data to be included in a graphical-model-based mechanism.","We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased."],"url":"http://arxiv.org/abs/2403.07797v1","category":"cs.LG"}
{"created":"2024-03-12 16:32:25","title":"Optimal regularity for nonlocal elliptic equations and free boundary problems","abstract":"In this article we establish for the first time the $C^s$ boundary regularity of solutions to nonlocal elliptic equations with kernels $K(y)\\asymp |y|^{-n-2s}$. This was known to hold only when $K$ is homogeneous, and it is quite surprising that it holds for general inhomogeneous kernels, too. As an application of our results, we also establish the optimal $C^{1+s}$ regularity of solutions to obstacle problems for general nonlocal operators with kernels $K(y)\\asymp |y|^{-n-2s}$. Again, this was only known when $K$ is homogeneous, and it solves a long-standing open question in the field. A new key idea is to construct a 1D solution as a minimizer of an appropriate nonlocal one-phase free boundary problem, for which we establish optimal $C^s$ regularity and non-degeneracy estimates.","sentences":["In this article we establish for the first time the $C^s$ boundary regularity of solutions to nonlocal elliptic equations with kernels $K(y)\\asymp |y|^{-n-2s}$.","This was known to hold only when $K$ is homogeneous, and it is quite surprising that it holds for general inhomogeneous kernels, too.","As an application of our results, we also establish the optimal $C^{1+s}$ regularity of solutions to obstacle problems for general nonlocal operators with kernels $K(y)\\asymp |y|^{-n-2s}$.","Again, this was only known when $K$ is homogeneous, and it solves a long-standing open question in the field.","A new key idea is to construct a 1D solution as a minimizer of an appropriate nonlocal one-phase free boundary problem, for which we establish optimal $C^s$ regularity and non-degeneracy estimates."],"url":"http://arxiv.org/abs/2403.07793v1","category":"math.AP"}
{"created":"2024-03-12 16:23:49","title":"DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation","abstract":"Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation. More details can be found at https://dex-cap.github.io","sentences":["Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks.","Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies.","To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data.","DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment.","Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands.","Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance.","Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation.","More details can be found at https://dex-cap.github.io"],"url":"http://arxiv.org/abs/2403.07788v1","category":"cs.RO"}
{"created":"2024-03-12 16:20:27","title":"Generative deep learning-enabled ultra-large field-of-view lens-free imaging","abstract":"Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities. Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research. Here, we present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging generative artificial intelligence (AI) for holographic image reconstruction. We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world's largest confocal microscope by 1.76 times. The resolution is at the sub-pixel level of 5.52 $\\mu m$, without the need for a shifting light source. The unsupervised learning-based reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible. This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery.","sentences":["Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities.","Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research.","Here, we present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging generative artificial intelligence (AI) for holographic image reconstruction.","We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world's largest confocal microscope by 1.76 times.","The resolution is at the sub-pixel level of 5.52 $\\mu m$, without the need for a shifting light source.","The unsupervised learning-based reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible.","This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery."],"url":"http://arxiv.org/abs/2403.07786v1","category":"physics.optics"}
{"created":"2024-03-12 16:14:27","title":"Multi-period stochastic covering location problems: Modeling framework and solution approach","abstract":"This paper introduces a very general discrete covering location model that accounts for uncertainty and time-dependent aspects. A MILP formulation is proposed for the problem. Afterwards, it is observed that most of the models existing in the literature related with covering location can be considered as particular cases of this formulation. In order to tackle large instances of this problem a Lagrangian relaxation based heuristic is developed. A computational study is addressed to check the potentials and limits of the formulation and some variants proposed for the problem, as well as to evaluate the heuristic. Finally, different measures to report the relevance of considering a multi-period stochastic setting are studied.","sentences":["This paper introduces a very general discrete covering location model that accounts for uncertainty and time-dependent aspects.","A MILP formulation is proposed for the problem.","Afterwards, it is observed that most of the models existing in the literature related with covering location can be considered as particular cases of this formulation.","In order to tackle large instances of this problem a Lagrangian relaxation based heuristic is developed.","A computational study is addressed to check the potentials and limits of the formulation and some variants proposed for the problem, as well as to evaluate the heuristic.","Finally, different measures to report the relevance of considering a multi-period stochastic setting are studied."],"url":"http://arxiv.org/abs/2403.07785v1","category":"math.OC"}
{"created":"2024-03-12 16:13:11","title":"Finite time BV blowup for Liu-admissible solutions to $p$-system via computer-assisted proof","abstract":"In this paper, we consider finite time blowup of the $BV$-norm for exact solutions to genuinely nonlinear hyperbolic systems in one space dimension, in particular the $p$-system. We consider solutions verifying shock admissibility criteria such as the Lax E-condition and the Liu E-condition. In particular, we present Riemann initial data which admits infinitely many bounded solutions, each of which experience, not just finite time, but in fact instantaneous blowup of the $BV$ norm. The Riemann initial data is allowed to come from an open set in state space. Our method provably does not admit a strictly convex entropy.   The main results in this article compare to Jenssen [SIAM J. Math. Anal., 31(4):894--908, 2000], who shows $BV$ blowup for bounded solutions, or alternatively, blowup in $L^\\infty$, for an artificial $3\\times 3$ system which is not genuinely nonlinear. Baiti-Jenssen [Discrete Contin. Dynam. Systems, 7(4):837--853, 2001] improves upon this Jenssen result and can consider a genuinely nonlinear system, but then the blowup is only in $L^\\infty$ and they cannot construct bounded solutions which blowup in $BV$. Moreover, their system is non-physical and provably does not admit a global, strictly convex entropy. Our result also shows sharpness of the recent Bressan-De Lellis result [Arch. Ration. Mech. Anal., 247(6):Paper No. 106, 12, 2023] concerning well-posedness via the Liu E-condition. The proof of our theorem is computer-assisted, following the framework of Sz\\'{e}kelyhidi [Arch. Ration. Mech. Anal., 172(1):133--152, 2004]. Our code is available on the GitHub.","sentences":["In this paper, we consider finite time blowup of the $BV$-norm for exact solutions to genuinely nonlinear hyperbolic systems in one space dimension, in particular the $p$-system.","We consider solutions verifying shock admissibility criteria such as the Lax E-condition and the Liu E-condition.","In particular, we present Riemann initial data which admits infinitely many bounded solutions, each of which experience, not just finite time, but in fact instantaneous blowup of the $BV$ norm.","The Riemann initial data is allowed to come from an open set in state space.","Our method provably does not admit a strictly convex entropy.   ","The main results in this article compare to Jenssen","[SIAM J. Math.","Anal., 31(4):894--908, 2000], who shows $BV$ blowup for bounded solutions, or alternatively, blowup in $L^\\infty$, for an artificial $3\\times 3$ system which is not genuinely nonlinear.","Baiti-Jenssen [Discrete Contin.","Dynam.","Systems, 7(4):837--853, 2001] improves upon this Jenssen result and can consider a genuinely nonlinear system, but then the blowup is only in $L^\\infty$ and they cannot construct bounded solutions which blowup in $BV$. Moreover, their system is non-physical and provably does not admit a global, strictly convex entropy.","Our result also shows sharpness of the recent Bressan-De Lellis result [Arch.","Ration.","Mech.","Anal., 247(6):Paper No. 106, 12, 2023] concerning well-posedness via the Liu E-condition.","The proof of our theorem is computer-assisted, following the framework of Sz\\'{e}kelyhidi","[Arch.","Ration.","Mech.","Anal., 172(1):133--152, 2004].","Our code is available on the GitHub."],"url":"http://arxiv.org/abs/2403.07784v1","category":"math.AP"}
{"created":"2024-03-12 16:09:50","title":"Minimal Elements of the Causal Boundary with Applications to Spacetime Splitting","abstract":"In 1972, Geroch, Kronheimer, and Penrose introduced what is now called the causal boundary of a spacetime. This boundary is constructed out of Terminal Indecomposable Past sets (TIPs) and their future analogues (TIFs), which are the pasts and futures of inextendible causal curves. The causal boundary is a key tool to understand the global structure of a spacetime. In this paper, we show that in a spacetime with compact Cauchy surfaces, there is always at least one minimal TIP and one minimal TIF, minimal meaning that it does not contain another TIP (resp. TIF) as a proper subset. We then study the implications of the minimal TIP and TIF meeting each other. This condition generalizes some of the ``no observer horizon'' conditions that have been used in the literature to obtain partial solutions of the Bartnik splitting conjecture.","sentences":["In 1972, Geroch, Kronheimer, and Penrose introduced what is now called the causal boundary of a spacetime.","This boundary is constructed out of Terminal Indecomposable Past sets (TIPs) and their future analogues (TIFs), which are the pasts and futures of inextendible causal curves.","The causal boundary is a key tool to understand the global structure of a spacetime.","In this paper, we show that in a spacetime with compact Cauchy surfaces, there is always at least one minimal TIP and one minimal TIF, minimal meaning that it does not contain another TIP (resp.","TIF) as a proper subset.","We then study the implications of the minimal TIP and TIF meeting each other.","This condition generalizes some of the ``no observer horizon'' conditions that have been used in the literature to obtain partial solutions of the Bartnik splitting conjecture."],"url":"http://arxiv.org/abs/2403.07782v1","category":"gr-qc"}
{"created":"2024-03-12 16:09:12","title":"Conservative Black Hole Scattering at Fifth Post-Minkowskian and First Self-Force Order","abstract":"We compute the 5PM order contributions to the scattering angle and impulse of classical black hole scattering in the conservative sector at first self-force order (1SF) using the worldline quantum field theory formalism. This challenging four-loop computation required the use of advanced integration-by-parts and differential equation technology implemented on high-perfomance computing systems. Use of partial fraction identities allowed us to render the complete integrand in a fully planar form. The resulting function space is simpler than expected: in the scattering angle we see only multiple polylogarithms up to weight three, and a total absence of the elliptic integrals that appeared at 4PM order. All checks on our result, both internal - cancellation of dimensional regularization poles, preservation of the on-shell condition - and external - matching the slow-velocity limit with the post-Newtonian (PN) literature up to 5PN order and matching the tail terms to the 4PM loss of energy - are passed.","sentences":["We compute the 5PM order contributions to the scattering angle and impulse of classical black hole scattering in the conservative sector at first self-force order (1SF) using the worldline quantum field theory formalism.","This challenging four-loop computation required the use of advanced integration-by-parts and differential equation technology implemented on high-perfomance computing systems.","Use of partial fraction identities allowed us to render the complete integrand in a fully planar form.","The resulting function space is simpler than expected: in the scattering angle we see only multiple polylogarithms up to weight three, and a total absence of the elliptic integrals that appeared at 4PM order.","All checks on our result, both internal - cancellation of dimensional regularization poles, preservation of the on-shell condition - and external - matching the slow-velocity limit with the post-Newtonian (PN) literature up to 5PN order and matching the tail terms to the 4PM loss of energy - are passed."],"url":"http://arxiv.org/abs/2403.07781v1","category":"hep-th"}
{"created":"2024-03-12 16:07:15","title":"A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics","abstract":"Algorithms for initializing particle distribution in SPH simulations of complex geometries have been proven essential for improving the accuracy of SPH simulations. However, no such algorithms exist for boundary integral SPH models, which can model complex geometries without needing virtual particle layers. This study introduces a Boundary Integral based Particle Initialization (BIPI) algorithm. It consists of a particle-shifting technique carefully designed to redistribute particles to fit the boundary by using the boundary integral formulation for particles adjacent to the boundary. The proposed BIPI algorithm gives special consideration to particles adjacent to the boundary to prevent artificial volume compression. It can automatically produce a \"uniform\" particle distribution with reduced and stabilized concentration gradient for domains with complex geometrical shapes. Finally, a number of examples are presented to demonstrate the effectiveness of the proposed algorithm.","sentences":["Algorithms for initializing particle distribution in SPH simulations of complex geometries have been proven essential for improving the accuracy of SPH simulations.","However, no such algorithms exist for boundary integral SPH models, which can model complex geometries without needing virtual particle layers.","This study introduces a Boundary Integral based Particle Initialization (BIPI) algorithm.","It consists of a particle-shifting technique carefully designed to redistribute particles to fit the boundary by using the boundary integral formulation for particles adjacent to the boundary.","The proposed BIPI algorithm gives special consideration to particles adjacent to the boundary to prevent artificial volume compression.","It can automatically produce a \"uniform\" particle distribution with reduced and stabilized concentration gradient for domains with complex geometrical shapes.","Finally, a number of examples are presented to demonstrate the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.07779v1","category":"cs.CE"}
{"created":"2024-03-12 15:59:08","title":"SemCity: Semantic Scene Generation with Triplane Diffusion","abstract":"We present \"SemCity,\" a 3D diffusion model for semantic scene generation in real-world outdoor environments. Most 3D diffusion models focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed. In this paper, we concentrate on generating a real-outdoor scene through learning a diffusion model on a real-world outdoor dataset. In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions. To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our diffusion model. Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane diffusion model. The manipulation improves our diffusion model's applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements. In experimental results, we demonstrate that our triplane diffusion model shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI. We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene. Further, it also enables the expansion of scenes toward a city-level scale. Finally, we evaluate our method on semantic scene completion refinements where our diffusion model enhances predictions of semantic scene completion networks by learning scene distribution. Our code is available at https://github.com/zoomin-lee/SemCity.","sentences":["We present \"SemCity,\" a 3D diffusion model for semantic scene generation in real-world outdoor environments.","Most 3D diffusion models focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed.","In this paper, we concentrate on generating a real-outdoor scene through learning a diffusion model on a real-world outdoor dataset.","In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions.","To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our diffusion model.","Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane diffusion model.","The manipulation improves our diffusion model's applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements.","In experimental results, we demonstrate that our triplane diffusion model shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI.","We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene.","Further, it also enables the expansion of scenes toward a city-level scale.","Finally, we evaluate our method on semantic scene completion refinements where our diffusion model enhances predictions of semantic scene completion networks by learning scene distribution.","Our code is available at https://github.com/zoomin-lee/SemCity."],"url":"http://arxiv.org/abs/2403.07773v2","category":"cs.CV"}
{"created":"2024-03-12 15:56:10","title":"Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations","abstract":"This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation). We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM.","sentences":["This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration.","Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving.","It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance.","In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation).","We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM."],"url":"http://arxiv.org/abs/2403.07769v1","category":"cs.AI"}
{"created":"2024-03-12 15:54:03","title":"Gapless superfluidity in neutron stars: Thermal properties","abstract":"The interior of mature neutron stars is expected to contain superfluid neutrons and superconducting protons. The influence of temperature and currents on superfluid properties is studied within the self-consistent time-dependent nuclear energy-density functional theory. We find that this theory predicts the existence of a regime in which nucleons are superfluid (the order parameter remains finite) even though the energy spectrum of quasiparticle excitations exhibits no gap. We show that the disappearance of the gap leads to a specific heat that is not exponentially suppressed at low temperatures as in the BCS regime but can be comparable to that in the normal phase. Introducing some dimensionless effective superfluid velocity, we show that the behavior of the specific heat is essentially universal and we derive general approximate analytical formulas for applications to neutron-star cooling simulations.","sentences":["The interior of mature neutron stars is expected to contain superfluid neutrons and superconducting protons.","The influence of temperature and currents on superfluid properties is studied within the self-consistent time-dependent nuclear energy-density functional theory.","We find that this theory predicts the existence of a regime in which nucleons are superfluid (the order parameter remains finite) even though the energy spectrum of quasiparticle excitations exhibits no gap.","We show that the disappearance of the gap leads to a specific heat that is not exponentially suppressed at low temperatures as in the BCS regime but can be comparable to that in the normal phase.","Introducing some dimensionless effective superfluid velocity, we show that the behavior of the specific heat is essentially universal and we derive general approximate analytical formulas for applications to neutron-star cooling simulations."],"url":"http://arxiv.org/abs/2403.07766v1","category":"nucl-th"}
{"created":"2024-03-12 15:53:36","title":"Configuration spaces of orbits and their $S_n$-equivariant $E$-polynomials","abstract":"In this paper, we introduce the configuration space of orbits, a generalization of the configuration space of points but for algebraic varieties that are acted by an algebraic reductive group. We develop a novel method for computing the $S_n$-equivariant $E$-polynomial of an algebraic variety, and we apply it to this new kind of varieties.","sentences":["In this paper, we introduce the configuration space of orbits, a generalization of the configuration space of points but for algebraic varieties that are acted by an algebraic reductive group.","We develop a novel method for computing the $S_n$-equivariant $E$-polynomial of an algebraic variety, and we apply it to this new kind of varieties."],"url":"http://arxiv.org/abs/2403.07765v1","category":"math.AG"}
{"created":"2024-03-12 15:53:14","title":"Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model","abstract":"Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields.","sentences":["Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios.","In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces.","Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details.","It also employs content and structural control modules to preserve the content and structural information of the source image.","With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image.","After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image.","Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on.","Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields."],"url":"http://arxiv.org/abs/2403.07764v1","category":"cs.CV"}
{"created":"2024-03-12 15:51:38","title":"Emerging Technologies for 6G Non-Terrestrial-Networks: From Academia to Industrial Applications","abstract":"Terrestrial networks form the fundamental infrastructure of modern communication systems, serving more than 4 billion users globally. However, terrestrial networks are facing a wide range of challenges, from coverage and reliability to interference and congestion. As the demands of the 6G era are expected to be much higher, it is crucial to address these challenges to ensure a robust and efficient communication infrastructure for the future. To address these problems, Non-terrestrial Network (NTN) has emerged to be a promising solution. NTNs are communication networks that leverage airborne (e.g., unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to facilitate ultra-reliable communications and connectivity with high data rates and low latency over expansive regions. This article aims to provide a comprehensive survey on the utilization of network slicing, Artificial Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to address diverse challenges of NTNs from the perspectives of both academia and industry. Particularly, we first provide an in-depth tutorial on NTN and the key enabling technologies including network slicing, AI/ML, and ORAN. Then, we provide a comprehensive survey on how network slicing and AI/ML have been leveraged to overcome the challenges that NTNs are facing. Moreover, we present how ORAN can be utilized for NTNs. Finally, we highlight important challenges, open issues, and future research directions of NTN in the 6G era.","sentences":["Terrestrial networks form the fundamental infrastructure of modern communication systems, serving more than 4 billion users globally.","However, terrestrial networks are facing a wide range of challenges, from coverage and reliability to interference and congestion.","As the demands of the 6G era are expected to be much higher, it is crucial to address these challenges to ensure a robust and efficient communication infrastructure for the future.","To address these problems, Non-terrestrial Network (NTN) has emerged to be a promising solution.","NTNs are communication networks that leverage airborne (e.g., unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to facilitate ultra-reliable communications and connectivity with high data rates and low latency over expansive regions.","This article aims to provide a comprehensive survey on the utilization of network slicing, Artificial Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to address diverse challenges of NTNs from the perspectives of both academia and industry.","Particularly, we first provide an in-depth tutorial on NTN and the key enabling technologies including network slicing, AI/ML, and ORAN.","Then, we provide a comprehensive survey on how network slicing and AI/ML have been leveraged to overcome the challenges that NTNs are facing.","Moreover, we present how ORAN can be utilized for NTNs.","Finally, we highlight important challenges, open issues, and future research directions of NTN in the 6G era."],"url":"http://arxiv.org/abs/2403.07763v1","category":"cs.NI"}
{"created":"2024-03-12 15:51:03","title":"Elliptical Halbach magnet and gradient modules for low-field portable MRI","abstract":"Objective. To develop methods to design the complete magnetic system for a truly portable MRI scanner for neurological and musculoskeletal (MSK) applications, optimized for field homogeneity, field of view (FoV) and gradient performance compared to existing low-weight configurations. Approach. We explore optimal elliptic-bore Halbach configurations based on discrete arrays of permanent magnets. In this way, we seek to improve the field homogeneity and remove constraints to the extent of the gradient coils typical of Halbach magnets. Specifically, we have optimized a tightly-packed distribution of magnetic Nd$_2$Fe$_14$B cubes with differential evolution algorithms, and a second array of shimming magnets with interior point and differential evolution methods. We have also designed and constructed an elliptical set of gradient coils that extend over the whole magnet length, maximizing the distance between the lobe centers. These are optimized with a target field method minimizing a cost function that considers also heat dissipation. Main result. We have employed the new toolbox to build the main magnet and gradient modules for a portable MRI scanner designed for point-of-care and residential use. The elliptical Halbach bore has semi-axes of 10 & 14 cm and the magnet generates a field of 87 mT homogeneous down to 5,700 ppm (parts per million) in a 20 cm diameter FoV, it weighs 216 kg and has a width of 65 cm and a height of 72 cm. Gradient efficiencies go up to around 0.8 mT/m/A, for a maximum of 12 mT/m with in 0.5 ms with 15 A & 15 V amplifier. The distance between lobes is 28 cm, significantly increased with respect to other Halbach-based scanners. Heat dissipation is around 25 W at maximum power, and gradient deviations from linearity are below 20% in a 20 cm sphere.","sentences":["Objective.","To develop methods to design the complete magnetic system for a truly portable MRI scanner for neurological and musculoskeletal (MSK) applications, optimized for field homogeneity, field of view (FoV) and gradient performance compared to existing low-weight configurations.","Approach.","We explore optimal elliptic-bore Halbach configurations based on discrete arrays of permanent magnets.","In this way, we seek to improve the field homogeneity and remove constraints to the extent of the gradient coils typical of Halbach magnets.","Specifically, we have optimized a tightly-packed distribution of magnetic Nd$_2$Fe$_14$B cubes with differential evolution algorithms, and a second array of shimming magnets with interior point and differential evolution methods.","We have also designed and constructed an elliptical set of gradient coils that extend over the whole magnet length, maximizing the distance between the lobe centers.","These are optimized with a target field method minimizing a cost function that considers also heat dissipation.","Main result.","We have employed the new toolbox to build the main magnet and gradient modules for a portable MRI scanner designed for point-of-care and residential use.","The elliptical Halbach bore has semi-axes of 10 & 14 cm and the magnet generates a field of 87 mT homogeneous down to 5,700 ppm (parts per million) in a 20 cm diameter FoV, it weighs 216 kg and has a width of 65 cm and a height of 72 cm.","Gradient efficiencies go up to around 0.8 mT/m/A, for a maximum of 12 mT/m with in 0.5 ms with 15 A & 15 V amplifier.","The distance between lobes is 28 cm, significantly increased with respect to other Halbach-based scanners.","Heat dissipation is around 25 W at maximum power, and gradient deviations from linearity are below 20% in a 20 cm sphere."],"url":"http://arxiv.org/abs/2403.07761v1","category":"physics.med-ph"}
{"created":"2024-03-12 15:44:12","title":"FAUST XI: Enhancement of the complex organic material in the shocked matter surrounding the [BHB2007] 11 protobinary system","abstract":"iCOMs are species commonly found in the interstellar medium. They are believed to be crucial seed species for the build-up of chemical complexity in star forming regions as well as our own Solar System. Thus, understanding how their abundances evolve during the star formation process and whether it enriches the emerging planetary system is of paramount importance. We use data from the ALMA Large Program FAUST to study the compact line emission towards the [BHB2007] 11 proto-binary system (sources A and B), where a complex structure of filaments connecting the two sources with a larger circumbinary disk has previously been detected. More than 45 CH3OCHO lines are clearly detected, as well as 8 CH3OCH3 transitions , 1 H2CCO transition and 4 t-HCOOH transitions. We compute the abundance ratios with respect to CH3OH for CH3OCHO, CH3OCH3, H2CCO, t-HCOOH (as well as an upper limit for CH3CHO) through a radiative transfer analysis. We also report the upper limits on the column densities of nitrogen bearing iCOMs, N(C2H5CN) and N(C2H3CN). The emission from the detected iCOMs and their precursors is compact and encompasses both protostars, which are separated by only 0.2\" (~ 28 au). The integrated intensities tend to align with the Southern filament, revealed by the high spatial resolution observations of the dust emission at 1.3 mm. A PV and 2D analysis are performed on the strongest and uncontaminated CH3OCH3 transition and show three different spatial and velocity regions, two of them being close to 11B (Southern filament) and the third one near 11A. All our observations suggest that the detected methanol, as well as the other iCOMs, are generated by the shocked gas from the incoming filaments streaming towards [BHB2007] 11A and 11B, respectively, making this source one of the few where chemical enrichment of the gas caused by the streaming material is observed.","sentences":["iCOMs are species commonly found in the interstellar medium.","They are believed to be crucial seed species for the build-up of chemical complexity in star forming regions as well as our own Solar System.","Thus, understanding how their abundances evolve during the star formation process and whether it enriches the emerging planetary system is of paramount importance.","We use data from the ALMA Large Program FAUST to study the compact line emission towards the [BHB2007] 11 proto-binary system (sources A and B), where a complex structure of filaments connecting the two sources with a larger circumbinary disk has previously been detected.","More than 45 CH3OCHO lines are clearly detected, as well as 8 CH3OCH3 transitions , 1 H2CCO transition and 4 t-HCOOH transitions.","We compute the abundance ratios with respect to CH3OH for CH3OCHO, CH3OCH3, H2CCO, t-HCOOH (as well as an upper limit for CH3CHO) through a radiative transfer analysis.","We also report the upper limits on the column densities of nitrogen bearing iCOMs, N(C2H5CN) and N(C2H3CN).","The emission from the detected iCOMs and their precursors is compact and encompasses both protostars, which are separated by only 0.2\" (~ 28 au).","The integrated intensities tend to align with the Southern filament, revealed by the high spatial resolution observations of the dust emission at 1.3 mm.","A PV and 2D analysis are performed on the strongest and uncontaminated CH3OCH3 transition and show three different spatial and velocity regions, two of them being close to 11B (Southern filament) and the third one near 11A. All our observations suggest that the detected methanol, as well as the other iCOMs, are generated by the shocked gas from the incoming filaments streaming towards [BHB2007] 11A and 11B, respectively, making this source one of the few where chemical enrichment of the gas caused by the streaming material is observed."],"url":"http://arxiv.org/abs/2403.07757v1","category":"astro-ph.SR"}
{"created":"2024-03-12 15:43:18","title":"Pediatric vaccine tender scheduling in low- and middle-income countries","abstract":"Effective and efficient scheduling of vaccine distribution can significantly impact vaccine uptake, which is critical to controlling the spread of infectious diseases. Ineffective scheduling can lead to waste, delays, and low vaccine coverage, potentially weakening the efforts to protect the public. Organizations such as UNICEF (United Nations Children's Fund), PAHO (Pan American Health Organization), and GAVI (Gavi, the Vaccine Alliance) coordinate vaccine tenders to ensure that enough supply is available on the international market at the lowest possible prices. Scheduling vaccine tenders over a planning horizon in a way that is equitable, efficient, and accessible is a complex problem that involves trade-offs between multiple objectives while ensuring that vaccine availability, demand, and logistical constraints are met. The current method for scheduling tenders is generally reactive and over short planning horizons. Vaccine tenders are scheduled when supply is insufficient to cover demand. We propose an optimization model to dynamically and proactively generate vaccine tender schedules over long planning horizons. This model helps us address the following research questions: What should the optimal sequencing and scheduling of vaccine tenders be to enhance affordability and profit over long time horizons? What is the optimal tender procurement schedule for single or multiple antigen scenarios? We use several real-life data sources to validate the model and address our research questions. Results from our analysis show when to schedule vaccine tenders, what volumes manufacturers should commit to, and the optimal tender lengths to satisfy demand. We show that vaccine tenders tend towards maximum lengths, generally converge over long time horizons, and are robust to changes in varying conditions.","sentences":["Effective and efficient scheduling of vaccine distribution can significantly impact vaccine uptake, which is critical to controlling the spread of infectious diseases.","Ineffective scheduling can lead to waste, delays, and low vaccine coverage, potentially weakening the efforts to protect the public.","Organizations such as UNICEF (United Nations Children's Fund), PAHO (Pan American Health Organization), and GAVI (Gavi, the Vaccine Alliance) coordinate vaccine tenders to ensure that enough supply is available on the international market at the lowest possible prices.","Scheduling vaccine tenders over a planning horizon in a way that is equitable, efficient, and accessible is a complex problem that involves trade-offs between multiple objectives while ensuring that vaccine availability, demand, and logistical constraints are met.","The current method for scheduling tenders is generally reactive and over short planning horizons.","Vaccine tenders are scheduled when supply is insufficient to cover demand.","We propose an optimization model to dynamically and proactively generate vaccine tender schedules over long planning horizons.","This model helps us address the following research questions: What should the optimal sequencing and scheduling of vaccine tenders be to enhance affordability and profit over long time horizons?","What is the optimal tender procurement schedule for single or multiple antigen scenarios?","We use several real-life data sources to validate the model and address our research questions.","Results from our analysis show when to schedule vaccine tenders, what volumes manufacturers should commit to, and the optimal tender lengths to satisfy demand.","We show that vaccine tenders tend towards maximum lengths, generally converge over long time horizons, and are robust to changes in varying conditions."],"url":"http://arxiv.org/abs/2403.07755v1","category":"math.OC"}
{"created":"2024-03-12 15:37:02","title":"Quotients of M-convex sets and M-convex functions","abstract":"We unify the study of quotients of matroids, polymatroids, valuated matroids and strong maps of submodular functions in the framework of Murota's discrete convex analysis. As a main result, we compile a list of ten equivalent characterizations of quotients for M-convex sets, generalizing existing formulations for (poly)matroids and submodular functions. We also initiate the study of quotients of M-convex functions, constructing a hierarchy of four separate characterizations. Our investigations yield new insights into the fundamental operation of induction, as well as the structure of linking sets and linking functions, which are generalizations of linking systems and bimatroids.","sentences":["We unify the study of quotients of matroids, polymatroids, valuated matroids and strong maps of submodular functions in the framework of Murota's discrete convex analysis.","As a main result, we compile a list of ten equivalent characterizations of quotients for M-convex sets, generalizing existing formulations for (poly)matroids and submodular functions.","We also initiate the study of quotients of M-convex functions, constructing a hierarchy of four separate characterizations.","Our investigations yield new insights into the fundamental operation of induction, as well as the structure of linking sets and linking functions, which are generalizations of linking systems and bimatroids."],"url":"http://arxiv.org/abs/2403.07751v1","category":"math.CO"}
{"created":"2024-03-12 15:36:42","title":"Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings","abstract":"The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space. This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization.","sentences":["The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs).","We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training.","Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM.","These synthetic pairs are then used to train a VLM.","Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data.","In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset.","Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space.","This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization."],"url":"http://arxiv.org/abs/2403.07750v1","category":"cs.CV"}
{"created":"2024-03-12 15:33:09","title":"Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph","abstract":"We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph. The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph. This improves the competitive ratio of collective graph exploration. In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps. All our guarantees are derived from a more general asynchronous setting in which the speeds of the agents are controlled by an adversary at all times. Our guarantees also generalize to weighted graphs, if the number of edges $m$ is replaced by the sum of all edge lengths.","sentences":["We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph.","The agents can read and write information on whiteboards that are located at all nodes.","They both move along one adjacent edge at every time-step.","In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges.","We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph.","This improves the competitive ratio of collective graph exploration.","In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible.","We introduce an algorithm guaranteeing rendezvous in at most $\\frac{3}{2}m$ time-steps.","This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps.","All our guarantees are derived from a more general asynchronous setting in which the speeds of the agents are controlled by an adversary at all times.","Our guarantees also generalize to weighted graphs, if the number of edges $m$ is replaced by the sum of all edge lengths."],"url":"http://arxiv.org/abs/2403.07748v1","category":"cs.MA"}
{"created":"2024-03-12 15:32:39","title":"FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models","abstract":"To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs. We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities. The dataset will be publicly available soon.","sentences":["To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels.","In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs.","FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs.","All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems.","We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs.","We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously.","These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities.","The dataset will be publicly available soon."],"url":"http://arxiv.org/abs/2403.07747v1","category":"cs.CL"}
{"created":"2024-03-12 15:28:21","title":"Probabilistic Easy Variational Causal Effect","abstract":"Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and several examples showing the generic capability of PEACE. We note that PEACE can deal with the causal problems for which micro-level or just macro-level changes in the value of the input variables are important. Finally, PEACE is stable under small changes in $\\partial g_{in}/\\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is obtained from $g$ by removing all functional relationships defining $X$ and $Z$.","sentences":["Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$.","In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems.","Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant.","PEACE is a function of $d\\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case.","Further, we investigate some properties of PEACE using measure theoretical concepts.","Furthermore, we provide some identifiability criteria and several examples showing the generic capability of PEACE.","We note that PEACE can deal with the causal problems for which micro-level or just macro-level changes in the value of the input variables are important.","Finally, PEACE is stable under small changes in $\\partial g_{in}/\\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is obtained from $g$ by removing all functional relationships defining $X$ and $Z$."],"url":"http://arxiv.org/abs/2403.07745v1","category":"stat.ML"}
{"created":"2024-03-12 15:22:05","title":"Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs","abstract":"Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep convolutional neural networks (DCNNs) and vision transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results. The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs. This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models. The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control.","sentences":["Histopathology is a gold standard for cancer diagnosis under a microscopic examination.","However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs).","Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions.","Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis.","In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs.","First, we train independent binary DL models as experts to capture particular artifact morphology.","Then, we ensemble their predictions using a fusion mechanism.","We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep convolutional neural networks (DCNNs) and vision transformers (ViTs).","DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results.","The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs.","This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models.","The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control."],"url":"http://arxiv.org/abs/2403.07743v1","category":"eess.IV"}
{"created":"2024-03-12 15:19:25","title":"Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation","abstract":"The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is one of the top-performing 6D object pose estimation approaches in the BOP Challenge 2022. We apply established metrics and concepts for deep uncertainty quantification to evaluate the results. Furthermore, we propose a novel uncertainty calibration score for regression tasks to quantify the quality of the estimated uncertainty.","sentences":["The estimation of 6D object poses is a fundamental task in many computer vision applications.","Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial.","In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed.","Many top-performing methods are not end-to-end trainable but consist of multiple stages.","In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates.","However, deep ensembles can only be applied to methods that can be trained end-to-end.","In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles.","For the implementation, we choose SurfEmb as representative, since it is one of the top-performing 6D object pose estimation approaches in the BOP Challenge 2022.","We apply established metrics and concepts for deep uncertainty quantification to evaluate the results.","Furthermore, we propose a novel uncertainty calibration score for regression tasks to quantify the quality of the estimated uncertainty."],"url":"http://arxiv.org/abs/2403.07741v1","category":"cs.CV"}
{"created":"2024-03-12 15:07:20","title":"CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control","abstract":"We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for popular online selection rules. We proved that CAS can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. For the decision-driven selection rule, including most online multiple-testing procedures, CAS can exactly control the real-time FCR below the target level without any distributional assumptions. For the online selection with symmetric thresholds, we establish the error bound for the control gap of FCR under mild distributional assumptions. To account for the distribution shift in online data, we also embed CAS into some recent dynamic conformal prediction methods and examine the long-run FCR control. Numerical results on both synthetic and real data corroborate that CAS can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.","sentences":["We study the problem of post-selection predictive inference in an online fashion.","To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks.","Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error.","We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals.","If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label.","We provide tractable constructions for the calibration set for popular online selection rules.","We proved that CAS can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes.","For the decision-driven selection rule, including most online multiple-testing procedures, CAS can exactly control the real-time FCR below the target level without any distributional assumptions.","For the online selection with symmetric thresholds, we establish the error bound for the control gap of FCR under mild distributional assumptions.","To account for the distribution shift in online data, we also embed CAS into some recent dynamic conformal prediction methods and examine the long-run FCR control.","Numerical results on both synthetic and real data corroborate that CAS can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings."],"url":"http://arxiv.org/abs/2403.07728v1","category":"stat.ML"}
{"created":"2024-03-12 15:06:22","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes","abstract":"This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While a majority of the teams did outperform our proposed baseline system, the performances of top-scoring systems are still consistent with a random handling of the more challenging items.","sentences":["This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate.","Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical.","The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   ","The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task.","We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies.","While a majority of the teams did outperform our proposed baseline system, the performances of top-scoring systems are still consistent with a random handling of the more challenging items."],"url":"http://arxiv.org/abs/2403.07726v1","category":"cs.CL"}
{"created":"2024-03-12 15:01:27","title":"Balancing Fairness and Accuracy in Data-Restricted Binary Classification","abstract":"Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair? Analysis is performed on a set of fairness definitions that include group and individual fairness. Experiments on three datasets demonstrate the utility of the proposed framework as a tool for quantifying the trade-offs among different fairness notions and their distributional dependencies.","sentences":["Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier.","For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions.","This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis.","Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset.","In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself.","This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair?","Analysis is performed on a set of fairness definitions that include group and individual fairness.","Experiments on three datasets demonstrate the utility of the proposed framework as a tool for quantifying the trade-offs among different fairness notions and their distributional dependencies."],"url":"http://arxiv.org/abs/2403.07724v1","category":"cs.LG"}
{"created":"2024-03-12 14:58:57","title":"Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion","abstract":"How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at https://anonymous.4open.science/status/EEG_Image_decode-DEEF.","sentences":["How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning.","Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction.","However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction.","In this study, we present an EEG-based visual reconstruction framework.","It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator.","Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks.","Our two-stage image generation strategy vividly reconstructs images seen by humans.","Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction.","The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality.","We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications.","The code of ATM is available at https://anonymous.4open.science/status/EEG_Image_decode-DEEF."],"url":"http://arxiv.org/abs/2403.07721v1","category":"cs.HC"}
{"created":"2024-03-12 14:58:52","title":"Multi-modal Auto-regressive Modeling via Visual Words","abstract":"Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to represent visual information. Experimental results and ablation studies on 5 VQA tasks and 4 benchmark toolkits validate the powerful performance of our proposed approach.","sentences":["Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities.","However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification.","In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time.","Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling.","We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to represent visual information.","Experimental results and ablation studies on 5 VQA tasks and 4 benchmark toolkits validate the powerful performance of our proposed approach."],"url":"http://arxiv.org/abs/2403.07720v1","category":"cs.CV"}
{"created":"2024-03-12 14:58:45","title":"WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?","abstract":"We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.","sentences":["We study the use of large language model-based agents for interacting with software via web browsers.","Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems.","To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform.","We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations.","Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation.","Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field."],"url":"http://arxiv.org/abs/2403.07718v1","category":"cs.LG"}
{"created":"2024-03-12 14:56:14","title":"Nonlocal Stokes equation with relaxation on the divergence free equation","abstract":"In this paper, we consider a new nonlocal approximation to the linear Stokes system with periodic boundary conditions in two and three dimensional spaces . A relaxation term is added to the equation of nonlocal divergence free equation, which is reminiscent to the relaxation of local Stokes equation with small artificial compressibility. Our analysis shows that the well-posedness of the nonlocal system can be established under some mild assumptions on the kernel of nonlocal interactions. Furthermore, the new nonlocal system converges to the conventional, local Stokes system in second order as the horizon parameter of the nonlocal interaction goes to zero. The study provides more theoretical understanding to some numerical methods, such as smoothed particle hydrodynamics, for simulating incompressible viscous flows.","sentences":["In this paper, we consider a new nonlocal approximation to the linear Stokes system with periodic boundary conditions in two and three dimensional spaces .","A relaxation term is added to the equation of nonlocal divergence free equation, which is reminiscent to the relaxation of local Stokes equation with small artificial compressibility.","Our analysis shows that the well-posedness of the nonlocal system can be established under some mild assumptions on the kernel of nonlocal interactions.","Furthermore, the new nonlocal system converges to the conventional, local Stokes system in second order as the horizon parameter of the nonlocal interaction goes to zero.","The study provides more theoretical understanding to some numerical methods, such as smoothed particle hydrodynamics, for simulating incompressible viscous flows."],"url":"http://arxiv.org/abs/2403.07712v1","category":"math.AP"}
{"created":"2024-03-12 14:53:56","title":"SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces","abstract":"Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.","sentences":["Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation.","Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features.","However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence.","This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models.","To overcome this challenge, we propose leveraging state-space models (SSMs).","SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length.","In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation.","In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150.","In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models.","Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models."],"url":"http://arxiv.org/abs/2403.07711v1","category":"cs.CV"}
{"created":"2024-03-12 14:53:37","title":"Focusing Surface Acoustic Waves with a Plasmonic Hypersonic Lens","abstract":"Plasmonic nanoantennas have proven to be efficient transducers of electromagnetic to mechanical energy and vice versa. The sudden thermal expansion of these structures after an ultrafast optical pulsed excitation leads to the emission of hypersonic acoustic waves to the supporting substrate, which can be detected by another antenna that acts as a high-sensitive mechanical probe due to the strong modulation of its optical response. Sophisticated fabrication techniques, together with the implementation of numerical simulations, have allowed the engineering of nanostructures for the controlled directional generation and detection of high-frequency acoustic phonons at the nanoscale, with many potential applications in telecommunications, sensing, mechanical switching, and energy transport. Here, we propose and experimentally demonstrate a nanoscale acoustic lens comprised of 11 gold nanodisks whose collective oscillation gives rise to an interference pattern that results in a diffraction-limited surface acoustic beam of about 340 nm width, with an amplitude contrast of 60%. Via spatially decoupled pump-probe experiments, we were able to map the radiated acoustic energy in the proximity of the focal area, obtaining a very good agreement with the continuum elastic theory.","sentences":["Plasmonic nanoantennas have proven to be efficient transducers of electromagnetic to mechanical energy and vice versa.","The sudden thermal expansion of these structures after an ultrafast optical pulsed excitation leads to the emission of hypersonic acoustic waves to the supporting substrate, which can be detected by another antenna that acts as a high-sensitive mechanical probe due to the strong modulation of its optical response.","Sophisticated fabrication techniques, together with the implementation of numerical simulations, have allowed the engineering of nanostructures for the controlled directional generation and detection of high-frequency acoustic phonons at the nanoscale, with many potential applications in telecommunications, sensing, mechanical switching, and energy transport.","Here, we propose and experimentally demonstrate a nanoscale acoustic lens comprised of 11 gold nanodisks whose collective oscillation gives rise to an interference pattern that results in a diffraction-limited surface acoustic beam of about 340 nm width, with an amplitude contrast of 60%.","Via spatially decoupled pump-probe experiments, we were able to map the radiated acoustic energy in the proximity of the focal area, obtaining a very good agreement with the continuum elastic theory."],"url":"http://arxiv.org/abs/2403.07710v1","category":"physics.optics"}
{"created":"2024-03-12 14:52:03","title":"Tomography of nonlinear materials via the Monotonicity Principle","abstract":"In this paper we present a first non-iterative imaging method for nonlinear materials, based on Monotonicity Principle. Specifically, we deal with the inverse obstacle problem, where the aim is to retrieve a nonlinear anomaly embedded in linear known background.   The Monotonicity Principle (MP) is a general property for various class of PDEs, that has recently generalized to nonlinear elliptic PDEs. Basically, it states a monotone relation between the point-wise value of the unknown material property and the boundary measurements. It is at the foundation of a class of non-iterative imaging methods, characterized by a very low execution time that makes them ideal candidates for real-time applications.   In this work, we develop an inversion method that overcomes some of the peculiar difficulties in practical application of MP to imaging of nonlinear materials, preserving the feasibility for real-time applications. For the sake of clarity, we focus on a specific application, i.e. the Magnetostatic Permeability Tomography where the goal is retrieving the unknown (nonlinear) permeability by boundary measurements in DC operations. This choice is motivated by applications in the inspection of boxes and containers for security.   Reconstructions from simulated data prove the effectiveness of the presented method.","sentences":["In this paper we present a first non-iterative imaging method for nonlinear materials, based on Monotonicity Principle.","Specifically, we deal with the inverse obstacle problem, where the aim is to retrieve a nonlinear anomaly embedded in linear known background.   ","The Monotonicity Principle (MP) is a general property for various class of PDEs, that has recently generalized to nonlinear elliptic PDEs.","Basically, it states a monotone relation between the point-wise value of the unknown material property and the boundary measurements.","It is at the foundation of a class of non-iterative imaging methods, characterized by a very low execution time that makes them ideal candidates for real-time applications.   ","In this work, we develop an inversion method that overcomes some of the peculiar difficulties in practical application of MP to imaging of nonlinear materials, preserving the feasibility for real-time applications.","For the sake of clarity, we focus on a specific application, i.e. the Magnetostatic Permeability Tomography where the goal is retrieving the unknown (nonlinear) permeability by boundary measurements in DC operations.","This choice is motivated by applications in the inspection of boxes and containers for security.   ","Reconstructions from simulated data prove the effectiveness of the presented method."],"url":"http://arxiv.org/abs/2403.07709v1","category":"math.NA"}
{"created":"2024-03-12 14:51:57","title":"Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards","abstract":"Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \\textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GPTs and humans, and our method consistently outperforms strong baselines.","sentences":["Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences.","Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile.","In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \\textit{contrastive rewards}.","%Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step.","We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO.","We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GPTs and humans, and our method consistently outperforms strong baselines."],"url":"http://arxiv.org/abs/2403.07708v1","category":"cs.CL"}
{"created":"2024-03-12 14:51:51","title":"Tightly Bounded Polynomials via Flexible Discretizations for Dynamic Optimization Problems","abstract":"Polynomials are widely used to represent the trajectories of states and/or inputs. It has been shown that a polynomial can be bounded by its coefficients, when expressed in the Bernstein basis. However, in general, the bounds provided by the Bernstein coefficients are not tight. We propose a method for obtaining numerical solutions to dynamic optimization problems, where a flexible discretization is used to achieve tight polynomial bounds. The proposed method is used to solve a constrained cart-pole swing-up optimal control problem. The flexible discretization eliminates the conservatism of the Bernstein bounds and enables a lower cost, in comparison with non-flexible discretizations. A theoretical result on obtaining tight polynomial bounds with a finite discretization is presented. In some applications with linear dynamics, the non-convexity introduced by the flexible discretization may be a drawback.","sentences":["Polynomials are widely used to represent the trajectories of states and/or inputs.","It has been shown that a polynomial can be bounded by its coefficients, when expressed in the Bernstein basis.","However, in general, the bounds provided by the Bernstein coefficients are not tight.","We propose a method for obtaining numerical solutions to dynamic optimization problems, where a flexible discretization is used to achieve tight polynomial bounds.","The proposed method is used to solve a constrained cart-pole swing-up optimal control problem.","The flexible discretization eliminates the conservatism of the Bernstein bounds and enables a lower cost, in comparison with non-flexible discretizations.","A theoretical result on obtaining tight polynomial bounds with a finite discretization is presented.","In some applications with linear dynamics, the non-convexity introduced by the flexible discretization may be a drawback."],"url":"http://arxiv.org/abs/2403.07707v1","category":"math.OC"}
{"created":"2024-03-12 14:50:05","title":"Robust Synthetic-to-Real Transfer for Stereo Matching","abstract":"With advancements in domain generalized stereo matching networks, models pre-trained on synthetic data demonstrate strong robustness to unseen domains. However, few studies have investigated the robustness after fine-tuning them in real-world scenarios, during which the domain generalization ability can be seriously degraded. In this paper, we explore fine-tuning stereo matching networks without compromising their robustness to unseen domains. Our motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for fine-tuning: GT degrades, but PL preserves the domain generalization ability. Empirically, we find the difference between GT and PL implies valuable information that can regularize networks during fine-tuning. We also propose a framework to utilize this difference for fine-tuning, consisting of a frozen Teacher, an exponential moving average (EMA) Teacher, and a Student network. The core idea is to utilize the EMA Teacher to measure what the Student has learned and dynamically improve GT and PL for fine-tuning. We integrate our framework with state-of-the-art networks and evaluate its effectiveness on several real-world datasets. Extensive experiments show that our method effectively preserves the domain generalization ability during fine-tuning.","sentences":["With advancements in domain generalized stereo matching networks, models pre-trained on synthetic data demonstrate strong robustness to unseen domains.","However, few studies have investigated the robustness after fine-tuning them in real-world scenarios, during which the domain generalization ability can be seriously degraded.","In this paper, we explore fine-tuning stereo matching networks without compromising their robustness to unseen domains.","Our motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for fine-tuning: GT degrades, but PL preserves the domain generalization ability.","Empirically, we find the difference between GT and PL implies valuable information that can regularize networks during fine-tuning.","We also propose a framework to utilize this difference for fine-tuning, consisting of a frozen Teacher, an exponential moving average (EMA) Teacher, and a Student network.","The core idea is to utilize the EMA Teacher to measure what the Student has learned and dynamically improve GT and PL for fine-tuning.","We integrate our framework with state-of-the-art networks and evaluate its effectiveness on several real-world datasets.","Extensive experiments show that our method effectively preserves the domain generalization ability during fine-tuning."],"url":"http://arxiv.org/abs/2403.07705v1","category":"cs.CV"}
{"created":"2024-03-12 14:49:19","title":"Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning","abstract":"In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.","sentences":["In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential.","The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution.","However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method.","To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution.","We evaluated the proposed method on continuous control benchmark tasks in MuJoCo.","It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution."],"url":"http://arxiv.org/abs/2403.07704v1","category":"cs.LG"}
{"created":"2024-03-12 14:46:03","title":"CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers","abstract":"In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation.","sentences":["In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models.","VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach.","Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy.","Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models.","Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach.","Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation."],"url":"http://arxiv.org/abs/2403.07700v1","category":"cs.CV"}
{"created":"2024-03-12 14:43:49","title":"Ion Kinetics and Neutron Generation Associated with Electromagnetic Turbulence in Laboratory-scale Counter-streaming Plasmas","abstract":"Electromagnetic turbulence and ion kinetics in counter-streaming plasmas hold great significance in laboratory astrophysics, such as turbulence field amplification and particle energization. Here, we quantitatively demonstrate for the first time how electromagnetic turbulence affects ion kinetics under achievable laboratory conditions (millimeter-scale interpenetrating plasmas with initial velocity of $2000\\ \\mathrm{km/s}$, density of $4 \\times 10^{19}\\ \\mathrm{cm}^{-3}$, and temperature of $100\\ \\mathrm{eV}$) utilizing a recently developed high-order implicit particle-in-cell code without scaling transformation. It is found that the electromagnetic turbulence is driven by ion two-stream and filamentation instabilities. For the magnetized scenarios where an applied magnetic field of tens of Tesla is perpendicular to plasma flows, the growth rates of instabilities increase with the strengthening of applied magnetic field, which therefore leads to a significant enhancement of turbulence fields. Under the competition between the stochastic acceleration due to electromagnetic turbulence and collisional thermalization, ion distribution function shows a distinct super-Gaussian shape, and the ion kinetics are manifested in neutron yields and spectra. Our results have well explained the recent unmagnetized experimental observations, and the findings of magnetized scenario can be verified by current astrophysical experiments.","sentences":["Electromagnetic turbulence and ion kinetics in counter-streaming plasmas hold great significance in laboratory astrophysics, such as turbulence field amplification and particle energization.","Here, we quantitatively demonstrate for the first time how electromagnetic turbulence affects ion kinetics under achievable laboratory conditions (millimeter-scale interpenetrating plasmas with initial velocity of $2000\\ \\mathrm{km/s}$, density of $4 \\times 10^{19}\\ \\mathrm{cm}^{-3}$, and temperature of $100\\ \\mathrm{eV}$) utilizing a recently developed high-order implicit particle-in-cell code without scaling transformation.","It is found that the electromagnetic turbulence is driven by ion two-stream and filamentation instabilities.","For the magnetized scenarios where an applied magnetic field of tens of Tesla is perpendicular to plasma flows, the growth rates of instabilities increase with the strengthening of applied magnetic field, which therefore leads to a significant enhancement of turbulence fields.","Under the competition between the stochastic acceleration due to electromagnetic turbulence and collisional thermalization, ion distribution function shows a distinct super-Gaussian shape, and the ion kinetics are manifested in neutron yields and spectra.","Our results have well explained the recent unmagnetized experimental observations, and the findings of magnetized scenario can be verified by current astrophysical experiments."],"url":"http://arxiv.org/abs/2403.07699v1","category":"physics.plasm-ph"}
{"created":"2024-03-12 14:40:19","title":"Using Equation of State Constraints to Classify Low-Mass Compact Binary Mergers","abstract":"Compact objects observed via gravitational waves are classified as black holes or neutron stars primarily based on their inferred mass with respect to stellar evolution expectations. However, astrophysical expectations for the lowest mass range, $\\lesssim 1.2 \\,M_\\odot$, are uncertain. If such low-mass compact objects exist, ground-based gravitational wave detectors may observe them in binary mergers. Lacking astrophysical expectations for classifying such observations, we go beyond the mass and explore the role of tidal effects. We evaluate how combined mass and tidal inference can inform whether each binary component is a black hole or a neutron star based on consistency with the supranuclear-density equation of state. Low-mass neutron stars experience a large tidal deformation; its observational identification (or lack thereof) can therefore aid in determining the nature of the binary components. Using simulated data, we find that the presence of a sub-solar mass neutron star (black hole) can be established with odds $\\sim 100:1$ when two neutron stars (black holes) merge and emit gravitational waves at signal-to-noise ratio $\\sim 20$. For the same systems, the absence of a black hole (neutron star) can be established with odds $\\sim 10:1$. For mixed neutron star-black hole binaries, we can establish that the system contains a neutron star with odds $\\gtrsim 5:1$. Establishing the presence of a black hole in mixed neutron star-black hole binaries is more challenging, except for the case of a $\\lesssim 1\\,M_{\\odot}$ black hole with a $\\gtrsim 1\\,M_{\\odot}$ neutron star companion. On the other hand, classifying each individual binary component suffers from an inherent labeling ambiguity.","sentences":["Compact objects observed via gravitational waves are classified as black holes or neutron stars primarily based on their inferred mass with respect to stellar evolution expectations.","However, astrophysical expectations for the lowest mass range, $\\lesssim 1.2 \\,M_\\odot$, are uncertain.","If such low-mass compact objects exist, ground-based gravitational wave detectors may observe them in binary mergers.","Lacking astrophysical expectations for classifying such observations, we go beyond the mass and explore the role of tidal effects.","We evaluate how combined mass and tidal inference can inform whether each binary component is a black hole or a neutron star based on consistency with the supranuclear-density equation of state.","Low-mass neutron stars experience a large tidal deformation; its observational identification (or lack thereof) can therefore aid in determining the nature of the binary components.","Using simulated data, we find that the presence of a sub-solar mass neutron star (black hole) can be established with odds $\\sim 100:1$ when two neutron stars (black holes) merge and emit gravitational waves at signal-to-noise ratio $\\sim 20$.","For the same systems, the absence of a black hole (neutron star) can be established with odds $\\sim 10:1$. For mixed neutron star-black hole binaries, we can establish that the system contains a neutron star with odds $\\gtrsim 5:1$. Establishing the presence of a black hole in mixed neutron star-black hole binaries is more challenging, except for the case of a $\\lesssim 1\\,M_{\\odot}$ black hole with a $\\gtrsim 1\\,M_{\\odot}$ neutron star companion.","On the other hand, classifying each individual binary component suffers from an inherent labeling ambiguity."],"url":"http://arxiv.org/abs/2403.07697v1","category":"astro-ph.HE"}
{"created":"2024-03-12 14:37:03","title":"Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization","abstract":"As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification. Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically.","sentences":["As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts.","To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset.","However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs.","Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization.","In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model.","Then, a disentangle reconstruction model is trained based on the generated data.","After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification.","Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically."],"url":"http://arxiv.org/abs/2403.07693v1","category":"cs.CL"}
{"created":"2024-03-12 14:36:52","title":"Masked AutoDecoder is Effective Multi-Task Vision Generalist","abstract":"Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive Transformers for sequence prediction. They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively. However, such autoregressive Transformers may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages. In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of two core designs. First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel. Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences. In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs. Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks. MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models. Code will be released.","sentences":["Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive Transformers for sequence prediction.","They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively.","However, such autoregressive Transformers may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages.","In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist.","MAD consists of two core designs.","First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel.","Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences.","In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs.","Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks.","MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models.","Code will be released."],"url":"http://arxiv.org/abs/2403.07692v1","category":"cs.CV"}
{"created":"2024-03-12 14:34:08","title":"Reference-free Monolithic Preference Optimization with Odds Ratio","abstract":"While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12. We release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).","sentences":["While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence.","In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT.","Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase.","We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.","We release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B)."],"url":"http://arxiv.org/abs/2403.07691v1","category":"cs.CL"}
{"created":"2024-03-12 14:28:06","title":"Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons","abstract":"When training deep neural networks, the phenomenon of $\\textit{dying neurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero during training$\\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization.","sentences":["When training deep neural networks, the phenomenon of $\\textit{dying neurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero during training$\\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios.","In this paper, we reassess this phenomenon, focusing on sparsity and pruning.","By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms.","We introduce $\\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity.","Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability.","Experiments on CIFAR10","and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups.","These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization."],"url":"http://arxiv.org/abs/2403.07688v1","category":"cs.LG"}
{"created":"2024-03-12 14:27:17","title":"Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost","abstract":"Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget.","sentences":["Current foundation models have shown impressive performance across various tasks.","However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process.","Most of this data comes from Western countries, leading to poor results for underrepresented countries.","To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck.","In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs.","Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models.","Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs.","The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget."],"url":"http://arxiv.org/abs/2403.07687v1","category":"cs.CV"}
{"created":"2024-03-12 14:21:30","title":"Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal","abstract":"Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos.","sentences":["Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops.","In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal.","However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions.","Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time.","In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process.","Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage.","During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation.","Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions.","Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos."],"url":"http://arxiv.org/abs/2403.07684v1","category":"cs.CV"}
{"created":"2024-03-12 14:20:11","title":"Towards a Unified Formalism of Multivariate Coefficients of Variation -- Application to the Analysis of Polarimetric Speckle Time Series","abstract":"This article primarily aims to unify the various formalisms of multivariate coefficients of variation, leveraging advanced concepts of generalized means, whether weighted or not, applied to the eigenvalues of covariance matrices. We highlight the existence of an infinite number of these coefficients and demonstrate that they are bounded. Moreover, we link the various coefficients of variation identified in the literature to specific instances within our unified formalism. We illustrate the utility of our method by applying it to a time series of polarimetric radar imagery. In this context, the coefficient of variation emerges as a key tool for detecting changes or identifying permanent scatterers, which are characterized by their remarkable temporal stability. The multidimensionality arises from the diversity of polarizations. The introduction of the various possible coefficients demonstrates how their selection impacts the detection of samples exhibiting specific temporal behaviors and underscores the contribution of polarimetry to dynamic speckle analysis.","sentences":["This article primarily aims to unify the various formalisms of multivariate coefficients of variation, leveraging advanced concepts of generalized means, whether weighted or not, applied to the eigenvalues of covariance matrices.","We highlight the existence of an infinite number of these coefficients and demonstrate that they are bounded.","Moreover, we link the various coefficients of variation identified in the literature to specific instances within our unified formalism.","We illustrate the utility of our method by applying it to a time series of polarimetric radar imagery.","In this context, the coefficient of variation emerges as a key tool for detecting changes or identifying permanent scatterers, which are characterized by their remarkable temporal stability.","The multidimensionality arises from the diversity of polarizations.","The introduction of the various possible coefficients demonstrates how their selection impacts the detection of samples exhibiting specific temporal behaviors and underscores the contribution of polarimetry to dynamic speckle analysis."],"url":"http://arxiv.org/abs/2403.07683v1","category":"physics.ins-det"}
{"created":"2024-03-12 14:18:38","title":"Testing Gravity with Binary Black Hole Gravitational Waves","abstract":"General Relativity (GR) remains the most accurate theory of gravity to date. It has passed many experimental tests in the Solar System as well as binary pulsar, cosmological and gravitational-wave (GW) observations. Some of these tests probe regimes where gravitational fields are weak, the spacetime curvature is small, and the characteristic velocities are not comparable to the speed of light. Observations of compact binary coalescences enable us to test GR in extreme environments of strong and dynamical gravitational fields, large spacetime curvature, and velocities comparable to the speed of light. Since the breakthrough observation of the first GW signal produced by the merger of two black holes, GW150914, in September 2015, the number of confirmed detections of binary mergers has rapidly increased to nearly 100. The analysis of these events has already placed significant constraints on possible deviations from GR and on the nature of the coalescing compact objects. In this chapter, we discuss a selection of tests of GR applicable to observations of GWs from compact binaries. In particular, we will cover consistency tests, which check for consistency between the different phases of the binary's evolution, tests of GW generation, polarization and propagation, and tests of the remnant's nature. We conclude with a brief overview of the challenges and prospects for present and future observatories.","sentences":["General Relativity (GR) remains the most accurate theory of gravity to date.","It has passed many experimental tests in the Solar System as well as binary pulsar, cosmological and gravitational-wave (GW) observations.","Some of these tests probe regimes where gravitational fields are weak, the spacetime curvature is small, and the characteristic velocities are not comparable to the speed of light.","Observations of compact binary coalescences enable us to test GR in extreme environments of strong and dynamical gravitational fields, large spacetime curvature, and velocities comparable to the speed of light.","Since the breakthrough observation of the first GW signal produced by the merger of two black holes, GW150914, in September 2015, the number of confirmed detections of binary mergers has rapidly increased to nearly 100.","The analysis of these events has already placed significant constraints on possible deviations from GR and on the nature of the coalescing compact objects.","In this chapter, we discuss a selection of tests of GR applicable to observations of GWs from compact binaries.","In particular, we will cover consistency tests, which check for consistency between the different phases of the binary's evolution, tests of GW generation, polarization and propagation, and tests of the remnant's nature.","We conclude with a brief overview of the challenges and prospects for present and future observatories."],"url":"http://arxiv.org/abs/2403.07682v1","category":"gr-qc"}
{"created":"2024-03-12 14:18:19","title":"Adapting LoRaWAN to the Open-RAN Architecture","abstract":"This article proposes O-LoRaWAN, an adaptation of the LoRaWAN architecture into a modular network architecture based on the Open RAN (O-RAN) principles. In our vision, standardization of the network components and interfaces will enable the reuse of network functions, and thus, foster an accelerated tailoring of the network functions to the changing application demands. LoRaWAN shares similarities to cellular networks and becomes an interesting candidate for a transformation to the O-RAN standard. In the article we draw several transition strategies, these include the reorganization of the LoRa gateway functions into Radio and Distributed Units; enhancing network performance with RAN Intelligent Controllers exploiting the network data; and the standardization of the management and orchestration of network components. Key for that adaptation are the O-RAN interfaces. Along the article, we analyze them and suggest protocol extensions or adjustments for compatibility and interoperability between network components, advocating for the design of extensible protocols","sentences":["This article proposes O-LoRaWAN, an adaptation of the LoRaWAN architecture into a modular network architecture based on the Open RAN (O-RAN) principles.","In our vision, standardization of the network components and interfaces will enable the reuse of network functions, and thus, foster an accelerated tailoring of the network functions to the changing application demands.","LoRaWAN shares similarities to cellular networks and becomes an interesting candidate for a transformation to the O-RAN standard.","In the article we draw several transition strategies, these include the reorganization of the LoRa gateway functions into Radio and Distributed Units; enhancing network performance with RAN Intelligent Controllers exploiting the network data; and the standardization of the management and orchestration of network components.","Key for that adaptation are the O-RAN interfaces.","Along the article, we analyze them and suggest protocol extensions or adjustments for compatibility and interoperability between network components, advocating for the design of extensible protocols"],"url":"http://arxiv.org/abs/2403.07680v1","category":"cs.NI"}
{"created":"2024-03-12 14:16:10","title":"Directional testing for one-way MANOVA in divergent dimensions","abstract":"Testing the equality of mean vectors across $g$ different groups plays an important role in many scientific fields. In regular frameworks, likelihood-based statistics under the normality assumption offer a general solution to this task. However, the accuracy of standard asymptotic results is not reliable when the dimension $p$ of the data is large relative to the sample size $n_i$ of each group. We propose here an exact directional test for the equality of $g$ normal mean vectors with identical unknown covariance matrix, provided that $\\sum_{i=1}^g n_i \\ge p+g+1$. In the case of two groups ($g=2$), the directional test is equivalent to the Hotelling's $T^2$ test. In the more general situation where the $g$ independent groups may have different unknown covariance matrices, although exactness does not hold, simulation studies show that the directional test is more accurate than most commonly used likelihood based solutions. Robustness of the directional approach and its competitors under deviation from multivariate normality is also numerically investigated.","sentences":["Testing the equality of mean vectors across $g$ different groups plays an important role in many scientific fields.","In regular frameworks, likelihood-based statistics under the normality assumption offer a general solution to this task.","However, the accuracy of standard asymptotic results is not reliable when the dimension $p$ of the data is large relative to the sample size $n_i$ of each group.","We propose here an exact directional test for the equality of $g$ normal mean vectors with identical unknown covariance matrix, provided that $\\sum_{i=1}^g n_i \\ge p+g+1$.","In the case of two groups ($g=2$), the directional test is equivalent to the Hotelling's $T^2$ test.","In the more general situation where the $g$ independent groups may have different unknown covariance matrices, although exactness does not hold, simulation studies show that the directional test is more accurate than most commonly used likelihood based solutions.","Robustness of the directional approach and its competitors under deviation from multivariate normality is also numerically investigated."],"url":"http://arxiv.org/abs/2403.07679v1","category":"math.ST"}
{"created":"2024-03-12 14:12:59","title":"MoralBERT: Detecting Moral Values in Social Discourse","abstract":"Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label. We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation. Our results showed that in-domain prediction models significantly outperformed traditional models. While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models. Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task. The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues.","sentences":["Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements.","Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews.","Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content.","Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT.","We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics.","This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts.","We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label.","We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation.","Our results showed that in-domain prediction models significantly outperformed traditional models.","While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models.","Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task.","The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues."],"url":"http://arxiv.org/abs/2403.07678v1","category":"cs.CL"}
{"created":"2024-03-12 14:12:17","title":"Parametrized higher semiadditivity and the universality of spans","abstract":"Using the framework of ambidexterity developed by Hopkins and Lurie, we introduce a parametrized analogue of higher semiadditivity called $\\mathcal Q$-semiadditivity, depending on a chosen class of morphisms $\\mathcal Q$. Our first main result identifies the free $\\mathcal Q$-semiadditive parametrized category on a single generator with a certain parametrized span category $\\underline{\\mathrm{Span}}(\\mathcal Q)$, simultaneously generalizing a result of Harpaz in the non-parametrized setting and a result of Nardin in the equivariant setting. As a consequence, we deduce that the $\\mathcal Q$-semiadditive completion of a parametrized category $\\mathcal C$ consists of the $\\mathcal Q$-commutative monoids in $\\mathcal C$, defined as $\\mathcal Q$-limit preserving parametrized functors from $\\underline{\\mathrm{Span}}(\\mathcal Q)$ to $\\mathcal C$.   As our second main result, we provide an explicit `Mackey sheaf' description of the free presentable $\\mathcal Q$-semiadditive category. Using this, we reprove the Mackey functor description of global spectra first obtained by the second-named author and generalize it to $G$-global spectra. Moreover, we obtain universal characterizations of the categories of $\\mathbb Z$-valued $G$-Mackey profunctors and of quasi-finitely genuine $G$-spectra as studied by Kaledin and Krause-McCandless-Nikolaus, respectively.","sentences":["Using the framework of ambidexterity developed by Hopkins and Lurie, we introduce a parametrized analogue of higher semiadditivity called $\\mathcal Q$-semiadditivity, depending on a chosen class of morphisms $\\mathcal Q$. Our first main result identifies the free $\\mathcal Q$-semiadditive parametrized category on a single generator with a certain parametrized span category $\\underline{\\mathrm{Span}}(\\mathcal Q)$, simultaneously generalizing a result of Harpaz in the non-parametrized setting and a result of Nardin in the equivariant setting.","As a consequence, we deduce that the $\\mathcal Q$-semiadditive completion of a parametrized category $\\mathcal C$ consists of the $\\mathcal Q$-commutative monoids in $\\mathcal C$, defined as $\\mathcal Q$-limit preserving parametrized functors from $\\underline{\\mathrm{Span}}(\\mathcal Q)$ to $\\mathcal C$.   ","As our second main result, we provide an explicit `Mackey sheaf' description of the free presentable $\\mathcal Q$-semiadditive category.","Using this, we reprove the Mackey functor description of global spectra first obtained by the second-named author and generalize it to $G$-global spectra.","Moreover, we obtain universal characterizations of the categories of $\\mathbb Z$-valued $G$-Mackey profunctors and of quasi-finitely genuine $G$-spectra as studied by Kaledin and Krause-McCandless-Nikolaus, respectively."],"url":"http://arxiv.org/abs/2403.07676v1","category":"math.AT"}
{"created":"2024-03-12 13:58:01","title":"Optical computing with supercontinuum generation in photonic crystal fibers","abstract":"We introduce a novel photonic neural network using photonic crystal fibers, leveraging femtosecond pulse supercontinuum generation for optical computing. Investigating its efficacy across machine learning tasks, we uncover the crucial impact of nonlinear pulse propagation dynamics on network performance. Our findings show that octave-spanning supercontinuum generation results in loss of dataset variety due to many-to-one mapping, and optimal performance requires balancing optical nonlinearity. This study offers guidance for designing energy-efficient and high-performance photonic neural network architectures by explaining the interplay between nonlinear dynamics and optical computing.","sentences":["We introduce a novel photonic neural network using photonic crystal fibers, leveraging femtosecond pulse supercontinuum generation for optical computing.","Investigating its efficacy across machine learning tasks, we uncover the crucial impact of nonlinear pulse propagation dynamics on network performance.","Our findings show that octave-spanning supercontinuum generation results in loss of dataset variety due to many-to-one mapping, and optimal performance requires balancing optical nonlinearity.","This study offers guidance for designing energy-efficient and high-performance photonic neural network architectures by explaining the interplay between nonlinear dynamics and optical computing."],"url":"http://arxiv.org/abs/2403.07667v1","category":"physics.optics"}
{"created":"2024-03-12 13:57:49","title":"Holistic numerical simulation of a quenching process on a real-size multifilamentary superconducting coil","abstract":"Superconductors play a crucial role in the advancement of high-field electromagnets. Unfortunately, their performance can be compromised by thermomagnetic instabilities, wherein the interplay of rapid magnetic and slow heat diffusion can result in catastrophic flux jumps eventually leading to irreversible damage. This issue has long plagued high-$J_c$ Nb$_3$Sn wires at the core of high-field magnets. In this study, we introduce a groundbreaking large-scale GPU-optimized algorithm aimed at tackling the complex intertwined effects of electromagnetism, heating, and strain acting concomitantly during the quenching process of superconducting coils. We validate our model by conducting comparisons with magnetization measurements obtained from short multifilamentary Nb$_3$Sn wires and further experimental tests conducted on solenoid coils while subject to ramping transport currents. Furthermore, leveraging our developed numerical algorithm, we unveil the dynamic propagation mechanisms underlying thermomagnetic instabilities (including flux jumps and quenches) within the coils. Remarkably, our findings reveal that the velocity field of flux jumps and quenches within the coil is correlated with the amount of Joule heating experienced by each wire over a specific time interval, rather than solely being dependent on instantaneous Joule heating or maximum temperature. These insights have the potential to pave the way for optimizing the design of next-generation superconducting magnets, thereby directly influencing a wide array of technologically relevant and multidisciplinary applications.","sentences":["Superconductors play a crucial role in the advancement of high-field electromagnets.","Unfortunately, their performance can be compromised by thermomagnetic instabilities, wherein the interplay of rapid magnetic and slow heat diffusion can result in catastrophic flux jumps eventually leading to irreversible damage.","This issue has long plagued high-$J_c$ Nb$_3$Sn wires at the core of high-field magnets.","In this study, we introduce a groundbreaking large-scale GPU-optimized algorithm aimed at tackling the complex intertwined effects of electromagnetism, heating, and strain acting concomitantly during the quenching process of superconducting coils.","We validate our model by conducting comparisons with magnetization measurements obtained from short multifilamentary Nb$_3$Sn wires and further experimental tests conducted on solenoid coils while subject to ramping transport currents.","Furthermore, leveraging our developed numerical algorithm, we unveil the dynamic propagation mechanisms underlying thermomagnetic instabilities (including flux jumps and quenches) within the coils.","Remarkably, our findings reveal that the velocity field of flux jumps and quenches within the coil is correlated with the amount of Joule heating experienced by each wire over a specific time interval, rather than solely being dependent on instantaneous Joule heating or maximum temperature.","These insights have the potential to pave the way for optimizing the design of next-generation superconducting magnets, thereby directly influencing a wide array of technologically relevant and multidisciplinary applications."],"url":"http://arxiv.org/abs/2403.07666v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 13:54:25","title":"Enabling self-identification in intelligent agent: insights from computational psychoanalysis","abstract":"Building upon prior framework of computational Lacanian psychoanalysis with the theory of active inference, this paper aims to further explore the concept of self-identification and its potential applications. Beginning with two classic paradigms in psychology, mirror self-recognition and rubber hand illusion, we suggest that imaginary identification is characterized by an integrated body schema with minimal free energy. Next, we briefly survey three dimensions of symbolic identification (sociological, psychoanalytic, and linguistical) and corresponding active inference accounts. To provide intuition, we respectively employ a convolutional neural network (CNN) and a multi-layer perceptron (MLP) supervised by ChatGPT to showcase optimization of free energy during motor skill and language mastery underlying identification formation. We then introduce Lacan's Graph II of desire, unifying imaginary and symbolic identification, and propose an illustrative model called FreeAgent. In concluding remarks, we discuss some key issues in the potential of computational Lacanian psychoanalysis to advance mental health and artificial intelligence, including digital twin mind, large language models as avatars of the Lacanian Other, and the feasibility of human-level artificial general intelligence with self-awareness in the context of post-structuralism.","sentences":["Building upon prior framework of computational Lacanian psychoanalysis with the theory of active inference, this paper aims to further explore the concept of self-identification and its potential applications.","Beginning with two classic paradigms in psychology, mirror self-recognition and rubber hand illusion, we suggest that imaginary identification is characterized by an integrated body schema with minimal free energy.","Next, we briefly survey three dimensions of symbolic identification (sociological, psychoanalytic, and linguistical) and corresponding active inference accounts.","To provide intuition, we respectively employ a convolutional neural network (CNN) and a multi-layer perceptron (MLP) supervised by ChatGPT to showcase optimization of free energy during motor skill and language mastery underlying identification formation.","We then introduce Lacan's Graph II of desire, unifying imaginary and symbolic identification, and propose an illustrative model called FreeAgent.","In concluding remarks, we discuss some key issues in the potential of computational Lacanian psychoanalysis to advance mental health and artificial intelligence, including digital twin mind, large language models as avatars of the Lacanian Other, and the feasibility of human-level artificial general intelligence with self-awareness in the context of post-structuralism."],"url":"http://arxiv.org/abs/2403.07664v1","category":"q-bio.NC"}
{"created":"2024-03-12 13:52:42","title":"Tilings of Benzels via Generalized Compression","abstract":"Defant, Li, Propp, and Young recently resolved two enumerative conjectures of Propp concerning the tilings of regions in the hexagonal grid called benzels using two types of prototiles called stones and bones (with varying constraints on allowed orientations of the tiles). Their primary tool, a bijection called compression that converts certain $k$-ribbon tilings to $(k-1)$-ribbon tilings, allowed them to reduce their problems to the enumeration of dimers (i.e., perfect matchings) of certain graphs. We present a generalized version of compression that no longer relies on the perspective of partitions and skew shapes. Using this strengthened tool, we resolve three more of Propp's conjectures and recast several others as problems about perfect matchings.","sentences":["Defant, Li, Propp, and Young recently resolved two enumerative conjectures of Propp concerning the tilings of regions in the hexagonal grid called benzels using two types of prototiles called stones and bones (with varying constraints on allowed orientations of the tiles).","Their primary tool, a bijection called compression that converts certain $k$-ribbon tilings to $(k-1)$-ribbon tilings, allowed them to reduce their problems to the enumeration of dimers (i.e., perfect matchings) of certain graphs.","We present a generalized version of compression that no longer relies on the perspective of partitions and skew shapes.","Using this strengthened tool, we resolve three more of Propp's conjectures and recast several others as problems about perfect matchings."],"url":"http://arxiv.org/abs/2403.07663v1","category":"math.CO"}
{"created":"2024-03-12 13:50:58","title":"Gender-ambiguous voice generation through feminine speaking style transfer in male voices","abstract":"Recently, and under the umbrella of Responsible AI, efforts have been made to develop gender-ambiguous synthetic speech to represent with a single voice all individuals in the gender spectrum. However, research efforts have completely overlooked the speaking style despite differences found among binary and non-binary populations. In this work, we synthesise gender-ambiguous speech by combining the timbre of a male speaker with the manner of speech of a female speaker using voice morphing and pitch shifting towards the male-female boundary. Subjective evaluations indicate that the ambiguity of the morphed samples that convey the female speech style is higher than those that undergo pure pitch transformations suggesting that the speaking style can be a contributing factor in creating gender-ambiguous speech. To our knowledge, this is the first study that explicitly uses the transfer of the speaking style to create gender-ambiguous voices.","sentences":["Recently, and under the umbrella of Responsible AI, efforts have been made to develop gender-ambiguous synthetic speech to represent with a single voice all individuals in the gender spectrum.","However, research efforts have completely overlooked the speaking style despite differences found among binary and non-binary populations.","In this work, we synthesise gender-ambiguous speech by combining the timbre of a male speaker with the manner of speech of a female speaker using voice morphing and pitch shifting towards the male-female boundary.","Subjective evaluations indicate that the ambiguity of the morphed samples that convey the female speech style is higher than those that undergo pure pitch transformations suggesting that the speaking style can be a contributing factor in creating gender-ambiguous speech.","To our knowledge, this is the first study that explicitly uses the transfer of the speaking style to create gender-ambiguous voices."],"url":"http://arxiv.org/abs/2403.07661v1","category":"eess.AS"}
{"created":"2024-03-12 13:47:50","title":"Scalable Spatiotemporal Prediction with Bayesian Neural Fields","abstract":"Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.","sentences":["Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting.","As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems.","This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography.","BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification.","By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent.","We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements.","The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform."],"url":"http://arxiv.org/abs/2403.07657v1","category":"cs.LG"}
{"created":"2024-03-12 13:42:49","title":"OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories","abstract":"How can we discover join relationships among columns of tabular data in a data repository? Can this be done effectively when metadata is missing? Traditional column matching works mainly rely on similarity measures based on exact value overlaps, hence missing important semantics or failing to handle noise in the data. At the same time, recent dataset discovery methods focusing on deep table representation learning techniques, do not take into consideration the rich set of column similarity signals found in prior matching and discovery methods. Finally, existing methods heavily depend on user-provided similarity thresholds, hindering their deployability in real-world settings. In this paper, we propose OmniMatch, a novel join discovery technique that detects equi-joins and fuzzy-joins betwen columns by combining column-pair similarity measures with Graph Neural Networks (GNNs). OmniMatch's GNN can capture column relatedness leveraging graph transitivity, significantly improving the recall of join discovery tasks. At the same time, OmniMatch also increases the precision by augmenting its training data with negative column join examples through an automated negative example generation process. Most importantly, compared to the state-of-the-art matching and discovery methods, OmniMatch exhibits up to 14% higher effectiveness in F1 score and AUC without relying on metadata or user-provided thresholds for each similarity metric.","sentences":["How can we discover join relationships among columns of tabular data in a data repository?","Can this be done effectively when metadata is missing?","Traditional column matching works mainly rely on similarity measures based on exact value overlaps, hence missing important semantics or failing to handle noise in the data.","At the same time, recent dataset discovery methods focusing on deep table representation learning techniques, do not take into consideration the rich set of column similarity signals found in prior matching and discovery methods.","Finally, existing methods heavily depend on user-provided similarity thresholds, hindering their deployability in real-world settings.","In this paper, we propose OmniMatch, a novel join discovery technique that detects equi-joins and fuzzy-joins betwen columns by combining column-pair similarity measures with Graph Neural Networks (GNNs).","OmniMatch's GNN can capture column relatedness leveraging graph transitivity, significantly improving the recall of join discovery tasks.","At the same time, OmniMatch also increases the precision by augmenting its training data with negative column join examples through an automated negative example generation process.","Most importantly, compared to the state-of-the-art matching and discovery methods, OmniMatch exhibits up to 14% higher effectiveness in F1 score and AUC without relying on metadata or user-provided thresholds for each similarity metric."],"url":"http://arxiv.org/abs/2403.07653v1","category":"cs.DB"}
{"created":"2024-03-12 13:35:57","title":"Directionally Tunable Co- and Counter-Propagating Photon Pairs from a Nonlinear Metasurface","abstract":"Nonlinear metasurfaces have recently been established as a new platform for generating photon pairs via spontaneous parametric down-conversion. While for classical harmonic generation in metasurfaces a high level of control over all degrees of freedom of light has been reached, this capability is yet to be developed for photon pair generation. In this work, we theoretically and experimentally demonstrate for the first time precise control of the emission angle of photon pairs generated from a nonlinear metasurface. Our measurements show angularly tunable pair-generation with high coincidence-to-accidental ratio for both co- and counter-propagating emission. The underlying principle is the transverse phase-matching of guided-mode resonances with strong angular dispersion in a nonlinear lithium niobate metagrating. We provide a straightforward design strategy for photon pair generation in such a device and find very good agreement between the calculations and experimental results. Here we use all-optical emission angle tuning by means of the pump wavelength, however the principle could be extended to modulation via the electro-optic effect in lithium niobate. In sum, this work provides an important addition to the toolset of sub-wavelength thickness photon pair sources.","sentences":["Nonlinear metasurfaces have recently been established as a new platform for generating photon pairs via spontaneous parametric down-conversion.","While for classical harmonic generation in metasurfaces a high level of control over all degrees of freedom of light has been reached, this capability is yet to be developed for photon pair generation.","In this work, we theoretically and experimentally demonstrate for the first time precise control of the emission angle of photon pairs generated from a nonlinear metasurface.","Our measurements show angularly tunable pair-generation with high coincidence-to-accidental ratio for both co- and counter-propagating emission.","The underlying principle is the transverse phase-matching of guided-mode resonances with strong angular dispersion in a nonlinear lithium niobate metagrating.","We provide a straightforward design strategy for photon pair generation in such a device and find very good agreement between the calculations and experimental results.","Here we use all-optical emission angle tuning by means of the pump wavelength, however the principle could be extended to modulation via the electro-optic effect in lithium niobate.","In sum, this work provides an important addition to the toolset of sub-wavelength thickness photon pair sources."],"url":"http://arxiv.org/abs/2403.07651v1","category":"physics.optics"}
{"created":"2024-03-12 13:34:46","title":"A Class of Semiparametric Yang and Prentice Frailty Models","abstract":"The Yang and Prentice (YP) regression models have garnered interest from the scientific community due to their ability to analyze data whose survival curves exhibit intersection. These models include proportional hazards (PH) and proportional odds (PO) models as specific cases. However, they encounter limitations when dealing with multivariate survival data due to potential dependencies between the times-to-event. A solution is introducing a frailty term into the hazard functions, making it possible for the times-to-event to be considered independent, given the frailty term. In this study, we propose a new class of YP models that incorporate frailty. We use the exponential distribution, the piecewise exponential distribution (PE), and Bernstein polynomials (BP) as baseline functions. Our approach adopts a Bayesian methodology. The proposed models are evaluated through a simulation study, which shows that the YP frailty models with BP and PE baselines perform similarly to the generator parametric model of the data. We apply the models in two real data sets.","sentences":["The Yang and Prentice (YP) regression models have garnered interest from the scientific community due to their ability to analyze data whose survival curves exhibit intersection.","These models include proportional hazards (PH) and proportional odds (PO) models as specific cases.","However, they encounter limitations when dealing with multivariate survival data due to potential dependencies between the times-to-event.","A solution is introducing a frailty term into the hazard functions, making it possible for the times-to-event to be considered independent, given the frailty term.","In this study, we propose a new class of YP models that incorporate frailty.","We use the exponential distribution, the piecewise exponential distribution (PE), and Bernstein polynomials (BP) as baseline functions.","Our approach adopts a Bayesian methodology.","The proposed models are evaluated through a simulation study, which shows that the YP frailty models with BP and PE baselines perform similarly to the generator parametric model of the data.","We apply the models in two real data sets."],"url":"http://arxiv.org/abs/2403.07650v1","category":"stat.ME"}
{"created":"2024-03-12 13:28:01","title":"Discrete Laplacian thermostat for flocks and swarms: the fully conserved Inertial Spin Model","abstract":"Experiments on bird flocks and midge swarms reveal that these natural systems are well described by an active theory in which conservation laws play a crucial role. By building a symplectic structure that couples the particles' velocities to the generator of their internal rotations (spin), the Inertial Spin Model (ISM) reinstates a second-order temporal dynamics that captures many phenomenological traits of flocks and swarms. The reversible structure of the ISM predicts that the total spin is a constant of motion, the central conservation law responsible for all the novel dynamical features of the model. However, fluctuations and dissipation introduced in the original model to make it relax, violate the spin conservation law, so that the ISM aligns with the biophysical phenomenology only within finite-size regimes, beyond which the overdamped dynamics characteristic of the Vicsek model takes over. Here, we introduce a novel version of the ISM, in which the irreversible terms needed to relax the dynamics strictly respect the conservation of the spin. We perform a numerical investigation of the fully conservative model, exploring both the fixed-network case, which belongs to the equilibrium class of Model G, and the active case, characterized by self-propulsion of the agents and an out-of-equilibrium reshuffling of the underlying interaction network. Our simulations not only capture the correct spin wave phenomenology of the ordered phase, but they also yield dynamical critical exponents in the near-ordering phase that agree very well with the theoretical predictions.","sentences":["Experiments on bird flocks and midge swarms reveal that these natural systems are well described by an active theory in which conservation laws play a crucial role.","By building a symplectic structure that couples the particles' velocities to the generator of their internal rotations (spin), the Inertial Spin Model (ISM) reinstates a second-order temporal dynamics that captures many phenomenological traits of flocks and swarms.","The reversible structure of the ISM predicts that the total spin is a constant of motion, the central conservation law responsible for all the novel dynamical features of the model.","However, fluctuations and dissipation introduced in the original model to make it relax, violate the spin conservation law, so that the ISM aligns with the biophysical phenomenology only within finite-size regimes, beyond which the overdamped dynamics characteristic of the Vicsek model takes over.","Here, we introduce a novel version of the ISM, in which the irreversible terms needed to relax the dynamics strictly respect the conservation of the spin.","We perform a numerical investigation of the fully conservative model, exploring both the fixed-network case, which belongs to the equilibrium class of Model G, and the active case, characterized by self-propulsion of the agents and an out-of-equilibrium reshuffling of the underlying interaction network.","Our simulations not only capture the correct spin wave phenomenology of the ordered phase, but they also yield dynamical critical exponents in the near-ordering phase that agree very well with the theoretical predictions."],"url":"http://arxiv.org/abs/2403.07644v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 13:24:19","title":"Asynchronous Approximate Byzantine Consensus: A Multi-hop Relay Method and Tight Graph Conditions","abstract":"We study a multi-agent resilient consensus problem, where some agents are of the Byzantine type and try to prevent the normal ones from reaching consensus. In our setting, normal agents communicate with each other asynchronously over multi-hop relay channels with delays. To solve this asynchronous Byzantine consensus problem, we develop the multi-hop weighted mean subsequence reduced (MW-MSR) algorithm. The main contribution is that we characterize a tight graph condition for our algorithm to achieve Byzantine consensus, which is expressed in the novel notion of strictly robust graphs. We show that the multi-hop communication is effective for enhancing the network's resilience against Byzantine agents. As a result, we also obtain novel conditions for resilient consensus under the malicious attack model, which are tighter than those known in the literature. Furthermore, the proposed algorithm can be viewed as a generalization of the conventional flooding-based algorithms, with less computational complexity. Lastly, we provide numerical examples to show the effectiveness of the proposed algorithm.","sentences":["We study a multi-agent resilient consensus problem, where some agents are of the Byzantine type and try to prevent the normal ones from reaching consensus.","In our setting, normal agents communicate with each other asynchronously over multi-hop relay channels with delays.","To solve this asynchronous Byzantine consensus problem, we develop the multi-hop weighted mean subsequence reduced (MW-MSR) algorithm.","The main contribution is that we characterize a tight graph condition for our algorithm to achieve Byzantine consensus, which is expressed in the novel notion of strictly robust graphs.","We show that the multi-hop communication is effective for enhancing the network's resilience against Byzantine agents.","As a result, we also obtain novel conditions for resilient consensus under the malicious attack model, which are tighter than those known in the literature.","Furthermore, the proposed algorithm can be viewed as a generalization of the conventional flooding-based algorithms, with less computational complexity.","Lastly, we provide numerical examples to show the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.07640v1","category":"cs.MA"}
{"created":"2024-03-12 13:18:22","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework","abstract":"Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports. This leads to the misalignment with the target disease's textual representation. In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies. This is achieved by consulting a large language model and medical experts. Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations. By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease. Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy. Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively. Our code is released at \\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.","sentences":["Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease.","Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports.","This leads to the misalignment with the target disease's textual representation.","In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies.","This is achieved by consulting a large language model and medical experts.","Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations.","By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease.","Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy.","Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively.","Our code is released at \\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}."],"url":"http://arxiv.org/abs/2403.07636v1","category":"cs.CV"}
{"created":"2024-03-12 13:13:38","title":"Asymptotic dynamics of generalized Kantorovich operators","abstract":"We characterize the family of continuous functions $f\\in C([0,1])$ such that the iterates $\\widehat{T}^{k}_{i} f$ converge uniformly on $[0,1]$, where $\\widehat{T}_i$ is a generalized Kantorovich operator. This gives an affirmative answer to the problem raised in 2021 by Acu and Rasa.","sentences":["We characterize the family of continuous functions $f\\in C([0,1])$ such that the iterates $\\widehat{T}^{k}_{i} f$ converge uniformly on $[0,1]$, where $\\widehat{T}_i$ is a generalized Kantorovich operator.","This gives an affirmative answer to the problem raised in 2021 by Acu and Rasa."],"url":"http://arxiv.org/abs/2403.07633v1","category":"math.PR"}
{"created":"2024-03-12 13:12:24","title":"CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability","abstract":"Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also serve independently as effective components of a virtual screening pipeline. We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates. Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows.","sentences":["Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel.","It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity.","In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity.","The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5","and","CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade.","These models can also serve independently as effective components of a virtual screening pipeline.","We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates.","Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG.","We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows."],"url":"http://arxiv.org/abs/2403.07632v1","category":"cs.LG"}
{"created":"2024-03-12 13:12:11","title":"Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography","abstract":"Navigation in complex 3D scenarios requires appropriate environment representation for efficient scene understanding and trajectory generation. We propose a highly efficient and extensible global navigation framework based on a tomographic understanding of the environment to navigate ground robots in multi-layer structures. Our approach generates tomogram slices using the point cloud map to encode the geometric structure as ground and ceiling elevations. Then it evaluates the scene traversability considering the robot's motion capabilities. Both the tomogram construction and the scene evaluation are accelerated through parallel computation. Our approach further alleviates the trajectory generation complexity compared with planning in 3D spaces directly. It generates 3D trajectories by searching through multiple tomogram slices and separately adjusts the robot height to avoid overhangs. We evaluate our framework in various simulation scenarios and further test it in the real world on a quadrupedal robot. Our approach reduces the scene evaluation time by 3 orders of magnitude and improves the path planning speed by 3 times compared with existing approaches, demonstrating highly efficient global navigation in various complex 3D environments. The code is available at: https://github.com/byangw/PCT_planner.","sentences":["Navigation in complex 3D scenarios requires appropriate environment representation for efficient scene understanding and trajectory generation.","We propose a highly efficient and extensible global navigation framework based on a tomographic understanding of the environment to navigate ground robots in multi-layer structures.","Our approach generates tomogram slices using the point cloud map to encode the geometric structure as ground and ceiling elevations.","Then it evaluates the scene traversability considering the robot's motion capabilities.","Both the tomogram construction and the scene evaluation are accelerated through parallel computation.","Our approach further alleviates the trajectory generation complexity compared with planning in 3D spaces directly.","It generates 3D trajectories by searching through multiple tomogram slices and separately adjusts the robot height to avoid overhangs.","We evaluate our framework in various simulation scenarios and further test it in the real world on a quadrupedal robot.","Our approach reduces the scene evaluation time by 3 orders of magnitude and improves the path planning speed by 3 times compared with existing approaches, demonstrating highly efficient global navigation in various complex 3D environments.","The code is available at: https://github.com/byangw/PCT_planner."],"url":"http://arxiv.org/abs/2403.07631v1","category":"cs.RO"}
{"created":"2024-03-12 13:11:58","title":"Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation","abstract":"Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at https://github.com/Barrett-python/CPAL.","sentences":["Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM).","In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics.","Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances.","The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias.","Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions.","With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension.","The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances.","We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features.","In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision.","Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance.","The project is available at https://github.com/Barrett-python/CPAL."],"url":"http://arxiv.org/abs/2403.07630v1","category":"cs.CV"}
{"created":"2024-03-12 13:09:15","title":"generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation","abstract":"Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.","sentences":["Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation.","However, the considered output candidates of the underlying search algorithm are under-explored and under-explained.","We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs.","To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities.","Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data.","Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods.","Additionally, we demonstrate the applicability of our approach in a qualitative user study.","Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases."],"url":"http://arxiv.org/abs/2403.07627v1","category":"cs.HC"}
{"created":"2024-03-12 13:05:51","title":"Multiple Latent Space Mapping for Compressed Dark Image Enhancement","abstract":"Dark image enhancement aims at converting dark images to normal-light images. Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance. However, in practice, dark images are often compressed before storage or transmission over the Internet. Current methods get poor performance when processing compressed dark images. Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers. Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification. Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space. Therefore, we handle the task in latent space. To this end, we propose a novel latent mapping network based on variational auto-encoder (VAE). Firstly, different from previous VAE-based methods with single-resolution features only, we exploit multiple latent spaces with multi-resolution features, to reduce the detail blur and improve image fidelity. Specifically, we train two multi-level VAEs to project compressed dark images and normal-light images into their latent spaces respectively. Secondly, we leverage a latent mapping network to transform features from compressed dark space to normal-light space. Specifically, since the degradation models of darkness and compression are different from each other, the latent mapping process is divided mapping into enlightening branch and deblocking branch. Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance in compressed dark image enhancement.","sentences":["Dark image enhancement aims at converting dark images to normal-light images.","Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance.","However, in practice, dark images are often compressed before storage or transmission over the Internet.","Current methods get poor performance when processing compressed dark images.","Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers.","Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification.","Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space.","Therefore, we handle the task in latent space.","To this end, we propose a novel latent mapping network based on variational auto-encoder (VAE).","Firstly, different from previous VAE-based methods with single-resolution features only, we exploit multiple latent spaces with multi-resolution features, to reduce the detail blur and improve image fidelity.","Specifically, we train two multi-level VAEs to project compressed dark images and normal-light images into their latent spaces respectively.","Secondly, we leverage a latent mapping network to transform features from compressed dark space to normal-light space.","Specifically, since the degradation models of darkness and compression are different from each other, the latent mapping process is divided mapping into enlightening branch and deblocking branch.","Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance in compressed dark image enhancement."],"url":"http://arxiv.org/abs/2403.07622v1","category":"cs.CV"}
{"created":"2024-03-12 12:59:23","title":"Price Gouging or Market Forces? Fairness Perceptions of Price Hikes in the Pandemic","abstract":"We report the results of surveys we conducted in the US and Israel in 2020, a time when many prices increased following the spread of the pandemic. To assess respondents perceptions of price increases, we focus on goods whose prices have increased during the pandemic, including some essential goods. Consistent with the principle of dual entitlement, we find that respondents perceive price increases as more acceptable if they are due to cost shocks than if they are due to demand shocks. However, we also find large differences across the two populations, as well as across goods.","sentences":["We report the results of surveys we conducted in the US and Israel in 2020, a time when many prices increased following the spread of the pandemic.","To assess respondents perceptions of price increases, we focus on goods whose prices have increased during the pandemic, including some essential goods.","Consistent with the principle of dual entitlement, we find that respondents perceive price increases as more acceptable if they are due to cost shocks than if they are due to demand shocks.","However, we also find large differences across the two populations, as well as across goods."],"url":"http://arxiv.org/abs/2403.07617v1","category":"econ.GN"}
{"created":"2024-03-12 12:56:33","title":"On some Fraisse limits with free amalgamation","abstract":"In the first section of this work we discuss some curiosities around stable Kim-forking. In the second section we give a general way of building some known and some new examples of NSOP1 theories as the limit of some Fraisse class satisfying some strong conditions. These limits will satisfy existence, that Kim independence and algebraic independence coincide over arbitrary sets, that forcing base monotonicity on Kim-independance gives forking independence, and they come with a stationary independence relation. This study is based on the results of Baudish, Ramsey, Chernikov and Kruckman.","sentences":["In the first section of this work we discuss some curiosities around stable Kim-forking.","In the second section we give a general way of building some known and some new examples of NSOP1 theories as the limit of some Fraisse class satisfying some strong conditions.","These limits will satisfy existence, that Kim independence and algebraic independence coincide over arbitrary sets, that forcing base monotonicity on Kim-independance gives forking independence, and they come with a stationary independence relation.","This study is based on the results of Baudish, Ramsey, Chernikov and Kruckman."],"url":"http://arxiv.org/abs/2403.07616v1","category":"math.LO"}
{"created":"2024-03-12 12:51:15","title":"Memory of a Random Walk: Astrometric deflections from gravitational wave memory accumulation over cosmological scales","abstract":"We study the impact of gravitational wave memory on the distribution of far away light sources in the sky. For the first time we compute the built up of small, but permanent tensor distortions of the metric over cosmological time-scales using realistic models of compact binary coalescences (CBCs) whose rate of occurrence is extrapolated at $z\\sim {\\cal O}(1)$. This allows for a consistent computation of the random-walk like evolution of gravitational wave memory which, in turn, is used to estimate the overall shape and magnitude of astrometric deflections of far away sources of light. We find that for pulsar or quasar proper motions, the near-Earth contribution to the astrometric deflections dominates the result and the deflection is analogous to a stochastic gravitational wave memory background that is generally subdominant to the primary stochastic gravitational wave background. We find that this contribution can be within the reach of future surveys such as Theia. Finally, we also study the deviation of the presently observed angular distribution of quasars from perfect isotropy, which arises from the slow build-up of gravitational wave memory over the entire history of the universe. In this case, we find that astrometric deflections depend on the entire light trajectory from the source to the Earth, yielding a quadruple pattern whose magnitude is unlikely to be within reach of the next generation of astrometric surveys due to shot noise and cosmic variance limitations.","sentences":["We study the impact of gravitational wave memory on the distribution of far away light sources in the sky.","For the first time we compute the built up of small, but permanent tensor distortions of the metric over cosmological time-scales using realistic models of compact binary coalescences (CBCs) whose rate of occurrence is extrapolated at $z\\sim {\\cal O}(1)$. This allows for a consistent computation of the random-walk like evolution of gravitational wave memory which, in turn, is used to estimate the overall shape and magnitude of astrometric deflections of far away sources of light.","We find that for pulsar or quasar proper motions, the near-Earth contribution to the astrometric deflections dominates the result and the deflection is analogous to a stochastic gravitational wave memory background that is generally subdominant to the primary stochastic gravitational wave background.","We find that this contribution can be within the reach of future surveys such as Theia.","Finally, we also study the deviation of the presently observed angular distribution of quasars from perfect isotropy, which arises from the slow build-up of gravitational wave memory over the entire history of the universe.","In this case, we find that astrometric deflections depend on the entire light trajectory from the source to the Earth, yielding a quadruple pattern whose magnitude is unlikely to be within reach of the next generation of astrometric surveys due to shot noise and cosmic variance limitations."],"url":"http://arxiv.org/abs/2403.07614v1","category":"astro-ph.CO"}
{"created":"2024-03-12 12:50:19","title":"Imagine a dragon made of seaweed: How images enhance learning in Wikipedia","abstract":"Though images are ubiquitous across Wikipedia, it is not obvious that the image choices optimally support learning. When well selected, images can enhance learning by dual coding, complementing, or supporting articles. When chosen poorly, images can mislead, distract, and confuse. We developed a large dataset containing 470 questions & answers to 94 Wikipedia articles with images on a wide range of topics. Through an online experiment (n=704), we determined whether the images displayed alongside the text of the article are effective in helping readers understand and learn. For certain tasks, such as learning to identify targets visually (e.g., \"which of these pictures is a gujia?\"), article images significantly improve accuracy. Images did not significantly improve general knowledge questions (e.g., \"where are gujia from?\"). Most interestingly, only some images helped with visual knowledge questions (e.g., \"what shape is a gujia?\"). Using our findings, we reflect on the implications for editors and tools to support image selection.","sentences":["Though images are ubiquitous across Wikipedia, it is not obvious that the image choices optimally support learning.","When well selected, images can enhance learning by dual coding, complementing, or supporting articles.","When chosen poorly, images can mislead, distract, and confuse.","We developed a large dataset containing 470 questions & answers to 94 Wikipedia articles with images on a wide range of topics.","Through an online experiment (n=704), we determined whether the images displayed alongside the text of the article are effective in helping readers understand and learn.","For certain tasks, such as learning to identify targets visually (e.g., \"which of these pictures is a gujia?\"), article images significantly improve accuracy.","Images did not significantly improve general knowledge questions (e.g., \"where are gujia from?\").","Most interestingly, only some images helped with visual knowledge questions (e.g., \"what shape is a gujia?\").","Using our findings, we reflect on the implications for editors and tools to support image selection."],"url":"http://arxiv.org/abs/2403.07613v1","category":"cs.HC"}
{"created":"2024-03-12 12:49:47","title":"Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning","abstract":"Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy. Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods. Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts.","sentences":["Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model.","This capability enables data holders to adhere strictly to data protection regulations.","However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage.","In response, this paper introduces a novel class of machine unlearning algorithms.","First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning.","In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model.","The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy.","Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods.","Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning.","Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts."],"url":"http://arxiv.org/abs/2403.07611v1","category":"cs.LG"}
{"created":"2024-03-12 12:49:47","title":"Minimal cellular automaton model with heterogeneous cell sizes predicts epithelial colony growth","abstract":"Regulation of cell proliferation is a crucial aspect of tissue development and homeostasis and plays a major role in morphogenesis, wound healing, and tumor invasion. A phenomenon of such regulation is contact inhibition, which describes the dramatic slowing of proliferation, cell migration and individual cell growth when multiple cells are in contact with each other. While many physiological, molecular and genetic factors are known, the mechanism of contact inhibition is still not fully understood. In particular, the relevance of cellular signaling due to interfacial contact for contact inhibition is still debated. Cellular automata (CA) have been employed in the past as numerically efficient mathematical models to study the dynamics of cell ensembles, but they are not suitable to explore the origins of contact inhibition as such agent-based models assume fixed cell sizes. We develop a minimal, data-driven model to simulate the dynamics of planar cell cultures by extending a probabilistic CA to incorporate size changes of individual cells during growth and cell division. We successfully apply this model to previous in-vitro experiments on contact inhibition in epithelial tissue: After a systematic calibration of the model parameters to measurements of single-cell dynamics, our CA model quantitatively reproduces independent measurements of emergent, culture-wide features, like colony size, cell density and collective cell migration. In particular, the dynamics of the CA model also exhibit the transition from a low-density confluent regime to a stationary postconfluent regime with a rapid decrease in cell size and motion. This implies that the volume exclusion principle, a mechanical constraint which is the only inter-cellular interaction incorporated in the model, paired with a size-dependent proliferation rate is sufficient to generate the observed contact inhibition.","sentences":["Regulation of cell proliferation is a crucial aspect of tissue development and homeostasis and plays a major role in morphogenesis, wound healing, and tumor invasion.","A phenomenon of such regulation is contact inhibition, which describes the dramatic slowing of proliferation, cell migration and individual cell growth when multiple cells are in contact with each other.","While many physiological, molecular and genetic factors are known, the mechanism of contact inhibition is still not fully understood.","In particular, the relevance of cellular signaling due to interfacial contact for contact inhibition is still debated.","Cellular automata (CA) have been employed in the past as numerically efficient mathematical models to study the dynamics of cell ensembles, but they are not suitable to explore the origins of contact inhibition as such agent-based models assume fixed cell sizes.","We develop a minimal, data-driven model to simulate the dynamics of planar cell cultures by extending a probabilistic CA to incorporate size changes of individual cells during growth and cell division.","We successfully apply this model to previous in-vitro experiments on contact inhibition in epithelial tissue: After a systematic calibration of the model parameters to measurements of single-cell dynamics, our CA model quantitatively reproduces independent measurements of emergent, culture-wide features, like colony size, cell density and collective cell migration.","In particular, the dynamics of the CA model also exhibit the transition from a low-density confluent regime to a stationary postconfluent regime with a rapid decrease in cell size and motion.","This implies that the volume exclusion principle, a mechanical constraint which is the only inter-cellular interaction incorporated in the model, paired with a size-dependent proliferation rate is sufficient to generate the observed contact inhibition."],"url":"http://arxiv.org/abs/2403.07612v1","category":"q-bio.CB"}
{"created":"2024-03-12 12:47:32","title":"Couler: Unified Machine Learning Workflow Optimization in Cloud","abstract":"Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions. We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines. This approach alleviates the need to understand various workflow engines' APIs. Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%.","sentences":["Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations.","Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming.","Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs.","Currently, numerous workflow engines are available (with over ten being widely recognized).","This variety poses a challenge for end-users in terms of mastering different engine APIs.","While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   ","In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud.","Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions.","We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines.","This approach alleviates the need to understand various workflow engines' APIs.","Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning.","These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training.","Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%."],"url":"http://arxiv.org/abs/2403.07608v1","category":"cs.DB"}
{"created":"2024-03-12 12:47:30","title":"On Graph Grammars and Games","abstract":"Graph grammars form an interesting area of research because of their versatility in modelling diverse situations with graphs as the structures which are to be manipulated. A new class of graph grammars, nc-eNCE Graph Grammars has been introduced recently with an aim of restricting the order of application of graph production rules, thereby generating different graph classes using the same set of rules. On the other hand 2D game design using an algorithmic approach known as procedural content generation has been of interest recently. In this paper we modify the structure of nc-eNCE graph grammars with the aim of generating directed graphs. We show that employing these graph grammars simplifies the design of 2D games. We have also developed an algorithm which makes use of these graph grammars for generating random game level layouts ensuring that the players will get a different gaming experience each time they play.","sentences":["Graph grammars form an interesting area of research because of their versatility in modelling diverse situations with graphs as the structures which are to be manipulated.","A new class of graph grammars, nc-eNCE Graph Grammars has been introduced recently with an aim of restricting the order of application of graph production rules, thereby generating different graph classes using the same set of rules.","On the other hand 2D game design using an algorithmic approach known as procedural content generation has been of interest recently.","In this paper we modify the structure of nc-eNCE graph grammars with the aim of generating directed graphs.","We show that employing these graph grammars simplifies the design of 2D games.","We have also developed an algorithm which makes use of these graph grammars for generating random game level layouts ensuring that the players will get a different gaming experience each time they play."],"url":"http://arxiv.org/abs/2403.07607v1","category":"cs.FL"}
{"created":"2024-03-12 12:44:34","title":"Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation","abstract":"In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.","sentences":["In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality.","However, producing good negative prompts is manual and tedious.","To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning.","Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set.","Furthermore, with NegOpt we can preferentially optimize the metrics most important to us.","Finally, we construct Negative Prompts DB, a dataset of negative prompts."],"url":"http://arxiv.org/abs/2403.07605v1","category":"cs.CV"}
{"created":"2024-03-12 12:42:28","title":"Approximating many-body quantum states with quantum circuits and measurements","abstract":"We introduce protocols to prepare many-body quantum states with quantum circuits assisted by local operations and classical communication. First, we show that by lifting the requirement of exact preparation, one can substantially save resources. In particular, the so-called $W$ and, more generally, Dicke states require a circuit depth and number of ancillas that are independent of the system size. We also show how one can save resources in the preparation of eigenstates of well-known spin models, both free and interacting. As a biproduct of our work, we introduce an efficient scheme to implement certain non-local, non-Clifford unitary operators.","sentences":["We introduce protocols to prepare many-body quantum states with quantum circuits assisted by local operations and classical communication.","First, we show that by lifting the requirement of exact preparation, one can substantially save resources.","In particular, the so-called $W$ and, more generally, Dicke states require a circuit depth and number of ancillas that are independent of the system size.","We also show how one can save resources in the preparation of eigenstates of well-known spin models, both free and interacting.","As a biproduct of our work, we introduce an efficient scheme to implement certain non-local, non-Clifford unitary operators."],"url":"http://arxiv.org/abs/2403.07604v1","category":"quant-ph"}
{"created":"2024-03-12 12:40:23","title":"ProPML: Probability Partial Multi-label Learning","abstract":"Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \\our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \\our{} outperforms existing approaches, especially for high noise in a candidate set.","sentences":["Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true.","In this paper, we introduce \\our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup.","In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture.","Furthermore, experiments conducted on artificial and real-world datasets indicate that \\our{} outperforms existing approaches, especially for high noise in a candidate set."],"url":"http://arxiv.org/abs/2403.07603v1","category":"cs.LG"}
{"created":"2024-03-12 12:40:08","title":"Unified Source-Free Domain Adaptation","abstract":"In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including closed-set, open-set, partial-set, and generalized settings. Existing methods, focusing on specific scenarios, not only address only a subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. To tackle this unified SFDA problem, we propose a novel approach called Latent Causal Factors Discovery (LCFD). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate LCFD from a causality perspective. The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that LCFD can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.Our code and data are available at https://github.com/tntek/source-free-domain-adaptation.","sentences":["In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including closed-set, open-set, partial-set, and generalized settings.","Existing methods, focusing on specific scenarios, not only address only a subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability.","In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner.","To tackle this unified SFDA problem, we propose a novel approach called Latent Causal Factors Discovery (LCFD).","In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate LCFD from a causality perspective.","The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts.","To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP.","This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees.","Extensive experiments demonstrate that LCFD can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.","Our code and data are available at https://github.com/tntek/source-free-domain-adaptation."],"url":"http://arxiv.org/abs/2403.07601v1","category":"cs.CV"}
{"created":"2024-03-12 12:39:49","title":"Asymptotic $\u03c8$-densities of subsets of natural numbers","abstract":"The sizes of subsets of the natural numbers are typically quantified in terms of asymptotic (linear) and logarithmic densities. These concepts have been generalized to weighted $w$-densities, where a specific weight function $w$ plays a key role. In this paper, a parallel theory of asymptotic $\\psi$-densities is introduced, where the weight is expressed slightly differently in terms of differentiable functions $\\psi$, which are either concave or convex and satisfy certain asymptotic properties. Alternative new proofs for known results on analytic and Abel densities are also given.","sentences":["The sizes of subsets of the natural numbers are typically quantified in terms of asymptotic (linear) and logarithmic densities.","These concepts have been generalized to weighted $w$-densities, where a specific weight function $w$ plays a key role.","In this paper, a parallel theory of asymptotic $\\psi$-densities is introduced, where the weight is expressed slightly differently in terms of differentiable functions $\\psi$, which are either concave or convex and satisfy certain asymptotic properties.","Alternative new proofs for known results on analytic and Abel densities are also given."],"url":"http://arxiv.org/abs/2403.07600v1","category":"math.CA"}
{"created":"2024-03-12 12:35:12","title":"Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference","abstract":"In this paper, we present Mondrian, an edge system that enables high-performance object detection on high-resolution video streams. Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos. To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism. In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of objects and scenes. It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU. Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams. We will release the code after the paper review.","sentences":["In this paper, we present Mondrian, an edge system that enables high-performance object detection on high-resolution video streams.","Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos.","To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism.","In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of objects and scenes.","It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU.","Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams.","We will release the code after the paper review."],"url":"http://arxiv.org/abs/2403.07598v1","category":"cs.CV"}
{"created":"2024-03-12 12:34:02","title":"Generalized paths and cycles in semicomplete multipartite digraphs","abstract":"It is well-known and easy to show that even the following version of the directed travelling salesman problem is NP-complete: Given a strongly connected complete digraph $D=(V,A)$, a cost function $w: A\\rightarrow \\{0,1\\}$ and a natural number $K$; decide whether $D$ has a directed Hamiltonian cycle of cost at most $K$. We study the following variant of this problem for $\\{0,1\\}$-weighted semicomplete digraphs where the set of arcs which have cost 1 form a collection of vertex-disjoint complete digraphs. A digraph is \\textbf{semicomplete multipartite} if it can be obtained from a semicomplete digraph $D$ by choosing a collection of vertex-disjoint subsets $X_1,\\ldots{},X_c$ of $V(D)$ and then deleting all arcs both of whose end-vertices lie inside some $X_i$. Let $D$ be a semicomplete digraph with a cost function $w$ as above, where $w(a)=1$ precisely when $a$ is an arc inside one of the subsets $X_1,\\ldots{},X_c$ and let $D^*$ be the corresponding \\smd{} that we obtain by deleting all arcs inside the $X_i$'s. Then every cycle $C$ of $D$ corresponds to a {\\bf generalized cycle} $C^g$ of $D^*$ which is either the cycle $C$ itself if $w(C)=0$ or a collection of two or more paths that we obtain by deleting all arcs of cost 1 on $C$. Similarly we can define a {\\bf generalized path} $P^g$ in a semicomplete multipartite digraph. The purpose of this paper is to study structural and algorithmic properties of generalized paths and cycles in semicomplete multipartite digraphs. This allows us to identify classes of directed $\\{0,1\\}$-weighted TSP instances that can be solved in polynomial time as well as others for which we can get very close to the optimum in polynomial time. Along with these results we also show that two natural questions about properties of cycles meeting all partite sets in semicomplete multipartite digraphs are NP-complete.","sentences":["It is well-known and easy to show that even the following version of the directed travelling salesman problem is NP-complete: Given a strongly connected complete digraph $D=(V,A)$, a cost function $w: A\\rightarrow \\{0,1\\}$ and a natural number $K$; decide whether $D$ has a directed Hamiltonian cycle of cost at most $K$. We study the following variant of this problem for $\\{0,1\\}$-weighted semicomplete digraphs where the set of arcs which have cost 1 form a collection of vertex-disjoint complete digraphs.","A digraph is \\textbf{semicomplete multipartite} if it can be obtained from a semicomplete digraph $D$ by choosing a collection of vertex-disjoint subsets $X_1,\\ldots{},X_c$ of $V(D)$ and then deleting all arcs both of whose end-vertices lie inside some $X_i$. Let $D$ be a semicomplete digraph with a cost function $w$ as above, where $w(a)=1$ precisely when $a$ is an arc inside one of the subsets $X_1,\\ldots{},X_c$ and let $D^*$ be the corresponding \\smd{} that we obtain by deleting all arcs inside the $X_i$'s.","Then every cycle $C$ of $D$ corresponds to a {\\bf generalized cycle} $C^g$ of $D^*$ which is either the cycle $C$ itself if $w(C)=0$ or a collection of two or more paths that we obtain by deleting all arcs of cost 1 on $C$.","Similarly we can define a {\\bf generalized path} $P^g$ in a semicomplete multipartite digraph.","The purpose of this paper is to study structural and algorithmic properties of generalized paths and cycles in semicomplete multipartite digraphs.","This allows us to identify classes of directed $\\{0,1\\}$-weighted TSP instances that can be solved in polynomial time as well as others for which we can get very close to the optimum in polynomial time.","Along with these results we also show that two natural questions about properties of cycles meeting all partite sets in semicomplete multipartite digraphs are NP-complete."],"url":"http://arxiv.org/abs/2403.07597v1","category":"math.CO"}
{"created":"2024-03-12 12:25:54","title":"MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions","abstract":"This paper presents MinkUNeXt, an effective and efficient architecture for place-recognition from point clouds entirely based on the new 3D MinkNeXt Block, a residual block composed of 3D sparse convolutions that follows the philosophy established by recent Transformers but purely using simple 3D convolutions. Feature extraction is performed at different scales by a U-Net encoder-decoder network and the feature aggregation of those features into a single descriptor is carried out by a Generalized Mean Pooling (GeM). The proposed architecture demonstrates that it is possible to surpass the current state-of-the-art by only relying on conventional 3D sparse convolutions without making use of more complex and sophisticated proposals such as Transformers, Attention-Layers or Deformable Convolutions. A thorough assessment of the proposal has been carried out using the Oxford RobotCar and the In-house datasets. As a result, MinkUNeXt proves to outperform other methods in the state-of-the-art.","sentences":["This paper presents MinkUNeXt, an effective and efficient architecture for place-recognition from point clouds entirely based on the new 3D MinkNeXt Block, a residual block composed of 3D sparse convolutions that follows the philosophy established by recent Transformers but purely using simple 3D convolutions.","Feature extraction is performed at different scales by a U-Net encoder-decoder network and the feature aggregation of those features into a single descriptor is carried out by a Generalized Mean Pooling (GeM).","The proposed architecture demonstrates that it is possible to surpass the current state-of-the-art by only relying on conventional 3D sparse convolutions without making use of more complex and sophisticated proposals such as Transformers, Attention-Layers or Deformable Convolutions.","A thorough assessment of the proposal has been carried out using the Oxford RobotCar and the In-house datasets.","As a result, MinkUNeXt proves to outperform other methods in the state-of-the-art."],"url":"http://arxiv.org/abs/2403.07593v1","category":"cs.CV"}
{"created":"2024-03-12 12:18:20","title":"Perennial Semantic Data Terms of Use for Decentralized Web","abstract":"In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a ``perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.","sentences":["In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations.","Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'.","However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods.","This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore.","This compromises user autonomy and impedes detection of data misuse.","We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner.","Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations.","Automated reasoning verifies compliance, and also derives policies for output data.","This constitutes a ``perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles.","Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine.","It ensures seamless integration with other semantic tools for enhanced interoperability.","We have successfully integrated this language into the Solid framework, and conducted performance benchmark.","We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability."],"url":"http://arxiv.org/abs/2403.07587v1","category":"cs.AI"}
{"created":"2024-03-12 12:16:40","title":"Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments","abstract":"As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings. Federated Averaging (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits.","sentences":["As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial.","For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences.","In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others.","Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings.","Federated Averaging (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits."],"url":"http://arxiv.org/abs/2403.07586v1","category":"cs.LG"}
{"created":"2024-03-12 12:15:57","title":"Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities","abstract":"The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers. Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training. As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time. Therefore, optimizing communication for distributed training has become an urgent issue. In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm. We then review current representative research advances with this three-layer paradigm. We find that layers in the current three-layer paradigm are relatively independent, but there is a rich design space for cross-layer collaborative optimization in distributed training scenarios. Therefore, we further advocate a communication-efficient five-layer paradigm underlining opportunities for collaboration designs and look forward to the perspectives of \"Vertical\", \"Horizontal\", \"Intra-Inter\" and \"Host-Net\" collaboration designs. We hope this article can shed some light on future research on communication optimization for distributed training.","sentences":["The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers.","Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training.","As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time.","Therefore, optimizing communication for distributed training has become an urgent issue.","In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm.","We then review current representative research advances with this three-layer paradigm.","We find that layers in the current three-layer paradigm are relatively independent, but there is a rich design space for cross-layer collaborative optimization in distributed training scenarios.","Therefore, we further advocate a communication-efficient five-layer paradigm underlining opportunities for collaboration designs and look forward to the perspectives of \"Vertical\", \"Horizontal\", \"Intra-Inter\" and \"Host-Net\" collaboration designs.","We hope this article can shed some light on future research on communication optimization for distributed training."],"url":"http://arxiv.org/abs/2403.07585v1","category":"cs.DC"}
{"created":"2024-03-12 12:10:22","title":"A Continuum Theory of Elastic Semiconductors with Consideration of Mobile Charge Inertia","abstract":"A set of nonlinear equations for the macroscopic theory of elastic semiconductors is derived which generalizes the seminal work of Tiersten by including the inertia of the mobile charge carriers. The equations obtained can describe carrier plasma waves and their interactions with elastic waves. Another generalization in this paper is that the internal energy densities of the mobile charges are allowed to depend on temperature in addition to charge densities. The equations are in SI units.","sentences":["A set of nonlinear equations for the macroscopic theory of elastic semiconductors is derived which generalizes the seminal work of Tiersten by including the inertia of the mobile charge carriers.","The equations obtained can describe carrier plasma waves and their interactions with elastic waves.","Another generalization in this paper is that the internal energy densities of the mobile charges are allowed to depend on temperature in addition to charge densities.","The equations are in SI units."],"url":"http://arxiv.org/abs/2403.07582v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 12:10:18","title":"LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model","abstract":"Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection.","sentences":["Personality detection aims to detect one's personality traits underlying in social media posts.","One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires.","Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels.","This leads to inferior quality of post features and consequently affects the performance.","In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them.","In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task.","Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection.","By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection.","Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance.","Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection."],"url":"http://arxiv.org/abs/2403.07581v1","category":"cs.CL"}
{"created":"2024-03-12 12:04:58","title":"Emergent (2+1)D topological orders from iterative (1+1)D gauging","abstract":"Gauging involves introducing new degrees of freedom, known as gauge fields, to localize an existing global symmetry. It is known that, following this process, the gauge fields exhibit a dual global symmetry. Subsequently, one can gauge this emergent global symmetry by creating new gauge fields that once again exhibit a global symmetry. We investigate this iterative process, wherein new degrees of freedom are created and entangled with the previous ones through local symmetries. We focus on gauging spin chains with Abelian group symmetries and arranging the new spins on a 2D lattice. The local symmetries of the emergent 2D state, which are modified by the concatenation of the following gauging maps surprisingly correspond to the stabilizer terms of the $XZZX$-code generalized to any Abelian group. We encode our construction in the family of tensor network states that we dub ``projected entangled pair emergent states'' (PEPES). By utilizing this representation and by considering the local symmetries as stabilizer Hamiltonian terms, we establish a connection between the condensable anyons at the boundary and the quantum phase of the initial symmetric state before the gauging process.","sentences":["Gauging involves introducing new degrees of freedom, known as gauge fields, to localize an existing global symmetry.","It is known that, following this process, the gauge fields exhibit a dual global symmetry.","Subsequently, one can gauge this emergent global symmetry by creating new gauge fields that once again exhibit a global symmetry.","We investigate this iterative process, wherein new degrees of freedom are created and entangled with the previous ones through local symmetries.","We focus on gauging spin chains with Abelian group symmetries and arranging the new spins on a 2D lattice.","The local symmetries of the emergent 2D state, which are modified by the concatenation of the following gauging maps surprisingly correspond to the stabilizer terms of the $XZZX$-code generalized to any Abelian group.","We encode our construction in the family of tensor network states that we dub ``projected entangled pair emergent states'' (PEPES).","By utilizing this representation and by considering the local symmetries as stabilizer Hamiltonian terms, we establish a connection between the condensable anyons at the boundary and the quantum phase of the initial symmetric state before the gauging process."],"url":"http://arxiv.org/abs/2403.07575v1","category":"quant-ph"}
{"created":"2024-03-12 12:03:16","title":"Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)","abstract":"In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research.","sentences":["In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites.","The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration.","While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources.","Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements.","ACNC encompasses two primary functionalities: state recognition and context detection.","Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure.","To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently.","These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources.","The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research."],"url":"http://arxiv.org/abs/2403.07573v1","category":"cs.NI"}
{"created":"2024-03-12 11:56:22","title":"Theoretical demonstration of mode transmission in ZGP-based micrometer waveguide platforms","abstract":"Birefringence phase-matching based \\c{hi}(2) ZnGeP2 (ZGP) waveguide platform has been recently reported for excellent mid-infrared laser generation. Here, a detailed theoretical characterization of mode transmission taking waveguide anisotropy and substrate material absorption into account in a micrometer ZGP waveguide platform (ZGP-on-SiO2) is conducted. Benefited from high-index contrast between ZGP and substrate (SiO2/Air), Transverse electric and magnetic (TM and TE) mode transmission loss at interested wavelengths range of 2 - 12 {\\mu}m is calculated to be less than 4 dB/cm and 1.5 dB/cm, respectively, in the designed ZGP waveguide. Notably, non-obvious oscillation of mode transmission loss versus phase-matching angles is observed, which is different from that in the previously reported weakly guided anisotropic waveguide. A vital phenomenon named mode crossing at some wavelengths in TM polarization is also exhibited in our waveguide platforms, which jeopardizes waveguide performances and could be avoided by changing the phase-matching angle in practice. This work provides a significant indication of ZGP waveguide design optimization in future and also exhibits extendibility to other birefringent crystal waveguide platforms.","sentences":["Birefringence phase-matching based \\c{hi}(2) ZnGeP2 (ZGP) waveguide platform has been recently reported for excellent mid-infrared laser generation.","Here, a detailed theoretical characterization of mode transmission taking waveguide anisotropy and substrate material absorption into account in a micrometer ZGP waveguide platform (ZGP-on-SiO2) is conducted.","Benefited from high-index contrast between ZGP and substrate (SiO2/Air), Transverse electric and magnetic (TM and TE) mode transmission loss at interested wavelengths range of 2 - 12 {\\mu}m is calculated to be less than 4 dB/cm and 1.5 dB/cm, respectively, in the designed ZGP waveguide.","Notably, non-obvious oscillation of mode transmission loss versus phase-matching angles is observed, which is different from that in the previously reported weakly guided anisotropic waveguide.","A vital phenomenon named mode crossing at some wavelengths in TM polarization is also exhibited in our waveguide platforms, which jeopardizes waveguide performances and could be avoided by changing the phase-matching angle in practice.","This work provides a significant indication of ZGP waveguide design optimization in future and also exhibits extendibility to other birefringent crystal waveguide platforms."],"url":"http://arxiv.org/abs/2403.07568v1","category":"physics.optics"}
{"created":"2024-03-12 11:53:27","title":"Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation","abstract":"Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short. Fine-tuning machine translation models emerges as the best method overall. These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal. We conclude with a qualitative analysis of generation errors and an ablation study.","sentences":["Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored.","In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative.","We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques.","We also develop an evaluation framework for T2X that measures how accurately generated text describes the data.","This enables future users of T2X to go beyond surface-level metrics in evaluation.","On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs).","We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG).","It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish).","We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short.","Fine-tuning machine translation models emerges as the best method overall.","These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal.","We conclude with a qualitative analysis of generation errors and an ablation study."],"url":"http://arxiv.org/abs/2403.07567v1","category":"cs.CL"}
{"created":"2024-03-12 11:53:00","title":"An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning","abstract":"Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is more efficient and reduces the influence from biasing targets. Our proposed method converges faster and achieves higher cumulative rewards compared to the benchmark in the same training environment, and improves the time-in-range (TIR), the percentage of time the patient's BG is within the target range, in the evaluation phase. Our work validates the effectiveness of multi-step reinforcement learning in BG control, which may help to explore the optimal glycemic control measure and improve the survival of diabetic patients.","sentences":["Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes.","However,traditional patient self-management is cumbersome and risky.","Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach.","In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem.","The Prioritized Experience Replay (PER) sampling method is also used in it.","Compared to single-step bootstrapped updates, multi-step learning is more efficient and reduces the influence from biasing targets.","Our proposed method converges faster and achieves higher cumulative rewards compared to the benchmark in the same training environment, and improves the time-in-range (TIR), the percentage of time the patient's BG is within the target range, in the evaluation phase.","Our work validates the effectiveness of multi-step reinforcement learning in BG control, which may help to explore the optimal glycemic control measure and improve the survival of diabetic patients."],"url":"http://arxiv.org/abs/2403.07566v1","category":"cs.AI"}
{"created":"2024-03-12 11:52:11","title":"Logarithmic critical slowing down in complex systems: from statics to dynamics","abstract":"We consider second-order phase transitions in which the order parameter is a replicated overlap matrix. We focus on a tricritical point that occurs in a variety of mean-field models and that, more generically, describes higher order liquid-liquid or liquid-glass transitions. We show that the static replicated theory implies slowing down with a logarithmic decay in time. The dynamical equations turn out to be those predicted by schematic Mode Coupling Theory for supercooled viscous liquids at a $A_3$ singularity, where the parameter exponent is $\\lambda=1$. We obtain a quantitative expression for the parameter $\\mu$ of the logarithmic decay in terms of cumulants of the overlap, which are physically observable in experiments or numerical simulations.","sentences":["We consider second-order phase transitions in which the order parameter is a replicated overlap matrix.","We focus on a tricritical point that occurs in a variety of mean-field models and that, more generically, describes higher order liquid-liquid or liquid-glass transitions.","We show that the static replicated theory implies slowing down with a logarithmic decay in time.","The dynamical equations turn out to be those predicted by schematic Mode Coupling Theory for supercooled viscous liquids at a $A_3$ singularity, where the parameter exponent is $\\lambda=1$. We obtain a quantitative expression for the parameter $\\mu$ of the logarithmic decay in terms of cumulants of the overlap, which are physically observable in experiments or numerical simulations."],"url":"http://arxiv.org/abs/2403.07565v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-12 11:51:59","title":"RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model","abstract":"The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection. However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model. RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts. Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities.","sentences":["The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc.","Remote sensing image building interpretation primarily encompasses building extraction and change detection.","However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge.","Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization.","In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model.","RSBuilding is designed to enhance cross-scene generalization and task universality.","Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information.","To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts.","Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks.","Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets.","The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities."],"url":"http://arxiv.org/abs/2403.07564v1","category":"cs.CV"}
{"created":"2024-03-12 11:51:55","title":"Learning Generalizable Feature Fields for Mobile Manipulation","abstract":"An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as well as running time, when performing open-vocabulary mobile manipulation in dynamic scenes.","sentences":["An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects.","The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale.","In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time.","To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation.","We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator.","We evaluate GeFF's ability to generalize to open-set objects as well as running time, when performing open-vocabulary mobile manipulation in dynamic scenes."],"url":"http://arxiv.org/abs/2403.07563v1","category":"cs.RO"}
{"created":"2024-03-12 11:48:49","title":"Unleashing Network Potentials for Semantic Scene Completion","abstract":"Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs. However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets. To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates. The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition. Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality. The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness. Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods.","sentences":["Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs.","However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets.","To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates.","The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition.","Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality.","The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness.","Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods."],"url":"http://arxiv.org/abs/2403.07560v1","category":"cs.CV"}
{"created":"2024-03-12 11:47:12","title":"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding","abstract":"Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF.","sentences":["Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability.","Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive.","However, existing works still struggle in structured environments with high obstacle density and a high number of agents.","To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH).","We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm.","We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase.","First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones.","Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations.","Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions.","We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF."],"url":"http://arxiv.org/abs/2403.07559v1","category":"cs.MA"}
{"created":"2024-03-12 11:40:44","title":"Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts","abstract":"Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental results show that TACS can effectively filter information in context and significantly improve the overall quality of LLMs' responses when presented with misleading information.","sentences":["Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations.","To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs.","TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM.","Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context.","Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information.","Experimental results show that TACS can effectively filter information in context and significantly improve the overall quality of LLMs' responses when presented with misleading information."],"url":"http://arxiv.org/abs/2403.07556v1","category":"cs.CL"}
{"created":"2024-03-12 11:40:20","title":"Steady Granular Flow in a Rotating Drum: Universal description of stress, velocity and packing fraction profiles covering grain shape effects from convex to very concave","abstract":"The flow behavior of granular matter is significantly influenced by the shape of constituent particles. This effect is particularly pronounced for very concave particles, which exhibit unique flow characteristics such as higher porosity and sharper phase transitions between jamming and unjamming states. Despite the richness and ubiquitousness of these systems, our understanding of their intricate flow behavior and the local mechanisms driving these behaviors remains incomplete. In this work, we investigate the effect of particle shape, ranging from spherical to highly concave, on steady flows in a rotating drum - a system that facilitates a continuous phase transition from a jamming state at greater depths to an unjamming state at shallower regions. We develop an analytical model to elucidate granular behavior within the rotating drum: (i) Firstly, by decomposing the shear stress, we reconcile the discrepancy between simulation data and theoretical predictions, establishing a relationship with the angle of repose. (ii)Secondly, we extend the generalized Bagnold scaling , coupled with a non-local fluidity relation based on packing fraction, providing a framework for a correlation between shear stress, shear rate, and packing fraction. Additionally, we introduce a characteristic length to quantify the influence of particle shape and drum speed. This analytical model offers explicit functional forms for physical quantity profiles, which are validated experimentally in a thin rotating drum and numerically in a two-dimensional rotating drum. Our results demonstrate that this model accurately describes the change of velocity due to the phase transition of granular flow within a rotating drum. Moreover, for different shapes of particle and drum speeds, the characteristic length captures the interplay between shear stress, shear rate, and the variation of packing fraction.","sentences":["The flow behavior of granular matter is significantly influenced by the shape of constituent particles.","This effect is particularly pronounced for very concave particles, which exhibit unique flow characteristics such as higher porosity and sharper phase transitions between jamming and unjamming states.","Despite the richness and ubiquitousness of these systems, our understanding of their intricate flow behavior and the local mechanisms driving these behaviors remains incomplete.","In this work, we investigate the effect of particle shape, ranging from spherical to highly concave, on steady flows in a rotating drum - a system that facilitates a continuous phase transition from a jamming state at greater depths to an unjamming state at shallower regions.","We develop an analytical model to elucidate granular behavior within the rotating drum: (i) Firstly, by decomposing the shear stress, we reconcile the discrepancy between simulation data and theoretical predictions, establishing a relationship with the angle of repose.","(ii)Secondly, we extend the generalized Bagnold scaling , coupled with a non-local fluidity relation based on packing fraction, providing a framework for a correlation between shear stress, shear rate, and packing fraction.","Additionally, we introduce a characteristic length to quantify the influence of particle shape and drum speed.","This analytical model offers explicit functional forms for physical quantity profiles, which are validated experimentally in a thin rotating drum and numerically in a two-dimensional rotating drum.","Our results demonstrate that this model accurately describes the change of velocity due to the phase transition of granular flow within a rotating drum.","Moreover, for different shapes of particle and drum speeds, the characteristic length captures the interplay between shear stress, shear rate, and the variation of packing fraction."],"url":"http://arxiv.org/abs/2403.07555v1","category":"cond-mat.soft"}
{"created":"2024-03-12 11:39:46","title":"An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes","abstract":"Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system, leading to enhanced accuracy of maintenance policies and, consequently, increasing the effectiveness of the equipment. In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables based on a Hidden Markov Model with covariates (IO-HMM). These covariates account for the correlation of the predicted variables with their past values and additional process measurements by means of a discrete model and a continuous model. The probabilities of the former are updated using Bayesian principles, while the parameter estimates for the latter are recursively computed through an adaptive algorithm that also admits a Bayesian interpretation. This approach permits the integration of new samples into the estimation of unknown parameters, computationally improving the efficiency of the process. We evaluate the performance of the method using a real data set obtained from a company of a particular sector; however, it is a versatile technique applicable to any other data set. The results show a consistent improvement over a persistence model, which assumes that future values are the same as current values, and more importantly, over univariate versions of our model.","sentences":["Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system, leading to enhanced accuracy of maintenance policies and, consequently, increasing the effectiveness of the equipment.","In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables based on a Hidden Markov Model with covariates (IO-HMM).","These covariates account for the correlation of the predicted variables with their past values and additional process measurements by means of a discrete model and a continuous model.","The probabilities of the former are updated using Bayesian principles, while the parameter estimates for the latter are recursively computed through an adaptive algorithm that also admits a Bayesian interpretation.","This approach permits the integration of new samples into the estimation of unknown parameters, computationally improving the efficiency of the process.","We evaluate the performance of the method using a real data set obtained from a company of a particular sector; however, it is a versatile technique applicable to any other data set.","The results show a consistent improvement over a persistence model, which assumes that future values are the same as current values, and more importantly, over univariate versions of our model."],"url":"http://arxiv.org/abs/2403.07554v1","category":"stat.AP"}
{"created":"2024-03-12 11:39:18","title":"The future of document indexing: GPT and Donut revolutionize table of content processing","abstract":"Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effectively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting efficiency and liberating critical resources in various industries.","sentences":["Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured information a major bottleneck.","This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5","Turbo, a robust large language model.","The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data.","Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5","Turbo reaching 89% in effectively organizing the ToCs.","This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting efficiency and liberating critical resources in various industries."],"url":"http://arxiv.org/abs/2403.07553v1","category":"cs.IR"}
{"created":"2024-03-12 11:38:17","title":"Springer correspondence and mirror symmetries for parabolic Hitchin systems","abstract":"We prove the Strominger--Yau--Zaslow and topological mirror symmetries for parabolic Hitchin systems of types B and C. In contrast to type A, a geometric reinterpretation of Springer duality is necessary. Furthermore, unlike Hitchin's construction in the non-parabolic case, the map between generic fibers in type B and C needs more analysis due to the change of partitions of Springer dual nilpotent orbits, which is the main difficulty in this article. To tackle this challenge, we first construct and study the geometry of the generic Hitchin fibers of moduli spaces of Higgs bundles associated to the nilpotent orbit closures. Then we study their relation with the generic Hitchin fibers of parabolic Hitchin systems. Along this way, we establish intriguing connections between Springer duality, Kazhdan--Lusztig maps, and singularities of spectral curves, and uncover a new geometric interpretation of Lusztig's canonical quotient.","sentences":["We prove the Strominger--Yau--Zaslow and topological mirror symmetries for parabolic Hitchin systems of types B and C.","In contrast to type A, a geometric reinterpretation of Springer duality is necessary.","Furthermore, unlike Hitchin's construction in the non-parabolic case, the map between generic fibers in type B and C needs more analysis due to the change of partitions of Springer dual nilpotent orbits, which is the main difficulty in this article.","To tackle this challenge, we first construct and study the geometry of the generic Hitchin fibers of moduli spaces of Higgs bundles associated to the nilpotent orbit closures.","Then we study their relation with the generic Hitchin fibers of parabolic Hitchin systems.","Along this way, we establish intriguing connections between Springer duality, Kazhdan--Lusztig maps, and singularities of spectral curves, and uncover a new geometric interpretation of Lusztig's canonical quotient."],"url":"http://arxiv.org/abs/2403.07552v1","category":"math.AG"}
{"created":"2024-03-12 11:37:36","title":"Isolated nearly flat higher Chern band in monolayer transition metal trihalides","abstract":"The interplay between non-trivial topology and strong electron interaction can generate a variety of exotic quantum matter. Here we theoretically propose that monolayer transition metal trihalides MoF$_3$ and WI$_3$ have isolated nearly flat band near the Fermi level with higher Chern number $\\mathcal{C}=+3$ and $\\mathcal{C}=-2$, respectively. The nontrivial topology of these flat Chern bands originates from the effective $sd^2$ hybridization of transition metal atom, which transform the apparent atomic $d$ orbitals on a hexagonal lattice into $(s, p_+, p_-)$ orbitals on a triangular lattice. Interestingly, the quantum geometry of flat Chern bands in both materials are comparable with those in moir\\'e systems exhibiting fractional Chern insulator state. The Hofstadter butterfly of such flat Chern bands are further studied. These natural materials, if realized experimentally, could offer new platforms to explore correlated phenomena driven by flat Chern band with higher Chern number.","sentences":["The interplay between non-trivial topology and strong electron interaction can generate a variety of exotic quantum matter.","Here we theoretically propose that monolayer transition metal trihalides MoF$_3$ and WI$_3$ have isolated nearly flat band near the Fermi level with higher Chern number $\\mathcal{C}=+3$ and $\\mathcal{C}=-2$, respectively.","The nontrivial topology of these flat Chern bands originates from the effective $sd^2$ hybridization of transition metal atom, which transform the apparent atomic $d$ orbitals on a hexagonal lattice into $(s, p_+, p_-)$ orbitals on a triangular lattice.","Interestingly, the quantum geometry of flat Chern bands in both materials are comparable with those in moir\\'e systems exhibiting fractional Chern insulator state.","The Hofstadter butterfly of such flat Chern bands are further studied.","These natural materials, if realized experimentally, could offer new platforms to explore correlated phenomena driven by flat Chern band with higher Chern number."],"url":"http://arxiv.org/abs/2403.07551v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 11:37:20","title":"The post--quasi-static approximation: An analytical approach to gravitational collapse","abstract":"A semi--numerical approach proposed many years ago for describing gravitational collapse in the post--quasi--static approximation, is modified in order to avoid the numerical integration of the basic differential equations the approach is based upon. For doing that we have to impose some restrictions on the fluid distribution. More specifically, we shall assume the vanishing complexity factor condition, which allows for analytical integration of the pertinent differential equations and leads to physically interesting models. Instead, we show that neither the homologous nor the quasi--homologous evolution are acceptable since they lead to geodesic fluids, which are unsuitable for being described in the post--quasi--static approximation. Also, we prove that, within this approximation, adiabatic evolution also leads to geodesic fluids and therefore we shall consider exclusively dissipative systems. Besides the vanishing complexity factor condition, additional information is required for a full description of models. We shall propose different strategies for obtaining such an information, which are based on observables quantities (e.g. luminosity and redshift), and/or heuristic mathematical ansatz. To illustrate the method, we present two models. One model is inspired in the well known Schwarzschild interior solution, and another one is inspired in Tolman VI solution.","sentences":["A semi--numerical approach proposed many years ago for describing gravitational collapse in the post--quasi--static approximation, is modified in order to avoid the numerical integration of the basic differential equations the approach is based upon.","For doing that we have to impose some restrictions on the fluid distribution.","More specifically, we shall assume the vanishing complexity factor condition, which allows for analytical integration of the pertinent differential equations and leads to physically interesting models.","Instead, we show that neither the homologous nor the quasi--homologous evolution are acceptable since they lead to geodesic fluids, which are unsuitable for being described in the post--quasi--static approximation.","Also, we prove that, within this approximation, adiabatic evolution also leads to geodesic fluids and therefore we shall consider exclusively dissipative systems.","Besides the vanishing complexity factor condition, additional information is required for a full description of models.","We shall propose different strategies for obtaining such an information, which are based on observables quantities (e.g. luminosity and redshift), and/or heuristic mathematical ansatz.","To illustrate the method, we present two models.","One model is inspired in the well known Schwarzschild interior solution, and another one is inspired in Tolman VI solution."],"url":"http://arxiv.org/abs/2403.07550v1","category":"gr-qc"}
{"created":"2024-03-12 11:33:48","title":"Online Continual Learning For Interactive Instruction Following Agents","abstract":"In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state of the art in our empirical validations by noticeable margins. The project page including codes is https://github.com/snumprlab/cl-alfred.","sentences":["In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning.","We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it.","To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL)","For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks.","However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available.","Here, we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA).","In the proposed Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state of the art in our empirical validations by noticeable margins.","The project page including codes is https://github.com/snumprlab/cl-alfred."],"url":"http://arxiv.org/abs/2403.07548v2","category":"cs.AI"}
{"created":"2024-03-12 11:32:47","title":"Homological stability for the Cremona groups","abstract":"The Cremona groups are the groups of all birational equivalences of projective spaces and, equivalently, the automorphism groups of the rational function fields. We construct highly connected spaces on which these groups act in a way that allows us to deduce that their abelianisations, and more generally, the homologies of these groups, stabilise as the dimension increases.","sentences":["The Cremona groups are the groups of all birational equivalences of projective spaces and, equivalently, the automorphism groups of the rational function fields.","We construct highly connected spaces on which these groups act in a way that allows us to deduce that their abelianisations, and more generally, the homologies of these groups, stabilise as the dimension increases."],"url":"http://arxiv.org/abs/2403.07546v1","category":"math.AG"}
{"created":"2024-03-12 11:32:43","title":"Artin-Schreier quandles of involutions in absolute Galois groups","abstract":"We introduce a new invariant of fields that refines their real spectrum and is related to their absolute Galois group: the Artin-Schreier quandle. For formally real number fields, it is freely generated in its variety by a Cantor space of indeterminates. For Laurent series fields, we compute it in terms of the Artin-Schreier quandle of the coefficient field. This result and other examples show that, in general, there are relations.","sentences":["We introduce a new invariant of fields that refines their real spectrum and is related to their absolute Galois group: the Artin-Schreier quandle.","For formally real number fields, it is freely generated in its variety by a Cantor space of indeterminates.","For Laurent series fields, we compute it in terms of the Artin-Schreier quandle of the coefficient field.","This result and other examples show that, in general, there are relations."],"url":"http://arxiv.org/abs/2403.07545v1","category":"math.NT"}
{"created":"2024-03-12 11:27:47","title":"Process Modeling With Large Language Models","abstract":"In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance flexibility, efficiency, and accessibility of process modeling for both expert and non-expert users. We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework's ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field.","sentences":["In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes.","Traditional process modeling methods often require extensive expertise and can be time-consuming.","This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance flexibility, efficiency, and accessibility of process modeling for both expert and non-expert users.","We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions.","Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism.","Moreover, we instantiate a concrete system extending our framework.","This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets.","Preliminary results demonstrate the framework's ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field."],"url":"http://arxiv.org/abs/2403.07541v1","category":"cs.SE"}
{"created":"2024-03-12 11:26:58","title":"WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces","abstract":"Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a ransomware emulator. This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be used to mimic the I/O behavior of existing ransomware thereby enabling safe trace collection. Both the emulator and its application represent significant steps forward in ransomware detection in the era of machine-learning-driven cybersecurity.","sentences":["Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide.","Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats.","This paper introduces three primary contributions to address this challenge.","First, we introduce a ransomware emulator.","This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior.","Second, we demonstrate how we use this emulator to create storage I/O traces.","These traces are then utilized to train machine-learning models.","Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools.","Third, we show how our emulator can be used to mimic the I/O behavior of existing ransomware thereby enabling safe trace collection.","Both the emulator and its application represent significant steps forward in ransomware detection in the era of machine-learning-driven cybersecurity."],"url":"http://arxiv.org/abs/2403.07540v1","category":"cs.CR"}
{"created":"2024-03-12 11:23:08","title":"On rainbow domination of cubic graphs","abstract":"The structure of minimal weight rainbow domination functions of cubic graphs are studied. Based on general observations for cubic graphs, generalized Petersen graphs $P(ck,k)$ are characterized whose 4- and 5-rainbow domination numbers equal the general lower bounds. As $t$-rainbow domination of cubic graphs for $t \\ge 6$ is trivial, characterizations of such generalized Petersen graphs $P(ck,k)$ are known for all $t$-rainbow domination numbers.In addition, new upper bounds for 4- and 5-rainbow domination numbers that are valid for all $P(ck,k)$ are provided.","sentences":["The structure of minimal weight rainbow domination functions of cubic graphs are studied.","Based on general observations for cubic graphs, generalized Petersen graphs $P(ck,k)$ are characterized whose 4- and 5-rainbow domination numbers equal the general lower bounds.","As $t$-rainbow domination of cubic graphs for $t \\ge 6$ is trivial, characterizations of such generalized Petersen graphs $P(ck,k)$ are known for all $t$-rainbow domination numbers.","In addition, new upper bounds for 4- and 5-rainbow domination numbers that are valid for all $P(ck,k)$ are provided."],"url":"http://arxiv.org/abs/2403.07538v1","category":"math.CO"}
{"created":"2024-03-12 11:06:56","title":"Carbon Economics of Different Agricultural Practices for Farming Soil","abstract":"The loss of soil organic carbon (SOC) poses a severe danger to agricultural sustainability around the World. This review examines various farming practices and their impact on soil organic carbon storage. After a careful review of the literature, most of the research indicated that different farming practices, such as organic farming, cover crops, conservation tillage, and agroforestry, play vital roles in increasing the SOC content of the soil sustainably. Root exudation from cover crops increases microbial activity and helps break down complex organic compounds into organic carbon. Conservation tillage enhances the soil structure and maintains carbon storage without disturbing the soil. Agroforestry systems boost organic carbon input and fasten nutrient cycling because the trees and crops have symbiotic relationships. Intercropping and crop rotations have a role in changing the composition of plant residues and promoting carbon storage. There were many understanding on the complex interactions between soil organic carbon dynamics and agricultural practices. Based on the study, the paper reveals, the role of different agricultural practices like Carbon storage through cover crops, crop rotation, mulching Conservation tillage, conventional tillage, zero tillage and organic amendments in organic carbon storage in the soil for maximum crop yield to improve the economic condition of the cultivators.","sentences":["The loss of soil organic carbon (SOC) poses a severe danger to agricultural sustainability around the World.","This review examines various farming practices and their impact on soil organic carbon storage.","After a careful review of the literature, most of the research indicated that different farming practices, such as organic farming, cover crops, conservation tillage, and agroforestry, play vital roles in increasing the SOC content of the soil sustainably.","Root exudation from cover crops increases microbial activity and helps break down complex organic compounds into organic carbon.","Conservation tillage enhances the soil structure and maintains carbon storage without disturbing the soil.","Agroforestry systems boost organic carbon input and fasten nutrient cycling because the trees and crops have symbiotic relationships.","Intercropping and crop rotations have a role in changing the composition of plant residues and promoting carbon storage.","There were many understanding on the complex interactions between soil organic carbon dynamics and agricultural practices.","Based on the study, the paper reveals, the role of different agricultural practices like Carbon storage through cover crops, crop rotation, mulching Conservation tillage, conventional tillage, zero tillage and organic amendments in organic carbon storage in the soil for maximum crop yield to improve the economic condition of the cultivators."],"url":"http://arxiv.org/abs/2403.07530v1","category":"econ.GN"}
{"created":"2024-03-12 11:05:07","title":"Evaluation and thermodynamic optimization of phase diagram of lithium niobate tantalate solid solutions","abstract":"The phase diagram of the lithium niobate and lithium tantalate solid solutions was investigated using experimental data from differential thermal analysis (DTA) and crystal growth. We used XRF analysis to determine the elemental composition of crystals. Based on the Neumann-Kopp rule, essential data of end members lithium niobate (LN) and lithium tantalate (LT) was created. The heats of fusion of end members given by DTA measurements of LN (103 kJ/mol at 1531 K) and LT (289 kJ/mol at 1913 K) were given as input parameters to generate the data. This data served as the basis for calculating a phase diagram for LN and LT solid solutions. Finally, based on the experimental data and thermodynamic solution model, the phase diagram was optimized in the Calphad Factsage module. We also generated thermodynamic parameters for Gibb's excess energy of the solid solution. A plot of segregation coefficient as a function of Ta concentration was derived from the phase diagram.","sentences":["The phase diagram of the lithium niobate and lithium tantalate solid solutions was investigated using experimental data from differential thermal analysis (DTA) and crystal growth.","We used XRF analysis to determine the elemental composition of crystals.","Based on the Neumann-Kopp rule, essential data of end members lithium niobate (LN) and lithium tantalate (LT) was created.","The heats of fusion of end members given by DTA measurements of LN (103 kJ/mol at 1531 K) and LT (289 kJ/mol at 1913 K) were given as input parameters to generate the data.","This data served as the basis for calculating a phase diagram for LN and LT solid solutions.","Finally, based on the experimental data and thermodynamic solution model, the phase diagram was optimized in the Calphad Factsage module.","We also generated thermodynamic parameters for Gibb's excess energy of the solid solution.","A plot of segregation coefficient as a function of Ta concentration was derived from the phase diagram."],"url":"http://arxiv.org/abs/2403.07527v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 11:05:05","title":"Physics-Transfer Learning for Material Strength Screening","abstract":"The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the Peierls stress from chemically accurate density functional theory-based calculations of material parameters. Notably, the strengths of single-crystalline metals can be predicted from a few single-point calculations for the deformed lattice and on the {\\gamma} surface, allowing efficient, high-throughput screening for material discovery. Uncertainty quantification is carried out to assess the accuracy of models and sources of errors, showing reduced physical and system uncertainties in the predictions by elevating the fidelity of training models. This physics-transfer framework can be generalized to other problems facing the accuracy-performance dilemma, by harnessing the hierarchy of physics in the multiscale models of materials science.","sentences":["The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance.","Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow.","The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips.","Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases.","In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the Peierls stress from chemically accurate density functional theory-based calculations of material parameters.","Notably, the strengths of single-crystalline metals can be predicted from a few single-point calculations for the deformed lattice and on the {\\gamma} surface, allowing efficient, high-throughput screening for material discovery.","Uncertainty quantification is carried out to assess the accuracy of models and sources of errors, showing reduced physical and system uncertainties in the predictions by elevating the fidelity of training models.","This physics-transfer framework can be generalized to other problems facing the accuracy-performance dilemma, by harnessing the hierarchy of physics in the multiscale models of materials science."],"url":"http://arxiv.org/abs/2403.07526v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 11:03:25","title":"Universal Chemical Formula Dependence of $Ab$ $Initio$ Low-Energy Effective Hamiltonian in Single-Layer Carrier Doped Cuprate Superconductors -- Study by Hierarchical Dependence Extraction Algorithm","abstract":"We explore the possibility to control the superconducting (SC) transition temperature at optimal hole doping $T_{c}^{\\rm opt}$ in cuprates by tuning the chemical formula (CF). $T_{c}^{\\rm opt}$ can be theoretically predicted from the parameters of the \\textit{ab initio} low-energy effective Hamiltonian (LEH) with one antibonding (AB) Cu$3d_{x^2-y^2}$/O$2p_{\\sigma}$ orbital per Cu atom in the CuO$_2$ plane, notably the nearest neighbor hopping amplitude $|t_1|$ and the ratio $u=U/|t_1|$, where $U$ is the onsite effective Coulomb repulsion. However, the CF dependence of $|t_1|$ and $u$ is a highly nontrivial question. In this paper, we propose the universal dependence of $|t_1|$ and $u$ on the CF and structural features in hole doped cuprates with a single CuO$_2$ layer sandwiched between block layers. To do so, we perform extensive \\textit{ab initio} calculations of $|t_1|$ and $u$ and analyze the results by employing a machine learning method called Hierarchical Dependence Extraction (HDE). The main results are the following: (a) $|t_1|$ has a main-order dependence on the radii $R_{\\rm X}$ and $R_{\\rm A}$ of the apical anion X and cation A in the block layer. ($|t_1|$ increases when $R_{\\rm X}$ or $R_{\\rm A}$ decreases.) (b) $u$ has a main-order dependence on the negative ionic charge $Z_{\\rm X}$ of X and the hole doping $\\delta$ of the AB orbital. ($u$ decreases when $|Z_{\\rm X}|$ increases or $\\delta$ increases.) We elucidate and discuss the microscopic mechanism of (a,b). We demonstrate the predictive power of the HDE by showing the consistency between (a,b) and results from previous works. The present results provide a basis for optimizing SC properties in cuprates and possibly akin materials. Also, the HDE method offers a general platform to identify dependencies between physical quantities.","sentences":["We explore the possibility to control the superconducting (SC) transition temperature at optimal hole doping $T_{c}^{\\rm opt}$ in cuprates by tuning the chemical formula (CF).","$T_{c}^{\\rm opt}$ can be theoretically predicted from the parameters of the \\textit{ab initio} low-energy effective Hamiltonian (LEH) with one antibonding (AB) Cu$3d_{x^2-y^2}$/O$2p_{\\sigma}$ orbital per Cu atom in the CuO$_2$ plane, notably the nearest neighbor hopping amplitude $|t_1|$ and the ratio $u=U/|t_1|$, where $U$ is the onsite effective Coulomb repulsion.","However, the CF dependence of $|t_1|$ and $u$ is a highly nontrivial question.","In this paper, we propose the universal dependence of $|t_1|$ and $u$ on the CF and structural features in hole doped cuprates with a single CuO$_2$ layer sandwiched between block layers.","To do so, we perform extensive \\textit{ab initio} calculations of $|t_1|$ and $u$ and analyze the results by employing a machine learning method called Hierarchical Dependence Extraction (HDE).","The main results are the following: (a) $|t_1|$ has a main-order dependence on the radii $R_{\\rm X}$ and $R_{\\rm A}$ of the apical anion X and cation A in the block layer.","($|t_1|$ increases when $R_{\\rm X}$ or $R_{\\rm A}$ decreases.)","(b) $u$ has a main-order dependence on the negative ionic charge $Z_{\\rm X}$ of X and the hole doping $\\delta$ of the AB orbital.","($u$ decreases when $|Z_{\\rm X}|$ increases or $\\delta$ increases.)","We elucidate and discuss the microscopic mechanism of (a,b).","We demonstrate the predictive power of the HDE by showing the consistency between (a,b) and results from previous works.","The present results provide a basis for optimizing SC properties in cuprates and possibly akin materials.","Also, the HDE method offers a general platform to identify dependencies between physical quantities."],"url":"http://arxiv.org/abs/2403.07525v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 10:57:28","title":"Holographic spin alignment for vector mesons","abstract":"We develop a general framework for studying the spin alignment $\\rho_{00}$ for flavorless vector mesons by using the gauge/gravity duality. Focusing on the dilepton production through vector meson decay, we derive the relation between production rates at each spin channel and meson's spectral function, which can be evaluated by holographic models for a strongly coupled system. As examples, we study $\\rho_{00}$ for $J/\\psi$ and $\\phi$ mesons, induced by the relative motion to a thermal background, within the soft-wall model. We show that $\\rho_{00}$ in the helicity frame for $J/\\psi$ and $\\phi$ mesons have positive and negative deviations from 1/3 at $T=150$ MeV, respectively, which consequently leads to different properties for their global spin alignments.","sentences":["We develop a general framework for studying the spin alignment $\\rho_{00}$ for flavorless vector mesons by using the gauge/gravity duality.","Focusing on the dilepton production through vector meson decay, we derive the relation between production rates at each spin channel and meson's spectral function, which can be evaluated by holographic models for a strongly coupled system.","As examples, we study $\\rho_{00}$ for $J/\\psi$ and $\\phi$ mesons, induced by the relative motion to a thermal background, within the soft-wall model.","We show that $\\rho_{00}$ in the helicity frame for $J/\\psi$ and $\\phi$ mesons have positive and negative deviations from 1/3 at $T=150$ MeV, respectively, which consequently leads to different properties for their global spin alignments."],"url":"http://arxiv.org/abs/2403.07522v1","category":"hep-ph"}
{"created":"2024-03-12 10:54:38","title":"Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss","abstract":"Scene text recognition is an important and challenging task in computer vision. However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications.   In this paper, we propose a novel open-vocabulary text recognition framework, Pseudo-OCR, to recognize OOV words. The key challenge in this task is the lack of OOV training data. To solve this problem, we first propose a pseudo label generation module that leverages character detection and image inpainting to produce substantial pseudo OOV training data from real-world images. Unlike previous synthetic data, our pseudo OOV data contains real characters and backgrounds to simulate real-world applications. Secondly, to reduce noises in pseudo data, we present a semantic checking mechanism to filter semantically meaningful data. Thirdly, we introduce a quality-aware margin loss to boost the training with pseudo data. Our loss includes a margin-based part to enhance the classification ability, and a quality-aware part to penalize low-quality samples in both real and pseudo data.   Extensive experiments demonstrate that our approach outperforms the state-of-the-art on eight datasets and achieves the first rank in the ICDAR2022 challenge.","sentences":["Scene text recognition is an important and challenging task in computer vision.","However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications.   ","In this paper, we propose a novel open-vocabulary text recognition framework, Pseudo-OCR, to recognize OOV words.","The key challenge in this task is the lack of OOV training data.","To solve this problem, we first propose a pseudo label generation module that leverages character detection and image inpainting to produce substantial pseudo OOV training data from real-world images.","Unlike previous synthetic data, our pseudo OOV data contains real characters and backgrounds to simulate real-world applications.","Secondly, to reduce noises in pseudo data, we present a semantic checking mechanism to filter semantically meaningful data.","Thirdly, we introduce a quality-aware margin loss to boost the training with pseudo data.","Our loss includes a margin-based part to enhance the classification ability, and a quality-aware part to penalize low-quality samples in both real and pseudo data.   ","Extensive experiments demonstrate that our approach outperforms the state-of-the-art on eight datasets and achieves the first rank in the ICDAR2022 challenge."],"url":"http://arxiv.org/abs/2403.07518v1","category":"cs.CV"}
{"created":"2024-03-12 10:47:53","title":"D4D: An RGBD diffusion model to boost monocular depth estimation","abstract":"Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.","sentences":["Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce.","A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step.","In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples.","We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements.","Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset."],"url":"http://arxiv.org/abs/2403.07516v1","category":"cs.CV"}
{"created":"2024-03-12 10:47:45","title":"Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation","abstract":"In the context of single domain generalisation, the objective is for models that have been exclusively trained on data from a single domain to demonstrate strong performance when confronted with various unfamiliar domains. In this paper, we introduce a novel model referred to as Contrastive Uncertainty Domain Generalisation Network (CUDGNet). The key idea is to augment the source capacity in both input and label spaces through the fictitious domain generator and jointly learn the domain invariant representation of each class through contrastive learning. Extensive experiments on two Single Source Domain Generalisation (SSDG) datasets demonstrate the effectiveness of our approach, which surpasses the state-of-the-art single-DG methods by up to $7.08\\%$. Our method also provides efficient uncertainty estimation at inference time from a single forward pass through the generator subnetwork.","sentences":["In the context of single domain generalisation, the objective is for models that have been exclusively trained on data from a single domain to demonstrate strong performance when confronted with various unfamiliar domains.","In this paper, we introduce a novel model referred to as Contrastive Uncertainty Domain Generalisation Network (CUDGNet).","The key idea is to augment the source capacity in both input and label spaces through the fictitious domain generator and jointly learn the domain invariant representation of each class through contrastive learning.","Extensive experiments on two Single Source Domain Generalisation (SSDG) datasets demonstrate the effectiveness of our approach, which surpasses the state-of-the-art single-DG methods by up to $7.08\\%$. Our method also provides efficient uncertainty estimation at inference time from a single forward pass through the generator subnetwork."],"url":"http://arxiv.org/abs/2403.07514v1","category":"cs.CV"}
{"created":"2024-03-12 10:45:45","title":"Relevance Score: A Landmark-Like Heuristic for Planning","abstract":"Landmarks are facts or actions that appear in all valid solutions of a planning problem. They have been used successfully to calculate heuristics that guide the search for a plan. We investigate an extension to this concept by defining a novel \"relevance score\" that helps identify facts or actions that appear in most but not all plans to achieve any given goal. We describe an approach to compute this relevance score and use it as a heuristic in the search for a plan. We experimentally compare the performance of our approach with that of a state of the art landmark-based heuristic planning approach using benchmark planning problems. While the original landmark-based heuristic leads to better performance on problems with well-defined landmarks, our approach substantially improves performance on problems that lack non-trivial landmarks.","sentences":["Landmarks are facts or actions that appear in all valid solutions of a planning problem.","They have been used successfully to calculate heuristics that guide the search for a plan.","We investigate an extension to this concept by defining a novel \"relevance score\" that helps identify facts or actions that appear in most but not all plans to achieve any given goal.","We describe an approach to compute this relevance score and use it as a heuristic in the search for a plan.","We experimentally compare the performance of our approach with that of a state of the art landmark-based heuristic planning approach using benchmark planning problems.","While the original landmark-based heuristic leads to better performance on problems with well-defined landmarks, our approach substantially improves performance on problems that lack non-trivial landmarks."],"url":"http://arxiv.org/abs/2403.07510v1","category":"cs.AI"}
{"created":"2024-03-12 10:45:44","title":"Conformal anomalies for (maximal) 6d conformal supergravity","abstract":"We compute the conformal anomalies for 6d (2,0) conformal supergravity by direct calculation in component fields. The main novel results consist of the type-B anomaly coefficients for the gravitino and the 3-form, as well as their explicit quadratic action on some specific backgrounds. We also comment on the graviton contribution, whose Lagrangian is essentially given by the $\\mathscr Q$-curvature. We confirm the expectation that, when coupling (2,0) conformal supergravity to 26 copies of the (2,0) tensor multiplet, the resulting theory is free of conformal anomalies. We also consider the conformal anomalies for its (1,0) truncation and confirm their relation with the chiral anomaly polynomial recently derived. For calculating the anomalies, we work with an Einstein on-shell background and make a factorised Ansatz for the operators governing the quadratic fluctuations. This reduces the calculation to evaluating heat-kernel coefficients of standard 2-derivative operators. We fix and check the Ansatz against the explicit evaluation of the component-field supergravity action in some cases.","sentences":["We compute the conformal anomalies for 6d (2,0) conformal supergravity by direct calculation in component fields.","The main novel results consist of the type-B anomaly coefficients for the gravitino and the 3-form, as well as their explicit quadratic action on some specific backgrounds.","We also comment on the graviton contribution, whose Lagrangian is essentially given by the $\\mathscr Q$-curvature.","We confirm the expectation that, when coupling (2,0) conformal supergravity to 26 copies of the (2,0) tensor multiplet, the resulting theory is free of conformal anomalies.","We also consider the conformal anomalies for its (1,0) truncation and confirm their relation with the chiral anomaly polynomial recently derived.","For calculating the anomalies, we work with an Einstein on-shell background and make a factorised Ansatz for the operators governing the quadratic fluctuations.","This reduces the calculation to evaluating heat-kernel coefficients of standard 2-derivative operators.","We fix and check the Ansatz against the explicit evaluation of the component-field supergravity action in some cases."],"url":"http://arxiv.org/abs/2403.07509v1","category":"hep-th"}
{"created":"2024-03-12 10:44:13","title":"MoAI: Mixture of All Intelligence for Large Language and Vision Models","abstract":"The rise of large language models (LLMs) and instruction tuning has led to the current trend of instruction-tuned large language and vision models (LLVMs). This trend involves either meticulously curating numerous instruction tuning datasets tailored to specific objectives or enlarging LLVMs to manage vast amounts of vision language (VL) data. However, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR). Instead, the existing LLVMs rely mainly on the large capacity and emergent capabilities of their LLM backbones. Therefore, we present a new LLVM, Mixture of All Intelligence (MoAI), which leverages auxiliary visual information obtained from the outputs of external segmentation, detection, SGG, and OCR models. MoAI operates through two newly introduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the outputs of the external CV models, the MoAI-Compressor aligns and condenses them to efficiently use relevant auxiliary visual information for VL tasks. MoAI-Mixer then blends three types of intelligence (1) visual features, (2) auxiliary features from the external CV models, and (3) language features by utilizing the concept of Mixture of Experts. Through this integration, MoAI significantly outperforms both open-source and closed-source LLVMs in numerous zero-shot VL tasks, particularly those related to real-world scene understanding such as object existence, positions, relations, and OCR without enlarging the model size or curating extra visual instruction tuning datasets.","sentences":["The rise of large language models (LLMs) and instruction tuning has led to the current trend of instruction-tuned large language and vision models (LLVMs).","This trend involves either meticulously curating numerous instruction tuning datasets tailored to specific objectives or enlarging LLVMs to manage vast amounts of vision language (VL) data.","However, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR).","Instead, the existing LLVMs rely mainly on the large capacity and emergent capabilities of their LLM backbones.","Therefore, we present a new LLVM, Mixture of All Intelligence (MoAI), which leverages auxiliary visual information obtained from the outputs of external segmentation, detection, SGG, and OCR models.","MoAI operates through two newly introduced modules: MoAI-Compressor and MoAI-Mixer.","After verbalizing the outputs of the external CV models, the MoAI-Compressor aligns and condenses them to efficiently use relevant auxiliary visual information for VL tasks.","MoAI-Mixer then blends three types of intelligence (1) visual features, (2) auxiliary features from the external CV models, and (3) language features by utilizing the concept of Mixture of Experts.","Through this integration, MoAI significantly outperforms both open-source and closed-source LLVMs in numerous zero-shot VL tasks, particularly those related to real-world scene understanding such as object existence, positions, relations, and OCR without enlarging the model size or curating extra visual instruction tuning datasets."],"url":"http://arxiv.org/abs/2403.07508v1","category":"cs.CV"}
{"created":"2024-03-12 10:42:07","title":"Short time asymptotics of the fundamental solutions for Schr\u00f6dinger equations with non-smooth potentials","abstract":"This paper deals with Schr\\\"{o}dinger equations with potentials which are time-dependent non-smooth and at most quadratic growth. In the case where potentials are smooth with respect to spatial variables, fundamental solutions have explicit formulas in short time by D. Fujiwara. On the otherhand in the case where ones are non-smooth, we cannot expect that fundamental solutions have similar formula as above because dispersive estimates fail to hold in general. We show that a principal part of an asymptotic form of the fundamental solution has similar form as above even in the case where a potential is in $C^2$ with respect to spatial variables.","sentences":["This paper deals with Schr\\\"{o}dinger equations with potentials which are time-dependent non-smooth and at most quadratic growth.","In the case where potentials are smooth with respect to spatial variables, fundamental solutions have explicit formulas in short time by D. Fujiwara.","On the otherhand in the case where ones are non-smooth, we cannot expect that fundamental solutions have similar formula as above because dispersive estimates fail to hold in general.","We show that a principal part of an asymptotic form of the fundamental solution has similar form as above even in the case where a potential is in $C^2$ with respect to spatial variables."],"url":"http://arxiv.org/abs/2403.07502v1","category":"math.AP"}
{"created":"2024-03-12 10:38:54","title":"Detecting Security-Relevant Methods using Multi-label Machine Learning","abstract":"To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.   In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels. The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine learning approach has a higher F1-Measure than related approaches. Moreover, the plugin reduces and simplifies the manual effort required when configuring and using static analysis tools.","sentences":["To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods.","Current approaches can automatically identify such methods using binary relevance machine learning approaches.","However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice.","Additionally, users have to nevertheless manually configure static analysis tools using the detected methods.","Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.   ","In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels.","The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA.","Our experiments reveal that Dev-Assist's machine learning approach has a higher F1-Measure than related approaches.","Moreover, the plugin reduces and simplifies the manual effort required when configuring and using static analysis tools."],"url":"http://arxiv.org/abs/2403.07501v1","category":"cs.LG"}
{"created":"2024-03-12 10:38:03","title":"Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation","abstract":"The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.","sentences":["The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles.","Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field.","Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation.","To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style.","Extensive experiments demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.07500v1","category":"cs.CV"}
{"created":"2024-03-12 10:25:29","title":"Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM","abstract":"Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. See project website https://steve-zeyu-zhang.github.io/MotionMamba/","sentences":["Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging.","Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it.","Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence.","To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs.","Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames.","We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame.","Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation.","See project website https://steve-zeyu-zhang.github.io/MotionMamba/"],"url":"http://arxiv.org/abs/2403.07487v1","category":"cs.CV"}
{"created":"2024-03-12 10:21:31","title":"XpertAI: uncovering model strategies for sub-manifolds","abstract":"In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitative and quantitative results, demonstrate the benefits of our approach.","sentences":["In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models.","While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models.","In regression, explanations need to be precisely formulated to address specific user queries (e.g.\\ distinguishing between `Why is the output above 0?'","and `Why is the output above 50?').","They should furthermore reflect the model's behavior on the relevant data sub-manifold.","In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies.","XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation.","Qualitative and quantitative results, demonstrate the benefits of our approach."],"url":"http://arxiv.org/abs/2403.07486v1","category":"cs.LG"}
{"created":"2024-03-12 10:20:33","title":"The Nikodym property and filters on $\u03c9$","abstract":"For a free filter $F$ on $\\omega$, let $N_F=\\omega\\cup\\{p_F\\}$, where $p_F\\not\\in\\omega$, be equipped with the following topology: every element of $\\omega$ is isolated whereas all open neighborhoods of $p_F$ are of the form $A\\cup\\{p_F\\}$ for $A\\in F$. The aim of this paper is to study spaces of the form $N_F$ in the context of the Nikodym property of Boolean algebras. By $\\mathcal{AN}$ we denote the class of all those ideals $\\mathcal{I}$ on $\\omega$ such that for the dual filter $\\mathcal{I}^*$ the space $N_{\\mathcal{I}^*}$ carries a sequence $\\langle\\mu_n\\colon n\\in\\omega\\rangle$ of finitely supported signed measures such that $\\|\\mu_n\\|\\rightarrow\\infty$ and $\\mu_n(A)\\rightarrow 0$ for every clopen subset $A\\subseteq N_{\\mathcal{I}^*}$. We prove that $\\mathcal{I}\\in\\mathcal{AN}$ if and only if there exists a density submeasure $\\varphi$ on $\\omega$ such that $\\varphi(\\omega)=\\infty$ and $\\mathcal{I}$ is contained in the exhaustive ideal $\\mbox{Exh}(\\varphi)$. Consequently, we get that if $\\mathcal{I}\\subseteq\\mbox{Exh}(\\varphi)$ for some density submeasure $\\varphi$ on $\\omega$ such that $\\varphi(\\omega)=\\infty$ and $N_{\\mathcal{I}^*}$ is homeomorphic to a subspace of the Stone space $St(\\mathcal{A})$ of a given Boolean algebra $\\mathcal{A}$, then $\\mathcal{A}$ does not have the Nikodym property.   We observe that each $\\mathcal{I}\\in\\mathcal{AN}$ is Kat\\v{e}tov below the asymptotic density zero ideal $\\mathcal{Z}$, and prove that the class $\\mathcal{AN}$ has a subset of size $\\mathfrak{d}$ which is dominating with respect to the Kat\\v{e}tov order $\\leq_K$, but $\\mathcal{AN}$ has no $\\leq_K$-maximal element. We show that for a density ideal $\\mathcal{I}$ it holds $\\mathcal{I}\\not\\in\\mathcal{AN}$ if and only if $\\mathcal{I}$ is totally bounded if and only if the Boolean algebra $\\mathcal{P}(\\omega)/\\mathcal{I}$ contains a countable splitting family.","sentences":["For a free filter $F$ on $\\omega$, let $N_F=\\omega\\cup\\{p_F\\}$, where $p_F\\not\\in\\omega$, be equipped with the following topology: every element of $\\omega$ is isolated whereas all open neighborhoods of $p_F$ are of the form $A\\cup\\{p_F\\}$ for $A\\in F$.","The aim of this paper is to study spaces of the form $N_F$ in the context of the Nikodym property of Boolean algebras.","By $\\mathcal{AN}$ we denote the class of all those ideals $\\mathcal{I}$ on $\\omega$ such that for the dual filter $\\mathcal{I}^*$ the space $N_{\\mathcal{I}^*}$ carries a sequence $\\langle\\mu_n\\colon n\\in\\omega\\rangle$ of finitely supported signed measures such that $\\|\\mu_n\\|\\rightarrow\\infty$ and $\\mu_n(A)\\rightarrow 0$ for every clopen subset $A\\subseteq N_{\\mathcal{I}^*}$.","We prove that $\\mathcal{I}\\in\\mathcal{AN}$ if and only if there exists a density submeasure $\\varphi$ on $\\omega$ such that $\\varphi(\\omega)=\\infty$ and $\\mathcal{I}$ is contained in the exhaustive ideal $\\mbox{Exh}(\\varphi)$. Consequently, we get that if $\\mathcal{I}\\subseteq\\mbox{Exh}(\\varphi)$ for some density submeasure $\\varphi$ on $\\omega$ such that $\\varphi(\\omega)=\\infty$ and $N_{\\mathcal{I}^*}$ is homeomorphic to a subspace of the Stone space $St(\\mathcal{A})$ of a given Boolean algebra $\\mathcal{A}$, then $\\mathcal{A}$ does not have the Nikodym property.   ","We observe that each $\\mathcal{I}\\in\\mathcal{AN}$ is Kat\\v{e}tov below the asymptotic density zero ideal $\\mathcal{Z}$, and prove that the class $\\mathcal{AN}$ has a subset of size $\\mathfrak{d}$ which is dominating with respect to the Kat\\v{e}tov order $\\leq_K$, but $\\mathcal{AN}$ has no $\\leq_K$-maximal element.","We show that for a density ideal $\\mathcal{I}$ it holds $\\mathcal{I}\\not\\in\\mathcal{AN}$ if and only if $\\mathcal{I}$ is totally bounded if and only if the Boolean algebra $\\mathcal{P}(\\omega)/\\mathcal{I}$ contains a countable splitting family."],"url":"http://arxiv.org/abs/2403.07484v1","category":"math.LO"}
{"created":"2024-03-12 10:18:59","title":"A Deep Learning Approach to Diabetes Diagnosis","abstract":"Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores the potential of deep learning models for robust diabetes diagnosis. See project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/","sentences":["Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body.","Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints.","Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance.","Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing.","Our method addresses existing challenges such as limited performance associated with traditional machine learning.","Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods.","Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset.","This underscores the potential of deep learning models for robust diabetes diagnosis.","See project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/"],"url":"http://arxiv.org/abs/2403.07483v1","category":"cs.LG"}
{"created":"2024-03-12 10:18:24","title":"Linking Invariants for Valuations and Orderings on Fields","abstract":"The mod-2 arithmetic Milnor invariants, introduced by Morishita, provide a decomposition law for primes in canonical Galois extensions with unitriangular Galois groups, and contain the Legendre and Redei symbols as special cases. Morishita further proposed a notion of mod-q arithmetic Milnor invariants, where q is a prime power, for number fields containing the q-th roots of unity and satisfying certain class field theory assumptions. We extend this theory from the number field context to general fields, by introducing a notion of a linking invariant for discrete valuations and orderings. We further express it as a Magnus homomorphism coefficient, and relate it to Massey product elements in Galois cohomology.","sentences":["The mod-2 arithmetic Milnor invariants, introduced by Morishita, provide a decomposition law for primes in canonical Galois extensions with unitriangular Galois groups, and contain the Legendre and Redei symbols as special cases.","Morishita further proposed a notion of mod-q arithmetic Milnor invariants, where q is a prime power, for number fields containing the q-th roots of unity and satisfying certain class field theory assumptions.","We extend this theory from the number field context to general fields, by introducing a notion of a linking invariant for discrete valuations and orderings.","We further express it as a Magnus homomorphism coefficient, and relate it to Massey product elements in Galois cohomology."],"url":"http://arxiv.org/abs/2403.07482v1","category":"math.NT"}
{"created":"2024-03-12 10:14:07","title":"On non-tameness of the Ellis semigroup","abstract":"The Ellis semigroup of a dynamical system $(X,T)$ is tame if every element is the limit of a sequence (as opposed to a net) of homeomorphisms coming from the $T$ action. This topological property is related to the cardinality of the semigroup. Non-tame Ellis semigroups have a cardinality which is that of the power set of the continuum $2^{\\mathfrak c}$.The semigroup admits a minimal bilateral ideal and this ideal is a union of isomorphic copies of a group $\\mathcal H$, the so-called structure group of $(X,T)$. For almost automorphic systems the cardinality of $\\mathcal H$ is at most $\\mathfrak c$, that of the continuum. We show a partial converse for minimal $(X,T)$ with abelian $T$, namely that the cardinality of the structure group is $2^{\\mathfrak c}$ if the proximal relation is not transitive and the subgroup generated by differences of singular points in the maximal equicontinuous factor is not open.This refines the above statement about non-tame Ellis semigroups, as it locates a particular algebraic component of the latter which has such a large cardinality.","sentences":["The Ellis semigroup of a dynamical system $(X,T)$ is tame if every element is the limit of a sequence (as opposed to a net) of homeomorphisms coming from the $T$ action.","This topological property is related to the cardinality of the semigroup.","Non-tame Ellis semigroups have a cardinality which is that of the power set of the continuum $2^{\\mathfrak c}$.The semigroup admits a minimal bilateral ideal and this ideal is a union of isomorphic copies of a group $\\mathcal H$, the so-called structure group of $(X,T)$. For almost automorphic systems the cardinality of $\\mathcal H$ is at most $\\mathfrak c$, that of the continuum.","We show a partial converse for minimal $(X,T)$ with abelian $T$, namely that the cardinality of the structure group is $2^{\\mathfrak c}$ if the proximal relation is not transitive and the subgroup generated by differences of singular points in the maximal equicontinuous factor is not open.","This refines the above statement about non-tame Ellis semigroups, as it locates a particular algebraic component of the latter which has such a large cardinality."],"url":"http://arxiv.org/abs/2403.07480v1","category":"math.DS"}
{"created":"2024-03-12 10:12:59","title":"Towards Graph Foundation Models for Personalization","abstract":"In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform.","sentences":["In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions.","In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs).","While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval.","In this paper, we present a graph-based foundation modeling approach tailored to personalization.","Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types.","To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity.","To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type.","This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data.","Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform."],"url":"http://arxiv.org/abs/2403.07478v1","category":"cs.IR"}
{"created":"2024-03-12 10:10:55","title":"Predicting the Risk of Ischemic Stroke in Patients with Atrial Fibrillation using Heterogeneous Drug-protein-disease Network-based Deep Learning","abstract":"We develop a deep learning model, ABioSPATH, to predict the one-year risk of ischemic stroke (IS) in atrial fibrillation (AF) patients. The model integrates drug-protein-disease pathways and real-world clinical data of AF patients to generate the IS risk and potential pathways for each patient. The model uses a multilayer network to identify the mechanism of drug action and disease comorbidity propagation pathways. The model is tested on the Electronic Health Record (EHR) data of 7859 AF patients from 43 hospitals in Hong Kong. The model outperforms all baselines across all metrics and provides valuable molecular-level insights for clinical use. The model also highlights key proteins in common pathways and potential IS risks tied to less-studied drugs. The model only requires routinely collected data, without requiring expensive biomarkers to be tested.","sentences":["We develop a deep learning model, ABioSPATH, to predict the one-year risk of ischemic stroke (IS) in atrial fibrillation (AF) patients.","The model integrates drug-protein-disease pathways and real-world clinical data of AF patients to generate the IS risk and potential pathways for each patient.","The model uses a multilayer network to identify the mechanism of drug action and disease comorbidity propagation pathways.","The model is tested on the Electronic Health Record (EHR) data of 7859","AF patients from 43 hospitals in Hong Kong.","The model outperforms all baselines across all metrics and provides valuable molecular-level insights for clinical use.","The model also highlights key proteins in common pathways and potential IS risks tied to less-studied drugs.","The model only requires routinely collected data, without requiring expensive biomarkers to be tested."],"url":"http://arxiv.org/abs/2403.07475v1","category":"q-bio.QM"}
{"created":"2024-03-12 10:09:56","title":"Optical chaos synchronisation in a cascaded injection experiment","abstract":"We experimentally study the synchronization of chaos generated by semiconductor lasers in a cascade injection configuration, i.e., a tunable master laser is used to generate chaos by optical injection in a transmitter laser that injects light into a receiver laser. Chaos synchronization between the transmitter and the receiver lasers is achieved with a correlation coefficient of 90 % for a measurement bandwidth up to 35 GHz. Two parameter regions of good synchronization are found, corresponding to the alignment of the oscillation frequencies of the receiver laser with either the transmitter laser or the master laser.","sentences":["We experimentally study the synchronization of chaos generated by semiconductor lasers in a cascade injection configuration, i.e., a tunable master laser is used to generate chaos by optical injection in a transmitter laser that injects light into a receiver laser.","Chaos synchronization between the transmitter and the receiver lasers is achieved with a correlation coefficient of 90 % for a measurement bandwidth up to 35 GHz.","Two parameter regions of good synchronization are found, corresponding to the alignment of the oscillation frequencies of the receiver laser with either the transmitter laser or the master laser."],"url":"http://arxiv.org/abs/2403.07474v1","category":"physics.optics"}
{"created":"2024-03-12 10:06:48","title":"On the nonconvexity of some push-forward constraints and its consequences in machine learning","abstract":"The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In a second time, we show how this result implies critical limitations on the design of convex optimization problems for learning generative models or group-fair predictors. This work will hopefully help researchers and practitioners have a better understanding of the critical impact of push-forward conditions onto convexity.","sentences":["The push-forward operation enables one to redistribute a probability measure through a deterministic map.","It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model.","However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems.","This paper aims at filling this gap.","In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures.","This highlights that for most probability measures, these push-forward constraints are not convex.","In a second time, we show how this result implies critical limitations on the design of convex optimization problems for learning generative models or group-fair predictors.","This work will hopefully help researchers and practitioners have a better understanding of the critical impact of push-forward conditions onto convexity."],"url":"http://arxiv.org/abs/2403.07471v1","category":"stat.ML"}
{"created":"2024-03-12 10:06:17","title":"DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models","abstract":"Motion planners are essential for the safe operation of automated vehicles across various scenarios. However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using large language models. Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. Leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. Furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. Our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively.","sentences":["Motion planners are essential for the safe operation of automated vehicles across various scenarios.","However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive.","To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using large language models.","Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages.","Leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions.","Furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes.","Our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively."],"url":"http://arxiv.org/abs/2403.07470v1","category":"cs.RO"}
{"created":"2024-03-12 10:04:08","title":"A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing Objects in 3D Scenes","abstract":"Three-Dimensional (3D) dense captioning is an emerging vision-language bridging task that aims to generate multiple detailed and accurate descriptions for 3D scenes. It presents significant potential and challenges due to its closer representation of the real world compared to 2D visual captioning, as well as complexities in data collection and processing of 3D point cloud sources. Despite the popularity and success of existing methods, there is a lack of comprehensive surveys summarizing the advancements in this field, which hinders its progress. In this paper, we provide a comprehensive review of 3D dense captioning, covering task definition, architecture classification, dataset analysis, evaluation metrics, and in-depth prosperity discussions. Based on a synthesis of previous literature, we refine a standard pipeline that serves as a common paradigm for existing methods. We also introduce a clear taxonomy of existing models, summarize technologies involved in different modules, and conduct detailed experiment analysis. Instead of a chronological order introduction, we categorize the methods into different classes to facilitate exploration and analysis of the differences and connections among existing techniques. We also provide a reading guideline to assist readers with different backgrounds and purposes in reading efficiently. Furthermore, we propose a series of promising future directions for 3D dense captioning by identifying challenges and aligning them with the development of related tasks, offering valuable insights and inspiring future research in this field. Our aim is to provide a comprehensive understanding of 3D dense captioning, foster further investigations, and contribute to the development of novel applications in multimedia and related domains.","sentences":["Three-Dimensional (3D) dense captioning is an emerging vision-language bridging task that aims to generate multiple detailed and accurate descriptions for 3D scenes.","It presents significant potential and challenges due to its closer representation of the real world compared to 2D visual captioning, as well as complexities in data collection and processing of 3D point cloud sources.","Despite the popularity and success of existing methods, there is a lack of comprehensive surveys summarizing the advancements in this field, which hinders its progress.","In this paper, we provide a comprehensive review of 3D dense captioning, covering task definition, architecture classification, dataset analysis, evaluation metrics, and in-depth prosperity discussions.","Based on a synthesis of previous literature, we refine a standard pipeline that serves as a common paradigm for existing methods.","We also introduce a clear taxonomy of existing models, summarize technologies involved in different modules, and conduct detailed experiment analysis.","Instead of a chronological order introduction, we categorize the methods into different classes to facilitate exploration and analysis of the differences and connections among existing techniques.","We also provide a reading guideline to assist readers with different backgrounds and purposes in reading efficiently.","Furthermore, we propose a series of promising future directions for 3D dense captioning by identifying challenges and aligning them with the development of related tasks, offering valuable insights and inspiring future research in this field.","Our aim is to provide a comprehensive understanding of 3D dense captioning, foster further investigations, and contribute to the development of novel applications in multimedia and related domains."],"url":"http://arxiv.org/abs/2403.07469v1","category":"cs.CV"}
{"created":"2024-03-12 09:59:34","title":"Backdoor Attack with Mode Mixture Latent Modification","abstract":"Backdoor attacks become a significant security concern for deep neural networks in recent years. An image classification model can be compromised if malicious backdoors are injected into it. This corruption will cause the model to function normally on clean images but predict a specific target label when triggers are present. Previous research can be categorized into two genres: poisoning a portion of the dataset with triggered images for users to train the model from scratch, or training a backdoored model alongside a triggered image generator. Both approaches require significant amount of attackable parameters for optimization to establish a connection between the trigger and the target label, which may raise suspicions as more people become aware of the existence of backdoor attacks. In this paper, we propose a backdoor attack paradigm that only requires minimal alterations (specifically, the output layer) to a clean model in order to inject the backdoor under the guise of fine-tuning. To achieve this, we leverage mode mixture samples, which are located between different modes in latent space, and introduce a novel method for conducting backdoor attacks. We evaluate the effectiveness of our method on four popular benchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet.","sentences":["Backdoor attacks become a significant security concern for deep neural networks in recent years.","An image classification model can be compromised if malicious backdoors are injected into it.","This corruption will cause the model to function normally on clean images but predict a specific target label when triggers are present.","Previous research can be categorized into two genres: poisoning a portion of the dataset with triggered images for users to train the model from scratch, or training a backdoored model alongside a triggered image generator.","Both approaches require significant amount of attackable parameters for optimization to establish a connection between the trigger and the target label, which may raise suspicions as more people become aware of the existence of backdoor attacks.","In this paper, we propose a backdoor attack paradigm that only requires minimal alterations (specifically, the output layer) to a clean model in order to inject the backdoor under the guise of fine-tuning.","To achieve this, we leverage mode mixture samples, which are located between different modes in latent space, and introduce a novel method for conducting backdoor attacks.","We evaluate the effectiveness of our method on four popular benchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet."],"url":"http://arxiv.org/abs/2403.07463v1","category":"cs.CR"}
{"created":"2024-03-12 09:58:37","title":"Compressed-sensing Lindbladian quantum tomography with trapped ions","abstract":"Characterizing the dynamics of quantum systems is a central task for the development of quantum information processors (QIPs). It serves to benchmark different devices, learn about their specific noise, and plan the next hardware upgrades. However, this task is also very challenging, for it requires a large number of measurements and time-consuming classical processing. Moreover, when interested in the time dependence of the noise, there is an additional overhead since the characterization must be performed repeatedly within the time interval of interest. To overcome this limitation while, at the same time, ordering the learned sources of noise by their relevance, we focus on the inference of the dynamical generators of the noisy dynamics using Lindbladian quantum tomography (LQT). We propose two different improvements of LQT that alleviate previous shortcomings. In the weak-noise regime of current QIPs, we manage to linearize the maximum likelihood estimation of LQT, turning the constrained optimization into a convex problem to reduce the classical computation cost and to improve its robustness. Moreover, by introducing compressed sensing techniques, we reduce the number of required measurements without sacrificing accuracy. To illustrate these improvements, we apply our LQT tools to trapped-ion experiments of single- and two-qubit gates, advancing in this way the previous state of the art.","sentences":["Characterizing the dynamics of quantum systems is a central task for the development of quantum information processors (QIPs).","It serves to benchmark different devices, learn about their specific noise, and plan the next hardware upgrades.","However, this task is also very challenging, for it requires a large number of measurements and time-consuming classical processing.","Moreover, when interested in the time dependence of the noise, there is an additional overhead since the characterization must be performed repeatedly within the time interval of interest.","To overcome this limitation while, at the same time, ordering the learned sources of noise by their relevance, we focus on the inference of the dynamical generators of the noisy dynamics using Lindbladian quantum tomography (LQT).","We propose two different improvements of LQT that alleviate previous shortcomings.","In the weak-noise regime of current QIPs, we manage to linearize the maximum likelihood estimation of LQT, turning the constrained optimization into a convex problem to reduce the classical computation cost and to improve its robustness.","Moreover, by introducing compressed sensing techniques, we reduce the number of required measurements without sacrificing accuracy.","To illustrate these improvements, we apply our LQT tools to trapped-ion experiments of single- and two-qubit gates, advancing in this way the previous state of the art."],"url":"http://arxiv.org/abs/2403.07462v1","category":"quant-ph"}
{"created":"2024-03-12 09:51:05","title":"A tutorial on multi-view autoencoders using the multi-view-AE library","abstract":"There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \\texttt{multi-view-AE} library. This library offers Python implementations of numerous multi-view autoencoder models, presented within a user-friendly framework. Through benchmarking experiments, we evaluate our implementations against previous ones, demonstrating comparable or superior performance. This work aims to establish a cohesive foundation for multi-modal modelling, serving as a valuable educational resource in the field.","sentences":["There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data.","Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand.","However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks.","To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations.","Moreover, we offer insights into the motivation and theoretical advantages of each model.","To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \\texttt{multi-view-AE} library.","This library offers Python implementations of numerous multi-view autoencoder models, presented within a user-friendly framework.","Through benchmarking experiments, we evaluate our implementations against previous ones, demonstrating comparable or superior performance.","This work aims to establish a cohesive foundation for multi-modal modelling, serving as a valuable educational resource in the field."],"url":"http://arxiv.org/abs/2403.07456v1","category":"cs.LG"}
{"created":"2024-03-12 09:48:17","title":"Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings","abstract":"Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as \"simulation-based inference\" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature.","sentences":["Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators.","These approaches are collectively known as \"simulation-based inference\" (SBI).","Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution.","However, they do not generally achieve an optimal trade-off between accuracy and computational demand.","In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions.","Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint.","We illustrate our results on several benchmark models from the SBI literature."],"url":"http://arxiv.org/abs/2403.07454v1","category":"stat.ML"}
{"created":"2024-03-12 09:34:42","title":"Compact structures for single-beam magneto-optical trapping of ytterbium","abstract":"Today's best optical lattice clocks are based on the spectroscopy of trapped alkaline-earth-like atoms such as ytterbium and strontium atoms. The development towards mobile or even space-borne clocks necessitates concepts for the compact laser-cooling and trapping of these atoms with reduced laser requirements. Here we present two compact and robust achromatic mirror structures for single-beam magneto-optical trapping of alkaline-earth-like atoms using two widely separated optical cooling frequencies. We have compared the trapping and cooling performance of a monolithic aluminium structure that generates a conventional trap geometry to a quasi-planar platform based on a periodic mirror structure for different isotopes of Yb. Compared to prior work with strontium in non-conventional traps, where only bosons were trapped on a narrow line transition, we demonstrate two-stage cooling and trapping of a fermionic alkaline-earth-like isotope in a single-beam quasi-planar structure.","sentences":["Today's best optical lattice clocks are based on the spectroscopy of trapped alkaline-earth-like atoms such as ytterbium and strontium atoms.","The development towards mobile or even space-borne clocks necessitates concepts for the compact laser-cooling and trapping of these atoms with reduced laser requirements.","Here we present two compact and robust achromatic mirror structures for single-beam magneto-optical trapping of alkaline-earth-like atoms using two widely separated optical cooling frequencies.","We have compared the trapping and cooling performance of a monolithic aluminium structure that generates a conventional trap geometry to a quasi-planar platform based on a periodic mirror structure for different isotopes of Yb.","Compared to prior work with strontium in non-conventional traps, where only bosons were trapped on a narrow line transition, we demonstrate two-stage cooling and trapping of a fermionic alkaline-earth-like isotope in a single-beam quasi-planar structure."],"url":"http://arxiv.org/abs/2403.07446v1","category":"physics.atom-ph"}
{"created":"2024-03-12 09:33:18","title":"A Survey on Federated Learning in Intelligent Transportation Systems","abstract":"The development of Intelligent Transportation System (ITS) has brought about comprehensive urban traffic information that not only provides convenience to urban residents in their daily lives but also enhances the efficiency of urban road usage, leading to a more harmonious and sustainable urban life. Typical scenarios in ITS mainly include traffic flow prediction, traffic target recognition, and vehicular edge computing. However, most current ITS applications rely on a centralized training approach where users upload source data to a cloud server with high computing power for management and centralized training. This approach has limitations such as poor real-time performance, data silos, and difficulty in guaranteeing data privacy. To address these limitations, federated learning (FL) has been proposed as a promising solution. In this paper, we present a comprehensive review of the application of FL in ITS, with a particular focus on three key scenarios: traffic flow prediction, traffic target recognition, and vehicular edge computing. For each scenario, we provide an in-depth analysis of its key characteristics, current challenges, and specific manners in which FL is leveraged. Moreover, we discuss the benefits that FL can offer as a potential solution to the limitations of the centralized training approach currently used in ITS applications.","sentences":["The development of Intelligent Transportation System (ITS) has brought about comprehensive urban traffic information that not only provides convenience to urban residents in their daily lives but also enhances the efficiency of urban road usage, leading to a more harmonious and sustainable urban life.","Typical scenarios in ITS mainly include traffic flow prediction, traffic target recognition, and vehicular edge computing.","However, most current ITS applications rely on a centralized training approach where users upload source data to a cloud server with high computing power for management and centralized training.","This approach has limitations such as poor real-time performance, data silos, and difficulty in guaranteeing data privacy.","To address these limitations, federated learning (FL) has been proposed as a promising solution.","In this paper, we present a comprehensive review of the application of FL in ITS, with a particular focus on three key scenarios: traffic flow prediction, traffic target recognition, and vehicular edge computing.","For each scenario, we provide an in-depth analysis of its key characteristics, current challenges, and specific manners in which FL is leveraged.","Moreover, we discuss the benefits that FL can offer as a potential solution to the limitations of the centralized training approach currently used in ITS applications."],"url":"http://arxiv.org/abs/2403.07444v1","category":"cs.NI"}
{"created":"2024-03-12 09:32:56","title":"Magnetic Phase Diagram and Skyrmions of the Hubbard Model on the Beta-Mn Type Lattice","abstract":"Magnetic phase diagram for the Hubbard model on the Beta-Mn type lattice has been calculated as a function of the Coulomb interaction energy parameter U and the electron number per atom n by using the generalized Hartree-Fock approximation combined with the recursion method for electronic-structure calculations. The ferromagnetic state, the ferrimagnetic state, and the helimagnetic state as well as the 3Q multiple spin density waves (3QMSDW) states have been obtained on the U-n plane. Their detailed structures are examined with use of the Fourier analysis. It is shown that the calculated phase diagram and the Dzyaloshinskii-Moriya interaction elucidate the magnetic interactions for the itinerant-electron skyrmions in transition metal alloys and compounds on the Beta-Mn type lattice.","sentences":["Magnetic phase diagram for the Hubbard model on the Beta-Mn type lattice has been calculated as a function of the Coulomb interaction energy parameter U and the electron number per atom n by using the generalized Hartree-Fock approximation combined with the recursion method for electronic-structure calculations.","The ferromagnetic state, the ferrimagnetic state, and the helimagnetic state as well as the 3Q multiple spin density waves (3QMSDW) states have been obtained on the U-n plane.","Their detailed structures are examined with use of the Fourier analysis.","It is shown that the calculated phase diagram and the Dzyaloshinskii-Moriya interaction elucidate the magnetic interactions for the itinerant-electron skyrmions in transition metal alloys and compounds on the Beta-Mn type lattice."],"url":"http://arxiv.org/abs/2403.07443v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 09:32:25","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning","abstract":"Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA). MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix T to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model's performance in downstream tasks. In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and WebNLG tasks, respectively.","sentences":["Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs.","Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources.","Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity.","We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity.","In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).","MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix T to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model's performance in downstream tasks.","In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and WebNLG tasks, respectively."],"url":"http://arxiv.org/abs/2403.07440v1","category":"cs.CL"}
{"created":"2024-03-12 09:16:26","title":"Baryonic Vortex Phase and Magnetic Field Generation in QCD with Isospin and Baryon Chemical Potentials","abstract":"We propose a novel baryonic vortex phase in low energy dense QCD with finite baryon and isospin chemical potentials. It is known that the homogeneous charged pion condensate takes up the ground state in the presence of the isospin chemical potential, and therein arises the Abrikosov vortex lattice with an applied magnetic field. We first demonstrate that a vortex sustaining the same quantized magnetic flux as the conventional Abrikosov vortex, carries a baryon number captured by the third homotopy group of Skyrmions, given the added modulation of the neutral pion inside the vortex core. Such a state is therefore dubbed the baryonic vortex. We reveal that when the baryon chemical potential is above a critical value, the baryonic vortex has negative tension measured from the charged pion condensation. It implies the baryonic vortex phase would emerge spontaneously without any external magnetic field, and take over the ground state at high density. This new phase contributes to the comprehension of QCD phase diagram and could be relevant to the generation of magnetic fields inside neutron stars.","sentences":["We propose a novel baryonic vortex phase in low energy dense QCD with finite baryon and isospin chemical potentials.","It is known that the homogeneous charged pion condensate takes up the ground state in the presence of the isospin chemical potential, and therein arises the Abrikosov vortex lattice with an applied magnetic field.","We first demonstrate that a vortex sustaining the same quantized magnetic flux as the conventional Abrikosov vortex, carries a baryon number captured by the third homotopy group of Skyrmions, given the added modulation of the neutral pion inside the vortex core.","Such a state is therefore dubbed the baryonic vortex.","We reveal that when the baryon chemical potential is above a critical value, the baryonic vortex has negative tension measured from the charged pion condensation.","It implies the baryonic vortex phase would emerge spontaneously without any external magnetic field, and take over the ground state at high density.","This new phase contributes to the comprehension of QCD phase diagram and could be relevant to the generation of magnetic fields inside neutron stars."],"url":"http://arxiv.org/abs/2403.07433v1","category":"hep-ph"}
{"created":"2024-03-12 09:15:12","title":"Knowledge Transfer across Multiple Principal Component Analysis Studies","abstract":"Transfer learning has aroused great interest in the statistical community. In this article, we focus on knowledge transfer for unsupervised learning tasks in contrast to the supervised learning tasks in the literature. Given the transferable source populations, we propose a two-step transfer learning algorithm to extract useful information from multiple source principal component analysis (PCA) studies, thereby enhancing estimation accuracy for the target PCA task. In the first step, we integrate the shared subspace information across multiple studies by a proposed method named as Grassmannian barycenter, instead of directly performing PCA on the pooled dataset. The proposed Grassmannian barycenter method enjoys robustness and computational advantages in more general cases. Then the resulting estimator for the shared subspace from the first step is further utilized to estimate the target private subspace in the second step. Our theoretical analysis credits the gain of knowledge transfer between PCA studies to the enlarged eigenvalue gap, which is different from the existing supervised transfer learning tasks where sparsity plays the central role. In addition, we prove that the bilinear forms of the empirical spectral projectors have asymptotic normality under weaker eigenvalue gap conditions after knowledge transfer. When the set of informativesources is unknown, we endow our algorithm with the capability of useful dataset selection by solving a rectified optimization problem on the Grassmann manifold, which in turn leads to a computationally friendly rectified Grassmannian K-means procedure. In the end, extensive numerical simulation results and a real data case concerning activity recognition are reported to support our theoretical claims and to illustrate the empirical usefulness of the proposed transfer learning methods.","sentences":["Transfer learning has aroused great interest in the statistical community.","In this article, we focus on knowledge transfer for unsupervised learning tasks in contrast to the supervised learning tasks in the literature.","Given the transferable source populations, we propose a two-step transfer learning algorithm to extract useful information from multiple source principal component analysis (PCA) studies, thereby enhancing estimation accuracy for the target PCA task.","In the first step, we integrate the shared subspace information across multiple studies by a proposed method named as Grassmannian barycenter, instead of directly performing PCA on the pooled dataset.","The proposed Grassmannian barycenter method enjoys robustness and computational advantages in more general cases.","Then the resulting estimator for the shared subspace from the first step is further utilized to estimate the target private subspace in the second step.","Our theoretical analysis credits the gain of knowledge transfer between PCA studies to the enlarged eigenvalue gap, which is different from the existing supervised transfer learning tasks where sparsity plays the central role.","In addition, we prove that the bilinear forms of the empirical spectral projectors have asymptotic normality under weaker eigenvalue gap conditions after knowledge transfer.","When the set of informativesources is unknown, we endow our algorithm with the capability of useful dataset selection by solving a rectified optimization problem on the Grassmann manifold, which in turn leads to a computationally friendly rectified Grassmannian K-means procedure.","In the end, extensive numerical simulation results and a real data case concerning activity recognition are reported to support our theoretical claims and to illustrate the empirical usefulness of the proposed transfer learning methods."],"url":"http://arxiv.org/abs/2403.07431v1","category":"stat.ML"}
{"created":"2024-03-12 09:13:12","title":"Dual orthogonally-polarized lasing assisted by imaginary Fermi arcs in organic microcavities","abstract":"The polarization control of micro/nano lasers is an important topic in nanophotonics. Up to now, the simultaneous generation of two distinguishable orthogonally-polarized lasing modes from a single organic microlaser remains a critical challenge. Here, we demonstrate simultaneously orthogonally-polarized dual lasing from a microcavity filled with an organic single crystal exhibiting selective strong coupling. We show that the non-Hermiticity due to polarization-dependent losses leads to the formation of real and imaginary Fermi arcs with exceptional points. Simultaneous orthogonally-polarized lasing becomes possible thanks to the eigenstate mixing by the photonic spin-orbit coupling at the imaginary Fermi arcs. Our work provides a novel way to develop linearly-polarized lasers and paves the way for the future fundamental research in topological photonics, non-Hermitian optics, and other fields.","sentences":["The polarization control of micro/nano lasers is an important topic in nanophotonics.","Up to now, the simultaneous generation of two distinguishable orthogonally-polarized lasing modes from a single organic microlaser remains a critical challenge.","Here, we demonstrate simultaneously orthogonally-polarized dual lasing from a microcavity filled with an organic single crystal exhibiting selective strong coupling.","We show that the non-Hermiticity due to polarization-dependent losses leads to the formation of real and imaginary Fermi arcs with exceptional points.","Simultaneous orthogonally-polarized lasing becomes possible thanks to the eigenstate mixing by the photonic spin-orbit coupling at the imaginary Fermi arcs.","Our work provides a novel way to develop linearly-polarized lasers and paves the way for the future fundamental research in topological photonics, non-Hermitian optics, and other fields."],"url":"http://arxiv.org/abs/2403.07429v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 09:11:02","title":"Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion Segmentation","abstract":"In machine learning larger databases are usually associated with higher classification accuracy due to better generalization. This generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies. This paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data. In this way heterogeneous databases are supported two-fold. First, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier. The proposed approach is evaluated using the SISS challenge. The proposed algorithm leads to a significant improvement of the classification accuracy.","sentences":["In machine learning larger databases are usually associated with higher classification accuracy due to better generalization.","This generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies.","This paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data.","In this way heterogeneous databases are supported two-fold.","First, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier.","The proposed approach is evaluated using the SISS challenge.","The proposed algorithm leads to a significant improvement of the classification accuracy."],"url":"http://arxiv.org/abs/2403.07428v1","category":"eess.IV"}
{"created":"2024-03-12 09:04:17","title":"Automated Discovery of Anomalous Features in Ultra-Large Planetary Remote Sensing Datasets using Variational Autoencoders","abstract":"The NASA Lunar Reconnaissance Orbiter (LRO) has returned petabytes of lunar high spatial resolution surface imagery over the past decade, impractical for humans to fully review manually. Here we develop an automated method using a deep generative visual model that rapidly retrieves scientifically interesting examples of LRO surface imagery representing the first planetary image anomaly detector. We give quantitative experimental evidence that our method preferentially retrieves anomalous samples such as notable geological features and known human landing and spacecraft crash sites. Our method addresses a major capability gap in planetary science and presents a novel way to unlock insights hidden in ever-increasing remote sensing data archives, with numerous applications to other science domains. We publish our code and data along with this paper.","sentences":["The NASA Lunar Reconnaissance Orbiter (LRO) has returned petabytes of lunar high spatial resolution surface imagery over the past decade, impractical for humans to fully review manually.","Here we develop an automated method using a deep generative visual model that rapidly retrieves scientifically interesting examples of LRO surface imagery representing the first planetary image anomaly detector.","We give quantitative experimental evidence that our method preferentially retrieves anomalous samples such as notable geological features and known human landing and spacecraft crash sites.","Our method addresses a major capability gap in planetary science and presents a novel way to unlock insights hidden in ever-increasing remote sensing data archives, with numerous applications to other science domains.","We publish our code and data along with this paper."],"url":"http://arxiv.org/abs/2403.07424v1","category":"astro-ph.EP"}
{"created":"2024-03-12 08:59:27","title":"On the Borel complexity and the complete metrizability of spaces of metrics","abstract":"Given a metrizable space $X$, let $AM(X)$ be the space of continuous bounded admissible metrics on $X$, which is endowed with the sup-metric. In this paper, we shall investigate the Borel complexity and the complete metrizability of $AM(X)$ and show that a separable metrizable space $X$ is $\\sigma$-compact if and only if $AM(X)$ is completely metrizable.","sentences":["Given a metrizable space $X$, let $AM(X)$ be the space of continuous bounded admissible metrics on $X$, which is endowed with the sup-metric.","In this paper, we shall investigate the Borel complexity and the complete metrizability of $AM(X)$ and show that a separable metrizable space $X$ is $\\sigma$-compact if and only if $AM(X)$ is completely metrizable."],"url":"http://arxiv.org/abs/2403.07421v1","category":"math.GN"}
{"created":"2024-03-12 08:57:29","title":"DragAnything: Motion Control for Anything using Entity Representation","abstract":"We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video generation. Comparison to existing motion control methods, DragAnything offers several advantages. Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g., masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background. Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our DragAnything achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods (e.g., DragNUWA) by 26% in human voting.","sentences":["We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video generation.","Comparison to existing motion control methods, DragAnything offers several advantages.","Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g., masks, depth maps) is labor-intensive.","Users only need to draw a line (trajectory) during interaction.","Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background.","Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects.","Extensive experiments demonstrate that our DragAnything achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods (e.g., DragNUWA) by 26% in human voting."],"url":"http://arxiv.org/abs/2403.07420v1","category":"cs.CV"}
{"created":"2024-03-12 08:40:34","title":"Strong asymptotic giant branch stars' spectral features in distant quiescent galaxies: Impact on galaxy evolution","abstract":"Age-dating and weighting stellar populations in galaxies at various cosmic epochs are essential steps to study galaxy formation through cosmic times. Evolutionary population synthesis models with different input physics are used towards this aim. In particular, the contribution from the thermally pulsing asymptotic-giant-branch (TP-AGB) stellar phase, which peaks for intermediate-age 0.6-2 Gyr systems, has been debated upon for decades. Here we report the detection of strong cool star signatures in the rest-frame near-infrared spectra of three young (~1 Gyr), massive (~10^10 Msun) quiescent galaxies at large look-back time, z=1-2, using JWST/NIRSpec. The co-existence of oxygen- and carbon-type absorption features, spectral edges and features from rare species such as Vanadium, and possibly Zirconium, reveal a strong contribution from TP-AGB stars. Population synthesis models with significant TP-AGB contribution reproduce the observations considerably better than those with weak TP-AGB, which are those commonly used. These findings call for revisions of published stellar population fitting results, pointing to lower masses and younger ages, with additional implications on cosmic dust production and chemical enrichment. These results will stimulate new generations of improved models informed by these and future observations.","sentences":["Age-dating and weighting stellar populations in galaxies at various cosmic epochs are essential steps to study galaxy formation through cosmic times.","Evolutionary population synthesis models with different input physics are used towards this aim.","In particular, the contribution from the thermally pulsing asymptotic-giant-branch (TP-AGB) stellar phase, which peaks for intermediate-age 0.6-2 Gyr systems, has been debated upon for decades.","Here we report the detection of strong cool star signatures in the rest-frame near-infrared spectra of three young (~1 Gyr), massive (~10^10 Msun) quiescent galaxies at large look-back time, z=1-2, using JWST/NIRSpec.","The co-existence of oxygen- and carbon-type absorption features, spectral edges and features from rare species such as Vanadium, and possibly Zirconium, reveal a strong contribution from TP-AGB stars.","Population synthesis models with significant TP-AGB contribution reproduce the observations considerably better than those with weak TP-AGB, which are those commonly used.","These findings call for revisions of published stellar population fitting results, pointing to lower masses and younger ages, with additional implications on cosmic dust production and chemical enrichment.","These results will stimulate new generations of improved models informed by these and future observations."],"url":"http://arxiv.org/abs/2403.07414v1","category":"astro-ph.GA"}
{"created":"2024-03-12 08:38:12","title":"Cube tilings with linear constraints","abstract":"We consider tilings $(\\mathcal{Q},\\Phi)$ of $\\mathbb{R}^d$ where $\\mathcal{Q}$ is the $d$-dimensional unit cube and the set of translations $\\Phi$ is constrained to lie in a pre-determined lattice $A \\mathbb{Z}^d$ in $\\mathbb{R}^d$. We provide a full characterization of matrices $A$ for which such cube tilings exist when $\\Phi$ is a sublattice of $A\\mathbb{Z}^d$ with any $d \\in \\mathbb{N}$ or a generic subset of $A\\mathbb{Z}^d$ with $d\\leq 7$. As a direct consequence of our results, we obtain a criterion for the existence of linearly constrained frequency sets, that is, $\\Phi \\subseteq A\\mathbb{Z}^d$, such that the respective set of complex exponential functions $\\mathcal{E} (\\Phi)$ is an orthogonal Fourier basis for the space of square integrable functions supported on a parallelepiped $B\\mathcal{Q}$, where $A, B \\in \\mathbb{R}^{d \\times d}$ are nonsingular matrices given a priori. Similarly constructed Riesz bases are considered in a companion paper.","sentences":["We consider tilings $(\\mathcal{Q},\\Phi)$ of $\\mathbb{R}^d$ where $\\mathcal{Q}$ is the $d$-dimensional unit cube and the set of translations $\\Phi$ is constrained to lie in a pre-determined lattice $A \\mathbb{Z}^d$ in $\\mathbb{R}^d$. We provide a full characterization of matrices $A$ for which such cube tilings exist when $\\Phi$ is a sublattice of $A\\mathbb{Z}^d$ with any $d \\in \\mathbb{N}$ or a generic subset of $A\\mathbb{Z}^d$ with $d\\leq 7$.","As a direct consequence of our results, we obtain a criterion for the existence of linearly constrained frequency sets, that is, $\\Phi \\subseteq A\\mathbb{Z}^d$, such that the respective set of complex exponential functions $\\mathcal{E} (\\Phi)$ is an orthogonal Fourier basis for the space of square integrable functions supported on a parallelepiped $B\\mathcal{Q}$, where $A, B \\in \\mathbb{R}^{d \\times d}$ are nonsingular matrices given a priori.","Similarly constructed Riesz bases are considered in a companion paper."],"url":"http://arxiv.org/abs/2403.07411v1","category":"math.CA"}
{"created":"2024-03-12 08:34:34","title":"In-context learning enables multimodal large language models to classify cancer pathology images","abstract":"Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to fine-tune foundation models. However, this process is computationally and technically demanding. In language processing, in-context learning provides an alternative, where models learn from within prompts, bypassing the need for parameter updates. Yet, in-context learning remains underexplored in medical image analysis. Here, we systematically evaluate the model Generative Pretrained Transformer 4 with Vision capabilities (GPT-4V) on cancer image processing with in-context learning on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections. Our results show that in-context learning is sufficient to match or even outperform specialized neural networks trained for particular tasks, while only requiring a minimal number of samples. In summary, this study demonstrates that large vision language models trained on non-domain specific data can be applied out-of-the box to solve medical image-processing tasks in histopathology. This democratizes access of generalist AI models to medical experts without technical background especially for areas where annotated data is scarce.","sentences":["Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to fine-tune foundation models.","However, this process is computationally and technically demanding.","In language processing, in-context learning provides an alternative, where models learn from within prompts, bypassing the need for parameter updates.","Yet, in-context learning remains underexplored in medical image analysis.","Here, we systematically evaluate the model Generative Pretrained Transformer 4 with Vision capabilities (GPT-4V) on cancer image processing with in-context learning on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections.","Our results show that in-context learning is sufficient to match or even outperform specialized neural networks trained for particular tasks, while only requiring a minimal number of samples.","In summary, this study demonstrates that large vision language models trained on non-domain specific data can be applied out-of-the box to solve medical image-processing tasks in histopathology.","This democratizes access of generalist AI models to medical experts without technical background especially for areas where annotated data is scarce."],"url":"http://arxiv.org/abs/2403.07407v1","category":"cs.CV"}
{"created":"2024-03-12 08:33:26","title":"Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning","abstract":"Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting. We assess the accuracy and computational cost of various continual learning techniques enhanced with early-exits and TLC across standard class-incremental learning benchmarks such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70\\% of their computations. Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points. Our research underscores the inherent synergy between early-exit networks and continual learning, emphasizing their practical utility in resource-constrained environments.","sentences":["Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention.","These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources.","However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data.","This study aims to explore the continual learning of the early-exit networks.","We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting.","We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources.","Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting.","We assess the accuracy and computational cost of various continual learning techniques enhanced with early-exits and TLC across standard class-incremental learning benchmarks such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70\\% of their computations.","Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points.","Our research underscores the inherent synergy between early-exit networks and continual learning, emphasizing their practical utility in resource-constrained environments."],"url":"http://arxiv.org/abs/2403.07404v1","category":"cs.LG"}
{"created":"2024-03-12 08:32:23","title":"From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios","abstract":"The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field. Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life. This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans. Toward this end, we present two new benchmarks, namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals. These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain. In addition, we also propose a simple yet effective baseline method named Multi-Cluster Reference Learning (MCRL) to tackle the aforementioned domain gap. MCRL is motivated by the observation that food images in daily-life scenarios exhibit greater intra-class appearance variance compared with those in well-curated benchmarks. Notably, MCRL can be seamlessly coupled with existing approaches, yielding non-trivial performance enhancements. We hope our new benchmarks can inspire the community to explore the transferability of food recognition models trained on well-curated datasets toward practical real-life applications.","sentences":["The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years.","Prominent benchmarks, such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field.","Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life.","This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans.","Toward this end, we present two new benchmarks, namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals.","These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain.","In addition, we also propose a simple yet effective baseline method named Multi-Cluster Reference Learning (MCRL) to tackle the aforementioned domain gap.","MCRL is motivated by the observation that food images in daily-life scenarios exhibit greater intra-class appearance variance compared with those in well-curated benchmarks.","Notably, MCRL can be seamlessly coupled with existing approaches, yielding non-trivial performance enhancements.","We hope our new benchmarks can inspire the community to explore the transferability of food recognition models trained on well-curated datasets toward practical real-life applications."],"url":"http://arxiv.org/abs/2403.07403v1","category":"cs.CV"}
{"created":"2024-03-12 08:29:40","title":"Renormalization of Complex Networks with Partition Functions","abstract":"While renormalization groups are fundamental in physics, renormalization of complex networks remains vague in its conceptual definition and methodology. Here, we propose a novel strategy to renormalize complex networks. Rather than resorting to handling the bare structure of a network, we overlay it with a readily renormalizable physical model, which reflects real-world scenarios with a broad generality. From the renormalization of the overlying system, we extract a rigorous and simple renormalization group transformation of arbitrary networks. In this way, we obtain a transparent, model-dependent physical meaning of the network renormalization, which in our case is a scale transformation preserving the transition dynamics of low-density particles. We define the strength of a node in accordance with the physical model and trace the change of its distribution under our renormalization process. This analysis demonstrates that the strength distributions of scale-free networks remain scale-invariant, whereas those of homogeneous random networks do not.","sentences":["While renormalization groups are fundamental in physics, renormalization of complex networks remains vague in its conceptual definition and methodology.","Here, we propose a novel strategy to renormalize complex networks.","Rather than resorting to handling the bare structure of a network, we overlay it with a readily renormalizable physical model, which reflects real-world scenarios with a broad generality.","From the renormalization of the overlying system, we extract a rigorous and simple renormalization group transformation of arbitrary networks.","In this way, we obtain a transparent, model-dependent physical meaning of the network renormalization, which in our case is a scale transformation preserving the transition dynamics of low-density particles.","We define the strength of a node in accordance with the physical model and trace the change of its distribution under our renormalization process.","This analysis demonstrates that the strength distributions of scale-free networks remain scale-invariant, whereas those of homogeneous random networks do not."],"url":"http://arxiv.org/abs/2403.07402v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 08:18:10","title":"Automorphisms of Hilbert schemes of Cayley's K3 surfaces","abstract":"We prove that the automorphism group of Hilbert square of a Cayley's K3 surface of Picard number 2 is the free product of three cyclic groups of order two. The generators are three Beauville involutions.","sentences":["We prove that the automorphism group of Hilbert square of a Cayley's K3 surface of Picard number 2 is the free product of three cyclic groups of order two.","The generators are three Beauville involutions."],"url":"http://arxiv.org/abs/2403.07399v1","category":"math.AG"}
{"created":"2024-03-12 08:13:52","title":"Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs","abstract":"Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations.","sentences":["Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship.","However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events.","To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions.","Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations."],"url":"http://arxiv.org/abs/2403.07398v1","category":"cs.CL"}
{"created":"2024-03-12 17:58:38","title":"OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation","abstract":"Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines. The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics. However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies. We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands and bimanual arms, through an easy-to-use app. Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views. We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots. A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework. Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and Allegro platforms, OPEN TEACH is fully open-sourced to promote broader adoption. Videos are available at https://open-teach.github.io/.","sentences":["Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines.","The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics.","However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies.","We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control.","Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands and bimanual arms, through an easy-to-use app.","Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views.","We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots.","A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework.","Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks.","Currently supporting Franka, xArm, Jaco, and Allegro platforms, OPEN TEACH is fully open-sourced to promote broader adoption.","Videos are available at https://open-teach.github.io/."],"url":"http://arxiv.org/abs/2403.07870v1","category":"cs.RO"}
{"created":"2024-03-12 17:11:50","title":"Parity questions in critical planar Brownian loop-soups (or \"where did the free planar bosons go?\")","abstract":"The critical two-dimensional Brownian loop-soup is an infinite collection of non-interacting Brownian loops in a planar domain that possesses some combinatorial features related to the notion of indistinguishability of bosons. The properly renormalized occupation time field of this collection of loops is known to be distributed like the properly defined square of a Gaussian free field. In the present paper, we investigate aspects of the question about how much information these fields provide about the loop-soup. Among other things, we show that the exact set of points that are actually visited by some loops in the loop-soup is not determined by these fields. We further prove that given the fields, a dense family of special points will each have a conditional probability 1/2 of being part of the loop-soup. We also exhibit another instance where the possible decompositions (given the field) into individual loops and excursions can be grouped into two clearly different groups, each having a conditional probability 1/2 of occurring.","sentences":["The critical two-dimensional Brownian loop-soup is an infinite collection of non-interacting Brownian loops in a planar domain that possesses some combinatorial features related to the notion of indistinguishability of bosons.","The properly renormalized occupation time field of this collection of loops is known to be distributed like the properly defined square of a Gaussian free field.","In the present paper, we investigate aspects of the question about how much information these fields provide about the loop-soup.","Among other things, we show that the exact set of points that are actually visited by some loops in the loop-soup is not determined by these fields.","We further prove that given the fields, a dense family of special points will each have a conditional probability 1/2 of being part of the loop-soup.","We also exhibit another instance where the possible decompositions (given the field) into individual loops and excursions can be grouped into two clearly different groups, each having a conditional probability 1/2 of occurring."],"url":"http://arxiv.org/abs/2403.07830v1","category":"math.PR"}
{"created":"2024-03-12 16:27:25","title":"RobotCycle: Assessing Cycling Safety in Urban Environments","abstract":"This paper introduces RobotCycle, a novel ongoing project that leverages Autonomous Vehicle (AV) research to investigate how cycling infrastructure influences cyclist behaviour and safety during real-world journeys. The project's requirements were defined in collaboration with key stakeholders (i.e. city planners, cyclists, and policymakers), informing the design of risk and safety metrics and the data collection criteria. We propose a data-driven approach relying on a novel, rich dataset of diverse traffic scenes captured through a custom-designed wearable sensing unit. We extract road-user trajectories and analyse deviations suggesting risk or potentially hazardous interactions in correlation with infrastructural elements in the environment. Driving profiles and trajectory patterns are associated with local road segments, driving conditions, and road-user interactions to predict traffic behaviour and identify critical scenarios. Moreover, leveraging advancements in AV research, the project extracts detailed 3D maps, traffic flow patterns, and trajectory models to provide an in-depth assessment and analysis of the behaviour of all traffic agents. This data can then inform the design of cyclist-friendly road infrastructure, improving road safety and cyclability, as it provides valuable insights for enhancing cyclist protection and promoting sustainable urban mobility.","sentences":["This paper introduces RobotCycle, a novel ongoing project that leverages Autonomous Vehicle (AV) research to investigate how cycling infrastructure influences cyclist behaviour and safety during real-world journeys.","The project's requirements were defined in collaboration with key stakeholders (i.e. city planners, cyclists, and policymakers), informing the design of risk and safety metrics and the data collection criteria.","We propose a data-driven approach relying on a novel, rich dataset of diverse traffic scenes captured through a custom-designed wearable sensing unit.","We extract road-user trajectories and analyse deviations suggesting risk or potentially hazardous interactions in correlation with infrastructural elements in the environment.","Driving profiles and trajectory patterns are associated with local road segments, driving conditions, and road-user interactions to predict traffic behaviour and identify critical scenarios.","Moreover, leveraging advancements in AV research, the project extracts detailed 3D maps, traffic flow patterns, and trajectory models to provide an in-depth assessment and analysis of the behaviour of all traffic agents.","This data can then inform the design of cyclist-friendly road infrastructure, improving road safety and cyclability, as it provides valuable insights for enhancing cyclist protection and promoting sustainable urban mobility."],"url":"http://arxiv.org/abs/2403.07789v1","category":"cs.RO"}
{"created":"2024-03-12 15:11:47","title":"Performance Analysis of Matrix Multiplication for Deep Learning on the Edge","abstract":"The devices designed for the Internet-of-Things encompass a large variety of distinct processor architectures, forming a highly heterogeneous zoo. In order to tackle this, we employ a simulator to estimate the performance of the matrix-matrix multiplication (GEMM) kernel on processors designed to operate at the edge. Our simulator adheres to the modern implementations of GEMM, advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the amount of data transfers across the memory hierarchy of different algorithmic variants of the kernel. %Armed with this tool, A small collection of experiments provide the necessary data to calibrate the simulator and deliver highly accurate estimations of the execution time for a given processor architecture.","sentences":["The devices designed for the Internet-of-Things encompass a large variety of distinct processor architectures, forming a highly heterogeneous zoo.","In order to tackle this, we employ a simulator to estimate the performance of the matrix-matrix multiplication (GEMM) kernel on processors designed to operate at the edge.","Our simulator adheres to the modern implementations of GEMM, advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the amount of data transfers across the memory hierarchy of different algorithmic variants of the kernel.","%Armed with this tool, A small collection of experiments provide the necessary data to calibrate the simulator and deliver highly accurate estimations of the execution time for a given processor architecture."],"url":"http://arxiv.org/abs/2403.07731v1","category":"cs.AR"}
{"created":"2024-03-12 13:31:14","title":"Characterization of Large Language Model Development in the Datacenter","abstract":"Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery. (2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization.","sentences":["Large Language Models (LLMs) have presented impressive performance across several transformative tasks.","However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization.","In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme.","Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures.","Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs.","Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery.","(2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization."],"url":"http://arxiv.org/abs/2403.07648v1","category":"cs.DC"}
{"created":"2024-03-12 13:04:37","title":"Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction","abstract":"Smart indoor tourist attractions, such as smart museums and aquariums, usually require a significant investment in indoor localization devices. The smartphone Global Positional Systems use is unsuitable for scenarios where dense materials such as concrete and metal block weaken the GPS signals, which is the most common scenario in an indoor tourist attraction. Deep learning makes it possible to perform region-wise indoor localization using smartphone images. This approach does not require any investment in infrastructure, reducing the cost and time to turn museums and aquariums into smart museums or smart aquariums. This paper proposes using deep learning algorithms to classify locations using smartphone camera images for indoor tourism attractions. We evaluate our proposal in a real-world scenario in Brazil. We extensively collect images from ten different smartphones to classify biome-themed fish tanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We tested seven state-of-the-art neural networks, three being transformer-based, achieving precision around 90% on average and recall and f-score around 89% on average. The results indicate good feasibility of the proposal in a most indoor tourist attractions.","sentences":["Smart indoor tourist attractions, such as smart museums and aquariums, usually require a significant investment in indoor localization devices.","The smartphone Global Positional Systems use is unsuitable for scenarios where dense materials such as concrete and metal block weaken the GPS signals, which is the most common scenario in an indoor tourist attraction.","Deep learning makes it possible to perform region-wise indoor localization using smartphone images.","This approach does not require any investment in infrastructure, reducing the cost and time to turn museums and aquariums into smart museums or smart aquariums.","This paper proposes using deep learning algorithms to classify locations using smartphone camera images for indoor tourism attractions.","We evaluate our proposal in a real-world scenario in Brazil.","We extensively collect images from ten different smartphones to classify biome-themed fish tanks inside the Pantanal Biopark, creating a new dataset of 3654 images.","We tested seven state-of-the-art neural networks, three being transformer-based, achieving precision around 90% on average and recall and f-score around 89% on average.","The results indicate good feasibility of the proposal in a most indoor tourist attractions."],"url":"http://arxiv.org/abs/2403.07621v1","category":"cs.CV"}
{"created":"2024-03-12 12:46:24","title":"Bilocal holography and locality in the bulk","abstract":"Bilocal holography provides a constructive approach to the vector model/higher spin gravity duality. It has two ingredients: a change of field variables and a change of space time coordinates. The change of field variables ensures that the loop expansion parameter becomes ${1\\over N}$. The change of coordinates solves the Clebsch-Gordan problem of moving from the tensor product basis (in which the collective bilocal field is written) to the direct sum basis (appropriate for the description of the gravity fields). We argue that the change of space time coordinates can be deduced by requiring that operators constructed in the bilocal collective field theory are dual to local operators in the AdS bulk.","sentences":["Bilocal holography provides a constructive approach to the vector model/higher spin gravity duality.","It has two ingredients: a change of field variables and a change of space time coordinates.","The change of field variables ensures that the loop expansion parameter becomes ${1\\over N}$. The change of coordinates solves the Clebsch-Gordan problem of moving from the tensor product basis (in which the collective bilocal field is written) to the direct sum basis (appropriate for the description of the gravity fields).","We argue that the change of space time coordinates can be deduced by requiring that operators constructed in the bilocal collective field theory are dual to local operators in the AdS bulk."],"url":"http://arxiv.org/abs/2403.07606v1","category":"hep-th"}
{"created":"2024-03-12 11:49:00","title":"Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance","abstract":"The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently. Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art. kDC not only runs the fastest in practice, but also achieves the best time complexity. Specifically, it runs in $O^*(\\gamma_k^n)$ time when ignoring polynomial factors; here, $\\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $G$. In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance. kDC-Two runs in $O^*( (\\alpha\\Delta)^{k+2} \\gamma_{k-1}^\\alpha)$ time when the maximum $k$-defective clique size $\\omega_k(G)$ is at least $k+2$, and in $O^*(\\gamma_{k-1}^n)$ time otherwise, where $\\alpha$ and $\\Delta$ are the degeneracy and maximum degree of $G$, respectively. In addition, with slight modification, kDC-Two also runs in $O^*( (\\alpha\\Delta)^{k+2} (k+1)^{\\alpha+k+1-\\omega_k(G)})$ time by using the degeneracy gap $\\alpha+k+1-\\omega_k(G)$ parameterization; this is better than $O^*( (\\alpha\\Delta)^{k+2}\\gamma_{k-1}^\\alpha)$ when $\\omega_k(G)$ is close to the degeneracy-based upper bound $\\alpha+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature. Extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude.","sentences":["The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently.","Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art.","kDC","not only runs the fastest in practice, but also achieves the best time complexity.","Specifically, it runs in $O^*(\\gamma_k^n)$ time when ignoring polynomial factors; here, $\\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $G$.","In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance.","kDC-Two runs in $O^*( (\\alpha\\Delta)^{k+2} \\gamma_{k-1}^\\alpha)$ time when the maximum $k$-defective clique size $\\omega_k(G)$ is at least $k+2$, and in $O^*(\\gamma_{k-1}^n)$ time otherwise, where $\\alpha$ and $\\Delta$ are the degeneracy and maximum degree of $G$, respectively.","In addition, with slight modification, kDC-Two also runs in $O^*( (\\alpha\\Delta)^{k+2} (k+1)^{\\alpha+k+1-\\omega_k(G)})$ time by using the degeneracy gap $\\alpha+k+1-\\omega_k(G)$ parameterization; this is better than $O^*( (\\alpha\\Delta)^{k+2}\\gamma_{k-1}^\\alpha)$ when $\\omega_k(G)$ is close to the degeneracy-based upper bound $\\alpha+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature.","Extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude."],"url":"http://arxiv.org/abs/2403.07561v1","category":"cs.DS"}
{"created":"2024-03-12 11:05:44","title":"Comments on characterizing demand flexibility to provide power grid services","abstract":"Many loads have flexibility in demand that can be used to provide ancillary services to power grids. A large body of literature exists on designing algorithms to coordinate actions of many loads to provide such a service. The topic of characterizing the flexibility of one or a collection of loads - to determine what kinds of demand deviation from the baseline is feasible - has also been studied. However, there is a large diversity in definitions of flexibility and methods proposed to characterize flexibility. As a result there are several gaps in the literature on flexibility characterization. Some approaches on flexibility characterization are based on ad-hoc approximations that lead to highly conservative estimates. In this paper we point out some of these issues and their implications, with the hope to encourage additional research to address them.","sentences":["Many loads have flexibility in demand that can be used to provide ancillary services to power grids.","A large body of literature exists on designing algorithms to coordinate actions of many loads to provide such a service.","The topic of characterizing the flexibility of one or a collection of loads - to determine what kinds of demand deviation from the baseline is feasible - has also been studied.","However, there is a large diversity in definitions of flexibility and methods proposed to characterize flexibility.","As a result there are several gaps in the literature on flexibility characterization.","Some approaches on flexibility characterization are based on ad-hoc approximations that lead to highly conservative estimates.","In this paper we point out some of these issues and their implications, with the hope to encourage additional research to address them."],"url":"http://arxiv.org/abs/2403.07529v1","category":"math.OC"}
{"created":"2024-03-12 10:47:17","title":"Is there (no) collective flow in \\textit{pp} collisions?","abstract":"The transverse momentum spectra and their multiplicity dependence serve as key tools for extracting parameters that can be compared with theoretical models. This comparison aims to establish the behaviour and nature of the system created in the collision. Over the past decade, the scientific community has extensively studied the possibility of a system analogous to quark-gluon plasma, predicted in heavy nuclei collisions, also existing in collisions involving light nuclei and proton-proton collisions. We have reanalysed the experimental data published by the ALICE Collaboration at the LHC, exploring a seemingly universal feature of transverse momentum spectra of charged particles. We have identified a specific range where the contribution of the hard part is nonexistent for the studied multiplicities. We present the dependence of the mean transverse momenta obtained in the soft and soft+hard (mixes) parts and discuss the results in the context of the ongoing controversy regarding the existence of collectivity in small systems. Finally, we also discuss possible refinements of the analyses concerning the use of statistical parameters of higher order, aimed at better distinguishing the agreement of theoretical and Monte Carlo models with the data.","sentences":["The transverse momentum spectra and their multiplicity dependence serve as key tools for extracting parameters that can be compared with theoretical models.","This comparison aims to establish the behaviour and nature of the system created in the collision.","Over the past decade, the scientific community has extensively studied the possibility of a system analogous to quark-gluon plasma, predicted in heavy nuclei collisions, also existing in collisions involving light nuclei and proton-proton collisions.","We have reanalysed the experimental data published by the ALICE Collaboration at the LHC, exploring a seemingly universal feature of transverse momentum spectra of charged particles.","We have identified a specific range where the contribution of the hard part is nonexistent for the studied multiplicities.","We present the dependence of the mean transverse momenta obtained in the soft and soft+hard (mixes) parts and discuss the results in the context of the ongoing controversy regarding the existence of collectivity in small systems.","Finally, we also discuss possible refinements of the analyses concerning the use of statistical parameters of higher order, aimed at better distinguishing the agreement of theoretical and Monte Carlo models with the data."],"url":"http://arxiv.org/abs/2403.07512v1","category":"hep-ph"}
{"created":"2024-03-12 10:43:52","title":"Reconstructions of Jupiter's magnetic field using physics informed neural networks","abstract":"Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior. However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales. In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits. The method can resolve local structures, and allows for weak ambient electrical currents. Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data. However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure. We estimate that the dynamo boundary is at a fractional radius of 0.8. At this depth, the magnetic field is arranged into longitudinal bands, and the great blue spot appears to be rooted in neighbouring structures of oppositely signed flux.","sentences":["Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior.","However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales.","In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits.","The method can resolve local structures, and allows for weak ambient electrical currents.","Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data.","However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure.","We estimate that the dynamo boundary is at a fractional radius of 0.8.","At this depth, the magnetic field is arranged into longitudinal bands, and the great blue spot appears to be rooted in neighbouring structures of oppositely signed flux."],"url":"http://arxiv.org/abs/2403.07507v1","category":"astro-ph.EP"}
{"created":"2024-03-12 07:57:33","title":"Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images","abstract":"Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches.","sentences":["Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training.","While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains.","This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays.","Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain.","Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches."],"url":"http://arxiv.org/abs/2403.07389v1","category":"cs.CV"}
{"created":"2024-03-12 07:47:07","title":"Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems","abstract":"In recent years, semantic communication is progressively emerging as an effective means of facilitating intelligent and context-aware communication. However, current researches seldom simultaneously consider the reliability and timeliness of semantic communication, where scheduling and resource allocation (SRA) plays a crucial role. In contrast, conventional age-based approaches cannot seamlessly extend to semantic communication due to their oversight of semantic importance. To bridge this gap, we introduce a novel metric: Age of Semantic Importance (AoSI), which adaptly captures both the freshness of information and its semantic importance. Utilizing AoSI, we formulate an average AoSI minimization problem by optimizing multi-source SRA. To address this problem, we proposed a AoSI-aware joint SRA algorithm based on Deep Q-Network (DQN). Simulation results validate the effectiveness of our proposed method, demonstrating its ability to facilitate timely and reliable semantic communication.","sentences":["In recent years, semantic communication is progressively emerging as an effective means of facilitating intelligent and context-aware communication.","However, current researches seldom simultaneously consider the reliability and timeliness of semantic communication, where scheduling and resource allocation (SRA) plays a crucial role.","In contrast, conventional age-based approaches cannot seamlessly extend to semantic communication due to their oversight of semantic importance.","To bridge this gap, we introduce a novel metric: Age of Semantic Importance (AoSI), which adaptly captures both the freshness of information and its semantic importance.","Utilizing AoSI, we formulate an average AoSI minimization problem by optimizing multi-source SRA.","To address this problem, we proposed a AoSI-aware joint SRA algorithm based on Deep Q-Network (DQN).","Simulation results validate the effectiveness of our proposed method, demonstrating its ability to facilitate timely and reliable semantic communication."],"url":"http://arxiv.org/abs/2403.07386v1","category":"cs.IT"}
{"created":"2024-03-12 07:45:33","title":"SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models","abstract":"Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset (Johnson et al., 2016), S2L again outperforms training on the full dataset using only 50% of the data. Notably, S2L can perform data selection using a reference model 40x smaller than the target model, proportionally reducing the cost of data selection.","sentences":["Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data.","To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models.","We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets.","Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et al., 2023b) by 16.6%.","In clinical text summarization on the MIMIC-III dataset (Johnson et al., 2016), S2L again outperforms training on the full dataset using only 50% of the data.","Notably, S2L can perform data selection using a reference model 40x smaller than the target model, proportionally reducing the cost of data selection."],"url":"http://arxiv.org/abs/2403.07384v1","category":"cs.CL"}
{"created":"2024-03-12 07:41:51","title":"Gabor-guided transformer for single image deraining","abstract":"Image deraining have have gained a great deal of attention in order to address the challenges posed by the effects of harsh weather conditions on visual tasks. While convolutional neural networks (CNNs) are popular, their limitations in capturing global information may result in ineffective rain removal. Transformer-based methods with self-attention mechanisms have improved, but they tend to distort high-frequency details that are crucial for image fidelity. To solve this problem, we propose the Gabor-guided tranformer (Gabformer) for single image deraining. The focus on local texture features is enhanced by incorporating the information processed by the Gabor filter into the query vector, which also improves the robustness of the model to noise due to the properties of the filter. Extensive experiments on the benchmarks demonstrate that our method outperforms state-of-the-art approaches.","sentences":["Image deraining have have gained a great deal of attention in order to address the challenges posed by the effects of harsh weather conditions on visual tasks.","While convolutional neural networks (CNNs) are popular, their limitations in capturing global information may result in ineffective rain removal.","Transformer-based methods with self-attention mechanisms have improved, but they tend to distort high-frequency details that are crucial for image fidelity.","To solve this problem, we propose the Gabor-guided tranformer (Gabformer) for single image deraining.","The focus on local texture features is enhanced by incorporating the information processed by the Gabor filter into the query vector, which also improves the robustness of the model to noise due to the properties of the filter.","Extensive experiments on the benchmarks demonstrate that our method outperforms state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.07380v1","category":"cs.CV"}
{"created":"2024-03-12 07:27:02","title":"NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning","abstract":"Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.","sentences":["Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions.","Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability.","However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus.","This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner.","Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps.","Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision.","Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants.","Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset.","We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications.","Code is available at https://github.com/expectorlin/NavCoT."],"url":"http://arxiv.org/abs/2403.07376v1","category":"cs.CV"}
{"created":"2024-03-12 06:52:24","title":"A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees","abstract":"Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection. There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability. Random forest is a general algorithm that is often used for classification under complex conditions. Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring. In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT). Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission. The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier systems. Extensive experiments demonstrate that the IFRF has competitative and superior performance compared to other state-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for ensemble learning with outstanding classification accuracy. This study is the first to propose a random forest ensemble based on the intuitionistic fuzzy theory.","sentences":["Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection.","There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability.","Random forest is a general algorithm that is often used for classification under complex conditions.","Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring.","In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT).","Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission.","The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier systems.","Extensive experiments demonstrate that the IFRF has competitative and superior performance compared to other state-of-the-art fuzzy and ensemble algorithms.","IFDT is more suitable for ensemble learning with outstanding classification accuracy.","This study is the first to propose a random forest ensemble based on the intuitionistic fuzzy theory."],"url":"http://arxiv.org/abs/2403.07363v1","category":"cs.AI"}
{"created":"2024-03-12 06:50:32","title":"Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning","abstract":"The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility. Our proposal offers a worst-case evaluation of MU's resilience and effectiveness. Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies. Our results illuminate the complex challenges of MU in practice, guiding the future development of more accurate and robust unlearning algorithms. The code is available at https://github.com/OPTML-Group/Unlearn-WorstCase.","sentences":["The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training.","This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning.","Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance.","To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint.","We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set.","Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility.","Our proposal offers a worst-case evaluation of MU's resilience and effectiveness.","Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies.","Our results illuminate the complex challenges of MU in practice, guiding the future development of more accurate and robust unlearning algorithms.","The code is available at https://github.com/OPTML-Group/Unlearn-WorstCase."],"url":"http://arxiv.org/abs/2403.07362v1","category":"cs.LG"}
{"created":"2024-03-12 06:28:41","title":"Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems","abstract":"This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.","sentences":["This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems.","The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization.","In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook.","A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function.","Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead."],"url":"http://arxiv.org/abs/2403.07355v2","category":"eess.SP"}
{"created":"2024-03-12 06:16:33","title":"KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models","abstract":"Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing data. We conducted experiments of different editing methods on five LVLMs, and thoroughly analyze how these methods impact the models. The results reveal strengths and deficiencies of these methods and, hopefully, provide insights into potential avenues for future research.","sentences":["Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs).","Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications.","An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs.","However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content.","We adopt different data collection methods to construct a new benchmark, $\\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation.","Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities.","This directional aspect can be further utilized to extract entity-related knowledge and form editing data.","We conducted experiments of different editing methods on five LVLMs, and thoroughly analyze how these methods impact the models.","The results reveal strengths and deficiencies of these methods and, hopefully, provide insights into potential avenues for future research."],"url":"http://arxiv.org/abs/2403.07350v1","category":"cs.CL"}
{"created":"2024-03-12 06:01:04","title":"Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning","abstract":"Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.","sentences":["Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data.","Existing approaches to ASTE often complicate the task with additional structures or external data.","In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges.","The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead.","Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios.","This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models."],"url":"http://arxiv.org/abs/2403.07342v1","category":"cs.CL"}
{"created":"2024-03-12 05:43:16","title":"D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications","abstract":"Semantic communications (SemCom) have emerged as a new paradigm for supporting sixth-generation applications, where semantic features of data are transmitted using artificial intelligence algorithms to attain high communication efficiencies. Most existing SemCom techniques utilize deep neural networks (DNNs) to implement analog source-channel mappings, which are incompatible with existing digital communication architectures. To address this issue, this paper proposes a novel framework of digital deep joint source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The framework features digital source and channel codings that are jointly optimized to reduce the end-to-end (E2E) distortion. First, deep source coding with an adaptive density model is designed to encode semantic features according to their distributions. Second, digital channel coding is employed to protect encoded features against channel distortion. To facilitate their joint design, the E2E distortion is characterized as a function of the source and channel rates via the analysis of the Bayesian model and Lipschitz assumption on the DNNs. Then to minimize the E2E distortion, a two-step algorithm is proposed to control the source-channel rates for a given channel signal-to-noise ratio. Simulation results reveal that the proposed framework outperforms classic deep JSCC and mitigates the cliff and leveling-off effects, which commonly exist for separation-based approaches.","sentences":["Semantic communications (SemCom) have emerged as a new paradigm for supporting sixth-generation applications, where semantic features of data are transmitted using artificial intelligence algorithms to attain high communication efficiencies.","Most existing SemCom techniques utilize deep neural networks (DNNs) to implement analog source-channel mappings, which are incompatible with existing digital communication architectures.","To address this issue, this paper proposes a novel framework of digital deep joint source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom.","The framework features digital source and channel codings that are jointly optimized to reduce the end-to-end (E2E) distortion.","First, deep source coding with an adaptive density model is designed to encode semantic features according to their distributions.","Second, digital channel coding is employed to protect encoded features against channel distortion.","To facilitate their joint design, the E2E distortion is characterized as a function of the source and channel rates via the analysis of the Bayesian model and Lipschitz assumption on the DNNs.","Then to minimize the E2E distortion, a two-step algorithm is proposed to control the source-channel rates for a given channel signal-to-noise ratio.","Simulation results reveal that the proposed framework outperforms classic deep JSCC and mitigates the cliff and leveling-off effects, which commonly exist for separation-based approaches."],"url":"http://arxiv.org/abs/2403.07338v1","category":"cs.IT"}
{"created":"2024-03-12 05:43:12","title":"Analysis of Intelligent Reflecting Surface-Enhanced Mobility Through a Line-of-Sight State Transition Model","abstract":"Rapid signal fluctuations due to blockage effects cause excessive handovers (HOs) and degrade mobility performance. By reconfiguring line-of-sight (LoS) Links through passive reflections, intelligent reflective surface (IRS) has the potential to address this issue. Due to the lack of introducing blocking effects, existing HO analyses cannot capture excessive HOs or exploit enhancements via IRSs. This paper proposes an LoS state transition model enabling analysis of mobility enhancement achieved by IRS-reconfigured LoS links, where LoS link blocking and reconfiguration utilizing IRS during user movement are explicitly modeled as stochastic processes. Specifically, the condition for blocking LoS links is characterized as a set of possible blockage locations, the distribution of available IRSs is thinned by the criteria for reconfiguring LoS links. In addition, BSs potentially handed over are categorized by probabilities of LoS states to enable HO decision analysis. By projecting distinct gains of LoS states onto a uniform equivalent distance criterion, mobility enhanced by IRS is quantified through the compact expression of HO probability. Results show the probability of dropping into non-LoS decreases by 70% when deploying IRSs with the density of 93/km$^2$, and HOs decrease by 67% under the optimal IRS distributed deployment parameter.","sentences":["Rapid signal fluctuations due to blockage effects cause excessive handovers (HOs) and degrade mobility performance.","By reconfiguring line-of-sight (LoS) Links through passive reflections, intelligent reflective surface (IRS) has the potential to address this issue.","Due to the lack of introducing blocking effects, existing HO analyses cannot capture excessive HOs or exploit enhancements via IRSs.","This paper proposes an LoS state transition model enabling analysis of mobility enhancement achieved by IRS-reconfigured LoS links, where LoS link blocking and reconfiguration utilizing IRS during user movement are explicitly modeled as stochastic processes.","Specifically, the condition for blocking LoS links is characterized as a set of possible blockage locations, the distribution of available IRSs is thinned by the criteria for reconfiguring LoS links.","In addition, BSs potentially handed over are categorized by probabilities of LoS states to enable HO decision analysis.","By projecting distinct gains of LoS states onto a uniform equivalent distance criterion, mobility enhanced by IRS is quantified through the compact expression of HO probability.","Results show the probability of dropping into non-LoS decreases by 70% when deploying IRSs with the density of 93/km$^2$, and HOs decrease by 67% under the optimal IRS distributed deployment parameter."],"url":"http://arxiv.org/abs/2403.07337v1","category":"eess.SP"}
{"created":"2024-03-12 05:34:51","title":"Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention","abstract":"In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic complexity. Additionally, we design a novel hierarchical and bidirectional Mamba block to further enhance the global and neighborhood spatial modeling capability of Mamba. Comprehensive experiments demonstrate the effectiveness and efficiency of our method and the feasibility of using large window size to achieve large receptive fields. Codes are available at https://github.com/wjh892521292/LMa-UNet.","sentences":["In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment.","In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling.","Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity.","In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation.","A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic complexity.","Additionally, we design a novel hierarchical and bidirectional Mamba block to further enhance the global and neighborhood spatial modeling capability of Mamba.","Comprehensive experiments demonstrate the effectiveness and efficiency of our method and the feasibility of using large window size to achieve large receptive fields.","Codes are available at https://github.com/wjh892521292/LMa-UNet."],"url":"http://arxiv.org/abs/2403.07332v1","category":"cs.CV"}
{"created":"2024-03-12 05:16:41","title":"Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks","abstract":"Owning to the reflection gain and double path loss featured by intelligent reflecting surface (IRS) channels, handover (HO) locations become irregular and the signal strength fluctuates sharply with variations in IRS connections during HO, the risk of HO failures (HOFs) is exacerbated and thus HO parameters require reconfiguration. However, existing HO models only assume monotonic negative exponential path loss and cannot obtain sound HO parameters. This paper proposes a discrete-time model to explicitly track the HO process with variations in IRS connections, where IRS connections and HO process are discretized as finite states by measurement intervals, and transitions between states are modeled as stochastic processes. Specifically, to capture signal fluctuations during HO, IRS connection state-dependent distributions of the user-IRS distance are modified by the correlation between measurement intervals. In addition, states of the HO process are formed with Time-to-Trigger and HO margin whose transition probabilities are integrated concerning all IRS connection states. Trigger location distributions and probabilities of HO, HOF, and ping-pong (PP) are obtained by tracing user HO states. Results show IRSs mitigate PPs by 48% but exacerbate HOFs by 90% under regular parameters. Optimal parameters are mined ensuring probabilities of HOF and PP are both less than 0.1%.","sentences":["Owning to the reflection gain and double path loss featured by intelligent reflecting surface (IRS) channels, handover (HO) locations become irregular and the signal strength fluctuates sharply with variations in IRS connections during HO, the risk of HO failures (HOFs) is exacerbated and thus HO parameters require reconfiguration.","However, existing HO models only assume monotonic negative exponential path loss and cannot obtain sound HO parameters.","This paper proposes a discrete-time model to explicitly track the HO process with variations in IRS connections, where IRS connections and HO process are discretized as finite states by measurement intervals, and transitions between states are modeled as stochastic processes.","Specifically, to capture signal fluctuations during HO, IRS connection state-dependent distributions of the user-IRS distance are modified by the correlation between measurement intervals.","In addition, states of the HO process are formed with Time-to-Trigger and HO margin whose transition probabilities are integrated concerning all IRS connection states.","Trigger location distributions and probabilities of HO, HOF, and ping-pong (PP) are obtained by tracing user HO states.","Results show IRSs mitigate PPs by 48% but exacerbate HOFs by 90% under regular parameters.","Optimal parameters are mined ensuring probabilities of HOF and PP are both less than 0.1%."],"url":"http://arxiv.org/abs/2403.07323v1","category":"eess.SP"}
{"created":"2024-03-12 05:15:42","title":"A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models","abstract":"Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This makes teachers accept the rationale behind the prediction results and utilize them to design teaching activities and tailored learning strategies for students. However, the inherent black-box nature of deep learning techniques often poses a hurdle for teachers to fully embrace the model's prediction results. To address these challenges, we propose a Question-centric Multi-experts Contrastive Learning framework for KT called Q-MCKT.","sentences":["Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes.","Deep neural networks (DNNs) have shown great potential in solving the KT problem.","However, there still exist some important challenges when applying deep learning techniques to model the KT process.","The first challenge lies in taking the individual information of the question into modeling.","This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly.","The second challenge lies in interpreting the prediction results from existing deep learning-based KT models.","In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable.","This makes teachers accept the rationale behind the prediction results and utilize them to design teaching activities and tailored learning strategies for students.","However, the inherent black-box nature of deep learning techniques often poses a hurdle for teachers to fully embrace the model's prediction results.","To address these challenges, we propose a Question-centric Multi-experts Contrastive Learning framework for KT called Q-MCKT."],"url":"http://arxiv.org/abs/2403.07322v1","category":"cs.CY"}
{"created":"2024-03-12 05:15:21","title":"GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method","abstract":"As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines.","sentences":["As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance.","In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT.","The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses.","We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality.","To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses.","This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet.","Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines."],"url":"http://arxiv.org/abs/2403.07321v1","category":"cs.CL"}
{"created":"2024-03-12 05:00:38","title":"Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement","abstract":"Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss encourages feature correlation with AUs while discouraging correlation with subject identities for improved generalization. We train BeCoME-Net for unilateral and bilateral AU detection and compare with state-of-the-art approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty healthy adult volunteers complete expression recognition and mimicry tasks in an online feasibility study while webcam-based eye-tracking and video are collected. We test validity of multiple constructs, including face preference during recognition and AUs during mimicry.","sentences":["Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more.","However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels.","Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs.","We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert.","To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection.","The beta-guided correlation loss encourages feature correlation with AUs while discouraging correlation with subject identities for improved generalization.","We train BeCoME-Net for unilateral and bilateral AU detection and compare with state-of-the-art approaches.","To assess construct validity of CADyFACE and BeCoME-Net, twenty healthy adult volunteers complete expression recognition and mimicry tasks in an online feasibility study while webcam-based eye-tracking and video are collected.","We test validity of multiple constructs, including face preference during recognition and AUs during mimicry."],"url":"http://arxiv.org/abs/2403.07314v1","category":"cs.HC"}
{"created":"2024-03-12 04:36:41","title":"Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer","abstract":"Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making\" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7\\% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respectively. Additionally, ablation studies underscore the critical role of the transformer-based decision maker and the integration of a mortality classifier in enhancing overall survival rates. In summary, our proposed approach presents a promising avenue for enhancing sepsis treatment outcomes, contributing to improved patient care and reduced healthcare costs.","sentences":["Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications.","Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%.","This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making\" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics.","A mortality classifier with 96.7\\% accuracy guides treatment decisions towards positive outcomes.","The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respectively.","Additionally, ablation studies underscore the critical role of the transformer-based decision maker and the integration of a mortality classifier in enhancing overall survival rates.","In summary, our proposed approach presents a promising avenue for enhancing sepsis treatment outcomes, contributing to improved patient care and reduced healthcare costs."],"url":"http://arxiv.org/abs/2403.07309v1","category":"cs.LG"}
{"created":"2024-03-12 04:29:43","title":"Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees","abstract":"Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verification failure to find a valid barrier function with finite-step termination guarantees: if there exist valid barrier functions, the fine-tuning algorithm is guaranteed to find one in a finite number of iterations. We demonstrate that our fine-tuning method can significantly boost the performance of the verification-aided learning framework on examples of different scales and using various neural network verifiers.","sentences":["Barrier functions are a general framework for establishing a safety guarantee for a system.","However, there is no general method for finding these functions.","To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework.","Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice.","In this paper, we propose a holistic approach to address these drawbacks.","With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verification failure to find a valid barrier function with finite-step termination guarantees: if there exist valid barrier functions, the fine-tuning algorithm is guaranteed to find one in a finite number of iterations.","We demonstrate that our fine-tuning method can significantly boost the performance of the verification-aided learning framework on examples of different scales and using various neural network verifiers."],"url":"http://arxiv.org/abs/2403.07308v1","category":"cs.LG"}
{"created":"2024-03-12 03:57:57","title":"Optical detection of bacterial cells on stainless-steel surface with a low-magnification light microscope","abstract":"A Rapid and cost-effective method for detecting bacterial cells on surfaces is critical to protect public health from various aspects, including food safety, clinical hygiene, and pharmacy quality. Herein, we first established an optical detection method based on a gold chip coating with 3-mercaptophenylboronic acid (3-MPBA) to capture bacterial cells, which allows for the detection and quantification of bacterial cells with a standard light microscope under low-magnification (10 fold) objective lens. Then, integrating the developed optical detection method with swab sampling to achieve to detect bacterial cells loading on stainless-steel surfaces. Using Salmonella enterica (SE1045) and Escherichia coli as model bacterial cells, we achieved a capture efficiency of up to 76.0 % for SE1045 cells and 81.1 % for E. coli cells at Log 3 CFU/mL upon the optimized conditions. Our assay showed good linear relationship between the concentrations of bacterial cells with the cell counting in images with the limit of detection (LOD) of Log 3 CFU/mL for both SE1045 and E. coli cells. A further increase in sensitivity in detecting E. coli cells was achieved through a heat treatment, enabling the LOD to be pushed as low as Log 2 CFU/mL. Furthermore, successful application was observed in assessing bacterial contamination on stainless-steel surface following integrating with swab collection, achieving a recovery rate of approximately 70 % suggests future prospects for evaluating the cleanliness of surfaces. The entire process was completed within around 2 hours, with a cost of merely 2 dollars per sample. Given a standard light microscope cost around 250 dollars, our developed method has shown great potential in practical industrial applications for bacterial contamination control on surfaces in low-resource settings.","sentences":["A Rapid and cost-effective method for detecting bacterial cells on surfaces is critical to protect public health from various aspects, including food safety, clinical hygiene, and pharmacy quality.","Herein, we first established an optical detection method based on a gold chip coating with 3-mercaptophenylboronic acid (3-MPBA) to capture bacterial cells, which allows for the detection and quantification of bacterial cells with a standard light microscope under low-magnification (10 fold) objective lens.","Then, integrating the developed optical detection method with swab sampling to achieve to detect bacterial cells loading on stainless-steel surfaces.","Using Salmonella enterica (SE1045) and Escherichia coli as model bacterial cells, we achieved a capture efficiency of up to 76.0 % for SE1045 cells and 81.1 % for E. coli cells at Log 3 CFU/mL upon the optimized conditions.","Our assay showed good linear relationship between the concentrations of bacterial cells with the cell counting in images with the limit of detection (LOD) of Log 3 CFU/mL for both SE1045 and E. coli cells.","A further increase in sensitivity in detecting E. coli cells was achieved through a heat treatment, enabling the LOD to be pushed as low as Log 2 CFU/mL. Furthermore, successful application was observed in assessing bacterial contamination on stainless-steel surface following integrating with swab collection, achieving a recovery rate of approximately 70 % suggests future prospects for evaluating the cleanliness of surfaces.","The entire process was completed within around 2 hours, with a cost of merely 2 dollars per sample.","Given a standard light microscope cost around 250 dollars, our developed method has shown great potential in practical industrial applications for bacterial contamination control on surfaces in low-resource settings."],"url":"http://arxiv.org/abs/2403.07297v1","category":"q-bio.QM"}
{"created":"2024-03-12 03:54:25","title":"Graph Data Condensation via Self-expressive Graph Structure Reconstruction","abstract":"With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \\textbf{G}raph Data \\textbf{C}ondensation via \\textbf{S}elf-expressive Graph Structure \\textbf{R}econstruction (\\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0","sentences":["With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase.","It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN.","However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator.","They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset.","To address these issues, we introduce a novel framework named \\textbf{G}raph Data \\textbf{C}ondensation via \\textbf{S}elf-expressive Graph Structure \\textbf{R}econstruction (\\textbf{GCSR}).","Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure.","Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets.","Our code is available at https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0"],"url":"http://arxiv.org/abs/2403.07294v1","category":"cs.LG"}
{"created":"2024-03-12 03:50:57","title":"Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure","abstract":"In real-world applications, image degeneration caused by adverse weather is always complex and changes with different weather conditions from days and seasons. Systems in real-world environments constantly encounter adverse weather conditions that are not previously observed. Therefore, it practically requires adverse weather removal models to continually learn from incrementally collected data reflecting various degeneration types. Existing adverse weather removal approaches, for either single or multiple adverse weathers, are mainly designed for a static learning paradigm, which assumes that the data of all types of degenerations to handle can be finely collected at one time before a single-phase learning process. They thus cannot directly handle the incremental learning requirements. To address this issue, we made the earliest effort to investigate the continual all-in-one adverse weather removal task, in a setting closer to real-world applications. Specifically, we develop a novel continual learning framework with effective knowledge replay (KR) on a unified network structure. Equipped with a principal component projection and an effective knowledge distillation mechanism, the proposed KR techniques are tailored for the all-in-one weather removal task. It considers the characteristics of the image restoration task with multiple degenerations in continual learning, and the knowledge for different degenerations can be shared and accumulated in the unified network structure. Extensive experimental results demonstrate the effectiveness of the proposed method to deal with this challenging task, which performs competitively to existing dedicated or joint training image restoration methods. Our code is available at https://github.com/xiaojihh/CL_all-in-one.","sentences":["In real-world applications, image degeneration caused by adverse weather is always complex and changes with different weather conditions from days and seasons.","Systems in real-world environments constantly encounter adverse weather conditions that are not previously observed.","Therefore, it practically requires adverse weather removal models to continually learn from incrementally collected data reflecting various degeneration types.","Existing adverse weather removal approaches, for either single or multiple adverse weathers, are mainly designed for a static learning paradigm, which assumes that the data of all types of degenerations to handle can be finely collected at one time before a single-phase learning process.","They thus cannot directly handle the incremental learning requirements.","To address this issue, we made the earliest effort to investigate the continual all-in-one adverse weather removal task, in a setting closer to real-world applications.","Specifically, we develop a novel continual learning framework with effective knowledge replay (KR) on a unified network structure.","Equipped with a principal component projection and an effective knowledge distillation mechanism, the proposed KR techniques are tailored for the all-in-one weather removal task.","It considers the characteristics of the image restoration task with multiple degenerations in continual learning, and the knowledge for different degenerations can be shared and accumulated in the unified network structure.","Extensive experimental results demonstrate the effectiveness of the proposed method to deal with this challenging task, which performs competitively to existing dedicated or joint training image restoration methods.","Our code is available at https://github.com/xiaojihh/CL_all-in-one."],"url":"http://arxiv.org/abs/2403.07292v1","category":"cs.CV"}
{"created":"2024-03-12 03:35:17","title":"MENTOR: Multilingual tExt detectioN TOward leaRning by analogy","abstract":"Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: \"We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training\". To this end, we propose \"MENTOR\", the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection.","sentences":["Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task.","For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings.","Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages.","However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable.","Even worse, such a routine would repeat whenever a novel language appears.","This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: \"We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training\".","To this end, we propose \"MENTOR\", the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection."],"url":"http://arxiv.org/abs/2403.07286v1","category":"cs.CV"}
{"created":"2024-03-12 03:17:59","title":"A Survey of Explainable Knowledge Tracing","abstract":"With the long term accumulation of high quality educational data, artificial intelligence has shown excellent performance in knowledge tracing. However, due to the lack of interpretability and transparency of some algorithms, this approach will result in reduced stakeholder trust and a decreased acceptance of intelligent decisions. Therefore, algorithms need to achieve high accuracy, and users need to understand the internal operating mechanism and provide reliable explanations for decisions. This paper thoroughly analyzes the interpretability of KT algorithms. First, the concepts and common methods of explainable artificial intelligence and knowledge tracing are introduced. Next, explainable knowledge tracing models are classified into two categories: transparent models and black box models. Then, the interpretable methods used are reviewed from three stages: ante hoc interpretable methods, post hoc interpretable methods, and other dimensions. It is worth noting that current evaluation methods for explainable knowledge tracing are lacking. Hence, contrast and deletion experiments are conducted to explain the prediction results of the deep knowledge tracing model on the ASSISTment2009 by using three XAI methods. Moreover, this paper offers some insights into evaluation methods from the perspective of educational stakeholders. This paper provides a detailed and comprehensive review of the research on explainable knowledge tracing, aiming to offer some basis and inspiration for researchers interested in the interpretability of knowledge tracing.","sentences":["With the long term accumulation of high quality educational data, artificial intelligence has shown excellent performance in knowledge tracing.","However, due to the lack of interpretability and transparency of some algorithms, this approach will result in reduced stakeholder trust and a decreased acceptance of intelligent decisions.","Therefore, algorithms need to achieve high accuracy, and users need to understand the internal operating mechanism and provide reliable explanations for decisions.","This paper thoroughly analyzes the interpretability of KT algorithms.","First, the concepts and common methods of explainable artificial intelligence and knowledge tracing are introduced.","Next, explainable knowledge tracing models are classified into two categories: transparent models and black box models.","Then, the interpretable methods used are reviewed from three stages: ante hoc interpretable methods, post hoc interpretable methods, and other dimensions.","It is worth noting that current evaluation methods for explainable knowledge tracing are lacking.","Hence, contrast and deletion experiments are conducted to explain the prediction results of the deep knowledge tracing model on the ASSISTment2009 by using three XAI methods.","Moreover, this paper offers some insights into evaluation methods from the perspective of educational stakeholders.","This paper provides a detailed and comprehensive review of the research on explainable knowledge tracing, aiming to offer some basis and inspiration for researchers interested in the interpretability of knowledge tracing."],"url":"http://arxiv.org/abs/2403.07279v1","category":"cs.CL"}
{"created":"2024-03-12 03:15:08","title":"A Bayesian Approach to OOD Robustness in Image Classification","abstract":"An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement. This approach, termed Unsupervised Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present. UGT is evaluated on different OOD benchmarks including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset).","sentences":["An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains.","We address this problem in the scenario where we have access to images from the target domains but no annotations.","Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification.","Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data.","We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision.","We obverse that some vMF kernels are similar between different domains, while others are not.","This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement.","This approach, termed Unsupervised Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present.","UGT is evaluated on different OOD benchmarks including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C","[9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset)."],"url":"http://arxiv.org/abs/2403.07277v1","category":"cs.CV"}
{"created":"2024-03-12 03:12:47","title":"Achievable Rate Analysis and Optimization of Double-RIS Assisted Spatially Correlated MIMO with Statistical CSI","abstract":"Reconfigurable intelligent surface (RIS) is a novel meta-material which can form a smart radio environment by dynamically altering reflection directions of the impinging electromagnetic waves. In the prior literature, the inter-RIS links which also contribute to the performance of the whole system are usually neglected when multiple RISs are deployed. In this paper we investigate a general double-RIS assisted multiple-input multiple-output (MIMO) wireless communication system under spatially correlated non line-of-sight propagation channels, where the cooperation of the double RISs is also considered. The design objective is to maximize the achievable ergodic rate based on full statistical channel state information (CSI). Specifically, we firstly present a closed-form asymptotic expression for the achievable ergodic rate by utilizing replica method from statistical physics. Then a full statistical CSI-enabled optimal design is proposed which avoids high pilot training overhead compared to instantaneous CSI-enabled design. To further reduce the signal processing overhead and lower the complexity for practical realization, a common-phase scheme is proposed to design the double RISs. Simulation results show that the derived asymptotic ergodic rate is quite accurate even for small-sized antenna arrays. And the proposed optimization algorithm can achieve substantial gain at the expense of a low overhead and complexity. Furthermore, the cooperative double-RIS assisted MIMO framework is proven to achieve superior ergodic rate performance and high communication reliability under harsh propagation environment.","sentences":["Reconfigurable intelligent surface (RIS) is a novel meta-material which can form a smart radio environment by dynamically altering reflection directions of the impinging electromagnetic waves.","In the prior literature, the inter-RIS links which also contribute to the performance of the whole system are usually neglected when multiple RISs are deployed.","In this paper we investigate a general double-RIS assisted multiple-input multiple-output (MIMO) wireless communication system under spatially correlated non line-of-sight propagation channels, where the cooperation of the double RISs is also considered.","The design objective is to maximize the achievable ergodic rate based on full statistical channel state information (CSI).","Specifically, we firstly present a closed-form asymptotic expression for the achievable ergodic rate by utilizing replica method from statistical physics.","Then a full statistical CSI-enabled optimal design is proposed which avoids high pilot training overhead compared to instantaneous CSI-enabled design.","To further reduce the signal processing overhead and lower the complexity for practical realization, a common-phase scheme is proposed to design the double RISs.","Simulation results show that the derived asymptotic ergodic rate is quite accurate even for small-sized antenna arrays.","And the proposed optimization algorithm can achieve substantial gain at the expense of a low overhead and complexity.","Furthermore, the cooperative double-RIS assisted MIMO framework is proven to achieve superior ergodic rate performance and high communication reliability under harsh propagation environment."],"url":"http://arxiv.org/abs/2403.07274v1","category":"cs.IT"}
{"created":"2024-03-12 03:00:15","title":"Anderson acceleration for iteratively reweighted $\\ell_1$ algorithm","abstract":"Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov acceleration-based algorithms. Furthermore, to ensure global convergence, we introduce a globally convergent Anderson accelerated IRL1 algorithm by incorporating a classical nonmonotone line search condition. Experimental results indicate that our algorithm outperforms existing Nesterov acceleration-based algorithms.","sentences":["Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization.","The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest.","Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges.","Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms.","Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate.","We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario.","Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov acceleration-based algorithms.","Furthermore, to ensure global convergence, we introduce a globally convergent Anderson accelerated IRL1 algorithm by incorporating a classical nonmonotone line search condition.","Experimental results indicate that our algorithm outperforms existing Nesterov acceleration-based algorithms."],"url":"http://arxiv.org/abs/2403.07271v1","category":"math.OC"}
{"created":"2024-03-12 02:56:12","title":"MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight","abstract":"We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation.","sentences":["We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers.","In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV.","Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles.","This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives.","To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments.","These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average.","To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation."],"url":"http://arxiv.org/abs/2403.07269v1","category":"cs.RO"}
{"created":"2024-03-12 02:43:41","title":"Advantage-Aware Policy Optimization for Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables. Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values. Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL benchmark demonstrate that A2PO yields results superior to state-of-the-art counterparts. Our code will be made publicly available.","sentences":["Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem.","However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space.","To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples.","In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets.","Specifically, A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables.","Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values.","Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL benchmark demonstrate that A2PO yields results superior to state-of-the-art counterparts.","Our code will be made publicly available."],"url":"http://arxiv.org/abs/2403.07262v1","category":"cs.LG"}
{"created":"2024-03-12 02:38:36","title":"Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation","abstract":"Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representations from previous OMRL methods tend to correlate spuriously with the behavior policy instead of reflecting the essential characteristics of the task, resulting in unfavorable out-of-distribution generalization. To alleviate this issue, we introduce a novel algorithm to disentangle the impact of behavior policy from task representation learning through a process called adversarial data augmentation. Specifically, the objective of adversarial data augmentation is not merely to generate data analogous to offline data distribution; instead, it aims to create adversarial examples designed to confound learned task representations and lead to incorrect task identification. Our experiments show that learning from such adversarial samples significantly enhances the robustness and effectiveness of the task identification process and realizes satisfactory out-of-distribution generalization.","sentences":["Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset.","For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy.","A major approach to train task representations is to adopt contrastive learning using multi-task offline data.","The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks.","Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings.","Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies.","We observed that learned task representations from previous OMRL methods tend to correlate spuriously with the behavior policy instead of reflecting the essential characteristics of the task, resulting in unfavorable out-of-distribution generalization.","To alleviate this issue, we introduce a novel algorithm to disentangle the impact of behavior policy from task representation learning through a process called adversarial data augmentation.","Specifically, the objective of adversarial data augmentation is not merely to generate data analogous to offline data distribution; instead, it aims to create adversarial examples designed to confound learned task representations and lead to incorrect task identification.","Our experiments show that learning from such adversarial samples significantly enhances the robustness and effectiveness of the task identification process and realizes satisfactory out-of-distribution generalization."],"url":"http://arxiv.org/abs/2403.07261v1","category":"cs.LG"}
{"created":"2024-03-12 02:24:37","title":"Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication","abstract":"In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously performs AD and DD. Through joint loss functions and interference cancellation modules, our approach supports end-to-end training, contributing to enhanced performances of AD, CE, and DD for both coherent and non-coherent schemes. Simulation results demonstrate the superiority of our approach over traditional techniques, exhibiting enhanced performances of AD, CE, and DD while maintaining lower computational complexity.","sentences":["In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems.","Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems.","Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes.","The first framework performs joint AD and CE using received pilot signals in the coherent scheme.","Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme.","The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously performs AD and DD.","Through joint loss functions and interference cancellation modules, our approach supports end-to-end training, contributing to enhanced performances of AD, CE, and DD for both coherent and non-coherent schemes.","Simulation results demonstrate the superiority of our approach over traditional techniques, exhibiting enhanced performances of AD, CE, and DD while maintaining lower computational complexity."],"url":"http://arxiv.org/abs/2403.07255v1","category":"eess.SP"}
{"created":"2024-03-12 01:20:34","title":"Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies","abstract":"In this study we investigated the impact of image segmentation methods on the results of stress computation in the wall of abdominal aortic aneurysms (AAAs). We compared wall stress distributions and magnitudes calculated from geometry models obtained from classical semi-automated segmentation versus automated neural network-based segmentation. Ten different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking, depending on the quality of an image, between 15 and 40 minutes of human effort per patient. The same images were automatically segmented using PRAEVAorta 2, commercial software by NUREA (https://www.nurea-soft.com/), developed based on artificial intelligence (AI) algorithms, requiring only 1-2 minutes of computer time per patient. Aneurysm wall stress calculations performed using the BioPARR software (https://bioparr.mech.uwa.edu.au/) revealed that, compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values. This difference is due to consistently larger lumen surface areas in automatically segmented models as compared to classical semi-automated segmentations, resulting in greater total pressure load on the wall. Our findings are a steppingstone toward a fully automated pipeline for biomechanical analysis of AAAs, starting with CT scans and concluding with wall stress assessment, while at the same time highlighting the critical importance of the repeatable and accurate segmentation of the lumen, the difficult problem often underestimated by the literature.","sentences":["In this study we investigated the impact of image segmentation methods on the results of stress computation in the wall of abdominal aortic aneurysms (AAAs).","We compared wall stress distributions and magnitudes calculated from geometry models obtained from classical semi-automated segmentation versus automated neural network-based segmentation.","Ten different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking, depending on the quality of an image, between 15 and 40 minutes of human effort per patient.","The same images were automatically segmented using PRAEVAorta 2, commercial software by NUREA (https://www.nurea-soft.com/), developed based on artificial intelligence (AI) algorithms, requiring only 1-2 minutes of computer time per patient.","Aneurysm wall stress calculations performed using the BioPARR software (https://bioparr.mech.uwa.edu.au/) revealed that, compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values.","This difference is due to consistently larger lumen surface areas in automatically segmented models as compared to classical semi-automated segmentations, resulting in greater total pressure load on the wall.","Our findings are a steppingstone toward a fully automated pipeline for biomechanical analysis of AAAs, starting with CT scans and concluding with wall stress assessment, while at the same time highlighting the critical importance of the repeatable and accurate segmentation of the lumen, the difficult problem often underestimated by the literature."],"url":"http://arxiv.org/abs/2403.07238v1","category":"cs.CE"}
{"created":"2024-03-12 00:58:19","title":"Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences","abstract":"Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set, highlighting its effectiveness. More specifically, Curry-DPO achieves a score of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs with similar parameter size. Curry-DPO also achieves the highest adjusted win rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and 87.9% respectively) in our experiments, with notable gains of upto 7.5% when compared to standard DPO technique.","sentences":["Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences.","In practice, multiple responses can exist for a given prompt with varying quality relative to each other.","With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt.","Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology.","In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria.","We show detailed comparisons of our proposed approach to the standard single-pair DPO setting.","Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set, highlighting its effectiveness.","More specifically, Curry-DPO achieves a score of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs with similar parameter size.","Curry-DPO also achieves the highest adjusted win rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and 87.9% respectively) in our experiments, with notable gains of upto 7.5% when compared to standard DPO technique."],"url":"http://arxiv.org/abs/2403.07230v1","category":"cs.CL"}
{"created":"2024-03-12 00:31:57","title":"3D Uncertain Distance Field Mapping using GMM and GP","abstract":"In this study, we address the challenge of constructing continuous three-dimensional (3D) models that accurately represent uncertain surfaces, derived from noisy and incomplete LiDAR scanning data. Building upon our prior work, which utilized the Gaussian Process (GP) and Gaussian Mixture Model (GMM) for structured building models, we introduce a more generalized approach tailored for complex surfaces in urban scenes, where four-dimensional (4D) GMM Regression and GP with derivative observations are applied. A Hierarchical GMM (HGMM) is employed to optimize the number of GMM components and speed up the GMM training. With the prior map obtained from HGMM, GP inference is followed for the refinement of the final map. Our approach models the implicit surface of the geo-object and enables the inference of the regions that are not completely covered by measurements. The integration of GMM and GP yields well-calibrated uncertainty estimates alongside the surface model, enhancing both accuracy and reliability. The proposed method is evaluated on the real data collected by a mobile mapping system. Compared to the performance in mapping accuracy and uncertainty quantification of other methods such as Gaussian Process Implicit Surface map (GPIS) and log-Gaussian Process Implicit Surface map (Log-GPIS), the proposed method achieves lower RMSEs, higher log-likelihood values and fewer computational costs for the evaluated datasets.","sentences":["In this study, we address the challenge of constructing continuous three-dimensional (3D) models that accurately represent uncertain surfaces, derived from noisy and incomplete LiDAR scanning data.","Building upon our prior work, which utilized the Gaussian Process (GP) and Gaussian Mixture Model (GMM) for structured building models, we introduce a more generalized approach tailored for complex surfaces in urban scenes, where four-dimensional (4D) GMM Regression and GP with derivative observations are applied.","A Hierarchical GMM (HGMM) is employed to optimize the number of GMM components and speed up the GMM training.","With the prior map obtained from HGMM, GP inference is followed for the refinement of the final map.","Our approach models the implicit surface of the geo-object and enables the inference of the regions that are not completely covered by measurements.","The integration of GMM and GP yields well-calibrated uncertainty estimates alongside the surface model, enhancing both accuracy and reliability.","The proposed method is evaluated on the real data collected by a mobile mapping system.","Compared to the performance in mapping accuracy and uncertainty quantification of other methods such as Gaussian Process Implicit Surface map (GPIS) and log-Gaussian Process Implicit Surface map (Log-GPIS), the proposed method achieves lower RMSEs, higher log-likelihood values and fewer computational costs for the evaluated datasets."],"url":"http://arxiv.org/abs/2403.07223v1","category":"cs.RO"}
{"created":"2024-03-11 23:36:55","title":"Evaluation of Eye Tracking Signal Quality for Virtual Reality Applications: A Case Study in the Meta Quest Pro","abstract":"We present an extensive, in-depth analysis of the eye tracking capabilities of the Meta Quest Pro virtual reality headset using a dataset of eye movement recordings collected from 78 participants. In addition to presenting classical signal quality metrics--spatial accuracy, spatial precision and linearity--in ideal settings, we also study the impact of background luminance and headset slippage on device performance. We additionally present a user-centered analysis of eye tracking signal quality, where we highlight the potential differences in user experience as a function of device performance. This work contributes to a growing understanding of eye tracking signal quality in virtual reality headsets, where the performance of applications such as gaze-based interaction, foveated rendering, and social gaze are directly dependent on the quality of eye tracking signal.","sentences":["We present an extensive, in-depth analysis of the eye tracking capabilities of the Meta Quest Pro virtual reality headset using a dataset of eye movement recordings collected from 78 participants.","In addition to presenting classical signal quality metrics--spatial accuracy, spatial precision and linearity--in ideal settings, we also study the impact of background luminance and headset slippage on device performance.","We additionally present a user-centered analysis of eye tracking signal quality, where we highlight the potential differences in user experience as a function of device performance.","This work contributes to a growing understanding of eye tracking signal quality in virtual reality headsets, where the performance of applications such as gaze-based interaction, foveated rendering, and social gaze are directly dependent on the quality of eye tracking signal."],"url":"http://arxiv.org/abs/2403.07210v1","category":"cs.HC"}
{"created":"2024-03-11 22:58:11","title":"A multi-cohort study on prediction of acute brain dysfunction states using selective state space models","abstract":"Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals throughout an ICU stay and validating on two public datasets. Our research also introduces the concept of dynamically predicting critical transitions from non-ABD to ABD and between different ABD states in real time, which could be clinically more informative for the hospital staff. We compared the predictive performance of two state-of-the-art neural network models, the MAMBA selective state space model and the Longformer Transformer model. Using the MAMBA model, we achieved a mean area under the receiving operator characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour intervals. The model achieves a mean AUROC of 0.79 when predicting transitions between ABD states. Our study uses a curated dataset from the University of Florida Health Shands Hospital for internal validation and two publicly available datasets, MIMIC-IV and eICU, for external validation, demonstrating robustness across ICU stays from 203 hospitals and 140,945 patients.","sentences":["Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes.","Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset.","Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU.","Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets.","Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals throughout an ICU stay and validating on two public datasets.","Our research also introduces the concept of dynamically predicting critical transitions from non-ABD to ABD and between different ABD states in real time, which could be clinically more informative for the hospital staff.","We compared the predictive performance of two state-of-the-art neural network models, the MAMBA selective state space model and the Longformer Transformer model.","Using the MAMBA model, we achieved a mean area under the receiving operator characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour intervals.","The model achieves a mean AUROC of 0.79 when predicting transitions between ABD states.","Our study uses a curated dataset from the University of Florida Health Shands Hospital for internal validation and two publicly available datasets, MIMIC-IV and eICU, for external validation, demonstrating robustness across ICU stays from 203 hospitals and 140,945 patients."],"url":"http://arxiv.org/abs/2403.07201v1","category":"cs.LG"}
{"created":"2024-03-11 22:46:46","title":"Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions","abstract":"We introduce a novel text-to-pose video editing method, ReimaginedAct. While existing video editing tasks are limited to changes in attributes, backgrounds, and styles, our method aims to predict open-ended human action changes in video. Moreover, our method can accept not only direct instructional text prompts but also `what if' questions to predict possible action changes. ReimaginedAct comprises video understanding, reasoning, and editing modules. First, an LLM is utilized initially to obtain a plausible answer for the instruction or question, which is then used for (1) prompting Grounded-SAM to produce bounding boxes of relevant individuals and (2) retrieving a set of pose videos that we have collected for editing human actions. The retrieved pose videos and the detected individuals are then utilized to alter the poses extracted from the original video. We also employ a timestep blending module to ensure the edited video retains its original content except where necessary modifications are needed. To facilitate research in text-to-pose video editing, we introduce a new evaluation dataset, WhatifVideo-1.0. This dataset includes videos of different scenarios spanning a range of difficulty levels, along with questions and text prompts. Experimental results demonstrate that existing video editing methods struggle with human action editing, while our approach can achieve effective action editing and even imaginary editing from counterfactual questions.","sentences":["We introduce a novel text-to-pose video editing method, ReimaginedAct.","While existing video editing tasks are limited to changes in attributes, backgrounds, and styles, our method aims to predict open-ended human action changes in video.","Moreover, our method can accept not only direct instructional text prompts but also `what if' questions to predict possible action changes.","ReimaginedAct comprises video understanding, reasoning, and editing modules.","First, an LLM is utilized initially to obtain a plausible answer for the instruction or question, which is then used for (1) prompting Grounded-SAM to produce bounding boxes of relevant individuals and (2) retrieving a set of pose videos that we have collected for editing human actions.","The retrieved pose videos and the detected individuals are then utilized to alter the poses extracted from the original video.","We also employ a timestep blending module to ensure the edited video retains its original content except where necessary modifications are needed.","To facilitate research in text-to-pose video editing, we introduce a new evaluation dataset, WhatifVideo-1.0.","This dataset includes videos of different scenarios spanning a range of difficulty levels, along with questions and text prompts.","Experimental results demonstrate that existing video editing methods struggle with human action editing, while our approach can achieve effective action editing and even imaginary editing from counterfactual questions."],"url":"http://arxiv.org/abs/2403.07198v1","category":"cs.CV"}
{"created":"2024-03-11 22:27:16","title":"CuentosIE: can a chatbot about \"tales with a message\" help to teach emotional intelligence?","abstract":"In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of \"tales with a message\" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.","sentences":["In this article, we present CuentosIE","(TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE.","The use of \"tales with a message\" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors.","The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development.","The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article."],"url":"http://arxiv.org/abs/2403.07193v1","category":"cs.CL"}
{"created":"2024-03-11 22:24:14","title":"$\\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model","abstract":"Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).","sentences":["Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale.","Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms.","To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers.","We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO)."],"url":"http://arxiv.org/abs/2403.07191v1","category":"cs.LG"}
{"created":"2024-03-11 22:00:39","title":"UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation","abstract":"We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions.","sentences":["We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions.","UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators.","We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning.","By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results.","UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered.","Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions."],"url":"http://arxiv.org/abs/2403.07187v1","category":"cs.LG"}
{"created":"2024-03-11 21:51:39","title":"Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews","abstract":"We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.","sentences":["We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM).","Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level.","We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023.","Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates.","The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals.","We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review.","We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices."],"url":"http://arxiv.org/abs/2403.07183v1","category":"cs.CL"}
{"created":"2024-03-11 21:50:22","title":"MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains","abstract":"The recent advances in language-based generative models have paved the way for the orchestration of multiple generators of different artefact types (text, image, audio, etc.) into one system. Presently, many open-source pre-trained models combine text with other modalities, thus enabling shared vector embeddings to be compared across different generators. Within this context we propose a novel approach to handle multimodal creative tasks using Quality Diversity evolution. Our contribution is a variation of the MAP-Elites algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored for multimodal creative tasks and leverages deep learned models that assess coherence across modalities. MEliTA decouples the artefacts' modalities and promotes cross-pollination between elites. As a test bed for this algorithm, we generate text descriptions and cover images for a hypothetical video game and assign each artefact a unique modality-specific behavioural characteristic. Results indicate that MEliTA can improve text-to-image mappings within the solution space, compared to a baseline MAP-Elites algorithm that strictly treats each image-text pair as one solution. Our approach represents a significant step forward in multimodal bottom-up orchestration and lays the groundwork for more complex systems coordinating multimodal creative agents in the future.","sentences":["The recent advances in language-based generative models have paved the way for the orchestration of multiple generators of different artefact types (text, image, audio, etc.) into one system.","Presently, many open-source pre-trained models combine text with other modalities, thus enabling shared vector embeddings to be compared across different generators.","Within this context we propose a novel approach to handle multimodal creative tasks using Quality Diversity evolution.","Our contribution is a variation of the MAP-Elites algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored for multimodal creative tasks and leverages deep learned models that assess coherence across modalities.","MEliTA decouples the artefacts' modalities and promotes cross-pollination between elites.","As a test bed for this algorithm, we generate text descriptions and cover images for a hypothetical video game and assign each artefact a unique modality-specific behavioural characteristic.","Results indicate that MEliTA can improve text-to-image mappings within the solution space, compared to a baseline MAP-Elites algorithm that strictly treats each image-text pair as one solution.","Our approach represents a significant step forward in multimodal bottom-up orchestration and lays the groundwork for more complex systems coordinating multimodal creative agents in the future."],"url":"http://arxiv.org/abs/2403.07182v1","category":"cs.NE"}
{"created":"2024-03-11 21:33:05","title":"Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing","abstract":"Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.","sentences":["Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model.","Such edits have previously been called disabling edits.","These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing.","In this paper, we make two main contributions.","Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset.","Secondly, we find that disabling edits are an artifact of the original implementation of ROME.","With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME."],"url":"http://arxiv.org/abs/2403.07175v1","category":"cs.CL"}
{"created":"2024-03-11 20:51:48","title":"Phononic bright and dark states: Investigating multi-mode light-matter interactions with a single trapped ion","abstract":"Interference underpins some of the most practical and impactful properties of both the classical and quantum worlds. In this work we experimentally investigate a new formalism to describe interference effects, based on collective states which have enhanced or suppressed coupling to a two-level system. We employ a single trapped ion, whose electronic state is coupled to two of the ion's motional modes in order to simulate a multi-mode light-matter interaction. We observe the emergence of phononic bright and dark states for both a single phonon and a superposition of coherent states and demonstrate that a view of interference which is based solely on their decomposition in the collective basis is able to intuitively describe their coupling to a single atom. This work also marks the first time that multi-mode bright and dark states have been formed with the bounded motion of a single trapped ion and we highlight the potential of the methods discussed here for use in quantum information processing.","sentences":["Interference underpins some of the most practical and impactful properties of both the classical and quantum worlds.","In this work we experimentally investigate a new formalism to describe interference effects, based on collective states which have enhanced or suppressed coupling to a two-level system.","We employ a single trapped ion, whose electronic state is coupled to two of the ion's motional modes in order to simulate a multi-mode light-matter interaction.","We observe the emergence of phononic bright and dark states for both a single phonon and a superposition of coherent states and demonstrate that a view of interference which is based solely on their decomposition in the collective basis is able to intuitively describe their coupling to a single atom.","This work also marks the first time that multi-mode bright and dark states have been formed with the bounded motion of a single trapped ion and we highlight the potential of the methods discussed here for use in quantum information processing."],"url":"http://arxiv.org/abs/2403.07154v1","category":"quant-ph"}
{"created":"2024-03-11 20:39:32","title":"Don't Forget What I did?: Assessing Client Contributions in Federated Learning","abstract":"Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client contribution under limited computational budget, we propose a scheduling procedure that considers a two-sided fairness criteria to perform expensive Shapley value computation only in a subset of training epochs. In experiments, we demonstrate a controlled trade-off between the correctness and efficiency of client contributions assessed via FLContrib. To demonstrate the benefits of history-aware client contributions, we apply FLContrib to detect dishonest clients conducting data poisoning in FL training.","sentences":["Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data.","Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training.","Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions.","In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.)","clients participate in each epoch of FL training.","By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs.","Additionally, to assess client contribution under limited computational budget, we propose a scheduling procedure that considers a two-sided fairness criteria to perform expensive Shapley value computation only in a subset of training epochs.","In experiments, we demonstrate a controlled trade-off between the correctness and efficiency of client contributions assessed via FLContrib.","To demonstrate the benefits of history-aware client contributions, we apply FLContrib to detect dishonest clients conducting data poisoning in FL training."],"url":"http://arxiv.org/abs/2403.07151v1","category":"cs.LG"}
{"created":"2024-03-11 20:05:48","title":"On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency","abstract":"Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning. Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design.","sentences":["Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning.","Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods.","However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions.","We explore this through a series of case studies focused on structures that arises in many important problems.","In several, there is no information loss and value-based methods are as statistically efficient as model based ones.","In other closely-related examples, information loss is severe and value-based methods are severely outperformed.","A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design."],"url":"http://arxiv.org/abs/2403.07136v1","category":"cs.LG"}
{"created":"2024-03-11 19:55:08","title":"Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation","abstract":"Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots' state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend.","sentences":["Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods.","These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA.","However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex.","This raises the question: can these heuristics be learned?","To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA.","Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots.","The original capsule attention network architecture is fundamentally modified by adding encoding of robots' state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn.","The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits.","During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend."],"url":"http://arxiv.org/abs/2403.07131v1","category":"cs.AI"}
{"created":"2024-03-11 18:57:45","title":"A slice classification neural network for automated classification of axial PET/CT slices from a multi-centric lymphoma dataset","abstract":"Automated slice classification is clinically relevant since it can be incorporated into medical image segmentation workflows as a preprocessing step that would flag slices with a higher probability of containing tumors, thereby directing physicians attention to the important slices. In this work, we train a ResNet-18 network to classify axial slices of lymphoma PET/CT images (collected from two institutions) depending on whether the slice intercepted a tumor (positive slice) in the 3D image or if the slice did not (negative slice). Various instances of the network were trained on 2D axial datasets created in different ways: (i) slice-level split and (ii) patient-level split; inputs of different types were used: (i) only PET slices and (ii) concatenated PET and CT slices; and different training strategies were employed: (i) center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), and various binary classification metrics. We observe and describe a performance overestimation in the case of slice-level split as compared to the patient-level split training. The model trained using patient-level split data with the network input containing only PET slices in the CAG training regime was the best performing/generalizing model on a majority of metrics. Our models were additionally more closely compared using the sensitivity metric on the positive slices from their respective test sets.","sentences":["Automated slice classification is clinically relevant since it can be incorporated into medical image segmentation workflows as a preprocessing step that would flag slices with a higher probability of containing tumors, thereby directing physicians attention to the important slices.","In this work, we train a ResNet-18 network to classify axial slices of lymphoma PET/CT images (collected from two institutions) depending on whether the slice intercepted a tumor (positive slice) in the 3D image or if the slice did not (negative slice).","Various instances of the network were trained on 2D axial datasets created in different ways: (i) slice-level split and (ii) patient-level split; inputs of different types were used: (i) only PET slices and (ii) concatenated PET and CT slices; and different training strategies were employed: (i) center-aware (CAW) and (ii) center-agnostic (CAG).","Model performances were compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), and various binary classification metrics.","We observe and describe a performance overestimation in the case of slice-level split as compared to the patient-level split training.","The model trained using patient-level split data with the network input containing only PET slices in the CAG training regime was the best performing/generalizing model on a majority of metrics.","Our models were additionally more closely compared using the sensitivity metric on the positive slices from their respective test sets."],"url":"http://arxiv.org/abs/2403.07105v1","category":"eess.IV"}
{"created":"2024-03-11 18:37:35","title":"Luminosity and Beam-Induced Background Studies for the Cool Copper Collider","abstract":"A high-energy electron-positron collider has been widely recognized by the particle physics community to be the next crucial step for detailed studies of the Higgs boson and other fundamental particles and processes. Several proposals for such colliders, either linear or circular, are currently under evaluation. Any such collider will be required to reach high lumimosities, in order to collect enough data at a reasonable time scale, while at the same time coping with high rates of background particles produced from beam-beam interactions during the collisions. In this paper, we analyze the luminosity and beam-beam interaction characteristics of the Cool Copper Collider (C$^3$) and perform a comparison with other linear collider proposals. We conclude that C$^3$ can reach the same or higher collision rates as the other proposals, without having to cope with higher beam-induced background fluxes. Thus, C$^3$ emerges as an attractive option for a future electron-positron collider, benefiting from the collective advancements in beam delivery and final focus system technologies developed by other linear collider initiatives.","sentences":["A high-energy electron-positron collider has been widely recognized by the particle physics community to be the next crucial step for detailed studies of the Higgs boson and other fundamental particles and processes.","Several proposals for such colliders, either linear or circular, are currently under evaluation.","Any such collider will be required to reach high lumimosities, in order to collect enough data at a reasonable time scale, while at the same time coping with high rates of background particles produced from beam-beam interactions during the collisions.","In this paper, we analyze the luminosity and beam-beam interaction characteristics of the Cool Copper Collider (C$^3$) and perform a comparison with other linear collider proposals.","We conclude that C$^3$ can reach the same or higher collision rates as the other proposals, without having to cope with higher beam-induced background fluxes.","Thus, C$^3$ emerges as an attractive option for a future electron-positron collider, benefiting from the collective advancements in beam delivery and final focus system technologies developed by other linear collider initiatives."],"url":"http://arxiv.org/abs/2403.07093v1","category":"physics.acc-ph"}
{"created":"2024-03-11 18:33:56","title":"Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams","abstract":"Social media platforms hold valuable insights, yet extracting essential information can be challenging. Traditional top-down approaches often struggle to capture critical signals in rapidly changing events. As global events evolve swiftly, social media narratives, including instances of disinformation, become significant sources of insights. To address the need for an inductive strategy, we explore a niche social media platform GAB and an established messaging service Telegram, to develop methodologies applicable on a broader scale. This study investigates narrative evolution on these platforms using quantitative corpus-based discourse analysis techniques. Our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise, allowing for useful and actionable insights. The paper details the technical and methodological aspects of gathering and preprocessing GAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying crucial nouns and verbs for deeper exploration. Empirically, this approach is applied to a case study of a well defined event that had global impact: the 2023 Wagner mutiny. The main findings are: (1) the time line can be deconstructed to provide useful data features allowing for improved interpretation; (2) a methodology is applied which provides a basis for generalization. The key contribution is an approach, that in some cases, provides the ability to capture the dynamic narrative shifts over time with elevated confidence. The approach can augment near-real-time assessment of key social movements, allowing for informed governance choices. This research is important because it lays out a useful methodology for time series relevant info-culling, which can enable proactive modes for positive social engagement.","sentences":["Social media platforms hold valuable insights, yet extracting essential information can be challenging.","Traditional top-down approaches often struggle to capture critical signals in rapidly changing events.","As global events evolve swiftly, social media narratives, including instances of disinformation, become significant sources of insights.","To address the need for an inductive strategy, we explore a niche social media platform GAB and an established messaging service Telegram, to develop methodologies applicable on a broader scale.","This study investigates narrative evolution on these platforms using quantitative corpus-based discourse analysis techniques.","Our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise, allowing for useful and actionable insights.","The paper details the technical and methodological aspects of gathering and preprocessing GAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying crucial nouns and verbs for deeper exploration.","Empirically, this approach is applied to a case study of a well defined event that had global impact: the 2023 Wagner mutiny.","The main findings are: (1) the time line can be deconstructed to provide useful data features allowing for improved interpretation; (2) a methodology is applied which provides a basis for generalization.","The key contribution is an approach, that in some cases, provides the ability to capture the dynamic narrative shifts over time with elevated confidence.","The approach can augment near-real-time assessment of key social movements, allowing for informed governance choices.","This research is important because it lays out a useful methodology for time series relevant info-culling, which can enable proactive modes for positive social engagement."],"url":"http://arxiv.org/abs/2403.07090v1","category":"cs.IR"}
{"created":"2024-03-11 18:25:01","title":"LSTM-Based Text Generation: A Study on Historical Datasets","abstract":"This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the work of Shakespeare is 0.9125, indicating a low error rate. The training time of the model is 100, mirroring the efficiency of the Nietzsche dataset. This efficiency demonstrates the effectiveness of the model design and training methodology, especially when handling complex literary texts. This research contributes to the field of natural language processing by showcasing the versatility of LSTM networks in text generation and offering a pathway for future explorations in historical linguistics and beyond.","sentences":["This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche.","LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts.","The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time.","The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations.","The accuracy of the model is 0.9521, indicating high accuracy.","The loss of the model is 0.2518, indicating its effectiveness.","The accuracy of the model in predicting text from the work of Shakespeare is 0.9125, indicating a low error rate.","The training time of the model is 100, mirroring the efficiency of the Nietzsche dataset.","This efficiency demonstrates the effectiveness of the model design and training methodology, especially when handling complex literary texts.","This research contributes to the field of natural language processing by showcasing the versatility of LSTM networks in text generation and offering a pathway for future explorations in historical linguistics and beyond."],"url":"http://arxiv.org/abs/2403.07087v1","category":"cs.CL"}
{"created":"2024-03-11 18:11:00","title":"Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning","abstract":"We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems.","sentences":["We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning.","Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications.","Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications.","In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions.","Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects.","Furthermore, models are usually trained on standard datasets with a closed-world assumption.","Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem.","Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems.","Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance.","Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots.","Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems."],"url":"http://arxiv.org/abs/2403.07078v1","category":"cs.LG"}
{"created":"2024-03-11 18:03:02","title":"Explainable Learning with Gaussian Processes","abstract":"The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising from uncertainty in the model. We demonstrate, both through theory and experimentation, the versatility and robustness of this approach. We also show that, when applicable, the exact expressions for GPR attributions are both more accurate and less computationally expensive than the approximations currently used in practice. The source code for this project is freely available under MIT license at https://github.com/KurtButler/2024_attributions_paper.","sentences":["The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions.","Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature.","In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR).","We take a principled approach to defining attributions under model uncertainty, extending the existing literature.","We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions.","When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising from uncertainty in the model.","We demonstrate, both through theory and experimentation, the versatility and robustness of this approach.","We also show that, when applicable, the exact expressions for GPR attributions are both more accurate and less computationally expensive than the approximations currently used in practice.","The source code for this project is freely available under MIT license at https://github.com/KurtButler/2024_attributions_paper."],"url":"http://arxiv.org/abs/2403.07072v1","category":"cs.LG"}
{"created":"2024-03-11 18:00:00","title":"The \u010fAlembert solution in hyperboloidal foliations","abstract":"We explicitly construct the analogue of the \\v{d}Alembert solution to the 1+1 wave equation in an hyperboloidal setting. This hyperboloidal \\v{d}Alembert solution is used, in turn, to gain intuition into the behaviour of solutions to the wave equation in a hyperboloidal foliation and to explain some apparently anomalous behaviour observed in numerically constructed solutions discussed in the literature.","sentences":["We explicitly construct the analogue of the \\v{d}Alembert solution to the 1+1 wave equation in an hyperboloidal setting.","This hyperboloidal \\v{d}Alembert solution is used, in turn, to gain intuition into the behaviour of solutions to the wave equation in a hyperboloidal foliation and to explain some apparently anomalous behaviour observed in numerically constructed solutions discussed in the literature."],"url":"http://arxiv.org/abs/2403.07045v1","category":"gr-qc"}
{"created":"2024-03-11 18:00:00","title":"Cosmological Amplitudes in Power-Law FRW Universe","abstract":"The correlators of large-scale fluctuations belong to the most important observables in modern cosmology. Recently, there have been considerable efforts in analytically understanding the cosmological correlators and the related wavefunction coefficients, which we collectively call cosmological amplitudes. In this work, we provide a set of simple rules to directly write down analytical answers for arbitrary tree-level amplitudes of conformal scalars with time-dependent interactions in power-law FRW universe. With the recently proposed family-tree decomposition method, we identify an over-complete set of multivariate hypergeometric functions, called family trees, to which all tree-level conformal scalar amplitudes can be easily reduced. Our method yields series expansions and monodromies of family trees in various kinematic limits, together with a large number of functional identities. The family trees are in a sense generalizations of polylogarithms and do reduce to polylogarithmic expressions for the cubic coupling in inflationary limit. We further show that all family trees can be decomposed into linear chains by taking shuffle products of all subfamilies, with which we find simple connection between bulk time integrals and boundary energy integrals.","sentences":["The correlators of large-scale fluctuations belong to the most important observables in modern cosmology.","Recently, there have been considerable efforts in analytically understanding the cosmological correlators and the related wavefunction coefficients, which we collectively call cosmological amplitudes.","In this work, we provide a set of simple rules to directly write down analytical answers for arbitrary tree-level amplitudes of conformal scalars with time-dependent interactions in power-law FRW universe.","With the recently proposed family-tree decomposition method, we identify an over-complete set of multivariate hypergeometric functions, called family trees, to which all tree-level conformal scalar amplitudes can be easily reduced.","Our method yields series expansions and monodromies of family trees in various kinematic limits, together with a large number of functional identities.","The family trees are in a sense generalizations of polylogarithms and do reduce to polylogarithmic expressions for the cubic coupling in inflationary limit.","We further show that all family trees can be decomposed into linear chains by taking shuffle products of all subfamilies, with which we find simple connection between bulk time integrals and boundary energy integrals."],"url":"http://arxiv.org/abs/2403.07050v1","category":"hep-th"}
{"created":"2024-03-11 17:57:41","title":"Memory-based Adapters for Online 3D Scene Perception","abstract":"In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. \\href{https://xuxw98.github.io/Online3D/}{Project page}.","sentences":["In this paper, we propose a new framework for online 3D scene perception.","Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos.","To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information.","To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability.","Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features.","Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame.","We further propose 3D-to-2D adapter to enhance image features with strong global context.","Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks.","Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs.","\\href{https://xuxw98.github.io/Online3D/}{Project page}."],"url":"http://arxiv.org/abs/2403.06974v1","category":"cs.CV"}
{"created":"2024-03-11 17:47:47","title":"Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena","abstract":"Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG). For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained. We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb. To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text. We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale. We then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of the CMC using the newly collected corpus. We find that all models struggle with understanding the motion component that the CMC adds to a sentence.","sentences":["Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG).","For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained.","We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb.","To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text.","We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale.","We then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of the CMC using the newly collected corpus.","We find that all models struggle with understanding the motion component that the CMC adds to a sentence."],"url":"http://arxiv.org/abs/2403.06965v1","category":"cs.CL"}
{"created":"2024-03-11 17:47:30","title":"The pitfalls of next-token prediction","abstract":"Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary evidence that this failure can be resolved when training to predict multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures","sentences":["Can a mere next-token predictor faithfully model human intelligence?","We crystallize this intuitive concern, which is fragmented in the literature.","As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly.","The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor.","This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place.","We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.","We provide preliminary evidence that this failure can be resolved when training to predict multiple tokens in advance.","We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm.","We make our code available under https://github.com/gregorbachmann/Next-Token-Failures"],"url":"http://arxiv.org/abs/2403.06963v1","category":"cs.CL"}
{"created":"2024-03-11 17:46:21","title":"Explainable Transformer Prototypes for Medical Diagnoses","abstract":"Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions. The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities. Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods. However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions. Our research endeavors to innovate a unique attention block that underscores the correlation between 'regions' rather than 'pixels'. To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights. A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset. Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics. The code is available at www.github.com/NUBagcilab/r2r_proto.","sentences":["Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions.","The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities.","Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods.","However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions.","Our research endeavors to innovate a unique attention block that underscores the correlation between 'regions' rather than 'pixels'.","To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights.","A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset.","Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics.","The code is available at www.github.com/NUBagcilab/r2r_proto."],"url":"http://arxiv.org/abs/2403.06961v1","category":"cs.CV"}
{"created":"2024-03-11 17:35:33","title":"SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data","abstract":"Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.","sentences":["Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions.","However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects.","In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging.","First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts.","Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging.","Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets.","We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation.","Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data.","Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models."],"url":"http://arxiv.org/abs/2403.06952v1","category":"cs.CV"}
{"created":"2024-03-11 17:28:46","title":"Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI","abstract":"Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation sequence from CPOW time series, (ii) compresses the CPOW streaming data with innovation autoencoder and subband coding, and (iii) detects unknown faults and novel trends via nonparametric sequential hypothesis testing.   Conclusion: This work argues that conventional monitoring using SCADA and phasor measurement unit (PMU) technologies is ill-suited for a future grid with deep penetration of inverter-based renewable generations and distributed energy resources. A monitoring system based on CPOW data streaming and AI data analytics should be the basic building blocks for situational awareness of a highly dynamic future grid.","sentences":["Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference.","Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables.","This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation sequence from CPOW time series, (ii) compresses the CPOW streaming data with innovation autoencoder and subband coding, and (iii) detects unknown faults and novel trends via nonparametric sequential hypothesis testing.   ","Conclusion: This work argues that conventional monitoring using SCADA and phasor measurement unit (PMU) technologies is ill-suited for a future grid with deep penetration of inverter-based renewable generations and distributed energy resources.","A monitoring system based on CPOW data streaming and AI data analytics should be the basic building blocks for situational awareness of a highly dynamic future grid."],"url":"http://arxiv.org/abs/2403.06942v1","category":"eess.SY"}
{"created":"2024-03-11 17:25:01","title":"TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives","abstract":"As the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the CPU and memory/storage. While processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory. This has led to a recent push for in-storage computation, where processing is performed inside the storage device.   We propose TCAM-SSD, a new framework for search-based computation inside the NAND flash memory arrays of a conventional solid-state drive (SSD), which requires lightweight modifications to only the array periphery and firmware. TCAM-SSD introduces a search manager and link table, which can logically partition the NAND flash memory's contents into search-enabled regions and standard storage regions. Together, these light firmware changes enable TCAM-SSD to seamlessly handle block I/O operations, in addition to new search operations, thereby reducing end-to-end execution time and total data movement. We provide an NVMe-compatible interface that provides programmers with the ability to dynamically allocate data on and make use of TCAM-SSD, allowing the system to be leveraged by a wide variety of applications. We evaluate three example use cases of TCAM-SSD to demonstrate its benefits. For transactional databases, TCAM-SSD can mitigate the performance penalties for applications with large datasets, achieving a 60.9% speedup over a conventional system that retrieves data from the SSD and computes using the CPU. For database analytics, TCAM-SSD provides an average speedup of 17.7x over a conventional system for a collection of analytical queries. For graph analytics, we combine TCAM-SSD's associative search with a sparse data structure, speeding up graph computing for larger-than-memory datasets by 14.5%.","sentences":["As the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the CPU and memory/storage.","While processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory.","This has led to a recent push for in-storage computation, where processing is performed inside the storage device.   ","We propose TCAM-SSD, a new framework for search-based computation inside the NAND flash memory arrays of a conventional solid-state drive (SSD), which requires lightweight modifications to only the array periphery and firmware.","TCAM-SSD introduces a search manager and link table, which can logically partition the NAND flash memory's contents into search-enabled regions and standard storage regions.","Together, these light firmware changes enable TCAM-SSD to seamlessly handle block I/O operations, in addition to new search operations, thereby reducing end-to-end execution time and total data movement.","We provide an NVMe-compatible interface that provides programmers with the ability to dynamically allocate data on and make use of TCAM-SSD, allowing the system to be leveraged by a wide variety of applications.","We evaluate three example use cases of TCAM-SSD to demonstrate its benefits.","For transactional databases, TCAM-SSD can mitigate the performance penalties for applications with large datasets, achieving a 60.9% speedup over a conventional system that retrieves data from the SSD and computes using the CPU.","For database analytics, TCAM-SSD provides an average speedup of 17.7x over a conventional system for a collection of analytical queries.","For graph analytics, we combine TCAM-SSD's associative search with a sparse data structure, speeding up graph computing for larger-than-memory datasets by 14.5%."],"url":"http://arxiv.org/abs/2403.06938v1","category":"cs.AR"}
{"created":"2024-03-11 17:21:39","title":"Counterfactual Reasoning with Knowledge Graph Embeddings","abstract":"Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules. In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention. In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning.","sentences":["Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories.","In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR.","We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules.","We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained.","We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark.","Our results indicate that KGEs learn patterns in the graph without explicit training.","We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns.","An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules.","In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention.","In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning."],"url":"http://arxiv.org/abs/2403.06936v1","category":"cs.LG"}
{"created":"2024-03-11 17:12:09","title":"Simplicity Bias of Transformers to Learn Low Sensitivity Functions","abstract":"Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of transformers.","sentences":["Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive.","Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space.","In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities.","We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks.","We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of transformers."],"url":"http://arxiv.org/abs/2403.06925v1","category":"cs.LG"}
{"created":"2024-03-11 17:03:04","title":"MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning","abstract":"Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning. Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of large language models","sentences":["Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations).","Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism.","Existing solutions attempt to distill lengthy demonstrations into compact vectors.","However, they often require task-specific retraining or compromise LLM's in-context learning performance.","To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task.","We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously.","MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning.","Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess.","It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands.","This innovation promises enhanced scalability and efficiency for the practical deployment of large language models"],"url":"http://arxiv.org/abs/2403.06914v2","category":"cs.CL"}
{"created":"2024-03-11 17:01:13","title":"Responsible Artificial Intelligence: A Structured Literature Review","abstract":"Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an approach for developing a future framework centered around this concept. Our findings advocate for a human-centric approach to Responsible AI. This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust.","sentences":["Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions.","The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon.","This dichotomy highlights the urgent need for international regulation.","Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations.","Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention.","This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI.","Through a structured literature review, we elucidate the current understanding of responsible AI.","Drawing from this analysis, we propose an approach for developing a future framework centered around this concept.","Our findings advocate for a human-centric approach to Responsible AI.","This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust."],"url":"http://arxiv.org/abs/2403.06910v1","category":"cs.AI"}
{"created":"2024-03-11 16:57:20","title":"Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints","abstract":"Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost.","sentences":["Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier.","Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints.","To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF).","DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations.","We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints.","The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost."],"url":"http://arxiv.org/abs/2403.06906v1","category":"cs.LG"}
{"created":"2024-03-11 16:54:44","title":"LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration","abstract":"The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \\textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration.","sentences":["The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry.","Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery.","In this paper, we propose a novel \\textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+).","We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ.","Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration."],"url":"http://arxiv.org/abs/2403.06901v1","category":"eess.IV"}
{"created":"2024-03-11 16:54:23","title":"Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning","abstract":"Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy. FL algorithms fall into two primary categories: synchronous and asynchronous. While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy. In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness. To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies. Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs. Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates. Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem. The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios. DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx.","sentences":["Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy.","FL algorithms fall into two primary categories: synchronous and asynchronous.","While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy.","In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness.","To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies.","Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs.","Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates.","Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem.","The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios.","DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx."],"url":"http://arxiv.org/abs/2403.06900v1","category":"cs.DC"}
{"created":"2024-03-11 16:34:23","title":"Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning","abstract":"Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models.","sentences":["Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards.","Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks.","Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes.","Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates.","Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition.","Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models."],"url":"http://arxiv.org/abs/2403.06880v1","category":"cs.LG"}
{"created":"2024-03-11 16:31:48","title":"Anderson-Higgs amplitude mode in Josephson junctions","abstract":"The Anderson-Higgs mode in a superconductor corresponds to a collective and coherent oscillation of the order parameter amplitude. We propose to detect this mode in a tunnel Josephson junction between two singlet s-wave diffusive superconductors. We find a strong enhancement of the tunneling current when the junction is pumped at the equilibrium gap frequency, corresponding to the activation of the Anderson-Higgs mode. By solving the Keldysh-Usadel equations, we obtain current peaks at specific bias voltages that can serve as signatures of the Anderson-Higgs mode.","sentences":["The Anderson-Higgs mode in a superconductor corresponds to a collective and coherent oscillation of the order parameter amplitude.","We propose to detect this mode in a tunnel Josephson junction between two singlet s-wave diffusive superconductors.","We find a strong enhancement of the tunneling current when the junction is pumped at the equilibrium gap frequency, corresponding to the activation of the Anderson-Higgs mode.","By solving the Keldysh-Usadel equations, we obtain current peaks at specific bias voltages that can serve as signatures of the Anderson-Higgs mode."],"url":"http://arxiv.org/abs/2403.06878v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 16:24:08","title":"Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents","abstract":"Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts. We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc. We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods.","sentences":["Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure.","Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation.","We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction.","Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering.","Which we use in another set of transformer encoder layers to learn the inter-chunk representations.","We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts.","We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc.","We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset.","Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.06872v1","category":"cs.CL"}
{"created":"2024-03-11 16:22:41","title":"Learning with Noisy Foundation Models","abstract":"Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.","sentences":["Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning.","However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks.","This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks.","Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different.","These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications.","We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently.","We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners.","We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation.","Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning."],"url":"http://arxiv.org/abs/2403.06869v1","category":"cs.LG"}
{"created":"2024-03-11 16:18:40","title":"On the Preservation of Africa's Cultural Heritage in the Age of Artificial Intelligence","abstract":"In this paper we delve into the historical evolution of data as a fundamental element in communication and knowledge transmission. The paper traces the stages of knowledge dissemination from oral traditions to the digital era, highlighting the significance of languages and cultural diversity in this progression. It also explores the impact of digital technologies on memory, communication, and cultural preservation, emphasizing the need for promoting a culture of the digital (rather than a digital culture) in Africa and beyond. Additionally, it discusses the challenges and opportunities presented by data biases in AI development, underscoring the importance of creating diverse datasets for equitable representation. We advocate for investing in data as a crucial raw material for fostering digital literacy, economic development, and, above all, cultural preservation in the digital age.","sentences":["In this paper we delve into the historical evolution of data as a fundamental element in communication and knowledge transmission.","The paper traces the stages of knowledge dissemination from oral traditions to the digital era, highlighting the significance of languages and cultural diversity in this progression.","It also explores the impact of digital technologies on memory, communication, and cultural preservation, emphasizing the need for promoting a culture of the digital (rather than a digital culture) in Africa and beyond.","Additionally, it discusses the challenges and opportunities presented by data biases in AI development, underscoring the importance of creating diverse datasets for equitable representation.","We advocate for investing in data as a crucial raw material for fostering digital literacy, economic development, and, above all, cultural preservation in the digital age."],"url":"http://arxiv.org/abs/2403.06865v1","category":"cs.CY"}
{"created":"2024-03-11 16:13:58","title":"Quantum thermodynamics of driven-dissipative condensates","abstract":"Polariton condensates occur away from thermal equilibrium, in an open system where heat and particles are continually exchanged with reservoirs. These phenomena have been extensively analyzed in terms of kinetic equations. Based on the collection of knowledge about polariton kinetics provided by these simulations and by experimental works, we constructed a few-level model that captures the main processes involved in the buildup of a ground-state population of polaritons. This allows condensation to be understood as the output of a heat engine and exposes the thermodynamic constraints on its occurrence. The model consists of a three-level system interacting with a field and connected to a hot and a cold thermal reservoir that represent a non-resonant pump and the lattice phonons. This subsystem can drive a condensate, through polariton-polariton scattering, which produces work in the form of coherent light emission from the microcavity. We obtain a phase diagram as a function of the temperatures of the two baths and investigate the possible types of phase transition that lead to the condensate phase.","sentences":["Polariton condensates occur away from thermal equilibrium, in an open system where heat and particles are continually exchanged with reservoirs.","These phenomena have been extensively analyzed in terms of kinetic equations.","Based on the collection of knowledge about polariton kinetics provided by these simulations and by experimental works, we constructed a few-level model that captures the main processes involved in the buildup of a ground-state population of polaritons.","This allows condensation to be understood as the output of a heat engine and exposes the thermodynamic constraints on its occurrence.","The model consists of a three-level system interacting with a field and connected to a hot and a cold thermal reservoir that represent a non-resonant pump and the lattice phonons.","This subsystem can drive a condensate, through polariton-polariton scattering, which produces work in the form of coherent light emission from the microcavity.","We obtain a phase diagram as a function of the temperatures of the two baths and investigate the possible types of phase transition that lead to the condensate phase."],"url":"http://arxiv.org/abs/2403.06861v1","category":"quant-ph"}
{"created":"2024-03-11 16:04:58","title":"All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)","abstract":"This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initialization for multiple tasks. Experiments show our method's effectiveness in enhancing model performance across different graph tasks.   Beyond the original work, in this extended abstract, we further discuss the graph prompt from a bigger picture and provide some of the latest work toward this area.","sentences":["This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan.","All in one: Multi-task prompting for graph neural networks.","KDD 23)","The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP.","Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs.","This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks.","By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initialization for multiple tasks.","Experiments show our method's effectiveness in enhancing model performance across different graph tasks.   ","Beyond the original work, in this extended abstract, we further discuss the graph prompt from a bigger picture and provide some of the latest work toward this area."],"url":"http://arxiv.org/abs/2403.07040v1","category":"cs.LG"}
{"created":"2024-03-11 16:03:21","title":"Towards an educational tool for supporting neonatologists in the delivery room","abstract":"Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth. However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet. Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge. Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients.","sentences":["Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth.","However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet.","Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   ","In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge.","Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients."],"url":"http://arxiv.org/abs/2403.06843v1","category":"cs.AI"}
{"created":"2024-03-11 16:01:05","title":"RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback","abstract":"Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations.","sentences":["Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters.","Moreover, updating this knowledge incurs high training costs.","Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge.","The model can answer questions","it couldn't previously by retrieving knowledge relevant to the query.","This approach improves performance in certain scenarios for specific tasks.","However, if irrelevant texts are retrieved, it may impair model performance.","In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities.","Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations."],"url":"http://arxiv.org/abs/2403.06840v1","category":"cs.CL"}
{"created":"2024-03-11 15:59:59","title":"ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts","abstract":"Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence.   Leveraging recent advancements in large language models (LLMs), we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFIX involves both offline and online phases. First, during the offline phase, ACFIX mines a taxonomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined. Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. This patch will then undergo a validity and effectiveness check. To evaluate ACFIX, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them. This represents a significant improvement compared to the baseline GPT-4, which achieved only 52.54%.","sentences":["Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical.","While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge.","Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence.   ","Leveraging recent advancements in large language models (LLMs), we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX.","The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality.","To this end, ACFIX involves both offline and online phases.","First, during the offline phase, ACFIX mines a taxonomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined.","Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch.","This patch will then undergo a validity and effectiveness check.","To evaluate ACFIX, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them.","This represents a significant improvement compared to the baseline GPT-4, which achieved only 52.54%."],"url":"http://arxiv.org/abs/2403.06838v1","category":"cs.SE"}
{"created":"2024-03-11 15:56:17","title":"Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting","abstract":"Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis. However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions. To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images. Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details. The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module. The anatomy-pathology prompting module automatically generates descriptive prompts for high-quality medical images. To further synthesize high-quality medical images from the generated prompts, the fine-grained alignment-based synthesis module pre-defines a visual codebook for the radiology dataset and performs fine-grained alignment between the codebook and generated prompts to obtain key patches as visual clues, facilitating accurate image synthesis. We validate the superiority of our method through experiments on public chest X-ray datasets and demonstrate that our synthetic images preserve accurate semantic information, making them valuable for various medical applications.","sentences":["Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis.","However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions.","To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images.","Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details.","The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module.","The anatomy-pathology prompting module automatically generates descriptive prompts for high-quality medical images.","To further synthesize high-quality medical images from the generated prompts, the fine-grained alignment-based synthesis module pre-defines a visual codebook for the radiology dataset and performs fine-grained alignment between the codebook and generated prompts to obtain key patches as visual clues, facilitating accurate image synthesis.","We validate the superiority of our method through experiments on public chest X-ray datasets and demonstrate that our synthetic images preserve accurate semantic information, making them valuable for various medical applications."],"url":"http://arxiv.org/abs/2403.06835v1","category":"cs.CV"}
{"created":"2024-03-11 15:48:43","title":"The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework","abstract":"The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and seven for MEMA), demonstrating its robustness and versatility. Besides, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Our code and data are available at: https://github.com/zjukg/SNAG.","sentences":["The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework.","This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations.","In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA).","Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs.","By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and seven for MEMA), demonstrating its robustness and versatility.","Besides, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements.","Our code and data are available at: https://github.com/zjukg/SNAG."],"url":"http://arxiv.org/abs/2403.06832v1","category":"cs.CL"}
{"created":"2024-03-11 15:44:38","title":"NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning","abstract":"Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation. We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments. Experiments demonstrate that NeuPAN outperforms various benchmarks, in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable.","sentences":["Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance.","This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution.","Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence.","The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop.","This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation.","We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments.","Experiments demonstrate that NeuPAN outperforms various benchmarks, in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot.","We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable."],"url":"http://arxiv.org/abs/2403.06828v1","category":"cs.RO"}
{"created":"2024-03-11 15:43:14","title":"In-context Exploration-Exploitation for Reinforcement Learning","abstract":"In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.","sentences":["In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization.","However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models.","We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning.","Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference.","Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time.","Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method."],"url":"http://arxiv.org/abs/2403.06826v1","category":"cs.LG"}
{"created":"2024-03-11 15:40:36","title":"Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How","abstract":"Advances in Generative Artificial Intelligence (AI) are resulting in AI-generated media output that is (nearly) indistinguishable from human-created content. This can drastically impact users and the media sector, especially given global risks of misinformation. While the currently discussed European AI Act aims at addressing these risks through Article 52's AI transparency obligations, its interpretation and implications remain unclear. In this early work, we adopt a participatory AI approach to derive key questions based on Article 52's disclosure obligations. We ran two workshops with researchers, designers, and engineers across disciplines (N=16), where participants deconstructed Article 52's relevant clauses using the 5W1H framework. We contribute a set of 149 questions clustered into five themes and 18 sub-themes. We believe these can not only help inform future legal developments and interpretations of Article 52, but also provide a starting point for Human-Computer Interaction research to (re-)examine disclosure transparency from a human-centered AI lens.","sentences":["Advances in Generative Artificial Intelligence (AI) are resulting in AI-generated media output that is (nearly) indistinguishable from human-created content.","This can drastically impact users and the media sector, especially given global risks of misinformation.","While the currently discussed European AI Act aims at addressing these risks through Article 52's AI transparency obligations, its interpretation and implications remain unclear.","In this early work, we adopt a participatory AI approach to derive key questions based on Article 52's disclosure obligations.","We ran two workshops with researchers, designers, and engineers across disciplines (N=16), where participants deconstructed Article 52's relevant clauses using the 5W1H framework.","We contribute a set of 149 questions clustered into five themes and 18 sub-themes.","We believe these can not only help inform future legal developments and interpretations of Article 52, but also provide a starting point for Human-Computer Interaction research to (re-)examine disclosure transparency from a human-centered AI lens."],"url":"http://arxiv.org/abs/2403.06823v1","category":"cs.HC"}
{"created":"2024-03-11 15:39:52","title":"A quasilinear Keller-Segel model with saturated discontinuous advection","abstract":"We consider the singular limit of a chemotaxis model of bacterial collective motion recently introduced in arXiv:2009.11048 [math.AP]. The equation models aggregation-diffusion phenomena with advection that is discontinuous and depends sharply on the gradient of the density itself. The quasi-linearity of the problem poses major challenges in the construction of the solution and complications arise in the proof of regularity. Our method overcomes these obstacle by relying solely on entropy inequalities and the theory of monotone operators. We provide existence, uniqueness and smoothing estimates in any dimensional space.","sentences":["We consider the singular limit of a chemotaxis model of bacterial collective motion recently introduced in arXiv:2009.11048","[math.AP].","The equation models aggregation-diffusion phenomena with advection that is discontinuous and depends sharply on the gradient of the density itself.","The quasi-linearity of the problem poses major challenges in the construction of the solution and complications arise in the proof of regularity.","Our method overcomes these obstacle by relying solely on entropy inequalities and the theory of monotone operators.","We provide existence, uniqueness and smoothing estimates in any dimensional space."],"url":"http://arxiv.org/abs/2403.06820v1","category":"math.AP"}
{"created":"2024-03-11 15:37:25","title":"User Tracking and Direction Estimation Codebook Design for IRS-Assisted mmWave Communication","abstract":"Future communication systems are envisioned to employ intelligent reflecting surfaces (IRSs) and the millimeter wave (mmWave) frequency band to provide reliable high-rate services. For mobile users, the time-varying channel state information (CSI) requires adequate adjustment of the reflection pattern of the IRS. We propose a novel codebook-based user tracking (UT) algorithm for IRS-assisted mmWave communication, allowing suitable reconfiguration of the IRS unit cell phase shifts, resulting in a high reflection gain. The presented algorithm acquires the direction information of the user based on a peak likelihood-based direction estimation. Using the direction information, the user's trajectory is extrapolated to proactively update the adopted codeword and adjust the IRS phase shift configuration accordingly. Furthermore, we conduct a theoretical analysis of the direction estimation error and utilize the obtained insights to design a codebook specifically optimized for direction estimation. Our numerical results reveal a lower direction estimation error of the proposed UT algorithm when employing our designed codebook compared to codebooks from the literature. Furthermore, the average achieved signal-to-noise ratio (SNR) as well as the average effective rate of the proposed UT algorithm are analyzed. The proposed UT algorithm requires only a low overhead for direction and channel estimation and avoids outdated IRS phase shifts. Furthermore, it is shown to outperform two benchmark schemes based on direct phase shift optimization and hierarchical codebook search, respectively, via computer simulations.","sentences":["Future communication systems are envisioned to employ intelligent reflecting surfaces (IRSs) and the millimeter wave (mmWave) frequency band to provide reliable high-rate services.","For mobile users, the time-varying channel state information (CSI) requires adequate adjustment of the reflection pattern of the IRS.","We propose a novel codebook-based user tracking (UT) algorithm for IRS-assisted mmWave communication, allowing suitable reconfiguration of the IRS unit cell phase shifts, resulting in a high reflection gain.","The presented algorithm acquires the direction information of the user based on a peak likelihood-based direction estimation.","Using the direction information, the user's trajectory is extrapolated to proactively update the adopted codeword and adjust the IRS phase shift configuration accordingly.","Furthermore, we conduct a theoretical analysis of the direction estimation error and utilize the obtained insights to design a codebook specifically optimized for direction estimation.","Our numerical results reveal a lower direction estimation error of the proposed UT algorithm when employing our designed codebook compared to codebooks from the literature.","Furthermore, the average achieved signal-to-noise ratio (SNR) as well as the average effective rate of the proposed UT algorithm are analyzed.","The proposed UT algorithm requires only a low overhead for direction and channel estimation and avoids outdated IRS phase shifts.","Furthermore, it is shown to outperform two benchmark schemes based on direct phase shift optimization and hierarchical codebook search, respectively, via computer simulations."],"url":"http://arxiv.org/abs/2403.06818v1","category":"eess.SP"}
{"created":"2024-03-11 15:34:57","title":"Are Targeted Messages More Effective?","abstract":"Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to two fragments of first-order logic with counting that we call modal and guarded.   The question whether the two versions differ in their expressivity has been mostly overlooked in the GNN literature and has only been asked recently (Grohe, LICS'23). We answer this question here. It turns out that the answer is not as straightforward as one might expect. By proving that the modal and guarded fragment of first-order logic with counting have the same expressivity over labelled undirected graphs, we show that in a non-uniform setting the two GNN versions have the same expressivity. However, we also prove that in a uniform setting the second version is strictly more expressive.","sentences":["Graph neural networks (GNN) are deep learning architectures for graphs.","Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data.","It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages.","The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   ","The core GNN architecture comes in two different versions.","In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices.","In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one.","On the logical side, the two versions correspond to two fragments of first-order logic with counting that we call modal and guarded.   ","The question whether the two versions differ in their expressivity has been mostly overlooked in the GNN literature and has only been asked recently (Grohe, LICS'23).","We answer this question here.","It turns out that the answer is not as straightforward as one might expect.","By proving that the modal and guarded fragment of first-order logic with counting have the same expressivity over labelled undirected graphs, we show that in a non-uniform setting the two GNN versions have the same expressivity.","However, we also prove that in a uniform setting the second version is strictly more expressive."],"url":"http://arxiv.org/abs/2403.06817v1","category":"cs.LO"}
{"created":"2024-03-11 15:16:20","title":"Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification","abstract":"Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness. Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT. In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on external transference. Comprehensive testing on dermatology HAM10000 dataset showed that the DPAAT not only achieved better robustness improvement and generalization preservation but also significantly enhanced mean average precision and interpretability on various CNNs, indicating its great potential as a generic adversarial training method on the MIC.","sentences":["Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs).","However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness.","Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT.","In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on external transference.","Comprehensive testing on dermatology HAM10000 dataset showed that the DPAAT not only achieved better robustness improvement and generalization preservation but also significantly enhanced mean average precision and interpretability on various CNNs, indicating its great potential as a generic adversarial training method on the MIC."],"url":"http://arxiv.org/abs/2403.06798v1","category":"eess.IV"}
{"created":"2024-03-11 15:15:50","title":"Leveraging Internal Representations of Model for Magnetic Image Classification","abstract":"Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.","sentences":["Data generated by edge devices has the potential to train intelligent autonomous systems across various domains.","Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations.","This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available.","We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity.","Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results.","This methodology presents a promising avenue for training machine learning models with minimal data."],"url":"http://arxiv.org/abs/2403.06797v1","category":"cs.LG"}
{"created":"2024-03-11 15:00:56","title":"Genetic Learning for Designing Sim-to-Real Data Augmentations","abstract":"Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data. This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection. We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data. Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model.","sentences":["Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data.","This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains.","Many image augmentation techniques exist, parametrized by different settings, such as strength and probability.","This leads to a large space of different possible augmentation policies.","Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why.","This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection.","We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data.","Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model."],"url":"http://arxiv.org/abs/2403.06786v1","category":"cs.CV"}
{"created":"2024-03-11 14:37:57","title":"XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage","abstract":"Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be combined to stretch toward the effective initialization for diverse unseen tasks.","sentences":["Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks.","However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains.","Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations.","We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task.","XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis.","Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be combined to stretch toward the effective initialization for diverse unseen tasks."],"url":"http://arxiv.org/abs/2403.06768v1","category":"cs.LG"}
{"created":"2024-03-11 14:36:43","title":"Determination of the number of $\u03c8(3686)$ events taken at BESIII","abstract":"The number of $\\psi(3686)$ events collected by the BESIII detector during the 2021 run period is determined to be $(2259.3\\pm 11.1)\\times 10^6$ by counting inclusive $\\psi(3686)$ hadronic events. The uncertainty is systematic and the statistical uncertainty is negligible. Meanwhile, the numbers of $\\psi(3686)$ events collected during the 2009 and 2012 run periods are updated to be $(107.7\\pm0.6)\\times 10^6$ and $(345.4\\pm 2.6)\\times 10^6$, respectively. Both numbers are consistent with the previous measurements within one standard deviation. The total number of $\\psi(3686)$ events in the three data samples is $(2712.4\\pm14.3)\\times10^6$.","sentences":["The number of $\\psi(3686)$ events collected by the BESIII detector during the 2021 run period is determined to be $(2259.3\\pm 11.1)\\times 10^6$ by counting inclusive $\\psi(3686)$ hadronic events.","The uncertainty is systematic and the statistical uncertainty is negligible.","Meanwhile, the numbers of $\\psi(3686)$ events collected during the 2009 and 2012 run periods are updated to be $(107.7\\pm0.6)\\times 10^6$ and $(345.4\\pm 2.6)\\times 10^6$, respectively.","Both numbers are consistent with the previous measurements within one standard deviation.","The total number of $\\psi(3686)$ events in the three data samples is $(2712.4\\pm14.3)\\times10^6$."],"url":"http://arxiv.org/abs/2403.06766v2","category":"hep-ex"}
{"created":"2024-03-11 14:35:32","title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models","abstract":"In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models. Code is released at https://github.com/pkunlp-icler/FastV.","sentences":["In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA.","We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling.","To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones.","Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks.","The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient.","It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance.","We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models.","Code is released at https://github.com/pkunlp-icler/FastV."],"url":"http://arxiv.org/abs/2403.06764v1","category":"cs.CV"}
{"created":"2024-03-11 14:28:40","title":"ALaRM: Align Language Models via Hierarchical Rewards Modeling","abstract":"We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io.","sentences":["We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences.","The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards.","This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks.","By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment.","We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines.","Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment.","We release our code at https://ALaRM-fdu.github.io."],"url":"http://arxiv.org/abs/2403.06754v1","category":"cs.CL"}
{"created":"2024-03-11 14:28:02","title":"Complementing cell taxonomies with a multicellular functional analysis of tissues","abstract":"The application of single-cell molecular profiling coupled with spatial technologies has enabled charting cellular heterogeneity in reference tissues and in disease. This new wave of molecular data has highlighted the expected diversity of single-cell dynamics upon shared external queues and spatial organizations. However, little is known about the relationship between single cell heterogeneity and the emergence and maintenance of robust multicellular processes in developed tissues and its role in (patho)physiology. Here, we present emerging computational modeling strategies that use increasingly available large-scale cross-condition single cell and spatial datasets, to study multicellular organization in tissues and complement cell taxonomies. This perspective should enable us to better understand how cells within tissues collectively process information and adapt synchronized responses in disease contexts and to bridge the gap between structural changes and functions in tissues.","sentences":["The application of single-cell molecular profiling coupled with spatial technologies has enabled charting cellular heterogeneity in reference tissues and in disease.","This new wave of molecular data has highlighted the expected diversity of single-cell dynamics upon shared external queues and spatial organizations.","However, little is known about the relationship between single cell heterogeneity and the emergence and maintenance of robust multicellular processes in developed tissues and its role in (patho)physiology.","Here, we present emerging computational modeling strategies that use increasingly available large-scale cross-condition single cell and spatial datasets, to study multicellular organization in tissues and complement cell taxonomies.","This perspective should enable us to better understand how cells within tissues collectively process information and adapt synchronized responses in disease contexts and to bridge the gap between structural changes and functions in tissues."],"url":"http://arxiv.org/abs/2403.06753v1","category":"q-bio.TO"}
{"created":"2024-03-11 14:10:57","title":"ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation","abstract":"Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual \\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine \\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \\model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth. Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood. Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \\model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation.","sentences":["Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning.","However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation.","For this issue, this paper introduces an \\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual \\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine \\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods.","In this method, \\model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth.","Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood.","Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \\model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation."],"url":"http://arxiv.org/abs/2403.06745v1","category":"cs.CL"}
{"created":"2024-03-11 14:10:08","title":"PolyominoIdeals: a package for Macaulay2 to work with the inner $2$-minor ideals of collections of cells","abstract":"Let $\\mathcal{P}$ be a collection of cells and $I_\\mathcal{P}$ be the associated ideal of inner $2$-minors as defined by A. A. Qureshi in 2012. In this paper, we provide a description of the package $\\texttt{PolyominoIdeals}$ for the computer algebra software $\\texttt{Macaulay2}$. More precisely, this package provides some functions that allow to define the ideal $I_{\\mathcal{P}}$ in $\\texttt{Macaulay2}$ and to compute its algebraic invariants or verifying its algebraic properties. We explain the usage of these functions and also give some examples.","sentences":["Let $\\mathcal{P}$ be a collection of cells and $I_\\mathcal{P}$ be the associated ideal of inner $2$-minors as defined by A. A. Qureshi in 2012.","In this paper, we provide a description of the package $\\texttt{PolyominoIdeals}$ for the computer algebra software $\\texttt{Macaulay2}$. More precisely, this package provides some functions that allow to define the ideal $I_{\\mathcal{P}}$ in $\\texttt{Macaulay2}$ and to compute its algebraic invariants or verifying its algebraic properties.","We explain the usage of these functions and also give some examples."],"url":"http://arxiv.org/abs/2403.06743v1","category":"math.AC"}
{"created":"2024-03-11 14:07:21","title":"A Two-Field-Scan Harmonic Hall Voltage Analysis For Fast, Accurate Quantification Of Spin-Orbit Torques In Magnetic Heterostructures","abstract":"The efficiencies of the spin-orbit torques (SOTs) play a key role in the determination of the power consumption, integration density, and endurance of SOT-driven devices. Accurate and time-efficient determination of the SOT efficiencies is of great importance not only for evaluating the practical potential of SOT devices but also for developing new mechanisms for enhancing the SOT efficiencies. Here, we develop a \"two-field-scan\" harmonic Hall voltage (HHV) analysis that collects the second HHV as a function of a swept in-plane magnetic field at 45{\\deg} and 0{\\deg} relative to the excitation current. We demonstrate that this two-field-scan analysis is as accurate as the well-established but time-consuming angle-scan HHV analysis even in the presence of considerable thermoelectric effects but takes more than a factor of 7 less measurement time. We also show that the 3-parameter fit of the HHV data from a single field scan at 0{\\deg}, which is commonly employed in the literature, is not reliable because the employment of too many free parameters in the fitting of the very slowly varying HHV signal allows unrealistic pseudo-solution and thus erroneous conclusion about the SOT efficiencies.","sentences":["The efficiencies of the spin-orbit torques (SOTs) play a key role in the determination of the power consumption, integration density, and endurance of SOT-driven devices.","Accurate and time-efficient determination of the SOT efficiencies is of great importance not only for evaluating the practical potential of SOT devices but also for developing new mechanisms for enhancing the SOT efficiencies.","Here, we develop a \"two-field-scan\" harmonic Hall voltage (HHV) analysis that collects the second HHV as a function of a swept in-plane magnetic field at 45{\\deg} and 0{\\deg} relative to the excitation current.","We demonstrate that this two-field-scan analysis is as accurate as the well-established but time-consuming angle-scan HHV analysis even in the presence of considerable thermoelectric effects but takes more than a factor of 7 less measurement time.","We also show that the 3-parameter fit of the HHV data from a single field scan at 0{\\deg}, which is commonly employed in the literature, is not reliable because the employment of too many free parameters in the fitting of the very slowly varying HHV signal allows unrealistic pseudo-solution and thus erroneous conclusion about the SOT efficiencies."],"url":"http://arxiv.org/abs/2403.06740v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 14:03:57","title":"Energy loss of a heavy fermion in a collisional QED plasma","abstract":"We compute the energy loss of heavy fermions moving in a plasma, taking into account the modification of the photon collective modes induced by collisions using a Bhatnagar-Gross-Krook collisional kernel. We include contributions from both hard and soft scatterings of the heavy fermion using a collisionally modified hard-thermal-loop resummed propagator. Using this method, one does not need to introduce a separation scale between hard- and soft-momentum exchanges. To place our calculation in context, we review other theoretical approaches to computing the collisional energy loss of fermions and discuss the systematics and results obtained in each approach compared to using a resummed propagator for both hard and soft momentum exchanges. Our final results indicate that self-consistently including the effect of collisions in the self-energies of the resummed propagator results in an increased energy loss compared to using collisionless hard-thermal-loop propagators. The effect becomes larger as the magnitude of the coupling constant and the velocity of the fermion increase.","sentences":["We compute the energy loss of heavy fermions moving in a plasma, taking into account the modification of the photon collective modes induced by collisions using a Bhatnagar-Gross-Krook collisional kernel.","We include contributions from both hard and soft scatterings of the heavy fermion using a collisionally modified hard-thermal-loop resummed propagator.","Using this method, one does not need to introduce a separation scale between hard- and soft-momentum exchanges.","To place our calculation in context, we review other theoretical approaches to computing the collisional energy loss of fermions and discuss the systematics and results obtained in each approach compared to using a resummed propagator for both hard and soft momentum exchanges.","Our final results indicate that self-consistently including the effect of collisions in the self-energies of the resummed propagator results in an increased energy loss compared to using collisionless hard-thermal-loop propagators.","The effect becomes larger as the magnitude of the coupling constant and the velocity of the fermion increase."],"url":"http://arxiv.org/abs/2403.06739v1","category":"hep-ph"}
{"created":"2024-03-11 13:57:05","title":"Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback","abstract":"Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models.","sentences":["Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions.","Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences.","In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans.","This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset.","Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced.","In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models."],"url":"http://arxiv.org/abs/2403.06735v1","category":"cs.CV"}
{"created":"2024-03-11 13:56:57","title":"Real-Time Multimodal Cognitive Assistant for Emergency Medical Services","abstract":"Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by large language models (LLMs); (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using graph-based attention mechanisms; (iii) an EMS Action Recognition module which leverages multimodal audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene. Our results show that for speech recognition we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data. Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server.","sentences":["Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making.","This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses.","CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition.","We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by large language models (LLMs); (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using graph-based attention mechanisms; (iii) an EMS Action Recognition module which leverages multimodal audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene.","Our results show that for speech recognition we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data.","Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server."],"url":"http://arxiv.org/abs/2403.06734v1","category":"cs.AI"}
{"created":"2024-03-11 13:44:49","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","abstract":"Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pairs accordingly. In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible. Our key idea is to introduce a reasonable and simple assumption that the normalized features in contrastive learning follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits. First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches. Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of contrastive pairs and derive a closed form of the expected contrastive loss for efficient optimization. Our code is available at https://github.com/LeapLabTHU/ProCo.","sentences":["Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples.","Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets.","Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance.","However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data.","To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pairs accordingly.","In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible.","Our key idea is to introduce a reasonable and simple assumption that the normalized features in contrastive learning follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits.","First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches.","Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of contrastive pairs and derive a closed form of the expected contrastive loss for efficient optimization.","Our code is available at https://github.com/LeapLabTHU/ProCo."],"url":"http://arxiv.org/abs/2403.06726v1","category":"cs.LG"}
{"created":"2024-03-11 13:44:43","title":"Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning","abstract":"Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subsequently facilitate effective adaptation to low-resource KT datasets. Specifically, we simplify existing sophisticated DLKT model architectures with purely a stack of transformer decoders. We design an encoding mechanism to incorporate student interactions from multiple KT data sources and develop an importance mechanism to prioritize updating parameters with high importance while constraining less important ones during the fine-tuning stage. We evaluate LoReKT on six public KT datasets and experimental results demonstrate the superiority of our approach in terms of AUC and Accuracy. To encourage reproducible research, we make our data and code publicly available at https://anonymous.4open.science/r/LoReKT-C619.","sentences":["Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions.","Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task.","These DLKT models heavily rely on the large number of available student interactions.","However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets.","Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture.","Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges.","Inspired by the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subsequently facilitate effective adaptation to low-resource KT datasets.","Specifically, we simplify existing sophisticated DLKT model architectures with purely a stack of transformer decoders.","We design an encoding mechanism to incorporate student interactions from multiple KT data sources and develop an importance mechanism to prioritize updating parameters with high importance while constraining less important ones during the fine-tuning stage.","We evaluate LoReKT on six public KT datasets and experimental results demonstrate the superiority of our approach in terms of AUC and Accuracy.","To encourage reproducible research, we make our data and code publicly available at https://anonymous.4open.science/r/LoReKT-C619."],"url":"http://arxiv.org/abs/2403.06725v1","category":"cs.CY"}
{"created":"2024-03-11 13:44:41","title":"A note on the Segal conjecture for large objects","abstract":"The Segal conjecture for $C_p$ (as proved by Lin and Gunawardena) asserts that the canonical map from the $p$-complete sphere spectrum to the Tate construction for the trivial action of $C_p$ on the $p$-complete sphere spectrum is an isomorphism. In this article we extend the collection of spectra for which the canonical map $X \\to X^{tC_p}$ is known to be an isomorphism to include any $p$-complete, bounded below spectrum whose mod $p$ homology, viewed a module over the Steenrod algebra, is complete with respect to the maximal ideal $I \\subseteq \\mathcal{A}$.","sentences":["The Segal conjecture for $C_p$ (as proved by Lin and Gunawardena) asserts that the canonical map from the $p$-complete sphere spectrum to the Tate construction for the trivial action of $C_p$ on the $p$-complete sphere spectrum is an isomorphism.","In this article we extend the collection of spectra for which the canonical map $X \\to X^{tC_p}$ is known to be an isomorphism to include any $p$-complete, bounded below spectrum whose mod $p$ homology, viewed a module over the Steenrod algebra, is complete with respect to the maximal ideal $I \\subseteq \\mathcal{A}$."],"url":"http://arxiv.org/abs/2403.06724v1","category":"math.AT"}
{"created":"2024-03-11 13:40:46","title":"Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based Method for Dynamic Observation Processing in Spatially Distributed Emergencies","abstract":"In emergencies, high stake decisions often have to be made under time pressure and strain. In order to support such decisions, information from various sources needs to be collected and processed rapidly. The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions. Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations. To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies. The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency. It thereby reduces complexity and cognitive load for decision makers. The output of the ERIMap method is a dynamically evolving and spatially resolved map of beliefs about key variables of an emergency that is updated each time a new observation becomes available. The method is illustrated in a case study in which an emergency response is triggered by an accident causing a gas leakage on a chemical plant site.","sentences":["In emergencies, high stake decisions often have to be made under time pressure and strain.","In order to support such decisions, information from various sources needs to be collected and processed rapidly.","The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions.","Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations.","To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies.","The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency.","It thereby reduces complexity and cognitive load for decision makers.","The output of the ERIMap method is a dynamically evolving and spatially resolved map of beliefs about key variables of an emergency that is updated each time a new observation becomes available.","The method is illustrated in a case study in which an emergency response is triggered by an accident causing a gas leakage on a chemical plant site."],"url":"http://arxiv.org/abs/2403.06716v1","category":"cs.IR"}
{"created":"2024-03-11 13:07:46","title":"Chart4Blind: An Intelligent Interface for Chart Accessibility Conversion","abstract":"In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge. Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people. Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals. Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty. To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats. Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards. Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%. In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers. For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/.","sentences":["In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge.","Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people.","Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals.","Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty.","To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats.","Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards.","Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%.","In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers.","For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/."],"url":"http://arxiv.org/abs/2403.06693v1","category":"cs.HC"}
{"created":"2024-03-11 13:04:48","title":"Dynamic of frustrated Kuramoto oscillators with modular connections","abstract":"Synchronization and collective movement are phenomena of a highly interdisciplinary nature, with examples ranging from neuronal activation to walking pedestrians. As of today, the Kuramoto model stands as the quintessential framework for investigating synchronization phenomena, displaying a second order phase transition from disordered motion to synchronization as the coupling between oscillators increases. The model was recently extended to higher dimensions allowing for the coupling parameter to be promoted to a matrix, leading to generalized frustration and new synchronized states. This model was previously investigated in the case of all-to-all and homogeneous interactions. Here, we extend the analysis to modular graphs, which mimic the community structure presented in many real systems. We investigated, both numerically and analytically, the matrix coupled Kuramoto model with oscillators divided into two groups with distinct coupling parameters to understand in which conditions they synchronize independently or globally. We discovered a very rich and complex dynamic, including an extended region in the parameter space in which the interactions between modules were destructive, leading to a global disordered motion even tough the uncoupled dynamic presented higher levels of synchronization. Additional simulations considering synthetic modular networks were performed to assess the robustness of our findings.","sentences":["Synchronization and collective movement are phenomena of a highly interdisciplinary nature, with examples ranging from neuronal activation to walking pedestrians.","As of today, the Kuramoto model stands as the quintessential framework for investigating synchronization phenomena, displaying a second order phase transition from disordered motion to synchronization as the coupling between oscillators increases.","The model was recently extended to higher dimensions allowing for the coupling parameter to be promoted to a matrix, leading to generalized frustration and new synchronized states.","This model was previously investigated in the case of all-to-all and homogeneous interactions.","Here, we extend the analysis to modular graphs, which mimic the community structure presented in many real systems.","We investigated, both numerically and analytically, the matrix coupled Kuramoto model with oscillators divided into two groups with distinct coupling parameters to understand in which conditions they synchronize independently or globally.","We discovered a very rich and complex dynamic, including an extended region in the parameter space in which the interactions between modules were destructive, leading to a global disordered motion even tough the uncoupled dynamic presented higher levels of synchronization.","Additional simulations considering synthetic modular networks were performed to assess the robustness of our findings."],"url":"http://arxiv.org/abs/2403.06689v1","category":"physics.soc-ph"}
{"created":"2024-03-11 12:49:37","title":"Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction","abstract":"In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings. Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop. Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate. Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees. Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings. For example, we derive Riemannian MARINA (R-MARINA) for distributed settings with communication compression, providing the best theoretical communication complexity guarantees for non-convex distributed optimization over Riemannian manifolds. Experimental results support our theoretical findings.","sentences":["In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings.","Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop.","Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate.","Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods.","These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees.","Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings.","For example, we derive Riemannian MARINA (R-MARINA) for distributed settings with communication compression, providing the best theoretical communication complexity guarantees for non-convex distributed optimization over Riemannian manifolds.","Experimental results support our theoretical findings."],"url":"http://arxiv.org/abs/2403.06677v1","category":"cs.LG"}
{"created":"2024-03-11 12:47:04","title":"Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code","abstract":"AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.","sentences":["AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL).","However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples.","In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code.","Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation.","Lastly, we discuss potential solutions to overcome this threat."],"url":"http://arxiv.org/abs/2403.06675v1","category":"cs.CR"}
{"created":"2024-03-11 12:46:53","title":"Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment","abstract":"Most computer vision applications aim to identify pixels in a scene and use them for diverse purposes. One intriguing application is car damage detection for insurance carriers which tends to detect all car damages by comparing both pre-trip and post-trip images, even requiring two components: (i) car damage detection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to detect car damages on custom images. Whereas for the image alignment section, we especially propose a novel self-supervised Patch-to-Patch SimCLR inspired alignment approach to find perspective transformations between custom pre/post car rental images except for traditional computer vision methods.","sentences":["Most computer vision applications aim to identify pixels in a scene and use them for diverse purposes.","One intriguing application is car damage detection for insurance carriers which tends to detect all car damages by comparing both pre-trip and post-trip images, even requiring two components: (i) car damage detection; (ii) image alignment.","Firstly, we implemented a Mask R-CNN model to detect car damages on custom images.","Whereas for the image alignment section, we especially propose a novel self-supervised Patch-to-Patch SimCLR inspired alignment approach to find perspective transformations between custom pre/post car rental images except for traditional computer vision methods."],"url":"http://arxiv.org/abs/2403.06674v1","category":"cs.CV"}
{"created":"2024-03-11 12:29:35","title":"FashionReGen: LLM-Empowered Fashion Report Generation","abstract":"Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.","sentences":["Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports.","It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people.","In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR.","Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation.","By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain.","It also inspires the explorations of more high-level tasks with industrial significance in other domains.","Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen."],"url":"http://arxiv.org/abs/2403.06660v1","category":"cs.MM"}
{"created":"2024-03-11 12:28:55","title":"Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement","abstract":"Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification. Based on MERL, we perform the first benchmark across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods. Notably, MERL achieves an average AUC score of 75.2% in zero-shot classification (without training data), 3.2% higher than linear probed eSSL methods with 10\\% annotated training data, averaged across all six datasets.","sentences":["Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice.","While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports.","This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility.","In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework.","Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks.","At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification.","Based on MERL, we perform the first benchmark across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods.","Notably, MERL achieves an average AUC score of 75.2% in zero-shot classification (without training data), 3.2% higher than linear probed eSSL methods with 10\\% annotated training data, averaged across all six datasets."],"url":"http://arxiv.org/abs/2403.06659v1","category":"eess.SP"}
{"created":"2024-03-11 12:04:20","title":"KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation","abstract":"The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method.","sentences":["The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches.","With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction.","However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination.","A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text.","Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method.","In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training.","Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.06642v1","category":"cs.IR"}
{"created":"2024-03-11 11:41:30","title":"Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings","abstract":"In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity. However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning. In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined. Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented. Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between performance and efficiency is highlighted in this low-data regime. Finally, this paper introduces a novel way to quantify this trade-off through a customized Efficiency Factor metric.","sentences":["In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity.","However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning.","In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined.","Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented.","Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between performance and efficiency is highlighted in this low-data regime.","Finally, this paper introduces a novel way to quantify this trade-off through a customized Efficiency Factor metric."],"url":"http://arxiv.org/abs/2403.06631v1","category":"cs.LG"}
{"created":"2024-03-11 11:26:44","title":"Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation","abstract":"Humans use UAVs to monitor changes in forest environments since they are lightweight and provide a large variety of surveillance data. However, their information does not present enough details for understanding the scene which is needed to assess the degree of deforestation. Deep learning algorithms must be trained on large amounts of data to output accurate interpretations, but ground truth recordings of annotated forest imagery are not available. To solve this problem, we introduce a new large aerial dataset for forest inspection which contains both real-world and virtual recordings of natural environments, with densely annotated semantic segmentation labels and depth maps, taken in different illumination conditions, at various altitudes and recording angles. We test the performance of two multi-scale neural networks for solving the semantic segmentation task (HRNet and PointFlow network), studying the impact of the various acquisition conditions and the capabilities of transfer learning from virtual to real data. Our results showcase that the best results are obtained when the training is done on a dataset containing a large variety of scenarios, rather than separating the data into specific categories. We also develop a framework to assess the deforestation degree of an area.","sentences":["Humans use UAVs to monitor changes in forest environments since they are lightweight and provide a large variety of surveillance data.","However, their information does not present enough details for understanding the scene which is needed to assess the degree of deforestation.","Deep learning algorithms must be trained on large amounts of data to output accurate interpretations, but ground truth recordings of annotated forest imagery are not available.","To solve this problem, we introduce a new large aerial dataset for forest inspection which contains both real-world and virtual recordings of natural environments, with densely annotated semantic segmentation labels and depth maps, taken in different illumination conditions, at various altitudes and recording angles.","We test the performance of two multi-scale neural networks for solving the semantic segmentation task (HRNet and PointFlow network), studying the impact of the various acquisition conditions and the capabilities of transfer learning from virtual to real data.","Our results showcase that the best results are obtained when the training is done on a dataset containing a large variety of scenarios, rather than separating the data into specific categories.","We also develop a framework to assess the deforestation degree of an area."],"url":"http://arxiv.org/abs/2403.06621v1","category":"cs.CV"}
{"created":"2024-03-11 10:57:45","title":"MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding","abstract":"With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored. This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art. Extensive ablation studies further reveal the effectiveness of each component of MedKP. This enhancement advances the development of reliable, automated medical consultation responses using LLMs, thereby broadening the potential accessibility of precise and real-time medical assistance.","sentences":["With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions.","However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored.","This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses.","In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions.","Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art.","Extensive ablation studies further reveal the effectiveness of each component of MedKP.","This enhancement advances the development of reliable, automated medical consultation responses using LLMs, thereby broadening the potential accessibility of precise and real-time medical assistance."],"url":"http://arxiv.org/abs/2403.06611v1","category":"cs.CL"}
{"created":"2024-03-11 10:53:20","title":"Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds","abstract":"Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical reasoning ability of LLMs.","sentences":["Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients.","This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc.","Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians.","This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches.","Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning.","However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians.","In this study, we introduce a novel framework, In-Context Padding (ICP), designed to enhance LLMs with medical knowledge.","Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs.","Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical reasoning ability of LLMs."],"url":"http://arxiv.org/abs/2403.06609v1","category":"cs.CL"}
{"created":"2024-03-11 10:48:56","title":"Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers","abstract":"Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model. Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging. This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision. In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers. We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data. We demonstrate our method's utility in cross-domain and cross-dimension experiments, where we pretrain our models on 2D satellite images before applying them to vastly different target domains in 2D and 3D. Our method consistently outperforms a series of baselines on challenging benchmarks, such as retinal or whole-brain vessel graph extraction.","sentences":["Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model.","Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging.","This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision.","In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers.","We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data.","We demonstrate our method's utility in cross-domain and cross-dimension experiments, where we pretrain our models on 2D satellite images before applying them to vastly different target domains in 2D and 3D.","Our method consistently outperforms a series of baselines on challenging benchmarks, such as retinal or whole-brain vessel graph extraction."],"url":"http://arxiv.org/abs/2403.06601v1","category":"cs.CV"}
{"created":"2024-03-12 17:59:50","title":"Stability of Anomalous Hall Crystals in multilayer rhombohedral graphene","abstract":"Recent experiments showing an integer quantum anomalous Hall effect in pentalayer rhombohedral graphene have been interpreted in terms of a valley-polarized interaction-induced Chern band. The resulting many-body state can be viewed as an Anomalous Hall Crystal (AHC), with a further coupling to a weak moir\\'e potential. We explain the origin of the Chern band and the corresponding AHC in the pentalayer system. We propose a simplified model that focuses on the physics near high symmetry Brillouin zone points to describe the competition between AHC and Wigner Crystal (WC) phases. We discuss the possible role of the moir\\'e potential. We emphasize that even if in the moir\\'e-less limit, the AHC is not favored over a correlated Fermi liquid, the moir\\'e potential will push the system into a `moir\\'e-enabled AHC'. We also suggest that there is a range of alignment angles between R5G and hBN where a $C = 2$ insulator may be found at integer filling.","sentences":["Recent experiments showing an integer quantum anomalous Hall effect in pentalayer rhombohedral graphene have been interpreted in terms of a valley-polarized interaction-induced Chern band.","The resulting many-body state can be viewed as an Anomalous Hall Crystal (AHC), with a further coupling to a weak moir\\'e potential.","We explain the origin of the Chern band and the corresponding AHC in the pentalayer system.","We propose a simplified model that focuses on the physics near high symmetry Brillouin zone points to describe the competition between AHC and Wigner Crystal (WC) phases.","We discuss the possible role of the moir\\'e potential.","We emphasize that even if in the moir\\'e-less limit, the AHC is not favored over a correlated Fermi liquid, the moir\\'e potential will push the system into a `moir\\'e-enabled AHC'.","We also suggest that there is a range of alignment angles between R5G and hBN where a $C = 2$ insulator may be found at integer filling."],"url":"http://arxiv.org/abs/2403.07873v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 17:59:02","title":"SIDE-real: Truncated marginal neural ratio estimation for Supernova Ia Dust Extinction with real data","abstract":"We present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia). Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redshift, and realistic instrumental noise. By utilising truncated marginal neural ratio estimation (TMNRE), a neural network-enabled simulation-based inference technique, we implicitly marginalise over 4000 latent variables (for a set of $\\approx 100$ SNae Ia) to efficiently infer SN Ia absolute magnitudes and host-galaxy dust properties at the population level while also constraining the parameters of individual objects. Amortisation of the inference procedure allows us to obtain coverage guarantees for our results through Bayesian validation and frequentist calibration. Furthermore, we show a detailed comparison to full likelihood-based inference, implemented through Hamiltonian Monte Carlo, on simulated data and then apply TMNRE to the light curves of 86 SNae Ia from the Carnegie Supernova Project, deriving marginal posteriors in excellent agreement with previous work. Given its ability to accommodate arbitrarily complex extensions to the forward model -- e.g. different populations based on host properties, redshift evolution, complicated photometric redshift estimates, selection effects, and non-Ia contamination -- without significant modifications to the inference procedure, TMNRE has the potential to become the tool of choice for cosmological parameter inference from future, large SN Ia samples.","sentences":["We present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia).","Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redshift, and realistic instrumental noise.","By utilising truncated marginal neural ratio estimation (TMNRE), a neural network-enabled simulation-based inference technique, we implicitly marginalise over 4000 latent variables (for a set of $\\approx 100$ SNae Ia) to efficiently infer SN Ia absolute magnitudes and host-galaxy dust properties at the population level while also constraining the parameters of individual objects.","Amortisation of the inference procedure allows us to obtain coverage guarantees for our results through Bayesian validation and frequentist calibration.","Furthermore, we show a detailed comparison to full likelihood-based inference, implemented through Hamiltonian Monte Carlo, on simulated data and then apply TMNRE to the light curves of 86 SNae Ia from the Carnegie Supernova Project, deriving marginal posteriors in excellent agreement with previous work.","Given its ability to accommodate arbitrarily complex extensions to the forward model -- e.g. different populations based on host properties, redshift evolution, complicated photometric redshift estimates, selection effects, and non-Ia contamination -- without significant modifications to the inference procedure, TMNRE has the potential to become the tool of choice for cosmological parameter inference from future, large SN Ia samples."],"url":"http://arxiv.org/abs/2403.07871v1","category":"astro-ph.CO"}
{"created":"2024-03-12 17:54:16","title":"Spectral invariants for non-compactly supported Hamiltonians on the disc, and an application to the mean action spectrum","abstract":"For a symplectic isotopy on the two-dimensional disc we show that the classical spectral invariants of Viterbo [20] can be extended in a meaningful way to {\\it non-compactly} supported Hamiltonians. We establish some basic properties of these extended invariants and as an application we show that Hutchings' inequality in [8] between the Calabi invariant and the mean action spectrum holds without any assumptions on the isotopy; in [8] it is assumed that the Calabi invariant is less than the rotation number (or action) on the boundary.","sentences":["For a symplectic isotopy on the two-dimensional disc we show that the classical spectral invariants of Viterbo [20] can be extended in a meaningful way to {\\it non-compactly} supported Hamiltonians.","We establish some basic properties of these extended invariants and as an application we show that Hutchings' inequality in [8] between the Calabi invariant and the mean action spectrum holds without any assumptions on the isotopy; in [8] it is assumed that the Calabi invariant is less than the rotation number (or action) on the boundary."],"url":"http://arxiv.org/abs/2403.07863v1","category":"math.SG"}
{"created":"2024-03-12 17:46:38","title":"Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis","abstract":"This study addresses the urgent need for improved prostate cancer detection methods by harnessing the power of advanced technological solutions. We introduce the application of Quantum Support Vector Machine (QSVM) to this critical healthcare challenge, showcasing an enhancement in diagnostic performance over the classical Support Vector Machine (SVM) approach. Our study not only outlines the remarkable improvements in diagnostic performance made by QSVM over the classic SVM technique, but it delves into the advancements brought about by the quantum feature map architecture, which has been carefully identified and evaluated, ensuring it aligns seamlessly with the unique characteristics of our prostate cancer dataset. This architecture succeded in creating a distinct feature space, enabling the detection of complex, non-linear patterns in the data. The findings reveal not only a comparable accuracy with classical SVM ($92\\%$) but also a $7.14\\%$ increase in sensitivity and a notably high F1-Score ($93.33\\%$). This study's important combination of quantum computing in medical diagnostics marks a pivotal step forward in cancer detection, offering promising implications for the future of healthcare technology.","sentences":["This study addresses the urgent need for improved prostate cancer detection methods by harnessing the power of advanced technological solutions.","We introduce the application of Quantum Support Vector Machine (QSVM) to this critical healthcare challenge, showcasing an enhancement in diagnostic performance over the classical Support Vector Machine (SVM) approach.","Our study not only outlines the remarkable improvements in diagnostic performance made by QSVM over the classic SVM technique, but it delves into the advancements brought about by the quantum feature map architecture, which has been carefully identified and evaluated, ensuring it aligns seamlessly with the unique characteristics of our prostate cancer dataset.","This architecture succeded in creating a distinct feature space, enabling the detection of complex, non-linear patterns in the data.","The findings reveal not only a comparable accuracy with classical SVM ($92\\%$) but also a $7.14\\%$ increase in sensitivity and a notably high F1-Score ($93.33\\%$).","This study's important combination of quantum computing in medical diagnostics marks a pivotal step forward in cancer detection, offering promising implications for the future of healthcare technology."],"url":"http://arxiv.org/abs/2403.07856v1","category":"cs.LG"}
{"created":"2024-03-12 17:43:20","title":"12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning","abstract":"Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems to expand their inference capabilities to new classes using only a few labeled examples, without forgetting the previously learned classes. Classical backpropagation-based learning and its variants are often unsuitable for battery-powered, memory-constrained systems at the extreme edge. In this work, we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a lightweight model consisting of a pretrained and metalearned feature extractor and an expandable explicit memory storing the class prototypes. The architecture is pretrained with a novel feature orthogonality regularization and metalearned with a multi-margin loss. For learning a new class, our approach extends the explicit memory with novel class prototypes, while the remaining architecture is kept frozen. This allows learning previously unseen classes based on only a few examples with one single pass (hence online). O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark, achieving state-of-the-art results. Tailored for ultra-low-power platforms, we implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online learning capabilities within just 12 mJ per new class.","sentences":["Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems to expand their inference capabilities to new classes using only a few labeled examples, without forgetting the previously learned classes.","Classical backpropagation-based learning and its variants are often unsuitable for battery-powered, memory-constrained systems at the extreme edge.","In this work, we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a lightweight model consisting of a pretrained and metalearned feature extractor and an expandable explicit memory storing the class prototypes.","The architecture is pretrained with a novel feature orthogonality regularization and metalearned with a multi-margin loss.","For learning a new class, our approach extends the explicit memory with novel class prototypes, while the remaining architecture is kept frozen.","This allows learning previously unseen classes based on only a few examples with one single pass (hence online).","O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark, achieving state-of-the-art results.","Tailored for ultra-low-power platforms, we implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online learning capabilities within just 12 mJ per new class."],"url":"http://arxiv.org/abs/2403.07851v1","category":"cs.LG"}
{"created":"2024-03-12 17:37:41","title":"Hyper-density functional theory of soft matter","abstract":"We present a scheme for investigating arbitrary thermal observables in spatially inhomogeneous many-body systems. Extending the equilibrium ensemble yields any given observable as an explicit hyper-density functional. Associated local fluctuation profiles follow from an exact hyper-Ornstein-Zernike equation. Simulation-based supervised machine learning trains neural networks that act as hyper-direct correlation functionals which facilitate efficient and accurate predictions. We exemplify the approach for the cluster statistics of hard rods and square well particles. The theory provides access to complex order parameters, as is impossible in standard density functional theory.","sentences":["We present a scheme for investigating arbitrary thermal observables in spatially inhomogeneous many-body systems.","Extending the equilibrium ensemble yields any given observable as an explicit hyper-density functional.","Associated local fluctuation profiles follow from an exact hyper-Ornstein-Zernike equation.","Simulation-based supervised machine learning trains neural networks that act as hyper-direct correlation functionals which facilitate efficient and accurate predictions.","We exemplify the approach for the cluster statistics of hard rods and square well particles.","The theory provides access to complex order parameters, as is impossible in standard density functional theory."],"url":"http://arxiv.org/abs/2403.07845v1","category":"cond-mat.soft"}
{"created":"2024-03-12 17:18:35","title":"Syncopated Dynamical Decoupling for Suppressing Crosstalk in Quantum Circuits","abstract":"Theoretically understanding and experimentally characterizing and modifying the underlying Hamiltonian of a quantum system is of utmost importance in achieving high-fidelity quantum gates for quantum computing. In this work, we explore the use of dynamical decoupling (DD) in characterizing undesired two-qubit couplings as well as the underlying single-qubit decoherence, and in suppressing them. We develop a syncopated dynamical decoupling technique which protects against decoherence and selectively targets unwanted two-qubit interactions, overcoming both significant hurdles to achieving precise quantum control and realizing quantum computing on many hardware prototypes. On a transmon-qubit-based superconducting quantum device, we identify separate white and $1/f$ noise components underlying the single-qubit decoherence and a static ZZ coupling between pairs of qubits. We suppress these errors using syncopated dynamical decoupling in two-qubit benchmarking experiments and significantly boost performance in a realistic algorithmic quantum circuit.","sentences":["Theoretically understanding and experimentally characterizing and modifying the underlying Hamiltonian of a quantum system is of utmost importance in achieving high-fidelity quantum gates for quantum computing.","In this work, we explore the use of dynamical decoupling (DD) in characterizing undesired two-qubit couplings as well as the underlying single-qubit decoherence, and in suppressing them.","We develop a syncopated dynamical decoupling technique which protects against decoherence and selectively targets unwanted two-qubit interactions, overcoming both significant hurdles to achieving precise quantum control and realizing quantum computing on many hardware prototypes.","On a transmon-qubit-based superconducting quantum device, we identify separate white and $1/f$ noise components underlying the single-qubit decoherence and a static ZZ coupling between pairs of qubits.","We suppress these errors using syncopated dynamical decoupling in two-qubit benchmarking experiments and significantly boost performance in a realistic algorithmic quantum circuit."],"url":"http://arxiv.org/abs/2403.07836v1","category":"quant-ph"}
{"created":"2024-03-12 17:05:27","title":"Computational modelling of complex multiphase behavior of environmentally-friendly materials for sustainable technological solutions","abstract":"This research introduces a detailed computational framework designed to analyze and forecast the complex multiphase characteristics of eco-friendly lead-free piezoelectric materials, which are essential for developing sustainable technological advancements. Lead-free piezoelectric materials have a significant thermo-electromechanical response, although their electromechanical characteristics vary throughout different phases of the material. Lead-free piezoelectric materials undergo phase changes, including rhombohedral (R3c), orthorhombic (Pnma), tetragonal (P4bm), and cubic (Cc) phases, when the temperature changes. These phases are determined by the symmetry and alignment of the ferroelectric domains. Furthermore, multiple phases exist simultaneously under certain temperature, electrical, and mechanical conditions, resulting in the material displaying intricate multiphase behavior. Studying such behaviour is crucial for evaluating the performance of these materials. The computational approach in this research relies on Landau-Ginzburg-Devonshire theory to model micro-domain phase changes in the material. This research will enhance our comprehension of the significance of complex multiphase behaviour in creating environment-friendly and sustainable technological solutions.","sentences":["This research introduces a detailed computational framework designed to analyze and forecast the complex multiphase characteristics of eco-friendly lead-free piezoelectric materials, which are essential for developing sustainable technological advancements.","Lead-free piezoelectric materials have a significant thermo-electromechanical response, although their electromechanical characteristics vary throughout different phases of the material.","Lead-free piezoelectric materials undergo phase changes, including rhombohedral (R3c), orthorhombic (Pnma), tetragonal (P4bm), and cubic (Cc) phases, when the temperature changes.","These phases are determined by the symmetry and alignment of the ferroelectric domains.","Furthermore, multiple phases exist simultaneously under certain temperature, electrical, and mechanical conditions, resulting in the material displaying intricate multiphase behavior.","Studying such behaviour is crucial for evaluating the performance of these materials.","The computational approach in this research relies on Landau-Ginzburg-Devonshire theory to model micro-domain phase changes in the material.","This research will enhance our comprehension of the significance of complex multiphase behaviour in creating environment-friendly and sustainable technological solutions."],"url":"http://arxiv.org/abs/2403.07826v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 17:04:28","title":"The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing","abstract":"Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques.","sentences":["Large Language Models have revolutionized numerous tasks with their remarkable efficacy.","However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space.","This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.","This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing.","Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect.","Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods.","However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques."],"url":"http://arxiv.org/abs/2403.07825v1","category":"cs.CL"}
{"created":"2024-03-12 17:03:40","title":"Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations","abstract":"A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with variable matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random variable coefficients sampled by Monte Carlo. Building on the assumption that a truncated Karhunen-Lo\\`{e}ve expansion of a known transform of the random variable coefficient is known, we introduce a compact representation of the random coefficient in the form of a Voronoi quantizer. The number of Voronoi cells, each of which is represented by a centroidal variable coefficient, is set to the prescribed number $P$ of preconditioners. Upon sampling the random variable coefficient, the linear system assembled with a given realization of the coefficient is solved with the preconditioner whose centroidal variable coefficient is the closest to the realization. We consider different ways to define and obtain the centroidal variable coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential simulations, and of load balancing for parallel simulations. Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random variable coefficient, is proposed with a stochastic dimension which increases with the number $P$ of preconditioners. This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random variable coefficient for a given number of preconditioners.","sentences":["A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with variable matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random variable coefficients sampled by Monte Carlo.","Building on the assumption that a truncated Karhunen-Lo\\`{e}ve expansion of a known transform of the random variable coefficient is known, we introduce a compact representation of the random coefficient in the form of a Voronoi quantizer.","The number of Voronoi cells, each of which is represented by a centroidal variable coefficient, is set to the prescribed number $P$ of preconditioners.","Upon sampling the random variable coefficient, the linear system assembled with a given realization of the coefficient is solved with the preconditioner whose centroidal variable coefficient is the closest to the realization.","We consider different ways to define and obtain the centroidal variable coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential simulations, and of load balancing for parallel simulations.","Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random variable coefficient, is proposed with a stochastic dimension which increases with the number $P$ of preconditioners.","This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random variable coefficient for a given number of preconditioners."],"url":"http://arxiv.org/abs/2403.07824v1","category":"math.NA"}
{"created":"2024-03-12 17:03:10","title":"Time-discretization method for a multi-term time fractional differential equation with delay","abstract":"This paper discusses a multi-term time-fractional delay differential equation in a real Hilbert space. An iterative scheme for a multi-term time-fractional differential equation is established using Rothe's method. The method of semi-discretization is extended to this kind of time fractional problem with delay in the case that the time delay parameter $\\nu >0$ satisfies $\\nu\\leq T$, where $T$ denotes the final time. We apply the accretivity of the operator $A$ in an iterative scheme to establish the existence and regularity of strong solutions to the considered problem. Finally, an example is provided to demonstrate the abstract result.","sentences":["This paper discusses a multi-term time-fractional delay differential equation in a real Hilbert space.","An iterative scheme for a multi-term time-fractional differential equation is established using Rothe's method.","The method of semi-discretization is extended to this kind of time fractional problem with delay in the case that the time delay parameter $\\nu >0$ satisfies $\\nu\\leq T$, where $T$ denotes the final time.","We apply the accretivity of the operator $A$ in an iterative scheme to establish the existence and regularity of strong solutions to the considered problem.","Finally, an example is provided to demonstrate the abstract result."],"url":"http://arxiv.org/abs/2403.07823v1","category":"math.NA"}
{"created":"2024-03-12 17:02:10","title":"The Variant of Designated Verifier Signature Scheme with Message Recovery","abstract":"In this work, we introduce a strong Designated Verifier Signature (DVS) scheme that incorporates a message recovery mechanism inspired by the concept of the Universal Designated Verifier Signature (UDVS) scheme. It is worth noting that Saeednia's strong designated verifier signature scheme fails to guarantee the privacy of the signature, making it unsuitable for certain applications such as medical record certificates or voting systems. To overcome this limitation, we extend Lee's strong designated verifier signature with a message recovery scheme to develop a universal designated verifier signature scheme. This universal designated verifier scheme is crafted to safeguard the privacy of signature holders, ensuring that only designated verifiers can authenticate the true signer and recover the messages.","sentences":["In this work, we introduce a strong Designated Verifier Signature (DVS) scheme that incorporates a message recovery mechanism inspired by the concept of the Universal Designated Verifier Signature (UDVS) scheme.","It is worth noting that Saeednia's strong designated verifier signature scheme fails to guarantee the privacy of the signature, making it unsuitable for certain applications such as medical record certificates or voting systems.","To overcome this limitation, we extend Lee's strong designated verifier signature with a message recovery scheme to develop a universal designated verifier signature scheme.","This universal designated verifier scheme is crafted to safeguard the privacy of signature holders, ensuring that only designated verifiers can authenticate the true signer and recover the messages."],"url":"http://arxiv.org/abs/2403.07820v1","category":"cs.CR"}
{"created":"2024-03-12 16:52:40","title":"Implications of tristability on localization phenomena: a necking bifurcation's tale","abstract":"We analyze the implication of tristability on localization phenomena in one-dimensional extended dissipative systems. In this context, localized states appear due to the interaction and locking of front waves connecting different extended states. In the tristable regime investigated here two extended uniform states coexist with one periodic Turing pattern. This scenario leads to the transition from the standard-homoclinic-snaking-related localized states associated with uniform-pattern bistability to the collapsed-homoclinic-snaking-related states which arise in a uniform-bistable configuration. We find that this transition is mediated by the emergence of hybrid states through codimension-two necking bifurcations. To perform this study we use bifurcation analysis on a non-variational mean-field model describing the spatiotemporal dynamics of light pulses in passive Kerr cavities.","sentences":["We analyze the implication of tristability on localization phenomena in one-dimensional extended dissipative systems.","In this context, localized states appear due to the interaction and locking of front waves connecting different extended states.","In the tristable regime investigated here two extended uniform states coexist with one periodic Turing pattern.","This scenario leads to the transition from the standard-homoclinic-snaking-related localized states associated with uniform-pattern bistability to the collapsed-homoclinic-snaking-related states which arise in a uniform-bistable configuration.","We find that this transition is mediated by the emergence of hybrid states through codimension-two necking bifurcations.","To perform this study we use bifurcation analysis on a non-variational mean-field model describing the spatiotemporal dynamics of light pulses in passive Kerr cavities."],"url":"http://arxiv.org/abs/2403.07814v1","category":"nlin.PS"}
{"created":"2024-03-12 16:46:54","title":"pyvene: A Library for Understanding and Improving PyTorch Models via Interventions","abstract":"Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.","sentences":["Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability.","To facilitate such research, we introduce $\\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules.","$\\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters.","We show how $\\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others.","We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization.","We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene."],"url":"http://arxiv.org/abs/2403.07809v1","category":"cs.LG"}
{"created":"2024-03-12 16:43:55","title":"A Stochastic GDA Method With Backtracking For Solving Nonconvex (Strongly) Concave Minimax Problems","abstract":"We propose a stochastic GDA (gradient descent ascent) method with backtracking (SGDA-B) to solve nonconvex-(strongly) concave (NCC) minimax problems $\\min_x \\max_y \\sum_{i=1}^N g_i(x_i)+f(x,y)-h(y)$, where $h$ and $g_i$ for $i = 1, \\ldots, N$ are closed, convex functions, $f$ is $L$-smooth and $\\mu$-strongly concave in $y$ for some $\\mu\\geq 0$. We consider two scenarios: (i) the deterministic setting where we assume one can compute $\\nabla f$ exactly, and (ii) the stochastic setting where we have only access to $\\nabla f$ through an unbiased stochastic oracle with a finite variance. While most of the existing methods assume knowledge of the Lipschitz constant $L$, SGDA-B is agnostic to $L$. Moreover, SGDA-B can support random block-coordinate updates. In the deterministic setting, SGDA-B can compute an $\\epsilon$-stationary point within $\\mathcal{O}(L\\kappa^2/\\epsilon^2)$ and $\\mathcal{O}(L^3/\\epsilon^4)$ gradient calls when $\\mu>0$ and $\\mu=0$, respectively, where $\\kappa=L/\\mu$. In the stochastic setting, for any $p \\in (0, 1)$ and $\\epsilon >0$, it can compute an $\\epsilon$-stationary point with high probability, which requires $\\mathcal{O}(L\\kappa^3\\epsilon^{-4}\\log(1/p))$ and $\\tilde{\\mathcal{O}}(L^4\\epsilon^{-7}\\log(1/p))$ stochastic oracle calls, with probability at least $1-p$, when $\\mu>0$ and $\\mu=0$, respectively. To our knowledge, SGDA-B is the first GDA-type method with backtracking to solve NCC minimax problems and achieves the best complexity among the methods that are agnostic to $L$. We also provide numerical results for SGDA-B on a distributionally robust learning problem illustrating the potential performance gains that can be achieved by SGDA-B.","sentences":["We propose a stochastic GDA (gradient descent ascent) method with backtracking (SGDA-B) to solve nonconvex-(strongly) concave (NCC) minimax problems $\\min_x \\max_y \\sum_{i=1}^N g_i(x_i)+f(x,y)-h(y)$, where $h$ and $g_i$ for $i = 1, \\ldots, N$ are closed, convex functions, $f$ is $L$-smooth and $\\mu$-strongly concave in $y$ for some $\\mu\\geq 0$.","We consider two scenarios: (i) the deterministic setting where we assume one can compute $\\nabla f$ exactly, and (ii) the stochastic setting where we have only access to $\\nabla f$ through an unbiased stochastic oracle with a finite variance.","While most of the existing methods assume knowledge of the Lipschitz constant $L$, SGDA-B is agnostic to $L$. Moreover, SGDA-B can support random block-coordinate updates.","In the deterministic setting, SGDA-B can compute an $\\epsilon$-stationary point within $\\mathcal{O}(L\\kappa^2/\\epsilon^2)$ and $\\mathcal{O}(L^3/\\epsilon^4)$ gradient calls when $\\mu>0$ and $\\mu=0$, respectively, where $\\kappa=L/\\mu$.","In the stochastic setting, for any $p \\in (0, 1)$ and $\\epsilon >0$, it can compute an $\\epsilon$-stationary point with high probability, which requires $\\mathcal{O}(L\\kappa^3\\epsilon^{-4}\\log(1/p))$ and $\\tilde{\\mathcal{O}}(L^4\\epsilon^{-7}\\log(1/p))$ stochastic oracle calls, with probability at least $1-p$, when $\\mu>0$ and $\\mu=0$, respectively.","To our knowledge, SGDA-B is the first GDA-type method with backtracking to solve NCC minimax problems and achieves the best complexity among the methods that are agnostic to $L$. We also provide numerical results for SGDA-B on a distributionally robust learning problem illustrating the potential performance gains that can be achieved by SGDA-B."],"url":"http://arxiv.org/abs/2403.07806v1","category":"math.OC"}
{"created":"2024-03-12 16:34:03","title":"Second gadolinium loading to Super-Kamiokande","abstract":"The first loading of gadolinium (Gd) into Super-Kamiokande in 2020 was successful, and the neutron capture efficiency on Gd reached 50\\%. To further increase the Gd neutron capture efficiency to 75\\%, 26.1 tons of $\\rm Gd_2(\\rm SO_4)_3\\cdot \\rm 8H_2O$ was additionally loaded into Super-Kamiokande (SK) from May 31 to July 4, 2022. As the amount of loaded $\\rm Gd_2(\\rm SO_4)_3\\cdot \\rm 8H_2O$ was doubled compared to the first loading, the capacity of the powder dissolving system was doubled. We also developed new batches of gadolinium sulfate with even further reduced radioactive impurities. In addition, a more efficient screening method was devised and implemented to evaluate these new batches of $\\rm Gd_2(\\rm SO_4)_3\\cdot \\rm 8H_2O$. Following the second loading, the Gd concentration in SK was measured to be $333.5\\pm2.5$ ppm via an Atomic Absorption Spectrometer (AAS). From the mean neutron capture time constant of neutrons from an Am/Be calibration source, the Gd concentration was independently measured to be 332.7 $\\pm$ 6.8(sys.) $\\pm$ 1.1(stat.) ppm, consistent with the AAS result. Furthermore, during the loading the Gd concentration was monitored continually using the capture time constant of each spallation neutron produced by cosmic-ray muons,and the final neutron capture efficiency was shown to become 1.5 times higher than that of the first loaded phase, as expected.","sentences":["The first loading of gadolinium (Gd) into Super-Kamiokande in 2020 was successful, and the neutron capture efficiency on Gd reached 50\\%.","To further increase the Gd neutron capture efficiency to 75\\%, 26.1 tons of $\\rm Gd_2(\\rm SO_4)_3\\cdot \\rm 8H_2O$ was additionally loaded into Super-Kamiokande (SK) from May 31 to July 4, 2022.","As the amount of loaded $\\rm Gd_2(\\rm SO_4)_3\\cdot \\rm 8H_2O$ was doubled compared to the first loading, the capacity of the powder dissolving system was doubled.","We also developed new batches of gadolinium sulfate with even further reduced radioactive impurities.","In addition, a more efficient screening method was devised and implemented to evaluate these new batches of $\\rm Gd_2(\\rm SO_4)_3\\cdot \\rm 8H_2O$.","Following the second loading, the Gd concentration in SK was measured to be $333.5\\pm2.5$ ppm via an Atomic Absorption Spectrometer (AAS).","From the mean neutron capture time constant of neutrons from an Am/Be calibration source, the Gd concentration was independently measured to be 332.7 $\\pm$ 6.8(sys.)","$\\pm$ 1.1(stat.)","ppm, consistent with the AAS result.","Furthermore, during the loading the Gd concentration was monitored continually using the capture time constant of each spallation neutron produced by cosmic-ray muons,and the final neutron capture efficiency was shown to become 1.5 times higher than that of the first loaded phase, as expected."],"url":"http://arxiv.org/abs/2403.07796v1","category":"physics.ins-det"}
{"created":"2024-03-12 16:33:32","title":"Fine-tuning Neural Network Quantum States","abstract":"Recent progress in the design and optimization of Neural Network Quantum States (NNQS) have made them an effective method to investigate ground-state properties of quantum many-body systems. In contrast to the standard approach of training a separate NNQS from scratch at every point of the phase diagram, we demonstrate that the optimization at a highly expressive point of the phase diagram (i.e., close to a phase transition) yields interpretable features that can be reused to accurately describe a wide region across the transition. We demonstrate the feasibility of our approach on different systems in one and two dimensions by initially pretraining a NNQS at a given point of the phase diagram, followed by fine-tuning only the output layer for all other points. Notably, the computational cost of the fine-tuning step is very low compared to the pretraining stage. We argue that the reduced cost of this paradigm has significant potential to advance the exploration of condensed matter systems using NNQS, mirroring the success of fine-tuning in machine learning and natural language processing.","sentences":["Recent progress in the design and optimization of Neural Network Quantum States (NNQS) have made them an effective method to investigate ground-state properties of quantum many-body systems.","In contrast to the standard approach of training a separate NNQS from scratch at every point of the phase diagram, we demonstrate that the optimization at a highly expressive point of the phase diagram (i.e., close to a phase transition) yields interpretable features that can be reused to accurately describe a wide region across the transition.","We demonstrate the feasibility of our approach on different systems in one and two dimensions by initially pretraining a NNQS at a given point of the phase diagram, followed by fine-tuning only the output layer for all other points.","Notably, the computational cost of the fine-tuning step is very low compared to the pretraining stage.","We argue that the reduced cost of this paradigm has significant potential to advance the exploration of condensed matter systems using NNQS, mirroring the success of fine-tuning in machine learning and natural language processing."],"url":"http://arxiv.org/abs/2403.07795v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-12 16:33:30","title":"Fine-tuning Large Language Models with Sequential Instructions","abstract":"Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tasks involving reasoning, multilingual, and multimodal abilities. To shed further light on our technique, we analyse how adversarial intermediate texts, unseen tasks, prompt verbalization, number of tasks, and prompt length affect SIT. We hope that this method will open new research avenues on instruction tuning for complex tasks.","sentences":["Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it.","This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks.","We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions.","After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tasks involving reasoning, multilingual, and multimodal abilities.","To shed further light on our technique, we analyse how adversarial intermediate texts, unseen tasks, prompt verbalization, number of tasks, and prompt length affect SIT.","We hope that this method will open new research avenues on instruction tuning for complex tasks."],"url":"http://arxiv.org/abs/2403.07794v1","category":"cs.CL"}
{"created":"2024-03-12 16:31:19","title":"Stability of the Favorable Falkner-Skan Profiles for the Stationary Prandtl Equations","abstract":"The (favorable) Falkner-Skan boundary layer profiles are a one parameter ($\\beta \\in [0,2]$) family of self-similar solutions to the stationary Prandtl system which describes the flow over a wedge with angle $\\beta \\frac{\\pi}{2}$. The most famous member of this family is the endpoint Blasius profile, $\\beta = 0$, which exhibits pressureless flow over a flat plate. In contrast, the $\\beta > 0$ profiles are physically expected to exhibit a \\textit{favorable pressure gradient}, a common adage in the physics literature. In this work, we prove quantitative scattering estimates as $x \\rightarrow \\infty$ which precisely captures the effect of this favorable gradient through the presence of new ``CK\" (Cauchy-Kovalevskaya) terms that appear in a quasilinear energy cascade.","sentences":["The (favorable) Falkner-Skan boundary layer profiles are a one parameter ($\\beta \\in [0,2]$) family of self-similar solutions to the stationary Prandtl system which describes the flow over a wedge with angle $\\beta \\frac{\\pi}{2}$. The most famous member of this family is the endpoint Blasius profile, $\\beta = 0$, which exhibits pressureless flow over a flat plate.","In contrast, the $\\beta > 0$ profiles are physically expected to exhibit a \\textit{favorable pressure gradient}, a common adage in the physics literature.","In this work, we prove quantitative scattering estimates as $x \\rightarrow \\infty$ which precisely captures the effect of this favorable gradient through the presence of new ``CK\" (Cauchy-Kovalevskaya) terms that appear in a quasilinear energy cascade."],"url":"http://arxiv.org/abs/2403.07791v1","category":"math.AP"}
{"created":"2024-03-12 16:22:31","title":"Transparent boundary condition and its effectively local approximation for the Schr\u00f6dinger equation on a rectangular computational domain","abstract":"The transparent boundary condition for the free Schr\\\"{o}dinger equation on a rectangular computational domain requires implementation of an operator of the form $\\sqrt{\\partial_t-i\\triangle_{\\Gamma}}$ where $\\triangle_{\\Gamma}$ is the Laplace-Beltrami operator. It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution. The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator. In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps. At the heart of this algorithm is a Pad\\'e approximant based rational approximation of certain fractional operators that handles corners of the domain adequately. For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded. A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary. The proposed novel scheme can be implemented within the framework of any one-step time marching schemes. In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR). For the sake of comparison, we also present a convolution quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard. Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically.","sentences":["The transparent boundary condition for the free Schr\\\"{o}dinger equation on a rectangular computational domain requires implementation of an operator of the form $\\sqrt{\\partial_t-i\\triangle_{\\Gamma}}$ where $\\triangle_{\\Gamma}$ is the Laplace-Beltrami operator.","It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution.","The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator.","In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps.","At the heart of this algorithm is a Pad\\'e approximant based rational approximation of certain fractional operators that handles corners of the domain adequately.","For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded.","A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary.","The proposed novel scheme can be implemented within the framework of any one-step time marching schemes.","In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR).","For the sake of comparison, we also present a convolution quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard.","Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically."],"url":"http://arxiv.org/abs/2403.07787v1","category":"math.NA"}
{"created":"2024-03-12 16:08:47","title":"FairRR: Pre-Processing for Group Fairness through Randomized Response","abstract":"The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness.","sentences":["The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems.","While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain.","This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework.","We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness."],"url":"http://arxiv.org/abs/2403.07780v1","category":"stat.ML"}
{"created":"2024-03-12 16:01:18","title":"Fragmentation of Dense Rotation-Dominated Structures Fed by Collapsing Gravomagneto-Sheetlets and Origin of Misaligned 100 au-Scale Binaries and Multiple Systems","abstract":"The majority of stars are in binary/multiple systems. How such systems form in turbulent, magnetized cores of molecular clouds in the presence of non-ideal MHD effects remains relatively under-explored. Through ATHENA++-based non-ideal MHD AMR simulations with ambipolar diffusion, we show that the collapsing protostellar envelope is dominated by dense gravomagneto-sheetlets, a turbulence-warped version of the classic pseuodisk produced by anisotropic magnetic resistance to the gravitational collapse, in agreement with previous simulations of turbulent, magnetized single-star formation. The sheetlets feed mass, magnetic fields, and angular momentum to a Dense ROtation-Dominated (DROD) structure, which fragments into binary/multiple systems. This DROD fragmentation scenario is a more dynamic variant of the traditional disk fragmentation scenario for binary/multiple formation, with dense spiral filaments created by inhomogeneous feeding from the highly structured larger-scale sheetlets rather than the need for angular momentum transport, which is dominated by magnetic braking. Collisions between the dense spiraling filaments play a key role in pushing the local magnetic Toomre parameter $Q_\\mathrm{m}$ below unity, leading to gravitational collapse and stellar companion formation provided that the local material is sufficiently demagnetized, with a plasma-$\\beta$ of order unity or more. This mechanism can naturally produce {\\it in situ} misaligned systems on the 100-au scale, often detected with high-resolution ALMA observations. Our simulations also highlight the importance of non-ideal MHD effects, which affect whether fragmentation occurs and, if so, the masses and orbital parameters of the stellar companions formed.","sentences":["The majority of stars are in binary/multiple systems.","How such systems form in turbulent, magnetized cores of molecular clouds in the presence of non-ideal MHD effects remains relatively under-explored.","Through ATHENA++-based non-ideal MHD AMR simulations with ambipolar diffusion, we show that the collapsing protostellar envelope is dominated by dense gravomagneto-sheetlets, a turbulence-warped version of the classic pseuodisk produced by anisotropic magnetic resistance to the gravitational collapse, in agreement with previous simulations of turbulent, magnetized single-star formation.","The sheetlets feed mass, magnetic fields, and angular momentum to a Dense ROtation-Dominated (DROD) structure, which fragments into binary/multiple systems.","This DROD fragmentation scenario is a more dynamic variant of the traditional disk fragmentation scenario for binary/multiple formation, with dense spiral filaments created by inhomogeneous feeding from the highly structured larger-scale sheetlets rather than the need for angular momentum transport, which is dominated by magnetic braking.","Collisions between the dense spiraling filaments play a key role in pushing the local magnetic Toomre parameter $Q_\\mathrm{m}$ below unity, leading to gravitational collapse and stellar companion formation provided that the local material is sufficiently demagnetized, with a plasma-$\\beta$ of order unity or more.","This mechanism can naturally produce {\\it in situ} misaligned systems on the 100-au scale, often detected with high-resolution ALMA observations.","Our simulations also highlight the importance of non-ideal MHD effects, which affect whether fragmentation occurs and, if so, the masses and orbital parameters of the stellar companions formed."],"url":"http://arxiv.org/abs/2403.07777v1","category":"astro-ph.SR"}
{"created":"2024-03-12 15:51:10","title":"Supporting Annotators with Affordances for Efficiently Labeling Conversational Data","abstract":"Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.","sentences":["Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data.","Unfortunately, crowdsourced labeling is time consuming and expensive.","To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling.","We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels.","We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet.","Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet."],"url":"http://arxiv.org/abs/2403.07762v1","category":"cs.HC"}
{"created":"2024-03-12 15:45:58","title":"Models of Accidental Dark Matter with a Fundamental Scalar","abstract":"We consider models of accidental dark matter, namely models in which the dark matter is a composite state that is stable thanks to an accidental symmetry of the theory. The fundamental constituents are vectorlike fermions, taken to be fragments of representations of the grand unifying gauge group $SU(5)$, as well as a scalar singlet. All the new fields are charged under a new confining gauge group, which we take to be $SU(N)$, leading to models with complex dark matter. We analyse the models in the context of $SU(5)$ grand unification with a non-standard approach recently proposed in the literature. The advantage of including the scalar mainly resides in the fact that it allows to break several undesired accidental symmetries leading to a larger set of viable models with respect to previous literature, in which only fermions (or only scalars) were considered. Moreover these models present distinct novelties, namely dark states with non-zero baryon and lepton number and the existence of composite hybrid states of fermions and scalars. We identify phenomena that are specific to the inclusion of the scalar and discuss possibilities to test this setup.","sentences":["We consider models of accidental dark matter, namely models in which the dark matter is a composite state that is stable thanks to an accidental symmetry of the theory.","The fundamental constituents are vectorlike fermions, taken to be fragments of representations of the grand unifying gauge group $SU(5)$, as well as a scalar singlet.","All the new fields are charged under a new confining gauge group, which we take to be $SU(N)$, leading to models with complex dark matter.","We analyse the models in the context of $SU(5)$ grand unification with a non-standard approach recently proposed in the literature.","The advantage of including the scalar mainly resides in the fact that it allows to break several undesired accidental symmetries leading to a larger set of viable models with respect to previous literature, in which only fermions (or only scalars) were considered.","Moreover these models present distinct novelties, namely dark states with non-zero baryon and lepton number and the existence of composite hybrid states of fermions and scalars.","We identify phenomena that are specific to the inclusion of the scalar and discuss possibilities to test this setup."],"url":"http://arxiv.org/abs/2403.07759v1","category":"hep-ph"}
{"created":"2024-03-12 15:45:57","title":"HermEIS: A Parallel Multichannel Approach to Rapid Spectral Characterization of Neural MEAs","abstract":"The promise of increasing channel counts in high density ($> 10^4$) neural Microelectrode Arrays (MEAs) for high resolution recording comes with the curse of developing faster characterization strategies for concurrent acquisition of multichannel electrode integrities over a wide frequency spectrum. To circumvent the latency associated with the current multiplexed technique for impedance acquisition, it is common practice to resort to the single frequency impedance measurement (i.e. $Z_{1 \\text{kHz}}$). This, however, does not offer sufficient spectral impedance information crucial for determining the capacity of electrodes at withstanding slow and fast-changing stimulus and recordings. In this work, we present \\textit{HermEIS}, a novel approach that leverages single cycle in-phase and quadrature signal integrations for reducing the massive data throughput characteristic of such high density acquisition systems. As an initial proof-of-concept, we demonstrate over $6$ decades of impedance bandwidth ($5\\times10^{-2} - 5\\times10^{4}\\text{ Hz}$) in a parallel $4$-channel potentiostatic setup composed of a custom PCB with off-the-shelf electronics working in tandem with an FPGA.","sentences":["The promise of increasing channel counts in high density ($> 10^4$) neural Microelectrode Arrays (MEAs) for high resolution recording comes with the curse of developing faster characterization strategies for concurrent acquisition of multichannel electrode integrities over a wide frequency spectrum.","To circumvent the latency associated with the current multiplexed technique for impedance acquisition, it is common practice to resort to the single frequency impedance measurement (i.e. $Z_{1 \\text{kHz}}$).","This, however, does not offer sufficient spectral impedance information crucial for determining the capacity of electrodes at withstanding slow and fast-changing stimulus and recordings.","In this work, we present \\textit{HermEIS}, a novel approach that leverages single cycle in-phase and quadrature signal integrations for reducing the massive data throughput characteristic of such high density acquisition systems.","As an initial proof-of-concept, we demonstrate over $6$ decades of impedance bandwidth ($5\\times10^{-2} - 5\\times10^{4}\\text{ Hz}$) in a parallel $4$-channel potentiostatic setup composed of a custom PCB with off-the-shelf electronics working in tandem with an FPGA."],"url":"http://arxiv.org/abs/2403.07758v1","category":"eess.SP"}
{"created":"2024-03-12 15:41:12","title":"A robust SVM-based approach with feature selection and outliers detection for classification problems","abstract":"This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection. The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features. The search of this classifier is modeled using a mixed-integer formulation with big M parameters. Two different approaches (exact and heuristic) are proposed to solve the model. The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach. In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency.","sentences":["This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection.","The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features.","The search of this classifier is modeled using a mixed-integer formulation with big M parameters.","Two different approaches (exact and heuristic) are proposed to solve the model.","The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach.","In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency."],"url":"http://arxiv.org/abs/2403.07753v1","category":"math.OC"}
{"created":"2024-03-12 15:34:55","title":"Distributed Estimation by Two Agents with Different Feature Spaces","abstract":"We consider the problem of estimation of a function by two agents (and a fusion center) given local data. Data comprises of samples of an independent variable and the corresponding value of a dependent variable. The agents are given a set of features using which they construct suitable function spaces to formulate and solve the estimation problem. The estimated functions are to be uploaded to a fusion space where they are fused to obtain the system estimate of the mapping and then downloaded by the agents to gather knowledge about the other agents estimate of the function. To this end, we present the following: a systematic construction of fusion space given the features of the agents; the derivation of an uploading operator for the agents to upload their estimated functions to a fusion space; an optimization problem in the fusion space to fuse the functions uploaded; the derivation of a downloading operator for the fused function to be downloaded. Through an example on least squares regression, we demonstrate the distributed estimation architecture that has been developed.","sentences":["We consider the problem of estimation of a function by two agents (and a fusion center) given local data.","Data comprises of samples of an independent variable and the corresponding value of a dependent variable.","The agents are given a set of features using which they construct suitable function spaces to formulate and solve the estimation problem.","The estimated functions are to be uploaded to a fusion space where they are fused to obtain the system estimate of the mapping and then downloaded by the agents to gather knowledge about the other agents estimate of the function.","To this end, we present the following: a systematic construction of fusion space given the features of the agents; the derivation of an uploading operator for the agents to upload their estimated functions to a fusion space; an optimization problem in the fusion space to fuse the functions uploaded; the derivation of a downloading operator for the fused function to be downloaded.","Through an example on least squares regression, we demonstrate the distributed estimation architecture that has been developed."],"url":"http://arxiv.org/abs/2403.07749v1","category":"eess.SY"}
{"created":"2024-03-12 15:28:51","title":"Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception","abstract":"Low-cost, vision-centric 3D perception systems for autonomous driving have made significant progress in recent years, narrowing the gap to expensive LiDAR-based methods. The primary challenge in becoming a fully reliable alternative lies in robust depth prediction capabilities, as camera-based systems struggle with long detection ranges and adverse lighting and weather conditions. In this work, we introduce HyDRa, a novel camera-radar fusion architecture for diverse 3D perception tasks. Building upon the principles of dense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid fusion approach to combine the strengths of complementary camera and radar features in two distinct representation spaces. Our Height Association Transformer module leverages radar features already in the perspective view to produce more robust and accurate depth predictions. In the BEV, we refine the initial sparse representation by a Radar-weighted Depth Consistency. HyDRa achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and 58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new semantically rich and spatially accurate BEV features can be directly converted into a powerful occupancy representation, beating all previous camera-based methods on the Occ3D benchmark by an impressive 3.7 mIoU.","sentences":["Low-cost, vision-centric 3D perception systems for autonomous driving have made significant progress in recent years, narrowing the gap to expensive LiDAR-based methods.","The primary challenge in becoming a fully reliable alternative lies in robust depth prediction capabilities, as camera-based systems struggle with long detection ranges and adverse lighting and weather conditions.","In this work, we introduce HyDRa, a novel camera-radar fusion architecture for diverse 3D perception tasks.","Building upon the principles of dense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid fusion approach to combine the strengths of complementary camera and radar features in two distinct representation spaces.","Our Height Association Transformer module leverages radar features already in the perspective view to produce more robust and accurate depth predictions.","In the BEV, we refine the initial sparse representation by a Radar-weighted Depth Consistency.","HyDRa","achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and 58.4 AMOTA (+1.5) on the public nuScenes dataset.","Moreover, our new semantically rich and spatially accurate BEV features can be directly converted into a powerful occupancy representation, beating all previous camera-based methods on the Occ3D benchmark by an impressive 3.7 mIoU."],"url":"http://arxiv.org/abs/2403.07746v1","category":"cs.CV"}
{"created":"2024-03-12 15:15:41","title":"Three statistical descriptions of classical systems and their extensions to hybrid quantum-classical systems","abstract":"We present three statistical descriptions for systems of classical particles and consider their extension to hybrid quantum-classical systems. The classical descriptions are ensembles on configuration space, ensembles on phase space, and a Hilbert space approach using van Hove operators which provides an alternative to the Koopman-von Neumann formulation. In all cases, there is a natural way to define classical observables and a corresponding Lie algebra that is isomorphic to the usual Poisson algebra in phase space. We show that in the case of classical particles, the three descriptions are equivalent and indicate how they are related. We then modify and extend these descriptions to introduce hybrid models where a classical particle interacts with a quantum particle. The approach of ensembles on phase space and the Hilbert space approach, which are novel, lead to equivalent hybrid models, while they are not equivalent to the hybrid model of the approach of ensembles on configuration space. Thus, we end up identifying two inequivalent types of hybrid systems, making different predictions, especially when it comes to entanglement. These results are of interest regarding ``no-go'' theorems about quantum systems interacting via a classical mediator which address the issue of whether gravity must be quantized. Such theorems typically require assumptions that make them model dependent. The hybrid systems that we discuss provide concrete examples of inequivalent models that can be used to compute simple examples to test the assumptions of the ``no-go'' theorems and their applicability.","sentences":["We present three statistical descriptions for systems of classical particles and consider their extension to hybrid quantum-classical systems.","The classical descriptions are ensembles on configuration space, ensembles on phase space, and a Hilbert space approach using van Hove operators which provides an alternative to the Koopman-von Neumann formulation.","In all cases, there is a natural way to define classical observables and a corresponding Lie algebra that is isomorphic to the usual Poisson algebra in phase space.","We show that in the case of classical particles, the three descriptions are equivalent and indicate how they are related.","We then modify and extend these descriptions to introduce hybrid models where a classical particle interacts with a quantum particle.","The approach of ensembles on phase space and the Hilbert space approach, which are novel, lead to equivalent hybrid models, while they are not equivalent to the hybrid model of the approach of ensembles on configuration space.","Thus, we end up identifying two inequivalent types of hybrid systems, making different predictions, especially when it comes to entanglement.","These results are of interest regarding ``no-go'' theorems about quantum systems interacting via a classical mediator which address the issue of whether gravity must be quantized.","Such theorems typically require assumptions that make them model dependent.","The hybrid systems that we discuss provide concrete examples of inequivalent models that can be used to compute simple examples to test the assumptions of the ``no-go'' theorems and their applicability."],"url":"http://arxiv.org/abs/2403.07738v1","category":"quant-ph"}
{"created":"2024-03-12 15:14:14","title":"Photo-induced Ferromagnetic and Superconducting Orders in Multi-orbital Hubbard Models","abstract":"The search for hidden orders in photoexcited lattice systems is an active research field driven by experimental reports of light-induced or light-stabilized phases. In this study, we investigate hidden electronic orders in strongly correlated two-orbital Hubbard models with orbital-dependent bandwidths. In equilibrium, the half-filled systems are antiferromagnetically ordered. Using non-equilibrium dynamical mean field theory we demonstrate the appearance of nonthermal ferromagnetic order in the photo-doped state, if the two bandwidths are sufficiently different, and its coexistence with spin-singlet $\\eta$-superconductivity in the high photo-doping region. Spin-triplet $\\eta$-superconducting order appears instead if the two bandwidths are comparable. The rich nonequilibrium phasediagram uncovered in this work shows that Mott insulating multi-orbital systems provide an interesting platform for the realization of nonthermal electronic orders.","sentences":["The search for hidden orders in photoexcited lattice systems is an active research field driven by experimental reports of light-induced or light-stabilized phases.","In this study, we investigate hidden electronic orders in strongly correlated two-orbital Hubbard models with orbital-dependent bandwidths.","In equilibrium, the half-filled systems are antiferromagnetically ordered.","Using non-equilibrium dynamical mean field theory we demonstrate the appearance of nonthermal ferromagnetic order in the photo-doped state, if the two bandwidths are sufficiently different, and its coexistence with spin-singlet $\\eta$-superconductivity in the high photo-doping region.","Spin-triplet $\\eta$-superconducting order appears instead if the two bandwidths are comparable.","The rich nonequilibrium phasediagram uncovered in this work shows that Mott insulating multi-orbital systems provide an interesting platform for the realization of nonthermal electronic orders."],"url":"http://arxiv.org/abs/2403.07737v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 15:11:50","title":"DESERE: The 1st Workshop on Decentralised Search and Recommendation","abstract":"The DESERE Workshop, our First Workshop on Decentralised Search and Recommendation, offers a platform for researchers to explore and share innovative ideas on decentralised web services, mainly focusing on three major topics: (i) societal impact of decentralised systems: their effect on privacy, policy, and regulation; (ii) decentralising applications: algorithmic and performance challenges that arise from decentralisation; and (iii) infrastructure to support decentralised systems and services: peer-to-peer networks, routing, and performance evaluation tools","sentences":["The DESERE Workshop, our First Workshop on Decentralised Search and Recommendation, offers a platform for researchers to explore and share innovative ideas on decentralised web services, mainly focusing on three major topics: (i) societal impact of decentralised systems: their effect on privacy, policy, and regulation; (ii) decentralising applications: algorithmic and performance challenges that arise from decentralisation; and (iii) infrastructure to support decentralised systems and services: peer-to-peer networks, routing, and performance evaluation tools"],"url":"http://arxiv.org/abs/2403.07732v1","category":"cs.IR"}
{"created":"2024-03-12 15:08:01","title":"Phonon pseudoangular momentum in $\u03b1$-MoO$_3$","abstract":"In recent studies, it has been discovered that phonons can carry angular momentum, leading to a series of investigations into systems with 3-fold rotation symmetry. However, for systems with 2-fold screw rotational symmetry, such as $\\alpha$-MoO$_3$, there has been no relevant discussion. In this paper, we investigated the pseudoangular momentum of phonons in crystals with 2-fold screw rotational symmetry. Taking $\\alpha$-MoO$_3$ as an example, we explain the selection rules in circularly polarized Raman experiments resulting from pseudoangular momentum conservation, providing important guidance for experiments. This study of pseudoangular momentum in $\\alpha$-MoO$_3$ opens up a new degree of freedom for its potential applications, expanding into new application domains.","sentences":["In recent studies, it has been discovered that phonons can carry angular momentum, leading to a series of investigations into systems with 3-fold rotation symmetry.","However, for systems with 2-fold screw rotational symmetry, such as $\\alpha$-MoO$_3$, there has been no relevant discussion.","In this paper, we investigated the pseudoangular momentum of phonons in crystals with 2-fold screw rotational symmetry.","Taking $\\alpha$-MoO$_3$ as an example, we explain the selection rules in circularly polarized Raman experiments resulting from pseudoangular momentum conservation, providing important guidance for experiments.","This study of pseudoangular momentum in $\\alpha$-MoO$_3$ opens up a new degree of freedom for its potential applications, expanding into new application domains."],"url":"http://arxiv.org/abs/2403.07729v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 15:06:24","title":"Tidal dissipation of binaries in asteroid pairs","abstract":"Tidal dissipation in a celestial body can be used to probe its internal structure. Tides govern the orbital evolution of binary systems and therefore constraints on the interior of binary system members can be derived by knowing the age and tidal state of the binary system. For asteroids, age estimates are challenging due to a lack of direct observation of their surface. However, the age of asteroid pairs formed by rotational fission of a parent body can be derived from dynamical modeling, and as such can be used to constrain the age of binary systems existing within asteroid pairs.We study 13 binary asteroid systems existing in asteroid pairs by modeling their tidal locking and eccentricity damping timescales from tidal dissipation in the primaries and secondaries. We consider the impact of thermal torques on these timescales from the YORP and BYORP effects. The resulting constraints on the tidal dissipation ratio Q/k2 are compared to monolithic and rubble pile asteroid theories, showing that all secondaries are consistent with rubble piles with regolith layers greater than 3m and suggest that Q/k2 for rubble piles increases with radius. A particular case is the first bound secondary of asteroid (3749) Balam, whose Q/k2 is constrained to be between 2.7x10^4 and 1.4x10^6, consistent with a rubble-pile with a regolith thickness between 30m and 100m.","sentences":["Tidal dissipation in a celestial body can be used to probe its internal structure.","Tides govern the orbital evolution of binary systems and therefore constraints on the interior of binary system members can be derived by knowing the age and tidal state of the binary system.","For asteroids, age estimates are challenging due to a lack of direct observation of their surface.","However, the age of asteroid pairs formed by rotational fission of a parent body can be derived from dynamical modeling, and as such can be used to constrain the age of binary systems existing within asteroid pairs.","We study 13 binary asteroid systems existing in asteroid pairs by modeling their tidal locking and eccentricity damping timescales from tidal dissipation in the primaries and secondaries.","We consider the impact of thermal torques on these timescales from the YORP and BYORP effects.","The resulting constraints on the tidal dissipation ratio Q/k2 are compared to monolithic and rubble pile asteroid theories, showing that all secondaries are consistent with rubble piles with regolith layers greater than 3m and suggest that Q/k2 for rubble piles increases with radius.","A particular case is the first bound secondary of asteroid (3749)","Balam, whose Q/k2 is constrained to be between 2.7x10^4 and 1.4x10^6, consistent with a rubble-pile with a regolith thickness between 30m and 100m."],"url":"http://arxiv.org/abs/2403.07727v1","category":"astro-ph.EP"}
{"created":"2024-03-12 14:58:32","title":"Resolving Dual Active Galactic Nuclei with ~100 pc separation in MCG-03-34-64","abstract":"Galaxy mergers are expected to play a key role in the evolution of galaxies and their central supermassive black holes (SMBHs). An observational signature of this phenomenon is the detection of dual active galactic nuclei (AGNs) amongst merging systems, as predicted by cosmological models of structure formation. Dual AGNs at sub-kiloparsec-scale separation are the precursors of merging black hole binaries, an important source of gravitational waves, but a paucity of such systems have been confirmed to date by direct imaging, while other similar claims have been strongly disputed. Here we report the serendipitous multiwavelength discovery of a dual black hole system with a separation of ~100 pc, in the gas-rich luminous infrared galaxy MCG-03-34-64 (z=0.016). Chandra/ACIS imaging shows two spatially-resolved peaks of equal intensity in the neutral Fe Ka emission line, consistent with a dual SMBH, which is supported by Hubble Space Telescope (HST), and Very Large Array (VLA) observations. The separation of ~100 pc is the closest dual AGN separation reported to date with spatially-resolved, multiwavelength observations.","sentences":["Galaxy mergers are expected to play a key role in the evolution of galaxies and their central supermassive black holes (SMBHs).","An observational signature of this phenomenon is the detection of dual active galactic nuclei (AGNs) amongst merging systems, as predicted by cosmological models of structure formation.","Dual AGNs at sub-kiloparsec-scale separation are the precursors of merging black hole binaries, an important source of gravitational waves, but a paucity of such systems have been confirmed to date by direct imaging, while other similar claims have been strongly disputed.","Here we report the serendipitous multiwavelength discovery of a dual black hole system with a separation of ~100 pc, in the gas-rich luminous infrared galaxy MCG-03-34-64 (z=0.016).","Chandra/ACIS imaging shows two spatially-resolved peaks of equal intensity in the neutral Fe Ka emission line, consistent with a dual SMBH, which is supported by Hubble Space Telescope (HST), and Very Large Array (VLA) observations.","The separation of ~100 pc is the closest dual AGN separation reported to date with spatially-resolved, multiwavelength observations."],"url":"http://arxiv.org/abs/2403.07717v1","category":"astro-ph.GA"}
{"created":"2024-03-12 14:58:28","title":"Main-sequence exoplanet systems: tidal evolution","abstract":"The easiest exoplanets to detect are those that orbit very close to their hoststars. As a result, even though these planets are quite rare, they represent amajor fraction of the current exoplanet population. A side-effect of theproximity between the planet and the star is that the two have strong mutualinteractions through a number of physical processes. One of the most importantof these processes is tides. Tides are thought to shape the orbits of close-inexoplanets, heat the planet making its radius expand, and even drive someplanets to spiral into their host stars. This chapter briefly introduces thebasics of tidal physics and describes the various fingerprints tides leavewithin the observed exoplanet population.","sentences":["The easiest exoplanets to detect are those that orbit very close to their hoststars.","As a result, even though these planets are quite rare, they represent amajor fraction of the current exoplanet population.","A side-effect of theproximity between the planet and the star is that the two have strong mutualinteractions through a number of physical processes.","One of the most importantof these processes is tides.","Tides are thought to shape the orbits of close-inexoplanets, heat the planet making its radius expand, and even drive someplanets to spiral into their host stars.","This chapter briefly introduces thebasics of tidal physics and describes the various fingerprints tides leavewithin the observed exoplanet population."],"url":"http://arxiv.org/abs/2403.07716v1","category":"astro-ph.EP"}
{"created":"2024-03-12 14:57:40","title":"StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models","abstract":"Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.","sentences":["Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges.","Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks.","However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status.","To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system.","The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status.","Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation.","Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system."],"url":"http://arxiv.org/abs/2403.07714v1","category":"cs.CL"}
{"created":"2024-03-12 14:56:56","title":"Coupling of radiation and magnetospheric accretion flow in ULX pulsars: radiation pressure and photon escape time","abstract":"The accretion flow within the magnetospheric radius of bright X-ray pulsars can form an optically thick envelope, concealing the central neutron star from the distant observer. Most photons are emitted at the surface of a neutron star and leave the system after multiple reflections by the accretion material covering the magnetosphere. Reflections cause momentum to be transferred between photons and the accretion flow, which contributes to the radiative force and should thus influence the dynamics of accretion. We employ Monte Carlo simulations and estimate the acceleration along magnetic field lines due to the radiative force as well as the radiation pressure across magnetic field lines. We demonstrate that the radiative acceleration can exceed gravitational acceleration along the field lines, and similarly, radiation pressure can exceed magnetic field pressure. Multiple reflections of X-ray photons back into the envelope tend to amplify both radiative force along the field lines and radiative pressure. We analyze the average photon escape time from the magnetosphere of a star and show that its absolute value is weakly dependent on the magnetic field strength of a star and roughly linearly dependent on the mass accretion rate being $\\sim 0.1\\,{\\rm s}$ at $\\dot{M}\\sim 10^{20}\\,{\\rm g\\,s^{-1}}$. At high mass accretion rates, the escape time can be longer than free-fall time from the inner disc radius.","sentences":["The accretion flow within the magnetospheric radius of bright X-ray pulsars can form an optically thick envelope, concealing the central neutron star from the distant observer.","Most photons are emitted at the surface of a neutron star and leave the system after multiple reflections by the accretion material covering the magnetosphere.","Reflections cause momentum to be transferred between photons and the accretion flow, which contributes to the radiative force and should thus influence the dynamics of accretion.","We employ Monte Carlo simulations and estimate the acceleration along magnetic field lines due to the radiative force as well as the radiation pressure across magnetic field lines.","We demonstrate that the radiative acceleration can exceed gravitational acceleration along the field lines, and similarly, radiation pressure can exceed magnetic field pressure.","Multiple reflections of X-ray photons back into the envelope tend to amplify both radiative force along the field lines and radiative pressure.","We analyze the average photon escape time from the magnetosphere of a star and show that its absolute value is weakly dependent on the magnetic field strength of a star and roughly linearly dependent on the mass accretion rate being $\\sim 0.1\\,{\\rm s}$ at $\\dot{M}\\sim","10^{20}\\,{\\rm g\\,s^{-1}}$. At high mass accretion rates, the escape time can be longer than free-fall time from the inner disc radius."],"url":"http://arxiv.org/abs/2403.07713v1","category":"astro-ph.HE"}
{"created":"2024-03-12 14:51:23","title":"Fast and Simple Explainability for Point Cloud Networks","abstract":"We propose a fast and simple explainable AI (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \\emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA results, in terms of classification explainability. We demonstrate how the proposed measure is helpful in analyzing and characterizing various aspects of 3D learning, such as rotation invariance, robustness to out-of-distribution (OOD) outliers or domain shift and dataset bias.","sentences":["We propose a fast and simple explainable AI (XAI) method for point cloud data.","It computes pointwise importance with respect to a trained network downstream task.","This allows better understanding of the network properties, which is imperative for safety-critical applications.","In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference.","This can be used to reduce uncertainty and to increase robustness.","In this work, we introduce \\emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck.","We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking.","We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures.","Our approach achieves SOTA results, in terms of classification explainability.","We demonstrate how the proposed measure is helpful in analyzing and characterizing various aspects of 3D learning, such as rotation invariance, robustness to out-of-distribution (OOD) outliers or domain shift and dataset bias."],"url":"http://arxiv.org/abs/2403.07706v1","category":"cs.CV"}
{"created":"2024-03-12 14:48:53","title":"Zitterbewegung CPT violation in neutral kaons system","abstract":"The observed CP violation in neutral kaons experiments is explained as an interplay between two oscillations in the earth's Schwarzschild geometry: (i) mixing associated with second order weak coupling and (ii) quark's zitterbewegung. This violation is in fact a CPT violation with T conservation rather than a T violation with CPT conservation. The Hermitian evolution of a stable kaons system leads to the identification of this CPT violation. Then, the finite lifetime of the short-lived kaon rotates the violation parameter such that it appears as a T violation.","sentences":["The observed CP violation in neutral kaons experiments is explained as an interplay between two oscillations in the earth's Schwarzschild geometry: (i) mixing associated with second order weak coupling and (ii) quark's zitterbewegung.","This violation is in fact a CPT violation with T conservation rather than a T violation with CPT conservation.","The Hermitian evolution of a stable kaons system leads to the identification of this CPT violation.","Then, the finite lifetime of the short-lived kaon rotates the violation parameter such that it appears as a T violation."],"url":"http://arxiv.org/abs/2403.07703v1","category":"hep-ph"}
{"created":"2024-03-12 14:38:46","title":"Precision timing of eclipsing binaries from TESS full frame images. Method and performance","abstract":"Several hundreds of thousands of eclipsing binaries (EBs) are expected to be detected in the Transiting Exoplanet Survey Satellite (TESS) full frame images (FFIs). This represents a significant increase in the number of EBs available for eclipse timing variation studies. In this paper, we investigate the feasibility of performing precise eclipse timing of TESS EBs using the FFIs. To this end, we developed a fast, automated method and applied it to a sample of $\\sim$100 EBs selected from the Villanova TESS EB catalog. Our timing analysis resulted in the detection of ten new triple candidates with outer periods shorter than $\\sim$1300$\\,$d. For five of them, we were able to constrain the outer orbit by analyzing independently the short-cadence (SC) and FFI data and to derive the minimum mass of the third body with a precision better than 4 per cent for SC and 12 per cent for FFI data. We then compared the results obtained from the two datasets and found that using the FFI data leads to (1) a degradation of both the accuracy and precision of the tertiary mass determination for the tightest EBs and (2) an overall underestimation of the third component's mass. However, we stress that our main conclusions on the nature of the detected signals do not depend on which dataset is used. This confirms the great potential of TESS FFIs, which will allow us to search for rare objects such as substellar circumbinary companions and compact triple stellar systems.","sentences":["Several hundreds of thousands of eclipsing binaries (EBs) are expected to be detected in the Transiting Exoplanet Survey Satellite (TESS) full frame images (FFIs).","This represents a significant increase in the number of EBs available for eclipse timing variation studies.","In this paper, we investigate the feasibility of performing precise eclipse timing of TESS EBs using the FFIs.","To this end, we developed a fast, automated method and applied it to a sample of $\\sim$100 EBs selected from the Villanova TESS EB catalog.","Our timing analysis resulted in the detection of ten new triple candidates with outer periods shorter than $\\sim$1300$\\,$d.","For five of them, we were able to constrain the outer orbit by analyzing independently the short-cadence (SC) and FFI data and to derive the minimum mass of the third body with a precision better than 4 per cent for SC and 12 per cent for FFI data.","We then compared the results obtained from the two datasets and found that using the FFI data leads to (1) a degradation of both the accuracy and precision of the tertiary mass determination for the tightest EBs and (2) an overall underestimation of the third component's mass.","However, we stress that our main conclusions on the nature of the detected signals do not depend on which dataset is used.","This confirms the great potential of TESS FFIs, which will allow us to search for rare objects such as substellar circumbinary companions and compact triple stellar systems."],"url":"http://arxiv.org/abs/2403.07694v1","category":"astro-ph.SR"}
{"created":"2024-03-12 14:23:00","title":"On fluctuations of complexity measures for the FIND algorithm","abstract":"The FIND algorithm (also called Quickselect) is a fundamental algorithm to select ranks or quantiles within a set of data. It was shown by Gr\\\"ubel and R\\\"osler that the number of key comparisons required by Find as a process of the quantiles $\\alpha\\in[0,1]$ in a natural probabilistic model converges after normalization in distribution within the c\\`adl\\`ag space $D[0,1]$ endowed with the Skorokhod metric. We show that the process of the residuals in the latter convergence after normalization converges in distribution to a mixture of Gaussian processes in $D[0,1]$ and identify the limit's conditional covariance functions. A similar result holds for the related algorithm QuickVal. Our method extends to other cost measures such as the number of swaps (key exchanges) required by Find or cost measures which are based on key comparisons but take into account that the cost of a comparison between two keys may depend on their values, an example being the number of bit comparisons needed to compare keys given by their bit expansions.","sentences":["The FIND algorithm (also called Quickselect) is a fundamental algorithm to select ranks or quantiles within a set of data.","It was shown by Gr\\\"ubel and R\\\"osler that the number of key comparisons required by Find as a process of the quantiles $\\alpha\\in[0,1]$ in a natural probabilistic model converges after normalization in distribution within the c\\`adl\\`ag space $D[0,1]$ endowed with the Skorokhod metric.","We show that the process of the residuals in the latter convergence after normalization converges in distribution to a mixture of Gaussian processes in $D[0,1]$ and identify the limit's conditional covariance functions.","A similar result holds for the related algorithm QuickVal.","Our method extends to other cost measures such as the number of swaps (key exchanges) required by Find or cost measures which are based on key comparisons but take into account that the cost of a comparison between two keys may depend on their values, an example being the number of bit comparisons needed to compare keys given by their bit expansions."],"url":"http://arxiv.org/abs/2403.07685v1","category":"math.PR"}
{"created":"2024-03-12 14:18:20","title":"Feasible climate policies in a democracy with a climate-denying party","abstract":"Climate policy has become increasingly politicized in many countries including the US, with some political parties unwilling to pursue strong measures. Therefore, to be successful in mitigation, climate policies must be politically feasible. Currently, climate mitigation pathways are explored in so-called Integrated Assessment Models (IAMs) which evaluate climate policies from an economic perspective, typically focusing on cost-effectiveness and overlooking transition costs. However, the economy is intertwined with the political system, in which policymakers impose economic policies, but are (in democracies) dependent on public opinion, which in turn can be influenced by economic performance. In cases where some parties are much less ambitious in climate mitigation than others, climate policy can be abruptly disrupted, influencing voting behaviour. In this study, we analyze the political feasibility of a set of green policies in case some parties are strongly unwilling to protect the climate. We show that this simple additional social layer of complexity largely affects the outcome of the abatement measures. In particular, we conclude that a (high) pure carbon tax is particularly vulnerable to abrupt interruptions and its economic side effects discourage votes for green parties. Nevertheless, a strategically selected combination of policies can reduce political uncertainty, resulting in a more feasible and effective mitigation measure.","sentences":["Climate policy has become increasingly politicized in many countries including the US, with some political parties unwilling to pursue strong measures.","Therefore, to be successful in mitigation, climate policies must be politically feasible.","Currently, climate mitigation pathways are explored in so-called Integrated Assessment Models (IAMs) which evaluate climate policies from an economic perspective, typically focusing on cost-effectiveness and overlooking transition costs.","However, the economy is intertwined with the political system, in which policymakers impose economic policies, but are (in democracies) dependent on public opinion, which in turn can be influenced by economic performance.","In cases where some parties are much less ambitious in climate mitigation than others, climate policy can be abruptly disrupted, influencing voting behaviour.","In this study, we analyze the political feasibility of a set of green policies in case some parties are strongly unwilling to protect the climate.","We show that this simple additional social layer of complexity largely affects the outcome of the abatement measures.","In particular, we conclude that a (high) pure carbon tax is particularly vulnerable to abrupt interruptions and its economic side effects discourage votes for green parties.","Nevertheless, a strategically selected combination of policies can reduce political uncertainty, resulting in a more feasible and effective mitigation measure."],"url":"http://arxiv.org/abs/2403.07681v1","category":"physics.soc-ph"}
{"created":"2024-03-12 14:11:29","title":"Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers","abstract":"In this work, we extend our previously proposed offline SpatialNet for long-term streaming multichannel speech enhancement in both static and moving speaker scenarios. SpatialNet exploits spatial information, such as the spatial/steering direction of speech, for discriminating between target speech and interferences, and achieved outstanding performance. The core of SpatialNet is a narrow-band self-attention module used for learning the temporal dynamic of spatial vectors. Towards long-term streaming speech enhancement, we propose to replace the offline self-attention network with online networks that have linear inference complexity w.r.t signal length and meanwhile maintain the capability of learning long-term information. Three variants are developed based on (i) masked self-attention, (ii) Retention, a self-attention variant with linear inference complexity, and (iii) Mamba, a structured-state-space-based RNN-like network. Moreover, we investigate the length extrapolation ability of different networks, namely test on signals that are much longer than training signals, and propose a short-signal training plus long-signal fine-tuning strategy, which largely improves the length extrapolation ability of the networks within limited training time. Overall, the proposed online SpatialNet achieves outstanding speech enhancement performance for long audio streams, and for both static and moving speakers. The proposed method will be open-sourced in https://github.com/Audio-WestlakeU/NBSS.","sentences":["In this work, we extend our previously proposed offline SpatialNet for long-term streaming multichannel speech enhancement in both static and moving speaker scenarios.","SpatialNet exploits spatial information, such as the spatial/steering direction of speech, for discriminating between target speech and interferences, and achieved outstanding performance.","The core of SpatialNet is a narrow-band self-attention module used for learning the temporal dynamic of spatial vectors.","Towards long-term streaming speech enhancement, we propose to replace the offline self-attention network with online networks that have linear inference complexity w.r.t signal length and meanwhile maintain the capability of learning long-term information.","Three variants are developed based on (i) masked self-attention, (ii) Retention, a self-attention variant with linear inference complexity, and (iii) Mamba, a structured-state-space-based RNN-like network.","Moreover, we investigate the length extrapolation ability of different networks, namely test on signals that are much longer than training signals, and propose a short-signal training plus long-signal fine-tuning strategy, which largely improves the length extrapolation ability of the networks within limited training time.","Overall, the proposed online SpatialNet achieves outstanding speech enhancement performance for long audio streams, and for both static and moving speakers.","The proposed method will be open-sourced in https://github.com/Audio-WestlakeU/NBSS."],"url":"http://arxiv.org/abs/2403.07675v1","category":"cs.SD"}
{"created":"2024-03-12 14:11:24","title":"The frequency problem of the three gap theorem","abstract":"The three gap theorem was originally a conjecture by Steinhaus, who asserted that there are at most three distinct gap lengths in the fractional parts of the sequence {\\alpha},{2}{\\alpha},{\\cdots},{N}{\\alpha} for any integer {N} and real number {\\alpha}. This conjecture has been proved by many different methods, and extended to higher dimensional cases. For the three gap theorem, what is the frequence of two gaps or three gaps? On the basis of Ravenstein's work, this paper studies the frequency of the three gap theorem through cake model. We show that for almost all {\\alpha\\in (0,1)\\backslash \\mathbb {Q}} in the sence of the Lebesgue measure, the frequency of the two gaps is {0}, in other words, the frequency of the three gaps is {1}. The proof makes full use of the continued fraction expansion of {\\alpha}, as well as partial results in the Diophantine approximation. Finally, we illustrate that two gaps occur with a frequency of {0}.","sentences":["The three gap theorem was originally a conjecture by Steinhaus, who asserted that there are at most three distinct gap lengths in the fractional parts of the sequence {\\alpha},{2}{\\alpha},{\\cdots},{N}{\\alpha} for any integer {N} and real number {\\alpha}.","This conjecture has been proved by many different methods, and extended to higher dimensional cases.","For the three gap theorem, what is the frequence of two gaps or three gaps?","On the basis of Ravenstein's work, this paper studies the frequency of the three gap theorem through cake model.","We show that for almost all {\\alpha\\in (0,1)\\backslash \\mathbb {Q}} in the sence of the Lebesgue measure, the frequency of the two gaps is {0}, in other words, the frequency of the three gaps is {1}.","The proof makes full use of the continued fraction expansion of {\\alpha}, as well as partial results in the Diophantine approximation.","Finally, we illustrate that two gaps occur with a frequency of {0}."],"url":"http://arxiv.org/abs/2403.07674v1","category":"math.NT"}
{"created":"2024-03-12 14:06:01","title":"Quantitative estimates in almost periodic homogenization of parabolic systems","abstract":"We consider a family of second-order parabolic operators $\\partial_t+\\mathcal{L}_\\varepsilon$ in divergence form with rapidly oscillating, time-dependent and almost-periodic coefficients. We establish uniform interior and boundary H\\\"older and Lipschitz estimates as well as convergence rate. The estimates of fundamental solution and Green's function are also established. In contrast to periodic case, the main difficulty is that the corrector equation $ (\\partial_s+\\mathcal{L}_1)(\\chi^\\beta_{j})=-\\mathcal{L}_1(P^\\beta_j) $ in $\\mathbb{R}^{d+1}$ may not be solvable in the almost periodic setting for linear functions $P(y)$ and $\\partial_t \\chi_S$ may not in $B^2(\\mathbb{R}^{d+1})$. Our results are new even in the case of time-independent coefficients.","sentences":["We consider a family of second-order parabolic operators $\\partial_t+\\mathcal{L}_\\varepsilon$ in divergence form with rapidly oscillating, time-dependent and almost-periodic coefficients.","We establish uniform interior and boundary H\\\"older and Lipschitz estimates as well as convergence rate.","The estimates of fundamental solution and Green's function are also established.","In contrast to periodic case, the main difficulty is that the corrector equation $ (\\partial_s+\\mathcal{L}_1)(\\chi^\\beta_{j})=-\\mathcal{L}_1(P^\\beta_j) $ in $\\mathbb{R}^{d+1}$ may not be solvable in the almost periodic setting for linear functions $P(y)$ and $\\partial_t \\chi_S$ may not in $B^2(\\mathbb{R}^{d+1})$. Our results are new even in the case of time-independent coefficients."],"url":"http://arxiv.org/abs/2403.07672v1","category":"math.AP"}
{"created":"2024-03-12 14:02:15","title":"Linear and non-linear integrate-and-fire neurons driven by synaptic shot noise with reversal potentials","abstract":"The steady-state firing rate and firing-rate response of the leaky and exponential integrate-and-fire models receiving synaptic shot noise with excitatory and inhibitory reversal potentials is examined. For the particular case where the underlying synaptic conductances are exponentially distributed, it is shown that the master equation for a population of such model neurons can be reduced from an integro-differential form to a more tractable set of three differential equations. The system is nevertheless more challenging analytically than for current-based synapses: where possible analytical results are provided with an efficient numerical scheme and code provided for other quantities. The increased tractability of the framework developed supports an ongoing critical comparison between models in which synapses are treated with and without reversal potentials, such as recently in the context of networks with balanced excitatory and inhibitory conductances.","sentences":["The steady-state firing rate and firing-rate response of the leaky and exponential integrate-and-fire models receiving synaptic shot noise with excitatory and inhibitory reversal potentials is examined.","For the particular case where the underlying synaptic conductances are exponentially distributed, it is shown that the master equation for a population of such model neurons can be reduced from an integro-differential form to a more tractable set of three differential equations.","The system is nevertheless more challenging analytically than for current-based synapses: where possible analytical results are provided with an efficient numerical scheme and code provided for other quantities.","The increased tractability of the framework developed supports an ongoing critical comparison between models in which synapses are treated with and without reversal potentials, such as recently in the context of networks with balanced excitatory and inhibitory conductances."],"url":"http://arxiv.org/abs/2403.07670v1","category":"q-bio.NC"}
{"created":"2024-03-12 14:00:50","title":"Machine Learning for Soccer Match Result Prediction","abstract":"Machine learning has become a common approach to predicting the outcomes of soccer matches, and the body of literature in this domain has grown substantially in the past decade and a half. This chapter discusses available datasets, the types of models and features, and ways of evaluating model performance in this application domain. The aim of this chapter is to give a broad overview of the current state and potential future developments in machine learning for soccer match results prediction, as a resource for those interested in conducting future studies in the area. Our main findings are that while gradient-boosted tree models such as CatBoost, applied to soccer-specific ratings such as pi-ratings, are currently the best-performing models on datasets containing only goals as the match features, there needs to be a more thorough comparison of the performance of deep learning models and Random Forest on a range of datasets with different types of features. Furthermore, new rating systems using both player- and team-level information and incorporating additional information from, e.g., spatiotemporal tracking and event data, could be investigated further. Finally, the interpretability of match result prediction models needs to be enhanced for them to be more useful for team management.","sentences":["Machine learning has become a common approach to predicting the outcomes of soccer matches, and the body of literature in this domain has grown substantially in the past decade and a half.","This chapter discusses available datasets, the types of models and features, and ways of evaluating model performance in this application domain.","The aim of this chapter is to give a broad overview of the current state and potential future developments in machine learning for soccer match results prediction, as a resource for those interested in conducting future studies in the area.","Our main findings are that while gradient-boosted tree models such as CatBoost, applied to soccer-specific ratings such as pi-ratings, are currently the best-performing models on datasets containing only goals as the match features, there needs to be a more thorough comparison of the performance of deep learning models and Random Forest on a range of datasets with different types of features.","Furthermore, new rating systems using both player- and team-level information and incorporating additional information from, e.g., spatiotemporal tracking and event data, could be investigated further.","Finally, the interpretability of match result prediction models needs to be enhanced for them to be more useful for team management."],"url":"http://arxiv.org/abs/2403.07669v1","category":"cs.LG"}
{"created":"2024-03-12 13:52:32","title":"Signatures of correlated defects in an ultra-clean Wigner crystal in the extreme quantum limit","abstract":"Low-disorder two-dimensional electron systems in the presence of a strong, perpendicular magnetic field terminate at very small Landau level filling factors in a Wigner crystal (WC), where the electrons form an ordered array to minimize the Coulomb repulsion. The nature of this exotic, many-body, quantum phase is yet to be fully understood and experimentally revealed. Here we probe one of WC's most fundamental parameters, namely the energy gap that determines its low-temperature conductivity, in record-mobility, ultra-high-purity, two-dimensional electrons confined to GaAs quantum wells. The WC domains in these samples contain $\\simeq$ 1000 electrons. The measured gaps are a factor of three larger than previously reported for lower quality samples, and agree remarkably well with values predicted for the lowest-energy, intrinsic, hyper-corelated bubble defects in a WC made of flux-electron composite fermions, rather than bare electrons. The agreement is particularly noteworthy, given that the calculations are done for disorder-free composite fermion WCs, and there are no adjustable parameters. The results reflect the exceptionally high quality of the samples, and suggest that composite fermion WCs are indeed more stable compared to their electron counterparts.","sentences":["Low-disorder two-dimensional electron systems in the presence of a strong, perpendicular magnetic field terminate at very small Landau level filling factors in a Wigner crystal (WC), where the electrons form an ordered array to minimize the Coulomb repulsion.","The nature of this exotic, many-body, quantum phase is yet to be fully understood and experimentally revealed.","Here we probe one of WC's most fundamental parameters, namely the energy gap that determines its low-temperature conductivity, in record-mobility, ultra-high-purity, two-dimensional electrons confined to GaAs quantum wells.","The WC domains in these samples contain $\\simeq$ 1000 electrons.","The measured gaps are a factor of three larger than previously reported for lower quality samples, and agree remarkably well with values predicted for the lowest-energy, intrinsic, hyper-corelated bubble defects in a WC made of flux-electron composite fermions, rather than bare electrons.","The agreement is particularly noteworthy, given that the calculations are done for disorder-free composite fermion WCs, and there are no adjustable parameters.","The results reflect the exceptionally high quality of the samples, and suggest that composite fermion WCs are indeed more stable compared to their electron counterparts."],"url":"http://arxiv.org/abs/2403.07662v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 13:50:01","title":"Broadcasting Quantum Information using Finite Resources","abstract":"Measurements can be viewed as interactions between systems and specifically prepared pointers. Ideally, these interactions create accurate copies of the information corresponding to the diagonal of the system's density operator with respect to the measurement basis. However, establishing measurement outcomes as objective facts requires redundancy. We therefore consider the problem of unitarily distributing this information to several quantum memories. We show that the accuracy of this broadcasting process is limited by thermodynamic restrictions on preparing the memories in pure states: ideal broadcasting is impossible using finite resources. For finite-temperature memories we put forward a lower bound on the entropy production of the broadcasting process. This Holevo-Landauer bound demonstrates that the mixedness of the initial memory limits the ability to accurately broadcast information to more than one memory component, thus fundamentally restricting the creation of redundancies while maintaining the integrity of the original information. Finally, we show how the full information can be recovered in the classical limit -- via coarse-graining or asymptotically as the number of subsystems of each memory component increases -- thus elucidating how objective properties can emerge despite inherent imperfections.","sentences":["Measurements can be viewed as interactions between systems and specifically prepared pointers.","Ideally, these interactions create accurate copies of the information corresponding to the diagonal of the system's density operator with respect to the measurement basis.","However, establishing measurement outcomes as objective facts requires redundancy.","We therefore consider the problem of unitarily distributing this information to several quantum memories.","We show that the accuracy of this broadcasting process is limited by thermodynamic restrictions on preparing the memories in pure states: ideal broadcasting is impossible using finite resources.","For finite-temperature memories we put forward a lower bound on the entropy production of the broadcasting process.","This Holevo-Landauer bound demonstrates that the mixedness of the initial memory limits the ability to accurately broadcast information to more than one memory component, thus fundamentally restricting the creation of redundancies while maintaining the integrity of the original information.","Finally, we show how the full information can be recovered in the classical limit -- via coarse-graining or asymptotically as the number of subsystems of each memory component increases -- thus elucidating how objective properties can emerge despite inherent imperfections."],"url":"http://arxiv.org/abs/2403.07660v1","category":"quant-ph"}
{"created":"2024-03-12 13:46:47","title":"Optimal control of stochastic cylinder flow using data-driven compressive sensing method","abstract":"A stochastic optimal control problem for incompressible Newtonian channel flow past a circular cylinder is used as a prototype optimal control problem for the stochastic Navier-Stokes equations. The inlet flow and the rotation speed of the cylinder are allowed to have stochastic perturbations. The control acts on the cylinder via adjustment of the rotation speed. Possible objectives of the control include, among others, tracking a desired (given) velocity field or minimizing the kinetic energy, enstrophy, or the drag of the flow over a given body. Owing to the high computational requirements, the direct application of the classical Monte Carlo methods for our problem is limited. To overcome the difficulty, we use a multi-fidelity data-driven compressive sensing based polynomial chaos expansions (MDCS-PCE). An effective gradient-based optimization for the discrete optimality systems resulted from the MDCS-PCE discretization is developed. The strategy can be applied broadly to many stochastic flow control problems. Numerical tests are performed to validate our methodology.","sentences":["A stochastic optimal control problem for incompressible Newtonian channel flow past a circular cylinder is used as a prototype optimal control problem for the stochastic Navier-Stokes equations.","The inlet flow and the rotation speed of the cylinder are allowed to have stochastic perturbations.","The control acts on the cylinder via adjustment of the rotation speed.","Possible objectives of the control include, among others, tracking a desired (given) velocity field or minimizing the kinetic energy, enstrophy, or the drag of the flow over a given body.","Owing to the high computational requirements, the direct application of the classical Monte Carlo methods for our problem is limited.","To overcome the difficulty, we use a multi-fidelity data-driven compressive sensing based polynomial chaos expansions (MDCS-PCE).","An effective gradient-based optimization for the discrete optimality systems resulted from the MDCS-PCE discretization is developed.","The strategy can be applied broadly to many stochastic flow control problems.","Numerical tests are performed to validate our methodology."],"url":"http://arxiv.org/abs/2403.07656v1","category":"math.OC"}
{"created":"2024-03-12 13:45:29","title":"Enhancing Physical Layer Security in Dual-Function Radar-Communication Systems with Hybrid Beamforming Architecture","abstract":"In this letter, we investigate enhancing the physical layer security (PLS) for the dual-function radar-communication (DFRC) system with hybrid beamforming (HBF) architecture, where the base station (BS) achieves downlink communication and radar target detection simultaneously. We consider an eavesdropper intercepting the information transmitted from the BS to the downlink communication users with imperfectly known channel state information. Additionally, the location of the radar target is also imperfectly known by the BS. To enhance PLS in the considered DFRC system, we propose a novel HBF architecture, which introduces a new integrated sensing and security (I2S) symbol. The secure HBF design problem for DFRC is formulated by maximizing the minimum legitimate user communication rate subject to radar interference-plus-noise ratio, eavesdropping rate, hardware and power constraints. To solve this non-convex problem, we propose an alternating optimization based method to jointly optimize transmit and receive beamformers. Numerical simulation results validate the effectiveness of the proposed algorithm and show the superiority of the proposed I2S-aided HBF architecture for achieving DFRC and enhancing PLS.","sentences":["In this letter, we investigate enhancing the physical layer security (PLS) for the dual-function radar-communication (DFRC) system with hybrid beamforming (HBF) architecture, where the base station (BS) achieves downlink communication and radar target detection simultaneously.","We consider an eavesdropper intercepting the information transmitted from the BS to the downlink communication users with imperfectly known channel state information.","Additionally, the location of the radar target is also imperfectly known by the BS.","To enhance PLS in the considered DFRC system, we propose a novel HBF architecture, which introduces a new integrated sensing and security (I2S) symbol.","The secure HBF design problem for DFRC is formulated by maximizing the minimum legitimate user communication rate subject to radar interference-plus-noise ratio, eavesdropping rate, hardware and power constraints.","To solve this non-convex problem, we propose an alternating optimization based method to jointly optimize transmit and receive beamformers.","Numerical simulation results validate the effectiveness of the proposed algorithm and show the superiority of the proposed I2S-aided HBF architecture for achieving DFRC and enhancing PLS."],"url":"http://arxiv.org/abs/2403.07655v1","category":"eess.SP"}
{"created":"2024-03-12 13:45:20","title":"Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models","abstract":"Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which do not rely on natural language prompt tokens), albeit to a lesser extent.","sentences":["Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding.","However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true.","Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models.","Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected.","Remarkably, the attacks also affect encoder-only relevance models (which do not rely on natural language prompt tokens), albeit to a lesser extent."],"url":"http://arxiv.org/abs/2403.07654v1","category":"cs.IR"}
{"created":"2024-03-12 13:41:15","title":"Harder Tasks Need More Experts: Dynamic Routing in MoE Models","abstract":"In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input's complexity. Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE.","sentences":["In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty.","Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input.","This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks.","Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters.","Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input's complexity.","Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks.","The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE."],"url":"http://arxiv.org/abs/2403.07652v1","category":"cs.LG"}
{"created":"2024-03-12 13:30:53","title":"Expiring opacity problems in parametric timed automata","abstract":"Information leakage can have dramatic consequences on the security of real-time systems. Timing leaks occur when an attacker is able to infer private behavior depending on timing information. In this work, we propose a definition of expiring timed opacity w.r.t. execution time, where a system is opaque whenever the attacker is unable to deduce the reachability of some private state solely based on the execution time; in addition, the secrecy is violated only when the private state was entered \"recently\", i.e., within a given time bound (or expiration date) prior to system completion. This has an interesting parallel with concrete applications, notably cache deducibility: it may be useless for the attacker to know the cache content too late after its observance. We study here expiring timed opacity problems in timed automata. We consider the set of time bounds (or expiration dates) for which a system is opaque and show when they can be effectively computed for timed automata. We then study the decidability of several parameterized problems, when not only the bounds, but also some internal timing constants become timing parameters of unknown constant values.","sentences":["Information leakage can have dramatic consequences on the security of real-time systems.","Timing leaks occur when an attacker is able to infer private behavior depending on timing information.","In this work, we propose a definition of expiring timed opacity w.r.t. execution time, where a system is opaque whenever the attacker is unable to deduce the reachability of some private state solely based on the execution time; in addition, the secrecy is violated only when the private state was entered \"recently\", i.e., within a given time bound (or expiration date) prior to system completion.","This has an interesting parallel with concrete applications, notably cache deducibility: it may be useless for the attacker to know the cache content too late after its observance.","We study here expiring timed opacity problems in timed automata.","We consider the set of time bounds (or expiration dates) for which a system is opaque and show when they can be effectively computed for timed automata.","We then study the decidability of several parameterized problems, when not only the bounds, but also some internal timing constants become timing parameters of unknown constant values."],"url":"http://arxiv.org/abs/2403.07647v1","category":"cs.LO"}
{"created":"2024-03-12 13:23:40","title":"A Framework for Controlling Multiple Industrial Robots using Mobile Applications","abstract":"Purpose: Over the last few decades, the development of the hardware and software has enabled the application of advanced systems. In the robotics field, the UI design is an intriguing area to be explored due to the creation of devices with a wide range of functionalities in a reduced size. Moreover, the idea of using the same UI to control several systems arouses a great interest considering that this involves less learning effort and time for the users. Therefore, this paper will present a mobile application to control two industrial robots with four modes of operation. Design/methodology/approach: The smartphone was selected to be the interface due to its wide range of capabilities and the MIT Inventor App was used to create the application, whose environment is supported by Android smartphones. For the validation, ROS was used since it is a fundamental framework utilised in industrial robotics and the Arduino Uno was used to establish the data transmission between the smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical interface was created to visualize the options available in the app whereas two scripts in python were programmed to perform the simulations in ROS and carry out the tests. Findings: The results indicated that the use of the sliders to control the robots is more favourable than the Orientation Sensor due to the sensibility of the sensor and human limitations to hold the smartphone perfectly still. Another important finding was the limitations of the autonomous mode, in which the robot grabs an object. In this case, the configuration of the Kinect camera and the controllers has a significant impact on the success of the simulation. Finally, it was observed that the delay was appropriate despite the use of the Arduino UNO to transfer the data between the Smartphone and the Nvidia Jetson TX2.","sentences":["Purpose:","Over the last few decades, the development of the hardware and software has enabled the application of advanced systems.","In the robotics field, the UI design is an intriguing area to be explored due to the creation of devices with a wide range of functionalities in a reduced size.","Moreover, the idea of using the same UI to control several systems arouses a great interest considering that this involves less learning effort and time for the users.","Therefore, this paper will present a mobile application to control two industrial robots with four modes of operation.","Design/methodology/approach: The smartphone was selected to be the interface due to its wide range of capabilities and the MIT Inventor App was used to create the application, whose environment is supported by Android smartphones.","For the validation, ROS was used since it is a fundamental framework utilised in industrial robotics and the Arduino Uno was used to establish the data transmission between the smartphone and the board NVIDIA Jetson TX2.","In MIT Inventor App, the graphical interface was created to visualize the options available in the app whereas two scripts in python were programmed to perform the simulations in ROS and carry out the tests.","Findings:","The results indicated that the use of the sliders to control the robots is more favourable than the Orientation Sensor due to the sensibility of the sensor and human limitations to hold the smartphone perfectly still.","Another important finding was the limitations of the autonomous mode, in which the robot grabs an object.","In this case, the configuration of the Kinect camera and the controllers has a significant impact on the success of the simulation.","Finally, it was observed that the delay was appropriate despite the use of the Arduino UNO to transfer the data between the Smartphone and the Nvidia Jetson TX2."],"url":"http://arxiv.org/abs/2403.07639v1","category":"cs.RO"}
{"created":"2024-03-12 13:23:31","title":"Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models","abstract":"Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal.","sentences":["Robotic manipulation relies on analytical or learned models to simulate the system dynamics.","These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle).","In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response.","We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning.","Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions.","To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones.","This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones.","Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal."],"url":"http://arxiv.org/abs/2403.07638v1","category":"cs.RO"}
{"created":"2024-03-12 13:16:29","title":"A Study on Centralised and Decentralised Swarm Robotics Architecture for Part Delivery System","abstract":"Drones are also known as UAVs are originally designed for military purposes. With the technological advances, they can be seen in most of the aspects of life from filming to logistics. The increased use of drones made it sometimes essential to form a collaboration between them to perform the task efficiently in a defined process. This paper investigates the use of a combined centralised and decentralised architecture for the collaborative operation of drones in a parts delivery scenario to enable and expedite the operation of the factories of the future. The centralised and decentralised approaches were extensively researched, with experimentation being undertaken to determine the appropriateness of each approach for this use-case. Decentralised control was utilised to remove the need for excessive communication during the operation of the drones, resulting in smoother operations. Initial results suggested that the decentralised approach is more appropriate for this use-case. The individual functionalities necessary for the implementation of a decentralised architecture were proven and assessed, determining that a combination of multiple individual functionalities, namely VSLAM, dynamic collision avoidance and object tracking, would give an appropriate solution for use in an industrial setting. A final architecture for the parts delivery system was proposed for future work, using a combined centralised and decentralised approach to combat the limitations inherent in each architecture.","sentences":["Drones are also known as UAVs are originally designed for military purposes.","With the technological advances, they can be seen in most of the aspects of life from filming to logistics.","The increased use of drones made it sometimes essential to form a collaboration between them to perform the task efficiently in a defined process.","This paper investigates the use of a combined centralised and decentralised architecture for the collaborative operation of drones in a parts delivery scenario to enable and expedite the operation of the factories of the future.","The centralised and decentralised approaches were extensively researched, with experimentation being undertaken to determine the appropriateness of each approach for this use-case.","Decentralised control was utilised to remove the need for excessive communication during the operation of the drones, resulting in smoother operations.","Initial results suggested that the decentralised approach is more appropriate for this use-case.","The individual functionalities necessary for the implementation of a decentralised architecture were proven and assessed, determining that a combination of multiple individual functionalities, namely VSLAM, dynamic collision avoidance and object tracking, would give an appropriate solution for use in an industrial setting.","A final architecture for the parts delivery system was proposed for future work, using a combined centralised and decentralised approach to combat the limitations inherent in each architecture."],"url":"http://arxiv.org/abs/2403.07635v1","category":"cs.RO"}
{"created":"2024-03-12 13:06:38","title":"Monocentric or polycentric city? An empirical perspective","abstract":"Do cities have just one or several centers? Studies performing radial or monocentric analyses of cities are usually criticised by researchers stating that cities are actually polycentric, and this has been well known for a long time. Reversely, when cities are studied independently of any center, other researchers will wonder how the variables of interest evolve with the distance to the center, because this distance is known to be a major determinant at the intra-urban scale. Both monocentric and polycentric formalisms have been introduced centuries (respectively, decades) ago for the study of urban areas, and used both on the empirical and the theoretical side in different disciplines (economics, geography, complex systems, physics...). The present work performs a synthesis of both viewpoints on cities, regarding their use in the literature, and explores with data on European urban areas how some cities considered to be the most polycentric in Europe compare to more standard cities when studied through a combination of radial analysis and scaling laws.","sentences":["Do cities have just one or several centers?","Studies performing radial or monocentric analyses of cities are usually criticised by researchers stating that cities are actually polycentric, and this has been well known for a long time.","Reversely, when cities are studied independently of any center, other researchers will wonder how the variables of interest evolve with the distance to the center, because this distance is known to be a major determinant at the intra-urban scale.","Both monocentric and polycentric formalisms have been introduced centuries (respectively, decades) ago for the study of urban areas, and used both on the empirical and the theoretical side in different disciplines (economics, geography, complex systems, physics...).","The present work performs a synthesis of both viewpoints on cities, regarding their use in the literature, and explores with data on European urban areas how some cities considered to be the most polycentric in Europe compare to more standard cities when studied through a combination of radial analysis and scaling laws."],"url":"http://arxiv.org/abs/2403.07624v1","category":"physics.soc-ph"}
{"created":"2024-03-12 13:06:31","title":"Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness","abstract":"Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \\textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \\textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at https://anonymous.4open.science/r/TSSR-2A27/.","sentences":["Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time.","Despite effectiveness, focusing only on the \\textit{collaborative signals} from behaviors does not fully grasp user interests.","It is also significant to model the \\textit{semantic relatedness} reflected in content features, e.g., images and text.","Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together.","Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues.","Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence.","Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality.","Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests.","We conduct extensive experiments on five public datasets.","The experimental results show that the TSSR could yield superior performance than competitive baselines.","We also make our experimental codes publicly available at https://anonymous.4open.science/r/TSSR-2A27/."],"url":"http://arxiv.org/abs/2403.07623v1","category":"cs.IR"}
{"created":"2024-03-12 13:01:54","title":"Strong Local Bosonic Fluctuation: The Key to Understanding Strongly Correlated Metals","abstract":"In this paper, we present a theoretical framework for understanding the Extremely Correlated Fermi Liquid (ECFL) phenomenon within the $U=\\infty$ Hubbard model. Our approach involves deriving equations of motion for the single-particle Green's function $G$ and its associated self-energy $\\Sigma$, which involves the product of the bosonic correlation function comprising both density ($D_N$) and spin ($D_S$) correlations with $G$. By solving these equations self-consistently, we explore the behavior of $G$, $D_N$, and $D_S$ as functions of frequency, temperature, and hole concentration. Our results reveal distinct coherent and incoherent Fermi liquid regimes characterized by the presence or absence of quasiparticle excitations. Additionally, we analyze the intrinsic dc resistivity $\\rho(T)$, observing a crossover from $T^2$ to linear behavior with increasing temperature. Our findings delineate Fermi liquid, quantum incoherent, and `classical' regimes in strongly correlated systems, emphasizing the importance of quantum diffusive local charge and spin fluctuations.","sentences":["In this paper, we present a theoretical framework for understanding the Extremely Correlated Fermi Liquid (ECFL) phenomenon within the $U=\\infty$ Hubbard model.","Our approach involves deriving equations of motion for the single-particle Green's function $G$ and its associated self-energy $\\Sigma$, which involves the product of the bosonic correlation function comprising both density ($D_N$) and spin ($D_S$) correlations with $G$. By solving these equations self-consistently, we explore the behavior of $G$, $D_N$, and $D_S$ as functions of frequency, temperature, and hole concentration.","Our results reveal distinct coherent and incoherent Fermi liquid regimes characterized by the presence or absence of quasiparticle excitations.","Additionally, we analyze the intrinsic dc resistivity $\\rho(T)$, observing a crossover from $T^2$ to linear behavior with increasing temperature.","Our findings delineate Fermi liquid, quantum incoherent, and `classical' regimes in strongly correlated systems, emphasizing the importance of quantum diffusive local charge and spin fluctuations."],"url":"http://arxiv.org/abs/2403.07620v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 13:01:43","title":"Simple-minded systems and coherent rings","abstract":"Let $A$ be a finite dimensional algebra over an algebraically closed field. We present a relationship between simple-minded systems and coherent rings.","sentences":["Let $A$ be a finite dimensional algebra over an algebraically closed field.","We present a relationship between simple-minded systems and coherent rings."],"url":"http://arxiv.org/abs/2403.07619v1","category":"math.RA"}
{"created":"2024-03-12 12:48:53","title":"FOXSI-2: Upgrades of the Focusing Optics X-ray Solar Imager for its Second Flight","abstract":"The Focusing Optics X-ray Solar Imager (FOXSI) sounding rocket payload flew for the second time on 2014 December 11. To enable direct Hard X-Ray (HXR) imaging spectroscopy, FOXSI makes use of grazing-incidence replicated focusing optics combined with fine-pitch solid-state detectors. FOXSI's first flight provided the first HXR focused images of the Sun. For FOXSI's second flight several updates were made to the instrument including updating the optics and detectors as well as adding a new Solar Aspect and Alignment System (SAAS). This paper provides an overview of these updates as well as a discussion of their measured performance.","sentences":["The Focusing Optics X-ray Solar Imager (FOXSI) sounding rocket payload flew for the second time on 2014 December 11.","To enable direct Hard X-Ray (HXR) imaging spectroscopy, FOXSI makes use of grazing-incidence replicated focusing optics combined with fine-pitch solid-state detectors.","FOXSI's first flight provided the first HXR focused images of the Sun.","For FOXSI's second flight several updates were made to the instrument including updating the optics and detectors as well as adding a new Solar Aspect and Alignment System (SAAS).","This paper provides an overview of these updates as well as a discussion of their measured performance."],"url":"http://arxiv.org/abs/2403.07610v1","category":"astro-ph.IM"}
{"created":"2024-03-12 12:28:34","title":"Scale-free identity: The emergence of social network science","abstract":"Social Network Analysis is a way of studying agents embedded in contexts. In about 1998, physicists discovered social networks as representations of complex systems. Small-world and scale-free networks are the paradigmatic models of this Network Science. Relying on various models and mechanisms of socio-cultural processes, an identity model is developed and calibrated in a case study of Social Network Science. This research domain results from the union of Social Network Analysis and Network Science. A unique dataset of 25,760 scholarly articles from one century of research (1916-2012) is created. Clustering this set of publications, five subdomains are detected and analyzed in terms of authorship, citation, and word usage structures and dynamics. The scaling hypothesis of percolation theory is formulated for socio-cultural systems, namely that power-law size distributions like Lotka's, Bradford's, and Zipf's Law mean that the described identity resides at the phase transition between the stability and change of meaning. In this case, it can be diagnosed using bivariate scaling laws and Abbott's heuristic of fractal distinctions. Identities are not dichotomies but dualities of social network and cultural domain, micro and macro phenomena, as well as stability and change. Story sets that give direction to research fluctuate less, are less distinctive, and more inert than the individuals doing the research. Identities are scale-free. Six senses are diagnostic of different aspects of identity, and when they come together as process, a complex socio-cultural system comes into existence. A mutual benefit that results from mating Relational Sociology and Network Science is identified. The latter can learn from the former that social systems are dualities of transactions and meaning. For the social sciences, the importance of Paretian thinking (scale invariance) is pointed out.","sentences":["Social Network Analysis is a way of studying agents embedded in contexts.","In about 1998, physicists discovered social networks as representations of complex systems.","Small-world and scale-free networks are the paradigmatic models of this Network Science.","Relying on various models and mechanisms of socio-cultural processes, an identity model is developed and calibrated in a case study of Social Network Science.","This research domain results from the union of Social Network Analysis and Network Science.","A unique dataset of 25,760 scholarly articles from one century of research (1916-2012) is created.","Clustering this set of publications, five subdomains are detected and analyzed in terms of authorship, citation, and word usage structures and dynamics.","The scaling hypothesis of percolation theory is formulated for socio-cultural systems, namely that power-law size distributions like Lotka's, Bradford's, and Zipf's Law mean that the described identity resides at the phase transition between the stability and change of meaning.","In this case, it can be diagnosed using bivariate scaling laws and Abbott's heuristic of fractal distinctions.","Identities are not dichotomies but dualities of social network and cultural domain, micro and macro phenomena, as well as stability and change.","Story sets that give direction to research fluctuate less, are less distinctive, and more inert than the individuals doing the research.","Identities are scale-free.","Six senses are diagnostic of different aspects of identity, and when they come together as process, a complex socio-cultural system comes into existence.","A mutual benefit that results from mating Relational Sociology and Network Science is identified.","The latter can learn from the former that social systems are dualities of transactions and meaning.","For the social sciences, the importance of Paretian thinking (scale invariance) is pointed out."],"url":"http://arxiv.org/abs/2403.07595v1","category":"physics.soc-ph"}
{"created":"2024-03-12 12:26:53","title":"Stability of Stationary Solutions to the Nonisentropic Euler-Poisson System in a Perturbed Half Space","abstract":"The main concern of this paper is to mathematically investigate the formation of a plasma sheath near the surface of nonplanar walls. We study the existence and asymptotic stability of stationary solutions for the nonisentropic Euler-Poisson equations in a domain of which boundary is drawn by a graph, by employing a space weighted energy method. Moreover, the convergence rate of the solution toward the stationary solution is obtained, provided that the initial perturbation belongs to the weighted Sobolev space. Because the domain is the perturbed half space, we first show the time-global solvability of the nonisentropic Euler-Poisson equations, then construct stationary solutions by using the time-global solutions.","sentences":["The main concern of this paper is to mathematically investigate the formation of a plasma sheath near the surface of nonplanar walls.","We study the existence and asymptotic stability of stationary solutions for the nonisentropic Euler-Poisson equations in a domain of which boundary is drawn by a graph, by employing a space weighted energy method.","Moreover, the convergence rate of the solution toward the stationary solution is obtained, provided that the initial perturbation belongs to the weighted Sobolev space.","Because the domain is the perturbed half space, we first show the time-global solvability of the nonisentropic Euler-Poisson equations, then construct stationary solutions by using the time-global solutions."],"url":"http://arxiv.org/abs/2403.07594v1","category":"math.AP"}
{"created":"2024-03-12 12:19:05","title":"PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution","abstract":"Recently, some large kernel convnets strike back with appealing performance and efficiency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance. Built on this, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO. For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements.","sentences":["Recently, some large kernel convnets strike back with appealing performance and efficiency.","However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem.","Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing.","In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains.","Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large.","Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance.","Built on this, we propose Parameter-efficient Large Kernel Network (PeLK).","Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO.","For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements."],"url":"http://arxiv.org/abs/2403.07589v1","category":"cs.CV"}
{"created":"2024-03-12 12:15:45","title":"Molecularity: a fast and efficient criterion for probing superconductivity","abstract":"We present an efficient criterion for probing the critical temperature of hydrogen based superconductors. We start by expanding the applicability of 3D descriptors of electron localization to superconducting states within the framework of superconducting DFT. We first apply this descriptor to a model system, the hydrogen chain, which allows to prove two main concepts: i) that the electron localization changes very little when the transition from the normal to the superconducting state takes place, i.e. that it can be described at the DFT level from the normal state; and ii) that the formation of molecules can be characterized within this theoretical framework, enabling to filter out systems with marked molecular character and hence with low potential to be good superconductors. These two ideas, are then exploited in real binary and ternary systems, showing i) that the bonding type can be characterized automatically; and ii) that this provides a new index which enables to feed machine learning algorithms for a better prediction of critical temperatures. Overall, this sets a grounded theoretical scenario for an automatic and efficient high-throughput of potential hydrogen based superconductors.","sentences":["We present an efficient criterion for probing the critical temperature of hydrogen based superconductors.","We start by expanding the applicability of 3D descriptors of electron localization to superconducting states within the framework of superconducting DFT.","We first apply this descriptor to a model system, the hydrogen chain, which allows to prove two main concepts: i) that the electron localization changes very little when the transition from the normal to the superconducting state takes place, i.e. that it can be described at the DFT level from the normal state; and ii) that the formation of molecules can be characterized within this theoretical framework, enabling to filter out systems with marked molecular character and hence with low potential to be good superconductors.","These two ideas, are then exploited in real binary and ternary systems, showing i) that the bonding type can be characterized automatically; and ii) that this provides a new index which enables to feed machine learning algorithms for a better prediction of critical temperatures.","Overall, this sets a grounded theoretical scenario for an automatic and efficient high-throughput of potential hydrogen based superconductors."],"url":"http://arxiv.org/abs/2403.07584v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 12:06:19","title":"Functional renormalization group for p=2 like glassy matrices in the planar approximation: I. Vertex expansion at equilibrium","abstract":"In this paper, we study the equilibrium states of a $N\\times N$ stochastic complex random matrix $M$, whose entries evolve in time accordingly with a Langevin equation including both Gaussian white noises and a linear disorder, materialized by the Wigner random matrices. In large $N$-limit, the disorders behave as effective kinetics, and we examine a coarse-graining over the Wigner spectrum accordingly with two different schemes that we call respectively active and passive. We then investigate explicit solutions of the nonperturbative renormalization group using vertex and derivative expansion, a simple way to deal with the nonlocal nature of the effective field theory at large $N$. Our main statement is the existence of well-behaved fixed point solutions and at least some evidence about a discontinuous (first order) phase transition between a condensed and a dilute phase. We finally interpret the resulting phase space regarding the out-of-equilibrium process related to the dynamical phase transitions.","sentences":["In this paper, we study the equilibrium states of a $N\\times N$ stochastic complex random matrix $M$, whose entries evolve in time accordingly with a Langevin equation including both Gaussian white noises and a linear disorder, materialized by the Wigner random matrices.","In large $N$-limit, the disorders behave as effective kinetics, and we examine a coarse-graining over the Wigner spectrum accordingly with two different schemes that we call respectively active and passive.","We then investigate explicit solutions of the nonperturbative renormalization group using vertex and derivative expansion, a simple way to deal with the nonlocal nature of the effective field theory at large $N$. Our main statement is the existence of well-behaved fixed point solutions and at least some evidence about a discontinuous (first order) phase transition between a condensed and a dilute phase.","We finally interpret the resulting phase space regarding the out-of-equilibrium process related to the dynamical phase transitions."],"url":"http://arxiv.org/abs/2403.07577v1","category":"hep-th"}
{"created":"2024-03-12 12:05:43","title":"FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification","abstract":"Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to transfer pre-trained models to downstream tasks, avoiding the high cost of updating entire large-scale pre-trained models (LPMs). In this work, we present Fine-grained Prompt Tuning (FPT), a novel PEFT method for medical image classification. FPT significantly reduces memory consumption compared to other PEFT methods, especially in high-resolution contexts. To achieve this, we first freeze the weights of the LPM and construct a learnable lightweight side network. The frozen LPM takes high-resolution images as input to extract fine-grained features, while the side network is fed low-resolution images to reduce memory usage. To allow the side network to access pre-trained knowledge, we introduce fine-grained prompts that summarize information from the LPM through a fusion module. Important tokens selection and preloading techniques are employed to further reduce training cost and memory requirements. We evaluate FPT on four medical datasets with varying sizes, modalities, and complexities. Experimental results demonstrate that FPT achieves comparable performance to fine-tuning the entire LPM while using only 1.8% of the learnable parameters and 13% of the memory costs of an encoder ViT-B model with a 512 x 512 input resolution.","sentences":["Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to transfer pre-trained models to downstream tasks, avoiding the high cost of updating entire large-scale pre-trained models (LPMs).","In this work, we present Fine-grained Prompt Tuning (FPT), a novel PEFT method for medical image classification.","FPT significantly reduces memory consumption compared to other PEFT methods, especially in high-resolution contexts.","To achieve this, we first freeze the weights of the LPM and construct a learnable lightweight side network.","The frozen LPM takes high-resolution images as input to extract fine-grained features, while the side network is fed low-resolution images to reduce memory usage.","To allow the side network to access pre-trained knowledge, we introduce fine-grained prompts that summarize information from the LPM through a fusion module.","Important tokens selection and preloading techniques are employed to further reduce training cost and memory requirements.","We evaluate FPT on four medical datasets with varying sizes, modalities, and complexities.","Experimental results demonstrate that FPT achieves comparable performance to fine-tuning the entire LPM while using only 1.8% of the learnable parameters and 13% of the memory costs of an encoder ViT-B model with a 512 x 512 input resolution."],"url":"http://arxiv.org/abs/2403.07576v1","category":"cs.CV"}
{"created":"2024-03-12 12:04:51","title":"Direct observation of strong t-e orbital hybridization and the effects of f orbitals","abstract":"Recent research has revealed that the Cr family perovskite ReCrO$_3$ exhibits intriguing magnetic coupling interactions within Cr pairs, which may not follow the Goodenough-Kanamori (GK) rules due to the t-e hybridization between Cr$^\\mathrm{III}$ ions. We investigate the complex magnetism involving both t-e hybridization and Re-$f$ orbitals in the molecular analogue of perovskite [$\\mathrm{Ce_2^{III}Ce^{IV}Cr_8^{III}O_8(O_2CPh)_{18}(HO_2CPh)}$] ($\\mathrm{Ce_3Cr_8}$) using first-principles method. Our results have shown that distinct from the bulk ReCrO$_3$, the superexchange via Cr-$d$ and O-$p$ orbitals can exhibit a unexpected dominate ferromagnetic (FM) Cr-O-Cr superexchange interaction in $\\mathrm{Ce_3Cr_8}$ due to the strong t-e hybridization originated from the distorted molecular structure. The great sensitivity of the t-e hybridization with respect to the molecular structure, e.g., the angle of Cr-O-Cr, can lead to a ground state transition from ferromagnetic to antiferromagnetic state with the changes in the angle of Cr-O-Cr. The Ce-$f$ orbitals near the Fermi level can reduce this sensitivity through interacting with the Cr-$d$ orbitals via the virtual charge transfer process. Our results are strongly supported by a modified superexchange model based on the t-e hybridization theory. These findings complete the theory of superexchange magnetism involving the t-e hybridization and $f$ orbitals, and in the meanwhile introduce a new avenue for fine-tuning the magnetic characteristics via Tm-d/Re-f interactions at nanoscale.","sentences":["Recent research has revealed that the Cr family perovskite ReCrO$_3$ exhibits intriguing magnetic coupling interactions within Cr pairs, which may not follow the Goodenough-Kanamori (GK) rules due to the t-e hybridization between Cr$^\\mathrm{III}$ ions.","We investigate the complex magnetism involving both t-e hybridization and Re-$f$ orbitals in the molecular analogue of perovskite [$\\mathrm{Ce_2^{III}Ce^{IV}Cr_8^{III}O_8(O_2CPh)_{18}(HO_2CPh)}$] ($\\mathrm{Ce_3Cr_8}$) using first-principles method.","Our results have shown that distinct from the bulk ReCrO$_3$, the superexchange via Cr-$d$ and O-$p$ orbitals can exhibit a unexpected dominate ferromagnetic (FM) Cr-O-Cr superexchange interaction in $\\mathrm{Ce_3Cr_8}$ due to the strong t-e hybridization originated from the distorted molecular structure.","The great sensitivity of the t-e hybridization with respect to the molecular structure, e.g., the angle of Cr-O-Cr, can lead to a ground state transition from ferromagnetic to antiferromagnetic state with the changes in the angle of Cr-O-Cr.","The Ce-$f$ orbitals near the Fermi level can reduce this sensitivity through interacting with the Cr-$d$ orbitals via the virtual charge transfer process.","Our results are strongly supported by a modified superexchange model based on the t-e hybridization theory.","These findings complete the theory of superexchange magnetism involving the t-e hybridization and $f$ orbitals, and in the meanwhile introduce a new avenue for fine-tuning the magnetic characteristics via Tm-d/Re-f interactions at nanoscale."],"url":"http://arxiv.org/abs/2403.07574v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 12:02:48","title":"On Weakly Contracting Dynamics for Convex Optimization","abstract":"We investigate the convergence characteristics of dynamics that are \\emph{globally weakly} and \\emph{locally strongly contracting}. Such dynamics naturally arise in the context of convex optimization problems with a unique minimizer. We show that convergence to the equilibrium is \\emph{linear-exponential}, in the sense that the distance between each solution and the equilibrium is upper bounded by a function that first decreases linearly and then exponentially. As we show, the linear-exponential dependency arises naturally in certain dynamics with saturations. Additionally, we provide a sufficient condition for local input-to-state stability. Finally, we illustrate our results on, and propose a conjecture for, continuous-time dynamical systems solving linear programs.","sentences":["We investigate the convergence characteristics of dynamics that are \\emph{globally weakly} and \\emph{locally strongly contracting}.","Such dynamics naturally arise in the context of convex optimization problems with a unique minimizer.","We show that convergence to the equilibrium is \\emph{linear-exponential}, in the sense that the distance between each solution and the equilibrium is upper bounded by a function that first decreases linearly and then exponentially.","As we show, the linear-exponential dependency arises naturally in certain dynamics with saturations.","Additionally, we provide a sufficient condition for local input-to-state stability.","Finally, we illustrate our results on, and propose a conjecture for, continuous-time dynamical systems solving linear programs."],"url":"http://arxiv.org/abs/2403.07572v1","category":"math.OC"}
{"created":"2024-03-12 11:58:50","title":"Proactive Recommendation with Iterative Preference Guidance","abstract":"Recommender systems mainly tailor personalized recommendations according to user interests learned from user feedback. However, such recommender systems passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization. To counteract this, proactive recommendation actively steers users towards developing new interests in a target item or topic by strategically modulating recommendation sequences. Existing work for proactive recommendation faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial recommender systems. To address these issues, we introduce an Iterative Preference Guidance (IPG) framework. IPG performs proactive recommendation in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value. These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions. Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in recommender accuracy. The code is available at https://github.com/GabyUSTC/IPG-Rec.","sentences":["Recommender systems mainly tailor personalized recommendations according to user interests learned from user feedback.","However, such recommender systems passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization.","To counteract this, proactive recommendation actively steers users towards developing new interests in a target item or topic by strategically modulating recommendation sequences.","Existing work for proactive recommendation faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial recommender systems.","To address these issues, we introduce an Iterative Preference Guidance (IPG) framework.","IPG performs proactive recommendation in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value.","These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions.","Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in recommender accuracy.","The code is available at https://github.com/GabyUSTC/IPG-Rec."],"url":"http://arxiv.org/abs/2403.07571v1","category":"cs.IR"}
{"created":"2024-03-12 11:58:37","title":"An Active Contour Model Driven By the Hybrid Signed Pressure Function","abstract":"Due to the influence of imaging equipment and complex imaging environments, most images in daily life have features of intensity inhomogeneity and noise. Therefore, many scholars have designed many image segmentation algorithms to address these issues. Among them, the active contour model is one of the most effective image segmentation algorithms.This paper proposes an active contour model driven by the hybrid signed pressure function that combines global and local information construction. Firstly, a new global region-based signed pressure function is introduced by combining the average intensity of the inner and outer regions of the curve with the median intensity of the inner region of the evolution curve. Then, the paper uses the energy differences between the inner and outer regions of the curve in the local region to design the signed pressure function of the local term. Combine the two SPF function to obtain a new signed pressure function and get the evolution equation of the new model. Finally, experiments and numerical analysis show that the model has excellent segmentation performance for both intensity inhomogeneous images and noisy images.","sentences":["Due to the influence of imaging equipment and complex imaging environments, most images in daily life have features of intensity inhomogeneity and noise.","Therefore, many scholars have designed many image segmentation algorithms to address these issues.","Among them, the active contour model is one of the most effective image segmentation algorithms.","This paper proposes an active contour model driven by the hybrid signed pressure function that combines global and local information construction.","Firstly, a new global region-based signed pressure function is introduced by combining the average intensity of the inner and outer regions of the curve with the median intensity of the inner region of the evolution curve.","Then, the paper uses the energy differences between the inner and outer regions of the curve in the local region to design the signed pressure function of the local term.","Combine the two SPF function to obtain a new signed pressure function and get the evolution equation of the new model.","Finally, experiments and numerical analysis show that the model has excellent segmentation performance for both intensity inhomogeneous images and noisy images."],"url":"http://arxiv.org/abs/2403.07570v1","category":"cs.CV"}
{"created":"2024-03-12 11:56:50","title":"Exploring Challenges in Deep Learning of Single-Station Ground Motion Records","abstract":"Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information on model performance. Experimental results reveal a strong reliance on the highly correlated P and S phase arrival information. Our observations highlight a potential gap in the field, indicating an absence of robust methodologies for deep learning of single-station ground motion recordings independent of any auxiliary information.","sentences":["Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering.","These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring.","However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed.","In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness.","We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information on model performance.","Experimental results reveal a strong reliance on the highly correlated P and S phase arrival information.","Our observations highlight a potential gap in the field, indicating an absence of robust methodologies for deep learning of single-station ground motion recordings independent of any auxiliary information."],"url":"http://arxiv.org/abs/2403.07569v1","category":"eess.SP"}
{"created":"2024-03-12 11:42:42","title":"Controlling Delegations in Liquid Democracy","abstract":"In liquid democracy, agents can either vote directly or delegate their vote to a different agent of their choice. This results in a power structure in which certain agents possess more voting weight than others. As a result, it opens up certain possibilities of vote manipulation, including control and bribery, that do not exist in standard voting scenarios of direct democracy. Here we formalize a certain kind of election control -- in which an external agent may change certain delegation arcs -- and study the computational complexity of the corresponding combinatorial problem.","sentences":["In liquid democracy, agents can either vote directly or delegate their vote to a different agent of their choice.","This results in a power structure in which certain agents possess more voting weight than others.","As a result, it opens up certain possibilities of vote manipulation, including control and bribery, that do not exist in standard voting scenarios of direct democracy.","Here we formalize a certain kind of election control -- in which an external agent may change certain delegation arcs -- and study the computational complexity of the corresponding combinatorial problem."],"url":"http://arxiv.org/abs/2403.07558v1","category":"cs.GT"}
{"created":"2024-03-12 11:34:33","title":"Consensus under Persistence Excitation","abstract":"We prove that a first-order cooperative system of interacting agents converges to consensus if the so-called Persistence Excitation condition holds. This condition requires that the interaction function between any pair of agents satisfies an integral lower bound. The interpretation is that the interaction needs to ensure a minimal amount of service.","sentences":["We prove that a first-order cooperative system of interacting agents converges to consensus if the so-called Persistence Excitation condition holds.","This condition requires that the interaction function between any pair of agents satisfies an integral lower bound.","The interpretation is that the interaction needs to ensure a minimal amount of service."],"url":"http://arxiv.org/abs/2403.07549v1","category":"math.OC"}
{"created":"2024-03-12 11:32:30","title":"MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki","abstract":"NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online.","sentences":["NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled.","The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality.","In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters.","We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information.","The toolkit is publicly available online."],"url":"http://arxiv.org/abs/2403.07544v1","category":"cs.CL"}
{"created":"2024-03-12 11:29:40","title":"A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions","abstract":"This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing. Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision. These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing. Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture. We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations. The survey concludes with future research directions, highlighting the growing role of Vision Transformers in Autonomous Driving.","sentences":["This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing.","Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision.","These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing.","Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture.","We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations.","The survey concludes with future research directions, highlighting the growing role of Vision Transformers in Autonomous Driving."],"url":"http://arxiv.org/abs/2403.07542v1","category":"cs.CV"}
{"created":"2024-03-12 11:20:50","title":"Vortices and Factorization","abstract":"We review applications of factorization methods to the problem of finding stationary point vortex patterns in two-dimensional fluid mechanics. Then we present a new class of patterns related to periodic analogs of Schrodinger operators from the ``even\" bi-spectral family. We also show that patterns related to rational and soliton solutions of the KdV hierarchy constitute complete solution of the problem for certain classes of vortex systems.","sentences":["We review applications of factorization methods to the problem of finding stationary point vortex patterns in two-dimensional fluid mechanics.","Then we present a new class of patterns related to periodic analogs of Schrodinger operators from the ``even\" bi-spectral family.","We also show that patterns related to rational and soliton solutions of the KdV hierarchy constitute complete solution of the problem for certain classes of vortex systems."],"url":"http://arxiv.org/abs/2403.07537v1","category":"math-ph"}
{"created":"2024-03-12 11:18:35","title":"Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving","abstract":"Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations. The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map. Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions. Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing. Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations. Project website: https://github.com/Junda24/AFNet/.","sentences":["Multi-view depth estimation has achieved impressive performance over various benchmarks.","However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving.","In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings.","Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings.","To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations.","The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map.","Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions.","Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing.","Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations.","Project website: https://github.com/Junda24/AFNet/."],"url":"http://arxiv.org/abs/2403.07535v1","category":"cs.CV"}
{"created":"2024-03-12 11:15:21","title":"Turbulent mixed convection in vertical and horizontal channels","abstract":"Turbulent shear flows driven by a combination of a pressure gradient and buoyancy forcing are investigated using direct numerical simulations. Specifically, we consider the setup of a differentially heated vertical channel subject to a Poiseuille-like horizontal pressure gradient. We explore the response of the system to its three control parameters: the Grashof number $Gr$, the Prandtl number $Pr$, and the Reynolds number $Re$ of the pressure-driven flow. From these input parameters, the relative strength of buoyancy driving to the pressure gradient can be quantified by the Richardson number $Ri=Gr/Re^2$. We compare the response of the mixed vertical convection configuration to that of mixed Rayleigh-B\\'enard convection and find a nearly identical behaviour, including an increase in wall friction at higher $Gr$ and a drop in the heat flux relative to natural convection for $Ri=O(1)$. This closely matched response is despite vastly different flow structures in the systems. No large-scale organisation is visible in visualisations of mixed vertical convection - an observation that is quantitatively confirmed by spectral analysis. This analysis, combined with a statistical description of the wall heat flux, highlights how moderate shear suppresses the growth of small-scale plumes and reduces the likelihood of extreme events in the local wall heat flux. Vice versa, starting from a pure shear flow, the addition of thermal driving enhances the drag due to the emission of thermal plumes.","sentences":["Turbulent shear flows driven by a combination of a pressure gradient and buoyancy forcing are investigated using direct numerical simulations.","Specifically, we consider the setup of a differentially heated vertical channel subject to a Poiseuille-like horizontal pressure gradient.","We explore the response of the system to its three control parameters: the Grashof number $Gr$, the Prandtl number $Pr$, and the Reynolds number $Re$ of the pressure-driven flow.","From these input parameters, the relative strength of buoyancy driving to the pressure gradient can be quantified by the Richardson number $Ri=Gr/Re^2$. We compare the response of the mixed vertical convection configuration to that of mixed Rayleigh-B\\'enard convection and find a nearly identical behaviour, including an increase in wall friction at higher $Gr$ and a drop in the heat flux relative to natural convection for $Ri=O(1)$. This closely matched response is despite vastly different flow structures in the systems.","No large-scale organisation is visible in visualisations of mixed vertical convection - an observation that is quantitatively confirmed by spectral analysis.","This analysis, combined with a statistical description of the wall heat flux, highlights how moderate shear suppresses the growth of small-scale plumes and reduces the likelihood of extreme events in the local wall heat flux.","Vice versa, starting from a pure shear flow, the addition of thermal driving enhances the drag due to the emission of thermal plumes."],"url":"http://arxiv.org/abs/2403.07533v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 11:11:19","title":"Open-World Semantic Segmentation Including Class Similarity","abstract":"Interpreting camera data is key for autonomously acting systems, such as autonomous vehicles. Vision systems that operate in real-world environments must be able to understand their surroundings and need the ability to deal with novel situations. This paper tackles open-world semantic segmentation, i.e., the variant of interpreting image data in which objects occur that have not been seen during training. We propose a novel approach that performs accurate closed-world semantic segmentation and, at the same time, can identify new categories without requiring any additional training data. Our approach additionally provides a similarity measure for every newly discovered class in an image to a known category, which can be useful information in downstream tasks such as planning or mapping. Through extensive experiments, we show that our model achieves state-of-the-art results on classes known from training data as well as for anomaly segmentation and can distinguish between different unknown classes.","sentences":["Interpreting camera data is key for autonomously acting systems, such as autonomous vehicles.","Vision systems that operate in real-world environments must be able to understand their surroundings and need the ability to deal with novel situations.","This paper tackles open-world semantic segmentation, i.e., the variant of interpreting image data in which objects occur that have not been seen during training.","We propose a novel approach that performs accurate closed-world semantic segmentation and, at the same time, can identify new categories without requiring any additional training data.","Our approach additionally provides a similarity measure for every newly discovered class in an image to a known category, which can be useful information in downstream tasks such as planning or mapping.","Through extensive experiments, we show that our model achieves state-of-the-art results on classes known from training data as well as for anomaly segmentation and can distinguish between different unknown classes."],"url":"http://arxiv.org/abs/2403.07532v1","category":"cs.CV"}
{"created":"2024-03-12 11:01:43","title":"Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs","abstract":"For the vertex selection problem $(\\sigma,\\rho)$-DomSet one is given two fixed sets $\\sigma$ and $\\rho$ of integers and the task is to decide whether we can select vertices of the input graph, such that, for every selected vertex, the number of selected neighbors is in $\\sigma$ and, for every unselected vertex, the number of selected neighbors is in $\\rho$. This framework covers Independent Set and Dominating Set for example.   We investigate the case when $\\sigma$ and $\\rho$ are periodic sets with the same period $m\\ge 2$, that is, the sets are two (potentially different) residue classes modulo $m$. We study the problem parameterized by treewidth and present an algorithm that solves in time $m^{tw} \\cdot n^{O(1)}$ the decision, minimization and maximization version of the problem. This significantly improves upon the known algorithms where for the case $m \\ge 3$ not even an explicit running time is known. We complement our algorithm by providing matching lower bounds which state that there is no $(m-\\epsilon)^{pw} \\cdot n^{O(1)}$ unless SETH fails. For $m = 2$, we extend these bound to the minimization version as the decision version is efficiently solvable.","sentences":["For the vertex selection problem $(\\sigma,\\rho)$-DomSet one is given two fixed sets $\\sigma$ and $\\rho$ of integers and the task is to decide whether we can select vertices of the input graph, such that, for every selected vertex, the number of selected neighbors is in $\\sigma$ and, for every unselected vertex, the number of selected neighbors is in $\\rho$. This framework covers Independent Set and Dominating Set for example.   ","We investigate the case when $\\sigma$ and $\\rho$ are periodic sets with the same period $m\\ge 2$, that is, the sets are two (potentially different) residue classes modulo $m$. We study the problem parameterized by treewidth and present an algorithm that solves in time $m^{tw} \\cdot n^{O(1)}$ the decision, minimization and maximization version of the problem.","This significantly improves upon the known algorithms where for the case $m \\ge 3$ not even an explicit running time is known.","We complement our algorithm by providing matching lower bounds which state that there is no $(m-\\epsilon)^{pw} \\cdot n^{O(1)}$ unless SETH fails.","For $m = 2$, we extend these bound to the minimization version as the decision version is efficiently solvable."],"url":"http://arxiv.org/abs/2403.07524v1","category":"cs.DS"}
{"created":"2024-03-12 10:55:20","title":"Time evolution of the Boltzmann entropy for a nonequilibrium dilute gas","abstract":"We investigate the time evolution of the Boltzmann entropy of a dilute gas of N particles, N>>1, as it undergoes a free expansion doubling its volume. The microstate of the system, a point in the 4N dimensional phase space, changes in time via Hamiltonian dynamics. Its entropy, at any time $t$, is given by the logarithm of the phase space volume of all the microstates giving rise to its macrostate at time $t$. The macrostates that we consider are defined by coarse graining the one-particle phase space into cells $\\Delta_\\alpha$. The initial and final macrostates of the system are equilibrium states in volumes $V$ and $2V$, with the same energy $E$ and particle number $N$. Their entropy per particle is given, for sufficiently large systems, by the thermodynamic entropy as a function of the particle and energy density, whose leading term is independent of the size of the $\\Delta_\\alpha$. The intermediate (non-equilibrium) entropy does however depend on the size of the cells $\\Delta_\\alpha$. Its change with time is due to (i) dispersal in physical space from free motion and to (ii) the collisions between particles which change their velocities. The former depends strongly on the size of the velocity coarse graining $\\Delta v$: it produces entropy at a rate proportional to $\\Delta v$. This dependence is investigated numerically and analytically for a dilute two-dimensional gas of hard discs. It becomes significant when the mean free path between collisions is of the same order or larger than the length scale of the initial spatial inhomogeneity. In the opposite limit, the rate of entropy production is essentially independent of $\\Delta v$ and is given by the Boltzmann equation for the limit $\\Delta v\\rightarrow 0$. We show that when both processes are active the time dependence of the entropy has a scaling form involving the ratio of the rates of its production by the two processes.","sentences":["We investigate the time evolution of the Boltzmann entropy of a dilute gas of N particles, N>>1, as it undergoes a free expansion doubling its volume.","The microstate of the system, a point in the 4N dimensional phase space, changes in time via Hamiltonian dynamics.","Its entropy, at any time $t$, is given by the logarithm of the phase space volume of all the microstates giving rise to its macrostate at time $t$. The macrostates that we consider are defined by coarse graining the one-particle phase space into cells $\\Delta_\\alpha$. The initial and final macrostates of the system are equilibrium states in volumes $V$ and $2V$, with the same energy $E$ and particle number $N$. Their entropy per particle is given, for sufficiently large systems, by the thermodynamic entropy as a function of the particle and energy density, whose leading term is independent of the size of the $\\Delta_\\alpha$. The intermediate (non-equilibrium) entropy does however depend on the size of the cells $\\Delta_\\alpha$. Its change with time is due to (i) dispersal in physical space from free motion and to (ii) the collisions between particles which change their velocities.","The former depends strongly on the size of the velocity coarse graining $\\Delta v$: it produces entropy at a rate proportional to $\\Delta v$.","This dependence is investigated numerically and analytically for a dilute two-dimensional gas of hard discs.","It becomes significant when the mean free path between collisions is of the same order or larger than the length scale of the initial spatial inhomogeneity.","In the opposite limit, the rate of entropy production is essentially independent of $\\Delta v$ and is given by the Boltzmann equation for the limit $\\Delta v\\rightarrow","0$.","We show that when both processes are active the time dependence of the entropy has a scaling form involving the ratio of the rates of its production by the two processes."],"url":"http://arxiv.org/abs/2403.07519v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 10:50:13","title":"Energy versus Output Quality of Non-volatile Writes in Intermittent Computing","abstract":"We explore how to improve the energy performance of battery-less Internet of Things (IoT) devices at the cost of a reduction in the quality of the output. Battery-less IoT devices are extremely resource-constrained energy-harvesting devices. Due to erratic energy patterns from the ambient, their executions become intermittent; periods of active computation are interleaved by periods of recharging small energy buffers. To cross periods of energy unavailability, a device persists application and system state onto Non-Volatile Memory (NVM) in anticipation of energy failures. We purposely control the energy invested in these operations, representing a major energy overhead, when using Spin-Transfer Torque Magnetic Random-Access Memory (STT-MRAM) as NVM. As a result, we abate the corresponding overhead, yet introduce write errors. Based on 1.9+ trillion experimental data points, we illustrate whether this is a gamble worth taking, when, and where. We measure the energy consumption and quality of output obtained from the execution of nine diverse benchmarks on top of seven different platforms. Our results allow us to draw three key observations: i) the trade-off between energy saving and reduction of output quality is program-specific; ii) the same trade-off is a function of a platform's specific compute efficiency and power figures; and iii) data encoding and input size impact a program's resilience to errors. As a paradigmatic example, we reveal cases where we achieve up to 50% reduction in energy consumption with negligible effects on output quality, as opposed to settings where a minimal energy gain causes drastic drops in output quality.","sentences":["We explore how to improve the energy performance of battery-less Internet of Things (IoT) devices at the cost of a reduction in the quality of the output.","Battery-less IoT devices are extremely resource-constrained energy-harvesting devices.","Due to erratic energy patterns from the ambient, their executions become intermittent; periods of active computation are interleaved by periods of recharging small energy buffers.","To cross periods of energy unavailability, a device persists application and system state onto Non-Volatile Memory (NVM) in anticipation of energy failures.","We purposely control the energy invested in these operations, representing a major energy overhead, when using Spin-Transfer Torque Magnetic Random-Access Memory (STT-MRAM) as NVM.","As a result, we abate the corresponding overhead, yet introduce write errors.","Based on 1.9+ trillion experimental data points, we illustrate whether this is a gamble worth taking, when, and where.","We measure the energy consumption and quality of output obtained from the execution of nine diverse benchmarks on top of seven different platforms.","Our results allow us to draw three key observations: i) the trade-off between energy saving and reduction of output quality is program-specific; ii) the same trade-off is a function of a platform's specific compute efficiency and power figures; and iii) data encoding and input size impact a program's resilience to errors.","As a paradigmatic example, we reveal cases where we achieve up to 50% reduction in energy consumption with negligible effects on output quality, as opposed to settings where a minimal energy gain causes drastic drops in output quality."],"url":"http://arxiv.org/abs/2403.07517v1","category":"eess.SY"}
{"created":"2024-03-12 10:47:49","title":"Existence of Weak Solutions to a Cahn-Hilliard-Biot System","abstract":"We prove existence of weak solutions to a diffuse interface model describing the flow of a fluid through a deformable porous medium consisting of two phases. The system non-linearly couples Biot's equations for poroelasticity, including phase-field dependent material properties, with the Cahn-Hilliard equation to model the evolution of the solid, and is further augmented by a visco-elastic regularization consistent with secondary consolidation. To obtain this result, we approximate the problem in two steps, where first a semi-Galerkin ansatz is employed to show existence of weak solutions to regularized systems, for which later on compactness arguments allow limit passage. Notably, we also establish a maximal regularity theory for linear visco-elastic problems.","sentences":["We prove existence of weak solutions to a diffuse interface model describing the flow of a fluid through a deformable porous medium consisting of two phases.","The system non-linearly couples Biot's equations for poroelasticity, including phase-field dependent material properties, with the Cahn-Hilliard equation to model the evolution of the solid, and is further augmented by a visco-elastic regularization consistent with secondary consolidation.","To obtain this result, we approximate the problem in two steps, where first a semi-Galerkin ansatz is employed to show existence of weak solutions to regularized systems, for which later on compactness arguments allow limit passage.","Notably, we also establish a maximal regularity theory for linear visco-elastic problems."],"url":"http://arxiv.org/abs/2403.07515v1","category":"math.AP"}
{"created":"2024-03-12 10:45:53","title":"The entropy of an extended map for abelian group actions","abstract":"In this paper, we mainly consider on the entropy of the extended map conditional to the natural extension of a dynamical system for an Abelian group action and we calculate the entropy is zero.","sentences":["In this paper, we mainly consider on the entropy of the extended map conditional to the natural extension of a dynamical system for an Abelian group action and we calculate the entropy is zero."],"url":"http://arxiv.org/abs/2403.07511v1","category":"math.DS"}
{"created":"2024-03-12 10:43:24","title":"Rotational Evolution of Classical T Tauri Stars: Models and Observations","abstract":"We developed a grid of stellar rotation models for low-mass and solar-type Classical T Tauri stars (CTTS) ($0.3M_{\\odot}<M_{\\ast}<1.2M_{\\odot}$). These models incorporate the star-disk interaction and magnetospheric ejections to investigate the evolution of the stellar rotation rate as a function of the mass of the star $M_{\\ast}$, the magnetic field ($B_{\\ast}$), and stellar wind ($\\dot{M}_{wind}$). We compiled and determined stellar parameters for 208 CTTS, such as projected rotational velocity $v\\sin(i)$, mass accretion rate $\\dot{M}_{acc}$, stellar mass $M_{\\ast}$, ages, and estimated rotational periods using TESS data. We also estimated a representative value of the mass-loss rate for our sample using the $[\\text{O}\\text{ I}]$ spectral line. Our results confirm that $v\\sin(i)$ measurements in CTTS agree with the rotation rates provided by our spin models in the accretion-powered stellar winds (APSW) picture. In addition, we used the Approximate Bayesian Computation (ABC) technique to explore the connection between the model parameters and the observational properties of CTTS. We find that the evolution of $v\\sin(i)$ with age might be regulated by variations in (1) the intensity of $B_{\\ast}$ and (2) the fraction of the accretion flow ejected in magnetic winds, removing angular momentum from these systems. The youngest stars in our sample ($\\sim $1 Myr) show a median branching ratio $\\dot{M}_{wind}/\\dot{M}_{acc}\\sim$ $0.16$ and median $B_{\\ast}\\sim$ 2000 G, in contrast to $\\sim 0.01$ and 1000 G, respectively, for stars with ages $\\gtrsim 3$ Myr.","sentences":["We developed a grid of stellar rotation models for low-mass and solar-type Classical T Tauri stars (CTTS) ($0.3M_{\\odot}<M_{\\ast}<1.2M_{\\odot}$).","These models incorporate the star-disk interaction and magnetospheric ejections to investigate the evolution of the stellar rotation rate as a function of the mass of the star $M_{\\ast}$, the magnetic field ($B_{\\ast}$), and stellar wind ($\\dot{M}_{wind}$).","We compiled and determined stellar parameters for 208 CTTS, such as projected rotational velocity $v\\sin(i)$, mass accretion rate $\\dot{M}_{acc}$, stellar mass $M_{\\ast}$, ages, and estimated rotational periods using TESS data.","We also estimated a representative value of the mass-loss rate for our sample using the $[\\text{O}\\text{ I}]$ spectral line.","Our results confirm that $v\\sin(i)$ measurements in CTTS agree with the rotation rates provided by our spin models in the accretion-powered stellar winds (APSW) picture.","In addition, we used the Approximate Bayesian Computation (ABC) technique to explore the connection between the model parameters and the observational properties of CTTS.","We find that the evolution of $v\\sin(i)$ with age might be regulated by variations in (1) the intensity of $B_{\\ast}$ and (2) the fraction of the accretion flow ejected in magnetic winds, removing angular momentum from these systems.","The youngest stars in our sample ($\\sim $1 Myr) show a median branching ratio $\\dot{M}_{wind}/\\dot{M}_{acc}\\sim$ $0.16$ and median $B_{\\ast}\\sim$ 2000 G, in contrast to $\\sim 0.01$ and 1000 G, respectively, for stars with ages $\\gtrsim 3$ Myr."],"url":"http://arxiv.org/abs/2403.07505v1","category":"astro-ph.SR"}
{"created":"2024-03-12 10:42:32","title":"Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach","abstract":"Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors. However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry. Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained reinforcement learning (CRL) for the first time globally. Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle's minimum fuel consumption under the battery electrical balance condition. We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we give vital steps to implement CRL approaches and compare the performance between the CVPO and Lagrangian-based approaches. Our case study found that CVPO and Lagrangian-based approaches can obtain the lowest fuel consumption while maintaining the SOC balance constraint. The CVPO approach converges stable, but the Lagrangian-based approach can obtain the lowest fuel consumption at 3.95 L/100km, though with more significant oscillations. This result verifies the effectiveness of our proposed CRL approaches to the COFC problem.","sentences":["Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors.","However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry.","Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained reinforcement learning (CRL) for the first time globally.","Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle's minimum fuel consumption under the battery electrical balance condition.","We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we give vital steps to implement CRL approaches and compare the performance between the CVPO and Lagrangian-based approaches.","Our case study found that CVPO and Lagrangian-based approaches can obtain the lowest fuel consumption while maintaining the SOC balance constraint.","The CVPO approach converges stable, but the Lagrangian-based approach can obtain the lowest fuel consumption at 3.95 L/100km, though with more significant oscillations.","This result verifies the effectiveness of our proposed CRL approaches to the COFC problem."],"url":"http://arxiv.org/abs/2403.07503v1","category":"cs.LG"}
{"created":"2024-03-12 10:37:22","title":"FLAME: Fitting Ly\u03b1 Absorption lines using Machine learning","abstract":"We introduce FLAME, a machine learning algorithm designed to fit Voigt profiles to HI Lyman-alpha (Ly$\\alpha$) absorption lines using deep convolutional neural networks. FLAME integrates two algorithms: the first determines the number of components required to fit Ly$\\alpha$ absorption lines, and the second calculates the Doppler parameter $b$, the HI column density N$_{\\rm HI}$, and the velocity separation of individual components. For the current version of FLAME, we trained it on low-redshift Ly$\\alpha$ forests observed with the Far Ultraviolet gratings of the Cosmic Origin Spectrograph (COS) aboard the Hubble Space Telescope (HST). Drawing on this data, we trained FLAME on $\\sim$ $10^6$ simulated Voigt profiles, forward-modeled to Ly$\\alpha$ absorption lines observed with HST-COS, to classify lines as either single or double components and then determine Voigt profile fitting parameters. FLAME shows impressive accuracy on the simulated data by identifying more than 98% (90%) of single (double) component lines. It determines $b$ values within $\\approx \\pm{8}~(15)$ km s$^{-1}$ and log $N_{\\rm HI}/ {\\rm cm}^2$ values within $\\approx \\pm 0.3~(0.8)$ for 90% of the single (double) component lines. However, when applied to real data, FLAME's component classification accuracy drops by $\\sim$ 10%. Despite this, there is a reasonable agreement between the $b$ and N$_{\\rm HI}$ distributions obtained from traditional Voigt profile fitting methods and FLAME's predictions. Our mock HST-COS data analysis, designed to emulate real data parameters, demonstrated that FLAME could achieve consistent accuracy comparable to its performance with simulated data. This finding suggests that the drop in FLAME's accuracy when used on real data primarily arises from the difficulty of replicating the full complexity of real data in the training sample.","sentences":["We introduce FLAME, a machine learning algorithm designed to fit Voigt profiles to HI Lyman-alpha (Ly$\\alpha$) absorption lines using deep convolutional neural networks.","FLAME integrates two algorithms: the first determines the number of components required to fit Ly$\\alpha$ absorption lines, and the second calculates the Doppler parameter $b$, the HI column density N$_{\\rm HI}$, and the velocity separation of individual components.","For the current version of FLAME, we trained it on low-redshift Ly$\\alpha$ forests observed with the Far Ultraviolet gratings of the Cosmic Origin Spectrograph (COS) aboard the Hubble Space Telescope (HST).","Drawing on this data, we trained FLAME on $\\sim$ $10^6$ simulated Voigt profiles, forward-modeled to Ly$\\alpha$ absorption lines observed with HST-COS, to classify lines as either single or double components and then determine Voigt profile fitting parameters.","FLAME shows impressive accuracy on the simulated data by identifying more than 98% (90%) of single (double) component lines.","It determines $b$ values within $\\approx \\pm{8}~(15)$ km s$^{-1}$ and log $N_{\\rm HI}/ {\\rm cm}^2$ values within $\\approx \\pm 0.3~(0.8)$ for 90% of the single (double) component lines.","However, when applied to real data, FLAME's component classification accuracy drops by $\\sim$ 10%.","Despite this, there is a reasonable agreement between the $b$ and N$_{\\rm HI}$ distributions obtained from traditional Voigt profile fitting methods and FLAME's predictions.","Our mock HST-COS data analysis, designed to emulate real data parameters, demonstrated that FLAME could achieve consistent accuracy comparable to its performance with simulated data.","This finding suggests that the drop in FLAME's accuracy when used on real data primarily arises from the difficulty of replicating the full complexity of real data in the training sample."],"url":"http://arxiv.org/abs/2403.07498v1","category":"astro-ph.CO"}
{"created":"2024-03-12 10:37:21","title":"Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system","abstract":"In this article, we introduce the concepts of Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system associated to an infinite countable discrete amenable group action. We obtain the dichotomy result to Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system when the corresponding skew product transformation is minimal and \\Omega is finite.","sentences":["In this article, we introduce the concepts of Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system associated to an infinite countable discrete amenable group action.","We obtain the dichotomy result to Weyl mean equicontinuity and Weyl mean sensitivity of a random dynamical system when the corresponding skew product transformation is minimal and \\Omega is finite."],"url":"http://arxiv.org/abs/2403.07497v1","category":"math.DS"}
{"created":"2024-03-12 10:33:26","title":"SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM","abstract":"We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering in real-time. In this system, we incorporate semantic feature embedding into 3D Gaussian representation, which effectively encodes semantic information within the spatial layout of the environment for precise semantic scene representation. Furthermore, we propose feature-level loss for updating 3D Gaussian representation, enabling higher-level guidance for 3D Gaussian optimization. In addition, to reduce cumulative drift and improve reconstruction accuracy, we introduce semantic-informed bundle adjustment leveraging semantic associations for joint optimization of 3D Gaussian representation and camera poses, leading to more robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates superior performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in novel-view semantic synthesis and 3D semantic mapping.","sentences":["We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering in real-time.","In this system, we incorporate semantic feature embedding into 3D Gaussian representation, which effectively encodes semantic information within the spatial layout of the environment for precise semantic scene representation.","Furthermore, we propose feature-level loss for updating 3D Gaussian representation, enabling higher-level guidance for 3D Gaussian optimization.","In addition, to reduce cumulative drift and improve reconstruction accuracy, we introduce semantic-informed bundle adjustment leveraging semantic associations for joint optimization of 3D Gaussian representation and camera poses, leading to more robust tracking and consistent mapping.","Our SemGauss-SLAM method demonstrates superior performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in novel-view semantic synthesis and 3D semantic mapping."],"url":"http://arxiv.org/abs/2403.07494v1","category":"cs.RO"}
{"created":"2024-03-12 10:32:35","title":"Signed graphs in data sciences via communicability geometry","abstract":"Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs.","sentences":["Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist.","These include data from biological, ecological, and social systems.","Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical.","We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way.","They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs."],"url":"http://arxiv.org/abs/2403.07493v1","category":"math.MG"}
{"created":"2024-03-12 10:28:24","title":"Spherical $p$-group complexes arising from finite groups of Lie type","abstract":"We show that the $p$-group complex of a finite group $G$ is homotopy equivalent to a wedge of spheres of dimension at most $n$ if $G$ contains a self-centralising normal subgroup $H$ which is isomorphic to a group of Lie type and Lie rank $n$ in characteristic $p$. If in addition every order-$p$ element of $G$ induces an inner or field automorphism on $H$, the $p$-group complex of $G$ is $G$-homotopy equivalent to a spherical complex obtained from the Tits building of $H$.   We also prove that the reduced Euler characteristic of the $p$-group complex of a finite group $G$ is non-zero if $G$ has trivial $p$-core and $H$ is a self-centralising normal subgroup of $G$ which is a group of Lie type (in any characteristic), except possibly when $p=2$ and $H=A_n(4^a)$ ($n\\geq 2$) or $E_6(4^a)$. In particular, we conclude that the Euler characteristic of the $p$-group complex of an almost simple group does not vanish for $p\\geq 7$.","sentences":["We show that the $p$-group complex of a finite group $G$ is homotopy equivalent to a wedge of spheres of dimension at most $n$ if $G$ contains a self-centralising normal subgroup $H$ which is isomorphic to a group of Lie type and Lie rank $n$ in characteristic $p$. If in addition every order-$p$ element of $G$ induces an inner or field automorphism on $H$, the $p$-group complex of $G$ is $G$-homotopy equivalent to a spherical complex obtained from the Tits building of $H$.   We also prove that the reduced Euler characteristic of the $p$-group complex of a finite group $G$ is non-zero if $G$ has trivial $p$-core and $H$ is a self-centralising normal subgroup of $G$ which is a group of Lie type (in any characteristic), except possibly when $p=2$ and $H=A_n(4^a)$ ($n\\geq 2$) or $E_6(4^a)$. In particular, we conclude that the Euler characteristic of the $p$-group complex of an almost simple group does not vanish for $p\\geq 7$."],"url":"http://arxiv.org/abs/2403.07489v1","category":"math.GR"}
{"created":"2024-03-12 10:25:32","title":"A variational principle for entropy of a random dynamical system","abstract":"In this article, I give a definition of topological entropy for random dynamical systems associated to an infinite countable discrete amenable group action. I obtain a variational principle between the topological entropy and measurable fiber entropy of a random dynamical system.","sentences":["In this article, I give a definition of topological entropy for random dynamical systems associated to an infinite countable discrete amenable group action.","I obtain a variational principle between the topological entropy and measurable fiber entropy of a random dynamical system."],"url":"http://arxiv.org/abs/2403.07488v1","category":"math.DS"}
{"created":"2024-03-12 10:13:02","title":"A quantum oscillator interacting with a classical oscillator","abstract":"We study a quantum oscillator interacting and back-reacting on a classical oscillator. This can be done consistently provided the quantum system decoheres, while the backreaction has a stochastic component which causes the classical system to undergo diffusion. Nonetheless the state of the quantum oscillator can remain pure conditioned on the trajectory of the classical oscillator. We solve the system using the classical-quantum path integral formulation, and investigate slow moving regimes of either the classical or quantum oscillator. Lastly, we study the correlators of this classicalquantum setup. We are able to identify the free correlators of the theory and compute the full partition function perturbatively up to second order. This serves as a toy model for a number of other systems in which one system can be treated as effectively classical, such as a scalar quantum field interacting with another field undergoing decoherence, or a system emitting radiation, one of which is treated classically.","sentences":["We study a quantum oscillator interacting and back-reacting on a classical oscillator.","This can be done consistently provided the quantum system decoheres, while the backreaction has a stochastic component which causes the classical system to undergo diffusion.","Nonetheless the state of the quantum oscillator can remain pure conditioned on the trajectory of the classical oscillator.","We solve the system using the classical-quantum path integral formulation, and investigate slow moving regimes of either the classical or quantum oscillator.","Lastly, we study the correlators of this classicalquantum setup.","We are able to identify the free correlators of the theory and compute the full partition function perturbatively up to second order.","This serves as a toy model for a number of other systems in which one system can be treated as effectively classical, such as a scalar quantum field interacting with another field undergoing decoherence, or a system emitting radiation, one of which is treated classically."],"url":"http://arxiv.org/abs/2403.07479v1","category":"quant-ph"}
{"created":"2024-03-12 10:11:56","title":"$M$-ary partition polynomials","abstract":"Let $M=(m_{i})_{i=0}^{\\infty}$ be a sequence of integers such that $m_{0}=1$ and $m_{i}\\geq 2$ for $i\\geq 1$. In this paper we study $M$-ary partition polynomials $(p_{M}(n,t))_{n=0}^{\\infty}$ defined as the coefficient in the following power series expansion: \\begin{align*} \\prod_{i=0}^{\\infty}\\frac{1}{1-tq^{M_{i}}} = \\sum_{n=0}^{\\infty} p_{M}(n,t)q^{n}, \\end{align*} where $M_{i}=\\prod_{j=0}^{i}m_{j}$. In particular, we provide a detailed description of their rational roots and show, that all their complex roots have absolute values not greater than $2$. We also study arithmetic properties of $M$-ary partition polynomials. One of our main results says that if $n=a_{0}+a_{1}M_{1}+\\cdots +a_{k}M_{k}$ is a (unique) representation such that $a_{j}\\in\\{0,\\ldots ,m_{j+1}-1\\}$ for every $j$, then \\begin{align*} p_{M}(n,t)\\equiv t^{a_{0}}\\prod t^{a_{j}}f(a_{j}+1,t^{m_{j}-1}) \\pmod{g_{k}(t)}, \\end{align*} where $f(a,t):=\\frac{t^{a}-1}{t-1}$ and $g_{k}(t):=\\gcd \\big(t^{m_{1}+m_{2}-1}f(m_{2},t^{m_{1}-1}),\\ldots ,t^{m_{k}+m_{k+1}-1}f(m_{k+1},t^{m_{k}-1})\\big)$. This is a polynomial generalisation of the well-known characterisation modulo $m$ of the sequence of $m$-ary partition.","sentences":["Let $M=(m_{i})_{i=0}^{\\infty}$ be a sequence of integers such that $m_{0}=1$ and $m_{i}\\geq 2$ for $i\\geq 1$.","In this paper we study $M$-ary partition polynomials $(p_{M}(n,t))_{n=0}^{\\infty}$ defined as the coefficient in the following power series expansion: \\begin{align*} \\prod_{i=0}^{\\infty}\\frac{1}{1-tq^{M_{i}}} = \\sum_{n=0}^{\\infty} p_{M}(n,t)q^{n}, \\end{align*} where $M_{i}=\\prod_{j=0}^{i}m_{j}$. In particular, we provide a detailed description of their rational roots and show, that all their complex roots have absolute values not greater than $2$. We also study arithmetic properties of $M$-ary partition polynomials.","One of our main results says that if $n=a_{0}+a_{1}M_{1}+\\cdots +a_{k}M_{k}$ is a (unique) representation such that $a_{j}\\in\\{0,\\ldots ,m_{j+1}-1\\}$ for every $j$, then \\begin{align*} p_{M}(n,t)\\equiv t^{a_{0}}\\prod t^{a_{j}}f(a_{j}+1,t^{m_{j}-1}) \\pmod{g_{k}(t)}, \\end{align*} where $f(a,t):=\\frac{t^{a}-1}{t-1}$ and $g_{k}(t):=\\gcd \\big(t^{m_{1}+m_{2}-1}f(m_{2},t^{m_{1}-1}),\\ldots ,t^{m_{k}+m_{k+1}-1}f(m_{k+1},t^{m_{k}-1})\\big)$. This is a polynomial generalisation of the well-known characterisation modulo $m$ of the sequence of $m$-ary partition."],"url":"http://arxiv.org/abs/2403.07477v1","category":"math.CO"}
{"created":"2024-03-12 10:09:53","title":"Pedophysics: an open-source python package for soil geophysics","abstract":"This study introduces Pedophysics, an open-source Python package designed to facilitate solutions for users who work in the field of soil assessment using near-surface geophysical electromagnetic techniques. At the core of this software is the ability to translate geophysical data into specific soil properties (and vice-versa) using pedophysical models (PM). Pedophysical modelling techniques offer valuable insights into various realms including precision agriculture, soil health, resource prospecting, nutrient and land management, hydrogeology, and heritage conservation. In developing a tool for pedophysical modelling, some challenges emerged: selecting suitable PMs from the extensive literature, adapting these to specific conditions, and ensuring adequate data availability. While addressing these, we designed an automated workflow that implements robust PMs (selected after a throughout review), apply different modelling approaches based on soil characteristics and targeted properties, and employs pedotransfer functions and assumptions to integrate missing soil data into PMs. The capabilities of Pedophysics extend to handling complex scenarios such as fusing data from different instruments, incorporating continuous monitoring measurements, and soil calibration data. With these solutions, Pedophysics automates the process of deriving targeted soil and geophysical properties with state-of-art accuracy. Hereby, users can rely on Pedophysics to implement specific knowledge about pedophysical modeling. The software promotes global access to advanced soil geophysical solutions by being open-source and encouraging community contributions. Pedophysics is written in pure Python and has minimal dependencies. It can be easily installed from the Python Package Index (PyPI).","sentences":["This study introduces Pedophysics, an open-source Python package designed to facilitate solutions for users who work in the field of soil assessment using near-surface geophysical electromagnetic techniques.","At the core of this software is the ability to translate geophysical data into specific soil properties (and vice-versa) using pedophysical models (PM).","Pedophysical modelling techniques offer valuable insights into various realms including precision agriculture, soil health, resource prospecting, nutrient and land management, hydrogeology, and heritage conservation.","In developing a tool for pedophysical modelling, some challenges emerged: selecting suitable PMs from the extensive literature, adapting these to specific conditions, and ensuring adequate data availability.","While addressing these, we designed an automated workflow that implements robust PMs (selected after a throughout review), apply different modelling approaches based on soil characteristics and targeted properties, and employs pedotransfer functions and assumptions to integrate missing soil data into PMs.","The capabilities of Pedophysics extend to handling complex scenarios such as fusing data from different instruments, incorporating continuous monitoring measurements, and soil calibration data.","With these solutions, Pedophysics automates the process of deriving targeted soil and geophysical properties with state-of-art accuracy.","Hereby, users can rely on Pedophysics to implement specific knowledge about pedophysical modeling.","The software promotes global access to advanced soil geophysical solutions by being open-source and encouraging community contributions.","Pedophysics is written in pure Python and has minimal dependencies.","It can be easily installed from the Python Package Index (PyPI)."],"url":"http://arxiv.org/abs/2403.07473v1","category":"physics.geo-ph"}
{"created":"2024-03-12 10:00:06","title":"One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices","abstract":"Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage.   In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks. It efficiently extracts features from one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to identify deviations from benign executions. The core intuition behind RAGE is to exploit the correspondence between execution trace, execution graph, and execution embeddings to eliminate the unrealistic requirement of having access to a complete CFG.   We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects 40 real-world attacks on embedded software; (ii) Further, we stress our scheme with synthetic return-oriented programming (ROP) and data-oriented programming (DOP) attacks on the real-world embedded software benchmark Embench, achieving 98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP attack detection, with an FPR of 5.47%.","sentences":["Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover).","Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware.","Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage.   ","In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements.","RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks.","It efficiently extracts features from one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to identify deviations from benign executions.","The core intuition behind RAGE is to exploit the correspondence between execution trace, execution graph, and execution embeddings to eliminate the unrealistic requirement of having access to a complete CFG.   ","We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects 40 real-world attacks on embedded software; (ii) Further, we stress our scheme with synthetic return-oriented programming (ROP) and data-oriented programming (DOP) attacks on the real-world embedded software benchmark Embench, achieving 98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP attack detection, with an FPR of 5.47%."],"url":"http://arxiv.org/abs/2403.07465v1","category":"cs.CR"}
{"created":"2024-03-12 09:56:50","title":"Localization-Delocalization Transitions in Non-Hermitian Aharonov-Bohm Cages","abstract":"A unique feature of non-Hermitian systems is the extreme sensitivity of the eigenspectrum to boundary conditions with the emergence of the non-Hermitian skin effect (NHSE). A NHSE originates from the point-gap topology of complex eigenspectrum, where an extensive number of eigenstates are anomalously localized at the boundary driven by nonreciprocal dissipation. Two different approaches to create localization are disorder and flat-band spectrum, and their interplay can lead to the anomalous inverse Anderson localization, where the Bernoulli anti-symmetric disorder induce mobility in a full-flat band system in the presence of Aharonov-Bohm (AB) Cage. In this work, we study the localization-delocalization transitions due to the interplay of the point-gap topology, flat band and correlated disorder in the one-dimensional rhombic lattice, where both its Hermitian and non-Hermitian structures show AB cage in the presence of magnetic flux. Although it remains the coexistence of localization and delocalization for the Hermitian rhombic lattice in the presence of the random anti-symmetric disorder, it surprisingly becomes complete delocalization, accompanied by the emergence of NHSE. To further study the effects from the Bernoulli anti-symmetric disorder, we found the similar NHSE due to the interplay of the point-gap topology, correlated disorder and flat bands. Our anomalous localization-delocalization property can be experimentally tested in the classical physical platform, such as electrical circuit.","sentences":["A unique feature of non-Hermitian systems is the extreme sensitivity of the eigenspectrum to boundary conditions with the emergence of the non-Hermitian skin effect (NHSE).","A NHSE originates from the point-gap topology of complex eigenspectrum, where an extensive number of eigenstates are anomalously localized at the boundary driven by nonreciprocal dissipation.","Two different approaches to create localization are disorder and flat-band spectrum, and their interplay can lead to the anomalous inverse Anderson localization, where the Bernoulli anti-symmetric disorder induce mobility in a full-flat band system in the presence of Aharonov-Bohm (AB) Cage.","In this work, we study the localization-delocalization transitions due to the interplay of the point-gap topology, flat band and correlated disorder in the one-dimensional rhombic lattice, where both its Hermitian and non-Hermitian structures show AB cage in the presence of magnetic flux.","Although it remains the coexistence of localization and delocalization for the Hermitian rhombic lattice in the presence of the random anti-symmetric disorder, it surprisingly becomes complete delocalization, accompanied by the emergence of NHSE.","To further study the effects from the Bernoulli anti-symmetric disorder, we found the similar NHSE due to the interplay of the point-gap topology, correlated disorder and flat bands.","Our anomalous localization-delocalization property can be experimentally tested in the classical physical platform, such as electrical circuit."],"url":"http://arxiv.org/abs/2403.07459v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 09:47:15","title":"Humans-in-the-Building: Getting Rid of Thermostats for Optimal Thermal Comfort Control in Energy Management Systems","abstract":"Given the widespread attention to individual thermal comfort, coupled with significant energy-saving potential inherent in energy management systems for optimizing indoor environments, this paper aims to introduce advanced \"Humans-in-the-building\" control techniques to redefine the paradigm of indoor temperature design. Firstly, we innovatively redefine the role of individuals in the control loop, establishing a model for users' thermal comfort and constructing discomfort signals based on individual preferences. Unlike traditional temperature-centric approaches, \"thermal comfort control\" prioritizes personalized comfort. Then, considering the diversity among users, we propose a novel method to determine the optimal indoor temperature range, thus minimizing discomfort for various users and reducing building energy consumption. Finally, the efficacy of the \"thermal comfort control\" approach is substantiated through simulations conducted using Matlab.","sentences":["Given the widespread attention to individual thermal comfort, coupled with significant energy-saving potential inherent in energy management systems for optimizing indoor environments, this paper aims to introduce advanced \"Humans-in-the-building\" control techniques to redefine the paradigm of indoor temperature design.","Firstly, we innovatively redefine the role of individuals in the control loop, establishing a model for users' thermal comfort and constructing discomfort signals based on individual preferences.","Unlike traditional temperature-centric approaches, \"thermal comfort control\" prioritizes personalized comfort.","Then, considering the diversity among users, we propose a novel method to determine the optimal indoor temperature range, thus minimizing discomfort for various users and reducing building energy consumption.","Finally, the efficacy of the \"thermal comfort control\" approach is substantiated through simulations conducted using Matlab."],"url":"http://arxiv.org/abs/2403.07453v1","category":"eess.SY"}
{"created":"2024-03-12 09:37:22","title":"Ab-initio variational wave functions for the time-dependent many-electron Schr\u00f6dinger equation","abstract":"Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials. However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space. This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations. The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution. To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations. We also show that we can incorporate neural networks to parameterize these functions. The time-dependent variational Monte Carlo technique is employed to efficiently compute the optimal time-dependent parameters. The approach is demonstrated in three distinct systems: the solvable harmonic interaction model, the dynamics of a diatomic molecule in intense laser fields, and a quenched quantum dot. In all cases, we show clear signatures of many-body correlations in the dynamics not captured by mean-field methods. The results showcase the ability of our variational approach to accurately capture the time evolution of quantum states, providing insight into the quantum dynamics of interacting electronic systems, beyond the capabilities of mean-field.","sentences":["Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials.","However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space.","This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations.","The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution.","To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations.","We also show that we can incorporate neural networks to parameterize these functions.","The time-dependent variational Monte Carlo technique is employed to efficiently compute the optimal time-dependent parameters.","The approach is demonstrated in three distinct systems: the solvable harmonic interaction model, the dynamics of a diatomic molecule in intense laser fields, and a quenched quantum dot.","In all cases, we show clear signatures of many-body correlations in the dynamics not captured by mean-field methods.","The results showcase the ability of our variational approach to accurately capture the time evolution of quantum states, providing insight into the quantum dynamics of interacting electronic systems, beyond the capabilities of mean-field."],"url":"http://arxiv.org/abs/2403.07447v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 09:32:41","title":"Proxy Methods for Domain Adaptation","abstract":"We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder. We develop a two-stage kernel estimation approach to adapt to complex distribution shifts in both settings. In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder.","sentences":["We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels.","In this setting, neither the covariate shift nor the label shift assumptions apply.","Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available.","We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables.","We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder.","We develop a two-stage kernel estimation approach to adapt to complex distribution shifts in both settings.","In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder."],"url":"http://arxiv.org/abs/2403.07442v1","category":"cs.LG"}
{"created":"2024-03-12 09:30:37","title":"Experimental analysis of the TRC benchmark system","abstract":"The Tribomechadynamics Research Challenge (TRC) was a blind prediction of the vibration behavior of a thin plate clamped on two sides using bolted joints. The first bending mode's natural frequency and damping ratio were requested as function of the amplitude, starting from the linear regime until high levels, where both frictional contact and nonlinear bending-stretching coupling become relevant. The predictions were confronted with experimental results in a companion paper; the present article addresses the experimental analysis of this benchmark system. Amplitude-dependent modal data was obtained from phase resonance and response controlled tests. An original variant of response controlled testing is proposed: Instead of a fixed frequency interval, a fixed phase interval is analyzed. This way, the high excitation levels required outside resonance, which could activate unwanted exciter nonlinearity, are avoided. Consistency of testing methods is carefully analyzed. Overall, these measures have permitted to gain high confidence in the acquired modal data. The different sources of the remaining uncertainty were further analyzed. A low reassembly-variability but a moderate time-variability were identified, where the latter is attributed to some thermal sensitivity of the system. Two nominally identical plates were analyzed, which both have an appreciable initial curvature, and a significant effect on the vibration behavior was found depending on whether the plate is aligned/misaligned with the support structure. Further, a 1:2 nonlinear modal interaction with the first torsion mode was observed, which only occurs in the aligned configurations.","sentences":["The Tribomechadynamics Research Challenge (TRC) was a blind prediction of the vibration behavior of a thin plate clamped on two sides using bolted joints.","The first bending mode's natural frequency and damping ratio were requested as function of the amplitude, starting from the linear regime until high levels, where both frictional contact and nonlinear bending-stretching coupling become relevant.","The predictions were confronted with experimental results in a companion paper; the present article addresses the experimental analysis of this benchmark system.","Amplitude-dependent modal data was obtained from phase resonance and response controlled tests.","An original variant of response controlled testing is proposed: Instead of a fixed frequency interval, a fixed phase interval is analyzed.","This way, the high excitation levels required outside resonance, which could activate unwanted exciter nonlinearity, are avoided.","Consistency of testing methods is carefully analyzed.","Overall, these measures have permitted to gain high confidence in the acquired modal data.","The different sources of the remaining uncertainty were further analyzed.","A low reassembly-variability but a moderate time-variability were identified, where the latter is attributed to some thermal sensitivity of the system.","Two nominally identical plates were analyzed, which both have an appreciable initial curvature, and a significant effect on the vibration behavior was found depending on whether the plate is aligned/misaligned with the support structure.","Further, a 1:2 nonlinear modal interaction with the first torsion mode was observed, which only occurs in the aligned configurations."],"url":"http://arxiv.org/abs/2403.07438v1","category":"eess.SP"}
{"created":"2024-03-12 09:22:52","title":"JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection","abstract":"Event-based moving object detection is a challenging task, where static background and moving object are mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object. However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object. We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection. Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image. In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two reasoning stages to extract the final moving object region. This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13\\%.","sentences":["Event-based moving object detection is a challenging task, where static background and moving object are mixed together.","Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object.","However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object.","We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp.","Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection.","Specifically, we first compensate the motion of background events using inertial measurement unit.","In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image.","In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background.","Finally, we fuse the results from the two reasoning stages to extract the final moving object region.","This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure.","Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13\\%."],"url":"http://arxiv.org/abs/2403.07436v1","category":"cs.CV"}
{"created":"2024-03-12 09:22:26","title":"Broadened-beam Uniform Rectangular Array Coefficient Design in LEO SatComs Under Quality of Service and Constant Modulus Constraints","abstract":"Satellite communications (SatComs) are anticipated to provide global Internet access. Low Earth orbit (LEO) satellites (SATs) have the advantage of providing higher downlink capacity owing to their smaller link budget compared with medium Earth orbit (MEO) and geostationary Earth orbit (GEO) SATs. In this paper, beam-broadening algorithms for uniform rectangular arrays (URAs) in LEO SatComs were studied. The proposed method is the first of its kind that jointly considers the path loss variation from SAT to user terminal (UT) due to the Earth's curvature to guarantee quality of service (QoS) inspired by the synthesis of isoflux radiation patterns in the literature, constant modulus constraint (CMC) favored for maximizing power amplifier (PA) efficiency, and out-of-beam radiation suppression to avoid interference. A URA design problem is formulated and decomposed into two uniform linear array (ULA) design subproblems utilizing the idea of Kronecker product beamforming to reduce the computational complexity of designing URA.The non-convex ULA subproblems are solved by a convex iterative algorithm. Simulation results reveal the advantages of the proposed method for suppressing out-of-beam radiation and achieving design criteria. In addition, channel capacity evaluation is carried out and shows that the proposed ``broadened-beam\" beamformers can offer capacities that are at least four times greater than ``narrow-beam\" beamformers employing an array steering vector when beam transition time is taken into account. The proposed method holds potential for LEO broadcasting applications such as digital video broadcasting (DVB).","sentences":["Satellite communications (SatComs) are anticipated to provide global Internet access.","Low Earth orbit (LEO) satellites (SATs) have the advantage of providing higher downlink capacity owing to their smaller link budget compared with medium Earth orbit (MEO) and geostationary Earth orbit (GEO) SATs.","In this paper, beam-broadening algorithms for uniform rectangular arrays (URAs) in LEO SatComs were studied.","The proposed method is the first of its kind that jointly considers the path loss variation from SAT to user terminal (UT) due to the Earth's curvature to guarantee quality of service (QoS) inspired by the synthesis of isoflux radiation patterns in the literature, constant modulus constraint (CMC) favored for maximizing power amplifier (PA) efficiency, and out-of-beam radiation suppression to avoid interference.","A URA design problem is formulated and decomposed into two uniform linear array (ULA) design subproblems utilizing the idea of Kronecker product beamforming to reduce the computational complexity of designing URA.The non-convex ULA subproblems are solved by a convex iterative algorithm.","Simulation results reveal the advantages of the proposed method for suppressing out-of-beam radiation and achieving design criteria.","In addition, channel capacity evaluation is carried out and shows that the proposed ``broadened-beam\" beamformers can offer capacities that are at least four times greater than ``narrow-beam\" beamformers employing an array steering vector when beam transition time is taken into account.","The proposed method holds potential for LEO broadcasting applications such as digital video broadcasting (DVB)."],"url":"http://arxiv.org/abs/2403.07435v1","category":"eess.SP"}
{"created":"2024-03-12 09:17:21","title":"DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images","abstract":"We propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation. The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup. The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone. The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling. The new approach is validated on labeled, multi-modal MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets. Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy. This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification.","sentences":["We propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation.","The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup.","The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone.","The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling.","The new approach is validated on labeled, multi-modal MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets.","Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy.","This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification."],"url":"http://arxiv.org/abs/2403.07434v1","category":"eess.IV"}
{"created":"2024-03-12 09:08:19","title":"White dwarf systems: exoplanets and debris disks","abstract":"Although there is abundant and diverse observational evidence in support of white dwarf stars hosting planets or debris disks which form in the catastrophic destruction of various planetary bodies, the key processes that explain these observations are still being intensely investigated. The study of white dwarf planetary systems offers a unique perspective on exo-solar composition, that cannot be obtained by any other means. This chapter describes the various observational techniques that are used in order to find and characterize exo-planets and debris disks around white dwarfs. In turn, it discusses how to theoretically interpret these observations by surveying an array of various research tools and models currently employed in this field.","sentences":["Although there is abundant and diverse observational evidence in support of white dwarf stars hosting planets or debris disks which form in the catastrophic destruction of various planetary bodies, the key processes that explain these observations are still being intensely investigated.","The study of white dwarf planetary systems offers a unique perspective on exo-solar composition, that cannot be obtained by any other means.","This chapter describes the various observational techniques that are used in order to find and characterize exo-planets and debris disks around white dwarfs.","In turn, it discusses how to theoretically interpret these observations by surveying an array of various research tools and models currently employed in this field."],"url":"http://arxiv.org/abs/2403.07427v1","category":"astro-ph.EP"}
{"created":"2024-03-12 09:08:07","title":"Self-phoretic oscillatory motion in a harmonic trap","abstract":"We consider the motion of a harmonically trapped overdamped particle, which is submitted to a self-phoretic force, that is proportional to the gradient of a diffusive field for which the particle itself is the source. In agreement with existing results for free particles or particles in a bounded domain, we find that the system exhibits a transition between an immobile phase, where the particle stays at the center of the trap, and an oscillatory state. We perform an exact analysis giving access to the bifurcation threshold, as well as the frequency of oscillations and their amplitude near the threshold. Our analysis also characterizes the shape of two-dimensional oscillations, that take place along a circle or a straight line. Our results are confirmed by numerical simulations.","sentences":["We consider the motion of a harmonically trapped overdamped particle, which is submitted to a self-phoretic force, that is proportional to the gradient of a diffusive field for which the particle itself is the source.","In agreement with existing results for free particles or particles in a bounded domain, we find that the system exhibits a transition between an immobile phase, where the particle stays at the center of the trap, and an oscillatory state.","We perform an exact analysis giving access to the bifurcation threshold, as well as the frequency of oscillations and their amplitude near the threshold.","Our analysis also characterizes the shape of two-dimensional oscillations, that take place along a circle or a straight line.","Our results are confirmed by numerical simulations."],"url":"http://arxiv.org/abs/2403.07426v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 09:06:51","title":"Modulational instability of nonuniformly damped, broad-banded waves: applications to waves in sea-ice","abstract":"This paper sets out to explore the modulational (or Benjamin-Feir) instability of a monochromatic wave propagating in the presence of damping such as that induced by sea-ice on the ocean surface. The fundamental wave motion is modelled using the spatial Zakharov equation, to which either uniform or non-uniform (frequency dependent) damping is added. By means of mode truncation the spatial analogue of the classical Benjamin-Feir instability can be studied analytically using dynamical systems techniques. The formulation readily yields the free surface envelope, giving insight into the physical implications of damping on the modulational instability. The evolution of an initially unstable mode is also studied numerically by integrating the damped, spatial Zakharov equation, in order to complement the analytical theory. This sheds light on the effects of damping on spectral broadening arising from this instability.","sentences":["This paper sets out to explore the modulational (or Benjamin-Feir) instability of a monochromatic wave propagating in the presence of damping such as that induced by sea-ice on the ocean surface.","The fundamental wave motion is modelled using the spatial Zakharov equation, to which either uniform or non-uniform (frequency dependent) damping is added.","By means of mode truncation the spatial analogue of the classical Benjamin-Feir instability can be studied analytically using dynamical systems techniques.","The formulation readily yields the free surface envelope, giving insight into the physical implications of damping on the modulational instability.","The evolution of an initially unstable mode is also studied numerically by integrating the damped, spatial Zakharov equation, in order to complement the analytical theory.","This sheds light on the effects of damping on spectral broadening arising from this instability."],"url":"http://arxiv.org/abs/2403.07425v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 09:00:58","title":"On the locomotion of the slider within a self-adaptive beam-slider system","abstract":"A beam-slider system is considered whose passive self-adaption relies on an intricate locomotion process involving both frictional and unilateral contact. The system also exploits geometric nonlinearity to achieve broadband efficacy. The dynamics of the system take place on three distinct time scales: On the fast time scale of the harmonic base excitation are the vibrations and the locomotion cycle. On the slow time scale, the slider changes its position along the beam, and the overall vibration level varies. Finally, on an intermediate time scale, strong modulations of the vibration amplitude may take place. In the present work, first, an analytical approximation of the beam's response on the slow time scale is derived as function of the slider position, which is a crucial prerequisite for identifying the main drivers of the slider's locomotion. Then, the most important forms of locomotion are described and approximations of their individual contribution to the overall slider transport are estimated. Finally, the theoretical results are compared against numerical results obtained from an experimentally validated model.","sentences":["A beam-slider system is considered whose passive self-adaption relies on an intricate locomotion process involving both frictional and unilateral contact.","The system also exploits geometric nonlinearity to achieve broadband efficacy.","The dynamics of the system take place on three distinct time scales: On the fast time scale of the harmonic base excitation are the vibrations and the locomotion cycle.","On the slow time scale, the slider changes its position along the beam, and the overall vibration level varies.","Finally, on an intermediate time scale, strong modulations of the vibration amplitude may take place.","In the present work, first, an analytical approximation of the beam's response on the slow time scale is derived as function of the slider position, which is a crucial prerequisite for identifying the main drivers of the slider's locomotion.","Then, the most important forms of locomotion are described and approximations of their individual contribution to the overall slider transport are estimated.","Finally, the theoretical results are compared against numerical results obtained from an experimentally validated model."],"url":"http://arxiv.org/abs/2403.07423v1","category":"eess.SY"}
{"created":"2024-03-12 08:45:51","title":"Cabello's nonlocality argument for multisetting high-dimensional systems and its experimental test","abstract":"Recent advancements have expanded Hardy's nonlocality arguments into multisetting and multidimensional systems to enhance quantum correlations. In comparison with Hardy's nonlocal argument, Cabello's nonlocal argument (CNA) emerges as a superior choice for illustrating nonlocal features. An open question persists regarding the potential extension of CNA to arbitrary (k, d) scenarios. Here, we answer this question both in theory and experiment. Theoretically, by utilizing compatibility graphs, we construct a new logical framework for multisetting and multidimensional CNA, demonstrating an increase in the maximum successful probability with setting k and dimension d. Experimentally, by employing controllable photonic orbital angular momentum entanglement, we exhibit nonlocality with an experimentally recorded probability of 20.29% in the (2, 4) scenario and 28.72% in the (6, 2) scenario. Our work showcases a sharper contradiction between quantum mechanics and classical theory, surpassing the bound limited by the original version.","sentences":["Recent advancements have expanded Hardy's nonlocality arguments into multisetting and multidimensional systems to enhance quantum correlations.","In comparison with Hardy's nonlocal argument, Cabello's nonlocal argument (CNA) emerges as a superior choice for illustrating nonlocal features.","An open question persists regarding the potential extension of CNA to arbitrary (k, d) scenarios.","Here, we answer this question both in theory and experiment.","Theoretically, by utilizing compatibility graphs, we construct a new logical framework for multisetting and multidimensional CNA, demonstrating an increase in the maximum successful probability with setting k and dimension d. Experimentally, by employing controllable photonic orbital angular momentum entanglement, we exhibit nonlocality with an experimentally recorded probability of 20.29% in the (2, 4) scenario and 28.72% in the (6, 2) scenario.","Our work showcases a sharper contradiction between quantum mechanics and classical theory, surpassing the bound limited by the original version."],"url":"http://arxiv.org/abs/2403.07417v1","category":"quant-ph"}
{"created":"2024-03-12 08:39:08","title":"GPU-Accelerated Vecchia Approximations of Gaussian Processes for Geospatial Data using Batched Matrix Computations","abstract":"Gaussian processes (GPs) are commonly used for geospatial analysis, but they suffer from high computational complexity when dealing with massive data. For instance, the log-likelihood function required in estimating the statistical model parameters for geospatial data is a computationally intensive procedure that involves computing the inverse of a covariance matrix with size n X n, where n represents the number of geographical locations. As a result, in the literature, studies have shifted towards approximation methods to handle larger values of n effectively while maintaining high accuracy. These methods encompass a range of techniques, including low-rank and sparse approximations. Vecchia approximation is one of the most promising methods to speed up evaluating the log-likelihood function. This study presents a parallel implementation of the Vecchia approximation, utilizing batched matrix computations on contemporary GPUs. The proposed implementation relies on batched linear algebra routines to efficiently execute individual conditional distributions in the Vecchia algorithm. We rely on the KBLAS linear algebra library to perform batched linear algebra operations, reducing the time to solution compared to the state-of-the-art parallel implementation of the likelihood estimation operation in the ExaGeoStat software by up to 700X, 833X, 1380X on 32GB GV100, 80GB A100, and 80GB H100 GPUs, respectively. We also successfully manage larger problem sizes on a single NVIDIA GPU, accommodating up to 1M locations with 80GB A100 and H100 GPUs while maintaining the necessary application accuracy. We further assess the accuracy performance of the implemented algorithm, identifying the optimal settings for the Vecchia approximation algorithm to preserve accuracy on two real geospatial datasets: soil moisture data in the Mississippi Basin area and wind speed data in the Middle East.","sentences":["Gaussian processes (GPs) are commonly used for geospatial analysis, but they suffer from high computational complexity when dealing with massive data.","For instance, the log-likelihood function required in estimating the statistical model parameters for geospatial data is a computationally intensive procedure that involves computing the inverse of a covariance matrix with size n X n, where n represents the number of geographical locations.","As a result, in the literature, studies have shifted towards approximation methods to handle larger values of n effectively while maintaining high accuracy.","These methods encompass a range of techniques, including low-rank and sparse approximations.","Vecchia approximation is one of the most promising methods to speed up evaluating the log-likelihood function.","This study presents a parallel implementation of the Vecchia approximation, utilizing batched matrix computations on contemporary GPUs.","The proposed implementation relies on batched linear algebra routines to efficiently execute individual conditional distributions in the Vecchia algorithm.","We rely on the KBLAS linear algebra library to perform batched linear algebra operations, reducing the time to solution compared to the state-of-the-art parallel implementation of the likelihood estimation operation in the ExaGeoStat software by up to 700X, 833X, 1380X on 32GB GV100, 80GB A100, and 80GB H100 GPUs, respectively.","We also successfully manage larger problem sizes on a single NVIDIA GPU, accommodating up to 1M locations with 80GB A100 and H100 GPUs while maintaining the necessary application accuracy.","We further assess the accuracy performance of the implemented algorithm, identifying the optimal settings for the Vecchia approximation algorithm to preserve accuracy on two real geospatial datasets: soil moisture data in the Mississippi Basin area and wind speed data in the Middle East."],"url":"http://arxiv.org/abs/2403.07412v1","category":"stat.CO"}
{"created":"2024-03-12 08:35:55","title":"Universal Slepian-Wolf coding for individual sequences","abstract":"We establish a coding theorem and a matching converse theorem for separate encodings and joint decoding of individual sequences using finite-state machines. The achievable rate region is characterized in terms of the Lempel-Ziv (LZ) complexities, the conditional LZ complexities and the joint LZ complexity of the two source sequences. An important feature that is needed to this end, which may be interesting on its own right, is a certain asymptotic form of a chain rule for LZ complexities, which we establish in this work. The main emphasis in the achievability scheme is on the universal decoder and its properties. We then show that the achievable rate region is universally attainable by a modified version of Draper's universal incremental Slepian-Wolf (SW) coding scheme, provided that there exists a low-rate reliable feedback link.","sentences":["We establish a coding theorem and a matching converse theorem for separate encodings and joint decoding of individual sequences using finite-state machines.","The achievable rate region is characterized in terms of the Lempel-Ziv (LZ) complexities, the conditional LZ complexities and the joint LZ complexity of the two source sequences.","An important feature that is needed to this end, which may be interesting on its own right, is a certain asymptotic form of a chain rule for LZ complexities, which we establish in this work.","The main emphasis in the achievability scheme is on the universal decoder and its properties.","We then show that the achievable rate region is universally attainable by a modified version of Draper's universal incremental Slepian-Wolf (SW) coding scheme, provided that there exists a low-rate reliable feedback link."],"url":"http://arxiv.org/abs/2403.07409v1","category":"cs.IT"}
{"created":"2024-03-12 08:33:34","title":"Radon Concentration Measurement with a High-Sensitivity Radon Detector at the Yemilab","abstract":"The radiation emitted from radon is a critical background in rare event search experiments conducted at the Yemi Underground Laboratory (Yemilab) in Jeongseon, Korea. A Radon Reduction System(RRS) has been developed and installed in Yemilab to reduce radon concentration in the air. The RRS primarily provides a purified air of 50 m3/h to the cleanroom used to assemble crystal detectors in the AMoRE, a neutrinoless double beta decay search experiment. RRS can reduce the radon level by a factor of 300, so a high-sensitivity radon detector was required. A highly sensitive radon detector was constructed using a 70 L chamber with a large PIN photodiode to measure radon concentration in the purified air. The radon detector shows an excellent resolution of 72 keV (FWHM) for 6.003 MeV alphas from 218Po decay and a sensitivity down to 23.8 +- 2.1 mBq/m3 with a boil-off N2 gas sample. The radon concentration level from the RRS measured by the radon detector was below 0.29 Bq/m3 with a reduction factor of about 300.","sentences":["The radiation emitted from radon is a critical background in rare event search experiments conducted at the Yemi Underground Laboratory (Yemilab) in Jeongseon, Korea.","A Radon Reduction System(RRS) has been developed and installed in Yemilab to reduce radon concentration in the air.","The RRS primarily provides a purified air of 50 m3/h to the cleanroom used to assemble crystal detectors in the AMoRE, a neutrinoless double beta decay search experiment.","RRS can reduce the radon level by a factor of 300, so a high-sensitivity radon detector was required.","A highly sensitive radon detector was constructed using a 70 L chamber with a large PIN photodiode to measure radon concentration in the purified air.","The radon detector shows an excellent resolution of 72 keV (FWHM) for 6.003 MeV alphas from 218Po decay and a sensitivity down to 23.8 +- 2.1 mBq/m3 with a boil-off N2 gas sample.","The radon concentration level from the RRS measured by the radon detector was below 0.29 Bq/m3 with a reduction factor of about 300."],"url":"http://arxiv.org/abs/2403.07405v1","category":"physics.ins-det"}
{"created":"2024-03-12 08:08:58","title":"Predicting the Slowing of Stellar Differential Rotation by Instability-Driven Turbulence","abstract":"Differentially rotating stars and planets transport angular momentum internally due to turbulence at rates that have long been a challenge to predict reliably. We develop a self-consistent saturation theory, using a statistical closure approximation, for hydrodynamic turbulence driven by the axisymmetric Goldreich--Schubert--Fricke (GSF) instability at the stellar equator with radial differential rotation. This instability arises when fast thermal diffusion eliminates the stabilizing effects of buoyancy forces in a system where a stabilizing entropy gradient dominates over the destabilizing angular momentum gradient. Our turbulence closure invokes a dominant three-wave coupling between pairs of linearly unstable eigenmodes and a near-zero frequency, viscously damped eigenmode that features latitudinal jets. We derive turbulent transport rates of momentum and heat, and provide them in analytic forms. Such formulae, free of tunable model parameters, are tested against direct numerical simulations; the comparison shows good agreement. They improve upon prior quasi-linear or ``parasitic saturation\" models containing a free parameter. Given model correspondences, we also extend this theory to heat and compositional transport for axisymmetric thermohaline instability-driven turbulence in certain regimes.","sentences":["Differentially rotating stars and planets transport angular momentum internally due to turbulence at rates that have long been a challenge to predict reliably.","We develop a self-consistent saturation theory, using a statistical closure approximation, for hydrodynamic turbulence driven by the axisymmetric Goldreich--Schubert--Fricke (GSF) instability at the stellar equator with radial differential rotation.","This instability arises when fast thermal diffusion eliminates the stabilizing effects of buoyancy forces in a system where a stabilizing entropy gradient dominates over the destabilizing angular momentum gradient.","Our turbulence closure invokes a dominant three-wave coupling between pairs of linearly unstable eigenmodes and a near-zero frequency, viscously damped eigenmode that features latitudinal jets.","We derive turbulent transport rates of momentum and heat, and provide them in analytic forms.","Such formulae, free of tunable model parameters, are tested against direct numerical simulations; the comparison shows good agreement.","They improve upon prior quasi-linear or ``parasitic saturation\" models containing a free parameter.","Given model correspondences, we also extend this theory to heat and compositional transport for axisymmetric thermohaline instability-driven turbulence in certain regimes."],"url":"http://arxiv.org/abs/2403.07395v1","category":"astro-ph.SR"}
{"created":"2024-03-12 08:07:17","title":"A regularization theorem for bounded-degree self-maps","abstract":"Let $K$ be an algebraically closed field of arbitrary characteristic and let $X$ be an irreducible projective variety over $K$. Let $G\\subseteq\\text{Bir}(X)$ be a bounded-degree subgroup. We prove that there exists an irreducible projective variety $Y$ birational to $X$, such that every element of $G$ becomes an automorphism of $Y$ after the birational transformation. If $K=\\mathbb{C}$, this result is stated in [Can14, Theorem 2.5] and the proof backs to [HZ96, Section 5]. The proof in [HZ96] is not purely algebraic. Inheriting the methods in [HZ96], we give a purely algebraic proof of this statement in arbitrary characteristic. We will also discuss a corollary of this result which is useful in arithmetic dynamics.","sentences":["Let $K$ be an algebraically closed field of arbitrary characteristic and let $X$ be an irreducible projective variety over $K$. Let","$G\\subseteq\\text{Bir}(X)$ be a bounded-degree subgroup.","We prove that there exists an irreducible projective variety $Y$ birational to $X$, such that every element of $G$ becomes an automorphism of $Y$ after the birational transformation.","If $K=\\mathbb{C}$, this result is stated in [Can14, Theorem 2.5] and the proof backs to [HZ96, Section 5].","The proof in [HZ96] is not purely algebraic.","Inheriting the methods in [HZ96], we give a purely algebraic proof of this statement in arbitrary characteristic.","We will also discuss a corollary of this result which is useful in arithmetic dynamics."],"url":"http://arxiv.org/abs/2403.07394v1","category":"math.AG"}
{"created":"2024-03-12 07:58:19","title":"Towards adiabatic-connection interpolation model with broader applicability","abstract":"The Adiabatic Connection Integrand Interpolation (ACII) method represents a general path for calculating correlation energies in electronic systems within the Den sity Functional Theory. ACII functionals include both exact-exchange and the second-order correlation energy, as well as an interpolating function toward the strictly-correlated electron (SCE) regime. Several interpolating functions have been proposed in the last years targeting different properties, yet an accurate ACII approach with broad applicability is sti ll missing. Recently, we have proposed an ACII functional that was made accurate for the three-dimensional (3D) uniform electron gas as well as for model metal clusters. In this work we present an ACII functional (named genISI2) which is very accurate for both three-dimensional (3D) and two-dimensional (2D) uniform electron gases and for the q uasi-2D infinite barrier model, where most of the exchange-correlation functionals fail badly, as well as for strongly correlated two-electrons systems. Using the exact-exchange Kohn-Sham orbitals, we have also assessed the genISI2 for various molecular systems, showing a superior performance with respect to the o ther ACII methods for total energies, atomization energies, and ionization potentials. The genISI2 functional can thus find application in a broad range of systems and properties.","sentences":["The Adiabatic Connection Integrand Interpolation (ACII) method represents a general path for calculating correlation energies in electronic systems within the Den sity Functional Theory.","ACII functionals include both exact-exchange and the second-order correlation energy, as well as an interpolating function toward the strictly-correlated electron (SCE) regime.","Several interpolating functions have been proposed in the last years targeting different properties, yet an accurate ACII approach with broad applicability is sti ll missing.","Recently, we have proposed an ACII functional that was made accurate for the three-dimensional (3D) uniform electron gas as well as for model metal clusters.","In this work we present an ACII functional (named genISI2) which is very accurate for both three-dimensional (3D) and two-dimensional (2D) uniform electron gases and for the q uasi-2D infinite barrier model, where most of the exchange-correlation functionals fail badly, as well as for strongly correlated two-electrons systems.","Using the exact-exchange Kohn-Sham orbitals, we have also assessed the genISI2 for various molecular systems, showing a superior performance with respect to the o ther ACII methods for total energies, atomization energies, and ionization potentials.","The genISI2 functional can thus find application in a broad range of systems and properties."],"url":"http://arxiv.org/abs/2403.07391v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 07:32:47","title":"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends","abstract":"We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billion parameters, to demonstrate the value of our approach.","sentences":["We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories.","Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size.","We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end.","Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others.","We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billion parameters, to demonstrate the value of our approach."],"url":"http://arxiv.org/abs/2403.07379v1","category":"cs.LG"}
{"created":"2024-03-12 07:25:51","title":"A Novel Method to Constrain Tidal Quality Factor from A Non-synchronized Exoplanetary System","abstract":"We propose a novel method to constrain the tidal quality factor, $Q'$, from a non-synchronized star-planet system consisting of a slowly rotating low-mass star and a close-in Jovian planet, taking into account the tidal interaction and the magnetic braking. On the basis of dynamical system theory, the track of the co-evolution of angular momentum for such a system exhibits the existence of a forbidden region in the $\\Omega_\\mathrm{orb}$ -- $\\Omega_\\mathrm{spin}$ plane , where $\\Omega_\\mathrm{spin}$ and $\\Omega_\\mathrm{orb}$ denote the angular velocity of the stellar spin and planetary orbit, respectively. The forbidden region is determined primarily by the strength of the tidal interaction. By comparing ($\\Omega_\\mathrm{orb},\\Omega_\\mathrm{spin}$) of a single star-planet system to the forbidden region, we can constrain the tidal quality factor regardless of the evolutionary history of the system. The application of this method to the star-planet system, NGTS-10 -- NGTS-10 b, gives $Q' \\gtrsim 10^8$, leading to an tight upper bound on the tidal torque. Since this cannot be explained by previous theoretical predictions for non-synchronized star-planet systems, our result requires mechanisms that suppress the tidal interaction in such systems.","sentences":["We propose a novel method to constrain the tidal quality factor, $Q'$, from a non-synchronized star-planet system consisting of a slowly rotating low-mass star and a close-in Jovian planet, taking into account the tidal interaction and the magnetic braking.","On the basis of dynamical system theory, the track of the co-evolution of angular momentum for such a system exhibits the existence of a forbidden region in the $\\Omega_\\mathrm{orb}$ -- $\\Omega_\\mathrm{spin}$ plane , where $\\Omega_\\mathrm{spin}$ and $\\Omega_\\mathrm{orb}$ denote the angular velocity of the stellar spin and planetary orbit, respectively.","The forbidden region is determined primarily by the strength of the tidal interaction.","By comparing ($\\Omega_\\mathrm{orb},\\Omega_\\mathrm{spin}$) of a single star-planet system to the forbidden region, we can constrain the tidal quality factor regardless of the evolutionary history of the system.","The application of this method to the star-planet system, NGTS-10 -- NGTS-10 b, gives $Q' \\gtrsim 10^8$, leading to an tight upper bound on the tidal torque.","Since this cannot be explained by previous theoretical predictions for non-synchronized star-planet systems, our result requires mechanisms that suppress the tidal interaction in such systems."],"url":"http://arxiv.org/abs/2403.07375v1","category":"astro-ph.EP"}
{"created":"2024-03-12 07:13:24","title":"Superconducting switching jump induced missing first Shapiro step in Al-InSb nanosheet Josephson junctions","abstract":"The absence of odd-order Shapiro steps is one of the predicted signatures for topological superconductors. Experimentally, the missing first-order Shapiro step has been reported in several superconducting systems presumably to be topologically non-trivial, as well as in the topologically trivial regime of superconductor-semiconductor Josephson junctions. In this work, we revisit the missing first Shapiro step signature in the topologically trivial regime of Al-InSb nanosheet Josephson junctions under microwave irradiation. The missing first Shapiro step is found to be accompanied by a sharp voltage jump during the superconducting switching and reappears when the jump is softened by increasing temperature or magnetic field. The missing first Shapiro step also reappears with an increased microwave frequency. The sharp switching jump, existing without microwave irradiation, deviates from the relation given by the standard resistively shunted junction (RSJ) model. Missing Shapiro step signatures are qualitatively captured by introducing the sharp voltage jump into the RSJ model. This work reveals a common, yet overlooked, phenomenon that leads to the missing first Shapiro step, providing a new perspective on fractional Josephson experiments.","sentences":["The absence of odd-order Shapiro steps is one of the predicted signatures for topological superconductors.","Experimentally, the missing first-order Shapiro step has been reported in several superconducting systems presumably to be topologically non-trivial, as well as in the topologically trivial regime of superconductor-semiconductor Josephson junctions.","In this work, we revisit the missing first Shapiro step signature in the topologically trivial regime of Al-InSb nanosheet Josephson junctions under microwave irradiation.","The missing first Shapiro step is found to be accompanied by a sharp voltage jump during the superconducting switching and reappears when the jump is softened by increasing temperature or magnetic field.","The missing first Shapiro step also reappears with an increased microwave frequency.","The sharp switching jump, existing without microwave irradiation, deviates from the relation given by the standard resistively shunted junction (RSJ) model.","Missing Shapiro step signatures are qualitatively captured by introducing the sharp voltage jump into the RSJ model.","This work reveals a common, yet overlooked, phenomenon that leads to the missing first Shapiro step, providing a new perspective on fractional Josephson experiments."],"url":"http://arxiv.org/abs/2403.07370v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 06:48:23","title":"Migration of two Interacting Micro-Confined Deformable Drops Under an Imposed Temperature Gradient","abstract":"A tiny drop of one liquid, suspended within another, may be set into motion aligned with an imposed thermal gradient, as influenced by thermocapillary action stemming from the gradients in interfacial tension due to the local variations in temperature. In real-world situations, however, such drops do not remain in isolation, as they interact with their neighbouring entities including other drops in the proximity as well as a nearby solid boundary, setting up a complex interplay between the confinement-mediated interactions and three-dimensional nature of the droplet dynamics. In this study, we present numerical solutions for the migration dynamics of a tightly-confined drop-couple, incorporating deformable interfaces, film flow, and Marangoni effects in the presence of dynamically evolving thermocapillary stresses induced by an imposed uniform temperature gradient. Unlike prior investigations, our work highlights the influence of the confinement towards orchestrating non-trivial features of drop migration, as dictated by an intricate coupling of the thermal and flow fields amidst the interferences of the domain boundaries. The study reveals that hydrodynamic interactions resulting from a juxtaposition of these influences deform the drops in a unique manner as compared to the characteristics evidenced from previously reported studies, causing a distortion of the local thermal fields around them. The consequent alteration in the drop velocities is shown to govern their migration in a distinctive manner, presenting unique signatures as compared to more restrictive scenarios studied previously. These findings hold significance in designing thermocapillary-driven micro-confined systems for controlling drop trajectories under an imposed thermal field, bearing far-reaching implications in a plethora of overarching applications ranging from droplet microfluidics to space technology.","sentences":["A tiny drop of one liquid, suspended within another, may be set into motion aligned with an imposed thermal gradient, as influenced by thermocapillary action stemming from the gradients in interfacial tension due to the local variations in temperature.","In real-world situations, however, such drops do not remain in isolation, as they interact with their neighbouring entities including other drops in the proximity as well as a nearby solid boundary, setting up a complex interplay between the confinement-mediated interactions and three-dimensional nature of the droplet dynamics.","In this study, we present numerical solutions for the migration dynamics of a tightly-confined drop-couple, incorporating deformable interfaces, film flow, and Marangoni effects in the presence of dynamically evolving thermocapillary stresses induced by an imposed uniform temperature gradient.","Unlike prior investigations, our work highlights the influence of the confinement towards orchestrating non-trivial features of drop migration, as dictated by an intricate coupling of the thermal and flow fields amidst the interferences of the domain boundaries.","The study reveals that hydrodynamic interactions resulting from a juxtaposition of these influences deform the drops in a unique manner as compared to the characteristics evidenced from previously reported studies, causing a distortion of the local thermal fields around them.","The consequent alteration in the drop velocities is shown to govern their migration in a distinctive manner, presenting unique signatures as compared to more restrictive scenarios studied previously.","These findings hold significance in designing thermocapillary-driven micro-confined systems for controlling drop trajectories under an imposed thermal field, bearing far-reaching implications in a plethora of overarching applications ranging from droplet microfluidics to space technology."],"url":"http://arxiv.org/abs/2403.07361v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 17:48:11","title":"Accelerating Biclique Counting on GPU","abstract":"Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge with broad applications, from densest subgraph discovery in algorithmic research to personalized content recommendation in practical scenarios. Despite its significance, current leading (p,q)-biclique counting algorithms fall short, particularly when faced with larger graph sizes and clique scales. Fortunately, the problem's inherent structure, allowing for the independent counting of each biclique starting from every vertex, combined with a substantial set intersections, makes it highly amenable to parallelization. Recent successes in GPU-accelerated algorithms across various domains motivate our exploration into harnessing the parallelism power of GPUs to efficiently address the (p,q)-biclique counting challenge. We introduce GBC (GPU-based Biclique Counting), a novel approach designed to enable efficient and scalable (p,q)-biclique counting on GPUs. To address major bottleneck arising from redundant comparisons in set intersections (occupying an average of 90% of the runtime), we introduce a novel data structure that hashes adjacency lists into truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND operations. Our innovative hybrid DFS-BFS exploration strategy further enhances thread utilization and effectively manages memory constraints. A composite load balancing strategy, integrating pre-runtime and runtime workload allocation, ensures equitable distribution among threads. Additionally, we employ vertex reordering and graph partitioning strategies for improved compactness and scalability. Experimental evaluations on eight real-life and two synthetic datasets demonstrate that GBC outperforms state-of-the-art algorithms by a substantial margin. In particular, GBC achieves an average speedup of 497.8x, with the largest instance achieving a remarkable 1217.7x speedup when p = q = 8.","sentences":["Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge with broad applications, from densest subgraph discovery in algorithmic research to personalized content recommendation in practical scenarios.","Despite its significance, current leading (p,q)-biclique counting algorithms fall short, particularly when faced with larger graph sizes and clique scales.","Fortunately, the problem's inherent structure, allowing for the independent counting of each biclique starting from every vertex, combined with a substantial set intersections, makes it highly amenable to parallelization.","Recent successes in GPU-accelerated algorithms across various domains motivate our exploration into harnessing the parallelism power of GPUs to efficiently address the (p,q)-biclique counting challenge.","We introduce GBC (GPU-based Biclique Counting), a novel approach designed to enable efficient and scalable (p,q)-biclique counting on GPUs.","To address major bottleneck arising from redundant comparisons in set intersections (occupying an average of 90% of the runtime), we introduce a novel data structure that hashes adjacency lists into truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND operations.","Our innovative hybrid DFS-BFS exploration strategy further enhances thread utilization and effectively manages memory constraints.","A composite load balancing strategy, integrating pre-runtime and runtime workload allocation, ensures equitable distribution among threads.","Additionally, we employ vertex reordering and graph partitioning strategies for improved compactness and scalability.","Experimental evaluations on eight real-life and two synthetic datasets demonstrate that GBC outperforms state-of-the-art algorithms by a substantial margin.","In particular, GBC achieves an average speedup of 497.8x, with the largest instance achieving a remarkable 1217.7x speedup when p = q = 8."],"url":"http://arxiv.org/abs/2403.07858v1","category":"cs.DC"}
{"created":"2024-03-12 17:44:45","title":"Distilling the Knowledge in Data Pruning","abstract":"With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes. On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms. Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results. Our code will be made available.","sentences":["With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research.","However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes.","In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset.","That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data.","By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions.","We first establish a theoretical motivation for employing self-distillation to improve training on pruned data.","Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes.","On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data.","Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight.","This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms.","Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results.","Our code will be made available."],"url":"http://arxiv.org/abs/2403.07854v1","category":"cs.CV"}
{"created":"2024-03-12 16:36:27","title":"BraSyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives","abstract":"This work is addressing the Brain Magnetic Resonance Image Synthesis for Tumor Segmentation (BraSyn) challenge which was hosted as part of the Brain Tumor Segmentation challenge (BraTS) 2023. In this challenge researchers are invited to work on synthesizing a missing magnetic resonance image sequence given other available sequences to facilitate tumor segmentation pipelines trained on complete sets of image sequences. This problem can be addressed using deep learning in the framework of paired images-to-image translation. In this work, we proposed to investigate the effectiveness of a commonly-used deep learning framework such as Pix2Pix trained under supervision of different image-quality loss functions. Our results indicate that using different loss functions significantly affects the synthesis quality. We systematically study the impact of different loss functions in the multi-sequence MR image synthesis setting of the BraSyn challenge. Furthermore, we show how image synthesis performance can be optimized by beneficially combining different learning objectives.","sentences":["This work is addressing the Brain Magnetic Resonance Image Synthesis for Tumor Segmentation (BraSyn) challenge which was hosted as part of the Brain Tumor Segmentation challenge (BraTS) 2023.","In this challenge researchers are invited to work on synthesizing a missing magnetic resonance image sequence given other available sequences to facilitate tumor segmentation pipelines trained on complete sets of image sequences.","This problem can be addressed using deep learning in the framework of paired images-to-image translation.","In this work, we proposed to investigate the effectiveness of a commonly-used deep learning framework such as Pix2Pix trained under supervision of different image-quality loss functions.","Our results indicate that using different loss functions significantly affects the synthesis quality.","We systematically study the impact of different loss functions in the multi-sequence MR image synthesis setting of the BraSyn challenge.","Furthermore, we show how image synthesis performance can be optimized by beneficially combining different learning objectives."],"url":"http://arxiv.org/abs/2403.07800v1","category":"eess.IV"}
{"created":"2024-03-12 16:35:34","title":"Equitable Pricing in Auctions","abstract":"We study how pricing affects the division of surplus among buyers in auctions for multiple units. Our equity objective may be important, e.g., for competition concerns in downstream markets, complementing the long-standing debate on revenue and efficiency. We study a canonical model of auctions for multiple indivisible units with unit demand buyers and valuations with a private and a common component and consider all pricing rules that are a mixture (i.e., a convex combination) of pay-as-bid and uniform pricing. We propose the winners' empirical variance (WEV), the expected empirical variance of surplus among the winners, as a metric for surplus equity. We show that, for a range of private-common value proportions, a strictly interior mix of pay-as-bid and uniform pricing minimizes WEV. From an equity perspective, auctions with a higher private value component benefit from more price discrimination, whereas only auctions with a sufficiently high common value justify a more uniform pricing rule. We provide a criterion under which strictly mixed pricing dominates uniform pricing, a partial ranking of different mixed pricing formats, and bounds on the WEV-minimizing pricing under the assumption of log-concave signal distributions. In numerical experiments, we further illustrate the WEV-minimal pricing as a function of the private-common-value mix.","sentences":["We study how pricing affects the division of surplus among buyers in auctions for multiple units.","Our equity objective may be important, e.g., for competition concerns in downstream markets, complementing the long-standing debate on revenue and efficiency.","We study a canonical model of auctions for multiple indivisible units with unit demand buyers and valuations with a private and a common component and consider all pricing rules that are a mixture (i.e., a convex combination) of pay-as-bid and uniform pricing.","We propose the winners' empirical variance (WEV), the expected empirical variance of surplus among the winners, as a metric for surplus equity.","We show that, for a range of private-common value proportions, a strictly interior mix of pay-as-bid and uniform pricing minimizes WEV.","From an equity perspective, auctions with a higher private value component benefit from more price discrimination, whereas only auctions with a sufficiently high common value justify a more uniform pricing rule.","We provide a criterion under which strictly mixed pricing dominates uniform pricing, a partial ranking of different mixed pricing formats, and bounds on the WEV-minimizing pricing under the assumption of log-concave signal distributions.","In numerical experiments, we further illustrate the WEV-minimal pricing as a function of the private-common-value mix."],"url":"http://arxiv.org/abs/2403.07799v1","category":"econ.TH"}
{"created":"2024-03-12 16:31:37","title":"Search for new bosons with ytterbium isotope shifts","abstract":"The Standard Model of particle physics describes the properties of elementary particles and their interactions remarkably well, but in particular does not account for dark matter. Isotope-shift spectroscopy is a sensitive probe of fifth forces and new particles that illuminate the dark matter sector. This method sets bounds on new bosons that couple neutrons and electrons with masses in the keV/c2 to MeV/c2 range. With increasing spectroscopic precision, such searches are limited by uncertainties of isotope masses and the understanding of nuclear structure. Here, we report on high-precision mass-ratio and isotope-shift measurements of the ytterbium isotopes $^{168,170,172,174,176}$Yb that exceed previous measurements by up to two orders of magnitude. From these measurements, we extract higher-order changes in the nuclear charge distribution along the Yb isotope chain and use these to benchmark novel ab initio calculations. Our measurements set new bounds on the existence of the proposed boson.","sentences":["The Standard Model of particle physics describes the properties of elementary particles and their interactions remarkably well, but in particular does not account for dark matter.","Isotope-shift spectroscopy is a sensitive probe of fifth forces and new particles that illuminate the dark matter sector.","This method sets bounds on new bosons that couple neutrons and electrons with masses in the keV/c2 to MeV/c2 range.","With increasing spectroscopic precision, such searches are limited by uncertainties of isotope masses and the understanding of nuclear structure.","Here, we report on high-precision mass-ratio and isotope-shift measurements of the ytterbium isotopes $^{168,170,172,174,176}$Yb that exceed previous measurements by up to two orders of magnitude.","From these measurements, we extract higher-order changes in the nuclear charge distribution along the Yb isotope chain and use these to benchmark novel ab initio calculations.","Our measurements set new bounds on the existence of the proposed boson."],"url":"http://arxiv.org/abs/2403.07792v1","category":"physics.atom-ph"}
{"created":"2024-03-12 15:58:53","title":"Privacy Guarantees in Posterior Sampling under Contamination","abstract":"In recent years, differential privacy has been adopted by tech-companies and governmental agencies as the standard for measuring privacy in algorithms. We study the level of differential privacy in Bayesian posterior sampling setups. As opposed to the common privatization approach of injecting Laplace/Gaussian noise into the output, Huber's contamination model is considered, where we replace at random the data points with samples from a heavy-tailed distribution. We derived bounds for the differential privacy level $(\\epsilon,\\delta)$ for our approach while lifting the common restriction on assuming bounded observation and parameter space seen in the existing literature. We further consider the effect of sample size on privacy level and the convergence rate of $(\\epsilon,\\delta)$ to zero. Asymptotically, the contamination approach is fully private at no cost of information loss. We also provide some examples depicting inference models that our setup is applicable to with a theoretical estimation of convergence rate.","sentences":["In recent years, differential privacy has been adopted by tech-companies and governmental agencies as the standard for measuring privacy in algorithms.","We study the level of differential privacy in Bayesian posterior sampling setups.","As opposed to the common privatization approach of injecting Laplace/Gaussian noise into the output, Huber's contamination model is considered, where we replace at random the data points with samples from a heavy-tailed distribution.","We derived bounds for the differential privacy level $(\\epsilon,\\delta)$ for our approach while lifting the common restriction on assuming bounded observation and parameter space seen in the existing literature.","We further consider the effect of sample size on privacy level and the convergence rate of $(\\epsilon,\\delta)$ to zero.","Asymptotically, the contamination approach is fully private at no cost of information loss.","We also provide some examples depicting inference models that our setup is applicable to with a theoretical estimation of convergence rate."],"url":"http://arxiv.org/abs/2403.07772v1","category":"math.ST"}
{"created":"2024-03-12 15:39:56","title":"Vision-based Vehicle Re-identification in Bridge Scenario using Flock Similarity","abstract":"Due to the needs of road traffic flow monitoring and public safety management, video surveillance cameras are widely distributed in urban roads. However, the information captured directly by each camera is siloed, making it difficult to use it effectively. Vehicle re-identification refers to finding a vehicle that appears under one camera in another camera, which can correlate the information captured by multiple cameras. While license plate recognition plays an important role in some applications, there are some scenarios where re-identification method based on vehicle appearance are more suitable. The main challenge is that the data of vehicle appearance has the characteristics of high inter-class similarity and large intra-class differences. Therefore, it is difficult to accurately distinguish between different vehicles by relying only on vehicle appearance information. At this time, it is often necessary to introduce some extra information, such as spatio-temporal information. Nevertheless, the relative position of the vehicles rarely changes when passing through two adjacent cameras in the bridge scenario. In this paper, we present a vehicle re-identification method based on flock similarity, which improves the accuracy of vehicle re-identification by utilizing vehicle information adjacent to the target vehicle. When the relative position of the vehicles remains unchanged and flock size is appropriate, we obtain an average relative improvement of 204% on VeRi dataset in our experiments. Then, the effect of the magnitude of the relative position change of the vehicles as they pass through two cameras is discussed. We present two metrics that can be used to quantify the difference and establish a connection between them. Although this assumption is based on the bridge scenario, it is often true in other scenarios due to driving safety and camera location.","sentences":["Due to the needs of road traffic flow monitoring and public safety management, video surveillance cameras are widely distributed in urban roads.","However, the information captured directly by each camera is siloed, making it difficult to use it effectively.","Vehicle re-identification refers to finding a vehicle that appears under one camera in another camera, which can correlate the information captured by multiple cameras.","While license plate recognition plays an important role in some applications, there are some scenarios where re-identification method based on vehicle appearance are more suitable.","The main challenge is that the data of vehicle appearance has the characteristics of high inter-class similarity and large intra-class differences.","Therefore, it is difficult to accurately distinguish between different vehicles by relying only on vehicle appearance information.","At this time, it is often necessary to introduce some extra information, such as spatio-temporal information.","Nevertheless, the relative position of the vehicles rarely changes when passing through two adjacent cameras in the bridge scenario.","In this paper, we present a vehicle re-identification method based on flock similarity, which improves the accuracy of vehicle re-identification by utilizing vehicle information adjacent to the target vehicle.","When the relative position of the vehicles remains unchanged and flock size is appropriate, we obtain an average relative improvement of 204% on VeRi dataset in our experiments.","Then, the effect of the magnitude of the relative position change of the vehicles as they pass through two cameras is discussed.","We present two metrics that can be used to quantify the difference and establish a connection between them.","Although this assumption is based on the bridge scenario, it is often true in other scenarios due to driving safety and camera location."],"url":"http://arxiv.org/abs/2403.07752v1","category":"cs.CV"}
{"created":"2024-03-12 15:27:35","title":"Harnessing two-photon dissipation for enhanced quantum measurement and control","abstract":"Dissipation engineering offers a powerful tool for quantum technologies. Recently, new superconducting devices demonstrated an engineered two-photon dissipation rate exceeding all other relevant timescales. In particular, they have proven most useful to prevent transitions between the logical states $|\\pm\\alpha\\rangle$ of a cat qubit. Here, we present three key applications of strong two-photon dissipation for quantum measurement and control, beyond cat qubit stabilization. Firstly, we demonstrate its efficacy in overcoming limitations encountered in Wigner tomography at high photon numbers. Secondly, we showcase its potential for realizing universal gates on cat qubits, exploiting the coherent mapping between cat qubit states and superpositions of 0 and 1 photons. Finally, we harness the transient dynamics of a cat state under two-photon dissipation to prepare squeezed cat states with a squeezing factor exceeding 3.8 dB.","sentences":["Dissipation engineering offers a powerful tool for quantum technologies.","Recently, new superconducting devices demonstrated an engineered two-photon dissipation rate exceeding all other relevant timescales.","In particular, they have proven most useful to prevent transitions between the logical states $|\\pm\\alpha\\rangle$ of a cat qubit.","Here, we present three key applications of strong two-photon dissipation for quantum measurement and control, beyond cat qubit stabilization.","Firstly, we demonstrate its efficacy in overcoming limitations encountered in Wigner tomography at high photon numbers.","Secondly, we showcase its potential for realizing universal gates on cat qubits, exploiting the coherent mapping between cat qubit states and superpositions of 0 and 1 photons.","Finally, we harness the transient dynamics of a cat state under two-photon dissipation to prepare squeezed cat states with a squeezing factor exceeding 3.8 dB."],"url":"http://arxiv.org/abs/2403.07744v1","category":"quant-ph"}
{"created":"2024-03-12 15:20:26","title":"Satellite quenching and morphological transformation of galaxies in groups and clusters","abstract":"We investigate the role that dense environments have on the quenching of star formation and the transformation of morphology for a sample of galaxies selected from the Sloan Digital Sky Survey. We make a distinction between galaxies falling into groups $(13 \\leq \\log{(M_{\\text{halo}}/M_{\\odot})} < 14)$ and clusters $(\\log{(M_{\\text{halo}}/M_{\\odot})} \\geq 14)$, and compare to a large sample of field galaxies. Using galaxy position in projected phase space as a proxy for time since infall, we study how galaxy specific star formation rate (sSFR) and morphology, parameterized by the bulge-to-total light ratio (B/T), change over time. After controlling for stellar mass, we find clear trends of increasing quenched and elliptical fractions as functions of infall time for galaxies falling into both groups and clusters. The trends are strongest for low mass galaxies falling into clusters. By computing quenching and morphological transformation timescales, we find evidence that star formation quenching occurs faster than morphological transformation in both environments. Comparing field galaxies to recently infalling galaxies, we determine there is pre-processing of both star formation and morphology, with pre-processing affecting star formation rates more strongly. Our analysis favours quenching mechanisms that act quickly to suppress star formation, while other mechanisms that act on longer timescales transform morphology through bulge growth and disc fading.","sentences":["We investigate the role that dense environments have on the quenching of star formation and the transformation of morphology for a sample of galaxies selected from the Sloan Digital Sky Survey.","We make a distinction between galaxies falling into groups $(13 \\leq \\log{(M_{\\text{halo}}/M_{\\odot})} < 14)$ and clusters $(\\log{(M_{\\text{halo}}/M_{\\odot})} \\geq 14)$, and compare to a large sample of field galaxies.","Using galaxy position in projected phase space as a proxy for time since infall, we study how galaxy specific star formation rate (sSFR) and morphology, parameterized by the bulge-to-total light ratio (B/T), change over time.","After controlling for stellar mass, we find clear trends of increasing quenched and elliptical fractions as functions of infall time for galaxies falling into both groups and clusters.","The trends are strongest for low mass galaxies falling into clusters.","By computing quenching and morphological transformation timescales, we find evidence that star formation quenching occurs faster than morphological transformation in both environments.","Comparing field galaxies to recently infalling galaxies, we determine there is pre-processing of both star formation and morphology, with pre-processing affecting star formation rates more strongly.","Our analysis favours quenching mechanisms that act quickly to suppress star formation, while other mechanisms that act on longer timescales transform morphology through bulge growth and disc fading."],"url":"http://arxiv.org/abs/2403.07742v1","category":"astro-ph.GA"}
{"created":"2024-03-12 15:18:38","title":"Gapless neutron superfluidity can explain the late time cooling of transiently accreting neutron stars","abstract":"The current interpretation of the observed late time cooling of transiently accreting neutron stars in low-mass X-ray binaries during quiescence requires the suppression of neutron superfluidity in their crust at variance with recent ab initio many-body calculations of dense matter. Focusing on the two emblematic sources KS~1731$-$260 and MXB~1659$-$29, we show that their thermal evolution can be naturally explained by considering the existence of a neutron superflow driven by the pinning of quantized vortices. Under such circumstances, we find that the neutron superfluid can be in a gapless state in which the specific heat is dramatically increased compared to that in the classical BCS state assumed so far, thus delaying the thermal relaxation of the crust. We have performed neutron-star cooling simulations taking into account gapless superfluidity and we have obtained excellent fits to the data thus reconciling astrophysical observations with microscopic theories. The imprint of gapless superfluidity on other observable phenomena is briefly discussed.","sentences":["The current interpretation of the observed late time cooling of transiently accreting neutron stars in low-mass X-ray binaries during quiescence requires the suppression of neutron superfluidity in their crust at variance with recent ab initio many-body calculations of dense matter.","Focusing on the two emblematic sources KS~1731$-$260 and MXB~1659$-$29, we show that their thermal evolution can be naturally explained by considering the existence of a neutron superflow driven by the pinning of quantized vortices.","Under such circumstances, we find that the neutron superfluid can be in a gapless state in which the specific heat is dramatically increased compared to that in the classical BCS state assumed so far, thus delaying the thermal relaxation of the crust.","We have performed neutron-star cooling simulations taking into account gapless superfluidity and we have obtained excellent fits to the data thus reconciling astrophysical observations with microscopic theories.","The imprint of gapless superfluidity on other observable phenomena is briefly discussed."],"url":"http://arxiv.org/abs/2403.07740v1","category":"astro-ph.HE"}
{"created":"2024-03-12 15:13:21","title":"The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels","abstract":"Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\\mathcal O\\!\\left(n^{-1/2}\\right)$. Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on $\\mathbb R^d$.","sentences":["Kernel techniques are among the most influential approaches in data science and statistics.","Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\\ge 2$ random variables.","Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature).","Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open.","In this work, we prove that the minimax optimal rate of HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\\mathcal O\\!\\left(n^{-1/2}\\right)$.","Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on $\\mathbb R^d$."],"url":"http://arxiv.org/abs/2403.07735v1","category":"math.ST"}
{"created":"2024-03-12 14:59:09","title":"Higgs photon associated production in a Two Higgs Doublet Type-II Seesaw Model at future electron-positron colliders","abstract":"We studied the one-loop prediction for the single production of a Higgs-like boson in association with a photon in electron-positron collisions in the context of two Higgs doublet type-II seesaw model (2HDMcT). We explored to what extent the new scalars in the (2HDMcT) spectrum affect its production cross-section, the ratio $R_{\\gamma h_1}$ as well as the signal strengths $R_{\\gamma Z}$ and $R_{\\gamma \\gamma}$ when $h_1$ is identified with the observed $SM $ Higgs boson within the $2HDMcT$ delimited parameter space. More specifically, we focused on $e^+e^- \\to h_1\\gamma$ process at one-loop, and analyzed how it evolves under a full set of theoretical constraints and the available experimental data, including $B\\to X_s\\gamma$ constraint at 95$\\%$ C.L. Our analysis showed that these observables are particularly sensitive to the parameters $\\alpha_1$, $\\lambda_{7}$, $\\lambda_{9}$, the trilinear Higgs couplings and to the charged Higgs masses. We found that $\\sigma (e^+e^- \\to h_1\\gamma)$ can significantly be enhanced up to $8.1\\,\\, 10^{-2}$ fb, so exceeding the Standard Model prediction. Additionally, as a byproduct, we also found that $R_{\\gamma h_1}$ is entirely correlated with both the $h_1\\to\\gamma\\gamma$ and $h_1\\to\\gamma Z$ signal strengths.","sentences":["We studied the one-loop prediction for the single production of a Higgs-like boson in association with a photon in electron-positron collisions in the context of two Higgs doublet type-II seesaw model (2HDMcT).","We explored to what extent the new scalars in the (2HDMcT) spectrum affect its production cross-section, the ratio $R_{\\gamma h_1}$ as well as the signal strengths $R_{\\gamma Z}$ and $R_{\\gamma \\gamma}$ when $h_1$ is identified with the observed $SM $ Higgs boson within the $2HDMcT$ delimited parameter space.","More specifically, we focused on $e^+e^- \\to h_1\\gamma$ process at one-loop, and analyzed how it evolves under a full set of theoretical constraints and the available experimental data, including $B\\to X_s\\gamma$ constraint at 95$\\%$ C.L.","Our analysis showed that these observables are particularly sensitive to the parameters $\\alpha_1$, $\\lambda_{7}$, $\\lambda_{9}$, the trilinear Higgs couplings and to the charged Higgs masses.","We found that $\\sigma (e^+e^- \\to h_1\\gamma)$ can significantly be enhanced up to $8.1\\,\\, 10^{-2}$ fb, so exceeding the Standard Model prediction.","Additionally, as a byproduct, we also found that $R_{\\gamma h_1}$ is entirely correlated with both the $h_1\\to\\gamma\\gamma$ and $h_1\\to\\gamma Z$ signal strengths."],"url":"http://arxiv.org/abs/2403.07722v1","category":"hep-ph"}
{"created":"2024-03-12 14:40:11","title":"Peak-Brightness Localization of the CNEOS 2014-01-08 (IM1) Fireball","abstract":"In a recent preprint, Fernando et al. (2024) used public data from infrasound stations to constrain the localization of the fireball of the CNEOS 2014-01-08 (IM1) bolide. The analysis inferred a 90-percent-confidence ellipse with semi-minor and semi-major axes of 186 and 388 km, respectively. This large error ellipse includes the much better localization box derived by sensors aboard U.S. Government satellites which detected the fireball light. At the fireball's peak brightness, the CNEOS localization box documented by NASA/JPL measures 11.112km on a side and is centered on a latitude of 1.3S and a longitude of 147.6E. Here, we point out that the recent expedition to retrieve materials from IM1's site (Loeb et al. 2024a,b,c) surveyed a region of tens of km around the CNEOS box center, and was not dictated by the data studied by Fernando et al. (2024) because of its larger uncertainties.","sentences":["In a recent preprint, Fernando et al. (2024) used public data from infrasound stations to constrain the localization of the fireball of the CNEOS 2014-01-08 (IM1) bolide.","The analysis inferred a 90-percent-confidence ellipse with semi-minor and semi-major axes of 186 and 388 km, respectively.","This large error ellipse includes the much better localization box derived by sensors aboard U.S. Government satellites which detected the fireball light.","At the fireball's peak brightness, the CNEOS localization box documented by NASA/JPL measures 11.112km on a side and is centered on a latitude of 1.3S and a longitude of 147.6E. Here, we point out that the recent expedition to retrieve materials from IM1's site (Loeb et al. 2024a,b,c) surveyed a region of tens of km around the CNEOS box center, and was not dictated by the data studied by Fernando et al. (2024) because of its larger uncertainties."],"url":"http://arxiv.org/abs/2403.07696v1","category":"astro-ph.EP"}
{"created":"2024-03-12 13:48:46","title":"An overdetermined problem in 2D linearised hydrostatics","abstract":"In two spatial dimensions, we discuss the relation between the solvability of Schiffer's overdetermined problem and the optimality, among sets of prescribed area, of the first eigenvalue in the buckling problem for a clamped plate and that of the first eigenvalue of the Stokes operator. For the latter, we deduce that the minimisers under area constraint that are smooth and simply connected must be discs from the fact that a pressureless velocity is a necessary condition of optimality.","sentences":["In two spatial dimensions, we discuss the relation between the solvability of Schiffer's overdetermined problem and the optimality, among sets of prescribed area, of the first eigenvalue in the buckling problem for a clamped plate and that of the first eigenvalue of the Stokes operator.","For the latter, we deduce that the minimisers under area constraint that are smooth and simply connected must be discs from the fact that a pressureless velocity is a necessary condition of optimality."],"url":"http://arxiv.org/abs/2403.07658v1","category":"math.AP"}
{"created":"2024-03-12 13:23:15","title":"Discovery of a Magnetic Topological Semimetal Eu3In2As4 with a Single Pair of Weyl Points","abstract":"Magnetic Weyl semimetal (MWS) is a unique topological state with open surface Fermi arc states and other exotic transport phenomena. However, most reported MWSs show multiple pairs of Weyl points and complicated Fermi surfaces, which increases the difficulty of the investigation into the intrinsic chiral transport property. In this wor, we successfully synthesized a soft magnetic Weyl semimetal Eu3In2As4 with a single pair of Weyl points under magnetic fields. The Shubnikov de Haas (SdH) oscillation with a single frequency, as well as a linear hall resistance with the same carrier density, is observed up to 50 Tesla, indicating a single pair of Weyl points around the Fermi level with a massless fermion (m* = 0.121 m0, Berry phase). Such a single pair of Weyl points is further confirmed by the density functional theory calculations. The magnetic ordering and band topology can be easily tuned by the external magnetic field. The field-induced MWS Eu3In2As4 with a single pair of Weyl points is a good platform to detect chiral transport properties, including possible quantum anomalous Hall effect.","sentences":["Magnetic Weyl semimetal (MWS) is a unique topological state with open surface Fermi arc states and other exotic transport phenomena.","However, most reported MWSs show multiple pairs of Weyl points and complicated Fermi surfaces, which increases the difficulty of the investigation into the intrinsic chiral transport property.","In this wor, we successfully synthesized a soft magnetic Weyl semimetal Eu3In2As4 with a single pair of Weyl points under magnetic fields.","The Shubnikov de Haas (SdH) oscillation with a single frequency, as well as a linear hall resistance with the same carrier density, is observed up to 50 Tesla, indicating a single pair of Weyl points around the Fermi level with a massless fermion (m* = 0.121 m0, Berry phase).","Such a single pair of Weyl points is further confirmed by the density functional theory calculations.","The magnetic ordering and band topology can be easily tuned by the external magnetic field.","The field-induced MWS Eu3In2As4 with a single pair of Weyl points is a good platform to detect chiral transport properties, including possible quantum anomalous Hall effect."],"url":"http://arxiv.org/abs/2403.07637v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 12:53:05","title":"Radiative corrections and threshold resummed predictions to pseudoscalar Higgs boson production in QCD","abstract":"This thesis studies the pseudoscalar Higgs boson production via gluon fusion in the EFT framework in a CP-conserving model. First, it presents the di-pseudoscalar Higgs boson production cross-section via gluon fusion till NNLO with the results valid for the pseudoscalar Higgs boson of MSSM and 2HDM with small tan $\\beta$ by adjusting the top Yukawa coupling. We have used dimensional regularization to regulate the UV and IR divergences while carefully treating the Levi-Civita tensor and $\\gamma_5$. Unlike the amplitudes involving a pair of scalar Higgs bosons, we do not need UV contact counter terms. We used Catani's predictions to factorize the IR singularities, and our IR poles agreed with Catani's predictions. Our results are essential for studies on producing a pair of pseudoscalar Higgs bosons at the LHC up to NNLO accuracy. This thesis also includes the next-to-soft-virtual (NSV) resummed corrections to $\\overline{\\text{NNLL}}$ accuracy, which are also matched to the NNLO cross-sections for a pseudoscalar Higgs boson production via gluon fusion. These NSV corrections are potentially significant compared to the conventional soft-virtual (SV) logarithms. We have estimated the uncertainties due to the choice of various PDFs and those due to the renormalization and factorization scales. The scale uncertainties show improvement for renormalization scale variation only, which suggests that NSV contributions from other parton channels and beyond NSV contributions in the gluon fusion channel are necessary for improved stability. We also studied the production cross-sections for mixed scalar-pseudoscalar states and their impact on QCD cross-sections for different values of the mixing angle $\\alpha$. The study indicates the necessity of improving the precision results for the pseudoscalar Higgs boson up to an order comparable to that of the scalar Higgs boson.","sentences":["This thesis studies the pseudoscalar Higgs boson production via gluon fusion in the EFT framework in a CP-conserving model.","First, it presents the di-pseudoscalar Higgs boson production cross-section via gluon fusion till NNLO with the results valid for the pseudoscalar Higgs boson of MSSM and 2HDM with small tan $\\beta$ by adjusting the top Yukawa coupling.","We have used dimensional regularization to regulate the UV and IR divergences while carefully treating the Levi-Civita tensor and $\\gamma_5$. Unlike the amplitudes involving a pair of scalar Higgs bosons, we do not need UV contact counter terms.","We used Catani's predictions to factorize the IR singularities, and our IR poles agreed with Catani's predictions.","Our results are essential for studies on producing a pair of pseudoscalar Higgs bosons at the LHC up to NNLO accuracy.","This thesis also includes the next-to-soft-virtual (NSV) resummed corrections to $\\overline{\\text{NNLL}}$ accuracy, which are also matched to the NNLO cross-sections for a pseudoscalar Higgs boson production via gluon fusion.","These NSV corrections are potentially significant compared to the conventional soft-virtual (SV) logarithms.","We have estimated the uncertainties due to the choice of various PDFs and those due to the renormalization and factorization scales.","The scale uncertainties show improvement for renormalization scale variation only, which suggests that NSV contributions from other parton channels and beyond NSV contributions in the gluon fusion channel are necessary for improved stability.","We also studied the production cross-sections for mixed scalar-pseudoscalar states and their impact on QCD cross-sections for different values of the mixing angle $\\alpha$.","The study indicates the necessity of improving the precision results for the pseudoscalar Higgs boson up to an order comparable to that of the scalar Higgs boson."],"url":"http://arxiv.org/abs/2403.07615v1","category":"hep-ph"}
{"created":"2024-03-12 12:33:54","title":"An Architecture for Noise-Aware Distributed Quantum Computation","abstract":"Suppose Alice has access to $n$ remote quantum computing nodes capable of universal quantum computation, connected to her by a quantum channel. She wants to use these remote nodes jointly to make computations and store her quantum states such that the actual computation is hidden from these remote nodes. We describe a protocol to help Alice carry out her computation using these remote nodes and store her computation results. We also make sure these nodes can handle noise themselves in case of any error on these nodes. More precisely, we develop an architecture for distributed quantum computation and storage, addressing key challenges in quantum processing across remote nodes. Additionally, we enhance the robustness of each node against noise by developing quantum error-correcting methods suitable for each node.","sentences":["Suppose Alice has access to $n$ remote quantum computing nodes capable of universal quantum computation, connected to her by a quantum channel.","She wants to use these remote nodes jointly to make computations and store her quantum states such that the actual computation is hidden from these remote nodes.","We describe a protocol to help Alice carry out her computation using these remote nodes and store her computation results.","We also make sure these nodes can handle noise themselves in case of any error on these nodes.","More precisely, we develop an architecture for distributed quantum computation and storage, addressing key challenges in quantum processing across remote nodes.","Additionally, we enhance the robustness of each node against noise by developing quantum error-correcting methods suitable for each node."],"url":"http://arxiv.org/abs/2403.07596v1","category":"quant-ph"}
{"created":"2024-03-12 12:24:11","title":"Robustifying and Boosting Training-Free Neural Architecture Search","abstract":"Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results.","sentences":["Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks.","Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics.","Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric.","Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance.","To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further.","Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights.","Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results."],"url":"http://arxiv.org/abs/2403.07591v1","category":"cs.LG"}
{"created":"2024-03-12 12:18:55","title":"Visual Privacy Auditing with Diffusion Models","abstract":"Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as effective auditing tools for visualizing privacy leakage.","sentences":["Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information.","Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging.","Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain.","In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target.","We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD.","We show that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as effective auditing tools for visualizing privacy leakage."],"url":"http://arxiv.org/abs/2403.07588v1","category":"cs.LG"}
{"created":"2024-03-12 12:08:45","title":"Electronic and dynamical properties of cobalt monogermanide CoGe phases under pressure","abstract":"We present the pressure dependence of the electronic and dynamical properties of six different CoGe phases: orthorhombic Cmmm, hexagonal P6/mmm and P$\\bar{6}$2m, monoclinic C2/m, cubic P2$_{1}$3, and orthorhombic Pnma. Using first-principles DFT calculations and the direct force-constants method, we study the dynamical stability of individual phases under external pressure. We show that the orthorombic Cmmm and hexagonal P6/mmm structures are unstable over a broad pressure range and most pronounced imaginary phonon soft mode in both cases leads to a stable hexagonal P$\\bar{6}$2m structure of the lowest ground-state energy of all studied phases at ambient and low (below $\\sim 3$ GPa) external pressure. Under these conditions, the cubic P2$_{1}$3 phase has the highest energy, however, together with monoclinic C2/m and orthorombic Pnma it is dynamically stable and all these three structures can potentially coexist as meta-stable phases. Above $\\sim 3$ GPa, the cubic P2$_{1}$3 phase becomes the most energetically favorable. Fitting the Birch--Murnaghan equation of state we derive bulk modulus for all mentioned phases, which indicate relatively high resistance of CoGe to compression. Such conclusions are confirmed by band structure calculations. Additionally, we show that electronic bands of the hexagonal P$\\bar{6}$2m phase reveal characteristic features of the kagome-like structure, while in the cubic P2$_{1}$3 phase spectrum, one can locate spin-1 and double Weyl fermions. In both cases, the external pressure induces the Lifshitz transition, related to the modification of the Fermi surface topology.","sentences":["We present the pressure dependence of the electronic and dynamical properties of six different CoGe phases: orthorhombic Cmmm, hexagonal P6/mmm and P$\\bar{6}$2m, monoclinic C2/m, cubic P2$_{1}$3, and orthorhombic Pnma.","Using first-principles DFT calculations and the direct force-constants method, we study the dynamical stability of individual phases under external pressure.","We show that the orthorombic Cmmm and hexagonal P6/mmm structures are unstable over a broad pressure range and most pronounced imaginary phonon soft mode in both cases leads to a stable hexagonal P$\\bar{6}$2m structure of the lowest ground-state energy of all studied phases at ambient and low (below $\\sim 3$ GPa) external pressure.","Under these conditions, the cubic P2$_{1}$3 phase has the highest energy, however, together with monoclinic C2/m and orthorombic Pnma it is dynamically stable and all these three structures can potentially coexist as meta-stable phases.","Above $\\sim 3$ GPa, the cubic P2$_{1}$3 phase becomes the most energetically favorable.","Fitting the Birch--Murnaghan equation of state we derive bulk modulus for all mentioned phases, which indicate relatively high resistance of CoGe to compression.","Such conclusions are confirmed by band structure calculations.","Additionally, we show that electronic bands of the hexagonal P$\\bar{6}$2m phase reveal characteristic features of the kagome-like structure, while in the cubic P2$_{1}$3 phase spectrum, one can locate spin-1 and double Weyl fermions.","In both cases, the external pressure induces the Lifshitz transition, related to the modification of the Fermi surface topology."],"url":"http://arxiv.org/abs/2403.07580v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 11:50:47","title":"A Flexible Cell Classification for ML Projects in Jupyter Notebooks","abstract":"Jupyter Notebook is an interactive development environment commonly used for rapid experimentation of machine learning (ML) solutions. Describing the ML activities performed along code cells improves the readability and understanding of Notebooks. Manual annotation of code cells is time-consuming and error-prone. Therefore, tools have been developed that classify the cells of a notebook concerning the ML activity performed in them. However, the current tools are not flexible, as they work based on look-up tables that have been created, which map function calls of commonly used ML libraries to ML activities. These tables must be manually adjusted to account for new or changed libraries.   This paper presents a more flexible approach to cell classification based on a hybrid classification approach that combines a rule-based and a decision tree classifier. We discuss the design rationales and describe the developed classifiers in detail. We implemented the new flexible cell classification approach in a tool called JupyLabel. Its evaluation and the obtained metric scores regarding precision, recall, and F1-score are discussed. Additionally, we compared JupyLabel with HeaderGen, an existing cell classification tool. We were able to show that the presented flexible cell classification approach outperforms this tool significantly.","sentences":["Jupyter Notebook is an interactive development environment commonly used for rapid experimentation of machine learning (ML) solutions.","Describing the ML activities performed along code cells improves the readability and understanding of Notebooks.","Manual annotation of code cells is time-consuming and error-prone.","Therefore, tools have been developed that classify the cells of a notebook concerning the ML activity performed in them.","However, the current tools are not flexible, as they work based on look-up tables that have been created, which map function calls of commonly used ML libraries to ML activities.","These tables must be manually adjusted to account for new or changed libraries.   ","This paper presents a more flexible approach to cell classification based on a hybrid classification approach that combines a rule-based and a decision tree classifier.","We discuss the design rationales and describe the developed classifiers in detail.","We implemented the new flexible cell classification approach in a tool called JupyLabel.","Its evaluation and the obtained metric scores regarding precision, recall, and F1-score are discussed.","Additionally, we compared JupyLabel with HeaderGen, an existing cell classification tool.","We were able to show that the presented flexible cell classification approach outperforms this tool significantly."],"url":"http://arxiv.org/abs/2403.07562v1","category":"cs.SE"}
{"created":"2024-03-12 10:47:29","title":"Spatiotemporal Representation Learning for Short and Long Medical Image Time Series","abstract":"Analyzing temporal developments is crucial for the accurate prognosis of many medical conditions. Temporal changes that occur over short time scales are key to assessing the health of physiological functions, such as the cardiac cycle. Moreover, tracking longer term developments that occur over months or years in evolving processes, such as age-related macular degeneration (AMD), is essential for accurate prognosis. Despite the importance of both short and long term analysis to clinical decision making, they remain understudied in medical deep learning. State of the art methods for spatiotemporal representation learning, developed for short natural videos, prioritize the detection of temporal constants rather than temporal developments. Moreover, they do not account for varying time intervals between acquisitions, which are essential for contextualizing observed changes. To address these issues, we propose two approaches. First, we combine clip-level contrastive learning with a novel temporal embedding to adapt to irregular time series. Second, we propose masking and predicting latent frame representations of the temporal sequence. Our two approaches outperform all prior methods on temporally-dependent tasks including cardiac output estimation and three prognostic AMD tasks. Overall, this enables the automated analysis of temporal patterns which are typically overlooked in applications of deep learning to medicine.","sentences":["Analyzing temporal developments is crucial for the accurate prognosis of many medical conditions.","Temporal changes that occur over short time scales are key to assessing the health of physiological functions, such as the cardiac cycle.","Moreover, tracking longer term developments that occur over months or years in evolving processes, such as age-related macular degeneration (AMD), is essential for accurate prognosis.","Despite the importance of both short and long term analysis to clinical decision making, they remain understudied in medical deep learning.","State of the art methods for spatiotemporal representation learning, developed for short natural videos, prioritize the detection of temporal constants rather than temporal developments.","Moreover, they do not account for varying time intervals between acquisitions, which are essential for contextualizing observed changes.","To address these issues, we propose two approaches.","First, we combine clip-level contrastive learning with a novel temporal embedding to adapt to irregular time series.","Second, we propose masking and predicting latent frame representations of the temporal sequence.","Our two approaches outperform all prior methods on temporally-dependent tasks including cardiac output estimation and three prognostic AMD tasks.","Overall, this enables the automated analysis of temporal patterns which are typically overlooked in applications of deep learning to medicine."],"url":"http://arxiv.org/abs/2403.07513v1","category":"cs.CV"}
{"created":"2024-03-12 10:10:56","title":"2-descent for Bloch--Kato Selmer groups and rational points on hyperelliptic curves II","abstract":"We give refined methods for proving finiteness of the Chabauty--Coleman--Kim set $X(\\mathbb{Q}_2 )_2 $, when $X$ is a hyperelliptic curve with a rational Weierstrass point. The main developments are methods for computing Selmer conditions at $2$ and $\\infty$ for the mod 2 Bloch--Kato Selmer group associated to the higher Chow group $\\mathrm{CH}^2 (\\mathrm{Jac}(X),1)$. As a result we show that most genus 2 curves in the LMFDB of Mordell--Weil rank 2 with exactly one rational Weierstrass point satsify $\\# X(\\mathbb{Q}_2 )_2 <\\infty $. We also obtain a field-theoretic description of second descent on the Jacobian of a hyperelliptic curve (under some conditions).","sentences":["We give refined methods for proving finiteness of the Chabauty--Coleman--Kim set $X(\\mathbb{Q}_2 )_2 $, when $X$ is a hyperelliptic curve with a rational Weierstrass point.","The main developments are methods for computing Selmer conditions at $2$ and $\\infty$ for the mod 2 Bloch--Kato Selmer group associated to the higher Chow group $\\mathrm{CH}^2 (\\mathrm{Jac}(X),1)$. As a result we show that most genus 2 curves in the LMFDB of Mordell--Weil rank 2 with exactly one rational Weierstrass point satsify $\\# X(\\mathbb{Q}_2 )_","2 <\\infty $.","We also obtain a field-theoretic description of second descent on the Jacobian of a hyperelliptic curve (under some conditions)."],"url":"http://arxiv.org/abs/2403.07476v1","category":"math.NT"}
{"created":"2024-03-12 10:01:06","title":"Norms in sinogram space and stability estimates for the Radon transform","abstract":"We consider different norms for the Radon transform $Rf$ of a function $f$ and investigate under which conditions they can be estimated from above or below by some standard norms for $f$. We define Fourier-based norms for $Rf$ that can be related to Bessel-potential space norms for $f$. Furthermore, we define a variant of a total-variation norm for $Rf$ and provide conditions under which it is equivalent to the total-variation norm of $f$. As an illustration of potential applications of these results, we propose a novel nonlinear backprojection method for inverting the Radon transform.","sentences":["We consider different norms for the Radon transform $Rf$ of a function $f$ and investigate under which conditions they can be estimated from above or below by some standard norms for $f$. We define Fourier-based norms for $Rf$ that can be related to Bessel-potential space norms for $f$. Furthermore, we define a variant of a total-variation norm for $Rf$ and provide conditions under which it is equivalent to the total-variation norm of $f$. As an illustration of potential applications of these results, we propose a novel nonlinear backprojection method for inverting the Radon transform."],"url":"http://arxiv.org/abs/2403.07466v1","category":"math.FA"}
{"created":"2024-03-12 08:44:15","title":"Grain growth competition and formation of grain boundaries during solidification of hcp alloys","abstract":"Grain growth competition during directional solidification of a polycrystal with hexagonal (hcp) symmetry (Mg-1wt%Gd alloy) is studied by phase-field modeling, exploring the effect of the temperature gradient G on the resulting grain boundary (GB) orientation selection. Results show that selection mechanisms and scaling laws derived for cubic (fcc, bcc) crystals also apply to hcp materials (within their basal plane), provided a re-estimation of fitting parameters and re-scaling to account for the sixfold symmetry. While grain growth competition remains stochastic with rare events of unexpected elimination or side-branching along the developing GBs, we also confirm an overall transition from a geometrical limit to a favorably oriented grain limit behavior with an increase of thermal gradient within the dendritic regime, and the progressive alignment of dendrites and GBs toward the temperature gradient direction with an increase of G during the dendritic-to-cellular morphological transition. Comparisons with original thin-sample directional solidification experiments show a qualitative agreement with PF results, yet with notable discrepancies, which nonetheless can be explained based on the stochastic variability of selected GB orientations, and the statistically limited experimental sample size. Overall, our results extend the understanding of GB formation and grain growth competition during solidification of hcp materials, and the effect of thermal conditions, nonetheless concluding on the challenges of extending the current studies to three dimensions, and the need for much broader (statistically significant) data sets of GB orientation selected under well-identified solidification conditions.","sentences":["Grain growth competition during directional solidification of a polycrystal with hexagonal (hcp) symmetry (Mg-1wt%Gd alloy) is studied by phase-field modeling, exploring the effect of the temperature gradient G on the resulting grain boundary (GB) orientation selection.","Results show that selection mechanisms and scaling laws derived for cubic (fcc, bcc) crystals also apply to hcp materials (within their basal plane), provided a re-estimation of fitting parameters and re-scaling to account for the sixfold symmetry.","While grain growth competition remains stochastic with rare events of unexpected elimination or side-branching along the developing GBs, we also confirm an overall transition from a geometrical limit to a favorably oriented grain limit behavior with an increase of thermal gradient within the dendritic regime, and the progressive alignment of dendrites and GBs toward the temperature gradient direction with an increase of G during the dendritic-to-cellular morphological transition.","Comparisons with original thin-sample directional solidification experiments show a qualitative agreement with PF results, yet with notable discrepancies, which nonetheless can be explained based on the stochastic variability of selected GB orientations, and the statistically limited experimental sample size.","Overall, our results extend the understanding of GB formation and grain growth competition during solidification of hcp materials, and the effect of thermal conditions, nonetheless concluding on the challenges of extending the current studies to three dimensions, and the need for much broader (statistically significant) data sets of GB orientation selected under well-identified solidification conditions."],"url":"http://arxiv.org/abs/2403.07416v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 08:43:51","title":"Frequency-explicit stability estimates for time-harmonic elastodynamic problems in nearly incompressible materials","abstract":"We consider time-harmonic elastodynamic problems in heterogeneous media.cWe focus on scattering problems in the high-frequency regime and incnearly incompressible media, where the the angular frequency $\\omega$ and ratio of the Lam\\'e parameters $\\lambda/\\mu$ may both be large. We derive stability estimates controlling the norm of the solution by the norm of the right-hand side up to a fully-explicit constant. Crucially, under natural assumptions on the domain and coefficients, this constant increases linearly with $\\omega$ and is uniform in the ratio $\\lambda/\\mu$.","sentences":["We consider time-harmonic elastodynamic problems in heterogeneous media.cWe focus on scattering problems in the high-frequency regime and incnearly incompressible media, where the the angular frequency $\\omega$ and ratio of the Lam\\'e parameters $\\lambda/\\mu$ may both be large.","We derive stability estimates controlling the norm of the solution by the norm of the right-hand side up to a fully-explicit constant.","Crucially, under natural assumptions on the domain and coefficients, this constant increases linearly with $\\omega$ and is uniform in the ratio $\\lambda/\\mu$."],"url":"http://arxiv.org/abs/2403.07415v1","category":"math.AP"}
{"created":"2024-03-12 08:34:05","title":"FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing","abstract":"Exemplar-free class-incremental learning (EFCIL) poses significant challenges, primarily due to catastrophic forgetting, necessitating a delicate balance between stability and plasticity to accurately recognize both new and previous classes. Traditional EFCIL approaches typically skew towards either model plasticity through successive fine-tuning or stability by employing a fixed feature extractor beyond the initial incremental state. Building upon the foundational FeTrIL framework, our research extends into novel experimental domains to examine the efficacy of various oversampling techniques and dynamic optimization strategies across multiple challenging datasets and incremental settings. We specifically explore how oversampling impacts accuracy relative to feature availability and how different optimization methodologies, including dynamic recalibration and feature pool diversification, influence incremental learning outcomes. The results from these comprehensive experiments, conducted on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior performance of FeTrIL in balancing accuracy for both new and past classes against ten contemporary methods. Notably, our extensions reveal the nuanced impacts of oversampling and optimization on EFCIL, contributing to a more refined understanding of feature-space manipulation for class incremental learning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way for more adaptable and efficient EFCIL methodologies, promising significant improvements in handling catastrophic forgetting without the need for exemplars.","sentences":["Exemplar-free class-incremental learning (EFCIL) poses significant challenges, primarily due to catastrophic forgetting, necessitating a delicate balance between stability and plasticity to accurately recognize both new and previous classes.","Traditional EFCIL approaches typically skew towards either model plasticity through successive fine-tuning or stability by employing a fixed feature extractor beyond the initial incremental state.","Building upon the foundational FeTrIL framework, our research extends into novel experimental domains to examine the efficacy of various oversampling techniques and dynamic optimization strategies across multiple challenging datasets and incremental settings.","We specifically explore how oversampling impacts accuracy relative to feature availability and how different optimization methodologies, including dynamic recalibration and feature pool diversification, influence incremental learning outcomes.","The results from these comprehensive experiments, conducted on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior performance of FeTrIL in balancing accuracy for both new and past classes against ten contemporary methods.","Notably, our extensions reveal the nuanced impacts of oversampling and optimization on EFCIL, contributing to a more refined understanding of feature-space manipulation for class incremental learning.","FeTrIL and its extended analysis in this paper FeTrIL++ pave the way for more adaptable and efficient EFCIL methodologies, promising significant improvements in handling catastrophic forgetting without the need for exemplars."],"url":"http://arxiv.org/abs/2403.07406v1","category":"cs.CV"}
{"created":"2024-03-12 08:12:56","title":"Crystal design of altermagnetism","abstract":"Symmetry plays a fundamental role in condensed matter. The unique entanglement between magnetic sublattices and alternating crystal environment in altermagnets provides a unique opportunity for designing magnetic space symmetry. There have been extensive experimental efforts concentrated on tuning the Neel vector to reconstruct altermagnetic symmetry. However, it remains challenging to modulate the altermagnetic symmetry through the crystal aspect. Here, the crystal design of altermagnetism is successfully realized, by breaking glide mirrors and magnetic mirrors of the (0001) crystallographic plane in CrSb films via crystal distortion. We establish a locking relationship between altermagnetic symmetry and the emergent Dzyaloshinskii-Moriya (DM) vectors in different CrSb films, realizing unprecedentedly room-temperature spontaneous anomalous Hall effect in an altermagnetic metal. The concept of exchange-coupling torques is broadened to include both antiferromagnetic exchange-coupling torque and DM torque. Their relationship is designable, determining electrical manipulation modes, e.g., field-assisted switching for CrSb(1-100)/Pt and field-free switching for W/CrSb(11-20). Particularly, the unprecedentedly field-free 100-percent switching of Neel vectors is realized by making these two torques parallel or antiparallel, dependent on Neel vector orientation. Besides unravelling the rich mechanisms for electrical manipulation of altermagnetism rooted in broadened concept of exchange-coupling torques, we list other material candidates and propose that crystal design of altermagnetism would bring rich designability to magnonics, topology, etc.","sentences":["Symmetry plays a fundamental role in condensed matter.","The unique entanglement between magnetic sublattices and alternating crystal environment in altermagnets provides a unique opportunity for designing magnetic space symmetry.","There have been extensive experimental efforts concentrated on tuning the Neel vector to reconstruct altermagnetic symmetry.","However, it remains challenging to modulate the altermagnetic symmetry through the crystal aspect.","Here, the crystal design of altermagnetism is successfully realized, by breaking glide mirrors and magnetic mirrors of the (0001) crystallographic plane in CrSb films via crystal distortion.","We establish a locking relationship between altermagnetic symmetry and the emergent Dzyaloshinskii-Moriya (DM) vectors in different CrSb films, realizing unprecedentedly room-temperature spontaneous anomalous Hall effect in an altermagnetic metal.","The concept of exchange-coupling torques is broadened to include both antiferromagnetic exchange-coupling torque and DM torque.","Their relationship is designable, determining electrical manipulation modes, e.g., field-assisted switching for CrSb(1-100)/Pt and field-free switching for W/CrSb(11-20).","Particularly, the unprecedentedly field-free 100-percent switching of Neel vectors is realized by making these two torques parallel or antiparallel, dependent on Neel vector orientation.","Besides unravelling the rich mechanisms for electrical manipulation of altermagnetism rooted in broadened concept of exchange-coupling torques, we list other material candidates and propose that crystal design of altermagnetism would bring rich designability to magnonics, topology, etc."],"url":"http://arxiv.org/abs/2403.07396v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 08:03:54","title":"Learning on the correct class for domain inverse problems of gravimetry","abstract":"We consider end-to-end learning approaches for inverse problems of gravimetry. Due to ill-posedness of the inverse gravimetry, the reliability of learning approaches is questionable. To deal with this problem, we propose the strategy of learning on the correct class. The well-posedness theorems are employed when designing the neural-network architecture and constructing the training set. Given the density-contrast function as a priori information, the domain of mass can be uniquely determined under certain constrains, and the domain inverse problem is a correct class of the inverse gravimetry. Under this correct class, we design the neural network for learning by mimicking the level-set formulation for the inverse gravimetry. Numerical examples illustrate that the method is able to recover mass models with non-constant density contrast.","sentences":["We consider end-to-end learning approaches for inverse problems of gravimetry.","Due to ill-posedness of the inverse gravimetry, the reliability of learning approaches is questionable.","To deal with this problem, we propose the strategy of learning on the correct class.","The well-posedness theorems are employed when designing the neural-network architecture and constructing the training set.","Given the density-contrast function as a priori information, the domain of mass can be uniquely determined under certain constrains, and the domain inverse problem is a correct class of the inverse gravimetry.","Under this correct class, we design the neural network for learning by mimicking the level-set formulation for the inverse gravimetry.","Numerical examples illustrate that the method is able to recover mass models with non-constant density contrast."],"url":"http://arxiv.org/abs/2403.07393v1","category":"physics.geo-ph"}
{"created":"2024-03-12 07:56:24","title":"Understanding the shear modulus of dense microgel suspensions","abstract":"Polymer microgels exhibit intriguing macroscopic flow properties arising from their unique microscopic structure. Microgel colloids comprise a crosslinked polymer network with a radially decaying density profile, resulting in a dense core surrounded by a fuzzy corona. Notably, microgels synthesized from poly(N-isopropylacrylamide) (PNIPAM) are thermoresponsive, capable of adjusting their size and density profile based on temperature. Above the lower critical solution temperature ($T_\\text{LCST}\\sim 33$ $^\\circ$C), the microgel's polymer network collapses, leading to the expulsion of water through a reversible process. Conversely, below $33$ $^\\circ$C, the microgel's network swells, becoming highly compressible and allowing overpacking to effective volume fractions exceeding one. Under conditions of dense packing, microgels undergo deformation in distinct stages: corona compression and faceting, interpenetration, and finally, isotropic compression. Each stage exhibits a characteristic signature in the yield stress and elastic modulus of the dense microgel suspensions. Here, we introduce a model for the linear elastic shear modulus through the minimization of a quasi-equilibrium free energy, encompassing all relevant energetic contributions. We validate our model by comparing its predictions to experimental results from oscillatory shear rheology tests on microgel suspensions at different densities and temperatures. Our findings demonstrate that combining macroscopic rheological measurements with the model allows for temperature-dependent characterization of polymer interaction parameters.","sentences":["Polymer microgels exhibit intriguing macroscopic flow properties arising from their unique microscopic structure.","Microgel colloids comprise a crosslinked polymer network with a radially decaying density profile, resulting in a dense core surrounded by a fuzzy corona.","Notably, microgels synthesized from poly(N-isopropylacrylamide) (PNIPAM) are thermoresponsive, capable of adjusting their size and density profile based on temperature.","Above the lower critical solution temperature ($T_\\text{LCST}\\sim 33$ $^\\circ$C), the microgel's polymer network collapses, leading to the expulsion of water through a reversible process.","Conversely, below $33$ $^\\circ$C, the microgel's network swells, becoming highly compressible and allowing overpacking to effective volume fractions exceeding one.","Under conditions of dense packing, microgels undergo deformation in distinct stages: corona compression and faceting, interpenetration, and finally, isotropic compression.","Each stage exhibits a characteristic signature in the yield stress and elastic modulus of the dense microgel suspensions.","Here, we introduce a model for the linear elastic shear modulus through the minimization of a quasi-equilibrium free energy, encompassing all relevant energetic contributions.","We validate our model by comparing its predictions to experimental results from oscillatory shear rheology tests on microgel suspensions at different densities and temperatures.","Our findings demonstrate that combining macroscopic rheological measurements with the model allows for temperature-dependent characterization of polymer interaction parameters."],"url":"http://arxiv.org/abs/2403.07388v1","category":"cond-mat.soft"}
{"created":"2024-03-12 07:44:38","title":"Plasmon-driven creation of magnetic topological structures","abstract":"In the present research, we demonstrate the usage of plasmonic effects in thin film structures to control magnetic topological textures, specifically skyrmions and skyrmioniums. We investigate numerically the generation and alteration of these topological structures caused by hemisphere gold nanoparticle placed over a magnetic layer coated with a dielectric material. The electromagnetic and photothermal models are used to clarify the processes of producing heat and absorption, and the results were implemented in micromagnetic formalism to reveal the dynamics of magnetization under various conditions. Our findings demonstrate the significance of the laser pulse duration and the contact area between nanoparticles and the underlying magnetic layer in forming topological textures. In particular, we show how to generate a single skyrmion, multiple skyrmions, and skyrmioniums, and how to dynamically transition between these states. These results highlight the possibility of manipulating magnetic textures by using plasmonic effects, which presents significant opportunities for spintronics and non-conventional computer applications.","sentences":["In the present research, we demonstrate the usage of plasmonic effects in thin film structures to control magnetic topological textures, specifically skyrmions and skyrmioniums.","We investigate numerically the generation and alteration of these topological structures caused by hemisphere gold nanoparticle placed over a magnetic layer coated with a dielectric material.","The electromagnetic and photothermal models are used to clarify the processes of producing heat and absorption, and the results were implemented in micromagnetic formalism to reveal the dynamics of magnetization under various conditions.","Our findings demonstrate the significance of the laser pulse duration and the contact area between nanoparticles and the underlying magnetic layer in forming topological textures.","In particular, we show how to generate a single skyrmion, multiple skyrmions, and skyrmioniums, and how to dynamically transition between these states.","These results highlight the possibility of manipulating magnetic textures by using plasmonic effects, which presents significant opportunities for spintronics and non-conventional computer applications."],"url":"http://arxiv.org/abs/2403.07382v1","category":"physics.app-ph"}
{"created":"2024-03-12 07:31:18","title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression","abstract":"The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios. The source code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.","sentences":["The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment.","Singular Value Decomposition (SVD) offers a promising solution for LLM compression.","However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation.","In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods.","SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss.","Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation.","We evaluate SVD-LLM on a total of 11 datasets and seven models from three different LLM families at four different scales.","Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.","The source code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM."],"url":"http://arxiv.org/abs/2403.07378v1","category":"cs.CL"}
{"created":"2024-03-12 07:04:03","title":"Observations of spiral and streamer on a candidate proto-brown dwarf","abstract":"Spirals and streamers are the hallmarks of mass accretion during the early stages of star formation. We present the first observations of a large-scale spiral and a streamer towards a very young brown dwarf candidate in its early formation stages. These observations show, for the first time, the influence of external environment that results in asymmetric mass accretion via feeding filaments onto a candidate proto-brown dwarf in the making. The impact of the streamer has produced emission in warm carbon-chain species close to the candidate proto-brown dwarf. Two contrasting scenarios, a pseudo-disk twisted by core rotation and the collision of dense cores, can both explain these structures. The former argues for the presence of a strong magnetic field in brown dwarf formation while the latter suggests that a minimal magnetic field allows large-scale spirals and clumps to form far from the candidate proto-brown dwarf.","sentences":["Spirals and streamers are the hallmarks of mass accretion during the early stages of star formation.","We present the first observations of a large-scale spiral and a streamer towards a very young brown dwarf candidate in its early formation stages.","These observations show, for the first time, the influence of external environment that results in asymmetric mass accretion via feeding filaments onto a candidate proto-brown dwarf in the making.","The impact of the streamer has produced emission in warm carbon-chain species close to the candidate proto-brown dwarf.","Two contrasting scenarios, a pseudo-disk twisted by core rotation and the collision of dense cores, can both explain these structures.","The former argues for the presence of a strong magnetic field in brown dwarf formation while the latter suggests that a minimal magnetic field allows large-scale spirals and clumps to form far from the candidate proto-brown dwarf."],"url":"http://arxiv.org/abs/2403.07367v1","category":"astro-ph.SR"}
{"created":"2024-03-12 07:01:57","title":"Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors","abstract":"Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation. DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently. For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions. Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild. Project page is publicly available at https://whitesnowdrop.github.io/DeYO/.","sentences":["Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data.","The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation.","To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error.","Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions.","Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD).","PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation.","DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently.","For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions.","Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild.","Project page is publicly available at https://whitesnowdrop.github.io/DeYO/."],"url":"http://arxiv.org/abs/2403.07366v1","category":"cs.CV"}
{"created":"2024-03-12 06:58:37","title":"Hybrid Kinetics Embedding Framework for Dynamic PET Reconstruction","abstract":"In dynamic positron emission tomography (PET) reconstruction, the importance of leveraging the temporal dependence of the data has been well appreciated. Current deep-learning solutions can be categorized in two groups in the way the temporal dynamics is modeled: data-driven approaches use spatiotemporal neural networks to learn the temporal dynamics of tracer kinetics from data, which relies heavily on data supervision; physics-based approaches leverage \\textit{a priori} tracer kinetic models to focus on inferring their parameters, which relies heavily on the accuracy of the prior kinetic model. In this paper, we marry the strengths of these two approaches in a hybrid kinetics embedding (HyKE-Net) framework for dynamic PET reconstruction. We first introduce a novel \\textit{hybrid} model of tracer kinetics consisting of a physics-based function augmented by a neural component to account for its gap to data-generating tracer kinetics, both identifiable from data. We then embed this hybrid model at the latent space of an encoding-decoding framework to enable both supervised and unsupervised identification of the hybrid kinetics and thereby dynamic PET reconstruction. Through both phantom and real-data experiments, we demonstrate the benefits of HyKE-Net -- especially in unsupervised reconstructions -- over existing physics-based and data-driven baselines as well as its ablated formulations where the embedded tracer kinetics are purely physics-based, purely neural, or hybrid but with a non-adaptable neural component.","sentences":["In dynamic positron emission tomography (PET) reconstruction, the importance of leveraging the temporal dependence of the data has been well appreciated.","Current deep-learning solutions can be categorized in two groups in the way the temporal dynamics is modeled: data-driven approaches use spatiotemporal neural networks to learn the temporal dynamics of tracer kinetics from data, which relies heavily on data supervision; physics-based approaches leverage \\textit{a priori} tracer kinetic models to focus on inferring their parameters, which relies heavily on the accuracy of the prior kinetic model.","In this paper, we marry the strengths of these two approaches in a hybrid kinetics embedding (HyKE-Net) framework for dynamic PET reconstruction.","We first introduce a novel \\textit{hybrid} model of tracer kinetics consisting of a physics-based function augmented by a neural component to account for its gap to data-generating tracer kinetics, both identifiable from data.","We then embed this hybrid model at the latent space of an encoding-decoding framework to enable both supervised and unsupervised identification of the hybrid kinetics and thereby dynamic PET reconstruction.","Through both phantom and real-data experiments, we demonstrate the benefits of HyKE-Net -- especially in unsupervised reconstructions -- over existing physics-based and data-driven baselines as well as its ablated formulations where the embedded tracer kinetics are purely physics-based, purely neural, or hybrid but with a non-adaptable neural component."],"url":"http://arxiv.org/abs/2403.07364v1","category":"eess.IV"}
{"created":"2024-03-12 06:48:16","title":"Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning","abstract":"Injecting greenhouse gas into deep underground reservoirs for permanent storage can inadvertently lead to fault reactivation, caprock fracturing and greenhouse gas leakage when the injection-induced stress exceeds the critical threshold. Extraction of pre-existing fluids at various stages of injection process, referred as pressure management, can mitigate associated risks and lessen environmental impact. However, identifying optimal pressure management strategies typically requires thousands of full-order simulations due to the need for function evaluations, making the process computationally prohibitive. This paper introduces a novel surrogate model-based reinforcement learning method for devising optimal pressure management strategies for geological CO2 sequestration efficiently. Our approach comprises two steps. Firstly, a surrogate model is developed through the embed to control method, which employs an encoder-transition-decoder structure to learn latent dynamics. Leveraging this proxy model, reinforcement learning is utilized to find an optimal strategy that maximizes economic benefits while satisfying various control constraints. The reinforcement learning agent receives the latent state space representation and immediate reward tailored for CO2 sequestration and choose real-time controls which are subject to predefined engineering constraints in order to maximize the long-term cumulative rewards. To demonstrate its effectiveness, this framework is applied to a compositional simulation model where CO2 is injected into saline aquifer. The results reveal that our surrogate model-based reinforcement learning approach significantly optimizes CO2 sequestration strategies, leading to notable economic gains compared to baseline scenarios.","sentences":["Injecting greenhouse gas into deep underground reservoirs for permanent storage can inadvertently lead to fault reactivation, caprock fracturing and greenhouse gas leakage when the injection-induced stress exceeds the critical threshold.","Extraction of pre-existing fluids at various stages of injection process, referred as pressure management, can mitigate associated risks and lessen environmental impact.","However, identifying optimal pressure management strategies typically requires thousands of full-order simulations due to the need for function evaluations, making the process computationally prohibitive.","This paper introduces a novel surrogate model-based reinforcement learning method for devising optimal pressure management strategies for geological CO2 sequestration efficiently.","Our approach comprises two steps.","Firstly, a surrogate model is developed through the embed to control method, which employs an encoder-transition-decoder structure to learn latent dynamics.","Leveraging this proxy model, reinforcement learning is utilized to find an optimal strategy that maximizes economic benefits while satisfying various control constraints.","The reinforcement learning agent receives the latent state space representation and immediate reward tailored for CO2 sequestration and choose real-time controls which are subject to predefined engineering constraints in order to maximize the long-term cumulative rewards.","To demonstrate its effectiveness, this framework is applied to a compositional simulation model where CO2 is injected into saline aquifer.","The results reveal that our surrogate model-based reinforcement learning approach significantly optimizes CO2 sequestration strategies, leading to notable economic gains compared to baseline scenarios."],"url":"http://arxiv.org/abs/2403.07360v1","category":"cs.CE"}
{"created":"2024-03-12 06:18:39","title":"Predicting the Scaling Relations between the Dark Matter Halo Mass and Observables from Generalised Profiles I: Kinematic Tracers","abstract":"We investigate the relationship between a dark matter halo's mass profile and measures of the velocity dispersion of kinematic tracers within its gravitational potential. By predicting the scaling relation of the halo mass with the aperture velocity dispersion, $M_\\mathrm{vir} - \\sigma_\\mathrm{ap}$, we present the expected form and dependence of this halo mass tracer on physical parameters within our analytic halo model: parameterised by the halo's negative inner logarithmic density slope, $\\alpha$, its concentration parameter, $c$, and its velocity anisotropy parameter, $\\beta$. For these idealised halos, we obtain a general solution to the Jeans equation, which is projected over the line of sight and averaged within an aperture to form the corresponding aperture velocity dispersion profile. Through dimensional analysis, the $M_\\mathrm{vir} - \\sigma_\\mathrm{ap}$ scaling relation is devised explicitly in terms of analytical bounds for these aperture velocity dispersion profiles: allowing constraints to be placed on this relation for motivated parameter choices. We predict the $M_{200} - \\sigma_\\mathrm{ap}$ and $M_{500} - \\sigma_\\mathrm{ap}$ scaling relations, each with an uncertainty of $60.5\\%$ and $56.2\\%$, respectively. These halo mass estimates are found to be weakly sensitive to the halo's concentration and mass scale, and most sensitive to the size of the aperture radius in which the aperture velocity dispersion is measured, the maximum value for the halo's inner slope, and the minimum and maximum values of the velocity anisotropy. Our results show that a halo's structural and kinematic profiles impose only a minor uncertainty in estimating its mass. Consequently, spectroscopic surveys aimed at constraining the halo mass using kinematic tracers can focus on characterising other, more complex sources of uncertainty and observational systematics.","sentences":["We investigate the relationship between a dark matter halo's mass profile and measures of the velocity dispersion of kinematic tracers within its gravitational potential.","By predicting the scaling relation of the halo mass with the aperture velocity dispersion, $M_\\mathrm{vir} - \\sigma_\\mathrm{ap}$, we present the expected form and dependence of this halo mass tracer on physical parameters within our analytic halo model: parameterised by the halo's negative inner logarithmic density slope, $\\alpha$, its concentration parameter, $c$, and its velocity anisotropy parameter, $\\beta$. For these idealised halos, we obtain a general solution to the Jeans equation, which is projected over the line of sight and averaged within an aperture to form the corresponding aperture velocity dispersion profile.","Through dimensional analysis, the $M_\\mathrm{vir} - \\sigma_\\mathrm{ap}$ scaling relation is devised explicitly in terms of analytical bounds for these aperture velocity dispersion profiles: allowing constraints to be placed on this relation for motivated parameter choices.","We predict the $M_{200} - \\sigma_\\mathrm{ap}$ and $M_{500} - \\sigma_\\mathrm{ap}$ scaling relations, each with an uncertainty of $60.5\\%$ and $56.2\\%$, respectively.","These halo mass estimates are found to be weakly sensitive to the halo's concentration and mass scale, and most sensitive to the size of the aperture radius in which the aperture velocity dispersion is measured, the maximum value for the halo's inner slope, and the minimum and maximum values of the velocity anisotropy.","Our results show that a halo's structural and kinematic profiles impose only a minor uncertainty in estimating its mass.","Consequently, spectroscopic surveys aimed at constraining the halo mass using kinematic tracers can focus on characterising other, more complex sources of uncertainty and observational systematics."],"url":"http://arxiv.org/abs/2403.07352v1","category":"astro-ph.GA"}
{"created":"2024-03-12 06:07:29","title":"Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture","abstract":"Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world. Prior methods directly model the motion field from the Eulerian perspective by Representation Learning that separates shape and texture or Multi-domain Learning from phase fluctuations. Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field. To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space. Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling. Besides, we innovatively design a contrastive regularization for this task to strengthen the model's ability to discriminate irrelevant features, reducing undesired motion magnification. Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\\times$ and boosts inference speed by 1.68$\\times$ than the latest method. Our code is available at https://github.com/Jiafei127/FD4MM.","sentences":["Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world.","Prior methods directly model the motion field from the Eulerian perspective by Representation Learning that separates shape and texture or Multi-domain Learning from phase fluctuations.","Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field.","To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space.","Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling.","Besides, we innovatively design a contrastive regularization for this task to strengthen the model's ability to discriminate irrelevant features, reducing undesired motion magnification.","Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods.","Meanwhile, FD4MM reduces FLOPs by 1.63$\\times$ and boosts inference speed by 1.68$\\times$ than the latest method.","Our code is available at https://github.com/Jiafei127/FD4MM."],"url":"http://arxiv.org/abs/2403.07347v1","category":"cs.CV"}
{"created":"2024-03-12 06:04:50","title":"Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction","abstract":"Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions. Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties, but it lacks key texture appearance for hand mesh reconstruction. In this paper, we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other. By fusing two modalities of data across time, space, and information dimensions,EvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR. We further propose EvRGBDegrader, which allows our model to generalize effectively in challenging scenes, even when trained solely on standard scenes, thus reducing data acquisition costs. Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both, and shows the potential of generalization to outdoor scenes and another type of event camera.","sentences":["Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions.","Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties, but it lacks key texture appearance for hand mesh reconstruction.","In this paper, we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other.","By fusing two modalities of data across time, space, and information dimensions,EvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR.","We further propose EvRGBDegrader, which allows our model to generalize effectively in challenging scenes, even when trained solely on standard scenes, thus reducing data acquisition costs.","Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both, and shows the potential of generalization to outdoor scenes and another type of event camera."],"url":"http://arxiv.org/abs/2403.07346v1","category":"cs.CV"}
{"created":"2024-03-12 06:04:15","title":"The transition operator of a random walk perturbated by sparse potentials","abstract":"We consider an operator $P_V=(1+V)P$ on $\\ell^2(Z^d)$, where $P$ is the transition operator of a symmetric irreducible random walk, and $V$ is a ``sparse'' potential. We first characterize the essential spectra of this operator. Secondly, we prove that all the eigenfunctions which correspond to discrete spectra decay exponentially fast. Thirdly, we give a sufficient condition for this operator to have an absolute spectral gap at the right edge of the spectra. Finally, as an application of the absolute spectral gap and the exponential decay of the eigenfunctions, we prove a limit theorem for the random walk under the Gibbs measure associated to the potential $V$.","sentences":["We consider an operator $P_V=(1+V)P$ on $\\ell^2(Z^d)$, where $P$ is the transition operator of a symmetric irreducible random walk, and $V$ is a ``sparse'' potential.","We first characterize the essential spectra of this operator.","Secondly, we prove that all the eigenfunctions which correspond to discrete spectra decay exponentially fast.","Thirdly, we give a sufficient condition for this operator to have an absolute spectral gap at the right edge of the spectra.","Finally, as an application of the absolute spectral gap and the exponential decay of the eigenfunctions, we prove a limit theorem for the random walk under the Gibbs measure associated to the potential $V$."],"url":"http://arxiv.org/abs/2403.07345v1","category":"math.SP"}
{"created":"2024-03-12 05:53:25","title":"Direct and inverse time-harmonic scattering by Dirichlet periodic curves with local perturbations","abstract":"This is a continuation of the authors' previous work (A. Kirsch, Math. Meth. Appl. Sci., 45 (2022): 5737-5773.) on well-posedness of time-harmonic scattering by locally perturbed periodic curves of Dirichlet kind. The scattering interface is supposed to be given by a non-self-intersecting Lipschitz curve. We study properties of the Green's function and prove new well-posedness results for scattering of plane waves at a propagative wave number. In such a case there exist guided waves to the unperturbed problem, which are also known as Bounded States in the Continuity (BICs) in physics. In this paper uniqueness of the forward scattering follows from an orthogonal constraint condition enforcing on the total field to the unperturbed scattering problem. This constraint condition, which is also valid under the Neumann boundary condition, is derived from the singular perturbation arguments and also from the approach of approximating a plane wave by point source waves. For the inverse problem of determining the defect, we prove several uniqueness results using a finite or infinite number of point source and plane waves, depending on whether a priori information on the size and height of the defect is available.","sentences":["This is a continuation of the authors' previous work (A. Kirsch, Math. Meth.","Appl.","Sci., 45 (2022): 5737-5773.)","on well-posedness of time-harmonic scattering by locally perturbed periodic curves of Dirichlet kind.","The scattering interface is supposed to be given by a non-self-intersecting Lipschitz curve.","We study properties of the Green's function and prove new well-posedness results for scattering of plane waves at a propagative wave number.","In such a case there exist guided waves to the unperturbed problem, which are also known as Bounded States in the Continuity (BICs) in physics.","In this paper uniqueness of the forward scattering follows from an orthogonal constraint condition enforcing on the total field to the unperturbed scattering problem.","This constraint condition, which is also valid under the Neumann boundary condition, is derived from the singular perturbation arguments and also from the approach of approximating a plane wave by point source waves.","For the inverse problem of determining the defect, we prove several uniqueness results using a finite or infinite number of point source and plane waves, depending on whether a priori information on the size and height of the defect is available."],"url":"http://arxiv.org/abs/2403.07340v1","category":"math.AP"}
{"created":"2024-03-12 05:44:27","title":"IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers","abstract":"GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\\em low} bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone. To address this issue, we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers. This allows {\\em equivalence} with the original GEMM, i.e., the exact result can be obtained using purely low bit-width integer GEMMs. This comes at the cost of additional operations -- we show that for many popular models, this overhead is quite small.","sentences":["GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint.","Therefore, improving its efficiency is an active topic of ongoing research.","A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix.","This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred.","In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\\em both} training and inference stages, and can achieve parity with floating point counterparts.","No sophisticated techniques are needed.","We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\\em low} bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone.","To address this issue, we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers.","This allows {\\em equivalence} with the original GEMM, i.e., the exact result can be obtained using purely low bit-width integer GEMMs.","This comes at the cost of additional operations -- we show that for many popular models, this overhead is quite small."],"url":"http://arxiv.org/abs/2403.07339v1","category":"cs.LG"}
{"created":"2024-03-12 05:39:53","title":"Mathematical analysis and numerical comparison of energy-conservative schemes for the Zakharov equations","abstract":"Furihata and Matsuo proposed in 2010 an energy-conserving scheme for the Zakharov equations, as an application of the discrete variational derivative method (DVDM).   This scheme is distinguished from conventional methods (in particular the one devised by Glassey in 1992) in that the invariants are consistent with respect to time, but it has not been sufficiently studied both theoretically and numerically.   In this study, we theoretically prove the solvability under the loosest possible assumptions.   We also prove the convergence of this DVDM scheme by improving the argument by Glassey.   Furthermore, we perform intensive numerical experiments for comparing the above two schemes.   It is found that the DVDM scheme is superior in terms of accuracy, but since it is fully-implicit, the linearly-implicit Glassey scheme is better for practical efficiency.   In addition, we proposed a way to choose a solution for the first step that would allow Glassey's scheme to work more efficiently.","sentences":["Furihata and Matsuo proposed in 2010 an energy-conserving scheme for the Zakharov equations, as an application of the discrete variational derivative method (DVDM).   ","This scheme is distinguished from conventional methods (in particular the one devised by Glassey in 1992) in that the invariants are consistent with respect to time, but it has not been sufficiently studied both theoretically and numerically.   ","In this study, we theoretically prove the solvability under the loosest possible assumptions.   ","We also prove the convergence of this DVDM scheme by improving the argument by Glassey.   ","Furthermore, we perform intensive numerical experiments for comparing the above two schemes.   ","It is found that the DVDM scheme is superior in terms of accuracy, but since it is fully-implicit, the linearly-implicit Glassey scheme is better for practical efficiency.   ","In addition, we proposed a way to choose a solution for the first step that would allow Glassey's scheme to work more efficiently."],"url":"http://arxiv.org/abs/2403.07336v1","category":"math.NA"}
{"created":"2024-03-12 05:34:55","title":"$\u03b1$-Generalized No-Scale Inflation","abstract":"We propose the $\\alpha$-generalized no-scale supergravity, and study the corresponding inflationary models. With a new parameter $0<\\alpha\\leq 1$, the $\\alpha$-generalized no-scale supergravity provides the continuous connections among the generic no-scale supergravity from string theory compactifications. The resulting prediction of the CMB, spectrum index $n_s$, and tensor-to-scalar ratio $r$ can be highly consistent with the latest Planck/BICEP/Keck Array observations. Notably, the models with $\\alpha\\neq 1$ give a smaller ratio $r\\leq 10^{-3}$, which is flexible even under the anticipated tighter observational constraints at the future experiments. Additionally, these models have the potential to generate a broad-band stochastic gravitational wave background, and thus explain the NANOGrav 15yr signal. Furthermore, they predict the formation of primordial black holes (PBHs) with various mass scales, which could account for an significant portion of dark matter relic density in the Universe.","sentences":["We propose the $\\alpha$-generalized no-scale supergravity, and study the corresponding inflationary models.","With a new parameter $0<\\alpha\\leq 1$, the $\\alpha$-generalized no-scale supergravity provides the continuous connections among the generic no-scale supergravity from string theory compactifications.","The resulting prediction of the CMB, spectrum index $n_s$, and tensor-to-scalar ratio $r$ can be highly consistent with the latest Planck/BICEP/Keck Array observations.","Notably, the models with $\\alpha\\neq 1$ give a smaller ratio $r\\leq 10^{-3}$, which is flexible even under the anticipated tighter observational constraints at the future experiments.","Additionally, these models have the potential to generate a broad-band stochastic gravitational wave background, and thus explain the NANOGrav 15yr signal.","Furthermore, they predict the formation of primordial black holes (PBHs) with various mass scales, which could account for an significant portion of dark matter relic density in the Universe."],"url":"http://arxiv.org/abs/2403.07333v1","category":"gr-qc"}
{"created":"2024-03-12 05:22:40","title":"Challenges in the extraction of physics beyond the Standard Model from electron scattering","abstract":"Precise measurements of electron and positron scattering, including parity violation, offer great promise in the search for physics beyond the Standard Model. In this context it is crucial to understand the corrections which might arise from charge symmetry violation, as well as the less well known strange and charm quark distributions. Our analysis, using state of the art parton distributions, suggests that these contributions lead to corrections in the extraction of the weak couplings $g^{eq}_{AV}$ and $g^{eq}_{VA}$ of the order $(1-2)\\%$, while they are as large as $4\\%$ for $g^{eq}_{AA}$, at a typical scale of $Q^2 = 10\\ {\\rm GeV}^2$. These results underline the importance of carrying out high precision measurements, which will not only provide information on physics beyond the Standard Model but also reduce the current uncertainties on our knowledge of the strange and charm quark distributions in the proton.","sentences":["Precise measurements of electron and positron scattering, including parity violation, offer great promise in the search for physics beyond the Standard Model.","In this context it is crucial to understand the corrections which might arise from charge symmetry violation, as well as the less well known strange and charm quark distributions.","Our analysis, using state of the art parton distributions, suggests that these contributions lead to corrections in the extraction of the weak couplings $g^{eq}_{AV}$ and $g^{eq}_{VA}$ of the order $(1-2)\\%$, while they are as large as $4\\%$ for $g^{eq}_{AA}$, at a typical scale of $Q^2 = 10\\ {\\rm GeV}^2$. These results underline the importance of carrying out high precision measurements, which will not only provide information on physics beyond the Standard Model but also reduce the current uncertainties on our knowledge of the strange and charm","quark distributions in the proton."],"url":"http://arxiv.org/abs/2403.07327v1","category":"hep-ph"}
{"created":"2024-03-12 05:04:58","title":"Test for high-dimensional linear hypothesis of mean vectors via random integration","abstract":"In this paper, we investigate hypothesis testing for the linear combination of mean vectors across multiple populations through the method of random integration. We have established the asymptotic distributions of the test statistics under both null and alternative hypotheses. Additionally, we provide a theoretical explanation for the special use of our test statistics in situations when the nonzero signal in the linear combination of the true mean vectors is weakly dense. Moreover, Monte-Carlo simulations are presented to evaluate the suggested test against existing high-dimensional tests. The findings from these simulations reveal that our test not only aligns with the performance of other tests in terms of size but also exhibits superior power.","sentences":["In this paper, we investigate hypothesis testing for the linear combination of mean vectors across multiple populations through the method of random integration.","We have established the asymptotic distributions of the test statistics under both null and alternative hypotheses.","Additionally, we provide a theoretical explanation for the special use of our test statistics in situations when the nonzero signal in the linear combination of the true mean vectors is weakly dense.","Moreover, Monte-Carlo simulations are presented to evaluate the suggested test against existing high-dimensional tests.","The findings from these simulations reveal that our test not only aligns with the performance of other tests in terms of size but also exhibits superior power."],"url":"http://arxiv.org/abs/2403.07318v1","category":"stat.AP"}
{"created":"2024-03-12 05:02:39","title":"Secant variety and syzygies of Hilbert scheme of two points","abstract":"In this paper, we prove that the singular locus of $\\mathrm{Sec} (X^{[2]})$ coincides with $X^{[2]}$ under the Grothendieck-Pl\\\"ucker embedding $X^{[2]} \\hookrightarrow \\mathbb{P}^N$ when $X$ is embedded by a $4$-very ample line bundle. We also prove that the embedding $X^{[2]} \\hookrightarrow \\mathbb{P}^N$ satisfies Green's condition $(N_p)$ when the embedding of $X$ is positive enough. As an application, we describe the geometry of a resolution of singularities from the secant bundle to $\\mathrm{Sec}(X^{[2]})$ when $X$ is a surface.","sentences":["In this paper, we prove that the singular locus of $\\mathrm{Sec} (X^{[2]})$ coincides with $X^{[2]}$ under the Grothendieck-Pl\\\"ucker embedding $X^{[2]} \\hookrightarrow \\mathbb{P}^N$ when $X$ is embedded by a $4$-very ample line bundle.","We also prove that the embedding $X^{[2]} \\hookrightarrow \\mathbb{P}^N$ satisfies Green's condition $(N_p)$ when the embedding of $X$ is positive enough.","As an application, we describe the geometry of a resolution of singularities from the secant bundle to $\\mathrm{Sec}(X^{[2]})$ when $X$ is a surface."],"url":"http://arxiv.org/abs/2403.07315v1","category":"math.AG"}
{"created":"2024-03-12 04:00:52","title":"Modelling response time contrasts in superconducting nanowire single photon detectors","abstract":"Superconducting Nanowire Single Photon Detector (SNSPD) emerges as a potential candidate in the multiple fields requiring sensitive and fast photodetection. While nanowires of low temperature superconducting detectors are mature with commercial solutions, other material options with higher transition temperature and faster responses are currently being explored. Towards this goal, we develop a generalized numerical model that incorporates the thermodynamic properties of the superconducting material and identifies the minimum resolvable photon count for a given bias and device parameters. A phase diagram of detection and latching phases with the minimum number of photons as a function of biasing current and biasing temperature for each material system is presented. We show using the developed model that while low temperature superconducting (LTS) nanowires are more sensitive to the incident photon at different wavelengths, the ultimate limit of a single photon can be achieved using high temperature superconducting (HTS) material such as YBa2Cu3O7-{\\delta}, albeit at stringent biasing conditions. On the contrary, ultrafast response time with three orders of magnitude smaller response times can be achieved in select HTS materials making it an appealing for several practical applications.","sentences":["Superconducting Nanowire Single Photon Detector (SNSPD) emerges as a potential candidate in the multiple fields requiring sensitive and fast photodetection.","While nanowires of low temperature superconducting detectors are mature with commercial solutions, other material options with higher transition temperature and faster responses are currently being explored.","Towards this goal, we develop a generalized numerical model that incorporates the thermodynamic properties of the superconducting material and identifies the minimum resolvable photon count for a given bias and device parameters.","A phase diagram of detection and latching phases with the minimum number of photons as a function of biasing current and biasing temperature for each material system is presented.","We show using the developed model that while low temperature superconducting (LTS) nanowires are more sensitive to the incident photon at different wavelengths, the ultimate limit of a single photon can be achieved using high temperature superconducting (HTS) material such as YBa2Cu3O7-{\\delta}, albeit at stringent biasing conditions.","On the contrary, ultrafast response time with three orders of magnitude smaller response times can be achieved in select HTS materials making it an appealing for several practical applications."],"url":"http://arxiv.org/abs/2403.07299v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 03:57:25","title":"Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal","abstract":"This paper presents a novel approach to noninvasive hyperglycemia monitoring utilizing electrocardiograms (ECG) from an extensive database comprising 1119 subjects. Previous research on hyperglycemia or glucose detection using ECG has been constrained by challenges related to generalization and scalability, primarily due to using all subjects' ECG in training without considering unseen subjects as a critical factor for developing methods with effective generalization. We designed a deep neural network model capable of identifying significant features across various spatial locations and examining the interdependencies among different features within each convolutional layer. To expedite processing speed, we segment the ECG of each user to isolate one heartbeat or one cycle of the ECG. Our model was trained using data from 727 subjects, while 168 were used for validation. The testing phase involved 224 unseen subjects, with a dataset consisting of 9,000 segments. The result indicates that the proposed algorithm effectively detects hyperglycemia with a 91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity.","sentences":["This paper presents a novel approach to noninvasive hyperglycemia monitoring utilizing electrocardiograms (ECG) from an extensive database comprising 1119 subjects.","Previous research on hyperglycemia or glucose detection using ECG has been constrained by challenges related to generalization and scalability, primarily due to using all subjects' ECG in training without considering unseen subjects as a critical factor for developing methods with effective generalization.","We designed a deep neural network model capable of identifying significant features across various spatial locations and examining the interdependencies among different features within each convolutional layer.","To expedite processing speed, we segment the ECG of each user to isolate one heartbeat or one cycle of the ECG.","Our model was trained using data from 727 subjects, while 168 were used for validation.","The testing phase involved 224 unseen subjects, with a dataset consisting of 9,000 segments.","The result indicates that the proposed algorithm effectively detects hyperglycemia with a 91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity."],"url":"http://arxiv.org/abs/2403.07296v1","category":"eess.SP"}
{"created":"2024-03-12 03:54:24","title":"Stability and Sharp Decay Estimates for 3D MHD Equations with Only Vertical Dissipation Near a Background Magnetic Field","abstract":"This paper is concerned with the stability and large-time behavior of 3D incompressible MHD equations with only vertical dissipation near a background magnetic field. By making full use of the dissipation generated by the background magnetic field, we first establish the global stability of the solutions in $H^3$-norm. Then, the optimal decay rates of the solutions are obtained, which are consistent with the 2D classical heat equation. Moreover, some enhanced decay rates of $(u_1,b_1)$ are also achieved. In other words, the decay estimates of the second or third component of velocity/magnetic field coincide with those of 2D heat kernel, while the first component behaves like the 3D heat kernel. This is mainly due to the divergence-free condition and the anisotropic structure. The results obtained improve the previous ones due to Lin-Wu-Zhu [24,25].","sentences":["This paper is concerned with the stability and large-time behavior of 3D incompressible MHD equations with only vertical dissipation near a background magnetic field.","By making full use of the dissipation generated by the background magnetic field, we first establish the global stability of the solutions in $H^3$-norm.","Then, the optimal decay rates of the solutions are obtained, which are consistent with the 2D classical heat equation.","Moreover, some enhanced decay rates of $(u_1,b_1)$ are also achieved.","In other words, the decay estimates of the second or third component of velocity/magnetic field coincide with those of 2D heat kernel, while the first component behaves like the 3D heat kernel.","This is mainly due to the divergence-free condition and the anisotropic structure.","The results obtained improve the previous ones due to Lin-Wu-Zhu","[24,25]."],"url":"http://arxiv.org/abs/2403.07293v1","category":"math.AP"}
{"created":"2024-03-12 03:44:46","title":"Learning Hierarchical Color Guidance for Depth Map Super-Resolution","abstract":"Color information is the most commonly used prior knowledge for depth map super-resolution (DSR), which can provide high-frequency boundary guidance for detail restoration. However, its role and functionality in DSR have not been fully developed. In this paper, we rethink the utilization of color information and propose a hierarchical color guidance network to achieve DSR. On the one hand, the low-level detail embedding module is designed to supplement high-frequency color information of depth features in a residual mask manner at the low-level stages. On the other hand, the high-level abstract guidance module is proposed to maintain semantic consistency in the reconstruction process by using a semantic mask that encodes the global guidance information. The color information of these two dimensions plays a role in the front and back ends of the attention-based feature projection (AFP) module in a more comprehensive form. Simultaneously, the AFP module integrates the multi-scale content enhancement block and adaptive attention projection block to make full use of multi-scale information and adaptively project critical restoration information in an attention manner for DSR. Compared with the state-of-the-art methods on four benchmark datasets, our method achieves more competitive performance both qualitatively and quantitatively.","sentences":["Color information is the most commonly used prior knowledge for depth map super-resolution (DSR), which can provide high-frequency boundary guidance for detail restoration.","However, its role and functionality in DSR have not been fully developed.","In this paper, we rethink the utilization of color information and propose a hierarchical color guidance network to achieve DSR.","On the one hand, the low-level detail embedding module is designed to supplement high-frequency color information of depth features in a residual mask manner at the low-level stages.","On the other hand, the high-level abstract guidance module is proposed to maintain semantic consistency in the reconstruction process by using a semantic mask that encodes the global guidance information.","The color information of these two dimensions plays a role in the front and back ends of the attention-based feature projection (AFP) module in a more comprehensive form.","Simultaneously, the AFP module integrates the multi-scale content enhancement block and adaptive attention projection block to make full use of multi-scale information and adaptively project critical restoration information in an attention manner for DSR.","Compared with the state-of-the-art methods on four benchmark datasets, our method achieves more competitive performance both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2403.07290v1","category":"cs.CV"}
{"created":"2024-03-12 03:41:34","title":"Efficient and Model-Agnostic Parameter Estimation Under Privacy-Preserving Post-randomization Data","abstract":"Protecting individual privacy is crucial when releasing sensitive data for public use. While data de-identification helps, it is not enough. This paper addresses parameter estimation in scenarios where data are perturbed using the Post-Randomization Method (PRAM) to enhance privacy. Existing methods for parameter estimation under PRAM data suffer from limitations like being parameter-specific, model-dependent, and lacking efficiency guarantees. We propose a novel, efficient method that overcomes these limitations. Our method is applicable to general parameters defined through estimating equations and makes no assumptions about the underlying data model. We further prove that the proposed estimator achieves the semiparametric efficiency bound, making it optimal in terms of asymptotic variance.","sentences":["Protecting individual privacy is crucial when releasing sensitive data for public use.","While data de-identification helps, it is not enough.","This paper addresses parameter estimation in scenarios where data are perturbed using the Post-Randomization Method (PRAM) to enhance privacy.","Existing methods for parameter estimation under PRAM data suffer from limitations like being parameter-specific, model-dependent, and lacking efficiency guarantees.","We propose a novel, efficient method that overcomes these limitations.","Our method is applicable to general parameters defined through estimating equations and makes no assumptions about the underlying data model.","We further prove that the proposed estimator achieves the semiparametric efficiency bound, making it optimal in terms of asymptotic variance."],"url":"http://arxiv.org/abs/2403.07288v1","category":"stat.ME"}
{"created":"2024-03-12 03:35:00","title":"Improved Algebraic Inverter Modelling for Four-Wire Power Flow Optimization","abstract":"This paper discusses the modeling of inverters used in distributed energy resources in steady state. Modeling the interaction between distribution grids and inverter-based resources is crucial to understand the consequences for the network's operational and planning processes. This work highlights the limitations of existing models and emphasizes the need for better representations of inverters and their control laws in decision-making contexts. Improved steady-state grid-following and grid-forming inverter models are presented, including both three-leg and four-leg converter variants. The advantages of these improved models in mathematical optimization contexts are showcased by investigating the power quality improvement capabilities of the inverters. Numerical studies integrating the proposed inverter models in a four-wire unbalanced optimal power flow engine are presented, and trade-offs between modeling detail and computational intensity are illustrated.","sentences":["This paper discusses the modeling of inverters used in distributed energy resources in steady state.","Modeling the interaction between distribution grids and inverter-based resources is crucial to understand the consequences for the network's operational and planning processes.","This work highlights the limitations of existing models and emphasizes the need for better representations of inverters and their control laws in decision-making contexts.","Improved steady-state grid-following and grid-forming inverter models are presented, including both three-leg and four-leg converter variants.","The advantages of these improved models in mathematical optimization contexts are showcased by investigating the power quality improvement capabilities of the inverters.","Numerical studies integrating the proposed inverter models in a four-wire unbalanced optimal power flow engine are presented, and trade-offs between modeling detail and computational intensity are illustrated."],"url":"http://arxiv.org/abs/2403.07285v1","category":"math.OC"}
{"created":"2024-03-12 03:34:03","title":"SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection","abstract":"Sparse 3D detectors have received significant attention since the query-based paradigm embraces low latency without explicit dense BEV feature construction. However, these detectors achieve worse performance than their dense counterparts. In this paper, we find the key to bridging the performance gap is to enhance the awareness of rich representations in two modalities. Here, we present a high-performance fully sparse detector for end-to-end multi-modality 3D object detection. The detector, termed SparseLIF, contains three key designs, which are (1) Perspective-Aware Query Generation (PAQG) to generate high-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS) to further refine prior queries by sampling RoI features from each modality, (3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of each sensor modality and adaptively conduct final multi-modality fusion, thus achieving great robustness against sensor noises. By the time of submission (2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes dataset, ranking 1st on both validation set and test benchmark, outperforming all state-of-the-art 3D object detectors by a notable margin. The source code will be released upon acceptance.","sentences":["Sparse 3D detectors have received significant attention since the query-based paradigm embraces low latency without explicit dense BEV feature construction.","However, these detectors achieve worse performance than their dense counterparts.","In this paper, we find the key to bridging the performance gap is to enhance the awareness of rich representations in two modalities.","Here, we present a high-performance fully sparse detector for end-to-end multi-modality 3D object detection.","The detector, termed SparseLIF, contains three key designs, which are (1) Perspective-Aware Query Generation (PAQG) to generate high-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS) to further refine prior queries by sampling RoI features from each modality, (3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of each sensor modality and adaptively conduct final multi-modality fusion, thus achieving great robustness against sensor noises.","By the time of submission (2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes dataset, ranking 1st on both validation set and test benchmark, outperforming all state-of-the-art 3D object detectors by a notable margin.","The source code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.07284v1","category":"cs.CV"}
{"created":"2024-03-12 03:01:31","title":"Error terms for the motives of discriminant complements and a Cayley-Bacharach theorem","abstract":"In this paper we prove under some simplifying hypotheses questions of Picoco and Levinson-Ullery on Cayley-Bacharach sets. Our results imply that, under suitable hypotheses Cayley-Bacharach sets lie on curves of low degree. We then use these results to estimate error terms to the normalized motive of the space of smooth degree $d$ hypersurfaces in $\\mathbb{P}^n$as $d$ grows to infinity. The error term can be expressed in terms of a certain `sum over points' on plane cubic curves and the associated Hodge structure can be expressed in terms of the cohomology of the moduli space of elliptic curves. We also prove convergence of the motive of degree $d$ hypersurfaces in $\\mathbb{P}^n$ as $n$ grows to infinity as well as other results on discriminant complements of high dimensional varieties.","sentences":["In this paper we prove under some simplifying hypotheses questions of Picoco and Levinson-Ullery on Cayley-Bacharach sets.","Our results imply that, under suitable hypotheses Cayley-Bacharach sets lie on curves of low degree.","We then use these results to estimate error terms to the normalized motive of the space of smooth degree $d$ hypersurfaces in $\\mathbb{P}^n$as $d$ grows to infinity.","The error term can be expressed in terms of a certain `sum over points' on plane cubic curves and the associated Hodge structure can be expressed in terms of the cohomology of the moduli space of elliptic curves.","We also prove convergence of the motive of degree $d$ hypersurfaces in $\\mathbb{P}^n$ as $n$ grows to infinity as well as other results on discriminant complements of high dimensional varieties."],"url":"http://arxiv.org/abs/2403.07272v1","category":"math.AG"}
{"created":"2024-03-12 02:56:02","title":"An Exactly Solvable Model for Single-Lane Unidirectional Ant Traffic","abstract":"In 2002, Chowdhury et al. introduced a simplified model aimed at depicting the dynamics of single-lane unidirectional ant traffic. Despite efforts, an exact solution for the stationary state of this ant-trail model remains elusive. The primary challenge arises from inherent fluctuations in the total amount of pheromone along the ant-trail. These fluctuations significantly influence ant movement. Consequently, unlike conventional models involving driven interacting particles, the average velocity exhibits non-monotonic behavior with ant density. In this study, we propose an exactly solvable model for ant traffic whose dynamics is based on the one in Chowdhury et al.'s model, and then discuss the circumstances under which the non-monotonic trend of average velocity arises.","sentences":["In 2002, Chowdhury et al. introduced a simplified model aimed at depicting the dynamics of single-lane unidirectional ant traffic.","Despite efforts, an exact solution for the stationary state of this ant-trail model remains elusive.","The primary challenge arises from inherent fluctuations in the total amount of pheromone along the ant-trail.","These fluctuations significantly influence ant movement.","Consequently, unlike conventional models involving driven interacting particles, the average velocity exhibits non-monotonic behavior with ant density.","In this study, we propose an exactly solvable model for ant traffic whose dynamics is based on the one in Chowdhury et al.'s model, and then discuss the circumstances under which the non-monotonic trend of average velocity arises."],"url":"http://arxiv.org/abs/2403.07267v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 02:47:00","title":"Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization","abstract":"We study the generalization capability of nearly-interpolating linear regressors: $\\boldsymbol{\\beta}$'s whose training error $\\tau$ is positive but small, i.e., below the noise floor. Under a random matrix theoretic assumption on the data distribution and an eigendecay assumption on the data covariance matrix $\\boldsymbol{\\Sigma}$, we demonstrate that any near-interpolator exhibits rapid norm growth: for $\\tau$ fixed, $\\boldsymbol{\\beta}$ has squared $\\ell_2$-norm $\\mathbb{E}[\\|{\\boldsymbol{\\beta}}\\|_{2}^{2}] = \\Omega(n^{\\alpha})$ where $n$ is the number of samples and $\\alpha >1$ is the exponent of the eigendecay, i.e., $\\lambda_i(\\boldsymbol{\\Sigma}) \\sim i^{-\\alpha}$. This implies that existing data-independent norm-based bounds are necessarily loose. On the other hand, in the same regime we precisely characterize the asymptotic trade-off between interpolation and generalization. Our characterization reveals that larger norm scaling exponents $\\alpha$ correspond to worse trade-offs between interpolation and generalization. We verify empirically that a similar phenomenon holds for nearly-interpolating shallow neural networks.","sentences":["We study the generalization capability of nearly-interpolating linear regressors: $\\boldsymbol{\\beta}$'s whose training error $\\tau$ is positive but small, i.e., below the noise floor.","Under a random matrix theoretic assumption on the data distribution and an eigendecay assumption on the data covariance matrix $\\boldsymbol{\\Sigma}$, we demonstrate that any near-interpolator exhibits rapid norm growth: for $\\tau$ fixed, $\\boldsymbol{\\beta}$ has squared $\\ell_2$-norm $\\mathbb{E}[\\|{\\boldsymbol{\\beta}}\\|_{2}^{2}] = \\Omega(n^{\\alpha})$ where $n$ is the number of samples and $\\alpha >1$ is the exponent of the eigendecay, i.e., $\\lambda_i(\\boldsymbol{\\Sigma})","\\sim i^{-\\alpha}$. This implies that existing data-independent norm-based bounds are necessarily loose.","On the other hand, in the same regime we precisely characterize the asymptotic trade-off between interpolation and generalization.","Our characterization reveals that larger norm scaling exponents $\\alpha$ correspond to worse trade-offs between interpolation and generalization.","We verify empirically that a similar phenomenon holds for nearly-interpolating shallow neural networks."],"url":"http://arxiv.org/abs/2403.07264v1","category":"stat.ML"}
{"created":"2024-03-12 02:45:24","title":"Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction","abstract":"Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across sizes. Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with actionably tight predictive uncertainty intervals.","sentences":["Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving.","We consider quantifying such uncertainty for multi-object detection.","In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes.","One challenge in doing so is that bounding box predictions are conditioned on the object's class label.","Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes.","This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required.","Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across sizes.","Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with actionably tight predictive uncertainty intervals."],"url":"http://arxiv.org/abs/2403.07263v1","category":"cs.CV"}
{"created":"2024-03-12 02:24:00","title":"Serre functors and complete torsion pairs","abstract":"Given a torsion pair $(\\mathcal{T},\\mathcal{F})$ in an abelian category $\\mathcal{A}$, there is a t-structure $(\\mathcal{U}_\\mathcal{T},\\mathcal{V}_\\mathcal{T})$ determined by $\\mathcal{T}$ on the derived category $D^b(\\mathcal{A})$. The existence of derived equivalence between heart $\\mathcal{B}$ of the t-structure and $\\mathcal{A}$ which naturally extends the embedding $\\mathcal{B}\\to D^b(\\mathcal{A})$ is determined by the completeness of the torsion pair [6]. When $\\mathcal{A}$ is the module category of a finite-dimensional hereditary algebra and $\\mathcal{U}_\\mathcal{T}$ is closed under Serre functor, then there exists a triangle equivalence $D^b(\\mathcal{B})\\to D^b(\\mathcal{A})$ [21]. In this case, we give a straightforward proof of the fact torsion pair $(\\mathcal{T},\\mathcal{F})$ is complete if and only if $\\mathcal{U}_\\mathcal{T}$ is closed under the Serre functor.","sentences":["Given a torsion pair $(\\mathcal{T},\\mathcal{F})$ in an abelian category $\\mathcal{A}$, there is a t-structure $(\\mathcal{U}_\\mathcal{T},\\mathcal{V}_\\mathcal{T})$ determined by $\\mathcal{T}$ on the derived category $D^b(\\mathcal{A})$. The existence of derived equivalence between heart $\\mathcal{B}$ of the t-structure and $\\mathcal{A}$ which naturally extends the embedding $\\mathcal{B}\\to D^b(\\mathcal{A})$ is determined by the completeness of the torsion pair","[6].","When $\\mathcal{A}$ is the module category of a finite-dimensional hereditary algebra and $\\mathcal{U}_\\mathcal{T}$ is closed under Serre functor, then there exists a triangle equivalence $D^b(\\mathcal{B})\\to D^b(\\mathcal{A})$","[21].","In this case, we give a straightforward proof of the fact torsion pair $(\\mathcal{T},\\mathcal{F})$ is complete if and only if $\\mathcal{U}_\\mathcal{T}$ is closed under the Serre functor."],"url":"http://arxiv.org/abs/2403.07252v1","category":"math.RT"}
{"created":"2024-03-12 02:17:30","title":"Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping","abstract":"Conventional approaches to grasp planning require perfect knowledge of an object's pose and geometry. Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure. Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object. In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or geometry affecting grasp planning before execution. To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp's force closure status. As a result, we show the Ferrari-Canny metric -- which measures the size of external disturbances a grasp can reject -- bounds the friction cone uncertainty a grasp can tolerate, and thus also measures intrinsic robustness. In tandem, we show that the recently proposed min-weight metric lower bounds the Ferrari-Canny metric, justifying it as a computationally-efficient, uncertainty-aware alternative. We validate this theory on hardware experiments versus a competitive baseline and demonstrate superior performance. Finally, we use our theory to develop an analytic notion of probabilistic force closure, which we show in simulation generates grasps that can incorporate uncertainty distributions over an object's geometry.","sentences":["Conventional approaches to grasp planning require perfect knowledge of an object's pose and geometry.","Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure.","Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object.","In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or geometry affecting grasp planning before execution.","To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp's force closure status.","As a result, we show the Ferrari-Canny metric -- which measures the size of external disturbances a grasp can reject -- bounds the friction cone uncertainty a grasp can tolerate, and thus also measures intrinsic robustness.","In tandem, we show that the recently proposed min-weight metric lower bounds the Ferrari-Canny metric, justifying it as a computationally-efficient, uncertainty-aware alternative.","We validate this theory on hardware experiments versus a competitive baseline and demonstrate superior performance.","Finally, we use our theory to develop an analytic notion of probabilistic force closure, which we show in simulation generates grasps that can incorporate uncertainty distributions over an object's geometry."],"url":"http://arxiv.org/abs/2403.07249v1","category":"cs.RO"}
{"created":"2024-03-12 02:13:29","title":"Atomicity and Abstraction for Cross-Blockchain Interactions","abstract":"A blockchain facilitates secure and atomic transactions between mutually untrusting parties on that chain. Today, there are multiple blockchains with differing interfaces and security properties. Programming in this multi-blockchain world is hindered by the lack of general and convenient abstractions for cross-chain communication and computation. Current cross-chain communication bridges have varied and low-level interfaces, making it difficult to develop portable applications. Current methods for multi-chain atomic transactions are limited in scope to cryptocurrency swaps.   This work addresses these issues. We first define a uniform, high-level interface for communication between chains. Building on this interface, we formulate a protocol that guarantees atomicity for general transactions whose operations may span several chains. We formulate and prove the desired correctness and security properties of these protocols. Our prototype implementation is built using the LayerZero cross-chain bridge. Experience with this implementation shows that the new abstractions considerably simplify the design and implementation of multi-chain transactions. Experimental evaluation with multi-chain swap transactions demonstrates performance comparable to that of custom-built implementations.","sentences":["A blockchain facilitates secure and atomic transactions between mutually untrusting parties on that chain.","Today, there are multiple blockchains with differing interfaces and security properties.","Programming in this multi-blockchain world is hindered by the lack of general and convenient abstractions for cross-chain communication and computation.","Current cross-chain communication bridges have varied and low-level interfaces, making it difficult to develop portable applications.","Current methods for multi-chain atomic transactions are limited in scope to cryptocurrency swaps.   ","This work addresses these issues.","We first define a uniform, high-level interface for communication between chains.","Building on this interface, we formulate a protocol that guarantees atomicity for general transactions whose operations may span several chains.","We formulate and prove the desired correctness and security properties of these protocols.","Our prototype implementation is built using the LayerZero cross-chain bridge.","Experience with this implementation shows that the new abstractions considerably simplify the design and implementation of multi-chain transactions.","Experimental evaluation with multi-chain swap transactions demonstrates performance comparable to that of custom-built implementations."],"url":"http://arxiv.org/abs/2403.07248v1","category":"cs.CR"}
{"created":"2024-03-12 02:05:06","title":"Dataset Condensation for Time Series Classification via Dual Domain Matching","abstract":"Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \\textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \\textit{\\textbf{Cond}}ensation for \\textit{\\textbf{T}}ime \\textit{\\textbf{S}}eries \\textit{\\textbf{C}}lassification via Dual Domain Matching (\\textbf{CondTSC}) which focuses on the time series classification dataset condensation task. Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains. Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains. Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data.","sentences":["Time series data has been demonstrated to be crucial in various research fields.","The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network.","Recently, a technique named \\textit{Dataset Condensation} has emerged as a solution to this problem.","This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification.","However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain.","In this paper, we propose a novel framework named Dataset \\textit{\\textbf{Cond}}ensation for \\textit{\\textbf{T}}ime \\textit{\\textbf{S}}eries \\textit{\\textbf{C}}lassification via Dual Domain Matching (\\textbf{CondTSC}) which focuses on the time series classification dataset condensation task.","Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains.","Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains.","Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data."],"url":"http://arxiv.org/abs/2403.07245v1","category":"cs.LG"}
{"created":"2024-03-12 01:21:22","title":"The Primal Pathwidth SETH","abstract":"Motivated by the importance of dynamic programming (DP) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can Dominating Set be solved in time $(3-\\epsilon)^{pw}n^{O(1)}$? (where $pw$ is the pathwidth) (ii) can Coloring be solved in time $pw^{(1-\\epsilon)pw}n^{O(1)}$? (iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-\\epsilon)k}$? Such questions are well-studied: in some cases the answer is No under the SETH, while in others coarse-grained lower bounds are known under the ETH. Even though questions such as the above seem \"morally equivalent\" as they all ask if a simple DP can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential FPT to XNLP-complete.   This paper's main contribution is to show that, despite their varying complexities, these questions are not just morally equivalent, but in fact they are the same question in disguise. We achieve this by putting forth a natural complexity assumption which we call the Primal Pathwidth-Strong Exponential Time Hypothesis (PP-SETH) and which states that 3-SAT cannot be solved in time $(2-\\epsilon)^{pw}n^{O(1)}$, for any $\\epsilon>0$, where $pw$ is the pathwidth of the primal graph of the input. We then show that numerous fine-grained questions in parameterized complexity, including the ones above, are equivalent to the PP-SETH, and hence to each other. This allows us to obtain sharp fine-grained lower bounds for problems for which previous lower bounds left a constant in the exponent undetermined, but also to increase our confidence in bounds which were previously known under the SETH, because we show that breaking any one such bound requires breaking all (old and new) bounds; and because we show that the PP-SETH is more plausible than the SETH.","sentences":["Motivated by the importance of dynamic programming (DP) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can Dominating Set be solved in time $(3-\\epsilon)^{pw}n^{O(1)}$? (where $pw$ is the pathwidth) (ii) can Coloring be solved in time $pw^{(1-\\epsilon)pw}n^{O(1)}$?","(iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-\\epsilon)k}$?","Such questions are well-studied: in some cases the answer is No under the SETH, while in others coarse-grained lower bounds are known under the ETH.","Even though questions such as the above seem \"morally equivalent\" as they all ask if a simple DP can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential FPT to XNLP-complete.   ","This paper's main contribution is to show that, despite their varying complexities, these questions are not just morally equivalent, but in fact they are the same question in disguise.","We achieve this by putting forth a natural complexity assumption which we call the Primal Pathwidth-Strong Exponential Time Hypothesis (PP-SETH) and which states that 3-SAT cannot be solved in time $(2-\\epsilon)^{pw}n^{O(1)}$, for any $\\epsilon>0$, where $pw$ is the pathwidth of the primal graph of the input.","We then show that numerous fine-grained questions in parameterized complexity, including the ones above, are equivalent to the PP-SETH, and hence to each other.","This allows us to obtain sharp fine-grained lower bounds for problems for which previous lower bounds left a constant in the exponent undetermined, but also to increase our confidence in bounds which were previously known under the SETH, because we show that breaking any one such bound requires breaking all (old and new) bounds; and because we show that the PP-SETH is more plausible than the SETH."],"url":"http://arxiv.org/abs/2403.07239v1","category":"cs.CC"}
{"created":"2024-03-12 01:18:03","title":"Cosmological implications from some proposals in Quantum Gravity","abstract":"In this thesis we present the cosmological applications of some proposals from Quantum Gravity. Namely, we will explore classical and quantum cosmological implications of the Generalized Uncertainty Principle (GUP), the Ho\\v{r}ava-Lifshitz (HL) theory of gravity and the Swampland Conjectures. Furthermore, we will also present a detailed analysis of Lorentzian Vacuum Transitions in various contexts.   First of all, we will present a study of the classical implications of a general form of the GUP to the standard inflationary scenario driven by a scalar field. We will then present the study of the compatibility between the dS swampland conjecture and the Ho\\v{r}ava-Lifshitz $F(\\bar{R})$ theories. Then we will study the effects of considering a GUP in the variables of the superspace in a model of the Wheeler-DeWitt (WDW) equation in HL gravity. Finally, we will present a general study of the Lorentzian vacuum transitions. We will present a general method to compute the transition probabilities between two minima of a scalar field potential for any model of the superspace that leads to a generic form of the Hamiltonian constraint, by solving the WDW equation with a semiclassical expansion. We will only consider up to first order in this expansion but the method provides enough information to compute the probabilities up to any desired order. We will then apply the method in different scenarios.   With these studies we will approach keys aspects of cosmology that are hoped to be solved by quantum gravity, that is we will explore (at a toy model level): the origin of inflation, the viability of the swampland conjectures and a plausible model to avoid the initial singularity. This thesis encompasses six research articles where these results were presented originally.","sentences":["In this thesis we present the cosmological applications of some proposals from Quantum Gravity.","Namely, we will explore classical and quantum cosmological implications of the Generalized Uncertainty Principle (GUP), the Ho\\v{r}ava-Lifshitz (HL) theory of gravity and the Swampland Conjectures.","Furthermore, we will also present a detailed analysis of Lorentzian Vacuum Transitions in various contexts.   ","First of all, we will present a study of the classical implications of a general form of the GUP to the standard inflationary scenario driven by a scalar field.","We will then present the study of the compatibility between the dS swampland conjecture and the Ho\\v{r}ava-Lifshitz $F(\\bar{R})$ theories.","Then we will study the effects of considering a GUP in the variables of the superspace in a model of the Wheeler-DeWitt (WDW) equation in HL gravity.","Finally, we will present a general study of the Lorentzian vacuum transitions.","We will present a general method to compute the transition probabilities between two minima of a scalar field potential for any model of the superspace that leads to a generic form of the Hamiltonian constraint, by solving the WDW equation with a semiclassical expansion.","We will only consider up to first order in this expansion but the method provides enough information to compute the probabilities up to any desired order.","We will then apply the method in different scenarios.   ","With these studies we will approach keys aspects of cosmology that are hoped to be solved by quantum gravity, that is we will explore (at a toy model level): the origin of inflation, the viability of the swampland conjectures and a plausible model to avoid the initial singularity.","This thesis encompasses six research articles where these results were presented originally."],"url":"http://arxiv.org/abs/2403.07237v1","category":"gr-qc"}
{"created":"2024-03-12 01:13:25","title":"Hessian estimates for shrinkers, expanders, translators, and rotators of the Lagrangian Mean Curvature Flow","abstract":"In this paper, we prove interior Hessian estimates for shrinkers, expanders, translators, and rotators of the Lagrangian mean curvature flow under the assumption that the Lagrangian phase is hypercritical. We further extend our results to a broader class of Lagrangian mean curvature type equations.","sentences":["In this paper, we prove interior Hessian estimates for shrinkers, expanders, translators, and rotators of the Lagrangian mean curvature flow under the assumption that the Lagrangian phase is hypercritical.","We further extend our results to a broader class of Lagrangian mean curvature type equations."],"url":"http://arxiv.org/abs/2403.07235v1","category":"math.AP"}
{"created":"2024-03-12 00:26:16","title":"LookupFFN: Making Transformers Compute-lite for CPU inference","abstract":"While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it in abundance). For RoBERTa language model pretraining, our formulation achieves similar performance compared to GEMM based FFNs, while dramatically reducing the required FLOP. Our development is complemented with a detailed hardware profiling of strategies that will maximize efficiency -- not just on contemporary hardware but on products that will be offered in the near/medium term future. Code is avaiable at \\url{https://github.com/mlpen/LookupFFN}.","sentences":["While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry.","But the imbalance between the compute capabilities of GPUs and CPUs is huge.","Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite.","Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs.","Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it in abundance).","For RoBERTa language model pretraining, our formulation achieves similar performance compared to GEMM based FFNs, while dramatically reducing the required FLOP.","Our development is complemented with a detailed hardware profiling of strategies that will maximize efficiency -- not just on contemporary hardware but on products that will be offered in the near/medium term future.","Code is avaiable at \\url{https://github.com/mlpen/LookupFFN}."],"url":"http://arxiv.org/abs/2403.07221v1","category":"cs.LG"}
{"created":"2024-03-12 00:26:13","title":"ACMI: An index for exposed coal mapping using Landsat imagery","abstract":"Remotely sensing the spatial distribution of exposed coal (EC) is significant for understanding the footprints of mining activities. However, widely applicable methods for the identification of EC surfaces remain inadequate because the choices of recent methods confront the diverse EC types and backgrounds. Therefore, this study proposed a new Automated Coal Mapping Index (ACMI) which was empirically formulated by an iterative process of identifying parameters that maximize the separability of EC and non-EC surfaces. The performance of ACMI was tested in six study areas worldwide with different landscape types and coal types. Based on the visual inspection, ACMI was more effective in highlighting EC surfaces and suppressing non-EC surfaces than the existing methods. Compared with the sample points obtained through direct interpretation, ACMI obtained better EC mapping results than previous methods with the F1 score and overall accuracy (OA) no less than 0.91 and 93.20% across all the selected Landsat images of the study areas, respectively. In addition, ACMI was demonstrated to have a stable optimal threshold and 0 can serve as its default threshold. The default threshold makes EC mapping using ACMI an automated process. The new index has the potential to support a variety of mining-activity-related studies, such as the identification of mining disturbances and illegal mining detection at multi-spatial-temporal scales.","sentences":["Remotely sensing the spatial distribution of exposed coal (EC) is significant for understanding the footprints of mining activities.","However, widely applicable methods for the identification of EC surfaces remain inadequate because the choices of recent methods confront the diverse EC types and backgrounds.","Therefore, this study proposed a new Automated Coal Mapping Index (ACMI) which was empirically formulated by an iterative process of identifying parameters that maximize the separability of EC and non-EC surfaces.","The performance of ACMI was tested in six study areas worldwide with different landscape types and coal types.","Based on the visual inspection, ACMI was more effective in highlighting EC surfaces and suppressing non-EC surfaces than the existing methods.","Compared with the sample points obtained through direct interpretation, ACMI obtained better EC mapping results than previous methods with the F1 score and overall accuracy (OA) no less than 0.91 and 93.20% across all the selected Landsat images of the study areas, respectively.","In addition, ACMI was demonstrated to have a stable optimal threshold and 0 can serve as its default threshold.","The default threshold makes EC mapping using ACMI an automated process.","The new index has the potential to support a variety of mining-activity-related studies, such as the identification of mining disturbances and illegal mining detection at multi-spatial-temporal scales."],"url":"http://arxiv.org/abs/2403.07220v1","category":"eess.IV"}
{"created":"2024-03-12 00:25:14","title":"SoK: Can Trajectory Generation Combine Privacy and Utility?","abstract":"While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing trajectory protection approaches, emphasising their shortcomings. This work focuses on the systematisation of the state-of-the-art generative models for trajectories in the context of the proposed framework. We find that no existing solution satisfies all requirements. Thus, we perform an experimental study evaluating the applicability of six sequential generative models to the trajectory domain. Finally, we conclude that a generative trajectory model providing semantic guarantees remains an open research question and propose concrete next steps for future research.","sentences":["While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences.","Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees.","However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks.","Synthetic trajectory data generation and release represent a promising alternative to protection algorithms.","While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees.","This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy.","Based on this framework, we briefly discuss the existing trajectory protection approaches, emphasising their shortcomings.","This work focuses on the systematisation of the state-of-the-art generative models for trajectories in the context of the proposed framework.","We find that no existing solution satisfies all requirements.","Thus, we perform an experimental study evaluating the applicability of six sequential generative models to the trajectory domain.","Finally, we conclude that a generative trajectory model providing semantic guarantees remains an open research question and propose concrete next steps for future research."],"url":"http://arxiv.org/abs/2403.07218v1","category":"cs.CR"}
{"created":"2024-03-12 00:14:49","title":"Arrow Relations in Lattices of Integer Partitions","abstract":"We give a complete characterisation of the single and double arrow relations of the standard context $K(L_n)$ of the lattice $L_n$ of partitions of any positive integer $n$ under the dominance order, thereby addressing an open question of Ganter, 2022.","sentences":["We give a complete characterisation of the single and double arrow relations of the standard context $K(L_n)$ of the lattice $L_n$ of partitions of any positive integer $n$ under the dominance order, thereby addressing an open question of Ganter, 2022."],"url":"http://arxiv.org/abs/2403.07217v1","category":"math.CO"}
{"created":"2024-03-11 23:52:46","title":"Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits","abstract":"Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections.   In this paper, we propose a time-increasing bandit algorithm TI-UCB, which effectively predicts the increase of model performances due to finetuning and efficiently balances exploration and exploitation in model selection. To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions. We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing bandit setting, which implies a fast convergence rate. The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of LLMs. Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of LLMs.","sentences":["Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs.","Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost.","Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance.","Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs.","Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models.","While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections.   ","In this paper, we propose a time-increasing bandit algorithm TI-UCB, which effectively predicts the increase of model performances due to finetuning and efficiently balances exploration and exploitation in model selection.","To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions.","We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing bandit setting, which implies a fast convergence rate.","The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of LLMs.","Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of LLMs."],"url":"http://arxiv.org/abs/2403.07213v1","category":"cs.LG"}
{"created":"2024-03-11 22:58:58","title":"SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser","abstract":"Structural priming is a widely used psycholinguistic paradigm to study human sentence representations. In this work we propose a framework for using empirical priming patterns to build a theory characterizing the structural representations humans construct when processing sentences. This framework uses a new cognitively motivated parser, SPAWN, to generate quantitative priming predictions from theoretical syntax and evaluate these predictions with empirical human behavior. As a case study, we apply this framework to study reduced relative clause representations in English. We use SPAWN to generate priming predictions from two theoretical accounts which make different assumptions about the structure of relative clauses. We find that the predictions from only one of these theories (Participial-Phase) align with empirical priming patterns, thus highlighting which assumptions about relative clause better capture human sentence representations.","sentences":["Structural priming is a widely used psycholinguistic paradigm to study human sentence representations.","In this work we propose a framework for using empirical priming patterns to build a theory characterizing the structural representations humans construct when processing sentences.","This framework uses a new cognitively motivated parser, SPAWN, to generate quantitative priming predictions from theoretical syntax and evaluate these predictions with empirical human behavior.","As a case study, we apply this framework to study reduced relative clause representations in English.","We use SPAWN to generate priming predictions from two theoretical accounts which make different assumptions about the structure of relative clauses.","We find that the predictions from only one of these theories (Participial-Phase) align with empirical priming patterns, thus highlighting which assumptions about relative clause better capture human sentence representations."],"url":"http://arxiv.org/abs/2403.07202v1","category":"cs.CL"}
{"created":"2024-03-11 22:40:15","title":"Simulating Quantum Circuits by Model Counting","abstract":"Quantum circuit compilation comprises many computationally hard reasoning tasks that nonetheless lie inside #$\\mathbf{P}$ and its decision counterpart in $\\mathbf{PP}$. The classical simulation of general quantum circuits is a core example. We show for the first time that a strong simulation of universal quantum circuits can be efficiently tackled through weighted model counting by providing a linear encoding of Clifford+T circuits. To achieve this, we exploit the stabilizer formalism by Knill, Gottesmann, and Aaronson and the fact that stabilizer states form a basis for density operators. With an open-source simulator implementation, we demonstrate empirically that model counting often outperforms state-of-the-art simulation techniques based on the ZX calculus and decision diagrams. Our work paves the way to apply the existing array of powerful classical reasoning tools to realize efficient quantum circuit compilation; one of the obstacles on the road towards quantum supremacy.","sentences":["Quantum circuit compilation comprises many computationally hard reasoning tasks that nonetheless lie inside #$\\mathbf{P}$ and its decision counterpart in $\\mathbf{PP}$. The classical simulation of general quantum circuits is a core example.","We show for the first time that a strong simulation of universal quantum circuits can be efficiently tackled through weighted model counting by providing a linear encoding of Clifford+T circuits.","To achieve this, we exploit the stabilizer formalism by Knill, Gottesmann, and Aaronson and the fact that stabilizer states form a basis for density operators.","With an open-source simulator implementation, we demonstrate empirically that model counting often outperforms state-of-the-art simulation techniques based on the ZX calculus and decision diagrams.","Our work paves the way to apply the existing array of powerful classical reasoning tools to realize efficient quantum circuit compilation; one of the obstacles on the road towards quantum supremacy."],"url":"http://arxiv.org/abs/2403.07197v1","category":"quant-ph"}
{"created":"2024-03-11 22:23:20","title":"Exploring gender differences in the Force Concept Inventory using a random effects meta-analysis of international studies","abstract":"The force concept inventory (FCI) is one of the research-based assessments (RBAs) established by the physics education research (PER) community to measure students' understanding of Newtonian mechanics. Former works have often recorded the notion of gendered mean FCI scores favoring male students notably in the North America (NA) based studies. Nevertheless, these performance gaps remain inconclusive and unexplored outside the NA context. This paper aims to fill this gap by meta-analyzing the mean FCI scores between gender based on the existing PER literature beyond the NA context. We analyzed the magnitude and direction on the mean FCI scores between gender on the basis of primary international studies published over the last two decades. We also explored the moderating impact of international study characteristics on the meta-analytic findings by performing a subgroup analysis to study the different study regions stratified by two subgroups (NA vs non-NA authors). Thirty-eight studies reporting the mean FCI scores by gender were included in the present meta-analysis. We employed Hedges' g statistic to estimate to what degree the mean FCI scores may be different between male and female students on each study. Under a random effects model, we meta-analyzed the findings and conducted a subgroup analysis to answer the research questions. In summary, our meta-analysis indicated a significantly positive and moderate amount of gendered mean FCI scores in favor of male students both in NA- and non-NA based regions, and the performance gaps were wider in the NA-based studies. Suggestions are discussed for promoting gender fairness in the FCI when interpreting its scores for teaching, learning, and forthcoming studies.","sentences":["The force concept inventory (FCI) is one of the research-based assessments (RBAs) established by the physics education research (PER) community to measure students' understanding of Newtonian mechanics.","Former works have often recorded the notion of gendered mean FCI scores favoring male students notably in the North America (NA) based studies.","Nevertheless, these performance gaps remain inconclusive and unexplored outside the NA context.","This paper aims to fill this gap by meta-analyzing the mean FCI scores between gender based on the existing PER literature beyond the NA context.","We analyzed the magnitude and direction on the mean FCI scores between gender on the basis of primary international studies published over the last two decades.","We also explored the moderating impact of international study characteristics on the meta-analytic findings by performing a subgroup analysis to study the different study regions stratified by two subgroups (NA vs non-NA authors).","Thirty-eight studies reporting the mean FCI scores by gender were included in the present meta-analysis.","We employed Hedges' g statistic to estimate to what degree the mean FCI scores may be different between male and female students on each study.","Under a random effects model, we meta-analyzed the findings and conducted a subgroup analysis to answer the research questions.","In summary, our meta-analysis indicated a significantly positive and moderate amount of gendered mean FCI scores in favor of male students both in NA- and non-NA based regions, and the performance gaps were wider in the NA-based studies.","Suggestions are discussed for promoting gender fairness in the FCI when interpreting its scores for teaching, learning, and forthcoming studies."],"url":"http://arxiv.org/abs/2403.07190v1","category":"physics.ed-ph"}
{"created":"2024-03-11 21:54:52","title":"Uncertainty in Graph Neural Networks: A Survey","abstract":"Graph Neural Networks (GNNs) have been extensively used in various real-world applications. However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions. Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the GNN predictions. This survey aims to provide a comprehensive overview of the GNNs from the perspective of uncertainty with an emphasis on its integration in graph learning. We compare and summarize existing graph uncertainty theory and methods, alongside the corresponding downstream tasks. Thereby, we bridge the gap between theory and practice, meanwhile connecting different GNN communities. Moreover, our work provides valuable insights into promising directions in this field.","sentences":["Graph Neural Networks (GNNs) have been extensively used in various real-world applications.","However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions.","Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the GNN predictions.","This survey aims to provide a comprehensive overview of the GNNs from the perspective of uncertainty with an emphasis on its integration in graph learning.","We compare and summarize existing graph uncertainty theory and methods, alongside the corresponding downstream tasks.","Thereby, we bridge the gap between theory and practice, meanwhile connecting different GNN communities.","Moreover, our work provides valuable insights into promising directions in this field."],"url":"http://arxiv.org/abs/2403.07185v1","category":"cs.LG"}
{"created":"2024-03-11 20:35:52","title":"Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities","abstract":"The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks. However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them. Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees. In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG. In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in the classical with-replacement SEG. As a byproduct of our results, we provide convergence guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the algorithm) and the Incremental Extragradient (does not shuffle the data). We supplement our analysis with experiments validating empirically the superior performance of SEG-RR over the classical with-replacement sampling SEG.","sentences":["The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks.","However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them.","Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees.","In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone.","We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG.","In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in the classical with-replacement SEG.","As a byproduct of our results, we provide convergence guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the algorithm) and the Incremental Extragradient (does not shuffle the data).","We supplement our analysis with experiments validating empirically the superior performance of SEG-RR over the classical with-replacement sampling SEG."],"url":"http://arxiv.org/abs/2403.07148v1","category":"math.OC"}
{"created":"2024-03-11 20:31:27","title":"Two novel pure-state coherence measures in quantifying coherence","abstract":"In the resource theory of coherence, the quantification of quantum-state coherence is an important task. In this regard, the key ingredients are the various coherence monotones (or measures). There are few coherence-monotone classes that solely depend on other coherence measures defined for all the pure states; in other words, they rely on the pure state coherence measures (PSCM). Here, we set forth two such novel PSCMs, and validate each of them through the fulfillment of all four necessary conditions. In addition, we delve into the most recent (as per our knowledge) coherence-monotone class based on the innovative idea of quantifying coherence in terms of pure-state coherence, further redefine it, and, through the study of convexity under mixing, justify why this coherence monotone class cannot be treated as a coherence-measure class in general.","sentences":["In the resource theory of coherence, the quantification of quantum-state coherence is an important task.","In this regard, the key ingredients are the various coherence monotones (or measures).","There are few coherence-monotone classes that solely depend on other coherence measures defined for all the pure states; in other words, they rely on the pure state coherence measures (PSCM).","Here, we set forth two such novel PSCMs, and validate each of them through the fulfillment of all four necessary conditions.","In addition, we delve into the most recent (as per our knowledge) coherence-monotone class based on the innovative idea of quantifying coherence in terms of pure-state coherence, further redefine it, and, through the study of convexity under mixing, justify why this coherence monotone class cannot be treated as a coherence-measure class in general."],"url":"http://arxiv.org/abs/2403.07146v1","category":"quant-ph"}
{"created":"2024-03-11 20:28:23","title":"New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production","abstract":"This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions).   I study three different settings when the principal contracts with a $\\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic-regret mechanism. Also, I identify a connection between linear contracts and posted-price auctions, showing the two can be reduced to one another, and give a regret lower bound on learning the optimal linear contract based on this observation.   I also study a $\\textit{team production}$ model. I identify a condition under which the principal's learning problem can be reformulated as solving a family of convex programs, thereby showing the optimal contract can be found efficiently.","sentences":["This work studies the repeated principal-agent problem from an online learning perspective.","The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions).   ","I study three different settings when the principal contracts with a $\\textit{single}$ agent each round: 1.","The agents are heterogeneous;","2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic.","I present different approaches and techniques for designing learning algorithms in each setting.","For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly.","For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory.","For strategic non-myopic agents, I design a low strategic-regret mechanism.","Also, I identify a connection between linear contracts and posted-price auctions, showing the two can be reduced to one another, and give a regret lower bound on learning the optimal linear contract based on this observation.   ","I also study a $\\textit{team production}$ model.","I identify a condition under which the principal's learning problem can be reformulated as solving a family of convex programs, thereby showing the optimal contract can be found efficiently."],"url":"http://arxiv.org/abs/2403.07143v1","category":"cs.GT"}
{"created":"2024-03-11 20:04:03","title":"COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization","abstract":"Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding operations. We update these variables in a carefully designed greedy order, significantly enhancing the accuracy. COMQ achieves remarkable results in quantizing 4-bit Vision Transformers, with a negligible loss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of convolutional neural networks, COMQ maintains near-lossless accuracy with a minimal drop of merely 0.3% in Top-1 accuracy.","sentences":["Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment.","However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge.","In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors.","We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code.","Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error.","Every iteration improves this error along a single coordinate while keeping all other variables constant.","COMQ is easy to use and requires no hyper-parameter tuning.","It instead involves only dot products and rounding operations.","We update these variables in a carefully designed greedy order, significantly enhancing the accuracy.","COMQ achieves remarkable results in quantizing 4-bit Vision Transformers, with a negligible loss of less than 1% in Top-1 accuracy.","In 4-bit INT quantization of convolutional neural networks, COMQ maintains near-lossless accuracy with a minimal drop of merely 0.3% in Top-1 accuracy."],"url":"http://arxiv.org/abs/2403.07134v1","category":"cs.LG"}
{"created":"2024-03-11 19:52:00","title":"RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning","abstract":"The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. Accordingly, this paper introduces RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that depend on predefined racing lines, RaceMOP operates without a map, relying solely on local observations to overtake other race cars at high speed. Our approach combines an artificial potential field method as a base policy with residual policy learning to introduce long-horizon planning capabilities. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Our experiments for twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners while generalizing to unknown racetracks, paving the way for further use of our method in robotics. We make the open-source code for RaceMOP available at http://github.com/raphajaner/racemop.","sentences":["The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars.","Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon.","Accordingly, this paper introduces RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars.","Unlike classical planners that depend on predefined racing lines, RaceMOP operates without a map, relying solely on local observations to overtake other race cars at high speed.","Our approach combines an artificial potential field method as a base policy with residual policy learning to introduce long-horizon planning capabilities.","We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space.","Our experiments for twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers.","RaceMOP demonstrates superior handling over existing mapless planners while generalizing to unknown racetracks, paving the way for further use of our method in robotics.","We make the open-source code for RaceMOP available at http://github.com/raphajaner/racemop."],"url":"http://arxiv.org/abs/2403.07129v1","category":"cs.RO"}
{"created":"2024-03-11 19:39:29","title":"Stochastic gradient descent-based inference for dynamic network models with attractors","abstract":"In Coevolving Latent Space Networks with Attractors (CLSNA) models, nodes in a latent space represent social actors, and edges indicate their dynamic interactions. Attractors are added at the latent level to capture the notion of attractive and repulsive forces between nodes, borrowing from dynamical systems theory. However, CLSNA reliance on MCMC estimation makes scaling difficult, and the requirement for nodes to be present throughout the study period limit practical applications. We address these issues by (i) introducing a Stochastic gradient descent (SGD) parameter estimation method, (ii) developing a novel approach for uncertainty quantification using SGD, and (iii) extending the model to allow nodes to join and leave over time. Simulation results show that our extensions result in little loss of accuracy compared to MCMC, but can scale to much larger networks. We apply our approach to the longitudinal social networks of members of US Congress on the social media platform X. Accounting for node dynamics overcomes selection bias in the network and uncovers uniquely and increasingly repulsive forces within the Republican Party.","sentences":["In Coevolving Latent Space Networks with Attractors (CLSNA) models, nodes in a latent space represent social actors, and edges indicate their dynamic interactions.","Attractors are added at the latent level to capture the notion of attractive and repulsive forces between nodes, borrowing from dynamical systems theory.","However, CLSNA reliance on MCMC estimation makes scaling difficult, and the requirement for nodes to be present throughout the study period limit practical applications.","We address these issues by (i) introducing a Stochastic gradient descent (SGD) parameter estimation method, (ii) developing a novel approach for uncertainty quantification using SGD, and (iii) extending the model to allow nodes to join and leave over time.","Simulation results show that our extensions result in little loss of accuracy compared to MCMC, but can scale to much larger networks.","We apply our approach to the longitudinal social networks of members of US Congress on the social media platform X. Accounting for node dynamics overcomes selection bias in the network and uncovers uniquely and increasingly repulsive forces within the Republican Party."],"url":"http://arxiv.org/abs/2403.07124v1","category":"stat.ME"}
{"created":"2024-03-11 19:19:59","title":"Narrating Causal Graphs with Large Language Models","abstract":"The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset.","sentences":["The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts.","In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges.","The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing.","Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings.","Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings.","Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset."],"url":"http://arxiv.org/abs/2403.07118v1","category":"cs.CL"}
{"created":"2024-03-11 19:12:50","title":"Operator size growth in Lindbladian SYK","abstract":"We investigate the growth of operator size in the Lindbladian SYK model with $q$-body interaction terms and linear jump terms at finite dissipation strength. We compute the operator size as well as its distribution numerically at finite $q$ and analytically at large $q$. With dissipative (productive) jump terms, the size converges to a value smaller (larger) than half the number of Majorana fermions. At weak dissipation, the evolution of operator size displays a quadratic-exponential-plateau behavior. The plateau value is determined by the ratios between the coupling of the interaction and the linear jump term in the large $q$ limit. The operator size distribution remains localized in the finite size region even at late times, contrasting with the unitary case. Moreover, we also derived the time-independent orthogonal basis for operator expansion which exhibits the ``operator size concentration'' at finite dissipation. Finally, we observe that the uncertainty relation for operator size growth is saturated at large $q$, leading to a classical dynamics of the operator size growth with dissipation.","sentences":["We investigate the growth of operator size in the Lindbladian SYK model with $q$-body interaction terms and linear jump terms at finite dissipation strength.","We compute the operator size as well as its distribution numerically at finite $q$ and analytically at large $q$. With dissipative (productive) jump terms, the size converges to a value smaller (larger) than half the number of Majorana fermions.","At weak dissipation, the evolution of operator size displays a quadratic-exponential-plateau behavior.","The plateau value is determined by the ratios between the coupling of the interaction and the linear jump term in the large $q$ limit.","The operator size distribution remains localized in the finite size region even at late times, contrasting with the unitary case.","Moreover, we also derived the time-independent orthogonal basis for operator expansion which exhibits the ``operator size concentration'' at finite dissipation.","Finally, we observe that the uncertainty relation for operator size growth is saturated at large $q$, leading to a classical dynamics of the operator size growth with dissipation."],"url":"http://arxiv.org/abs/2403.07115v1","category":"hep-th"}
{"created":"2024-03-11 19:06:04","title":"Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies","abstract":"Object detection, a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance. This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors. This study introduces a benchmarking framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance. We crafted a novel 10-class long-tailed dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common real-world detection scenarios with a limited number of object classes. Against this backdrop, we scrutinized three established techniques: sampling, loss weighing, and data augmentation. Our comparative analysis reveals that sampling and loss reweighing methods, while shown to be beneficial in two-stage detector settings, do not translate as effectively in improving YOLOv5's performance on the COCO-ZIPF dataset. On the other hand, data augmentation methods, specifically mosaic and mixup, significantly enhance the model's mean Average Precision (mAP), by introducing more variability and complexity into the training data. (Code available: https://github.com/craston/object_detection_cib)","sentences":["Object detection, a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance.","This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors.","This study introduces a benchmarking framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance.","We crafted a novel 10-class long-tailed dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common real-world detection scenarios with a limited number of object classes.","Against this backdrop, we scrutinized three established techniques: sampling, loss weighing, and data augmentation.","Our comparative analysis reveals that sampling and loss reweighing methods, while shown to be beneficial in two-stage detector settings, do not translate as effectively in improving YOLOv5's performance on the COCO-ZIPF dataset.","On the other hand, data augmentation methods, specifically mosaic and mixup, significantly enhance the model's mean Average Precision (mAP), by introducing more variability and complexity into the training data.","(Code available: https://github.com/craston/object_detection_cib)"],"url":"http://arxiv.org/abs/2403.07113v1","category":"cs.CV"}
{"created":"2024-03-11 19:05:59","title":"Parameterized Task Graph Scheduling Algorithm for Comparing Algorithmic Components","abstract":"Scheduling distributed applications modeled as directed, acyclic task graphs to run on heterogeneous compute networks is a fundamental (NP-Hard) problem in distributed computing for which many heuristic algorithms have been proposed over the past decades. Many of these algorithms fall under the list-scheduling paradigm, whereby the algorithm first computes priorities for the tasks and then schedules them greedily to the compute node that minimizes some cost function. Thus, many algorithms differ from each other only in a few key components (e.g., the way they prioritize tasks, their cost functions, where the algorithms consider inserting tasks into a partially complete schedule, etc.). In this paper, we propose a generalized parametric list-scheduling algorithm that allows mixing and matching different algorithmic components to produce 72 unique algorithms. We benchmark these algorithms on four datasets to study the individual and combined effects of different algorithmic components on performance and runtime.","sentences":["Scheduling distributed applications modeled as directed, acyclic task graphs to run on heterogeneous compute networks is a fundamental (NP-Hard) problem in distributed computing for which many heuristic algorithms have been proposed over the past decades.","Many of these algorithms fall under the list-scheduling paradigm, whereby the algorithm first computes priorities for the tasks and then schedules them greedily to the compute node that minimizes some cost function.","Thus, many algorithms differ from each other only in a few key components (e.g., the way they prioritize tasks, their cost functions, where the algorithms consider inserting tasks into a partially complete schedule, etc.).","In this paper, we propose a generalized parametric list-scheduling algorithm that allows mixing and matching different algorithmic components to produce 72 unique algorithms.","We benchmark these algorithms on four datasets to study the individual and combined effects of different algorithmic components on performance and runtime."],"url":"http://arxiv.org/abs/2403.07112v1","category":"cs.DC"}
{"created":"2024-03-11 19:00:06","title":"Many-Body Localization in the Age of Classical Computing","abstract":"Statistical mechanics provides a framework for describing the physics of large, complex many-body systems using only a few macroscopic parameters to determine the state of the system. For isolated quantum many-body systems, such a description is achieved via the eigenstate thermalization hypothesis (ETH), which links thermalization, ergodicity and quantum chaotic behavior. However, tendency towards thermalization is not observed at finite system sizes and evolution times in a robust many-body localization (MBL) regime found numerically and experimentally in the dynamics of interacting many-body systems at strong disorder. Although the phenomenology of the MBL regime is well-established, the central question remains unanswered: under what conditions does the MBL regime give rise to an MBL phase in which the thermalization does not occur even in the asymptotic limit of infinite system size and evolution time?   This review focuses on recent numerical investigations aiming to clarify the status of the MBL phase, and it establishes the critical open questions about the dynamics of disordered many-body systems. Persistent finite size drifts towards ergodicity consistently emerge in spectral properties of disordered many-body systems, excluding naive single-parameter scaling hypothesis and preventing comprehension of the status of the MBL phase. The drifts are related to tendencies towards thermalization and non-vanishing transport observed in the dynamics of many-body systems, even at strong disorder. These phenomena impede understanding of microscopic processes at the ETH-MBL crossover. Nevertheless, the abrupt slowdown of dynamics with increasing disorder strength suggests the proximity of the MBL phase. This review concludes that the questions about thermalization and its failure in disordered many-body systems remain a captivating area open for further explorations.","sentences":["Statistical mechanics provides a framework for describing the physics of large, complex many-body systems using only a few macroscopic parameters to determine the state of the system.","For isolated quantum many-body systems, such a description is achieved via the eigenstate thermalization hypothesis (ETH), which links thermalization, ergodicity and quantum chaotic behavior.","However, tendency towards thermalization is not observed at finite system sizes and evolution times in a robust many-body localization (MBL) regime found numerically and experimentally in the dynamics of interacting many-body systems at strong disorder.","Although the phenomenology of the MBL regime is well-established, the central question remains unanswered: under what conditions does the MBL regime give rise to an MBL phase in which the thermalization does not occur even in the asymptotic limit of infinite system size and evolution time?   ","This review focuses on recent numerical investigations aiming to clarify the status of the MBL phase, and it establishes the critical open questions about the dynamics of disordered many-body systems.","Persistent finite size drifts towards ergodicity consistently emerge in spectral properties of disordered many-body systems, excluding naive single-parameter scaling hypothesis and preventing comprehension of the status of the MBL phase.","The drifts are related to tendencies towards thermalization and non-vanishing transport observed in the dynamics of many-body systems, even at strong disorder.","These phenomena impede understanding of microscopic processes at the ETH-MBL crossover.","Nevertheless, the abrupt slowdown of dynamics with increasing disorder strength suggests the proximity of the MBL phase.","This review concludes that the questions about thermalization and its failure in disordered many-body systems remain a captivating area open for further explorations."],"url":"http://arxiv.org/abs/2403.07111v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-11 18:52:46","title":"Advanced-Step Real-time Iterations with Four Levels -- New Error Bounds and Fast Implementation in acados","abstract":"The Real-Time Iteration (RTI) is an online nonlinear model predictive control algorithm that performs a single Sequential Quadratic Programming (SQP) per sampling time. The algorithm is split into a preparation and a feedback phase, where the latter one performs as little computations as possible solving a single prepared quadratic program. To further improve the accuracy of this method, the Advanced-Step RTI (AS-RTI) performs additional Multi-Level Iterations (MLI) in the preparation phase, such as inexact or zero-order SQP iterations on a problem with a predicted state estimate. This paper extends and streamlines the existing analysis of AS-RTI, such as analyzing MLI of level A and B for the first time, and significantly simplifying the proofs for levels C and D. Moreover, this paper provides an efficient open-source implementation in acados, making it widely accessible to practitioners.","sentences":["The Real-Time Iteration (RTI) is an online nonlinear model predictive control algorithm that performs a single Sequential Quadratic Programming (SQP) per sampling time.","The algorithm is split into a preparation and a feedback phase, where the latter one performs as little computations as possible solving a single prepared quadratic program.","To further improve the accuracy of this method, the Advanced-Step RTI (AS-RTI) performs additional Multi-Level Iterations (MLI) in the preparation phase, such as inexact or zero-order SQP iterations on a problem with a predicted state estimate.","This paper extends and streamlines the existing analysis of AS-RTI, such as analyzing MLI of level A and B for the first time, and significantly simplifying the proofs for levels C and D. Moreover, this paper provides an efficient open-source implementation in acados, making it widely accessible to practitioners."],"url":"http://arxiv.org/abs/2403.07101v1","category":"math.OC"}
{"created":"2024-03-11 18:47:16","title":"Rate-independent continuous inhibitory chemical reaction networks are Turing-universal","abstract":"We study the model of continuous chemical reaction networks (CRNs), consisting of reactions such as $A+B \\to C+D$ that can transform some continuous, nonnegative real-valued quantity (called a *concentration*) of chemical species $A$ and $B$ into equal concentrations of $C$ and $D$. Such a reaction can occur from any state in which both reactants $A$ and $B$ are present, i.e., have positive concentration. We modify the model to allow *inhibitors*, for instance, reaction $A+B \\to^{I} C+D$ can occur only if the reactants $A$ and $B$ are present and the inhibitor $I$ is absent. The computational power of non-inhibitory CRNs has been studied. For instance, the reaction $X_1+X_2 \\to Y$ can be thought to compute the function $f(x_1,x_2) = \\min(x_1,x_2)$. Under an \"adversarial\" model in which reaction rates can vary arbitrarily over time, it was found that exactly the continuous, piecewise linear functions can be computed, ruling out even simple functions such as $f(x) = x^2$. In contrast, in this paper we show that inhibitory CRNs can compute any computable function $f:\\mathbb{N}\\to\\mathbb{N}$.","sentences":["We study the model of continuous chemical reaction networks (CRNs), consisting of reactions such as $A+B \\to C+D$ that can transform some continuous, nonnegative real-valued quantity (called a *concentration*) of chemical species $A$ and $B$ into equal concentrations of $C$ and $D$. Such a reaction can occur from any state in which both reactants $A$ and $B$ are present, i.e., have positive concentration.","We modify the model to allow *inhibitors*, for instance, reaction $A+B \\to^{I} C+D$ can occur only if the reactants $A$ and $B$ are present and the inhibitor $I$ is absent.","The computational power of non-inhibitory CRNs has been studied.","For instance, the reaction $X_1+X_2 \\to Y$ can be thought to compute the function $f(x_1,x_2) = \\min(x_1,x_2)$. Under an \"adversarial\" model in which reaction rates can vary arbitrarily over time, it was found that exactly the continuous, piecewise linear functions can be computed, ruling out even simple functions such as $f(x) = x^2$.","In contrast, in this paper we show that inhibitory CRNs can compute any computable function $f:\\mathbb{N}\\to\\mathbb{N}$."],"url":"http://arxiv.org/abs/2403.07099v1","category":"cs.ET"}
{"created":"2024-03-11 18:40:47","title":"FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning","abstract":"The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization framework which makes use of our ILP solver. Using problem structure (e.g., the low-rank structure of approx. Hessian), we can address instances with millions of parameters. Our experiments demonstrate that FALCON achieves superior accuracy compared to other pruning approaches within a fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs retained, our approach improves the accuracy by 48% relative to state-of-the-art. Furthermore, in gradual pruning settings with re-training between pruning steps, our framework outperforms existing pruning methods, emphasizing the significance of incorporating both FLOP and sparsity constraints for effective network pruning.","sentences":["The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices.","Network pruning offers a solution to reduce model size and computational cost while maintaining performance.","However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs).","In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints.","A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints.","We present a novel algorithm to approximately solve the ILP.","We propose a novel first-order method for our optimization framework which makes use of our ILP solver.","Using problem structure (e.g., the low-rank structure of approx.","Hessian), we can address instances with millions of parameters.","Our experiments demonstrate that FALCON achieves superior accuracy compared to other pruning approaches within a fixed FLOP budget.","For instance, for ResNet50 with 20% of the total FLOPs retained, our approach improves the accuracy by 48% relative to state-of-the-art.","Furthermore, in gradual pruning settings with re-training between pruning steps, our framework outperforms existing pruning methods, emphasizing the significance of incorporating both FLOP and sparsity constraints for effective network pruning."],"url":"http://arxiv.org/abs/2403.07094v1","category":"cs.LG"}
{"created":"2024-03-11 18:26:02","title":"SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation","abstract":"Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leave the parameters containing general information on the high computational devices.","sentences":["Large language models(LLMs) have shown its outperforming ability on various tasks and question answering.","However, LLMs require high computation cost and large memory cost.","At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information.","In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints.","Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency.","Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.","Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leave the parameters containing general information on the high computational devices."],"url":"http://arxiv.org/abs/2403.07088v1","category":"cs.CL"}
{"created":"2024-03-11 18:22:49","title":"Dual Role of Accretion Disk Winds as X-ray Obscurers and UV Line Absorbers in AGN","abstract":"X-ray obscuration of active galactic nuclei (AGNs) is considered in the context of ionized winds of stratified structure launched from accretion disks. We argue that a Compton-thick layer of a large-scale disk wind can obscure continuum X-rays and also lead to broad UV absorption such as in the blue wing of Civ; the former originates from the inner wind while the latter from the outer wind as a dual role. Motivated by a number of observational evidence showing strong AGN obscuration phenomena in Seyfert 1 AGNs, we demonstrate in this work, by utilizing a physically-motivated wind model coupled to post-process radiative transfer calculations, that an extended disk wind under certain physical conditions (e.g. morphology and density) could naturally cause a sufficient obscuration qualitatively consistent with UV/X-ray observations. Predicted UV/X-ray correlation is also presented as a consequence of variable spatial size of the wind in this scenario.","sentences":["X-ray obscuration of active galactic nuclei (AGNs) is considered in the context of ionized winds of stratified structure launched from accretion disks.","We argue that a Compton-thick layer of a large-scale disk wind can obscure continuum X-rays and also lead to broad UV absorption such as in the blue wing of Civ; the former originates from the inner wind while the latter from the outer wind as a dual role.","Motivated by a number of observational evidence showing strong AGN obscuration phenomena in Seyfert 1 AGNs, we demonstrate in this work, by utilizing a physically-motivated wind model coupled to post-process radiative transfer calculations, that an extended disk wind under certain physical conditions (e.g. morphology and density) could naturally cause a sufficient obscuration qualitatively consistent with UV/X-ray observations.","Predicted UV/X-ray correlation is also presented as a consequence of variable spatial size of the wind in this scenario."],"url":"http://arxiv.org/abs/2403.07086v1","category":"astro-ph.HE"}
{"created":"2024-03-11 18:11:09","title":"Holography and Regge Phases with $U(1)$ Charge","abstract":"We use holography to study the large spin $J$ limit of the spectrum of low energy states with charge $Q$ under a $U(1)$ conserved current in CFTs in $d>2$ dimensions, with a focus on $d=3$ and $d=4$. For $Q=2$, the spectrum of such states is known to be universal and properly captured by the long-distance limit of holographic theories, regardless of whether the CFT itself is holographic. We study in detail the holographic description of such states at $Q>2$, by considering the contribution to the energies of $Q$ scalar particles coming from single photon and graviton exchange in the bulk of AdS; in some cases, scalar exchange and bulk contact terms are also included. For a range of finite values of $Q$ and $J$, we numerically diagonalize the Hamiltonian for such states and examine the resulting spectrum and wavefunctions as a function of the dimension $\\Delta$ of the charge-one operator and the central charges $c_{\\mathcal{T}}, c_{\\mathcal{J}}$ of the stress tensor and U(1) current, finding multiple regions in parameter space with qualitatively different behavior. We discuss the extension of these results to the regime of parametrically large charge $Q$, as well as to what extent such results are expected to hold universally, beyond the limit of holographic CFTs. We compare our holographic computations to results from the conformal bootstrap for the $3d$ O(2) model at $Q=3$ and $Q=4$ and find excellent agreement.","sentences":["We use holography to study the large spin $J$ limit of the spectrum of low energy states with charge $Q$ under a $U(1)$ conserved current in CFTs in $d>2$ dimensions, with a focus on $d=3$ and $d=4$. For $Q=2$, the spectrum of such states is known to be universal and properly captured by the long-distance limit of holographic theories, regardless of whether the CFT itself is holographic.","We study in detail the holographic description of such states at $Q>2$, by considering the contribution to the energies of $Q$ scalar particles coming from single photon and graviton exchange in the bulk of AdS; in some cases, scalar exchange and bulk contact terms are also included.","For a range of finite values of $Q$ and $J$, we numerically diagonalize the Hamiltonian for such states and examine the resulting spectrum and wavefunctions as a function of the dimension $\\Delta$ of the charge-one operator and the central charges $c_{\\mathcal{T}}, c_{\\mathcal{J}}$ of the stress tensor and U(1) current, finding multiple regions in parameter space with qualitatively different behavior.","We discuss the extension of these results to the regime of parametrically large charge $Q$, as well as to what extent such results are expected to hold universally, beyond the limit of holographic CFTs.","We compare our holographic computations to results from the conformal bootstrap for the $3d$ O(2) model at $Q=3$ and $Q=4$ and find excellent agreement."],"url":"http://arxiv.org/abs/2403.07079v1","category":"hep-th"}
{"created":"2024-03-11 18:02:52","title":"LISO: Lidar-only Self-Supervised 3D Object Detection","abstract":"3D object detection is one of the most important components in any Self-Driving stack, but current state-of-the-art (SOTA) lidar object detectors require costly & slow manual annotation of 3D bounding boxes to perform well. Recently, several methods emerged to generate pseudo ground truth without human supervision, however, all of these methods have various drawbacks: Some methods require sensor rigs with full camera coverage and accurate calibration, partly supplemented by an auxiliary optical flow engine. Others require expensive high-precision localization to find objects that disappeared over multiple drives. We introduce a novel self-supervised method to train SOTA lidar object detection networks which works on unlabeled sequences of lidar point clouds only, which we call trajectory-regularized self-training. It utilizes a SOTA self-supervised lidar scene flow network under the hood to generate, track, and iteratively refine pseudo ground truth. We demonstrate the effectiveness of our approach for multiple SOTA object detection networks across multiple real-world datasets. Code will be released.","sentences":["3D object detection is one of the most important components in any Self-Driving stack, but current state-of-the-art (SOTA) lidar object detectors require costly & slow manual annotation of 3D bounding boxes to perform well.","Recently, several methods emerged to generate pseudo ground truth without human supervision, however, all of these methods have various drawbacks: Some methods require sensor rigs with full camera coverage and accurate calibration, partly supplemented by an auxiliary optical flow engine.","Others require expensive high-precision localization to find objects that disappeared over multiple drives.","We introduce a novel self-supervised method to train SOTA lidar object detection networks which works on unlabeled sequences of lidar point clouds only, which we call trajectory-regularized self-training.","It utilizes a SOTA self-supervised lidar scene flow network under the hood to generate, track, and iteratively refine pseudo ground truth.","We demonstrate the effectiveness of our approach for multiple SOTA object detection networks across multiple real-world datasets.","Code will be released."],"url":"http://arxiv.org/abs/2403.07071v1","category":"cs.CV"}
{"created":"2024-03-11 18:01:55","title":"The quantum Hall effect under the influence of gravity and inertia: A unified approach","abstract":"The quantum Hall effect under the influence of gravity and inertia is studied in a unified way. We make use of an algebraic approach, as opposed to an analytic approach. We examine how both the integer and the fractional quantum Hall effects behave under a combined influence of gravity and inertia using a unified Hamiltonian. For that purpose, we first re-derive, using the purely algebraic method, the energy spectrum of charged particles moving in a plane perpendicular to a constant and uniform magnetic field either (i) under the influence of a nonlinear gravitational potential or (ii) under the influence of a constant rotation. The general Hamiltonian for describing the combined effect of gravity, rotation and inertia on the electrons of a Hall sample is then built and the eigenstates are obtained. The electrons mutual Coulomb interaction that gives rise to the familiar fractional quantum Hall effect is also discussed within a such a combination.","sentences":["The quantum Hall effect under the influence of gravity and inertia is studied in a unified way.","We make use of an algebraic approach, as opposed to an analytic approach.","We examine how both the integer and the fractional quantum Hall effects behave under a combined influence of gravity and inertia using a unified Hamiltonian.","For that purpose, we first re-derive, using the purely algebraic method, the energy spectrum of charged particles moving in a plane perpendicular to a constant and uniform magnetic field either (i) under the influence of a nonlinear gravitational potential or (ii) under the influence of a constant rotation.","The general Hamiltonian for describing the combined effect of gravity, rotation and inertia on the electrons of a Hall sample is then built and the eigenstates are obtained.","The electrons mutual Coulomb interaction that gives rise to the familiar fractional quantum Hall effect is also discussed within a such a combination."],"url":"http://arxiv.org/abs/2403.07069v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 18:00:47","title":"Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models","abstract":"Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation. In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies.","sentences":["Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks.","However, SSL strategies must be adapted to the type of training data and downstream tasks required.","We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning.","By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator.","Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation.","In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies."],"url":"http://arxiv.org/abs/2403.07066v1","category":"hep-ph"}
{"created":"2024-03-11 18:00:16","title":"On classical de Sitter solutions and parametric control","abstract":"Finding string backgrounds with de Sitter spacetime, where all approximations and corrections are controlled, is an open problem. We revisit the search for de Sitter solutions in the classical regime for specific type IIB supergravity compactifications on group manifolds, an under-explored corner of the landscape that offers an interesting testing ground for swampland conjectures. While the supergravity de Sitter solutions we obtain numerically are ambiguous in terms of their classicality, we find an analytic scaling that makes four out of six compactification radii, as well as the overall volume, arbitrarily large. This potentially provides parametric control over corrections. If we could show that these solutions, or others to be found, are fully classical, they would constitute a counterexample to conjectures stating that asymptotic de Sitter solutions do not exist. We discuss this point in great detail.","sentences":["Finding string backgrounds with de Sitter spacetime, where all approximations and corrections are controlled, is an open problem.","We revisit the search for de Sitter solutions in the classical regime for specific type IIB supergravity compactifications on group manifolds, an under-explored corner of the landscape that offers an interesting testing ground for swampland conjectures.","While the supergravity de Sitter solutions we obtain numerically are ambiguous in terms of their classicality, we find an analytic scaling that makes four out of six compactification radii, as well as the overall volume, arbitrarily large.","This potentially provides parametric control over corrections.","If we could show that these solutions, or others to be found, are fully classical, they would constitute a counterexample to conjectures stating that asymptotic de Sitter solutions do not exist.","We discuss this point in great detail."],"url":"http://arxiv.org/abs/2403.07065v1","category":"hep-th"}
{"created":"2024-03-11 18:00:12","title":"CANUCS: An Updated Mass and Magnification Model of Abell 370 with JWST","abstract":"We report an updated mass and magnification model of galaxy cluster Abell 370 using new NIRCam and NIRISS data from the CAnadian NIRISS Unbiased Cluster Survey (CANUCS). Using Lenstool and a combination of archival HST and MUSE data with new JWST data as constraints, we derive an improved gravitational lensing model and extract magnifications of background galaxies with uncertainties. Using our best fit model, we perform a search for new multiply imaged systems via predicted positions. We report no new multiply imaged systems with identifiable redshifts, likely due to already very deep HST and Spitzer data, but confirm a $z\\sim8$ multiply imaged system by measuring its redshift with NIRISS and NIRSpec spectra. We find that the overall shape of the critical curve for a source at $z = 9.0$ is similar to previous models of Abell 370, with small changes. We investigate the $z\\sim8$ galaxy with two images observable with an apparent magnitude in the F125W band of $26.0\\pm0.2$ and $25.6\\pm0.1$. After correcting for the magnifications of the images, 7.2$^{+0.2}_{-1.2}$ and 8.7$^{+0.4}_{-0.4}$, we use SED fitting to find an intrinsic stellar mass of log($M^*/M_{\\odot})$ = 7.35$^{+0.04}_{-0.05}$, intrinsic SFR of 3.5$^{+2.2}_{-1.4}$ M$_{\\odot}$/yr, and $M_{UV}$ of -21.3$^{+0.2}_{-0.2}$, which is close to the knee of the luminosity function at that redshift. Our model, and corresponding magnification, shear, and convergence maps are available on request and will be made publicly available on MAST in a CANUCS data release (DOI: 10.17909/ph4n-6n76).","sentences":["We report an updated mass and magnification model of galaxy cluster Abell 370 using new NIRCam and NIRISS data from the CAnadian NIRISS Unbiased Cluster Survey (CANUCS).","Using Lenstool and a combination of archival HST and MUSE data with new JWST data as constraints, we derive an improved gravitational lensing model and extract magnifications of background galaxies with uncertainties.","Using our best fit model, we perform a search for new multiply imaged systems via predicted positions.","We report no new multiply imaged systems with identifiable redshifts, likely due to already very deep HST and Spitzer data, but confirm a $z\\sim8$ multiply imaged system by measuring its redshift with NIRISS and NIRSpec spectra.","We find that the overall shape of the critical curve for a source at $z = 9.0$ is similar to previous models of Abell 370, with small changes.","We investigate the $z\\sim8$ galaxy with two images observable with an apparent magnitude in the F125W band of $26.0\\pm0.2$ and $25.6\\pm0.1$. After correcting for the magnifications of the images, 7.2$^{+0.2}_{-1.2}$ and 8.7$^{+0.4}_{-0.4}$, we use SED fitting to find an intrinsic stellar mass of log($M^*/M_{\\odot})$ = 7.35$^{+0.04}_{-0.05}$, intrinsic SFR of 3.5$^{+2.2}_{-1.4}$ M$_{\\odot}$/yr, and $M_{UV}$ of -21.3$^{+0.2}_{-0.2}$, which is close to the knee of the luminosity function at that redshift.","Our model, and corresponding magnification, shear, and convergence maps are available on request and will be made publicly available on MAST in a CANUCS data release (DOI: 10.17909/ph4n-6n76)."],"url":"http://arxiv.org/abs/2403.07062v1","category":"astro-ph.GA"}
{"created":"2024-03-11 18:00:06","title":"Better than classical? The subtle art of benchmarking quantum machine learning models","abstract":"Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that \"quantumness\" may not be the crucial ingredient for the small learning tasks considered here. Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results.","sentences":["Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available.","However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights.","To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets.","We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers.","Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that \"quantumness\" may not be the crucial ingredient for the small learning tasks considered here.","Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results."],"url":"http://arxiv.org/abs/2403.07059v1","category":"quant-ph"}
{"created":"2024-03-11 18:00:06","title":"Scrutinising evidence for the triggering of Active Galactic Nuclei in the outskirts of massive galaxy clusters at $z\\approx1$","abstract":"Environmental effects are believed to play an important yet poorly understood role in triggering accretion events onto the supermassive black holes (SMBHs) of galaxies (Active Galactic Nuclei; AGN). Massive clusters, which represent the densest structures in the Universe, provide an excellent laboratory to isolate environmental effects and study their impact on black hole growth. In this work, we critically review observational evidence for the preferential activation of SMBHs in the outskirts of galaxy clusters. We develop a semi-empirical model under the assumption that the incidence of AGN in galaxies is independent of environment. We demonstrate that the model is broadly consistent with recent observations on the AGN halo occupation at $z=0.2$, although it may overpredict satellite AGN in massive halos at that low redshift. We then use this model to interpret the projected radial distribution of X-ray sources around high redshift ($z\\approx1$) massive ($>5 \\times 10^{14} \\, M_\\odot$) clusters, which show excess counts outside their virial radius. Such an excess naturally arises in our model as a result of sample variance. Up to 20% of the simulated projected radial distributions show excess counts similar to the observations, which are however, because of background/foreground AGN and hence, not physically associated with the cluster. Our analysis emphasises the importance of projection effects and shows that current observations of $z\\approx1$ clusters remain inconclusive on the activation of SMBHs during infall.","sentences":["Environmental effects are believed to play an important yet poorly understood role in triggering accretion events onto the supermassive black holes (SMBHs) of galaxies (Active Galactic Nuclei; AGN).","Massive clusters, which represent the densest structures in the Universe, provide an excellent laboratory to isolate environmental effects and study their impact on black hole growth.","In this work, we critically review observational evidence for the preferential activation of SMBHs in the outskirts of galaxy clusters.","We develop a semi-empirical model under the assumption that the incidence of AGN in galaxies is independent of environment.","We demonstrate that the model is broadly consistent with recent observations on the AGN halo occupation at $z=0.2$, although it may overpredict satellite AGN in massive halos at that low redshift.","We then use this model to interpret the projected radial distribution of X-ray sources around high redshift ($z\\approx1$) massive ($>5 \\times 10^{14} \\, M_\\odot$) clusters, which show excess counts outside their virial radius.","Such an excess naturally arises in our model as a result of sample variance.","Up to 20% of the simulated projected radial distributions show excess counts similar to the observations, which are however, because of background/foreground AGN and hence, not physically associated with the cluster.","Our analysis emphasises the importance of projection effects and shows that current observations of $z\\approx1$ clusters remain inconclusive on the activation of SMBHs during infall."],"url":"http://arxiv.org/abs/2403.07060v1","category":"astro-ph.GA"}
{"created":"2024-03-11 18:00:01","title":"Lorentzian contours for tree-level string amplitudes","abstract":"We engineer compact contours on the moduli spaces of genus-zero Riemann surfaces that achieve analytic continuation from Euclidean to Lorentzian worldsheets. These generalized Pochhammer contours are based on the combinatorics of associahedra and make the analytic properties of tree-level amplitudes entirely manifest for any number and type of external strings. We use them in practice to perform first numerical computations of open and closed string amplitudes directly in the physical kinematics for $n=4,5,6,7,8,9$. We provide a code that allows anyone to do such computations.","sentences":["We engineer compact contours on the moduli spaces of genus-zero Riemann surfaces that achieve analytic continuation from Euclidean to Lorentzian worldsheets.","These generalized Pochhammer contours are based on the combinatorics of associahedra and make the analytic properties of tree-level amplitudes entirely manifest for any number and type of external strings.","We use them in practice to perform first numerical computations of open and closed string amplitudes directly in the physical kinematics for $n=4,5,6,7,8,9$. We provide a code that allows anyone to do such computations."],"url":"http://arxiv.org/abs/2403.07051v1","category":"hep-th"}
{"created":"2024-03-11 18:00:00","title":"Lagrangian Perturbation Theory for Biased Tracers: Halo numbers are conserved?","abstract":"The Lagrangian perturbation theory (LPT) provides a simple yet powerful way of computing the nonlinear matter power spectrum, and it has been applied to biased tracers such as halos and galaxies. The number conservation of matter particles allows a simple relation between the fluctuations at the initial and the late times, which is essential in deriving the exact expression for the nonlinear matter power spectrum. Biased tracers in contrast evolve through mergers and accretion, violating the assumption of number conservation. Furthermore, a sample of halos with a narrow mass bin at initial time evolves to become halos with very different masses at late time, making it difficult to directly connect predictions of the LPT to real observations, as observed samples have similar properties. Here we use $N$-body simulations to test these core predictions of the LPT for biased tracers and demonstrate that the LPT predictions for halos overestimates the power spectrum at $z=0$ by a factor of three for the mass bin sample $\\Delta \\log M_h~(h^{-1}M_{\\odot})= 0.5$ at~$z\\simeq 3$, while the predictions can be made consistent if we impose by hand the number conservation of halos in the simulations throughout the entire evolution. In reality, LPT modeling of biased tracers involves marginalization of unknown bias parameters, alleviating the problem. We discuss the implications for field-level models based on the LPT applications.","sentences":["The Lagrangian perturbation theory (LPT) provides a simple yet powerful way of computing the nonlinear matter power spectrum, and it has been applied to biased tracers such as halos and galaxies.","The number conservation of matter particles allows a simple relation between the fluctuations at the initial and the late times, which is essential in deriving the exact expression for the nonlinear matter power spectrum.","Biased tracers in contrast evolve through mergers and accretion, violating the assumption of number conservation.","Furthermore, a sample of halos with a narrow mass bin at initial time evolves to become halos with very different masses at late time, making it difficult to directly connect predictions of the LPT to real observations, as observed samples have similar properties.","Here we use $N$-body simulations to test these core predictions of the LPT for biased tracers and demonstrate that the LPT predictions for halos overestimates the power spectrum at $z=0$ by a factor of three for the mass bin sample $\\Delta \\log M_h~(h^{-1}M_{\\odot})= 0.5$ at~$z\\simeq 3$, while the predictions can be made consistent if we impose by hand the number conservation of halos in the simulations throughout the entire evolution.","In reality, LPT modeling of biased tracers involves marginalization of unknown bias parameters, alleviating the problem.","We discuss the implications for field-level models based on the LPT applications."],"url":"http://arxiv.org/abs/2403.07046v1","category":"astro-ph.CO"}
{"created":"2024-03-11 18:00:00","title":"Firewalls at exponentially late times","abstract":"We consider a version of the typical state firewall setup recently reintroduced by Stanford and Yang, who found that wormholes may create firewalls. We examine a late-time double scaling limit in JT gravity in which one can resum the expansion in the number of wormholes, and we use this to study the exact distribution of interior slices at times exponential in the entropy. We consider a thermofield double with and without early perturbations on a boundary. These perturbations can appear on interior slices as dangerous high energy shocks. For exponentially late times, wormholes tend to teleport the particles created by perturbations and render the interior more dangerous. In states with many perturbations separated by large times, the probability of a safe interior is exponentially small. Such states thus almost certainly have firewalls at the horizon, even though they would be safe without wormholes. With perturbation, even in the safest state we conceive, the odds of encountering a firewall are fifty-fifty. One interpretation of the phenomena found here is that wormholes can change time-ordered contours into effective out-of-time-ordered folds, making shockwaves appear in unexpected places.","sentences":["We consider a version of the typical state firewall setup recently reintroduced by Stanford and Yang, who found that wormholes may create firewalls.","We examine a late-time double scaling limit in JT gravity in which one can resum the expansion in the number of wormholes, and we use this to study the exact distribution of interior slices at times exponential in the entropy.","We consider a thermofield double with and without early perturbations on a boundary.","These perturbations can appear on interior slices as dangerous high energy shocks.","For exponentially late times, wormholes tend to teleport the particles created by perturbations and render the interior more dangerous.","In states with many perturbations separated by large times, the probability of a safe interior is exponentially small.","Such states thus almost certainly have firewalls at the horizon, even though they would be safe without wormholes.","With perturbation, even in the safest state we conceive, the odds of encountering a firewall are fifty-fifty.","One interpretation of the phenomena found here is that wormholes can change time-ordered contours into effective out-of-time-ordered folds, making shockwaves appear in unexpected places."],"url":"http://arxiv.org/abs/2403.07049v1","category":"hep-th"}
{"created":"2024-03-11 17:59:41","title":"Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling","abstract":"In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition. Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen. This approach greatly reduces the number of learnable parameters compared to full tuning. For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning. However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results. This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference. To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block. Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection. The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition. The code and pre-trained models are available at https://github.com/wgcban/apt","sentences":["In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition.","Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen.","This approach greatly reduces the number of learnable parameters compared to full tuning.","For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning.","However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results.","This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference.","To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block.","Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection.","The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition.","The code and pre-trained models are available at https://github.com/wgcban/apt"],"url":"http://arxiv.org/abs/2403.06978v1","category":"cs.CV"}
{"created":"2024-03-11 17:54:42","title":"A representation-learning game for classes of prediction tasks","abstract":"We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features. For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation. For general representations and loss functions, we propose an efficient algorithm to optimize a randomized representation. The algorithm only requires the gradients of the loss function, and is based on incrementally adding a representation rule to a mixture of such rules.","sentences":["We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available.","In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge.","The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features.","For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation.","For general representations and loss functions, we propose an efficient algorithm to optimize a randomized representation.","The algorithm only requires the gradients of the loss function, and is based on incrementally adding a representation rule to a mixture of such rules."],"url":"http://arxiv.org/abs/2403.06971v1","category":"cs.LG"}
{"created":"2024-03-11 17:54:33","title":"MRL Parsing Without Tears: The Case of Hebrew","abstract":"Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking. Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity. Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward. Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test case, we present a new \"flipped pipeline\": decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task. The classifiers are independent of one another, and only at the end do we synthesize their predictions. This blazingly fast approach sets a new SOTA in Hebrew POS tagging and dependency parsing, while also reaching near-SOTA performance on other Hebrew NLP tasks. Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs.","sentences":["Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking.","Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity.","Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward.","Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow.","In contrast, and taking Hebrew as a test case, we present a new \"flipped pipeline\": decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task.","The classifiers are independent of one another, and only at the end do we synthesize their predictions.","This blazingly fast approach sets a new SOTA in Hebrew POS tagging and dependency parsing, while also reaching near-SOTA performance on other Hebrew NLP tasks.","Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs."],"url":"http://arxiv.org/abs/2403.06970v1","category":"cs.CL"}
{"created":"2024-03-11 17:53:37","title":"Non-Abelian R-symmetries in $\\mathcal{N}=1$ supersymmetry","abstract":"We investigate non-Abelian R-symmetries in $\\mathcal{N}=1$ supersymmetric theory, where fields may transform under the R-symmetry in representations with dimension higher than one. While a continuous non-Abelian R-symmetry can always be decomposed to a $U(1)$ R-symmetry and non-R symmetries, there are non-trivial discrete non-Abelian R-symmetries that do not admit such a decomposition, and effective R-charges cannot be defined in such models. Previous results on sufficient conditions for R-symmetric supersymmetric vacua in Wess-Zumino models still hold, and do not depend on fields in representations of dimension greater than one. However, fields in higher-dimensional representations enter the sufficient conditions for supersymmetric vacua that break R-symmetry, but it is difficult to identify the independent variables which can be used to solve the F-flatness equation in this case, unless other conditions are fulfilled. We present examples with discrete non-Abelian R-symmetries of the lowest order in this case.","sentences":["We investigate non-Abelian R-symmetries in $\\mathcal{N}=1$ supersymmetric theory, where fields may transform under the R-symmetry in representations with dimension higher than one.","While a continuous non-Abelian R-symmetry can always be decomposed to a $U(1)$ R-symmetry and non-R symmetries, there are non-trivial discrete non-Abelian R-symmetries that do not admit such a decomposition, and effective R-charges cannot be defined in such models.","Previous results on sufficient conditions for R-symmetric supersymmetric vacua in Wess-Zumino models still hold, and do not depend on fields in representations of dimension greater than one.","However, fields in higher-dimensional representations enter the sufficient conditions for supersymmetric vacua that break R-symmetry, but it is difficult to identify the independent variables which can be used to solve the F-flatness equation in this case, unless other conditions are fulfilled.","We present examples with discrete non-Abelian R-symmetries of the lowest order in this case."],"url":"http://arxiv.org/abs/2403.06969v1","category":"hep-th"}
{"created":"2024-03-11 17:34:25","title":"Materials science in the era of large language models: a perspective","abstract":"Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers. We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains. It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research.","sentences":["Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems.","In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers.","We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale.","At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains.","It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research."],"url":"http://arxiv.org/abs/2403.06949v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 17:31:49","title":"Sj$\\ddot{\\text{o}}$qvist quantum geometric tensor of finite-temperature mixed states","abstract":"The quantum geometric tensor (QGT) reveals local geometric properties and associated topological information of quantum states. Here a generalization of the QGT to mixed quantum states at finite temperatures based on the Sj$\\ddot{\\text{o}}$qvist distance is developed. The resulting Sj$\\ddot{\\text{o}}$qvist QGT is invariant under gauge transformations of individual spectrum levels. A Pythagorean-like relation connects the distances and gauge transformations, which clarifies the role of the parallel-transport condition. The real part of the QGT naturally decomposes into a sum of the Fisher-Rao metric and Fubini-Study metrics, allowing a distinction between different contributions to the quantum distance. The imaginary part of the QGT is proportional to the weighted summation of the Berry curvatures, which leads to a geometric phase for mixed states under certain conditions. We present three examples of different dimensions to illustrate the temperature dependence of the QGT and a discussion on possible implications.","sentences":["The quantum geometric tensor (QGT) reveals local geometric properties and associated topological information of quantum states.","Here a generalization of the QGT to mixed quantum states at finite temperatures based on the Sj$\\ddot{\\text{o}}$qvist distance is developed.","The resulting Sj$\\ddot{\\text{o}}$qvist QGT is invariant under gauge transformations of individual spectrum levels.","A Pythagorean-like relation connects the distances and gauge transformations, which clarifies the role of the parallel-transport condition.","The real part of the QGT naturally decomposes into a sum of the Fisher-Rao metric and Fubini-Study metrics, allowing a distinction between different contributions to the quantum distance.","The imaginary part of the QGT is proportional to the weighted summation of the Berry curvatures, which leads to a geometric phase for mixed states under certain conditions.","We present three examples of different dimensions to illustrate the temperature dependence of the QGT and a discussion on possible implications."],"url":"http://arxiv.org/abs/2403.06944v1","category":"quant-ph"}
{"created":"2024-03-11 17:26:51","title":"Comparison of Static Analysis Architecture Recovery Tools for Microservice Applications","abstract":"Architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle. This is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture. Various tools and techniques for this task are presented in academic and grey literature sources. Practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities. However, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness. With the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery. We will focus on static approaches because they are also suitable for integration into fast-paced CI/CD pipelines.","sentences":["Architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle.","This is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture.","Various tools and techniques for this task are presented in academic and grey literature sources.","Practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities.","However, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness.","With the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery.","We will focus on static approaches because they are also suitable for integration into fast-paced CI/CD pipelines."],"url":"http://arxiv.org/abs/2403.06941v1","category":"cs.SE"}
{"created":"2024-03-11 17:26:18","title":"Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction","abstract":"Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression. Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care. However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately. Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression. To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis. Our conditional diffusion model utilizes all available data during the training phase to make predictions based solely on baseline information during inference without needing prior history about CTh progression. The prediction accuracy of the proposed CTh prediction pipeline using a conditional score-based model was compared for sub-groups consisting of cognitively normal, mild cognitive impairment, and AD subjects. The Bland-Altman analysis shows our diffusion-based prediction model has a near-zero bias with narrow 95% confidential interval compared to the ground-truth CTh in 6-36 months. In addition, our conditional diffusion model has a stochastic generative nature, therefore, we demonstrated an uncertainty analysis of patient-specific CTh prediction through multiple realizations.","sentences":["Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression.","Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care.","However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately.","Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression.","To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis.","Our conditional diffusion model utilizes all available data during the training phase to make predictions based solely on baseline information during inference without needing prior history about CTh progression.","The prediction accuracy of the proposed CTh prediction pipeline using a conditional score-based model was compared for sub-groups consisting of cognitively normal, mild cognitive impairment, and AD subjects.","The Bland-Altman analysis shows our diffusion-based prediction model has a near-zero bias with narrow 95% confidential interval compared to the ground-truth CTh in 6-36 months.","In addition, our conditional diffusion model has a stochastic generative nature, therefore, we demonstrated an uncertainty analysis of patient-specific CTh prediction through multiple realizations."],"url":"http://arxiv.org/abs/2403.06940v1","category":"eess.IV"}
{"created":"2024-03-12 16:46:29","title":"Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability","abstract":"Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies. A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies. Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location. This can cause presumed false positives or imprecise results. Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains. An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location. We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance benchmark to compare different analysis configurations, and conducted an expert interview. Result: We found that 50 % of the projects with a report had at least one error chain. Our runtime benchmark demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %. The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis. Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results. The runtime benchmark indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects. Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains.","sentences":["Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies.","A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies.","Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location.","This can cause presumed false positives or imprecise results.","Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains.","An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location.","We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance benchmark to compare different analysis configurations, and conducted an expert interview.","Result:","We found that 50 % of the projects with a report had at least one error chain.","Our runtime benchmark demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %.","The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis.","Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results.","The runtime benchmark indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects.","Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains."],"url":"http://arxiv.org/abs/2403.07808v1","category":"cs.SE"}
{"created":"2024-03-12 15:42:05","title":"An Optimal Sequence Reconstruction Algorithm for Reed-Solomon Codes","abstract":"The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a scenario where the sender transmits a codeword from some codebook, and the receiver obtains $N$ noisy outputs of the codeword. We study the problem of efficient reconstruction using $N$ outputs that are each corrupted by at most $t$ substitutions. Specifically, for the ubiquitous Reed-Solomon codes, we adapt the Koetter-Vardy soft-decoding algorithm, presenting a reconstruction algorithm capable of correcting beyond Johnson radius. Furthermore, the algorithm uses $\\mathcal{O}(nN)$ field operations, where $n$ is the codeword length.","sentences":["The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a scenario where the sender transmits a codeword from some codebook, and the receiver obtains $N$ noisy outputs of the codeword.","We study the problem of efficient reconstruction using $N$ outputs that are each corrupted by at most $t$ substitutions.","Specifically, for the ubiquitous Reed-Solomon codes, we adapt the Koetter-Vardy soft-decoding algorithm, presenting a reconstruction algorithm capable of correcting beyond Johnson radius.","Furthermore, the algorithm uses $\\mathcal{O}(nN)$ field operations, where $n$ is the codeword length."],"url":"http://arxiv.org/abs/2403.07754v1","category":"cs.IT"}
{"created":"2024-03-12 13:32:46","title":"A phase-resolved Fermi-LAT analysis of the mode-changing pulsar PSR J2021+4026 shows hints of a multipolar magnetosphere","abstract":"The goal of our work is to study the mode changes of the radio-quiet gamma-ray pulsar PSR J2021+4026 with improved detail. By accurately characterizing variations in the gamma-ray spectrum and pulse profile, we aim to relate the Fermi-LAT observations to theoretical models and interpret the mode changes in terms of variations in the structure of a multipolar dissipative magnetosphere. We continually monitored the rotational evolution and the gamma-ray flux of PSR J2021+4026 using more than 13 years of Fermi-LAT data with a binned likelihood approach. We clearly detect the previous mode changes and confirm a more recent mode change that occurred around June 2020. We investigated the features of the phase-resolved spectrum and pulse profile, and we inferred the macroscopic conductivity, the electric field parallel to the magnetic field, and the curvature radiation cutoff energy. These physical quantities are related to the spin-down rate and the gamma-ray flux and therefore are relevant to the theoretical interpretation of the mode changes. We computed the relative variations in the best-fit parameters, finding typical flux changes between 13% and 20%. Correlations appear between the gamma-ray flux and the spectral parameters, as the peak of the spectrum shifts by about 10% toward lower energies when the flux decreases. The analysis of the pulse profile reveals that the pulsed fraction of the light curve is larger when the flux is low. We introduced a simple magnetosphere model that combines a dipole field with a strong quadrupole component. We simulated magnetic field configurations to determine the positions of the polar caps for different sets of parameters, and we conclude that some configurations could explain the observed multiwavelength variability.","sentences":["The goal of our work is to study the mode changes of the radio-quiet gamma-ray pulsar PSR J2021+4026 with improved detail.","By accurately characterizing variations in the gamma-ray spectrum and pulse profile, we aim to relate the Fermi-LAT observations to theoretical models and interpret the mode changes in terms of variations in the structure of a multipolar dissipative magnetosphere.","We continually monitored the rotational evolution and the gamma-ray flux of PSR J2021+4026 using more than 13 years of Fermi-LAT data with a binned likelihood approach.","We clearly detect the previous mode changes and confirm a more recent mode change that occurred around June 2020.","We investigated the features of the phase-resolved spectrum and pulse profile, and we inferred the macroscopic conductivity, the electric field parallel to the magnetic field, and the curvature radiation cutoff energy.","These physical quantities are related to the spin-down rate and the gamma-ray flux and therefore are relevant to the theoretical interpretation of the mode changes.","We computed the relative variations in the best-fit parameters, finding typical flux changes between 13% and 20%.","Correlations appear between the gamma-ray flux and the spectral parameters, as the peak of the spectrum shifts by about 10% toward lower energies when the flux decreases.","The analysis of the pulse profile reveals that the pulsed fraction of the light curve is larger when the flux is low.","We introduced a simple magnetosphere model that combines a dipole field with a strong quadrupole component.","We simulated magnetic field configurations to determine the positions of the polar caps for different sets of parameters, and we conclude that some configurations could explain the observed multiwavelength variability."],"url":"http://arxiv.org/abs/2403.07649v1","category":"astro-ph.HE"}
{"created":"2024-03-12 13:08:33","title":"Trade-offs and thermodynamics of energy-relay proofreading","abstract":"Biological processes that are able to discriminate between different molecules consume energy and dissipate heat. They operate at different levels of fidelity and speed, and as a consequence there exist fundamental trade-offs between these quantities and the entropy production rate. Usually, the energy source required to operate in a high-fidelity regime comes from the consumption of external energetic molecules, e.g., GTP hydrolysis in protein translation . In this work, we study trade-offs between several kinetic and thermodynamic observables for Hopfield's energy-relay mechanism, which does not consume external molecules and is able to operate in depleted regions, at the cost of a higher error rate. The trade-offs are obtained both analytically and numerically via Pareto optimal fronts. We find that the scheme is able to operate in three distinct regimes: an energy relay regime, a mixed relay-Michaelis-Menten regime, and a Michaelis-Menten regime, depending on the kinetic and energetic parameters that tune transitions between states. The mixed regime features a dynamical phase transition in the error-entropy production Pareto trade-off, while the pure energy relay regime contains a region where this type of proofreading energetically outperforms standard kinetic proofreading.","sentences":["Biological processes that are able to discriminate between different molecules consume energy and dissipate heat.","They operate at different levels of fidelity and speed, and as a consequence there exist fundamental trade-offs between these quantities and the entropy production rate.","Usually, the energy source required to operate in a high-fidelity regime comes from the consumption of external energetic molecules, e.g., GTP hydrolysis in protein translation .","In this work, we study trade-offs between several kinetic and thermodynamic observables for Hopfield's energy-relay mechanism, which does not consume external molecules and is able to operate in depleted regions, at the cost of a higher error rate.","The trade-offs are obtained both analytically and numerically via Pareto optimal fronts.","We find that the scheme is able to operate in three distinct regimes: an energy relay regime, a mixed relay-Michaelis-Menten regime, and a Michaelis-Menten regime, depending on the kinetic and energetic parameters that tune transitions between states.","The mixed regime features a dynamical phase transition in the error-entropy production Pareto trade-off, while the pure energy relay regime contains a region where this type of proofreading energetically outperforms standard kinetic proofreading."],"url":"http://arxiv.org/abs/2403.07626v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 10:35:40","title":"Tuning diagonal scale matrices for HMC","abstract":"Three approaches for adaptively tuning diagonal scale matrices for HMC are discussed and compared. The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark. Scaling according to the mean log-target gradient (ISG), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives. Numerical studies suggest that the ISG method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies. The ISG method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice.","sentences":["Three approaches for adaptively tuning diagonal scale matrices for HMC are discussed and compared.","The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark.","Scaling according to the mean log-target gradient (ISG), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives.","Numerical studies suggest that the ISG method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies.","The ISG method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice."],"url":"http://arxiv.org/abs/2403.07495v1","category":"stat.CO"}
{"created":"2024-03-12 10:21:21","title":"PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates","abstract":"We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems. Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function.","sentences":["We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO).","The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit.","We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   ","The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization.","Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES).","This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems.","Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function."],"url":"http://arxiv.org/abs/2403.07485v1","category":"math.OC"}
{"created":"2024-03-12 09:58:12","title":"A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics","abstract":"The modeling of cracks is an important topic - both in engineering as well as in mathematics. Since crack propagation is characterized by a free boundary value problem (the geometry of the crack is not known beforehand, but part of the solution), approximations of the underlying sharp-interface problem based on phase-field models are often considered. Focusing on a rate-independent setting, these models are defined by a unidirectional gradient-flow of an energy functional. Since this energy functional is non-convex, the evolution of the variables such as the displacement field and the phase-field variable might be discontinuous in time leading to so-called brutal crack growth. For this reason, solution concepts have to be carefully chosen in order to predict discontinuities that are physically reasonable. One such concept is that of Balanced Viscosity solutions (BV solutions). This concept predicts physically sound energy trajectories that do not jump across energy barriers. The paper deals with a time-adaptive finite element phase-field model for rate-independent fracture which converges to BV solutions. The model is motivated by constraining the pseudo-velocity of the crack tip. The resulting constrained minimization problem is solved by the augmented Lagrangian method. Numerical examples highlight the predictive capabilities of the model and furthermore show the efficiency and the robustness of the final algorithm.","sentences":["The modeling of cracks is an important topic - both in engineering as well as in mathematics.","Since crack propagation is characterized by a free boundary value problem (the geometry of the crack is not known beforehand, but part of the solution), approximations of the underlying sharp-interface problem based on phase-field models are often considered.","Focusing on a rate-independent setting, these models are defined by a unidirectional gradient-flow of an energy functional.","Since this energy functional is non-convex, the evolution of the variables such as the displacement field and the phase-field variable might be discontinuous in time leading to so-called brutal crack growth.","For this reason, solution concepts have to be carefully chosen in order to predict discontinuities that are physically reasonable.","One such concept is that of Balanced Viscosity solutions (BV solutions).","This concept predicts physically sound energy trajectories that do not jump across energy barriers.","The paper deals with a time-adaptive finite element phase-field model for rate-independent fracture which converges to BV solutions.","The model is motivated by constraining the pseudo-velocity of the crack tip.","The resulting constrained minimization problem is solved by the augmented Lagrangian method.","Numerical examples highlight the predictive capabilities of the model and furthermore show the efficiency and the robustness of the final algorithm."],"url":"http://arxiv.org/abs/2403.07461v1","category":"cs.CE"}
{"created":"2024-03-12 07:06:50","title":"Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery","abstract":"In this paper, we study the problem of Generalized Category Discovery (GCD), which aims to cluster unlabeled data from both known and unknown categories using the knowledge of labeled data from known categories. Current GCD methods rely on only visual cues, which however neglect the multi-modality perceptive nature of human cognitive processes in discovering novel visual categories. To address this, we propose a two-phase TextGCD framework to accomplish multi-modality GCD by exploiting powerful Visual-Language Models. TextGCD mainly includes a retrieval-based text generation (RTG) phase and a cross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon using category tags from diverse datasets and attributes from Large Language Models, generating descriptive texts for images in a retrieval manner. Second, CCT leverages disparities between textual and visual modalities to foster mutual learning, thereby enhancing visual GCD. In addition, we design an adaptive class aligning strategy to ensure the alignment of category perceptions between modalities as well as a soft-voting mechanism to integrate multi-modality cues. Experiments on eight datasets show the large superiority of our approach over state-of-the-art methods. Notably, our approach outperforms the best competitor, by 7.7% and 10.8% in All accuracy on ImageNet-1k and CUB, respectively.","sentences":["In this paper, we study the problem of Generalized Category Discovery (GCD), which aims to cluster unlabeled data from both known and unknown categories using the knowledge of labeled data from known categories.","Current GCD methods rely on only visual cues, which however neglect the multi-modality perceptive nature of human cognitive processes in discovering novel visual categories.","To address this, we propose a two-phase TextGCD framework to accomplish multi-modality GCD by exploiting powerful Visual-Language Models.","TextGCD mainly includes a retrieval-based text generation (RTG) phase and a cross-modality co-teaching (CCT) phase.","First, RTG constructs a visual lexicon using category tags from diverse datasets and attributes from Large Language Models, generating descriptive texts for images in a retrieval manner.","Second, CCT leverages disparities between textual and visual modalities to foster mutual learning, thereby enhancing visual GCD.","In addition, we design an adaptive class aligning strategy to ensure the alignment of category perceptions between modalities as well as a soft-voting mechanism to integrate multi-modality cues.","Experiments on eight datasets show the large superiority of our approach over state-of-the-art methods.","Notably, our approach outperforms the best competitor, by 7.7% and 10.8% in All accuracy on ImageNet-1k and CUB, respectively."],"url":"http://arxiv.org/abs/2403.07369v1","category":"cs.CV"}
{"created":"2024-03-12 06:29:54","title":"Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning","abstract":"Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along with the pre-training classification head. We find that the backbone of our pre-trained networks can learn representations useful for the downstream continual learning problem, thus becoming a valuable input to any existing continual learning method. Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification benchmarks. Supporting code can be found at https://github.com/cl-premonition/premonition.","sentences":["Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed.","It is rare, however, that the data and task changes are completely unpredictable.","Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it.","We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time.","We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future.","These descriptions are then rendered using Stable Diffusion to generate new labelled image samples.","The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along with the pre-training classification head.","We find that the backbone of our pre-trained networks can learn representations useful for the downstream continual learning problem, thus becoming a valuable input to any existing continual learning method.","Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification benchmarks.","Supporting code can be found at https://github.com/cl-premonition/premonition."],"url":"http://arxiv.org/abs/2403.07356v1","category":"cs.CV"}
{"created":"2024-03-12 04:13:45","title":"Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models","abstract":"Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement. We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection benchmark with a clear margin and exhibits seamless scalability to additional visual tasks. Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights. The code will be released at https://github.com/SxJyJay/Lumen.","sentences":["Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields.","A recent trend is to further extend and enhance the perception capabilities of LMMs.","The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM.","This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities.","To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement.","We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages.","Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks.","Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper.","Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts.","Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection benchmark with a clear margin and exhibits seamless scalability to additional visual tasks.","Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights.","The code will be released at https://github.com/SxJyJay/Lumen."],"url":"http://arxiv.org/abs/2403.07304v1","category":"cs.CV"}
{"created":"2024-03-12 04:10:06","title":"Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation","abstract":"U-Net has been widely used for segmenting abdominal organs, achieving promising performance. However, when it is used for multi-organ segmentation, first, it may be limited in exploiting global long-range contextual information due to the implementation of standard convolutions. Second, the use of spatial-wise downsampling (e.g., max pooling or strided convolutions) in the encoding path may lead to the loss of deformable or discriminative details. Third, features upsampled from the higher level are concatenated with those that persevered via skip connections. However, repeated downsampling and upsampling operations lead to misalignments between them and their concatenation degrades segmentation performance. To address these limitations, we propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated Downsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules, respectively. The DCC module can utilize global inter-dependencies between spatial and channel features to calibrate these features adaptively. The DCD module enables networks to adaptively preserve deformable or discriminative features during downsampling. The DCU module can dynamically align and calibrate upsampled features to eliminate misalignments before concatenations. We integrated the proposed modules into a standard U-Net, resulting in a new architecture, termed Dynamic U-Net. This architectural design enables U-Net to dynamically adjust features for different organs. We evaluated Dynamic U-Net in two abdominal multi-organ segmentation benchmarks. Dynamic U-Net achieved statistically improved segmentation accuracy compared with standard U-Net. Our code is available at https://github.com/sotiraslab/DynamicUNet.","sentences":["U-Net has been widely used for segmenting abdominal organs, achieving promising performance.","However, when it is used for multi-organ segmentation, first, it may be limited in exploiting global long-range contextual information due to the implementation of standard convolutions.","Second, the use of spatial-wise downsampling (e.g., max pooling or strided convolutions) in the encoding path may lead to the loss of deformable or discriminative details.","Third, features upsampled from the higher level are concatenated with those that persevered via skip connections.","However, repeated downsampling and upsampling operations lead to misalignments between them and their concatenation degrades segmentation performance.","To address these limitations, we propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated Downsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules, respectively.","The DCC module can utilize global inter-dependencies between spatial and channel features to calibrate these features adaptively.","The DCD module enables networks to adaptively preserve deformable or discriminative features during downsampling.","The DCU module can dynamically align and calibrate upsampled features to eliminate misalignments before concatenations.","We integrated the proposed modules into a standard U-Net, resulting in a new architecture, termed Dynamic U-Net.","This architectural design enables U-Net to dynamically adjust features for different organs.","We evaluated Dynamic U-Net in two abdominal multi-organ segmentation benchmarks.","Dynamic U-Net achieved statistically improved segmentation accuracy compared with standard U-Net.","Our code is available at https://github.com/sotiraslab/DynamicUNet."],"url":"http://arxiv.org/abs/2403.07303v1","category":"eess.IV"}
{"created":"2024-03-12 04:07:00","title":"Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller","abstract":"Storytelling aims to generate reasonable and vivid narratives based on an ordered image stream. The fidelity to the image story theme and the divergence of story plots attract readers to keep reading. Previous works iteratively improved the alignment of multiple modalities but ultimately resulted in the generation of simplistic storylines for image streams. In this work, we propose a new pipeline, termed LLaMS, to generate multimodal human-level stories that are embodied in expressiveness and consistency. Specifically, by fully exploiting the commonsense knowledge within the LLM, we first employ a sequence data auto-enhancement strategy to enhance factual content expression and leverage a textual reasoning architecture for expressive story generation and prediction. Secondly, we propose SQ-Adatpter module for story illustration generation which can maintain sequence consistency. Numerical results are conducted through human evaluation to verify the superiority of proposed LLaMS. Evaluations show that LLaMS achieves state-of-the-art storytelling performance and 86% correlation and 100% consistency win rate as compared with previous SOTA methods. Furthermore, ablation experiments are conducted to verify the effectiveness of proposed sequence data enhancement and SQ-Adapter.","sentences":["Storytelling aims to generate reasonable and vivid narratives based on an ordered image stream.","The fidelity to the image story theme and the divergence of story plots attract readers to keep reading.","Previous works iteratively improved the alignment of multiple modalities but ultimately resulted in the generation of simplistic storylines for image streams.","In this work, we propose a new pipeline, termed LLaMS, to generate multimodal human-level stories that are embodied in expressiveness and consistency.","Specifically, by fully exploiting the commonsense knowledge within the LLM, we first employ a sequence data auto-enhancement strategy to enhance factual content expression and leverage a textual reasoning architecture for expressive story generation and prediction.","Secondly, we propose SQ-Adatpter module for story illustration generation which can maintain sequence consistency.","Numerical results are conducted through human evaluation to verify the superiority of proposed LLaMS.","Evaluations show that LLaMS achieves state-of-the-art storytelling performance and 86% correlation and 100% consistency win rate as compared with previous SOTA methods.","Furthermore, ablation experiments are conducted to verify the effectiveness of proposed sequence data enhancement and SQ-Adapter."],"url":"http://arxiv.org/abs/2403.07301v1","category":"cs.CV"}
{"created":"2024-03-12 03:44:40","title":"Rediscovering BCE Loss for Uniform Classification","abstract":"This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample. We also propose the uniform classification accuracy as a metric to measure the model's performance in uniform classification. Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias. We demonstrate the unified threshold could be learned via the bias. The extensive experiments on six classification datasets and three feature extraction models show that, compared to the SoftMax loss, the models trained with the BCE loss not only exhibit higher uniform classification accuracy but also higher sample-wise classification accuracy. In addition, the learned bias from BCE loss is very close to the unified threshold used in the uniform classification. The features extracted by the models trained with BCE loss not only possess uniformity but also demonstrate better intra-class compactness and inter-class distinctiveness, yielding superior performance on open-set tasks such as face recognition.","sentences":["This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample.","We also propose the uniform classification accuracy as a metric to measure the model's performance in uniform classification.","Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias.","We demonstrate the unified threshold could be learned via the bias.","The extensive experiments on six classification datasets and three feature extraction models show that, compared to the SoftMax loss, the models trained with the BCE loss not only exhibit higher uniform classification accuracy but also higher sample-wise classification accuracy.","In addition, the learned bias from BCE loss is very close to the unified threshold used in the uniform classification.","The features extracted by the models trained with BCE loss not only possess uniformity but also demonstrate better intra-class compactness and inter-class distinctiveness, yielding superior performance on open-set tasks such as face recognition."],"url":"http://arxiv.org/abs/2403.07289v1","category":"cs.CV"}
{"created":"2024-03-12 03:30:04","title":"A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism","abstract":"As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios.","sentences":["As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services.","Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy.","In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk.","With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods.","Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings.","To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios."],"url":"http://arxiv.org/abs/2403.07283v1","category":"cs.CR"}
{"created":"2024-03-12 01:05:25","title":"It's All About Your Sketch: Democratising Sketch Control in Diffusion Models","abstract":"This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of \"what you sketch is what you get\". A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.","sentences":["This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI.","We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of \"what you sketch is what you get\".","A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning.","To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association.","Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices!","We welcome everyone to examine results presented in the paper and its supplementary.","Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments."],"url":"http://arxiv.org/abs/2403.07234v1","category":"cs.CV"}
{"created":"2024-03-12 00:08:54","title":"Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control","abstract":"The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller. Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight. The primary goal of this controller is to minimize tracking error while following a specified trajectory. The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics. The results show that the adaptive gain scheme achieves over 40$\\%$ decrease in tracking error as compared to the static gain controller.","sentences":["The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller.","Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight.","The primary goal of this controller is to minimize tracking error while following a specified trajectory.","The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics.","The results show that the adaptive gain scheme achieves over 40$\\%$ decrease in tracking error as compared to the static gain controller."],"url":"http://arxiv.org/abs/2403.07216v1","category":"eess.SY"}
{"created":"2024-03-11 22:47:26","title":"Computing $p$-presentation distances is hard","abstract":"Recently, $p$-presentation distances for $p\\in [1,\\infty]$ were introduced for merge trees and multiparameter persistence modules as more sensitive variations of the respective interleaving distances ($p=\\infty$). It is well-known that computing the interleaving distance is NP-hard in both cases. We extend this result by showing that computing the $p$-presentation distance is NP-hard for all $p\\in [1,\\infty)$ for both merge trees and $t$-parameter persistence modules for any $t\\geq 2$. Though the details differ, both proofs follow the same novel strategy, suggesting that our approach can be adapted to proving the NP-hardness of other distances based on sums or $p$-norms.","sentences":["Recently, $p$-presentation distances for $p\\in [1,\\infty]$ were introduced for merge trees and multiparameter persistence modules as more sensitive variations of the respective interleaving distances ($p=\\infty$).","It is well-known that computing the interleaving distance is NP-hard in both cases.","We extend this result by showing that computing the $p$-presentation distance is NP-hard for all $p\\in [1,\\infty)$ for both merge trees and $t$-parameter persistence modules for any $t\\geq 2$.","Though the details differ, both proofs follow the same novel strategy, suggesting that our approach can be adapted to proving the NP-hardness of other distances based on sums or $p$-norms."],"url":"http://arxiv.org/abs/2403.07200v1","category":"cs.CG"}
{"created":"2024-03-11 22:26:45","title":"Accelerating Interface Adaptation with User-Friendly Priors","abstract":"Robots often need to convey information to human users. For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state. In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down. But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive. In this paper we enable robots to adapt their interfaces to the current user so that the human's personalized interpretation is aligned with the robot's meaning. We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot. But to ensure that this learning policy is intuitive -- and to accelerate how quickly the interface adapts to the human -- we recognize that humans have priors over how interfaces should function. For instance, humans expect interface signals to be proportional and convex. Our approach biases the robot's interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations. Our simulations and user study results across $15$ participants suggest that these priors improve robot-to-human communication. See videos here: https://youtu.be/Re3OLg57hp8","sentences":["Robots often need to convey information to human users.","For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state.","In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down.","But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive.","In this paper we enable robots to adapt their interfaces to the current user so that the human's personalized interpretation is aligned with the robot's meaning.","We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot.","But to ensure that this learning policy is intuitive -- and to accelerate how quickly the interface adapts to the human -- we recognize that humans have priors over how interfaces should function.","For instance, humans expect interface signals to be proportional and convex.","Our approach biases the robot's interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations.","Our simulations and user study results across $15$ participants suggest that these priors improve robot-to-human communication.","See videos here: https://youtu.be/Re3OLg57hp8"],"url":"http://arxiv.org/abs/2403.07192v1","category":"cs.RO"}
{"created":"2024-03-11 21:45:48","title":"Study of the Impact of the Big Data Era on Accounting and Auditing","abstract":"Big data revolutionizes accounting and auditing, offering deep insights but also introducing challenges like data privacy and security. With data from IoT, social media, and transactions, traditional practices are evolving. Professionals must adapt to these changes, utilizing AI and machine learning for efficient data analysis and anomaly detection. Key to overcoming these challenges are enhanced analytics tools, continuous learning, and industry collaboration. By addressing these areas, the accounting and auditing fields can harness big data's potential while ensuring accuracy, transparency, and integrity in financial reporting. Keywords: Big Data, Accounting, Audit, Data Privacy, AI, Machine Learning, Transparency.","sentences":["Big data revolutionizes accounting and auditing, offering deep insights but also introducing challenges like data privacy and security.","With data from IoT, social media, and transactions, traditional practices are evolving.","Professionals must adapt to these changes, utilizing AI and machine learning for efficient data analysis and anomaly detection.","Key to overcoming these challenges are enhanced analytics tools, continuous learning, and industry collaboration.","By addressing these areas, the accounting and auditing fields can harness big data's potential while ensuring accuracy, transparency, and integrity in financial reporting.","Keywords: Big Data, Accounting, Audit, Data Privacy, AI, Machine Learning, Transparency."],"url":"http://arxiv.org/abs/2403.07180v1","category":"q-fin.GN"}
{"created":"2024-03-11 21:38:41","title":"Collusive Outcomes Without Collusion","abstract":"We develop a model of algorithmic pricing that shuts down every channel for explicit or implicit collusion while still generating collusive outcomes. We analyze the dynamics of a duopoly market where both firms use pricing algorithms consisting of a parameterized family of model specifications. The firms update both the parameters and the weights on models to adapt endogenously to market outcomes. We show that the market experiences recurrent episodes where both firms set prices at collusive levels. We analytically characterize the dynamics of the model, using large deviation theory to explain the recurrent episodes of collusive outcomes. Our results show that collusive outcomes may be a recurrent feature of algorithmic environments with complementarities and endogenous adaptation, providing a challenge for competition policy.","sentences":["We develop a model of algorithmic pricing that shuts down every channel for explicit or implicit collusion while still generating collusive outcomes.","We analyze the dynamics of a duopoly market where both firms use pricing algorithms consisting of a parameterized family of model specifications.","The firms update both the parameters and the weights on models to adapt endogenously to market outcomes.","We show that the market experiences recurrent episodes where both firms set prices at collusive levels.","We analytically characterize the dynamics of the model, using large deviation theory to explain the recurrent episodes of collusive outcomes.","Our results show that collusive outcomes may be a recurrent feature of algorithmic environments with complementarities and endogenous adaptation, providing a challenge for competition policy."],"url":"http://arxiv.org/abs/2403.07177v1","category":"econ.TH"}
{"created":"2024-03-11 17:59:34","title":"VideoMamba: State Space Model for Efficient Video Understanding","abstract":"Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain. The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers. Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding. Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding. All the code and models are available at https://github.com/OpenGVLab/VideoMamba.","sentences":["Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain.","The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers.","Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding.","Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts.","Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding.","All the code and models are available at https://github.com/OpenGVLab/VideoMamba."],"url":"http://arxiv.org/abs/2403.06977v2","category":"cs.CV"}
{"created":"2024-03-11 17:59:31","title":"BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion","abstract":"Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence.","sentences":["Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs).","Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality.","Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches.","This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion.","Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes.","Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment.","Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence."],"url":"http://arxiv.org/abs/2403.06976v1","category":"cs.CV"}
{"created":"2024-03-11 17:33:12","title":"Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation","abstract":"Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS","sentences":["Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task.","Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities.","In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation.","Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components.","Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances.","We align features across domains using a modality discriminator.","Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs.","Code: https://github.com/TL-UESTC/UniMoS"],"url":"http://arxiv.org/abs/2403.06946v1","category":"cs.CV"}
{"created":"2024-03-11 17:17:18","title":"Heavy Ball Momentum for Non-Strongly Convex Optimization","abstract":"When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see Polyak [32], Nesterov [28], and references therein). By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error. We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition. In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions. To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes. No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known. This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown.","sentences":["When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see","Polyak","[32], Nesterov","[28], and references therein).","By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error.","We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition.","In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions.","To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes.","No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known.","This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown."],"url":"http://arxiv.org/abs/2403.06930v1","category":"math.OC"}
{"created":"2024-03-11 17:05:16","title":"Energy dissipation in earthquakes","abstract":"Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves. The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process. Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales. We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics.","sentences":["Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves.","The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process.","Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales.","We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics."],"url":"http://arxiv.org/abs/2403.06916v1","category":"physics.geo-ph"}
{"created":"2024-03-11 16:55:19","title":"Deep adaptative spectral zoom for improved remote heart rate estimation","abstract":"Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy. However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal. While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation. This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator. The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets. This is achieved through a Sparse Matrix Optimization (SMO). We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics. The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method.","sentences":["Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy.","However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal.","While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution.","In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation.","This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator.","The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets.","This is achieved through a Sparse Matrix Optimization (SMO).","We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics.","The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method."],"url":"http://arxiv.org/abs/2403.06902v1","category":"cs.CV"}
{"created":"2024-03-11 16:47:09","title":"Application of Quantum Tensor Networks for Protein Classification","abstract":"We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity.","sentences":["We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems.","We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms.","Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences.","We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results.","We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above.","Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings.","It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters.","We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity."],"url":"http://arxiv.org/abs/2403.06890v1","category":"quant-ph"}
{"created":"2024-03-11 16:31:25","title":"SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection","abstract":"We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/","sentences":["We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures.","This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals.","We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss.","We use submapping to scale the system to large-scale environments captured over long trajectories.","We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building.","Website: https://ori-drs.github.io/projects/silvr/"],"url":"http://arxiv.org/abs/2403.06877v1","category":"cs.RO"}
{"created":"2024-03-11 16:26:35","title":"COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification","abstract":"High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset. SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize. Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability. The framework can easily be extended or adapted to other tasks and media modalities.","sentences":["High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models.","In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality.","We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model.","The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind.","COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection.","We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.","SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize.","Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability.","The framework can easily be extended or adapted to other tasks and media modalities."],"url":"http://arxiv.org/abs/2403.06874v1","category":"cs.CV"}
{"created":"2024-03-11 16:23:38","title":"Semantic Residual Prompts for Continual Learning","abstract":"Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a second pool. The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers. Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test. Notably, our findings hold true even for datasets with a substantial domain gap w.r.t. the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets.","sentences":["Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts.","Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values).","However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches.","For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts.","To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism.","Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes.","The second level, instead, uses these prototypes along with the query image as keys to index a second pool.","The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity.","In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers.","Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test.","Notably, our findings hold true even for datasets with a substantial domain gap w.r.t.","the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets."],"url":"http://arxiv.org/abs/2403.06870v1","category":"cs.LG"}
{"created":"2024-03-11 16:08:22","title":"Human-Exoskeleton Interaction Portrait","abstract":"Human-robot physical interaction contains crucial information for optimizing user experience, enhancing robot performance, and objectively assessing user adaptation. This study introduces a new method to evaluate human-robot co-adaptation in lower limb exoskeletons by analyzing muscle activity and interaction torque as a two-dimensional random variable. We introduce the Interaction Portrait (IP), which visualizes this variable's distribution in polar coordinates. We applied this metric to compare a recent torque controller (HTC) based on kinematic state feedback and a novel feedforward controller (AMTC) with online learning, proposed herein, against a time-based controller (TBC) during treadmill walking at varying speeds. Compared to TBC, both HTC and AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced user-exoskeleton coordination. IP analysis reveals this improvement stems from two distinct co-adaptation strategies, unidentifiable by traditional muscle activity or interaction torque analyses alone. HTC encourages users to yield control to the exoskeleton, decreasing muscular effort but increasing interaction torque, as the exoskeleton compensates for user dynamics. Conversely, AMTC promotes user engagement through increased muscular effort and reduced interaction torques, aligning it more closely with rehabilitation and gait training applications. IP phase evolution provides insight into each user's interaction strategy development, showcasing IP analysis's potential in comparing and designing novel controllers to optimize human-robot interaction in wearable robots.","sentences":["Human-robot physical interaction contains crucial information for optimizing user experience, enhancing robot performance, and objectively assessing user adaptation.","This study introduces a new method to evaluate human-robot co-adaptation in lower limb exoskeletons by analyzing muscle activity and interaction torque as a two-dimensional random variable.","We introduce the Interaction Portrait (IP), which visualizes this variable's distribution in polar coordinates.","We applied this metric to compare a recent torque controller (HTC) based on kinematic state feedback and a novel feedforward controller (AMTC) with online learning, proposed herein, against a time-based controller (TBC) during treadmill walking at varying speeds.","Compared to TBC, both HTC and AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced user-exoskeleton coordination.","IP analysis reveals this improvement stems from two distinct co-adaptation strategies, unidentifiable by traditional muscle activity or interaction torque analyses alone.","HTC encourages users to yield control to the exoskeleton, decreasing muscular effort but increasing interaction torque, as the exoskeleton compensates for user dynamics.","Conversely, AMTC promotes user engagement through increased muscular effort and reduced interaction torques, aligning it more closely with rehabilitation and gait training applications.","IP phase evolution provides insight into each user's interaction strategy development, showcasing IP analysis's potential in comparing and designing novel controllers to optimize human-robot interaction in wearable robots."],"url":"http://arxiv.org/abs/2403.06851v1","category":"cs.RO"}
{"created":"2024-03-11 15:48:17","title":"HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution","abstract":"High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images. Caused by large motion and severe under-/over-exposure among input LDR images, HDR imaging suffers from ghosting artifacts and fusion distortions. To address these critical issues, we propose an HDR Transformer Deformation Convolution (HDRTransDC) network to generate high-quality HDR images, which consists of the Transformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic Weight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM extracts long-distance content similar to the reference feature in the entire non-reference features, which can accurately remove misalignment and fill the content occluded by moving objects. For the purpose of eliminating fusion distortions, we propose DWFB to spatially adaptively select useful information across frames to effectively fuse multi-exposed features. Extensive experiments show that our method quantitatively and qualitatively achieves state-of-the-art performance.","sentences":["High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.","Caused by large motion and severe under-/over-exposure among input LDR images, HDR imaging suffers from ghosting artifacts and fusion distortions.","To address these critical issues, we propose an HDR Transformer Deformation Convolution (HDRTransDC) network to generate high-quality HDR images, which consists of the Transformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic Weight Fusion Block (DWFB).","To solve the ghosting artifacts, the proposed TDCAM extracts long-distance content similar to the reference feature in the entire non-reference features, which can accurately remove misalignment and fill the content occluded by moving objects.","For the purpose of eliminating fusion distortions, we propose DWFB to spatially adaptively select useful information across frames to effectively fuse multi-exposed features.","Extensive experiments show that our method quantitatively and qualitatively achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.06831v1","category":"cs.CV"}
{"created":"2024-03-11 15:33:40","title":"\u03b5-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment","abstract":"Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed bandits (CMAB) in general lead to better sample efficiency compared to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation. Moreover, an {\\epsilon}-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called {\\epsilon}-Neural Thompson sampling ({\\epsilon}-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment. The {\\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients' brains. The results show that our method outperforms both existing cDBS methods and CMAB baselines.","sentences":["Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD).","Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).","However, they in general suffer from energy inefficiency and side effects, such as speech impairment.","Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS.","Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy.","However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS.","In contrast, contextual multi-armed bandits (CMAB) in general lead to better sample efficiency compared to RL.","In this study, we propose a CMAB solution for aDBS.","Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation.","Moreover, an {\\epsilon}-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called {\\epsilon}-Neural Thompson sampling ({\\epsilon}-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment.","The {\\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients' brains.","The results show that our method outperforms both existing cDBS methods and CMAB baselines."],"url":"http://arxiv.org/abs/2403.06814v1","category":"cs.LG"}
{"created":"2024-03-11 15:33:32","title":"LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations","abstract":"Contrastive instance discrimination outperforms supervised learning in downstream tasks like image classification and object detection. However, this approach heavily relies on data augmentation during representation learning, which may result in inferior results if not properly implemented. Random cropping followed by resizing is a common form of data augmentation used in contrastive learning, but it can lead to degraded representation learning if the two random crops contain distinct semantic content. To address this issue, this paper introduces LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Representations), a framework that employs a new instance discrimination approach and an adapted loss function that ensures the shared region between positive pairs is semantically correct. The experimental results show that our approach consistently improves representation learning across different datasets compared to baseline models. For example, our approach outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several other methods on transfer learning tasks.","sentences":["Contrastive instance discrimination outperforms supervised learning in downstream tasks like image classification and object detection.","However, this approach heavily relies on data augmentation during representation learning, which may result in inferior results if not properly implemented.","Random cropping followed by resizing is a common form of data augmentation used in contrastive learning, but it can lead to degraded representation learning if the two random crops contain distinct semantic content.","To address this issue, this paper introduces LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Representations), a framework that employs a new instance discrimination approach and an adapted loss function that ensures the shared region between positive pairs is semantically correct.","The experimental results show that our approach consistently improves representation learning across different datasets compared to baseline models.","For example, our approach outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several other methods on transfer learning tasks."],"url":"http://arxiv.org/abs/2403.06813v1","category":"cs.CV"}
{"created":"2024-03-11 14:48:57","title":"From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing","abstract":"This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing. It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets. It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization. These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images. In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models. This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance.","sentences":["This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing.","It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets.","It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization.","These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images.","In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models.","This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance."],"url":"http://arxiv.org/abs/2403.06779v1","category":"q-fin.ST"}
{"created":"2024-03-11 14:20:13","title":"Generalising Multi-Agent Cooperation through Task-Agnostic Communication","abstract":"Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder. Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-distribution events in an environment. Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks. Our implementation of this work is available at https://github.com/proroklab/task-agnostic-comms.","sentences":["Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task.","We address this inefficiency by introducing a communication strategy applicable to any task within a given environment.","We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder.","Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations.","Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation.","Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-distribution events in an environment.","Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks.","Our implementation of this work is available at https://github.com/proroklab/task-agnostic-comms."],"url":"http://arxiv.org/abs/2403.06750v1","category":"cs.MA"}
{"created":"2024-03-11 13:36:00","title":"HILL: A Hallucination Identifier for Large Language Models","abstract":"Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL's interface design by surveying 17 participants. Further, we investigated HILL's functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.","sentences":["Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text.","Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors.","To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\".","First, we identified design features for HILL with a Wizard of Oz approach with nine participants.","Subsequently, we implemented HILL based on the identified design features and evaluated HILL's interface design by surveying 17 participants.","Further, we investigated HILL's functionality to identify hallucinations based on an existing question-answering dataset and five user interviews.","We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution.","With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts."],"url":"http://arxiv.org/abs/2403.06710v1","category":"cs.HC"}
{"created":"2024-03-11 12:56:36","title":"Trustworthy Partial Label Learning with Out-of-distribution Detection","abstract":"Partial Label Learning (PLL) grapples with learning from ambiguously labelled data, and it has been successfully applied in fields such as image recognition. Nevertheless, traditional PLL methods rely on the closed-world assumption, which can be limiting in open-world scenarios and negatively impact model performance and generalization. To tackle these challenges, our study introduces a novel method called PLL-OOD, which is the first to incorporate Out-of-Distribution (OOD) detection into the PLL framework. PLL-OOD significantly enhances model adaptability and accuracy by merging self-supervised learning with partial label loss and pioneering the Partial-Energy (PE) score for OOD detection. This approach improves data feature representation and effectively disambiguates candidate labels, using a dynamic label confidence matrix to refine predictions. The PE score, adjusted by label confidence, precisely identifies OOD instances, optimizing model training towards in-distribution data. This innovative method markedly boosts PLL model robustness and performance in open-world settings. To validate our approach, we conducted a comprehensive comparative experiment combining the existing state-of-the-art PLL model with multiple OOD scores on the CIFAR-10 and CIFAR-100 datasets with various OOD datasets. The results demonstrate that the proposed PLL-OOD framework is highly effective and effectiveness outperforms existing models, showcasing its superiority and effectiveness.","sentences":["Partial Label Learning (PLL) grapples with learning from ambiguously labelled data, and it has been successfully applied in fields such as image recognition.","Nevertheless, traditional PLL methods rely on the closed-world assumption, which can be limiting in open-world scenarios and negatively impact model performance and generalization.","To tackle these challenges, our study introduces a novel method called PLL-OOD, which is the first to incorporate Out-of-Distribution (OOD) detection into the PLL framework.","PLL-OOD significantly enhances model adaptability and accuracy by merging self-supervised learning with partial label loss and pioneering the Partial-Energy (PE) score for OOD detection.","This approach improves data feature representation and effectively disambiguates candidate labels, using a dynamic label confidence matrix to refine predictions.","The PE score, adjusted by label confidence, precisely identifies OOD instances, optimizing model training towards in-distribution data.","This innovative method markedly boosts PLL model robustness and performance in open-world settings.","To validate our approach, we conducted a comprehensive comparative experiment combining the existing state-of-the-art PLL model with multiple OOD scores on the CIFAR-10 and CIFAR-100 datasets with various OOD datasets.","The results demonstrate that the proposed PLL-OOD framework is highly effective and effectiveness outperforms existing models, showcasing its superiority and effectiveness."],"url":"http://arxiv.org/abs/2403.06681v1","category":"cs.CV"}
{"created":"2024-03-11 12:51:37","title":"Answering Diverse Questions via Text Attached with Key Audio-Visual Clues","abstract":"Audio-visual question answering (AVQA) requires reference to video content and auditory information, followed by correlating the question to predict the most precise answer. Although mining deeper layers of audio-visual information to interact with questions facilitates the multimodal fusion process, the redundancy of audio-visual parameters tends to reduce the generalization of the inference engine to multiple question-answer pairs in a single video. Indeed, the natural heterogeneous relationship between audiovisuals and text makes the perfect fusion challenging, to prevent high-level audio-visual semantics from weakening the network's adaptability to diverse question types, we propose a framework for performing mutual correlation distillation (MCD) to aid question inference. MCD is divided into three main steps: 1) firstly, the residual structure is utilized to enhance the audio-visual soft associations based on self-attention, then key local audio-visual features relevant to the question context are captured hierarchically by shared aggregators and coupled in the form of clues with specific question vectors. 2) Secondly, knowledge distillation is enforced to align audio-visual-text pairs in a shared latent space to narrow the cross-modal semantic gap. 3) And finally, the audio-visual dependencies are decoupled by discarding the decision-level integrations. We evaluate the proposed method on two publicly available datasets containing multiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show that our method outperforms other state-of-the-art methods, and one interesting finding behind is that removing deep audio-visual features during inference can effectively mitigate overfitting. The source code is released at http://github.com/rikeilong/MCD-forAVQA.","sentences":["Audio-visual question answering (AVQA) requires reference to video content and auditory information, followed by correlating the question to predict the most precise answer.","Although mining deeper layers of audio-visual information to interact with questions facilitates the multimodal fusion process, the redundancy of audio-visual parameters tends to reduce the generalization of the inference engine to multiple question-answer pairs in a single video.","Indeed, the natural heterogeneous relationship between audiovisuals and text makes the perfect fusion challenging, to prevent high-level audio-visual semantics from weakening the network's adaptability to diverse question types, we propose a framework for performing mutual correlation distillation (MCD) to aid question inference.","MCD is divided into three main steps: 1) firstly, the residual structure is utilized to enhance the audio-visual soft associations based on self-attention, then key local audio-visual features relevant to the question context are captured hierarchically by shared aggregators and coupled in the form of clues with specific question vectors.","2) Secondly, knowledge distillation is enforced to align audio-visual-text pairs in a shared latent space to narrow the cross-modal semantic gap.","3) And finally, the audio-visual dependencies are decoupled by discarding the decision-level integrations.","We evaluate the proposed method on two publicly available datasets containing multiple question-and-answer pairs, i.e., Music-AVQA and AVQA.","Experiments show that our method outperforms other state-of-the-art methods, and one interesting finding behind is that removing deep audio-visual features during inference can effectively mitigate overfitting.","The source code is released at http://github.com/rikeilong/MCD-forAVQA."],"url":"http://arxiv.org/abs/2403.06679v1","category":"cs.CV"}
{"created":"2024-03-11 12:27:20","title":"Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework","abstract":"Large vision models based in deep learning architectures have been consistently advancing the state-of-the-art in biometric recognition. However, three weaknesses are commonly reported for such kind of approaches: 1) their extreme demands in terms of learning data; 2) the difficulties in generalising between different domains; and 3) the lack of interpretability/explainability, with biometrics being of particular interest, as it is important to provide evidence able to be used for forensics/legal purposes (e.g., in courts). To the best of our knowledge, this paper describes the first recognition framework/strategy that aims at addressing the three weaknesses simultaneously. At first, it relies exclusively in synthetic samples for learning purposes. Instead of requiring a large amount and variety of samples for each subject, the idea is to exclusively enroll a 3D point cloud per identity. Then, using generative strategies, we synthesize a very large (potentially infinite) number of samples, containing all the desired covariates (poses, clothing, distances, perspectives, lighting, occlusions,...). Upon the synthesizing method used, it is possible to adapt precisely to different kind of domains, which accounts for generalization purposes. Such data are then used to learn a model that performs local registration between image pairs, establishing positive correspondences between body parts that are the key, not only to recognition (according to cardinality and distribution), but also to provide an interpretable description of the response (e.g.: \"both samples are from the same person, as they have similar facial shape, hair color and legs thickness\").","sentences":["Large vision models based in deep learning architectures have been consistently advancing the state-of-the-art in biometric recognition.","However, three weaknesses are commonly reported for such kind of approaches: 1) their extreme demands in terms of learning data; 2) the difficulties in generalising between different domains; and 3) the lack of interpretability/explainability, with biometrics being of particular interest, as it is important to provide evidence able to be used for forensics/legal purposes (e.g., in courts).","To the best of our knowledge, this paper describes the first recognition framework/strategy that aims at addressing the three weaknesses simultaneously.","At first, it relies exclusively in synthetic samples for learning purposes.","Instead of requiring a large amount and variety of samples for each subject, the idea is to exclusively enroll a 3D point cloud per identity.","Then, using generative strategies, we synthesize a very large (potentially infinite) number of samples, containing all the desired covariates (poses, clothing, distances, perspectives, lighting, occlusions,...).","Upon the synthesizing method used, it is possible to adapt precisely to different kind of domains, which accounts for generalization purposes.","Such data are then used to learn a model that performs local registration between image pairs, establishing positive correspondences between body parts that are the key, not only to recognition (according to cardinality and distribution), but also to provide an interpretable description of the response (e.g.: \"both samples are from the same person, as they have similar facial shape, hair color and legs thickness\")."],"url":"http://arxiv.org/abs/2403.06658v1","category":"cs.CV"}
{"created":"2024-03-11 12:11:19","title":"Ray Launching-Based Computation of Exact Paths with Noisy Dense Point Clouds","abstract":"Point clouds have been a recent interest for ray tracing-based radio channel characterization, as sensors such as RGB-D cameras and laser scanners can be utilized to generate an accurate virtual copy of a physical environment. In this paper, a novel ray launching algorithm is presented, which operates directly on noisy point clouds acquired from sensor data. It produces coarse paths that are further refined to exact paths consisting of reflections and diffractions. A commercial ray tracing tool is utilized as the baseline for validating the simulated paths. A significant majority of the baseline paths is found. The robustness to noise is examined by artificially applying noise along the normal vector of each point. It is observed that the proposed method is capable of adapting to noise and finds similar paths compared to the baseline path trajectories with noisy point clouds. This is prevalent especially if the normal vectors of the points are estimated accurately. Lastly, a simulation is performed with a reconstructed point cloud and compared against channel measurements and the baseline paths. The resulting paths demonstrate similarity with the baseline path trajectories and exhibit an analogous pattern to the aggregated impulse response extracted from the measurements. Code available at https://github.com/nvaara/NimbusRT","sentences":["Point clouds have been a recent interest for ray tracing-based radio channel characterization, as sensors such as RGB-D cameras and laser scanners can be utilized to generate an accurate virtual copy of a physical environment.","In this paper, a novel ray launching algorithm is presented, which operates directly on noisy point clouds acquired from sensor data.","It produces coarse paths that are further refined to exact paths consisting of reflections and diffractions.","A commercial ray tracing tool is utilized as the baseline for validating the simulated paths.","A significant majority of the baseline paths is found.","The robustness to noise is examined by artificially applying noise along the normal vector of each point.","It is observed that the proposed method is capable of adapting to noise and finds similar paths compared to the baseline path trajectories with noisy point clouds.","This is prevalent especially if the normal vectors of the points are estimated accurately.","Lastly, a simulation is performed with a reconstructed point cloud and compared against channel measurements and the baseline paths.","The resulting paths demonstrate similarity with the baseline path trajectories and exhibit an analogous pattern to the aggregated impulse response extracted from the measurements.","Code available at https://github.com/nvaara/NimbusRT"],"url":"http://arxiv.org/abs/2403.06648v1","category":"eess.SP"}
{"created":"2024-03-11 11:58:27","title":"Robust and fast backbone tracking via phase-locked loops","abstract":"Phase-locked loops are commonly used for shaker-based backbone tracking of nonlinear structures. The state of the art is to tune the control parameters by trial and error. In the present work, an approach is proposed to make backbone tracking much more robust and faster. A simple PI controller is proposed, and closed-form expressions for the gains are provided that lead to an optimal settling of the phase transient. The required input parameters are obtained from a conventional shaker-based linear modal test, and an open-loop sine test at a single frequency and level. For phase detection, an adaptive filter based on the LMS algorithm is used, which is shown to be superior to the synchronous demodulation commonly used. Once the phase has locked, one can directly take the next step along the backbone, eliminating the hold times. The latter are currently used for recording the steady state, and to estimate Fourier coefficients in the post-process, which becomes unnecessary since the adaptive filter yields a highly accurate estimation at runtime. The excellent performance of the proposed approach is demonstrated for a doubly clamped beam undergoing bending-stretching coupling leading to a 20 percent shift of the lowest modal frequency. Even for fixed control parameters, designed for the linear regime, only about 100 periods are needed per backbone point, also in the nonlinear regime. This is much faster than what has been reported in the literature so far. In fact, the nonlinear backbone test becomes faster than the linear modal test, shifting the established paradigm.","sentences":["Phase-locked loops are commonly used for shaker-based backbone tracking of nonlinear structures.","The state of the art is to tune the control parameters by trial and error.","In the present work, an approach is proposed to make backbone tracking much more robust and faster.","A simple PI controller is proposed, and closed-form expressions for the gains are provided that lead to an optimal settling of the phase transient.","The required input parameters are obtained from a conventional shaker-based linear modal test, and an open-loop sine test at a single frequency and level.","For phase detection, an adaptive filter based on the LMS algorithm is used, which is shown to be superior to the synchronous demodulation commonly used.","Once the phase has locked, one can directly take the next step along the backbone, eliminating the hold times.","The latter are currently used for recording the steady state, and to estimate Fourier coefficients in the post-process, which becomes unnecessary since the adaptive filter yields a highly accurate estimation at runtime.","The excellent performance of the proposed approach is demonstrated for a doubly clamped beam undergoing bending-stretching coupling leading to a 20 percent shift of the lowest modal frequency.","Even for fixed control parameters, designed for the linear regime, only about 100 periods are needed per backbone point, also in the nonlinear regime.","This is much faster than what has been reported in the literature so far.","In fact, the nonlinear backbone test becomes faster than the linear modal test, shifting the established paradigm."],"url":"http://arxiv.org/abs/2403.06639v1","category":"eess.SY"}
{"created":"2024-03-11 11:51:42","title":"Design and Control of Delta: Deformable Multilinked Multirotor with Rolling Locomotion Ability in Terrestrial Domain","abstract":"In recent years, multiple types of locomotion methods for robots have been developed and enabled to adapt to multiple domains. In particular, aerial robots are useful for exploration in several situations, taking advantage of its three-dimensional mobility. Moreover, some aerial robots have achieved manipulation tasks in the air. However, energy consumption for flight is large and thus locomotion ability on the ground is also necessary for aerial robots to do tasks for long time. Therefore, in this work, we aim to develop deformable multirotor robot capable of rolling movement with its entire body and achieve motions on the ground and in the air. In this paper, we first describe the design methodology of a deformable multilinked air-ground hybrid multirotor. We also introduce its mechanical design and rotor configuration based on control stability. Then, thrust control method for locomotion in air and ground domains is described. Finally, we show the implemented prototype of the proposed robot and evaluate through experiments in air and terrestrial domains. To the best of our knowledge, this is the first time to achieve the rolling locomotion by multilink structured mutltrotor.","sentences":["In recent years, multiple types of locomotion methods for robots have been developed and enabled to adapt to multiple domains.","In particular, aerial robots are useful for exploration in several situations, taking advantage of its three-dimensional mobility.","Moreover, some aerial robots have achieved manipulation tasks in the air.","However, energy consumption for flight is large and thus locomotion ability on the ground is also necessary for aerial robots to do tasks for long time.","Therefore, in this work, we aim to develop deformable multirotor robot capable of rolling movement with its entire body and achieve motions on the ground and in the air.","In this paper, we first describe the design methodology of a deformable multilinked air-ground hybrid multirotor.","We also introduce its mechanical design and rotor configuration based on control stability.","Then, thrust control method for locomotion in air and ground domains is described.","Finally, we show the implemented prototype of the proposed robot and evaluate through experiments in air and terrestrial domains.","To the best of our knowledge, this is the first time to achieve the rolling locomotion by multilink structured mutltrotor."],"url":"http://arxiv.org/abs/2403.06636v1","category":"cs.RO"}
{"created":"2024-03-11 11:12:38","title":"Mapping a dissipative quantum spin chain onto a generalized Coulomb gas","abstract":"An XXZ spin chain at zero magnetization is subject to spatially correlated baths acting as dissipation. We show that the low-energy excitations of this model are described by a dissipative sine-Gordon field theory, i.e. a sine-Gordon action with an additional long-range interaction emerging from dissipation. The field theory is then exactly mapped onto a generalized Coulomb gas which, in addition to the usual integer charges, displays half-integer charges that originate from the dissipative baths. These new charges come in pairs linked by a charge-independent logarithmic interaction. In the Coulomb gas picture, we identify a Berezinsky-Kosterlitz-Thouless-like phase transition corresponding to the binding of charges and derive the associated perturbative renormalization group equations. For superohmic baths, the transition is due to the binding of the integer charges, while for subohmic baths, it is due to the binding of the half-integer charges, thereby signaling a dissipation-induced transition.","sentences":["An XXZ spin chain at zero magnetization is subject to spatially correlated baths acting as dissipation.","We show that the low-energy excitations of this model are described by a dissipative sine-Gordon field theory, i.e. a sine-Gordon action with an additional long-range interaction emerging from dissipation.","The field theory is then exactly mapped onto a generalized Coulomb gas which, in addition to the usual integer charges, displays half-integer charges that originate from the dissipative baths.","These new charges come in pairs linked by a charge-independent logarithmic interaction.","In the Coulomb gas picture, we identify a Berezinsky-Kosterlitz-Thouless-like phase transition corresponding to the binding of charges and derive the associated perturbative renormalization group equations.","For superohmic baths, the transition is due to the binding of the integer charges, while for subohmic baths, it is due to the binding of the half-integer charges, thereby signaling a dissipation-induced transition."],"url":"http://arxiv.org/abs/2403.06618v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 11:06:41","title":"Density-Guided Label Smoothing for Temporal Localization of Driving Actions","abstract":"Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies. However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization. In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization. To this end, we first develop a density-guided label smoothing technique based on label probability distributions to facilitate better learning from boundary video-segments that typically include multiple labels. Second, we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives. Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271.","sentences":["Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies.","However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization.","In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization.","To this end, we first develop a density-guided label smoothing technique based on label probability distributions to facilitate better learning from boundary video-segments that typically include multiple labels.","Second, we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives.","Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271."],"url":"http://arxiv.org/abs/2403.06616v1","category":"cs.CV"}
{"created":"2024-03-11 10:35:33","title":"HDA-LVIO: A High-Precision LiDAR-Visual-Inertial Odometry in Urban Environments with Hybrid Data Association","abstract":"To enhance localization accuracy in urban environments, an innovative LiDAR-Visual-Inertial odometry, named HDA-LVIO, is proposed by employing hybrid data association. The proposed HDA_LVIO system can be divided into two subsystems: the LiDAR-Inertial subsystem (LIS) and the Visual-Inertial subsystem (VIS). In the LIS, the LiDAR pointcloud is utilized to calculate the Iterative Closest Point (ICP) error, serving as the measurement value of Error State Iterated Kalman Filter (ESIKF) to construct the global map. In the VIS, an incremental method is firstly employed to adaptively extract planes from the global map. And the centroids of these planes are projected onto the image to obtain projection points. Then, feature points are extracted from the image and tracked along with projection points using Lucas-Kanade (LK) optical flow. Next, leveraging the vehicle states from previous intervals, sliding window optimization is performed to estimate the depth of feature points. Concurrently, a method based on epipolar geometric constraints is proposed to address tracking failures for feature points, which can improve the accuracy of depth estimation for feature points by ensuring sufficient parallax within the sliding window. Subsequently, the feature points and projection points are hybridly associated to construct reprojection error, serving as the measurement value of ESIKF to estimate vehicle states. Finally, the localization accuracy of the proposed HDA-LVIO is validated using public datasets and data from our equipment. The results demonstrate that the proposed algorithm achieves obviously improvement in localization accuracy compared to various existing algorithms.","sentences":["To enhance localization accuracy in urban environments, an innovative LiDAR-Visual-Inertial odometry, named HDA-LVIO, is proposed by employing hybrid data association.","The proposed HDA_LVIO system can be divided into two subsystems: the LiDAR-Inertial subsystem (LIS) and the Visual-Inertial subsystem (VIS).","In the LIS, the LiDAR pointcloud is utilized to calculate the Iterative Closest Point (ICP) error, serving as the measurement value of Error State Iterated Kalman Filter (ESIKF) to construct the global map.","In the VIS, an incremental method is firstly employed to adaptively extract planes from the global map.","And the centroids of these planes are projected onto the image to obtain projection points.","Then, feature points are extracted from the image and tracked along with projection points using Lucas-Kanade (LK) optical flow.","Next, leveraging the vehicle states from previous intervals, sliding window optimization is performed to estimate the depth of feature points.","Concurrently, a method based on epipolar geometric constraints is proposed to address tracking failures for feature points, which can improve the accuracy of depth estimation for feature points by ensuring sufficient parallax within the sliding window.","Subsequently, the feature points and projection points are hybridly associated to construct reprojection error, serving as the measurement value of ESIKF to estimate vehicle states.","Finally, the localization accuracy of the proposed HDA-LVIO is validated using public datasets and data from our equipment.","The results demonstrate that the proposed algorithm achieves obviously improvement in localization accuracy compared to various existing algorithms."],"url":"http://arxiv.org/abs/2403.06590v1","category":"cs.RO"}
{"created":"2024-03-11 10:27:36","title":"DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification","abstract":"The surge in popularity of machine learning (ML) has driven significant investments in training Deep Neural Networks (DNNs). However, these models that require resource-intensive training are vulnerable to theft and unauthorized use. This paper addresses this challenge by introducing DNNShield, a novel approach for DNN protection that integrates seamlessly before training. DNNShield embeds unique identifiers within the model architecture using specialized protection layers. These layers enable secure training and deployment while offering high resilience against various attacks, including fine-tuning, pruning, and adaptive adversarial attacks. Notably, our approach achieves this security with minimal performance and computational overhead (less than 5\\% runtime increase). We validate the effectiveness and efficiency of DNNShield through extensive evaluations across three datasets and four model architectures. This practical solution empowers developers to protect their DNNs and intellectual property rights.","sentences":["The surge in popularity of machine learning (ML) has driven significant investments in training Deep Neural Networks (DNNs).","However, these models that require resource-intensive training are vulnerable to theft and unauthorized use.","This paper addresses this challenge by introducing DNNShield, a novel approach for DNN protection that integrates seamlessly before training.","DNNShield embeds unique identifiers within the model architecture using specialized protection layers.","These layers enable secure training and deployment while offering high resilience against various attacks, including fine-tuning, pruning, and adaptive adversarial attacks.","Notably, our approach achieves this security with minimal performance and computational overhead (less than 5\\% runtime increase).","We validate the effectiveness and efficiency of DNNShield through extensive evaluations across three datasets and four model architectures.","This practical solution empowers developers to protect their DNNs and intellectual property rights."],"url":"http://arxiv.org/abs/2403.06581v1","category":"cs.CR"}
{"created":"2024-03-11 10:26:38","title":"Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition","abstract":"Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies. Temporal localization is challenging because it requires robustness, reliability, and accuracy. In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model. Therefore, we design a transformer-based fusion architecture to effectively combine 2D-pose features and spatio-temporal features. The model uses 2D-pose features as the positional embedding of the transformer architecture and spatio-temporal features as the main input to the encoder of the transformer. The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output. Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives. The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079.","sentences":["Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies.","Temporal localization is challenging because it requires robustness, reliability, and accuracy.","In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model.","Therefore, we design a transformer-based fusion architecture to effectively combine 2D-pose features and spatio-temporal features.","The model uses 2D-pose features as the positional embedding of the transformer architecture and spatio-temporal features as the main input to the encoder of the transformer.","The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output.","Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives.","The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079."],"url":"http://arxiv.org/abs/2403.06577v1","category":"cs.CV"}
{"created":"2024-03-11 10:20:44","title":"Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings","abstract":"Mastering autonomous drone landing on dynamic platforms presents formidable challenges due to unpredictable velocities and external disturbances caused by the wind, ground effect, turbines or propellers of the docking platform. This study introduces an advanced Deep Reinforcement Learning (DRL) agent, Lander:AI, designed to navigate and land on platforms in the presence of windy conditions, thereby enhancing drone autonomy and safety. Lander:AI is rigorously trained within the gym-pybullet-drone simulation, an environment that mirrors real-world complexities, including wind turbulence, to ensure the agent's robustness and adaptability.   The agent's capabilities were empirically validated with Crazyflie 2.1 drones across various test scenarios, encompassing both simulated environments and real-world conditions. The experimental results showcased Lander:AI's high-precision landing and its ability to adapt to moving platforms, even under wind-induced disturbances. Furthermore, the system performance was benchmarked against a baseline PID controller augmented with an Extended Kalman Filter, illustrating significant improvements in landing precision and error recovery. Lander:AI leverages bio-inspired learning to adapt to external forces like birds, enhancing drone adaptability without knowing force magnitudes.This research not only advances drone landing technologies, essential for inspection and emergency applications, but also highlights the potential of DRL in addressing intricate aerodynamic challenges.","sentences":["Mastering autonomous drone landing on dynamic platforms presents formidable challenges due to unpredictable velocities and external disturbances caused by the wind, ground effect, turbines or propellers of the docking platform.","This study introduces an advanced Deep Reinforcement Learning (DRL) agent, Lander:AI, designed to navigate and land on platforms in the presence of windy conditions, thereby enhancing drone autonomy and safety.","Lander:AI is rigorously trained within the gym-pybullet-drone simulation, an environment that mirrors real-world complexities, including wind turbulence, to ensure the agent's robustness and adaptability.   ","The agent's capabilities were empirically validated with Crazyflie 2.1 drones across various test scenarios, encompassing both simulated environments and real-world conditions.","The experimental results showcased Lander:AI's high-precision landing and its ability to adapt to moving platforms, even under wind-induced disturbances.","Furthermore, the system performance was benchmarked against a baseline PID controller augmented with an Extended Kalman Filter, illustrating significant improvements in landing precision and error recovery.","Lander:AI leverages bio-inspired learning to adapt to external forces like birds, enhancing drone adaptability without knowing force magnitudes.","This research not only advances drone landing technologies, essential for inspection and emergency applications, but also highlights the potential of DRL in addressing intricate aerodynamic challenges."],"url":"http://arxiv.org/abs/2403.06572v2","category":"cs.RO"}
{"created":"2024-03-11 10:10:45","title":"Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming","abstract":"Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech and amputee mobility.","sentences":["Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide.","The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients.","A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb.","However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects.","To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters.","With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees.","The findings in this study have significant implications for advancing assistive tech and amputee mobility."],"url":"http://arxiv.org/abs/2403.06569v2","category":"cs.LG"}
{"created":"2024-03-11 09:57:56","title":"Unconditional deep-water limit of the intermediate long wave equation in low-regularity","abstract":"In this paper, we establish the unconditional deep-water limit of the intermediate long wave equation (ILW) to the Benjamin-Ono equation (BO) in low-regularity Sobolev spaces on both the real line and the circle. Our main tool is new unconditional uniqueness results for ILW in $H^s$ when $s_0<s\\leq \\frac 14$ on the line and $s_0<s< \\frac 12$ on the circle, where $s_0 = 3-\\sqrt{33/4}\\approx 0.1277$. Here, we adapt the strategy of Mo\\c{s}incat-Pilod (2023) for BO to the setting of ILW by viewing ILW as a perturbation of BO and making use of the smoothing property of the perturbation term.","sentences":["In this paper, we establish the unconditional deep-water limit of the intermediate long wave equation (ILW) to the Benjamin-Ono equation (BO) in low-regularity Sobolev spaces on both the real line and the circle.","Our main tool is new unconditional uniqueness results for ILW in $H^s$ when $s_0<s\\leq","\\frac 14$ on the line and $s_0<s< \\frac 12$ on the circle, where $s_0 = 3-\\sqrt{33/4}\\approx 0.1277$. Here, we adapt the strategy of Mo\\c{s}incat-Pilod (2023) for BO to the setting of ILW by viewing ILW as a perturbation of BO and making use of the smoothing property of the perturbation term."],"url":"http://arxiv.org/abs/2403.06554v1","category":"math.AP"}
{"created":"2024-03-11 09:52:32","title":"ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval","abstract":"Tool learning aims to extend the capabilities of large language models (LLMs) with external tools. A major challenge in tool learning is how to support a large number of tools, including unseen tools. To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query. However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval. Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results. Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries. Experimental results show that ToolRerank can improve the quality of the retrieval results, leading to better execution results generated by the LLM.","sentences":["Tool learning aims to extend the capabilities of large language models (LLMs) with external tools.","A major challenge in tool learning is how to support a large number of tools, including unseen tools.","To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query.","However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval.","Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results.","Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries.","Experimental results show that ToolRerank can improve the quality of the retrieval results, leading to better execution results generated by the LLM."],"url":"http://arxiv.org/abs/2403.06551v1","category":"cs.IR"}
{"created":"2024-03-11 09:51:02","title":"Fine boundary continuity for degenerate double-phase diffusion","abstract":"We study the boundary behavior of solutions to parabolic double-phase equations through the celebrated Wiener's sufficiency criterion. The analysis is conducted for cylindrical domains and the regularity up to the lateral boundary is shown in terms of either its $p$ or $q$ capacity, depending on whether the phase vanishes at the boundary or not. Eventually we obtain a fine boundary estimate that, when considering uniform geometric conditions as density or fatness, leads us to the boundary H\\\"older continuity of solutions. In particular, the double-phase elicits new questions on the definition of an adapted capacity.","sentences":["We study the boundary behavior of solutions to parabolic double-phase equations through the celebrated Wiener's sufficiency criterion.","The analysis is conducted for cylindrical domains and the regularity up to the lateral boundary is shown in terms of either its $p$ or $q$ capacity, depending on whether the phase vanishes at the boundary or not.","Eventually we obtain a fine boundary estimate that, when considering uniform geometric conditions as density or fatness, leads us to the boundary H\\\"older continuity of solutions.","In particular, the double-phase elicits new questions on the definition of an adapted capacity."],"url":"http://arxiv.org/abs/2403.06550v1","category":"math.AP"}
{"created":"2024-03-11 09:47:21","title":"Fun Maximizing Search, (Non) Instance Optimality, and Video Games for Parrots","abstract":"Computerized Adaptive Testing (CAT) measures an examinee's ability while adapting to their level. Both too many questions and too many hard questions can make a test frustrating. Are there some CAT algorithms which can be proven to be theoretically better than others, and in which framework? We show that slightly extending the traditional framework yields a partial order on CAT algorithms. For uni-dimensional knowledge domains, we analyze the theoretical performance of some old and new algorithms, and we prove that none of the algorithms presented are instance optimal, conjecturing that no instance optimal can exist for the CAT problem.","sentences":["Computerized Adaptive Testing (CAT) measures an examinee's ability while adapting to their level.","Both too many questions and too many hard questions can make a test frustrating.","Are there some CAT algorithms which can be proven to be theoretically better than others, and in which framework?","We show that slightly extending the traditional framework yields a partial order on CAT algorithms.","For uni-dimensional knowledge domains, we analyze the theoretical performance of some old and new algorithms, and we prove that none of the algorithms presented are instance optimal, conjecturing that no instance optimal can exist for the CAT problem."],"url":"http://arxiv.org/abs/2403.06547v1","category":"cs.DS"}
{"created":"2024-03-11 09:24:06","title":"On the Consideration of AI Openness: Can Good Intent Be Abused?","abstract":"Openness is critical for the advancement of science. In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries. However, this openness also means that technologies can be freely used for socially harmful purposes. Can open-source models or datasets be used for malicious purposes? If so, how easy is it to adapt technology for such goals? Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences. To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents. We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities. This implies that although open-source technologies contribute to scientific progress, some care must be taken to mitigate possible malicious use cases. Warning: This paper contains contents that some may find unethical.","sentences":["Openness is critical for the advancement of science.","In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries.","However, this openness also means that technologies can be freely used for socially harmful purposes.","Can open-source models or datasets be used for malicious purposes?","If so, how easy is it to adapt technology for such goals?","Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences.","To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents.","We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities.","This implies that although open-source technologies contribute to scientific progress, some care must be taken to mitigate possible malicious use cases.","Warning:","This paper contains contents that some may find unethical."],"url":"http://arxiv.org/abs/2403.06537v1","category":"cs.CL"}
{"created":"2024-03-11 09:23:20","title":"Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution","abstract":"Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.","sentences":["Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR).","Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance.","Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA).","Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging.","MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance.","Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network.","We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks."],"url":"http://arxiv.org/abs/2403.06536v1","category":"cs.CV"}
{"created":"2024-03-11 09:21:11","title":"Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning","abstract":"Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs. To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption. To further augment the system's expressive capabilities and computational efficiency, we apply algorithm unrolling, leveraging the advantages of both mathematical optimization and neural networks. This allows the agents to `learn to collaborate' through the supervision of training tasks. Our theoretical analysis verifies that inter-agent collaboration is communication efficient under a small number of communication rounds. The experimental results verify its ability to facilitate the discovery of collaboration strategies and adaptation to dynamic learning scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in classification accuracy. We expect our work can serve as a foundational technique to facilitate future works towards an intelligent, decentralized, and dynamic multi-agent system. Code is available at https://github.com/ShuoTang123/DeLAMA.","sentences":["Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time.","To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations.","In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs.","To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors.","To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption.","To further augment the system's expressive capabilities and computational efficiency, we apply algorithm unrolling, leveraging the advantages of both mathematical optimization and neural networks.","This allows the agents to `learn to collaborate' through the supervision of training tasks.","Our theoretical analysis verifies that inter-agent collaboration is communication efficient under a small number of communication rounds.","The experimental results verify its ability to facilitate the discovery of collaboration strategies and adaptation to dynamic learning scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in classification accuracy.","We expect our work can serve as a foundational technique to facilitate future works towards an intelligent, decentralized, and dynamic multi-agent system.","Code is available at https://github.com/ShuoTang123/DeLAMA."],"url":"http://arxiv.org/abs/2403.06535v1","category":"cs.LG"}
{"created":"2024-03-11 09:12:24","title":"Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis","abstract":"2D face recognition encounters challenges in unconstrained environments due to varying illumination, occlusion, and pose. Recent studies focus on RGB-D face recognition to improve robustness by incorporating depth information. However, collecting sufficient paired RGB-D training data is expensive and time-consuming, hindering wide deployment. In this work, we first construct a diverse depth dataset generated by 3D Morphable Models for depth model pre-training. Then, we propose a domain-independent pre-training framework that utilizes readily available pre-trained RGB and depth models to separately perform face recognition without needing additional paired data for retraining. To seamlessly integrate the two distinct networks and harness the complementary benefits of RGB and depth information for improved accuracy, we propose an innovative Adaptive Confidence Weighting (ACW). This mechanism is designed to learn confidence estimates for each modality to achieve modality fusion at the score level. Our method is simple and lightweight, only requiring ACW training beyond the backbone models. Experiments on multiple public RGB-D face recognition benchmarks demonstrate state-of-the-art performance surpassing previous methods based on depth estimation and feature fusion, validating the efficacy of our approach.","sentences":["2D face recognition encounters challenges in unconstrained environments due to varying illumination, occlusion, and pose.","Recent studies focus on RGB-D face recognition to improve robustness by incorporating depth information.","However, collecting sufficient paired RGB-D training data is expensive and time-consuming, hindering wide deployment.","In this work, we first construct a diverse depth dataset generated by 3D Morphable Models for depth model pre-training.","Then, we propose a domain-independent pre-training framework that utilizes readily available pre-trained RGB and depth models to separately perform face recognition without needing additional paired data for retraining.","To seamlessly integrate the two distinct networks and harness the complementary benefits of RGB and depth information for improved accuracy, we propose an innovative Adaptive Confidence Weighting (ACW).","This mechanism is designed to learn confidence estimates for each modality to achieve modality fusion at the score level.","Our method is simple and lightweight, only requiring ACW training beyond the backbone models.","Experiments on multiple public RGB-D face recognition benchmarks demonstrate state-of-the-art performance surpassing previous methods based on depth estimation and feature fusion, validating the efficacy of our approach."],"url":"http://arxiv.org/abs/2403.06529v1","category":"cs.CV"}
{"created":"2024-03-11 09:10:37","title":"Adaptive Federated Learning Over the Air","abstract":"We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role in the training efficiency: the heavier the tail, the slower the algorithm converges. In contrast, an Adam-like algorithm converges at the $\\mathcal{O}( 1/T )$ rate, demonstrating its advantage in expediting the model training process. We conduct extensive experiments that corroborate our theoretical findings and affirm the practical efficacy of our proposed federated adaptive gradient methods.","sentences":["We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training.","This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation.","Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update.","We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions.","Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the electromagnetic interference.","This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role in the training efficiency: the heavier the tail, the slower the algorithm converges.","In contrast, an Adam-like algorithm converges at the $\\mathcal{O}( 1/T )$ rate, demonstrating its advantage in expediting the model training process.","We conduct extensive experiments that corroborate our theoretical findings and affirm the practical efficacy of our proposed federated adaptive gradient methods."],"url":"http://arxiv.org/abs/2403.06528v1","category":"cs.LG"}
{"created":"2024-03-11 08:58:42","title":"Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward","abstract":"We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.","sentences":["We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario.","Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models.","In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques."],"url":"http://arxiv.org/abs/2403.06524v1","category":"cs.LG"}
{"created":"2024-03-11 08:43:57","title":"Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning","abstract":"Recent advances in text-conditioned image generation diffusion models have begun paving the way for new opportunities in modern medical domain, in particular, generating Chest X-rays (CXRs) from diagnostic reports. Nonetheless, to further drive the diffusion models to generate CXRs that faithfully reflect the complexity and diversity of real data, it has become evident that a nontrivial learning approach is needed. In light of this, we propose CXRL, a framework motivated by the potential of reinforcement learning (RL). Specifically, we integrate a policy gradient RL approach with well-designed multiple distinctive CXR-domain specific reward models. This approach guides the diffusion denoising trajectory, achieving precise CXR posture and pathological details. Here, considering the complex medical image environment, we present \"RL with Comparative Feedback\" (RLCF) for the reward mechanism, a human-like comparative evaluation that is known to be more effective and reliable in complex scenarios compared to direct evaluation. Our CXRL framework includes jointly optimizing learnable adaptive condition embeddings (ACE) and the image generator, enabling the model to produce more accurate and higher perceptual CXR quality. Our extensive evaluation of the MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning approach. Consequently, our CXRL generates pathologically realistic CXRs, establishing a new standard for generating CXRs with high fidelity to real-world clinical scenarios.","sentences":["Recent advances in text-conditioned image generation diffusion models have begun paving the way for new opportunities in modern medical domain, in particular, generating Chest X-rays (CXRs) from diagnostic reports.","Nonetheless, to further drive the diffusion models to generate CXRs that faithfully reflect the complexity and diversity of real data, it has become evident that a nontrivial learning approach is needed.","In light of this, we propose CXRL, a framework motivated by the potential of reinforcement learning (RL).","Specifically, we integrate a policy gradient RL approach with well-designed multiple distinctive CXR-domain specific reward models.","This approach guides the diffusion denoising trajectory, achieving precise CXR posture and pathological details.","Here, considering the complex medical image environment, we present \"RL with Comparative Feedback\" (RLCF) for the reward mechanism, a human-like comparative evaluation that is known to be more effective and reliable in complex scenarios compared to direct evaluation.","Our CXRL framework includes jointly optimizing learnable adaptive condition embeddings (ACE) and the image generator, enabling the model to produce more accurate and higher perceptual CXR quality.","Our extensive evaluation of the MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning approach.","Consequently, our CXRL generates pathologically realistic CXRs, establishing a new standard for generating CXRs with high fidelity to real-world clinical scenarios."],"url":"http://arxiv.org/abs/2403.06516v1","category":"cs.CV"}
{"created":"2024-03-11 08:09:30","title":"QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning","abstract":"Transformer-based models have gained widespread popularity in both the computer vision (CV) and natural language processing (NLP) fields. However, significant challenges arise during post-training linear quantization, leading to noticeable reductions in inference accuracy. Our study focuses on uncovering the underlying causes of these accuracy drops and proposing a quantization-friendly fine-tuning method, \\textbf{QuantTune}. Firstly, our analysis revealed that, on average, 65\\% of quantization errors result from the precision loss incurred by the dynamic range amplification effect of outliers across the target Transformer-based models. Secondly, \\textbf{QuantTune} adjusts weights based on the deviation of outlier activations and effectively constrains the dynamic ranges of the problematic activations. As a result, it successfully mitigates the negative impact of outliers on the inference accuracy of quantized models. Lastly, \\textbf{QuantTune} can be seamlessly integrated into the back-propagation pass in the fine-tuning process without requiring extra complexity in inference software and hardware design. Our approach showcases significant improvements in post-training quantization across a range of Transformer-based models, including ViT, Bert-base, and OPT. QuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at 7-bit compared to top calibration methods, outperforming state-of-the-art solutions by over 18.84\\% across ViT models.","sentences":["Transformer-based models have gained widespread popularity in both the computer vision (CV) and natural language processing (NLP) fields.","However, significant challenges arise during post-training linear quantization, leading to noticeable reductions in inference accuracy.","Our study focuses on uncovering the underlying causes of these accuracy drops and proposing a quantization-friendly fine-tuning method, \\textbf{QuantTune}.","Firstly, our analysis revealed that, on average, 65\\% of quantization errors result from the precision loss incurred by the dynamic range amplification effect of outliers across the target Transformer-based models.","Secondly, \\textbf{QuantTune} adjusts weights based on the deviation of outlier activations and effectively constrains the dynamic ranges of the problematic activations.","As a result, it successfully mitigates the negative impact of outliers on the inference accuracy of quantized models.","Lastly, \\textbf{QuantTune} can be seamlessly integrated into the back-propagation pass in the fine-tuning process without requiring extra complexity in inference software and hardware design.","Our approach showcases significant improvements in post-training quantization across a range of Transformer-based models, including ViT, Bert-base, and OPT.","QuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at 7-bit compared to top calibration methods, outperforming state-of-the-art solutions by over 18.84\\% across ViT models."],"url":"http://arxiv.org/abs/2403.06497v1","category":"cs.CV"}
{"created":"2024-03-11 07:42:40","title":"Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching","abstract":"Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at https://github.com/wrld/Ada-Tracker.","sentences":["Soft tissue tracking is crucial for computer-assisted interventions.","Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches.","However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery.","To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template.","Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames.","To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates.","Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation.","We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets.","The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works.","Code is available at https://github.com/wrld/Ada-Tracker."],"url":"http://arxiv.org/abs/2403.06479v1","category":"cs.CV"}
{"created":"2024-03-11 07:40:00","title":"Repetitive Infection Spreading and Directed Evolution in the Susceptible-Infected-Recovered-Susceptible Model","abstract":"We study two simple mathematical models of the epidemic. At first, we study the repetitive infection spreading in a simplified SIRS model including the effect of the decay of the acquired immune. The model is an intermediate model of the SIRS model including the recruitment and death terms and the SIR model in which the recovered population is assumed to be never infected again. When the decay rate \\delta of the immune is sufficiently small, the multiple infection spreading occurs in spikes. The model equation can be reduced to be a map when the decay rate \\delta is sufficiently small, and the spike-like multiple infection spreading is reproduced in the mapping. The period-doubling bifurcation and chaos are found in the simplified SIRS model with seasonal variation. The nonlinear phenomena are reproduced by the map. Next, we study coupled SIRS equations for the directed evolution where the mutation is expressed with a diffusion-type term. A kind of reaction-diffusion equation is derived by the continuum approximation for the infected population I. The reaction-diffusion equation with the linear dependence of infection rate on the type space has an exact Gaussian solution with a time-dependent average and variance. The propagation of the Gaussian pulse corresponds to the successive transitions of the dominant variant.","sentences":["We study two simple mathematical models of the epidemic.","At first, we study the repetitive infection spreading in a simplified SIRS model including the effect of the decay of the acquired immune.","The model is an intermediate model of the SIRS model including the recruitment and death terms and the SIR model in which the recovered population is assumed to be never infected again.","When the decay rate \\delta of the immune is sufficiently small, the multiple infection spreading occurs in spikes.","The model equation can be reduced to be a map when the decay rate \\delta is sufficiently small, and the spike-like multiple infection spreading is reproduced in the mapping.","The period-doubling bifurcation and chaos are found in the simplified SIRS model with seasonal variation.","The nonlinear phenomena are reproduced by the map.","Next, we study coupled SIRS equations for the directed evolution where the mutation is expressed with a diffusion-type term.","A kind of reaction-diffusion equation is derived by the continuum approximation for the infected population I.","The reaction-diffusion equation with the linear dependence of infection rate on the type space has an exact Gaussian solution with a time-dependent average and variance.","The propagation of the Gaussian pulse corresponds to the successive transitions of the dominant variant."],"url":"http://arxiv.org/abs/2403.07034v1","category":"q-bio.PE"}
{"created":"2024-03-11 06:56:08","title":"Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation","abstract":"Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an unlabeled target domain by leveraging the complementary multi-modal inputs in an online manner. Previous MM-TTA methods rely on predictions of cross-modal information in each input frame, while they ignore the fact that predictions of geometric neighborhoods within consecutive frames are highly correlated, leading to unstable predictions across time. To fulfill this gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages reliable cross-modal spatial-temporal correspondences for multi-modal 3D segmentation. Motivated by the fact that reliable predictions should be consistent with their spatial-temporal correspondences, Latte aggregates consecutive frames in a slide window manner and constructs ST voxel to capture temporally local prediction consistency for each modality. After filtering out ST voxels with high ST entropy, Latte conducts cross-modal learning for each point and pixel by attending to those with reliable and consistent predictions among both spatial and temporal neighborhoods. Experimental results show that Latte achieves state-of-the-art performance on three different MM-TTA benchmarks compared to previous MM-TTA or TTA methods.","sentences":["Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an unlabeled target domain by leveraging the complementary multi-modal inputs in an online manner.","Previous MM-TTA methods rely on predictions of cross-modal information in each input frame, while they ignore the fact that predictions of geometric neighborhoods within consecutive frames are highly correlated, leading to unstable predictions across time.","To fulfill this gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages reliable cross-modal spatial-temporal correspondences for multi-modal 3D segmentation.","Motivated by the fact that reliable predictions should be consistent with their spatial-temporal correspondences, Latte aggregates consecutive frames in a slide window manner and constructs ST voxel to capture temporally local prediction consistency for each modality.","After filtering out ST voxels with high ST entropy, Latte conducts cross-modal learning for each point and pixel by attending to those with reliable and consistent predictions among both spatial and temporal neighborhoods.","Experimental results show that Latte achieves state-of-the-art performance on three different MM-TTA benchmarks compared to previous MM-TTA or TTA methods."],"url":"http://arxiv.org/abs/2403.06461v1","category":"cs.CV"}
{"created":"2024-03-11 06:08:16","title":"FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications","abstract":"Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge. While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains. To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large vision-language model with typographical knowledge. We integrate typography-specific knowledge into the comprehensive vision-language knowledge of a pretrained CLIP model through a novel finetuning approach. We propose to use a compound descriptive prompt that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters. FontCLIP's semantic typographic latent space demonstrates two unprecedented generalization abilities. First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only finetuned using fonts of Roman characters. Second, FontCLIP can recognize the semantic attributes that are not presented in the training data. FontCLIP's dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts.","sentences":["Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge.","While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains.","To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large vision-language model with typographical knowledge.","We integrate typography-specific knowledge into the comprehensive vision-language knowledge of a pretrained CLIP model through a novel finetuning approach.","We propose to use a compound descriptive prompt that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters.","FontCLIP's semantic typographic latent space demonstrates two unprecedented generalization abilities.","First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only finetuned using fonts of Roman characters.","Second, FontCLIP can recognize the semantic attributes that are not presented in the training data.","FontCLIP's dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts."],"url":"http://arxiv.org/abs/2403.06453v1","category":"cs.CV"}
{"created":"2024-03-11 05:24:52","title":"Symmetry Hierarchy and Thermalization Frustration in Graphene Nanoresonators","abstract":"As the essential cause of the intrinsic dissipation that limits the quality of graphene nanoresonators, intermodal energy transfer is also a key issue in thermalization dynamics. Typically systems with larger initial energy demand shorter time to be thermalized. However, we find quantitatively that instead of becoming shorter, the equipartition time of the graphene nanoresonator can increase abruptly by one order of magnitude. This thermalization frustration emerges due to the partition of the normal modes based on the hierarchical symmetry, and a sensitive on-off switching of the energy flow channels between symmetry classes controlled by Mathieu instabilities. The results uncover the decisive roles of symmetry in the thermalization at the nanoscale, and may also lead to strategies for improving the performance of graphene nanoresonators.","sentences":["As the essential cause of the intrinsic dissipation that limits the quality of graphene nanoresonators, intermodal energy transfer is also a key issue in thermalization dynamics.","Typically systems with larger initial energy demand shorter time to be thermalized.","However, we find quantitatively that instead of becoming shorter, the equipartition time of the graphene nanoresonator can increase abruptly by one order of magnitude.","This thermalization frustration emerges due to the partition of the normal modes based on the hierarchical symmetry, and a sensitive on-off switching of the energy flow channels between symmetry classes controlled by Mathieu instabilities.","The results uncover the decisive roles of symmetry in the thermalization at the nanoscale, and may also lead to strategies for improving the performance of graphene nanoresonators."],"url":"http://arxiv.org/abs/2403.06442v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 04:56:10","title":"STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow","abstract":"Scene flow prediction is a crucial underlying task in understanding dynamic scenes as it offers fundamental motion information. However, contemporary scene flow methods encounter three major challenges. Firstly, flow estimation solely based on local receptive fields lacks long-dependency matching of point pairs. To address this issue, we propose global attentive flow embedding to match all-to-all point pairs in both feature space and Euclidean space, providing global initialization before local refinement. Secondly, there are deformations existing in non-rigid objects after warping, which leads to variations in the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow, a spatial temporal feature re-embedding module is devised to acquire the sequence features after deformation. Furthermore, previous methods perform poor generalization due to the significant domain gap between the synthesized and LiDAR-scanned datasets. We leverage novel domain adaptive losses to effectively bridge the gap of motion inference from synthetic to real-world. Experiments demonstrate that our approach achieves state-of-the-art performance across various datasets, with particularly outstanding results on real-world LiDAR-scanned datasets. Our code is available at https://github.com/O-VIGIA/StarFlow.","sentences":["Scene flow prediction is a crucial underlying task in understanding dynamic scenes as it offers fundamental motion information.","However, contemporary scene flow methods encounter three major challenges.","Firstly, flow estimation solely based on local receptive fields lacks long-dependency matching of point pairs.","To address this issue, we propose global attentive flow embedding to match all-to-all point pairs in both feature space and Euclidean space, providing global initialization before local refinement.","Secondly, there are deformations existing in non-rigid objects after warping, which leads to variations in the spatiotemporal relation between the consecutive frames.","For a more precise estimation of residual flow, a spatial temporal feature re-embedding module is devised to acquire the sequence features after deformation.","Furthermore, previous methods perform poor generalization due to the significant domain gap between the synthesized and LiDAR-scanned datasets.","We leverage novel domain adaptive losses to effectively bridge the gap of motion inference from synthetic to real-world.","Experiments demonstrate that our approach achieves state-of-the-art performance across various datasets, with particularly outstanding results on real-world LiDAR-scanned datasets.","Our code is available at https://github.com/O-VIGIA/StarFlow."],"url":"http://arxiv.org/abs/2403.07032v1","category":"cs.CV"}
{"created":"2024-03-11 04:44:26","title":"AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration","abstract":"Deep learning-based face restoration models, increasingly prevalent in smart devices, have become targets for sophisticated backdoor attacks. These attacks, through subtle trigger injection into input face images, can lead to unexpected restoration outcomes. Unlike conventional methods focused on classification tasks, our approach introduces a unique degradation objective tailored for attacking restoration models. Moreover, we propose the Adaptive Selective Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural network for input-specific trigger generation in the frequency domain, seamlessly blending triggers with benign images. This results in imperceptible yet effective attacks, guiding restoration predictions towards subtly degraded outputs rather than conspicuous targets. Extensive experiments demonstrate the efficacy of the degradation objective on state-of-the-art face restoration models. Additionally, it is notable that AS-FIBA can insert effective backdoors that are more imperceptible than existing backdoor attack methods, including WaNet, ISSBA, and FIBA.","sentences":["Deep learning-based face restoration models, increasingly prevalent in smart devices, have become targets for sophisticated backdoor attacks.","These attacks, through subtle trigger injection into input face images, can lead to unexpected restoration outcomes.","Unlike conventional methods focused on classification tasks, our approach introduces a unique degradation objective tailored for attacking restoration models.","Moreover, we propose the Adaptive Selective Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural network for input-specific trigger generation in the frequency domain, seamlessly blending triggers with benign images.","This results in imperceptible yet effective attacks, guiding restoration predictions towards subtly degraded outputs rather than conspicuous targets.","Extensive experiments demonstrate the efficacy of the degradation objective on state-of-the-art face restoration models.","Additionally, it is notable that AS-FIBA can insert effective backdoors that are more imperceptible than existing backdoor attack methods, including WaNet, ISSBA, and FIBA."],"url":"http://arxiv.org/abs/2403.06430v1","category":"cs.CV"}
{"created":"2024-03-11 04:25:41","title":"Bridging Domains with Approximately Shared Features","abstract":"Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains. A fundamental challenge is devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features. To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across domains. Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task. Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both source and target tasks, thus partly resolving the paradox mentioned above. Inspired by our theory, we proposed a more practical way to isolate the content (invariant+approximately shared) from environmental features and further consolidate our theoretical findings.","sentences":["Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains.","A fundamental challenge is devising the optimal strategy for feature selection.","Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features.","To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across domains.","Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task.","Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both source and target tasks, thus partly resolving the paradox mentioned above.","Inspired by our theory, we proposed a more practical way to isolate the content (invariant+approximately shared) from environmental features and further consolidate our theoretical findings."],"url":"http://arxiv.org/abs/2403.06424v1","category":"stat.ML"}
{"created":"2024-03-11 03:38:48","title":"Can LLMs' Tuning Methods Work in Medical Multimodal Domain?","abstract":"While large language models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments. Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization. To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs). In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks. Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency? In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on existing multimodal models in the medical domain from the training data level and the model structure level. We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models. We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields. Code and dataset will be released upon acceptance.","sentences":["While large language models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments.","Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization.","To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs).","In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks.","Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency?","In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on existing multimodal models in the medical domain from the training data level and the model structure level.","We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models.","We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields.","Code and dataset will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.06407v1","category":"cs.CV"}
{"created":"2024-03-11 03:28:20","title":"PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models","abstract":"Recent success of vision foundation models have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks. PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames. Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation. Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models. Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks. PointSeg demonstrates impressive segmentation performance across various datasets, all without training. Specifically, our approach significantly surpasses the state-of-the-art specialist model by 13.4$\\%$, 11.3$\\%$, and 12$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top of that, PointSeg can incorporate with various segmentation models and even surpasses the supervised methods.","sentences":["Recent success of vision foundation models have shown promising performance for the 2D perception tasks.","However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly.","In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks.","PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames.","Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation.","Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models.","Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks.","PointSeg demonstrates impressive segmentation performance across various datasets, all without training.","Specifically, our approach significantly surpasses the state-of-the-art specialist model by 13.4$\\%$, 11.3$\\%$, and 12$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively.","On top of that, PointSeg can incorporate with various segmentation models and even surpasses the supervised methods."],"url":"http://arxiv.org/abs/2403.06403v1","category":"cs.CV"}
{"created":"2024-03-11 03:28:13","title":"'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification","abstract":"Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data instance is predicted by the Softmax posteriors of a classifier. The parameters of this classifier are fitted on the optimal number of examples in ICL required to correctly infer the label of each instance in the training set with the hypothesis that a test instance that is similar to a training instance should use the same (or a closely matching) number of few-shot examples. Our experiments show that our AICL method results in improvement in text classification task on several standard datasets.","sentences":["Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data.","An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts).","An important component of ICL is the use of a small number of labelled data instances as examples in the prompt.","While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data.","This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier.","In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data instance is predicted by the Softmax posteriors of a classifier.","The parameters of this classifier are fitted on the optimal number of examples in ICL required to correctly infer the label of each instance in the training set with the hypothesis that a test instance that is similar to a training instance should use the same (or a closely matching) number of few-shot examples.","Our experiments show that our AICL method results in improvement in text classification task on several standard datasets."],"url":"http://arxiv.org/abs/2403.06402v1","category":"cs.CL"}
{"created":"2024-03-11 02:59:30","title":"FSViewFusion: Few-Shots View Generation of Novel Objects","abstract":"Novel view synthesis has observed tremendous developments since the arrival of NeRFs. However, Nerf models overfit on a single scene, lacking generalization to out of distribution objects. Recently, diffusion models have exhibited remarkable performance on introducing generalization in view synthesis. Inspired by these advancements, we explore the capabilities of a pretrained stable diffusion model for view synthesis without explicit 3D priors. Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots. Our research reveals two interesting findings. First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve finetuning diffusions on large amounts of multi-view data. Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object's identify from which the views are learnt. Motivated by this, we introduce a learning strategy, FSViewFusion, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from few shots, using low rank adapters. Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images. Code and models will be released.","sentences":["Novel view synthesis has observed tremendous developments since the arrival of NeRFs.","However, Nerf models overfit on a single scene, lacking generalization to out of distribution objects.","Recently, diffusion models have exhibited remarkable performance on introducing generalization in view synthesis.","Inspired by these advancements, we explore the capabilities of a pretrained stable diffusion model for view synthesis without explicit 3D priors.","Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots.","Our research reveals two interesting findings.","First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve finetuning diffusions on large amounts of multi-view data.","Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object's identify from which the views are learnt.","Motivated by this, we introduce a learning strategy, FSViewFusion, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from few shots, using low rank adapters.","Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images.","Code and models will be released."],"url":"http://arxiv.org/abs/2403.06394v1","category":"cs.CV"}
{"created":"2024-03-11 02:00:15","title":"The Geometry of Cyclical Social Trends","abstract":"We investigate the emergence of periodic behavior in opinion dynamics and its underlying geometry. For this, we use a bounded-confidence model with contrarian agents in a convolution social network. This means that agents adapt their opinions by interacting with their neighbors in a time-varying social network. Being contrarian, the agents are kept from reaching consensus. This is the key feature that allows the emergence of cyclical trends. We show that the systems either converge to nonconsensual equilibrium or are attracted to periodic or quasi-periodic orbits. We bound the dimension of the attractors and the period of cyclical trends. We exhibit instances where each orbit is dense and uniformly distributed within its attractor. We also investigate the case of randomly changing social networks.","sentences":["We investigate the emergence of periodic behavior in opinion dynamics and its underlying geometry.","For this, we use a bounded-confidence model with contrarian agents in a convolution social network.","This means that agents adapt their opinions by interacting with their neighbors in a time-varying social network.","Being contrarian, the agents are kept from reaching consensus.","This is the key feature that allows the emergence of cyclical trends.","We show that the systems either converge to nonconsensual equilibrium or are attracted to periodic or quasi-periodic orbits.","We bound the dimension of the attractors and the period of cyclical trends.","We exhibit instances where each orbit is dense and uniformly distributed within its attractor.","We also investigate the case of randomly changing social networks."],"url":"http://arxiv.org/abs/2403.06376v1","category":"cs.MA"}
{"created":"2024-03-11 01:57:37","title":"Intrinsic polarization conversion and avoided-mode crossing in X-cut lithium niobate microrings","abstract":"Compared with well-developed free space polarization converters, polarization conversion between TE and TM modes in waveguide is generally considered to be caused by shape birefringence, like curvature, morphology of waveguide cross section and scattering. Here, we reveal a hidden polarization conversion mechanism in X-cut lithium niobate microrings, that is the conversion can be implemented by birefringence of waveguides, which will also introduce an unavoidable avoided-mode crossing. In the experiment, we find that this mode crossing results in severe suppression of one sideband in local nondegenerate four-wave mixing and disrupts the cascaded four-wave mixing on this side. Simultaneously, we proposed, for the first time to our best knowledge, one two-dimensional method to simulate the eigenmodes (TE and TM) in X-cut microrings, which avoids the obstacle from large computational effort in three-dimensional anisotropic microrings simulation, and the mode crossing point. This work will provide an entirely novel approach to the design of polarization converters and simulation for monolithic photonics integrated circuits, and may be helpful to the studies of missed temporal dissipative soliton formation in X-cut lithium niobate rings.","sentences":["Compared with well-developed free space polarization converters, polarization conversion between TE and TM modes in waveguide is generally considered to be caused by shape birefringence, like curvature, morphology of waveguide cross section and scattering.","Here, we reveal a hidden polarization conversion mechanism in X-cut lithium niobate microrings, that is the conversion can be implemented by birefringence of waveguides, which will also introduce an unavoidable avoided-mode crossing.","In the experiment, we find that this mode crossing results in severe suppression of one sideband in local nondegenerate four-wave mixing and disrupts the cascaded four-wave mixing on this side.","Simultaneously, we proposed, for the first time to our best knowledge, one two-dimensional method to simulate the eigenmodes (TE and TM) in X-cut microrings, which avoids the obstacle from large computational effort in three-dimensional anisotropic microrings simulation, and the mode crossing point.","This work will provide an entirely novel approach to the design of polarization converters and simulation for monolithic photonics integrated circuits, and may be helpful to the studies of missed temporal dissipative soliton formation in X-cut lithium niobate rings."],"url":"http://arxiv.org/abs/2403.06374v1","category":"physics.optics"}
{"created":"2024-03-11 01:20:03","title":"Say Anything with Any Style","abstract":"Generating stylized talking head with diverse head motions is crucial for achieving natural-looking videos but still remains challenging. Previous works either adopt a regressive method to capture the speaking style, resulting in a coarse style that is averaged across all training data, or employ a universal network to synthesize videos with different styles which causes suboptimal performance. To address these, we propose a novel dynamic-weight method, namely Say Anything withAny Style (SAAS), which queries the discrete style representation via a generative model with a learned style codebook. Specifically, we develop a multi-task VQ-VAE that incorporates three closely related tasks to learn a style codebook as a prior for style extraction. This discrete prior, along with the generative model, enhances the precision and robustness when extracting the speaking styles of the given style clips. By utilizing the extracted style, a residual architecture comprising a canonical branch and style-specific branch is employed to predict the mouth shapes conditioned on any driving audio while transferring the speaking style from the source to any desired one. To adapt to different speaking styles, we steer clear of employing a universal network by exploring an elaborate HyperStyle to produce the style-specific weights offset for the style branch. Furthermore, we construct a pose generator and a pose codebook to store the quantized pose representation, allowing us to sample diverse head motions aligned with the audio and the extracted style. Experiments demonstrate that our approach surpasses state-of-theart methods in terms of both lip-synchronization and stylized expression. Besides, we extend our SAAS to video-driven style editing field and achieve satisfactory performance.","sentences":["Generating stylized talking head with diverse head motions is crucial for achieving natural-looking videos but still remains challenging.","Previous works either adopt a regressive method to capture the speaking style, resulting in a coarse style that is averaged across all training data, or employ a universal network to synthesize videos with different styles which causes suboptimal performance.","To address these, we propose a novel dynamic-weight method, namely Say Anything withAny Style (SAAS), which queries the discrete style representation via a generative model with a learned style codebook.","Specifically, we develop a multi-task VQ-VAE that incorporates three closely related tasks to learn a style codebook as a prior for style extraction.","This discrete prior, along with the generative model, enhances the precision and robustness when extracting the speaking styles of the given style clips.","By utilizing the extracted style, a residual architecture comprising a canonical branch and style-specific branch is employed to predict the mouth shapes conditioned on any driving audio while transferring the speaking style from the source to any desired one.","To adapt to different speaking styles, we steer clear of employing a universal network by exploring an elaborate HyperStyle to produce the style-specific weights offset for the style branch.","Furthermore, we construct a pose generator and a pose codebook to store the quantized pose representation, allowing us to sample diverse head motions aligned with the audio and the extracted style.","Experiments demonstrate that our approach surpasses state-of-theart methods in terms of both lip-synchronization and stylized expression.","Besides, we extend our SAAS to video-driven style editing field and achieve satisfactory performance."],"url":"http://arxiv.org/abs/2403.06363v2","category":"cs.CV"}
{"created":"2024-03-11 01:18:49","title":"See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI","abstract":"Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.","sentences":["Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system.","However, the scarcity of fMRI data and noise hamper brain decoding model performance.","Previous approaches primarily employ subject-specific models, sensitive to training sample size.","In this paper, we explore a straightforward but overlooked solution to address data scarcity.","We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations.","Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space.","During training, we leverage both visual and textual supervision for multi-modal brain decoding.","Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience.","Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines.","Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics.","Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data.","Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training.","Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds."],"url":"http://arxiv.org/abs/2403.06361v1","category":"cs.CV"}
{"created":"2024-03-11 01:12:25","title":"Inference for Median and a Generalization of HulC","abstract":"Constructing distribution-free confidence intervals for the median, a classic problem in statistics, has seen numerous solutions in the literature. While coverage validity has received ample attention, less has been explored about interval width. Our study breaks new ground by investigating the width of these intervals under non-standard assumptions. Surprisingly, we find that properly scaled, the interval width converges to a non-degenerate random variable, unlike traditional intervals. We also adapt our findings for constructing improved confidence intervals for general parameters, enhancing the existing HulC procedure. These advances provide practitioners with more robust tools for data analysis, reducing the need for strict distributional assumptions.","sentences":["Constructing distribution-free confidence intervals for the median, a classic problem in statistics, has seen numerous solutions in the literature.","While coverage validity has received ample attention, less has been explored about interval width.","Our study breaks new ground by investigating the width of these intervals under non-standard assumptions.","Surprisingly, we find that properly scaled, the interval width converges to a non-degenerate random variable, unlike traditional intervals.","We also adapt our findings for constructing improved confidence intervals for general parameters, enhancing the existing HulC procedure.","These advances provide practitioners with more robust tools for data analysis, reducing the need for strict distributional assumptions."],"url":"http://arxiv.org/abs/2403.06357v1","category":"math.ST"}
{"created":"2024-03-11 01:02:01","title":"Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems","abstract":"In this paper, we explore how to design lightweight CNN architecture for embedded computing systems. We propose L-Mobilenet model for ZYNQ based hardware platform. L-Mobilenet can adapt well to the hardware computing and accelerating, and its network structure is inspired by the state-of-the-art work of Inception-ResnetV1 and MobilenetV2, which can effectively reduce parameters and delay while maintaining the accuracy of inference. We deploy our L-Mobilenet model to ZYNQ embedded platform for fully evaluating the performance of our design. By measuring in cifar10 and cifar100 datasets, L-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than MobileNetV2 while maintaining a similar accuracy. It also can obtain 2x speed up and 1.5x fewer parameters than ShufflenetV2 while maintaining the same accuracy. Experiments show that our network model can obtain better performance because of the special considerations for hardware accelerating and software-hardware co-design strategies in our L-Mobilenet bottleneck architecture.","sentences":["In this paper, we explore how to design lightweight CNN architecture for embedded computing systems.","We propose L-Mobilenet model for ZYNQ based hardware platform.","L-Mobilenet can adapt well to the hardware computing and accelerating, and its network structure is inspired by the state-of-the-art work of Inception-ResnetV1 and MobilenetV2, which can effectively reduce parameters and delay while maintaining the accuracy of inference.","We deploy our L-Mobilenet model to ZYNQ embedded platform for fully evaluating the performance of our design.","By measuring in cifar10 and cifar100 datasets, L-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than MobileNetV2 while maintaining a similar accuracy.","It also can obtain 2x speed up and 1.5x fewer parameters than ShufflenetV2 while maintaining the same accuracy.","Experiments show that our network model can obtain better performance because of the special considerations for hardware accelerating and software-hardware co-design strategies in our L-Mobilenet bottleneck architecture."],"url":"http://arxiv.org/abs/2403.06352v1","category":"cs.CV"}
{"created":"2024-03-11 00:30:25","title":"Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation","abstract":"High-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, anomaly detection, and trend analysis. To quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods. However, real-world sparse tensors exhibit highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures. This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. To demonstrate the efficacy of ALTO, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across a range of real-world sparse tensors. Additionally, we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, which require multiple tensor copies, ALTO achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage.","sentences":["High-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, anomaly detection, and trend analysis.","To quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods.","However, real-world sparse tensors exhibit highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures.","This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form.","Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution.","To demonstrate the efficacy of ALTO, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across a range of real-world sparse tensors.","Additionally, we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics.","Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats.","Compared to the best mode-specific formats, which require multiple tensor copies, ALTO achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage."],"url":"http://arxiv.org/abs/2403.06348v1","category":"cs.DC"}
{"created":"2024-03-12 17:44:22","title":"On the One-dimensional Singular Abreu Equations","abstract":"Singular fourth-order Abreu equations have been used to approximate minimizers of convex functionals subject to a convexity constraint in dimensions higher than or equal to two. For Abreu type equations, they often exhibit different solvability phenomena in dimension one and dimensions at least two. We prove the analogues of these results for the variational problem and singular Abreu equations in dimension one, and use the approximation scheme to obtain a characterization of limiting minimizers to the one-dimensional variational problem.","sentences":["Singular fourth-order Abreu equations have been used to approximate minimizers of convex functionals subject to a convexity constraint in dimensions higher than or equal to two.","For Abreu type equations, they often exhibit different solvability phenomena in dimension one and dimensions at least two.","We prove the analogues of these results for the variational problem and singular Abreu equations in dimension one, and use the approximation scheme to obtain a characterization of limiting minimizers to the one-dimensional variational problem."],"url":"http://arxiv.org/abs/2403.07852v1","category":"math.AP"}
{"created":"2024-03-12 17:41:27","title":"Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations","abstract":"We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL). The goal is to improve predictive performance of GNN using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned \"vanilla\" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods. Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's training dynamics.","sentences":["We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL).","The goal is to improve predictive performance of GNN using explanations.","EEGL is an iterative self-improving algorithm, which starts with a learned \"vanilla\" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs.","These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods.","Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem.","We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that of vanilla GNNs.","We also analyze EEGL's training dynamics."],"url":"http://arxiv.org/abs/2403.07849v1","category":"cs.LG"}
{"created":"2024-03-12 17:26:00","title":"Critical metrics of eigenvalue functionals via Clarke subdifferential","abstract":"We set up a new framework to study critical points of functionals defined as combinations of eigenvalues of operators with respect to a given set of parameters: Riemannian metrics, potentials, etc. Our setting builds upon Clarke's differentiation theory to provide a novel understanding of critical metrics. In particular, we unify and refine previous research carried out on Laplace and Steklov eigenvalues. We also use our theory to tackle original examples such as the conformal GJMS operators, the conformal Laplacian, and the Laplacian with mixed boundary conditions.","sentences":["We set up a new framework to study critical points of functionals defined as combinations of eigenvalues of operators with respect to a given set of parameters: Riemannian metrics, potentials, etc.","Our setting builds upon Clarke's differentiation theory to provide a novel understanding of critical metrics.","In particular, we unify and refine previous research carried out on Laplace and Steklov eigenvalues.","We also use our theory to tackle original examples such as the conformal GJMS operators, the conformal Laplacian, and the Laplacian with mixed boundary conditions."],"url":"http://arxiv.org/abs/2403.07841v1","category":"math.DG"}
{"created":"2024-03-12 16:10:01","title":"QCD Equation of State at nonzero baryon density in external magnetic field","abstract":"This paper is devoted to the study of QCD equation of state in external magnetic field and nonzero baryon density. Our study is carried out by means of lattice simulation with 2+1 dynamical staggered quarks at the physical masses. The simulation is conducted at imaginary baryon chemical potential what allowed us to overcome the sign problem. We expand the pressure in the baryon imaginary chemical potential and study three leading nonzero coefficients in this expansion. These coefficients were calculated for the following values of magnetic field: $eB=0.3$, $0.6$, $1.2$ GeV$^2$ with the lattice sizes $8\\times32^3$, $10\\times40^3$, $12\\times48^3$. Using these data we take continuum limit for the coefficients. Our results indicate considerable enhancement of the expansion coefficients by the magnetic field.","sentences":["This paper is devoted to the study of QCD equation of state in external magnetic field and nonzero baryon density.","Our study is carried out by means of lattice simulation with 2+1 dynamical staggered quarks at the physical masses.","The simulation is conducted at imaginary baryon chemical potential what allowed us to overcome the sign problem.","We expand the pressure in the baryon imaginary chemical potential and study three leading nonzero coefficients in this expansion.","These coefficients were calculated for the following values of magnetic field: $eB=0.3$, $0.6$, $1.2$ GeV$^2$ with the lattice sizes $8\\times32^3$, $10\\times40^3$, $12\\times48^3$.","Using these data we take continuum limit for the coefficients.","Our results indicate considerable enhancement of the expansion coefficients by the magnetic field."],"url":"http://arxiv.org/abs/2403.07783v1","category":"hep-lat"}
{"created":"2024-03-12 14:48:45","title":"Lipschitz maps with prescribed local Lipschitz constants","abstract":"Let $\\Gamma$ be a closed subset of a complete Riemannian manifold $M$ of dimension $\\geq 2$, let $f: M \\to N$ be a Lipschitz map to a complete Riemannian manifold $N$, and let $\\psi$ be a continuous function which dominates the local Lipschitz constant of $f$. We construct a Lipschitz map which agress with $f$ on $\\Gamma$ and whose local Lipschitz constant is $\\psi$.","sentences":["Let $\\Gamma$ be a closed subset of a complete Riemannian manifold $M$ of dimension $\\geq 2$, let $f: M \\to N$ be a Lipschitz map to a complete Riemannian manifold $N$, and let $\\psi$ be a continuous function which dominates the local Lipschitz constant of $f$. We construct a Lipschitz map which agress with $f$ on $\\Gamma$ and whose local Lipschitz constant is $\\psi$."],"url":"http://arxiv.org/abs/2403.07702v1","category":"math.DG"}
{"created":"2024-03-12 14:41:57","title":"The Kazdan-Warner problem on compact K\u00e4hler surfaces","abstract":"In this paper, we investigate a Kazdan-Warner problem on compact K\\\"ahler surfaces with negative Gauduchon degree, which corresponds to prescribing sign-changing Chern scalar curvatures. By the method of our recent paper [J. Funt. Anal. 285 (2023): 109948], we establish a Chen-Li type existence theorem on compact K\\\"ahler surfaces when the candidate curvature function is of negative average. Moreover, we give an alternative proof of Ding-Liu's theorem [Trans. Amer. Math. Soc. 347(1995) 1059-1066] on prescribing sign-changing Gaussian curvatures by using the $\\sup+\\inf$ inequality due to H. Brezis, Y. Y. Li and I. Shafrir.","sentences":["In this paper, we investigate a Kazdan-Warner problem on compact K\\\"ahler surfaces with negative Gauduchon degree, which corresponds to prescribing sign-changing Chern scalar curvatures.","By the method of our recent paper [J. Funt.","Anal. 285 (2023): 109948], we establish a Chen-Li type existence theorem on compact K\\\"ahler surfaces when the candidate curvature function is of negative average.","Moreover, we give an alternative proof of Ding-Liu's theorem [Trans.","Amer.","Math.","Soc.","347(1995) 1059-1066] on prescribing sign-changing Gaussian curvatures by using the $\\sup+\\inf$ inequality due to H. Brezis, Y. Y. Li and I. Shafrir."],"url":"http://arxiv.org/abs/2403.07698v1","category":"math.DG"}
{"created":"2024-03-12 14:31:38","title":"Probing anomalous $Z\u03b3\u03b3\u03b3$ couplings at a future muon collider","abstract":"The sensitivity to anomalous quartic gauge couplings (AQGCs) of the $\\gamma\\gamma\\gamma Z$ interaction is studied in the $\\mu^+\\mu^- \\rightarrow \\mu^+\\gamma\\gamma \\mu^-$ scattering at a future muon collider with unpolarized beams. The anomalous $\\gamma\\gamma\\gamma Z$ vertex is described by two couplings, $\\zeta_1$ and $\\zeta_2$. The differential and total cross sections are calculated for the center-of-mass energies of 3 TeV, 14 TeV, and 100 TeV. For these values of the collision energy the $95\\%$ C.L. exclusion regions for AQGCs are obtained depending on the systematic error. In particular, for the 14 TeV muon collider with the integrated luminosity $L = 20$ ab$^{-1}$ the best sensitivities are derived to be $\\zeta_1 = 3.1 \\times 10^{-5}$ TeV$^{-4}$ and $\\zeta_2 = 6.5 \\times 10^{-5}$ TeV$^{-4}$. These constraints are three orders of magnitude stronger than the bounds obtained for the 27 TeV HE-LHC with $L = 15$ ab$^{-1}$. At the 100 TeV muon collider with $L = 1000$ ab$^{-1}$ AQGCs can be probed up to $(1.64 \\div 3.4) \\times 10^{-8}$ TeV$^{-4}$. The partial-wave unitarity constraints on couplings $\\zeta_1$, $\\zeta_2$ are evaluated. It is shown that the unitarity is not violated in the region of the AQGCs examined in the present paper.","sentences":["The sensitivity to anomalous quartic gauge couplings (AQGCs) of the $\\gamma\\gamma\\gamma Z$ interaction is studied in the $\\mu^+\\mu^- \\rightarrow \\mu^+\\gamma\\gamma \\mu^-$ scattering at a future muon collider with unpolarized beams.","The anomalous $\\gamma\\gamma\\gamma Z$ vertex is described by two couplings, $\\zeta_1$ and $\\zeta_2$. The differential and total cross sections are calculated for the center-of-mass energies of 3 TeV, 14 TeV, and 100 TeV. For these values of the collision energy the $95\\%$ C.L. exclusion regions for AQGCs are obtained depending on the systematic error.","In particular, for the 14 TeV muon collider with the integrated luminosity $L = 20$ ab$^{-1}$ the best sensitivities are derived to be $\\zeta_1 = 3.1 \\times 10^{-5}$ TeV$^{-4}$ and $\\zeta_2 = 6.5 \\times 10^{-5}$ TeV$^{-4}$. These constraints are three orders of magnitude stronger than the bounds obtained for the 27 TeV HE-LHC with $L = 15$ ab$^{-1}$. At the 100 TeV muon collider with $L = 1000$ ab$^{-1}$ AQGCs can be probed up to $(1.64 \\div 3.4)","\\times 10^{-8}$ TeV$^{-4}$.","The partial-wave unitarity constraints on couplings $\\zeta_1$, $\\zeta_2$ are evaluated.","It is shown that the unitarity is not violated in the region of the AQGCs examined in the present paper."],"url":"http://arxiv.org/abs/2403.07689v1","category":"hep-ph"}
{"created":"2024-03-12 14:06:44","title":"Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation","abstract":"Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies.","sentences":["Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services.","Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities.","However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated.","This paper unveils the threat of MEA in I2IT tasks from a new perspective.","Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift.","This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution.","Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics.","A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies."],"url":"http://arxiv.org/abs/2403.07673v1","category":"cs.CR"}
{"created":"2024-03-12 13:58:40","title":"Conditions of positivity on a shadow Markoff Tree","abstract":"An analogue of the Markoff equation has recently been introduced by the author and Valentin Ovsienko. A conjecture about the necessary and sufficient conditions for positivity of solutions to this equation is formulated and discussed.","sentences":["An analogue of the Markoff equation has recently been introduced by the author and Valentin Ovsienko.","A conjecture about the necessary and sufficient conditions for positivity of solutions to this equation is formulated and discussed."],"url":"http://arxiv.org/abs/2403.07668v1","category":"math.CO"}
{"created":"2024-03-12 13:27:11","title":"Quantitative 2D propagation of smallness and control for 1D heat equations with power growth potentials","abstract":"We study the relation between propagation of smallness in the plane and control for heat equations. The former has been proved by Zhu who showed how the value of solutions in some small set propagates to a larger domain. By reviewing his proof, we establish a quantitative version with the explicit dependence of parameters. Using this explicit version, we establish new exact null-controllability results of 1D heat equations with any nonnegative power growth potentials $V\\in\\mathcal{C}(\\mathbb{R})$. As a key ingredient, new spectral inequalities are established. The control set $\\Omega$ that we consider satisfy \\begin{equation*}   \\left|\\Omega\\cap [x-L\\langle x\\rangle ^{-s},x+L\\langle x\\rangle ^{-s}]\\right|\\ge \\gamma^{\\langle x\\rangle^\\tau}2L\\langle x\\rangle^{-s} \\end{equation*} for some $\\gamma\\in(0,1)$, $L>0$, $\\tau,s\\ge 0$, and $\\langle x\\rangle:=(1+|x|^2)^{1 /2} $. In particular, the null-controllability result for the case of thick sets that allow the decay of the density (\\textit{i.e.}, $s=0$ and $\\tau\\ge 0$) is included. These extend the Zhu-Zhuge's results from $\\Omega$ being the union of equidistributive open sets to thick sets in the 1-dimensional case, and Su-Sun-Yuan's result from bounded potentials to certain unbounded ones.","sentences":["We study the relation between propagation of smallness in the plane and control for heat equations.","The former has been proved by Zhu who showed how the value of solutions in some small set propagates to a larger domain.","By reviewing his proof, we establish a quantitative version with the explicit dependence of parameters.","Using this explicit version, we establish new exact null-controllability results of 1D heat equations with any nonnegative power growth potentials $V\\in\\mathcal{C}(\\mathbb{R})$. As a key ingredient, new spectral inequalities are established.","The control set $\\Omega$ that we consider satisfy \\begin{equation*}   \\left|\\Omega\\cap","[x-L\\langle x\\rangle ^{-s},x+L\\langle x\\rangle ^{-s}]\\right|\\ge \\gamma^{\\langle x\\rangle^\\tau}2L\\langle x\\rangle^{-s} \\end{equation*} for some $\\gamma\\in(0,1)$, $L>0$, $\\tau,s\\ge 0$, and $\\langle x\\rangle:=(1+|x|^2)^{1 /2} $.","In particular, the null-controllability result for the case of thick sets that allow the decay of the density (\\textit{i.e.}, $s=0$ and $\\tau\\ge 0$) is included.","These extend the Zhu-Zhuge's results from $\\Omega$ being the union of equidistributive open sets to thick sets in the 1-dimensional case, and Su-Sun-Yuan's result from bounded potentials to certain unbounded ones."],"url":"http://arxiv.org/abs/2403.07643v1","category":"math.AP"}
{"created":"2024-03-12 13:01:35","title":"Markov Chain Aggregation with Error Bounds on Transient Distributions","abstract":"We extend the existing theory of formal error bounds for the transient distribution of an aggregated (or lumped) Markov chain when compared to the transient distribution of the original chain, for both discrete- and continuous-time Markov chains. In the discrete-time setting, we bound the stepwise increment of the error, and in the continuous-time setting, we bound the rate at which the error grows. We then compare these error bounds with relevant concepts in the literature such as exact and ordinary lumpability as well as deflatability and aggregatability. These concepts define stricter than necessary conditions to identify settings in which the aggregation error is zero. We also consider possible algorithms for finding suitable aggregations for which the formal error bounds are low, and we analyse first experiments with these algorithms on different models.","sentences":["We extend the existing theory of formal error bounds for the transient distribution of an aggregated (or lumped) Markov chain when compared to the transient distribution of the original chain, for both discrete- and continuous-time Markov chains.","In the discrete-time setting, we bound the stepwise increment of the error, and in the continuous-time setting, we bound the rate at which the error grows.","We then compare these error bounds with relevant concepts in the literature such as exact and ordinary lumpability as well as deflatability and aggregatability.","These concepts define stricter than necessary conditions to identify settings in which the aggregation error is zero.","We also consider possible algorithms for finding suitable aggregations for which the formal error bounds are low, and we analyse first experiments with these algorithms on different models."],"url":"http://arxiv.org/abs/2403.07618v1","category":"math.PR"}
{"created":"2024-03-12 12:22:25","title":"Topological Quantum Mechanics on Orbifolds and Orbifold Index","abstract":"In this paper, we study topological quantum mechanical models on symplectic orbifolds. The correlation map gives an explicit orbifold version of quantum HKR map. The exact semi-classical approximation in this model leads to a geometric and quantum field theoretic interpretation of the orbifold algebraic index.","sentences":["In this paper, we study topological quantum mechanical models on symplectic orbifolds.","The correlation map gives an explicit orbifold version of quantum HKR map.","The exact semi-classical approximation in this model leads to a geometric and quantum field theoretic interpretation of the orbifold algebraic index."],"url":"http://arxiv.org/abs/2403.07590v1","category":"math.QA"}
{"created":"2024-03-12 12:14:22","title":"Semi-brittle flow of rocks: Cracks, dislocations and strain hardening","abstract":"Strain hardening is a key feature observed in many rocks deformed in the so-called ``semi-brittle'' regime, where both crystal plastic and brittle deformation mechanisms operate. Dislocation storage has long been recognised as a major process leading to strain hardening. Here, we suggest that tensile microcracks may be viewed as dislocation sinks, by offering internal free surfaces where dislocations can escape individual crystals within an aggregate. Strain hardening is modelled with a conventional approach, combining Taylor's equation relating stress to dislocation density, and a dislocation density evolution law based on dislocation mean-free path and dynamic recovery. The initiation of microcracks is modelled as a function dislocation density, assuming dislocation pile-ups at grain boundaries. Microcrack growth is modelled using linear elastic fracture mechanics. The model captures important qualitative features observed in calcite marble deformation experiments: pressure-dependency of strength in the ductile regime, and a reduction in hardening linked to an increase in crack growth with decreasing confining pressure. Grain-size dependency of strength and hardening is also captured but requires significant toughening (or limitation to crack growth) at small grain sizes. The model can be improved significantly once detailed, systematic microstructural observations become available.","sentences":["Strain hardening is a key feature observed in many rocks deformed in the so-called ``semi-brittle'' regime, where both crystal plastic and brittle deformation mechanisms operate.","Dislocation storage has long been recognised as a major process leading to strain hardening.","Here, we suggest that tensile microcracks may be viewed as dislocation sinks, by offering internal free surfaces where dislocations can escape individual crystals within an aggregate.","Strain hardening is modelled with a conventional approach, combining Taylor's equation relating stress to dislocation density, and a dislocation density evolution law based on dislocation mean-free path and dynamic recovery.","The initiation of microcracks is modelled as a function dislocation density, assuming dislocation pile-ups at grain boundaries.","Microcrack growth is modelled using linear elastic fracture mechanics.","The model captures important qualitative features observed in calcite marble deformation experiments: pressure-dependency of strength in the ductile regime, and a reduction in hardening linked to an increase in crack growth with decreasing confining pressure.","Grain-size dependency of strength and hardening is also captured but requires significant toughening (or limitation to crack growth) at small grain sizes.","The model can be improved significantly once detailed, systematic microstructural observations become available."],"url":"http://arxiv.org/abs/2403.07583v1","category":"physics.geo-ph"}
{"created":"2024-03-12 12:07:56","title":"On HRTF Notch Frequency Prediction Using Anthropometric Features and Neural Networks","abstract":"High fidelity spatial audio often performs better when produced using a personalized head-related transfer function (HRTF). However, the direct acquisition of HRTFs is cumbersome and requires specialized equipment. Thus, many personalization methods estimate HRTF features from easily obtained anthropometric features of the pinna, head, and torso. The first HRTF notch frequency (N1) is known to be a dominant feature in elevation localization, and thus a useful feature for HRTF personalization. This paper describes the prediction of N1 frequency from pinna anthropometry using a neural model. Prediction is performed separately on three databases, both simulated and measured, and then by domain mixing in-between the databases. The model successfully predicts N1 frequency for individual databases and by domain mixing between some databases. Prediction errors are better or comparable to those previously reported, showing significant improvement when acquired over a large database and with a larger output range.","sentences":["High fidelity spatial audio often performs better when produced using a personalized head-related transfer function (HRTF).","However, the direct acquisition of HRTFs is cumbersome and requires specialized equipment.","Thus, many personalization methods estimate HRTF features from easily obtained anthropometric features of the pinna, head, and torso.","The first HRTF notch frequency (N1) is known to be a dominant feature in elevation localization, and thus a useful feature for HRTF personalization.","This paper describes the prediction of N1 frequency from pinna anthropometry using a neural model.","Prediction is performed separately on three databases, both simulated and measured, and then by domain mixing in-between the databases.","The model successfully predicts N1 frequency for individual databases and by domain mixing between some databases.","Prediction errors are better or comparable to those previously reported, showing significant improvement when acquired over a large database and with a larger output range."],"url":"http://arxiv.org/abs/2403.07579v1","category":"eess.AS"}
{"created":"2024-03-12 11:32:57","title":"SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields","abstract":"Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively.","sentences":["Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity.","However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes.","While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction.","Additionally, these methods are plagued by slow training and rendering speed.","To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images.","The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs.","Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2403.07547v1","category":"cs.CV"}
{"created":"2024-03-12 11:19:46","title":"LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes","abstract":"Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three tasks in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction, featuring meshes of up to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful architecture for learning with high-fidelity meshes which has the potential to enable interesting downstream applications. Our implementation is publicly available.","sentences":["Many anatomical structures can be described by surface or volume meshes.","Machine learning is a promising tool to extract information from these 3D models.","However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures.","Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms.","We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation.","Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients.","LaB-GATr achieves state-of-the-art results on three tasks in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction, featuring meshes of up to 200,000 vertices.","Our results demonstrate that LaB-GATr is a powerful architecture for learning with high-fidelity meshes which has the potential to enable interesting downstream applications.","Our implementation is publicly available."],"url":"http://arxiv.org/abs/2403.07536v1","category":"cs.CV"}
{"created":"2024-03-12 11:16:28","title":"Frobenius numbers associated with Diophantine triples of $x^2+y^2=z^r$ (extended version)","abstract":"We give an explicit formula for the $p$-Frobenius number of triples associated with Diophantine equations $x^2+y^2=z^r$, that is, the largest positive integer that can only be represented in $p$ ways by combining the three integers of the solutions of Diophantine equations $x^2+y^2=z^r$. When $r=2$, the Frobenius number has already been given.","sentences":["We give an explicit formula for the $p$-Frobenius number of triples associated with Diophantine equations $x^2+y^2=z^r$, that is, the largest positive integer that can only be represented in $p$ ways by combining the three integers of the solutions of Diophantine equations $x^2+y^2=z^r$. When $r=2$, the Frobenius number has already been given."],"url":"http://arxiv.org/abs/2403.07534v1","category":"math.NT"}
{"created":"2024-03-12 10:57:01","title":"Cohomologies and deformations of differential algebra morphisms","abstract":"This paper studies the formal deformations of differential algebra morphisms. As a consequence, we develop a cohomology theory of differential algebra morphisms to interpret the lower degree cohomology groups as formal deformations. Then, we prove the Cohomology Comparison Theorem of differential algebra morphisms, i.e., the cohomology of a morphism of differential algebras is isomorphic to the cohomology of an auxiliary differential algebra. Finally, we can give a minimal model for morphism of differential algebras with weight=0.","sentences":["This paper studies the formal deformations of differential algebra morphisms.","As a consequence, we develop a cohomology theory of differential algebra morphisms to interpret the lower degree cohomology groups as formal deformations.","Then, we prove the Cohomology Comparison Theorem of differential algebra morphisms, i.e., the cohomology of a morphism of differential algebras is isomorphic to the cohomology of an auxiliary differential algebra.","Finally, we can give a minimal model for morphism of differential algebras with weight=0."],"url":"http://arxiv.org/abs/2403.07521v1","category":"math.RA"}
{"created":"2024-03-12 09:48:30","title":"Novel Signatures of Radiation Reaction in Electron-Laser Sidescattering","abstract":"In this article we investigate novel signatures of radiation reaction via the angular deflection of an electron beam colliding at 90 degrees with an intense laser pulse. Due to the radiation reaction effect, the electrons can be deflected towards the beam axis for plane wave backgrounds, which is not possible in the absence of radiation reaction effects. The magnitude and size of the deflection angle can be controlled by tailoring the laser pulse shapes. The effect is first derived analytically using the Landau-Lifshitz equation, which allows to determine the important scaling behavior with laser intensity and particle energy. We then move on to full scale 3D Monte Carlo simulations to verify the effect is observable with present day laser technology. We investigate the opportunities for an indirect observation of laser depletion in such side scattering scenarios.","sentences":["In this article we investigate novel signatures of radiation reaction via the angular deflection of an electron beam colliding at 90 degrees with an intense laser pulse.","Due to the radiation reaction effect, the electrons can be deflected towards the beam axis for plane wave backgrounds, which is not possible in the absence of radiation reaction effects.","The magnitude and size of the deflection angle can be controlled by tailoring the laser pulse shapes.","The effect is first derived analytically using the Landau-Lifshitz equation, which allows to determine the important scaling behavior with laser intensity and particle energy.","We then move on to full scale 3D Monte Carlo simulations to verify the effect is observable with present day laser technology.","We investigate the opportunities for an indirect observation of laser depletion in such side scattering scenarios."],"url":"http://arxiv.org/abs/2403.07455v1","category":"physics.plasm-ph"}
{"created":"2024-03-12 09:42:44","title":"Single-Switch Transformer-less Power Supply for Low Temperature Plasma Jet -- 3.3 kV SiC MOSFET opportunities","abstract":"This work presents a simple power converter, without any high voltage transformer, able to supply and control a plasma jet based on dielectric barrier discharge. The converter, operating in pulsed current mode, requires a single power switch and is fed by a low voltage DC source. It can deliver very short duration pulses to the plasma jet with high current amplitude. The operating principle is explained by means of the state plane analysis and is validated with simulations and experimental results. The equations provided allow for the calculation during the design stage of important characteristics of the plasma jet as the peak voltage and the duration of the pulses. The power can be easily adjusted during experimentation to comply with the desired appearance of the plasma jet.","sentences":["This work presents a simple power converter, without any high voltage transformer, able to supply and control a plasma jet based on dielectric barrier discharge.","The converter, operating in pulsed current mode, requires a single power switch and is fed by a low voltage DC source.","It can deliver very short duration pulses to the plasma jet with high current amplitude.","The operating principle is explained by means of the state plane analysis and is validated with simulations and experimental results.","The equations provided allow for the calculation during the design stage of important characteristics of the plasma jet as the peak voltage and the duration of the pulses.","The power can be easily adjusted during experimentation to comply with the desired appearance of the plasma jet."],"url":"http://arxiv.org/abs/2403.07449v1","category":"physics.plasm-ph"}
{"created":"2024-03-12 09:33:52","title":"The fourth-order Schr\u00f6dinger equation on lattices","abstract":"In this paper, we study the fourth-order Schr\\\"{o}dinger equation \\begin{equation*}   i \\partial_t u + {\\Delta}^2 u - \\gamma \\Delta u = \\pm |u|^{s-1}u \\end{equation*} on the lattice $\\mathbb{Z}^d$ with dimensions $d=1,2$ and parameter $\\gamma \\in \\mathbb{R}$. In order to establish sharp dispersive estimates, we consider the fundamental solution as an oscillatory integral and analyze the Newton polyhedron of its phase function. Furthermore, we prove Strichartz estimates which yield the existence of global solutions to nonlinear equations with small data.","sentences":["In this paper, we study the fourth-order Schr\\\"{o}dinger equation \\begin{equation*}   i \\partial_t u + {\\Delta}^2 u - \\gamma \\Delta u = \\pm |u|^{s-1}u \\end{equation*} on the lattice $\\mathbb{Z}^d$ with dimensions $d=1,2$ and parameter $\\gamma \\in \\mathbb{R}$. In order to establish sharp dispersive estimates, we consider the fundamental solution as an oscillatory integral and analyze the Newton polyhedron of its phase function.","Furthermore, we prove Strichartz estimates which yield the existence of global solutions to nonlinear equations with small data."],"url":"http://arxiv.org/abs/2403.07445v1","category":"math.AP"}
{"created":"2024-03-12 08:20:41","title":"Boundedness of energy for N-body Schr\u00f6dinger equations with time dependent small potentials","abstract":"We prove that Sobolev norms of solutions to time dependent Schr\\\"odinger equations for $d$-dimensional $N$-partcles interacting via time dependent two body potentials are bounded in time if certain Lebesgue norms of the potentials are small uniformly in time. The proof uses the scattering theory in the extended phase space which proves that all particles scatter freely in the remote past and far future.","sentences":["We prove that Sobolev norms of solutions to time dependent Schr\\\"odinger equations for $d$-dimensional $N$-partcles interacting via time dependent two body potentials are bounded in time if certain Lebesgue norms of the potentials are small uniformly in time.","The proof uses the scattering theory in the extended phase space which proves that all particles scatter freely in the remote past and far future."],"url":"http://arxiv.org/abs/2403.07400v1","category":"math.AP"}
{"created":"2024-03-12 07:45:36","title":"Phases and Duality in Fundamental Kazakov-Migdal Model on the Graph","abstract":"We examine the fundamental Kazakov-Migdal (FKM) model on a generic graph, whose partition function is represented by the Ihara zeta function weighted by unitary matrices. The FKM model becomes unstable in the critical strip of the Ihara zeta function. We discover a duality between small and large couplings, associated with the functional equation of the Ihara zeta function for regular graphs. Although the duality is not precise for irregular graphs, we show that the effective action in the large coupling region can be represented by a summation of all possible Wilson loops on the graph similar to that in the small coupling region. We estimate the phase structure of the FKM model both in the small and large coupling regions by comparing it with the Gross-Witten-Wadia (GWW) model. We further validate the theoretical analysis through detailed numerical simulations.","sentences":["We examine the fundamental Kazakov-Migdal (FKM) model on a generic graph, whose partition function is represented by the Ihara zeta function weighted by unitary matrices.","The FKM model becomes unstable in the critical strip of the Ihara zeta function.","We discover a duality between small and large couplings, associated with the functional equation of the Ihara zeta function for regular graphs.","Although the duality is not precise for irregular graphs, we show that the effective action in the large coupling region can be represented by a summation of all possible Wilson loops on the graph similar to that in the small coupling region.","We estimate the phase structure of the FKM model both in the small and large coupling regions by comparing it with the Gross-Witten-Wadia (GWW) model.","We further validate the theoretical analysis through detailed numerical simulations."],"url":"http://arxiv.org/abs/2403.07385v1","category":"hep-th"}
{"created":"2024-03-12 06:35:38","title":"A novel fast iterative moment method for near-continuum flows","abstract":"In this paper, we develop a novel fast iterative moment method for the steady-state simulation of near-continuum flows, which are modeled by the high-order moment system derived from the Boltzmann-BGK equation. The fast convergence of the present method is mainly achieved by alternately solving the moment system and the hydrodynamic equations with compatible constitutive relations and boundary conditions. To be specific, the compatible hydrodynamic equations are solved in each iteration to get improved predictions of macroscopic quantities, which are subsequently utilized to expedite the evolution of the moment system. Additionally, a semi-implicit scheme treating the collision term implicitly is introduced for the moment system. With cell-by-cell sweeping strategy, the resulting alternating iteration can be further accelerated for steady-state computation. It is also worth mentioning that such an alternating iteration works well with the nonlinear multigrid method. Numerical experiments for planar Couette flow, shock structure, and lid-driven cavity flow are carried out to investigate the performance of the proposed fast iterative moment method, and all results show wonderful efficiency and robustness.","sentences":["In this paper, we develop a novel fast iterative moment method for the steady-state simulation of near-continuum flows, which are modeled by the high-order moment system derived from the Boltzmann-BGK equation.","The fast convergence of the present method is mainly achieved by alternately solving the moment system and the hydrodynamic equations with compatible constitutive relations and boundary conditions.","To be specific, the compatible hydrodynamic equations are solved in each iteration to get improved predictions of macroscopic quantities, which are subsequently utilized to expedite the evolution of the moment system.","Additionally, a semi-implicit scheme treating the collision term implicitly is introduced for the moment system.","With cell-by-cell sweeping strategy, the resulting alternating iteration can be further accelerated for steady-state computation.","It is also worth mentioning that such an alternating iteration works well with the nonlinear multigrid method.","Numerical experiments for planar Couette flow, shock structure, and lid-driven cavity flow are carried out to investigate the performance of the proposed fast iterative moment method, and all results show wonderful efficiency and robustness."],"url":"http://arxiv.org/abs/2403.07358v1","category":"math.NA"}
{"created":"2024-03-12 06:22:10","title":"Graph Unlearning with Efficient Partial Retraining","abstract":"Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experiments to demonstrate the superiority of our proposed approach.","sentences":["Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications.","However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability.","To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining.","However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models.","In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs.","Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation.","We conduct extensive experiments to demonstrate the superiority of our proposed approach."],"url":"http://arxiv.org/abs/2403.07353v1","category":"cs.LG"}
{"created":"2024-03-12 06:14:21","title":"On Mirror Symmetry and Irrationality of Zeta Values","abstract":"A fundamental object of study in mirror symmetry of $n$-dimensional Fano varieties is the A-side connection on small quantum cohomology. When the Picard rank is 1, the Borel transform relates the quantum differential operator of the Fano to the Picard-Fuchs operator of the associated pencil of anticanonical Calabi-Yau $(n-1)$-folds on the Fano variety. Expanding on related work by W. Yang on the Beukers-Peters pencil of K3 surfaces associated with Ap\\'ery's proof for the irrationality of $\\zeta(3)$, for such operators we define holomorphic prepotentials, virtual Yukawa couplings, and virtual instanton numbers, analogous to the usual ingredients of Calabi-Yau mirror symmetry. We prove that when the underlying Calabi-Yau operator is modular, the virtual Yukawa coupling is a modular form of weight-$(n+1)$, with the holomorphic prepotential as an Eichler integral. We then analyze the quantum differential operators for modular pencils of K3 surfaces arising as anticanonical linear systems for the 17 deformation classes of Fano threefolds of Picard rank-1 classified by Iskovskikh from the perspective of Golyshev \\& Zagier's proof of the Gamma conjecture for such Fanos, the natural setting of Yang's work. Here, the virtual instanton numbers are proven to be periodic integers with period equal to the level of the modular subgroup. Finally, we conjecture that the geometric nature of these virtual instanton numbers can be understood in terms of genus zero Gromov-Witten invariants of certain local Calabi-Yau fourfolds.","sentences":["A fundamental object of study in mirror symmetry of $n$-dimensional Fano varieties is the A-side connection on small quantum cohomology.","When the Picard rank is 1, the Borel transform relates the quantum differential operator of the Fano to the Picard-Fuchs operator of the associated pencil of anticanonical Calabi-Yau $(n-1)$-folds on the Fano variety.","Expanding on related work by W. Yang on the Beukers-Peters pencil of K3 surfaces associated with Ap\\'ery's proof for the irrationality of $\\zeta(3)$, for such operators we define holomorphic prepotentials, virtual Yukawa couplings, and virtual instanton numbers, analogous to the usual ingredients of Calabi-Yau mirror symmetry.","We prove that when the underlying Calabi-Yau operator is modular, the virtual Yukawa coupling is a modular form of weight-$(n+1)$, with the holomorphic prepotential as an Eichler integral.","We then analyze the quantum differential operators for modular pencils of K3 surfaces arising as anticanonical linear systems for the 17 deformation classes of Fano threefolds of Picard rank-1 classified by Iskovskikh from the perspective of Golyshev \\& Zagier's proof of the Gamma conjecture for such Fanos, the natural setting of Yang's work.","Here, the virtual instanton numbers are proven to be periodic integers with period equal to the level of the modular subgroup.","Finally, we conjecture that the geometric nature of these virtual instanton numbers can be understood in terms of genus zero Gromov-Witten invariants of certain local Calabi-Yau fourfolds."],"url":"http://arxiv.org/abs/2403.07349v1","category":"math.AG"}
{"created":"2024-03-12 05:35:26","title":"From the Fokker-Planck equation to a contact Hamiltonian system","abstract":"The Fokker-Planck equation is one of the fundamental equations in nonequilibrium statistical mechanics, and this equation is known to be derived from the Wasserstein gradient flow equation with a free energy. This gradient flow equation describes relaxation processes and is formulated on Riemannian manifolds. Meanwhile contact Hamiltonian systems are also known to describe relaxation processes. Hence a relation between these two equations is expected to be clarified, which gives a solid foundation in geometric statistical mechanics. In this paper a class of contact Hamiltonian systems is derived from a class of the Fokker-Planck equations on Riemannian manifolds. In the course of the derivation, the Fokker-Planck equation is shown to be written as a diffusion equation with a weighted Laplacian without any approximation, which enables to employ a theory of eigenvalue problems.","sentences":["The Fokker-Planck equation is one of the fundamental equations in nonequilibrium statistical mechanics, and this equation is known to be derived from the Wasserstein gradient flow equation with a free energy.","This gradient flow equation describes relaxation processes and is formulated on Riemannian manifolds.","Meanwhile contact Hamiltonian systems are also known to describe relaxation processes.","Hence a relation between these two equations is expected to be clarified, which gives a solid foundation in geometric statistical mechanics.","In this paper a class of contact Hamiltonian systems is derived from a class of the Fokker-Planck equations on Riemannian manifolds.","In the course of the derivation, the Fokker-Planck equation is shown to be written as a diffusion equation with a weighted Laplacian without any approximation, which enables to employ a theory of eigenvalue problems."],"url":"http://arxiv.org/abs/2403.07334v1","category":"math-ph"}
{"created":"2024-03-12 05:30:01","title":"The potential of QQQ in the anisotropic background","abstract":"In this work, we use the AdS/CFT correspondence to study the behavior of a triply heavy baryon within anisotropic backgrounds. Beginning with the total action of the three quarks, we derive the balance equation for the three-quark system and compute the separation distance and potential energy. Our results reveal a consistent decrease in both the separation distance and potential energy for the A configuration and the B configuration as the anisotropy coefficient $a$ increases. This suggests that the presence of an anisotropic background promotes the dissolution of the three-quark system. Additionally, we compare the potential energies of the A and B configurations and observe that the A configuration has a slightly smaller potential energy, suggesting greater stability compared to the B configuration.","sentences":["In this work, we use the AdS/CFT correspondence to study the behavior of a triply heavy baryon within anisotropic backgrounds.","Beginning with the total action of the three quarks, we derive the balance equation for the three-quark system and compute the separation distance and potential energy.","Our results reveal a consistent decrease in both the separation distance and potential energy for the A configuration and the B configuration as the anisotropy coefficient $a$ increases.","This suggests that the presence of an anisotropic background promotes the dissolution of the three-quark system.","Additionally, we compare the potential energies of the A and B configurations and observe that the A configuration has a slightly smaller potential energy, suggesting greater stability compared to the B configuration."],"url":"http://arxiv.org/abs/2403.07330v1","category":"hep-ph"}
{"created":"2024-03-12 05:19:58","title":"Gauge Symmetries and Conserved Currents in AdS/BCFT","abstract":"In this paper, we study massless/massive vector and $p$-form field perturbations in AdS spacetime with an end-of-the-world brane. By imposing $U(1)$ preserving Neumann boundary condition on the end-of-the-world brane, we study their spectrum and discuss their implications for dual BCFT operators. When the perturbation is massless, the dual BCFT operator is a conserved current and we show that such an operator indeed satisfies the $U(1)$ preserving conformal boundary condition. On the other hand, when the perturbation is massive, in general there exists non-vanishing perpendicular components of the dual BCFT operator, even in the massless limit. We explain this difference between massless and massive perturbations from the point of view of the bulk gauge symmetry, or equivalently from different structure of equations of motion. We also find several brane-tension-independent modes in massless perturbations, and these are understood as boundary-condition-independent modes from the dual BCFT point of view.","sentences":["In this paper, we study massless/massive vector and $p$-form field perturbations in AdS spacetime with an end-of-the-world brane.","By imposing $U(1)$ preserving Neumann boundary condition on the end-of-the-world brane, we study their spectrum and discuss their implications for dual BCFT operators.","When the perturbation is massless, the dual BCFT operator is a conserved current and we show that such an operator indeed satisfies the $U(1)$ preserving conformal boundary condition.","On the other hand, when the perturbation is massive, in general there exists non-vanishing perpendicular components of the dual BCFT operator, even in the massless limit.","We explain this difference between massless and massive perturbations from the point of view of the bulk gauge symmetry, or equivalently from different structure of equations of motion.","We also find several brane-tension-independent modes in massless perturbations, and these are understood as boundary-condition-independent modes from the dual BCFT point of view."],"url":"http://arxiv.org/abs/2403.07325v1","category":"hep-th"}
{"created":"2024-03-12 05:09:25","title":"Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding","abstract":"Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence. We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design. By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. On general vector sources, LTC improves upon standard neural compressors in one-shot coding performance. LTC also enables neural compressors that perform block coding on i.i.d. vector sources, which yields coding gain over optimal one-shot coding.","sentences":["Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity.","Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded.","While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence.","We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design.","By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity.","On general vector sources, LTC improves upon standard neural compressors in one-shot coding performance.","LTC also enables neural compressors that perform block coding on i.i.d. vector sources, which yields coding gain over optimal one-shot coding."],"url":"http://arxiv.org/abs/2403.07320v1","category":"cs.IT"}
{"created":"2024-03-12 04:38:05","title":"How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance","abstract":"Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in the medium regime and all mean are close to zero, the learning performance is most desirable in the sense of a small sample complexity, a fast training rate, and a high average and group-level testing accuracy. Moreover, we show that increasing the fraction of the minority group in the training data does not necessarily improve the generalization performance of the minority group. Our theoretical results are validated on both synthetic and empirical datasets, such as CelebA and CIFAR-10 in image classification.","sentences":["Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group.","Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive.","By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance.","Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance.","Sample insights of our theoretical results include that when all group-level co-variance is in the medium regime and all mean are close to zero, the learning performance is most desirable in the sense of a small sample complexity, a fast training rate, and a high average and group-level testing accuracy.","Moreover, we show that increasing the fraction of the minority group in the training data does not necessarily improve the generalization performance of the minority group.","Our theoretical results are validated on both synthetic and empirical datasets, such as CelebA and CIFAR-10 in image classification."],"url":"http://arxiv.org/abs/2403.07310v1","category":"stat.ML"}
{"created":"2024-03-12 04:20:32","title":"A Spin model for global flat-foldability of random origami","abstract":"We map the problem of determining flat-foldability of the origami diagram onto the ground-state search problem of spin glass model on random graphs. If the origami diagram is locally flat-foldable around each vertex, a pre-folded diagram, showing the planar-positional relationship of the facet, can be obtained. For remaining combinatorial problem on layer ordering of facets can be described as a spin model. A spin variable is assigned for the layer-ordering of each pair of facets which have an overlap in the pre-folded diagram. The interactions to prohibit the intrusion of each facet into the other component of the same origami diagram are introduced among two or four spins. The flat-foldability of the diagram is closely related to the (non-)existence of frustrated loops on the spin model with the interactions on the random (hyper)graph.","sentences":["We map the problem of determining flat-foldability of the origami diagram onto the ground-state search problem of spin glass model on random graphs.","If the origami diagram is locally flat-foldable around each vertex, a pre-folded diagram, showing the planar-positional relationship of the facet, can be obtained.","For remaining combinatorial problem on layer ordering of facets can be described as a spin model.","A spin variable is assigned for the layer-ordering of each pair of facets which have an overlap in the pre-folded diagram.","The interactions to prohibit the intrusion of each facet into the other component of the same origami diagram are introduced among two or four spins.","The flat-foldability of the diagram is closely related to the (non-)existence of frustrated loops on the spin model with the interactions on the random (hyper)graph."],"url":"http://arxiv.org/abs/2403.07306v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-12 04:00:41","title":"Multiple elliptic integrals and differential equations","abstract":"We introduce and prove evaluations for families of multiple elliptic integrals by solving special types of ordinary and partial differential equations. As an application, we obtain new expressions of Ramanujan-type series of level 4 and associated singular values for the complete elliptic integral $\\mathbf K$ with integrals involving $\\mathbf K$.","sentences":["We introduce and prove evaluations for families of multiple elliptic integrals by solving special types of ordinary and partial differential equations.","As an application, we obtain new expressions of Ramanujan-type series of level 4 and associated singular values for the complete elliptic integral $\\mathbf K$ with integrals involving $\\mathbf K$."],"url":"http://arxiv.org/abs/2403.07298v1","category":"math.CA"}
{"created":"2024-03-12 03:26:58","title":"Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling","abstract":"Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and downstream tasks. Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in BMA performance.","sentences":["Transfer learning has recently shown significant performance across various tasks involving deep neural networks.","In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA).","While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data.","This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning.","The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and downstream tasks.","Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in BMA performance."],"url":"http://arxiv.org/abs/2403.07282v1","category":"cs.LG"}
{"created":"2024-03-12 03:26:48","title":"Locally constrained flows and geometric inequalities in hyperbolic space and sphere","abstract":"In this paper, we prove some families of geometric inequalities in hyperbolic space and sphere by some kinds of locally constrained flows. In hyperbolic space, we obtain the geometric inequalities involving two weighted curvature integrals for static convex domains. While in sphere, a new family of ``three terms'' geometric inequalities involving two weighted curvature integrals and one quermassintegral are proved. Unlike hyperbolic spaces, we also obtain an inverse weighted geometric inequality in sphere.","sentences":["In this paper, we prove some families of geometric inequalities in hyperbolic space and sphere by some kinds of locally constrained flows.","In hyperbolic space, we obtain the geometric inequalities involving two weighted curvature integrals for static convex domains.","While in sphere, a new family of ``three terms'' geometric inequalities involving two weighted curvature integrals and one quermassintegral are proved.","Unlike hyperbolic spaces, we also obtain an inverse weighted geometric inequality in sphere."],"url":"http://arxiv.org/abs/2403.07281v1","category":"math.DG"}
{"created":"2024-03-12 02:29:17","title":"Harmonic metrics for Higgs bundles of rank 3 in the Hitchin section","abstract":"Given a tuple of holomorphic differentials on a Riemann surface, one can define a Higgs bundle in the Hitchin section and a natural symmetric pairing of the Higgs bundle. We study whether a Higgs bundle of rank 3 in the Hitchin section has a compatible harmonic metric when the spectral curve is a 2-sheeted branched covering of the Riemann surface. In particular, we give a condition for Higgs bundles in the Hitchin section on $\\mathbb{C}$ or $\\mathbb{C}^*$ to have compatible harmonic metrics.","sentences":["Given a tuple of holomorphic differentials on a Riemann surface, one can define a Higgs bundle in the Hitchin section and a natural symmetric pairing of the Higgs bundle.","We study whether a Higgs bundle of rank 3 in the Hitchin section has a compatible harmonic metric when the spectral curve is a 2-sheeted branched covering of the Riemann surface.","In particular, we give a condition for Higgs bundles in the Hitchin section on $\\mathbb{C}$ or $\\mathbb{C}^*$ to have compatible harmonic metrics."],"url":"http://arxiv.org/abs/2403.07258v1","category":"math.DG"}
{"created":"2024-03-12 02:24:15","title":"Bourgain's counterexample in the sequential convergence problem for the Schr\u00f6dinger equation","abstract":"We study the problem of pointwise convegence for the Schr\\\"odinger operator on $\\mathbb R^n$ along time sequences. We show that the sharp counterexample to the sequential Schr\\\"odinger maximal estimate given recently by Li, Wang and Yan based in the construction by Luc\\`a and Rogers can also be achieved with the construction of Bourgain, and we extend it to the fractal setting.","sentences":["We study the problem of pointwise convegence for the Schr\\\"odinger operator on $\\mathbb R^n$ along time sequences.","We show that the sharp counterexample to the sequential Schr\\\"odinger maximal estimate given recently by Li, Wang and Yan based in the construction by Luc\\`a and Rogers can also be achieved with the construction of Bourgain, and we extend it to the fractal setting."],"url":"http://arxiv.org/abs/2403.07253v1","category":"math.AP"}
{"created":"2024-03-12 02:18:54","title":"Integrating Gauss-Bonnet Corrections in Quantum Field Theory: Implications for Torsion, Gauge Bosons, and the Hubble Constant","abstract":"We explore matter fields containing Gauss-Bonnet correction terms within the framework of renormalizable quantum field theory. By revising the gauge model with a charged scalar multiplier and two sets of fermion families within a flat universe model using torsion, we introduce Gauss-Bonnet corrections into the action to investigate field equations within the context of supersymmetric mixed inflationary models. After analytically computing the modified gauge boson field equations through the incorporation of Gauss-Bonnet theory into the fundamental field equations, we derive the torsion characteristics and energy-momentum tensor properties of a flat universe. Our analysis can extend to more specific Gauss-Bonnet correction models, enabling the derivation of gravitational characteristic equations with practical applications. This streamlines Gauss-Bonnet models, assesses the model's sensitivity to correction parameters, and explores high-sensitivity models with observational significance. Furthermore, in this derivation process, we identify that Gauss-Bonnet correction terms can directly impact the Hubble constant through Einstein's equations, indicating the potential for verifying Gauss-Bonnet theory through astronomical observations.","sentences":["We explore matter fields containing Gauss-Bonnet correction terms within the framework of renormalizable quantum field theory.","By revising the gauge model with a charged scalar multiplier and two sets of fermion families within a flat universe model using torsion, we introduce Gauss-Bonnet corrections into the action to investigate field equations within the context of supersymmetric mixed inflationary models.","After analytically computing the modified gauge boson field equations through the incorporation of Gauss-Bonnet theory into the fundamental field equations, we derive the torsion characteristics and energy-momentum tensor properties of a flat universe.","Our analysis can extend to more specific Gauss-Bonnet correction models, enabling the derivation of gravitational characteristic equations with practical applications.","This streamlines Gauss-Bonnet models, assesses the model's sensitivity to correction parameters, and explores high-sensitivity models with observational significance.","Furthermore, in this derivation process, we identify that Gauss-Bonnet correction terms can directly impact the Hubble constant through Einstein's equations, indicating the potential for verifying Gauss-Bonnet theory through astronomical observations."],"url":"http://arxiv.org/abs/2403.07250v1","category":"gr-qc"}
{"created":"2024-03-12 02:09:39","title":"GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation","abstract":"The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \\textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corresponding mask slice to incorporate both style information and anatomical guidance. This pipeline guarantees high fidelity and variability as well as exact alignment between generated CT volumes and tissue masks. Both qualitative and quantitative experiments on 3D abdominal CTs demonstrate a high performance of our proposed pipeline, thereby proving our method can serve as a dataset generator and provide potential benefits to downstream tasks. It is hoped that our work will offer a promising solution on the multimodality generation of CT and its anatomical mask. Our source code is publicly available at https://github.com/OvO1111/JointImageGeneration.","sentences":["The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating.","This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field.","As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints.","This paper explores this possibility and presents \\textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt.","Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks.","Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corresponding mask slice to incorporate both style information and anatomical guidance.","This pipeline guarantees high fidelity and variability as well as exact alignment between generated CT volumes and tissue masks.","Both qualitative and quantitative experiments on 3D abdominal CTs demonstrate a high performance of our proposed pipeline, thereby proving our method can serve as a dataset generator and provide potential benefits to downstream tasks.","It is hoped that our work will offer a promising solution on the multimodality generation of CT and its anatomical mask.","Our source code is publicly available at https://github.com/OvO1111/JointImageGeneration."],"url":"http://arxiv.org/abs/2403.07247v1","category":"eess.IV"}
{"created":"2024-03-12 01:03:42","title":"Exploring Multiscale Quantum Media: High-Precision Efficient Numerical Solution of the Fractional Schr\u00f6dinger equation, Eigenfunctions with Physical Potentials, and Fractionally-Enhanced Quantum Tunneling","abstract":"Fractional evolution equations lack generally accessible and well-converged codes excepting anomalous diffusion. A particular equation of strong interest to the growing intersection of applied mathematics and quantum information science and technology is the fractional Schr\\\"odinger equation, which describes sub-and super-dispersive behavior of quantum wavefunctions induced by multiscale media. We derive a computationally efficient sixth-order split-step numerical method to converge the eigenfunctions of the FSE to arbitrary numerical precision for arbitrary fractional order derivative. We demonstrate applications of this code to machine precision for classic quantum problems such as the finite well and harmonic oscillator, which take surprising twists due to the non-local nature of the fractional derivative. For example, the evanescent wave tails in the finite well take a Mittag-Leffer-like form which decay much slower than the well-known exponential from integer-order derivative wave theories, enhancing penetration into the barrier and therefore quantum tunneling rates. We call this effect \\emph{fractionally enhanced quantum tunneling}. This work includes an open source code for communities from quantum experimentalists to applied mathematicians to easily and efficiently explore the solutions of the fractional Schr\\\"odinger equation in a wide variety of practical potentials for potential realization in quantum tunneling enhancement and other quantum applications.","sentences":["Fractional evolution equations lack generally accessible and well-converged codes excepting anomalous diffusion.","A particular equation of strong interest to the growing intersection of applied mathematics and quantum information science and technology is the fractional Schr\\\"odinger equation, which describes sub-and super-dispersive behavior of quantum wavefunctions induced by multiscale media.","We derive a computationally efficient sixth-order split-step numerical method to converge the eigenfunctions of the FSE to arbitrary numerical precision for arbitrary fractional order derivative.","We demonstrate applications of this code to machine precision for classic quantum problems such as the finite well and harmonic oscillator, which take surprising twists due to the non-local nature of the fractional derivative.","For example, the evanescent wave tails in the finite well take a Mittag-Leffer-like form which decay much slower than the well-known exponential from integer-order derivative wave theories, enhancing penetration into the barrier and therefore quantum tunneling rates.","We call this effect \\emph{fractionally enhanced quantum tunneling}.","This work includes an open source code for communities from quantum experimentalists to applied mathematicians to easily and efficiently explore the solutions of the fractional Schr\\\"odinger equation in a wide variety of practical potentials for potential realization in quantum tunneling enhancement and other quantum applications."],"url":"http://arxiv.org/abs/2403.07233v1","category":"quant-ph"}
{"created":"2024-03-12 00:45:34","title":"Physics-constrained Active Learning for Soil Moisture Estimation and Optimal Sensor Placement","abstract":"Soil moisture is a crucial hydrological state variable that has significant importance to the global environment and agriculture. Precise monitoring of soil moisture in crop fields is critical to reducing agricultural drought and improving crop yield. In-situ soil moisture sensors, which are buried at pre-determined depths and distributed across the field, are promising solutions for monitoring soil moisture. However, high-density sensor deployment is neither economically feasible nor practical. Thus, to achieve a higher spatial resolution of soil moisture dynamics using a limited number of sensors, we integrate a physics-based agro-hydrological model based on Richards' equation in a physics-constrained deep learning framework to accurately predict soil moisture dynamics in the soil's root zone. This approach ensures that soil moisture estimates align well with sensor observations while obeying physical laws at the same time. Furthermore, to strategically identify the locations for sensor placement, we introduce a novel active learning framework that combines space-filling design and physics residual-based sampling to maximize data acquisition potential with limited sensors. Our numerical results demonstrate that integrating Physics-constrained Deep Learning (P-DL) with an active learning strategy within a unified framework--named the Physics-constrained Active Learning (P-DAL) framework--significantly improves the predictive accuracy and effectiveness of field-scale soil moisture monitoring using in-situ sensors.","sentences":["Soil moisture is a crucial hydrological state variable that has significant importance to the global environment and agriculture.","Precise monitoring of soil moisture in crop fields is critical to reducing agricultural drought and improving crop yield.","In-situ soil moisture sensors, which are buried at pre-determined depths and distributed across the field, are promising solutions for monitoring soil moisture.","However, high-density sensor deployment is neither economically feasible nor practical.","Thus, to achieve a higher spatial resolution of soil moisture dynamics using a limited number of sensors, we integrate a physics-based agro-hydrological model based on Richards' equation in a physics-constrained deep learning framework to accurately predict soil moisture dynamics in the soil's root zone.","This approach ensures that soil moisture estimates align well with sensor observations while obeying physical laws at the same time.","Furthermore, to strategically identify the locations for sensor placement, we introduce a novel active learning framework that combines space-filling design and physics residual-based sampling to maximize data acquisition potential with limited sensors.","Our numerical results demonstrate that integrating Physics-constrained Deep Learning (P-DL) with an active learning strategy within a unified framework--named the Physics-constrained Active Learning (P-DAL) framework--significantly improves the predictive accuracy and effectiveness of field-scale soil moisture monitoring using in-situ sensors."],"url":"http://arxiv.org/abs/2403.07228v1","category":"eess.SP"}
{"created":"2024-03-11 23:37:02","title":"Schauder-type estimates for fully nonlinear degenerate elliptic equations","abstract":"In this paper, we examine regularity estimates for solutions to fully nonlinear, degenerated elliptic equations, at interior vanishing source points. At these points, we obtain Schauder-type regularity estimates, which depend on the H\\\"older-like source-ellipticity vanishing rate.","sentences":["In this paper, we examine regularity estimates for solutions to fully nonlinear, degenerated elliptic equations, at interior vanishing source points.","At these points, we obtain Schauder-type regularity estimates, which depend on the H\\\"older-like source-ellipticity vanishing rate."],"url":"http://arxiv.org/abs/2403.07211v1","category":"math.AP"}
{"created":"2024-03-11 23:26:47","title":"The entropic doubling constant and robustness of Gaussian codebooks for additive-noise channels","abstract":"Entropy comparison inequalities are obtained for the differential entropy $h(X+Y)$ of the sum of two independent random vectors $X,Y$, when one is replaced by a Gaussian. For identically distributed random vectors $X,Y$, these are closely related to bounds on the entropic doubling constant, which quantifies the entropy increase when adding an independent copy of a random vector to itself. Consequences of both large and small doubling are explored. For the former, lower bounds are deduced on the entropy increase when adding an independent Gaussian, while for the latter, a qualitative stability result for the entropy power inequality is obtained. In the more general case of non-identically distributed random vectors $X,Y$, a Gaussian comparison inequality with interesting implications for channel coding is established: For additive-noise channels with a power constraint, Gaussian codebooks come within a $\\frac{{\\sf snr}}{3{\\sf snr}+2}$ factor of capacity. In the low-SNR regime this improves the half-a-bit additive bound of Zamir and Erez (2004). Analogous results are obtained for additive-noise multiple access channels, and for linear, additive-noise MIMO channels.","sentences":["Entropy comparison inequalities are obtained for the differential entropy $h(X+Y)$ of the sum of two independent random vectors $X,Y$, when one is replaced by a Gaussian.","For identically distributed random vectors $X,Y$, these are closely related to bounds on the entropic doubling constant, which quantifies the entropy increase when adding an independent copy of a random vector to itself.","Consequences of both large and small doubling are explored.","For the former, lower bounds are deduced on the entropy increase when adding an independent Gaussian, while for the latter, a qualitative stability result for the entropy power inequality is obtained.","In the more general case of non-identically distributed random vectors $X,Y$, a Gaussian comparison inequality with interesting implications for channel coding is established: For additive-noise channels with a power constraint, Gaussian codebooks come within a $\\frac{{\\sf snr}}{3{\\sf snr}+2}$ factor of capacity.","In the low-SNR regime this improves the half-a-bit additive bound of Zamir and Erez (2004).","Analogous results are obtained for additive-noise multiple access channels, and for linear, additive-noise MIMO channels."],"url":"http://arxiv.org/abs/2403.07209v1","category":"cs.IT"}
{"created":"2024-03-11 23:09:28","title":"Asmptotic properties of the Stokes flow in an exterior domain with slowly decaying initial data and its application to the Navier-Stokes equations","abstract":"In this paper, we study the decay rate of the Stokes flow in an exterior domain with a slowly decaying initial data ${\\bf u}_0(x)=O(|x|^{-\\al}), 0<\\al\\leq n$. %which is not $L^1$ integrable. As an application we find the unique strong solution of the Navier-Stokes equations corresponding to a slowly decaying initial data. We also derive the pointwise decay estimate of the Navier-Stokes flow. Our decay rates will be optimal compared with the decay rates of the heat flow.","sentences":["In this paper, we study the decay rate of the Stokes flow in an exterior domain with a slowly decaying initial data ${\\bf u}_0(x)=O(|x|^{-\\al}), 0<\\al\\leq","n$. %which is not $L^1$ integrable.","As an application we find the unique strong solution of the Navier-Stokes equations corresponding to a slowly decaying initial data.","We also derive the pointwise decay estimate of the Navier-Stokes flow.","Our decay rates will be optimal compared with the decay rates of the heat flow."],"url":"http://arxiv.org/abs/2403.07205v1","category":"math.AP"}
{"created":"2024-03-11 23:08:29","title":"How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?","abstract":"In this paper, we propose a novel abstraction-aware sketch-based image retrieval framework capable of handling sketch abstraction at varied levels. Prior works had mainly focused on tackling sub-factors such as drawing style and order, we instead attempt to model abstraction as a whole, and propose feature-level and retrieval granularity-level designs so that the system builds into its DNA the necessary means to interpret abstraction. On learning abstraction-aware features, we for the first-time harness the rich semantic embedding of pre-trained StyleGAN model, together with a novel abstraction-level mapper that deciphers the level of abstraction and dynamically selects appropriate dimensions in the feature matrix correspondingly, to construct a feature matrix embedding that can be freely traversed to accommodate different levels of abstraction. For granularity-level abstraction understanding, we dictate that the retrieval model should not treat all abstraction-levels equally and introduce a differentiable surrogate Acc.@q loss to inject that understanding into the system. Different to the gold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to narrow/broaden its focus in terms of how stringent the evaluation should be - the more abstract a sketch, the less stringent (higher $q$). Extensive experiments depict our method to outperform existing state-of-the-arts in standard SBIR tasks along with challenging scenarios like early retrieval, forensic sketch-photo matching, and style-invariant retrieval.","sentences":["In this paper, we propose a novel abstraction-aware sketch-based image retrieval framework capable of handling sketch abstraction at varied levels.","Prior works had mainly focused on tackling sub-factors such as drawing style and order, we instead attempt to model abstraction as a whole, and propose feature-level and retrieval granularity-level designs so that the system builds into its DNA the necessary means to interpret abstraction.","On learning abstraction-aware features, we for the first-time harness the rich semantic embedding of pre-trained StyleGAN model, together with a novel abstraction-level mapper that deciphers the level of abstraction and dynamically selects appropriate dimensions in the feature matrix correspondingly, to construct a feature matrix embedding that can be freely traversed to accommodate different levels of abstraction.","For granularity-level abstraction understanding, we dictate that the retrieval model should not treat all abstraction-levels equally and introduce a differentiable surrogate Acc.@q loss to inject that understanding into the system.","Different to the gold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to narrow/broaden its focus in terms of how stringent the evaluation should be - the more abstract a sketch, the less stringent (higher $q$).","Extensive experiments depict our method to outperform existing state-of-the-arts in standard SBIR tasks along with challenging scenarios like early retrieval, forensic sketch-photo matching, and style-invariant retrieval."],"url":"http://arxiv.org/abs/2403.07203v1","category":"cs.CV"}
{"created":"2024-03-11 22:47:07","title":"iRoCo: Intuitive Robot Control From Anywhere Using a Smartwatch","abstract":"This paper introduces iRoCo (intuitive Robot Control) - a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone. By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices. We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications. Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks. Additionally, iRoCo users complete drone piloting tasks 32\\% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire. Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smartphones from anywhere, at any time. The code is available at www.github.com/wearable-motion-capture","sentences":["This paper introduces iRoCo (intuitive Robot Control) - a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone.","By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices.","We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications.","Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks.","Additionally, iRoCo users complete drone piloting tasks 32\\% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire.","Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smartphones from anywhere, at any time.","The code is available at www.github.com/wearable-motion-capture"],"url":"http://arxiv.org/abs/2403.07199v1","category":"cs.RO"}
{"created":"2024-03-11 22:11:04","title":"A multiscale cavity method for sublinear-rank symmetric matrix factorization","abstract":"We consider a statistical model for symmetric matrix factorization with additive Gaussian noise in the high-dimensional regime where the rank $M$ of the signal matrix to infer scales with its size $N$ as $M = o(N^{1/10})$. Allowing for a $N$-dependent rank offers new challenges and requires new methods. Working in the Bayesian-optimal setting, we show that whenever the signal has i.i.d. entries the limiting mutual information between signal and data is given by a variational formula involving a rank-one replica symmetric potential. In other words, from the information-theoretic perspective, the case of a (slowly) growing rank is the same as when $M = 1$ (namely, the standard spiked Wigner model). The proof is primarily based on a novel multiscale cavity method allowing for growing rank along with some information-theoretic identities on worst noise for the Gaussian vector channel. We believe that the cavity method developed here will play a role in the analysis of a broader class of inference and spin models where the degrees of freedom are large arrays instead of vectors.","sentences":["We consider a statistical model for symmetric matrix factorization with additive Gaussian noise in the high-dimensional regime where the rank $M$ of the signal matrix to infer scales with its size $N$ as $M = o(N^{1/10})$. Allowing for a $N$-dependent rank offers new challenges and requires new methods.","Working in the Bayesian-optimal setting, we show that whenever the signal has i.i.d. entries the limiting mutual information between signal and data is given by a variational formula involving a rank-one replica symmetric potential.","In other words, from the information-theoretic perspective, the case of a (slowly) growing rank is the same as when $M = 1$ (namely, the standard spiked Wigner model).","The proof is primarily based on a novel multiscale cavity method allowing for growing rank along with some information-theoretic identities on worst noise for the Gaussian vector channel.","We believe that the cavity method developed here will play a role in the analysis of a broader class of inference and spin models where the degrees of freedom are large arrays instead of vectors."],"url":"http://arxiv.org/abs/2403.07189v1","category":"cs.IT"}
{"created":"2024-03-11 21:32:06","title":"Exploring the Impact of Extra Dimensions on Neutron Star Structure and Equation of State","abstract":"In this work, we explore the impact of higher dimensional spacetime on the stellar structure and thermodynamic properties of neutron stars. Utilizing the density-dependent relativistic hadron field theory, we introduce modifications to incorporate the influence of higher dimensionality, a novel approach not explored in existing literature to our best knowledge. Our methodology involves solving the essential stellar structure equations in D-dimensional spacetime ($D \\geq 4$), starting with the modification of the Einstein-Hilbert action, derivation of the Einstein field equation in D dimensions, and application of the resulting exterior Schwarzschild spacetime metric for D-dimension. Our findings reveal that with incremental dimensions, the central density $\\rho_{c} G_D$ and central pressure $p_c G_D$ gradually increase, leading to progressively stiffer neutron matter. Incremental dimensionality also results in a gradual increase in the maximum mass attained, limited to our study between $D=4$ and $D=6$, as no maximum mass value is obtained for $D>6$. We consistently observe the criteria $dM/d\\rho_c>0$ fulfilled up to the maximum mass point, supported by stability analysis against infinitesimal radial pulsations. The validity of our solution is confirmed through causality conditions, ensuring that the matter sound speed remains within the speed of light for all cases. Additionally, our examination indicates that the total mass-to-radius ratio for all discussed D-dimensional cases comfortably resides within the modified Buchdahl limit, which exhibits the physical validity of achieved results.","sentences":["In this work, we explore the impact of higher dimensional spacetime on the stellar structure and thermodynamic properties of neutron stars.","Utilizing the density-dependent relativistic hadron field theory, we introduce modifications to incorporate the influence of higher dimensionality, a novel approach not explored in existing literature to our best knowledge.","Our methodology involves solving the essential stellar structure equations in D-dimensional spacetime ($D \\geq 4$), starting with the modification of the Einstein-Hilbert action, derivation of the Einstein field equation in D dimensions, and application of the resulting exterior Schwarzschild spacetime metric for D-dimension.","Our findings reveal that with incremental dimensions, the central density $\\rho_{c} G_D$ and central pressure $p_c G_D$ gradually increase, leading to progressively stiffer neutron matter.","Incremental dimensionality also results in a gradual increase in the maximum mass attained, limited to our study between $D=4$ and $D=6$, as no maximum mass value is obtained for $D>6$. We consistently observe the criteria $dM/d\\rho_c>0$ fulfilled up to the maximum mass point, supported by stability analysis against infinitesimal radial pulsations.","The validity of our solution is confirmed through causality conditions, ensuring that the matter sound speed remains within the speed of light for all cases.","Additionally, our examination indicates that the total mass-to-radius ratio for all discussed D-dimensional cases comfortably resides within the modified Buchdahl limit, which exhibits the physical validity of achieved results."],"url":"http://arxiv.org/abs/2403.07174v1","category":"gr-qc"}
{"created":"2024-03-11 21:28:06","title":"Mixed virtual element approximation for the five-field formulation of the steady Boussinesq problem with temperature-dependent parameters","abstract":"In this work, we develop recent research on the fully mixed virtual element method (mixed-VEM) based on the Banach space for the stationary Boussinesq equation to suggest and analyze a new mixed-VEM for the stationary two-dimensional Boussinesq equation with temperature-dependent parameters in terms of the pseudostress, vorticity, velocity, pseudoheat vector and temperature fields. The well-posedness of the continuous formulation is analyzed utilizing a fixed-point strategy, a smallness assumption on the data, and some additional regularities on the solution. The discretization for the mentioned variables is based on the coupling $\\mathbb{H}(\\mathbf{div}_{6/5})$ -- and $\\mathbf{H}(\\mathrm{div}_{6/5})$ -- conforming virtual element techniques. The proposed scheme is rewritten as an equivalent fixed point operator equation, so that its existence and stability estimates have been proven. In addition, an a priori convergence analysis is established by utilizing the C\\'ea estimate and a suitable assumption on data for all variables in their natural norms showing an optimal rate of convergence. Finally, several numerical examples are presented to illustrate the performance of the proposed method.","sentences":["In this work, we develop recent research on the fully mixed virtual element method (mixed-VEM) based on the Banach space for the stationary Boussinesq equation to suggest and analyze a new mixed-VEM for the stationary two-dimensional Boussinesq equation with temperature-dependent parameters in terms of the pseudostress, vorticity, velocity, pseudoheat vector and temperature fields.","The well-posedness of the continuous formulation is analyzed utilizing a fixed-point strategy, a smallness assumption on the data, and some additional regularities on the solution.","The discretization for the mentioned variables is based on the coupling $\\mathbb{H}(\\mathbf{div}_{6/5})$ -- and $\\mathbf{H}(\\mathrm{div}_{6/5})$ -- conforming virtual element techniques.","The proposed scheme is rewritten as an equivalent fixed point operator equation, so that its existence and stability estimates have been proven.","In addition, an a priori convergence analysis is established by utilizing the C\\'ea estimate and a suitable assumption on data for all variables in their natural norms showing an optimal rate of convergence.","Finally, several numerical examples are presented to illustrate the performance of the proposed method."],"url":"http://arxiv.org/abs/2403.07173v1","category":"math.NA"}
{"created":"2024-03-11 21:10:22","title":"Stationary phase analysis of ambient noise cross-correlations: Focusing on non-ballistic arrivals","abstract":"Stacked cross-correlation functions have become ubiquitous in the ambient seismic imaging and monitoring community as approximations to the Green's function between two receivers. While theoretical understanding of this approximation to the ballistic arrivals is well established, the equivalent analysis for the non-ballistic arrivals is alarmingly inadequate compared to the exponential growth of its applications. To provide a fundamental understanding of the cross-correlation functions beyond the ballistic arrivals, we derive analytical stationary phase solutions for ambient noise cross-correlations with a focus on non-ballistic arrivals. We establish the mathematical and corresponding physical conditions that drastically differentiate the non-ballistic arrivals in the stacked cross-correlation and the actual Green's functions. In ambient noise environments, the coda waves due to random medium scatterings of an impulsive source cannot be distinguished from the cross-talk artifacts due to overlapping random noise sources. Therefore, changes in the non-ballistic arrivals cannot be uniquely attributed to changes in the medium or changes in the noise source environment without additional constraints. The theoretical results demand that interpreting large-elapse-time arrivals in the stacked cross-correlation functions as coda waves for deterministic information about the propagation medium should be conducted only after the source influence is sufficiently ruled out. Once the source influence is eliminated, the stationary phase solutions for scattering waves provide a solid basis for extracting reliable scattering information from the noise correlation functions for higher-resolution imaging and monitoring.","sentences":["Stacked cross-correlation functions have become ubiquitous in the ambient seismic imaging and monitoring community as approximations to the Green's function between two receivers.","While theoretical understanding of this approximation to the ballistic arrivals is well established, the equivalent analysis for the non-ballistic arrivals is alarmingly inadequate compared to the exponential growth of its applications.","To provide a fundamental understanding of the cross-correlation functions beyond the ballistic arrivals, we derive analytical stationary phase solutions for ambient noise cross-correlations with a focus on non-ballistic arrivals.","We establish the mathematical and corresponding physical conditions that drastically differentiate the non-ballistic arrivals in the stacked cross-correlation and the actual Green's functions.","In ambient noise environments, the coda waves due to random medium scatterings of an impulsive source cannot be distinguished from the cross-talk artifacts due to overlapping random noise sources.","Therefore, changes in the non-ballistic arrivals cannot be uniquely attributed to changes in the medium or changes in the noise source environment without additional constraints.","The theoretical results demand that interpreting large-elapse-time arrivals in the stacked cross-correlation functions as coda waves for deterministic information about the propagation medium should be conducted only after the source influence is sufficiently ruled out.","Once the source influence is eliminated, the stationary phase solutions for scattering waves provide a solid basis for extracting reliable scattering information from the noise correlation functions for higher-resolution imaging and monitoring."],"url":"http://arxiv.org/abs/2403.07167v1","category":"physics.geo-ph"}
{"created":"2024-03-11 21:05:25","title":"How closed is cosmology?","abstract":"Classical cosmology exhibits a particular kind of scaling symmetry. The dynamics of the invariants of this symmetry forms a system that exhibits many of the features of open systems such as the non-conservation of mechanical energy and the focusing of measures along the dynamical flow. From these properties, we show that important dynamical features emerge that are not present in closed systems. In particular, a large and physically plausible class of cosmological models give rise to a natural arrow of time. We then argue that the appropriate notion of closure in cosmology is dynamical closure - that a system can be integrated without reference to external factors. This is realised in physical systems in terms of the algebraic closure of the equations of motion such that the system is autonomous. Remarkably, in a growing class of models it can be shown that the autonomous system obtained remains regular and can be integrated through the big bang.","sentences":["Classical cosmology exhibits a particular kind of scaling symmetry.","The dynamics of the invariants of this symmetry forms a system that exhibits many of the features of open systems such as the non-conservation of mechanical energy and the focusing of measures along the dynamical flow.","From these properties, we show that important dynamical features emerge that are not present in closed systems.","In particular, a large and physically plausible class of cosmological models give rise to a natural arrow of time.","We then argue that the appropriate notion of closure in cosmology is dynamical closure - that a system can be integrated without reference to external factors.","This is realised in physical systems in terms of the algebraic closure of the equations of motion such that the system is autonomous.","Remarkably, in a growing class of models it can be shown that the autonomous system obtained remains regular and can be integrated through the big bang."],"url":"http://arxiv.org/abs/2403.07161v1","category":"gr-qc"}
{"created":"2024-03-11 21:03:11","title":"Essential self-adjointness of $\\left(\u0394^2 +c|x|^{-4}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^n \\backslash \\{0\\})}$","abstract":"Let $n\\in\\mathbb{N}, n\\geq 2$. We prove that the strongly singular differential operator \\[\\left(\\Delta^2 +c|x|^{-4}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^n \\backslash \\{0\\})}, \\quad c \\in \\mathbb{R}, \\] is essentially self-adjoint in $L^2(\\mathbb{R}^n; d^n x)$ if and only if \\[c\\geq \\begin{cases}3(n+2)(6-n)&\\mbox{for $2\\leq n\\leq 5$};\\\\[5pt] {\\displaystyle -\\frac{n(n+4)(n-4)(n-8)}{16}}&\\mbox{for $n\\geq 6$}.\\end{cases}\\]   Via separation of variables, our proof reduces to studying the essential self-adjointness on the space $C_0^{\\infty}((0,\\infty))$ of fourth-order Euler-type differential operators of the form \\[ \\frac{d^4}{dr^4}+c_1\\left(\\frac{1}{r^2}\\frac{d^2}{dr^2}+\\frac{d^2}{dr^2}\\frac{1}{r^2}\\right)+\\frac{c_2}{r^4},\\quad r\\in(0,\\infty),\\quad(c_1,c_2)\\in \\mathbb{R}^2,\\] in $L^2((0,\\infty);dr)$.   Our methods generalize to differential operators related to higher-order powers of the Laplacian, however, there are some nontrivial subtleties that arise. For example, the natural expectation that for $m,n\\in\\mathbb{N}$, $n \\geq 2$, there exist $c_{m,n}\\in\\mathbb{R}$ such that $\\left(\\Delta^m+c|x|^{-2m}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^n \\backslash \\{0\\})}$ is essentially self-adjoint in $L^2(\\mathbb{R}^n; d^n x)$ if and only if $c \\geq c_{m,n}$, turns out to be false. Indeed, for $n=20$, we prove that the differential operator \\[ \\left((-\\Delta)^5+c|x|^{-10}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^{20} \\backslash \\{0\\})}, \\quad c \\in \\mathbb{R},\\] is essentially self-adjoint in $L^2\\big( \\mathbb{R}^{20}; d^{20} x\\big)$ if and only if $c\\in [0,\\beta]\\cup [\\gamma,\\infty)$, where $\\beta\\approx 1.0436\\times 10^{10}$, and $\\gamma\\approx 1.8324\\times 10^{10}$ are the two real roots of the quartic equation \\begin{align*}&3125z^4-83914629120000z^3+429438995162964368031744 z^2\\\\&\\quad+1045471534388841527438982355353600z\\\\&\\quad +629847004905001626921946285352115240960000=0.\\end{align*}","sentences":["Let $n\\in\\mathbb{N},","n\\geq 2$. We prove that the strongly singular differential operator \\[\\left(\\Delta^2 +c|x|^{-4}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^n \\backslash \\{0\\})}, \\quad c \\in \\mathbb{R}, \\] is essentially self-adjoint in $L^2(\\mathbb{R}^n; d^n x)$ if and only if \\[c\\geq \\begin{cases}3(n+2)(6-n)&\\mbox{for $2\\leq n\\leq 5$};\\\\[5pt]","{\\displaystyle -\\frac{n(n+4)(n-4)(n-8)}{16}}&\\mbox{for $n\\geq 6$}.\\end{cases}\\]   Via separation of variables, our proof reduces to studying the essential self-adjointness on the space $C_0^{\\infty}((0,\\infty))$ of fourth-order Euler-type differential operators of the form \\[ \\frac{d^4}{dr^4}+c_1\\left(\\frac{1}{r^2}\\frac{d^2}{dr^2}+\\frac{d^2}{dr^2}\\frac{1}{r^2}\\right)+\\frac{c_2}{r^4},\\quad r\\in(0,\\infty),\\quad(c_1,c_2)\\in \\mathbb{R}^2,\\] in $L^2((0,\\infty);dr)$.   Our methods generalize to differential operators related to higher-order powers of the Laplacian, however, there are some nontrivial subtleties that arise.","For example, the natural expectation that for $m,n\\in\\mathbb{N}$, $n \\geq 2$, there exist $c_{m,n}\\in\\mathbb{R}$ such that $\\left(\\Delta^m+c|x|^{-2m}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^n \\backslash \\{0\\})}$ is essentially self-adjoint in $L^2(\\mathbb{R}^n; d^n x)$ if and only if $c \\geq c_{m,n}$, turns out to be false.","Indeed, for $n=20$, we prove that the differential operator \\[ \\left((-\\Delta)^5+c|x|^{-10}\\right)\\big|_{C_0^{\\infty}(\\mathbb{R}^{20} \\backslash \\{0\\})}, \\quad c \\in \\mathbb{R},\\] is essentially self-adjoint in $L^2\\big( \\mathbb{R}^{20}; d^{20} x\\big)$","if and only if $c\\in [0,\\beta]\\cup [\\gamma,\\infty)$, where $\\beta\\approx 1.0436\\times 10^{10}$, and $\\gamma\\approx 1.8324\\times 10^{10}$ are the two real roots of the quartic equation \\begin{align*}&3125z^4-83914629120000z^3+429438995162964368031744 z^2\\\\&\\quad+1045471534388841527438982355353600z\\\\&\\quad +629847004905001626921946285352115240960000=0.\\end{align*}"],"url":"http://arxiv.org/abs/2403.07160v1","category":"math.SP"}
{"created":"2024-03-11 21:02:10","title":"Existence for a Nonlocal Multi-Species Advection Diffusion Equation","abstract":"We establish short-time existence of bounded, smooth non-negative solutions to a multi-species advection diffusion equation for a wide class of singular interaction kernels. We also give conditions on the interaction matrix, whose coefficients determine species attraction or repulsion, which ensure global existence of solutions.","sentences":["We establish short-time existence of bounded, smooth non-negative solutions to a multi-species advection diffusion equation for a wide class of singular interaction kernels.","We also give conditions on the interaction matrix, whose coefficients determine species attraction or repulsion, which ensure global existence of solutions."],"url":"http://arxiv.org/abs/2403.07159v1","category":"math.AP"}
{"created":"2024-03-11 20:52:31","title":"A Real-time Dyson Expansion Scheme: Efficient Inclusion of Dynamical Correlations in Non-equilibrium Spectral Propertie","abstract":"Time resolved photoemission spectroscopy is the key technique to probe the real-time non-equilibrium dynamics of electronic states. Theoretical predictions of the time dependent spectral function for realistic systems is however, a challenge. Employing the Kadanoff-Baym equations to find this quantity results in a cubic scaling in the total number of time steps, quickly becoming prohibitive and often fail quantitatively and even qualitatively. In comparison, mean field methods have more favorable numerical scaling both in the number number of time steps and in the complexity associated with the cost of a evolving for a single time step, however they miss key spectral properties such as emergent spectral features. Here we present a scheme that allows for the inclusion of dynamical correlations to the spectral function while maintaining the same scaling in the number of time steps as for mean field approaches, while capturing the emergent physics. Further, the scheme can be efficiently implemented on top of equilibrium real-time many-body perturbation theory schemes and codes. We see excellent agreement with exact results for test systems. Furthermore we exemplify the method on a periodic system and demonstrate clear evidence that our proposed scheme produces complex spectral features including excitonic band replicas, features that are not observed using static mean field approaches.","sentences":["Time resolved photoemission spectroscopy is the key technique to probe the real-time non-equilibrium dynamics of electronic states.","Theoretical predictions of the time dependent spectral function for realistic systems is however, a challenge.","Employing the Kadanoff-Baym equations to find this quantity results in a cubic scaling in the total number of time steps, quickly becoming prohibitive and often fail quantitatively and even qualitatively.","In comparison, mean field methods have more favorable numerical scaling both in the number number of time steps and in the complexity associated with the cost of a evolving for a single time step, however they miss key spectral properties such as emergent spectral features.","Here we present a scheme that allows for the inclusion of dynamical correlations to the spectral function while maintaining the same scaling in the number of time steps as for mean field approaches, while capturing the emergent physics.","Further, the scheme can be efficiently implemented on top of equilibrium real-time many-body perturbation theory schemes and codes.","We see excellent agreement with exact results for test systems.","Furthermore we exemplify the method on a periodic system and demonstrate clear evidence that our proposed scheme produces complex spectral features including excitonic band replicas, features that are not observed using static mean field approaches."],"url":"http://arxiv.org/abs/2403.07155v1","category":"physics.comp-ph"}
{"created":"2024-03-11 20:23:59","title":"One Category One Prompt: Dataset Distillation using Diffusion Models","abstract":"The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models. Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets. By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets.","sentences":["The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts.","Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples.","However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization.","Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation.","Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images.","In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models.","Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets.","By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget.","We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets."],"url":"http://arxiv.org/abs/2403.07142v1","category":"cs.CV"}
{"created":"2024-03-11 20:19:32","title":"Effect of Ir growth pressure on the domain wall dynamics in Ta/Pt/Co/Ir/Ta stacks","abstract":"The dynamical response of magnetic domain walls to external magnetic fields in ultra-thin multilayer magnetic films is determined not only by the composition and thickness of the layers but also by the growth conditions. Growth conditions can induce significant structural changes inside the layers and at the interfaces between them, affecting in particular the dynamics of domain walls, their mobility, elastic tension, and the pinning forces acting on them. In this work, we focus specifically on the effect of Ir layer growth pressure in Ta/Pt/Co/Ir/Ta ultra-thin multilayers films. Measurements of the DC magnetic properties, domain wall velocity and domain morphology in the creep regime for both constant and alternating field pulses, were performed for a batch of samples where the Ir layer was grown at different pressures. We find that the saturation magnetization, the effective anisotropy constant and the domain wall surface tension grow with increasing pressure and saturate at a threshold pressure, while the Dzyaloshinskii-Moriya field and the strength of the disorder remain practically unaltered over the range of pressures considered.","sentences":["The dynamical response of magnetic domain walls to external magnetic fields in ultra-thin multilayer magnetic films is determined not only by the composition and thickness of the layers but also by the growth conditions.","Growth conditions can induce significant structural changes inside the layers and at the interfaces between them, affecting in particular the dynamics of domain walls, their mobility, elastic tension, and the pinning forces acting on them.","In this work, we focus specifically on the effect of Ir layer growth pressure in Ta/Pt/Co/Ir/Ta ultra-thin multilayers films.","Measurements of the DC magnetic properties, domain wall velocity and domain morphology in the creep regime for both constant and alternating field pulses, were performed for a batch of samples where the Ir layer was grown at different pressures.","We find that the saturation magnetization, the effective anisotropy constant and the domain wall surface tension grow with increasing pressure and saturate at a threshold pressure, while the Dzyaloshinskii-Moriya field and the strength of the disorder remain practically unaltered over the range of pressures considered."],"url":"http://arxiv.org/abs/2403.07141v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-11 20:14:20","title":"Properties of Dynamical Black Hole Entropy","abstract":"We study the first law for non-stationary perturbations of a stationary black hole whose event horizon is a Killing horizon, that relates the first-order change in the mass and angular momentum to the change in the entropy of an arbitrary horizon cross-section. Recently, Hollands, Wald and Zhang [1] have shown that the dynamical black hole entropy that satisfies this first law, for general relativity, is $S_{\\text{dyn}}=(1-v\\partial_v)S_{\\text{BH}}$, where $v$ is the affine parameter of the null horizon generators and $S_{\\text{BH}}$ is the Bekenstein-Hawking entropy, and for general diffeomorphism covariant theories of gravity $S_{\\text{dyn}}=(1-v\\partial_v)S_{\\text{Wall}}$, where $S_{\\text{Wall}}$ is the Wall entropy. They obtained the first law by applying the Noether charge method to non-stationary perturbations and arbitrary cross-sections. In this formalism, the dynamical black hole entropy is defined as an \"improved\" Noether charge, which is unambiguous to first order in the perturbation. In the present article we provide a pedagogical derivation of the physical process version of the non-stationary first law for general relativity by integrating the linearised Raychaudhuri equation between two arbitrary horizon cross-sections. Moreover, we generalise the derivation of the first law in [1] to non-minimally coupled matter fields, using boost weight arguments rather than Killing field arguments, and we relax some of the gauge conditions on the perturbations by allowing for non-zero variations of the horizon Killing field and surface gravity. Finally, for $f(\\text{Riemann})$ theories of gravity we show explicitly using Gaussian null coordinates that the improved Noether charge is $S_{\\text{dyn}}=(1-v\\partial_v)S_{\\text{Wall}}$, which is a non-trivial check of [1].","sentences":["We study the first law for non-stationary perturbations of a stationary black hole whose event horizon is a Killing horizon, that relates the first-order change in the mass and angular momentum to the change in the entropy of an arbitrary horizon cross-section.","Recently, Hollands, Wald and Zhang [1] have shown that the dynamical black hole entropy that satisfies this first law, for general relativity, is $S_{\\text{dyn}}=(1-v\\partial_v)S_{\\text{BH}}$, where $v$ is the affine parameter of the null horizon generators and $S_{\\text{BH}}$ is the Bekenstein-Hawking entropy, and for general diffeomorphism covariant theories of gravity $S_{\\text{dyn}}=(1-v\\partial_v)S_{\\text{Wall}}$, where $S_{\\text{Wall}}$ is the Wall entropy.","They obtained the first law by applying the Noether charge method to non-stationary perturbations and arbitrary cross-sections.","In this formalism, the dynamical black hole entropy is defined as an \"improved\" Noether charge, which is unambiguous to first order in the perturbation.","In the present article we provide a pedagogical derivation of the physical process version of the non-stationary first law for general relativity by integrating the linearised Raychaudhuri equation between two arbitrary horizon cross-sections.","Moreover, we generalise the derivation of the first law in [1] to non-minimally coupled matter fields, using boost weight arguments rather than Killing field arguments, and we relax some of the gauge conditions on the perturbations by allowing for non-zero variations of the horizon Killing field and surface gravity.","Finally, for $f(\\text{Riemann})$ theories of gravity we show explicitly using Gaussian null coordinates that the improved Noether charge is $S_{\\text{dyn}}=(1-v\\partial_v)S_{\\text{Wall}}$, which is a non-trivial check of [1]."],"url":"http://arxiv.org/abs/2403.07140v1","category":"hep-th"}
{"created":"2024-03-11 20:02:17","title":"A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis Degree Classification","abstract":"Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a respiratory disorder that affects the health and welfare of the dogs with various symptoms. In this paper, a new annotated dataset composed of 190 images of bulldogs' nostrils is presented. Three degrees of stenosis are approximately equally represented in the dataset: mild, moderate and severe stenosis. The dataset also comprises a small quantity of non stenotic nostril images. To the best of our knowledge, this is the first image dataset addressing this problem. Furthermore, deep learning is investigated as an alternative to automatically infer stenosis degree using nostril images. In this work, several neural networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT. For this evaluation, the problem was modeled in two different ways: first, as a three-class classification problem (mild or open, moderate, and severe); second, as a binary classification problem, with severe stenosis as target. For the multiclass classification, a maximum median f-score of 53.77\\% was achieved by the MobileNetV3. For binary classification, a maximum median f-score of 72.08\\% has been reached by ResNet50, indicating that the problem is challenging but possibly tractable.","sentences":["Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a respiratory disorder that affects the health and welfare of the dogs with various symptoms.","In this paper, a new annotated dataset composed of 190 images of bulldogs' nostrils is presented.","Three degrees of stenosis are approximately equally represented in the dataset: mild, moderate and severe stenosis.","The dataset also comprises a small quantity of non stenotic nostril images.","To the best of our knowledge, this is the first image dataset addressing this problem.","Furthermore, deep learning is investigated as an alternative to automatically infer stenosis degree using nostril images.","In this work, several neural networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT. For this evaluation, the problem was modeled in two different ways: first, as a three-class classification problem (mild or open, moderate, and severe); second, as a binary classification problem, with severe stenosis as target.","For the multiclass classification, a maximum median f-score of 53.77\\% was achieved by the MobileNetV3.","For binary classification, a maximum median f-score of 72.08\\% has been reached by ResNet50, indicating that the problem is challenging but possibly tractable."],"url":"http://arxiv.org/abs/2403.07132v1","category":"eess.IV"}
{"created":"2024-03-11 19:54:30","title":"Selecting energy-momentum trace dependent gravity theories with LSS","abstract":"We study scalar cosmological perturbations in $f(R, T)$ modified gravity theories being $T$ the trace of the energy-momentum tensor. We provide detailed equations for the matter energy density contrast. We solve then numerically to promote a comparison with available large scale structure (LSS) formation observational data on $f \\sigma_8$ and also addressing the $S_8$ tension. We identify $f(R,T)$ models that lead either to growth enhancement or suppression. Since recent results in the literature indicate a preference for the latter feature, this type of analysis is quite useful to select viable modifications of gravity. We studied class of such $f(R,T)$ models are either ruled out or severely restricted.","sentences":["We study scalar cosmological perturbations in $f(R, T)$ modified gravity theories being $T$ the trace of the energy-momentum tensor.","We provide detailed equations for the matter energy density contrast.","We solve then numerically to promote a comparison with available large scale structure (LSS) formation observational data on $f \\sigma_8$ and also addressing the $S_8$ tension.","We identify $f(R,T)$ models that lead either to growth enhancement or suppression.","Since recent results in the literature indicate a preference for the latter feature, this type of analysis is quite useful to select viable modifications of gravity.","We studied class of such $f(R,T)$ models are either ruled out or severely restricted."],"url":"http://arxiv.org/abs/2403.07130v1","category":"gr-qc"}
{"created":"2024-03-11 19:51:01","title":"FAX: Scalable and Differentiable Federated Primitives in JAX","abstract":"We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at https://github.com/google-research/google-research/tree/master/fax .","sentences":["We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications.","FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways.","FAX embeds building blocks for federated computations as primitives in JAX.","This enables three key benefits.","First, FAX computations can be translated to XLA HLO.","Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations.","Last, FAX computations can be interpreted out to existing production cross-device federated compute systems.","We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center.","FAX is available at https://github.com/google-research/google-research/tree/master/fax ."],"url":"http://arxiv.org/abs/2403.07128v1","category":"cs.DC"}
{"created":"2024-03-11 19:20:39","title":"Solvability of a class of systems of quadratic integral equations","abstract":"The article is devoted to the existence of solutions of a certain system of quadratic integral equations in H^1(R, R^N). We show the existence of a perturbed solution by using a fixed point technique in the Sobolev space on the real line.","sentences":["The article is devoted to the existence of solutions of a certain system of quadratic integral equations in H^1(R, R^N).","We show the existence of a perturbed solution by using a fixed point technique in the Sobolev space on the real line."],"url":"http://arxiv.org/abs/2403.07119v1","category":"math.AP"}
{"created":"2024-03-11 18:44:36","title":"Overcoming the Paradox of Certified Training with Gaussian Smoothing","abstract":"Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challenging due to high computational cost, our results clearly demonstrate the promise of Gaussian Loss Smoothing for training certifiably robust neural networks.","sentences":["Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts.","While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations.","Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations.","In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues.","We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations.","When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network.","While scaling PGPE-based training remains challenging due to high computational cost, our results clearly demonstrate the promise of Gaussian Loss Smoothing for training certifiably robust neural networks."],"url":"http://arxiv.org/abs/2403.07095v1","category":"cs.LG"}
{"created":"2024-03-11 18:33:21","title":"Graph learning methods to extract empathy supporting regions in a naturalistic stimuli fMRI","abstract":"Functional MRI (fMRI) research, employing naturalistic stimuli like movies, explores brain network interactions in complex cognitive processes such as empathy. The empathy network encompasses multiple brain areas, including the Insula, PFC, ACC, and parietal regions. Our novel processing pipeline applies graph learning methods to whole-brain timeseries signals, incorporating high-pass filtering, voxel-level clustering, and windowed graph learning with a sparsity-based approach. The study involves two short movies shown to 14 healthy volunteers, considering 54 regions extracted from the AAL Atlas. The sparsity-based graph learning consistently outperforms, achieving over 88% accuracy in capturing emotion contagion variations. Temporal analysis reveals a gradual induction of empathy, supported by the method's effectiveness in capturing dynamic connectomes through graph clustering. Edge-weight dynamics analysis underscores sparsity-based learning's superiority, while connectome-network analysis highlights the pivotal role of the Insula, Amygdala, and Thalamus in empathy. Spectral filtering analysis emphasizes the band-pass filter's significance in isolating regions linked to emotional and empathetic processing during empathy HIGH states. Key regions like Amygdala, Insula, and Angular Gyrus consistently activate, supporting their critical role in immediate emotional responses. Strong similarities across movies in graph cluster labels, connectome-network analysis, and spectral filtering-based analyses reveal robust neural correlates of empathy. These findings advance our understanding of empathy-related neural dynamics and identify specific regions in empathetic responses, offering insights for targeted interventions and treatments associated with empathetic processing.","sentences":["Functional MRI (fMRI) research, employing naturalistic stimuli like movies, explores brain network interactions in complex cognitive processes such as empathy.","The empathy network encompasses multiple brain areas, including the Insula, PFC, ACC, and parietal regions.","Our novel processing pipeline applies graph learning methods to whole-brain timeseries signals, incorporating high-pass filtering, voxel-level clustering, and windowed graph learning with a sparsity-based approach.","The study involves two short movies shown to 14 healthy volunteers, considering 54 regions extracted from the AAL Atlas.","The sparsity-based graph learning consistently outperforms, achieving over 88% accuracy in capturing emotion contagion variations.","Temporal analysis reveals a gradual induction of empathy, supported by the method's effectiveness in capturing dynamic connectomes through graph clustering.","Edge-weight dynamics analysis underscores sparsity-based learning's superiority, while connectome-network analysis highlights the pivotal role of the Insula, Amygdala, and Thalamus in empathy.","Spectral filtering analysis emphasizes the band-pass filter's significance in isolating regions linked to emotional and empathetic processing during empathy HIGH states.","Key regions like Amygdala, Insula, and Angular Gyrus consistently activate, supporting their critical role in immediate emotional responses.","Strong similarities across movies in graph cluster labels, connectome-network analysis, and spectral filtering-based analyses reveal robust neural correlates of empathy.","These findings advance our understanding of empathy-related neural dynamics and identify specific regions in empathetic responses, offering insights for targeted interventions and treatments associated with empathetic processing."],"url":"http://arxiv.org/abs/2403.07089v1","category":"q-bio.NC"}
{"created":"2024-03-11 18:19:06","title":"Is the effective potential, effective for dynamics?","abstract":"We critically examine the applicability of the effective potential within dynamical situations and find, in short, that the answer is negative. An important caveat of the use of an effective potential in dynamical equations of motion is an explicit violation of energy conservation.   An \\emph{adiabatic} effective potential is introduced in a consistent quasi-static approximation, and its narrow regime of validity is discussed. Two ubiquitous instances in which even the adiabatic effective potential is not valid in dynamics are studied in detail: parametric amplification in the case of oscillating mean fields, and spinodal instabilities associated with spontaneous symmetry breaking. In both cases profuse particle production is directly linked to the failure of the effective potential to describe the dynamics. We introduce a consistent, renormalized, energy conserving dynamical framework that is amenable to numerical implementation. Energy conservation leads to the emergence of asymptotic highly excited, entangled stationary states from the dynamical evolution. As a corollary, decoherence via dephasing of the density matrix in the adiabatic basis is argued to lead to an emergent entropy, formally equivalent to the entanglement entropy. The results suggest novel characterization of asymptotic equilibrium states in terms of order parameter vs. energy density.","sentences":["We critically examine the applicability of the effective potential within dynamical situations and find, in short, that the answer is negative.","An important caveat of the use of an effective potential in dynamical equations of motion is an explicit violation of energy conservation.   ","An \\emph{adiabatic} effective potential is introduced in a consistent quasi-static approximation, and its narrow regime of validity is discussed.","Two ubiquitous instances in which even the adiabatic effective potential is not valid in dynamics are studied in detail: parametric amplification in the case of oscillating mean fields, and spinodal instabilities associated with spontaneous symmetry breaking.","In both cases profuse particle production is directly linked to the failure of the effective potential to describe the dynamics.","We introduce a consistent, renormalized, energy conserving dynamical framework that is amenable to numerical implementation.","Energy conservation leads to the emergence of asymptotic highly excited, entangled stationary states from the dynamical evolution.","As a corollary, decoherence via dephasing of the density matrix in the adiabatic basis is argued to lead to an emergent entropy, formally equivalent to the entanglement entropy.","The results suggest novel characterization of asymptotic equilibrium states in terms of order parameter vs. energy density."],"url":"http://arxiv.org/abs/2403.07084v1","category":"hep-ph"}
{"created":"2024-03-11 18:14:40","title":"Viscous current-induced forces","abstract":"We study the motion (translational, vibrational, and rotational) of a diatomic impurity immersed in an electron liquid and exposed to electronic current. An approach based on the linear response time-dependent density functional theory combined with the Ehrenfest dynamics leads to a system of linear algebraic equations, which account for the competing and counteracting effects of the current-induced force (electron wind) and the electronic friction. These forces, by means of the dynamic exchange-correlation kernel $f_{xc}({\\bf r},{\\bf r}',\\omega)$, include the electronic viscosity contribution. Starting from the ground state at the equilibrium inter-nuclear distance and applying a current pulse, we observe three phases of the motion: (I) acceleration due to the prevalence of the current-induced force, (II) stabilization upon balancing of the two forces, and (III) deceleration due to the friction after the end of the pulse. The viscous contribution to the force largely increases the acceleration (deceleration) at the first (third) phase of the process. For the aluminium HEG electron density, we find this correction to amount to up to 70% of the total electron wind and friction effects.","sentences":["We study the motion (translational, vibrational, and rotational) of a diatomic impurity immersed in an electron liquid and exposed to electronic current.","An approach based on the linear response time-dependent density functional theory combined with the Ehrenfest dynamics leads to a system of linear algebraic equations, which account for the competing and counteracting effects of the current-induced force (electron wind) and the electronic friction.","These forces, by means of the dynamic exchange-correlation kernel $f_{xc}({\\bf r},{\\bf r}',\\omega)$, include the electronic viscosity contribution.","Starting from the ground state at the equilibrium inter-nuclear distance and applying a current pulse, we observe three phases of the motion: (I) acceleration due to the prevalence of the current-induced force, (II) stabilization upon balancing of the two forces, and (III) deceleration due to the friction after the end of the pulse.","The viscous contribution to the force largely increases the acceleration (deceleration) at the first (third) phase of the process.","For the aluminium HEG electron density, we find this correction to amount to up to 70% of the total electron wind and friction effects."],"url":"http://arxiv.org/abs/2403.07081v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 18:00:01","title":"Dark Matter-induced electron excitations in silicon and germanium with Deep Learning","abstract":"We train a deep neural network (DNN) to output rates of dark matter (DM) induced electron excitations in silicon and germanium detectors. Our DNN provides a massive speedup of around $5$ orders of magnitude relative to existing methods (i.e. QEdark-EFT), allowing for extensive parameter scans in the event of an observed DM signal. The network is also lighter and simpler to use than alternative computational frameworks based on a direct calculation of the DM-induced excitation rate. The DNN can be downloaded $\\href{https://github.com/urdshals/DEDD}{\\text{here}}$.","sentences":["We train a deep neural network (DNN) to output rates of dark matter (DM) induced electron excitations in silicon and germanium detectors.","Our DNN provides a massive speedup of around $5$ orders of magnitude relative to existing methods (i.e. QEdark-EFT), allowing for extensive parameter scans in the event of an observed DM signal.","The network is also lighter and simpler to use than alternative computational frameworks based on a direct calculation of the DM-induced excitation rate.","The DNN can be downloaded $\\href{https://github.com/urdshals/DEDD}{\\text{here}}$."],"url":"http://arxiv.org/abs/2403.07053v1","category":"hep-ph"}
{"created":"2024-03-11 17:50:20","title":"POD-ROM methods: from a finite set of snapshots to continuous-in-time approximations","abstract":"This paper studies discretization of time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Most of the analysis in the literature has been performed on fully-discrete methods using first order methods in time, typically the implicit Euler time integrator. Our aim is to show which kind of error bounds can be obtained using any time integrator, both in the full order model (FOM), applied to compute the snapshots, and in the POD-ROM method. To this end, we analyze in this paper the continuous-in-time case for both the FOM and POD-ROM methods, although the POD basis is obtained from snapshots taken at a discrete (i.e., not continuous) set times. Two cases for the set of snapshots are considered: The case in which the snapshots are based on first order divided differences in time and the case in which they are based on temporal derivatives. Optimal pointwise-in-time error bounds {between the FOM and the POD-ROM solutions} are proved for the $L^2(\\Omega)$ norm of the error for a semilinear reaction-diffusion model problem. The dependency of the errors on the distance in time between two consecutive snapshots and on the tail of the POD eigenvalues is tracked. Our detailed analysis allows to show that, in some situations, a small number of snapshots in a given time interval might be sufficient to accurately approximate the solution in the full interval. Numerical studies support the error analysis.","sentences":["This paper studies discretization of time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs).","Most of the analysis in the literature has been performed on fully-discrete methods using first order methods in time, typically the implicit Euler time integrator.","Our aim is to show which kind of error bounds can be obtained using any time integrator, both in the full order model (FOM), applied to compute the snapshots, and in the POD-ROM method.","To this end, we analyze in this paper the continuous-in-time case for both the FOM and POD-ROM methods, although the POD basis is obtained from snapshots taken at a discrete (i.e., not continuous) set times.","Two cases for the set of snapshots are considered: The case in which the snapshots are based on first order divided differences in time and the case in which they are based on temporal derivatives.","Optimal pointwise-in-time error bounds {between the FOM and the POD-ROM solutions} are proved for the $L^2(\\Omega)$ norm of the error for a semilinear reaction-diffusion model problem.","The dependency of the errors on the distance in time between two consecutive snapshots and on the tail of the POD eigenvalues is tracked.","Our detailed analysis allows to show that, in some situations, a small number of snapshots in a given time interval might be sufficient to accurately approximate the solution in the full interval.","Numerical studies support the error analysis."],"url":"http://arxiv.org/abs/2403.06967v1","category":"math.NA"}
{"created":"2024-03-11 17:45:10","title":"Notes on solitary-wave solutions of Rosenau-type equations","abstract":"The present paper is concerned with the existence of solitary wave solutions of Rosenau-type equations. By using two standard theories, Normal Form Theory and Concentration-Compactness Theory, some results of existence of solitary waves of three different forms are derived. The results depend on some conditions on the speed of the waves with respect to the parameters of the equations. They are discussed for several families of Rosenau equations present in the literature. The analysis is illustrated with a numerical study of generation of approximate solitary-wave profiles from a numerical procedure based on the Petviashvili iteration.","sentences":["The present paper is concerned with the existence of solitary wave solutions of Rosenau-type equations.","By using two standard theories, Normal Form Theory and Concentration-Compactness Theory, some results of existence of solitary waves of three different forms are derived.","The results depend on some conditions on the speed of the waves with respect to the parameters of the equations.","They are discussed for several families of Rosenau equations present in the literature.","The analysis is illustrated with a numerical study of generation of approximate solitary-wave profiles from a numerical procedure based on the Petviashvili iteration."],"url":"http://arxiv.org/abs/2403.06958v1","category":"math.AP"}
{"created":"2024-03-11 17:19:40","title":"On the stability of fully nonlinear hydraulic-fall solutions to the forced water-wave problem","abstract":"Two-dimensional free-surface flow over localised topography is examined with the emphasis on the stability of hydraulic-fall solutions. A Gaussian topography profile is assumed with a positive or negative amplitude modelling a bump or a dip, respectively. Steady hydraulic-fall solutions to the full incompressible, irrotational Euler equations are computed, and their linear and nonlinear stability is analysed by computing eigenspectra of the pertinent linearised operator and by solving an initial value problem. The computations are carried out numerically using a specially developed computational framework based on the finite element method. The Hamiltonian structure of the problem is demonstrated and stability is determined by computing eigenspectra of the pertinent linearised operator. It is found that a hydraulic-fall flow over a bump is spectrally stable. The corresponding flow over a dip is found to be linearly unstable. In the latter case, time-dependent simulations show that the flow ultimately settles into a time-periodic motion that corresponds to an invariant solution in an appropriately defined phase space. Physically, the solution consists of a localised large amplitude wave that pulsates above the dip while simultaneously emitting nonlinear cnoidal waves in the upstream direction and multi-harmonic linear waves in the downstream direction.","sentences":["Two-dimensional free-surface flow over localised topography is examined with the emphasis on the stability of hydraulic-fall solutions.","A Gaussian topography profile is assumed with a positive or negative amplitude modelling a bump or a dip, respectively.","Steady hydraulic-fall solutions to the full incompressible, irrotational Euler equations are computed, and their linear and nonlinear stability is analysed by computing eigenspectra of the pertinent linearised operator and by solving an initial value problem.","The computations are carried out numerically using a specially developed computational framework based on the finite element method.","The Hamiltonian structure of the problem is demonstrated and stability is determined by computing eigenspectra of the pertinent linearised operator.","It is found that a hydraulic-fall flow over a bump is spectrally stable.","The corresponding flow over a dip is found to be linearly unstable.","In the latter case, time-dependent simulations show that the flow ultimately settles into a time-periodic motion that corresponds to an invariant solution in an appropriately defined phase space.","Physically, the solution consists of a localised large amplitude wave that pulsates above the dip while simultaneously emitting nonlinear cnoidal waves in the upstream direction and multi-harmonic linear waves in the downstream direction."],"url":"http://arxiv.org/abs/2403.06933v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 17:11:31","title":"A unified diagrammatic approach in Liouville space to quantum transport for bosonic and fermionic reservoirs","abstract":"We present a diagrammatic approach to quantum transport based on a master equation formalism in Liouville space. It can be applied to linear and nonlinear transport in generic multi-level junctions coupled to bosonic or fermionic reservoirs and presents a convenient perturbation expansion in the strength of the coupling between the reservoirs and the junction. The Redfield theory is recovered at second order, with the partial and full secular master equations discussed. Analytical, approximate expressions are provided up to fourth order for the steady-state boson transport that generalize to multi-level systems the known formula for the low-temperature thermal conductance in the spin-boson model. The formalism is applied to the problem of heat transport in a qubit-resonator junction modeled by the quantum Rabi model. Nontrivial transport features emerge as a result of the interplay between the qubit-oscillator detuning and coupling strength. For quasi-degenerate spectra, nonvanishing steady-state coherences cause a suppression of the thermal conductance.","sentences":["We present a diagrammatic approach to quantum transport based on a master equation formalism in Liouville space.","It can be applied to linear and nonlinear transport in generic multi-level junctions coupled to bosonic or fermionic reservoirs and presents a convenient perturbation expansion in the strength of the coupling between the reservoirs and the junction.","The Redfield theory is recovered at second order, with the partial and full secular master equations discussed.","Analytical, approximate expressions are provided up to fourth order for the steady-state boson transport that generalize to multi-level systems the known formula for the low-temperature thermal conductance in the spin-boson model.","The formalism is applied to the problem of heat transport in a qubit-resonator junction modeled by the quantum Rabi model.","Nontrivial transport features emerge as a result of the interplay between the qubit-oscillator detuning and coupling strength.","For quasi-degenerate spectra, nonvanishing steady-state coherences cause a suppression of the thermal conductance."],"url":"http://arxiv.org/abs/2403.06923v1","category":"quant-ph"}
{"created":"2024-03-11 17:10:40","title":"Kantowski-Sachs and Bianchi III dynamics in $f\\left(Q\\right)$-gravity","abstract":"We explore the phase-space of homogeneous and anisotropic spacetimes within symmetric teleparallel $f(Q)$-gravity. Specifically, we consider the Kantowski-Sachs and locally rotational Bianchi III geometries to describe the physical space. By analyzing the phase-space, we reconstruct the cosmological history dictated by $f(Q)$-gravity and comment about the theory's viability. Our findings suggest that the free parameters of the connection must be constrained to eliminate nonlinear terms in the field equations. Consequently, new stationary points emerge, rendering the theory cosmologically viable. We identify the existence of anisotropic accelerated universes, which may correspond to the pre-inflationary epoch.","sentences":["We explore the phase-space of homogeneous and anisotropic spacetimes within symmetric teleparallel $f(Q)$-gravity.","Specifically, we consider the Kantowski-Sachs and locally rotational Bianchi III geometries to describe the physical space.","By analyzing the phase-space, we reconstruct the cosmological history dictated by $f(Q)$-gravity and comment about the theory's viability.","Our findings suggest that the free parameters of the connection must be constrained to eliminate nonlinear terms in the field equations.","Consequently, new stationary points emerge, rendering the theory cosmologically viable.","We identify the existence of anisotropic accelerated universes, which may correspond to the pre-inflationary epoch."],"url":"http://arxiv.org/abs/2403.06922v1","category":"gr-qc"}
{"created":"2024-03-11 16:26:06","title":"Ant Colony Sampling with GFlowNets for Combinatorial Optimization","abstract":"This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \\url{https://github.com/ai4co/gfacs}.","sentences":["This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization.","GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology.","GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances.","Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS.","Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems.","The source code is available at \\url{https://github.com/ai4co/gfacs}."],"url":"http://arxiv.org/abs/2403.07041v1","category":"cs.LG"}
{"created":"2024-03-11 16:23:42","title":"On the Generalization Ability of Unsupervised Pretraining","abstract":"Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Autoencoder pre-training with deep transformers, followed by fine-tuning on a binary classification task. Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of fine-tuned model. Overall, our results contribute to a better understanding of unsupervised pre-training and fine-tuning paradigm, and can shed light on the design of more effective pre-training algorithms.","sentences":["Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization.","However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking.","Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage.","To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks.","We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Autoencoder pre-training with deep transformers, followed by fine-tuning on a binary classification task.","Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of fine-tuned model.","Overall, our results contribute to a better understanding of unsupervised pre-training and fine-tuning paradigm, and can shed light on the design of more effective pre-training algorithms."],"url":"http://arxiv.org/abs/2403.06871v1","category":"cs.LG"}
{"created":"2024-03-11 16:22:03","title":"Exact Multi-Point Correlations in the Stochastic Heat Equation for Strictly Sublinear Coordinates","abstract":"We consider the Stochastic Heat Equation (SHE) in $(1+1)$ dimensions with delta Dirac initial data and spacetime white noise. We prove exact large-time asymptotics for multi-point correlations of the SHE for strictly sublinear space coordinates. The sublinear condition is optimal, in the sense that different asymptotics are known to occur when the space coordinates grow linearly [Lin 2023, Theorem 1.1]. Lastly, a notable feature of our result is that it confirms the connection between multi-point correlations in the SHE and the ground state of the Hamiltonian of the delta-Bose gas.","sentences":["We consider the Stochastic Heat Equation (SHE) in $(1+1)$ dimensions with delta Dirac initial data and spacetime white noise.","We prove exact large-time asymptotics for multi-point correlations of the SHE for strictly sublinear space coordinates.","The sublinear condition is optimal, in the sense that different asymptotics are known to occur when the space coordinates grow linearly [Lin 2023, Theorem 1.1].","Lastly, a notable feature of our result is that it confirms the connection between multi-point correlations in the SHE and the ground state of the Hamiltonian of the delta-Bose gas."],"url":"http://arxiv.org/abs/2403.06868v1","category":"math.PR"}
{"created":"2024-03-11 16:11:57","title":"Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs","abstract":"Mesh texture synthesis is a key component in the automatic generation of 3D content. Existing learning-based methods have drawbacks -- either by disregarding the shape manifold during texture generation or by requiring a large number of different views to mitigate occlusion-related inconsistencies. In this paper, we present a novel surface-aware approach for mesh texture synthesis that overcomes these drawbacks by leveraging the pre-trained weights of 2D Convolutional Neural Networks (CNNs) with the same architecture, but with convolutions designed for 3D meshes. Our proposed network keeps track of the oriented patches surrounding each texel, enabling seamless texture synthesis and retaining local similarity to classical 2D convolutions with square kernels. Our approach allows us to synthesize textures that account for the geometric content of mesh surfaces, eliminating discontinuities and achieving comparable quality to 2D image synthesis algorithms. We compare our approach with state-of-the-art methods where, through qualitative and quantitative evaluations, we demonstrate that our approach is more effective for a variety of meshes and styles, while also producing visually appealing and consistent textures on meshes.","sentences":["Mesh texture synthesis is a key component in the automatic generation of 3D content.","Existing learning-based methods have drawbacks -- either by disregarding the shape manifold during texture generation or by requiring a large number of different views to mitigate occlusion-related inconsistencies.","In this paper, we present a novel surface-aware approach for mesh texture synthesis that overcomes these drawbacks by leveraging the pre-trained weights of 2D Convolutional Neural Networks (CNNs) with the same architecture, but with convolutions designed for 3D meshes.","Our proposed network keeps track of the oriented patches surrounding each texel, enabling seamless texture synthesis and retaining local similarity to classical 2D convolutions with square kernels.","Our approach allows us to synthesize textures that account for the geometric content of mesh surfaces, eliminating discontinuities and achieving comparable quality to 2D image synthesis algorithms.","We compare our approach with state-of-the-art methods where, through qualitative and quantitative evaluations, we demonstrate that our approach is more effective for a variety of meshes and styles, while also producing visually appealing and consistent textures on meshes."],"url":"http://arxiv.org/abs/2403.06855v1","category":"cs.GR"}
{"created":"2024-03-11 16:03:35","title":"ExoCubed: A Riemann-Solver based Cubed-Sphere Dynamic Core for Planetary Atmospheres","abstract":"The computational fluid dynamics on a sphere is relevant to global simulations of geophysical fluid dynamics. Using the conventional spherical-polar (or lat-lon) grid results in a singularity at the poles, with orders of magnitude smaller cell sizes at the poles in comparison to the equator. To address this problem, we developed a general circulation model (dynamic core) with a gnomonic equiangular cubed-sphere configuration. This model is developed based on the Simulating Nonhydrostatic Atmospheres on Planets (SNAP) model, using a finite volume numerical scheme with a Riemann-solver-based dynamic core and the vertical implicit correction (VIC) scheme. This change of the horizontal configuration gives a 20-time acceleration of global simulations compared to the lat-lon grid with a similar number of cells at medium resolution. We presented standard tests ranging from 2D shallow-water models to 3D general circulation tests, including earth-like planets and shallow hot Jupiters, to validate the accuracy of the model. The method described in this article is generic to transform any existing finite-volume hydrodynamic model in the Cartesian geometry to the spherical geometry.","sentences":["The computational fluid dynamics on a sphere is relevant to global simulations of geophysical fluid dynamics.","Using the conventional spherical-polar (or lat-lon) grid results in a singularity at the poles, with orders of magnitude smaller cell sizes at the poles in comparison to the equator.","To address this problem, we developed a general circulation model (dynamic core) with a gnomonic equiangular cubed-sphere configuration.","This model is developed based on the Simulating Nonhydrostatic Atmospheres on Planets (SNAP) model, using a finite volume numerical scheme with a Riemann-solver-based dynamic core and the vertical implicit correction (VIC) scheme.","This change of the horizontal configuration gives a 20-time acceleration of global simulations compared to the lat-lon grid with a similar number of cells at medium resolution.","We presented standard tests ranging from 2D shallow-water models to 3D general circulation tests, including earth-like planets and shallow hot Jupiters, to validate the accuracy of the model.","The method described in this article is generic to transform any existing finite-volume hydrodynamic model in the Cartesian geometry to the spherical geometry."],"url":"http://arxiv.org/abs/2403.06844v1","category":"astro-ph.EP"}
{"created":"2024-03-11 16:01:07","title":"Inverse Garment and Pattern Modeling with a Differentiable Simulator","abstract":"The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns. This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS). Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry. Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication. We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern.","sentences":["The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world.","This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry.","To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns.","This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS).","Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry.","Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication.","We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern."],"url":"http://arxiv.org/abs/2403.06841v1","category":"cs.GR"}
{"created":"2024-03-11 15:51:56","title":"Relativistic Roche problem for stars in precessing orbits around a spinning black hole","abstract":"Tidal disruptions of stars on the equatorial plane orbiting Kerr black holes have been widely studied. However thus far, there have been fewer studies of stars in inclined precessing orbits around a Kerr black hole. In this paper, by using tensor virial equations, we show the presence of possible resonances in these systems for typical physical parameters of black hole-neutron star binaries in close orbits or of a white dwarf/an ordinary star orbiting a supermassive black hole. This suggests the presence of a new instability before the tidal disruption limit is encountered in such systems.","sentences":["Tidal disruptions of stars on the equatorial plane orbiting Kerr black holes have been widely studied.","However thus far, there have been fewer studies of stars in inclined precessing orbits around a Kerr black hole.","In this paper, by using tensor virial equations, we show the presence of possible resonances in these systems for typical physical parameters of black hole-neutron star binaries in close orbits or of a white dwarf/an ordinary star orbiting a supermassive black hole.","This suggests the presence of a new instability before the tidal disruption limit is encountered in such systems."],"url":"http://arxiv.org/abs/2403.06834v1","category":"astro-ph.HE"}
{"created":"2024-03-11 15:40:40","title":"Noise-induced transitions past the onset of a steady symmetry-breaking bifurcation: the case of the sudden expansion","abstract":"We consider fluid flows, governed by the Navier-Stokes equations, subject to a steady symmetry-breaking bifurcation and forced by a weak noise acting on a slow time scale. By generalizing the multiple-scale weakly nonlinear expansion technique employed in the literature for the response of the Duffing oscillator, we rigorously derive a stochastically forced Stuart-Landau equation for the dominant symmetry-breaking mode. The probability density function of the solution, and of the escape time from one attractor to the other, are then determined by solving the associated Fokker-Planck equation. The validity of this reduced order model is tested on the flow past a sudden expansion, for a given Reynolds number and different noise amplitudes. At a very low numerical cost, the statistics obtained from the amplitude equation accurately reproduce those of long-time direct numerical simulations.","sentences":["We consider fluid flows, governed by the Navier-Stokes equations, subject to a steady symmetry-breaking bifurcation and forced by a weak noise acting on a slow time scale.","By generalizing the multiple-scale weakly nonlinear expansion technique employed in the literature for the response of the Duffing oscillator, we rigorously derive a stochastically forced Stuart-Landau equation for the dominant symmetry-breaking mode.","The probability density function of the solution, and of the escape time from one attractor to the other, are then determined by solving the associated Fokker-Planck equation.","The validity of this reduced order model is tested on the flow past a sudden expansion, for a given Reynolds number and different noise amplitudes.","At a very low numerical cost, the statistics obtained from the amplitude equation accurately reproduce those of long-time direct numerical simulations."],"url":"http://arxiv.org/abs/2403.06824v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 15:31:25","title":"Deep Learning Approaches for Human Action Recognition in Video Data","abstract":"Human action recognition in videos is a critical task with significant implications for numerous applications, including surveillance, sports analytics, and healthcare. The challenge lies in creating models that are both precise in their recognition capabilities and efficient enough for practical use. This study conducts an in-depth analysis of various deep learning models to address this challenge. Utilizing a subset of the UCF101 Videos dataset, we focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Two-Stream ConvNets. The research reveals that while CNNs effectively capture spatial features and RNNs encode temporal sequences, Two-Stream ConvNets exhibit superior performance by integrating spatial and temporal dimensions. These insights are distilled from the evaluation metrics of accuracy, precision, recall, and F1-score. The results of this study underscore the potential of composite models in achieving robust human action recognition and suggest avenues for future research in optimizing these models for real-world deployment.","sentences":["Human action recognition in videos is a critical task with significant implications for numerous applications, including surveillance, sports analytics, and healthcare.","The challenge lies in creating models that are both precise in their recognition capabilities and efficient enough for practical use.","This study conducts an in-depth analysis of various deep learning models to address this challenge.","Utilizing a subset of the UCF101 Videos dataset, we focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Two-Stream ConvNets.","The research reveals that while CNNs effectively capture spatial features and RNNs encode temporal sequences, Two-Stream ConvNets exhibit superior performance by integrating spatial and temporal dimensions.","These insights are distilled from the evaluation metrics of accuracy, precision, recall, and F1-score.","The results of this study underscore the potential of composite models in achieving robust human action recognition and suggest avenues for future research in optimizing these models for real-world deployment."],"url":"http://arxiv.org/abs/2403.06810v1","category":"cs.CV"}
{"created":"2024-03-11 15:23:35","title":"On the Robustness of Lexicase Selection to Contradictory Objectives","abstract":"Lexicase and epsilon-lexicase selection are state of the art parent selection techniques for problems featuring multiple selection criteria. Originally, lexicase selection was developed for cases where these selection criteria are unlikely to be in conflict with each other, but preliminary work suggests it is also a highly effective many-objective optimization algorithm. However, to predict whether these results generalize, we must understand lexicase selection's performance on contradictory objectives. Prior work has shown mixed results on this question. Here, we develop theory identifying circumstances under which lexicase selection will succeed or fail to find a Pareto-optimal solution. To make this analysis tractable, we restrict our investigation to a theoretical problem with maximally contradictory objectives. Ultimately, we find that lexicase and epsilon-lexicase selection each have a region of parameter space where they are incapable of optimizing contradictory objectives. Outside of this region, however, they perform well despite the presence of contradictory objectives. Based on these findings, we propose theoretically-backed guidelines for parameter choice. Additionally, we identify other properties that may affect whether a many-objective optimization problem is a good fit for lexicase or epsilon-lexicase selection.","sentences":["Lexicase and epsilon-lexicase selection are state of the art parent selection techniques for problems featuring multiple selection criteria.","Originally, lexicase selection was developed for cases where these selection criteria are unlikely to be in conflict with each other, but preliminary work suggests it is also a highly effective many-objective optimization algorithm.","However, to predict whether these results generalize, we must understand lexicase selection's performance on contradictory objectives.","Prior work has shown mixed results on this question.","Here, we develop theory identifying circumstances under which lexicase selection will succeed or fail to find a Pareto-optimal solution.","To make this analysis tractable, we restrict our investigation to a theoretical problem with maximally contradictory objectives.","Ultimately, we find that lexicase and epsilon-lexicase selection each have a region of parameter space where they are incapable of optimizing contradictory objectives.","Outside of this region, however, they perform well despite the presence of contradictory objectives.","Based on these findings, we propose theoretically-backed guidelines for parameter choice.","Additionally, we identify other properties that may affect whether a many-objective optimization problem is a good fit for lexicase or epsilon-lexicase selection."],"url":"http://arxiv.org/abs/2403.06805v1","category":"cs.NE"}
{"created":"2024-03-11 15:23:11","title":"Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction","abstract":"We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data. SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape. During the process, an unsupervised functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction. To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations. SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy. Our code can be found online: https://github.com/pvnieo/SNK","sentences":["We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data.","SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape.","During the process, an unsupervised functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction.","To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations.","SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy.","Our code can be found online: https://github.com/pvnieo/SNK"],"url":"http://arxiv.org/abs/2403.06804v1","category":"cs.CV"}
{"created":"2024-03-11 15:19:52","title":"Joint Source-and-Channel Coding for Small Satellite Applications","abstract":"Small satellites are widely used today as cost effective means to perform Earth observation and other tasks that generate large amounts of high-dimensional data, such as multi-spectral imagery. These satellites typically operate in low earth orbit, which poses significant challenges for data transmission due to short contact times with ground stations, low bandwidth, and high packet loss probabilities. In this paper, we introduce JSCC-Sat, which applies joint source-and-channel coding using neural networks to provide efficient and robust transmission of compressed image data for satellite applications. We evaluate our mechanism against traditional transmission schemes with separate source and channel coding and demonstrate that it outperforms the existing approaches when applied to Earth observation data of the Sentinel-2 mission.","sentences":["Small satellites are widely used today as cost effective means to perform Earth observation and other tasks that generate large amounts of high-dimensional data, such as multi-spectral imagery.","These satellites typically operate in low earth orbit, which poses significant challenges for data transmission due to short contact times with ground stations, low bandwidth, and high packet loss probabilities.","In this paper, we introduce JSCC-Sat, which applies joint source-and-channel coding using neural networks to provide efficient and robust transmission of compressed image data for satellite applications.","We evaluate our mechanism against traditional transmission schemes with separate source and channel coding and demonstrate that it outperforms the existing approaches when applied to Earth observation data of the Sentinel-2 mission."],"url":"http://arxiv.org/abs/2403.06802v1","category":"cs.NI"}
{"created":"2024-03-11 15:13:37","title":"Jeffery Orbits with Noise Revisited","abstract":"The behavior of non-spherical particles in a shear-flow is of significant practical and theoretical interest. These systems have been the object of numerous investigations since the pioneering work of Jeffery a century ago. His eponymous orbits describe the deterministic motion of an isolated, rod-like particle in a shear flow. Subsequently, the effect of adding noise was investigated. The theory has been applied to colloidal particles, macromolecules, anisometric granular particles and most recently to microswimmers, for example bacteria. We study the Jeffery orbits of elongated particles subject to noise using Langevin simulations and a Fokker-Planck equation. We extend the analytical solution for infinitely thin needles ($\\beta=1$) obtained by Doi and Edwards to particles with arbitrary shape factor ($0\\le \\beta\\le 1$) and validate the theory by comparing it with simulations. We examine the rotation of the particle around the vorticity axis and study the orientational order matrix. We use the latter to obtain scalar order parameters $s$ and $r$ describing nematic ordering and biaxiality from the orientational distribution function. The value of $s$ (nematic ordering) increases monotonically with increasing P\\'eclet number, while $r$ (measure of biaxiality) displays a maximum value. From perturbation theory we obtain simple expressions that provide accurate descriptions at low noise (or large P\\'eclet numbers). We also examine the orientational distribution in the v-grad v plane and in the perpendicular direction. Finally we present the solution of the Fokker-Planck equation for a strictly two-dimensional (2D) system. For the same noise amplitude the average rotation speed of the particle in 3D is larger than in 2D.","sentences":["The behavior of non-spherical particles in a shear-flow is of significant practical and theoretical interest.","These systems have been the object of numerous investigations since the pioneering work of Jeffery a century ago.","His eponymous orbits describe the deterministic motion of an isolated, rod-like particle in a shear flow.","Subsequently, the effect of adding noise was investigated.","The theory has been applied to colloidal particles, macromolecules, anisometric granular particles and most recently to microswimmers, for example bacteria.","We study the Jeffery orbits of elongated particles subject to noise using Langevin simulations and a Fokker-Planck equation.","We extend the analytical solution for infinitely thin needles ($\\beta=1$) obtained by Doi and Edwards to particles with arbitrary shape factor ($0\\le \\beta\\le 1$) and validate the theory by comparing it with simulations.","We examine the rotation of the particle around the vorticity axis and study the orientational order matrix.","We use the latter to obtain scalar order parameters $s$ and $r$ describing nematic ordering and biaxiality from the orientational distribution function.","The value of $s$ (nematic ordering) increases monotonically with increasing P\\'eclet number, while $r$ (measure of biaxiality) displays a maximum value.","From perturbation theory we obtain simple expressions that provide accurate descriptions at low noise (or large P\\'eclet numbers).","We also examine the orientational distribution in the v-grad v plane and in the perpendicular direction.","Finally we present the solution of the Fokker-Planck equation for a strictly two-dimensional (2D) system.","For the same noise amplitude the average rotation speed of the particle in 3D is larger than in 2D."],"url":"http://arxiv.org/abs/2403.06795v1","category":"cond-mat.soft"}
{"created":"2024-03-12 17:32:52","title":"A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce","abstract":"In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers. Consequently, ecommerce enterprises frequently. Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform. Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions. The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers. To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of Poisson Gamma model to predict customer order patterns with precision. This paper provides an in-depth exploration of the strategic fusion of machine learning and an empirical Bayesian approach, bolstered by the judicious selection of pertinent features. This innovative approach has yielded a remarkable 3 times increase in customer order rates, show casing its potential for transformative impact in the ecommerce industry.","sentences":["In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers.","Consequently, ecommerce enterprises frequently.","Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform.","Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions.","The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers.","To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of Poisson Gamma model to predict customer order patterns with precision.","This paper provides an in-depth exploration of the strategic fusion of machine learning and an empirical Bayesian approach, bolstered by the judicious selection of pertinent features.","This innovative approach has yielded a remarkable 3 times increase in customer order rates, show casing its potential for transformative impact in the ecommerce industry."],"url":"http://arxiv.org/abs/2403.07843v1","category":"cs.LG"}
{"created":"2024-03-12 15:54:32","title":"Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets","abstract":"Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the reliability of existing datasets and methodologies, ensuring that machine learning models genuinely learn what they are designed to recognize.","sentences":["Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP.","However, the integrity of these datasets is seldom scrutinized for text-dependency.","This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features.","By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling.","Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features.","The study serves as a call to action for the research community to reevaluate the reliability of existing datasets and methodologies, ensuring that machine learning models genuinely learn what they are designed to recognize."],"url":"http://arxiv.org/abs/2403.07767v1","category":"eess.AS"}
{"created":"2024-03-12 15:01:17","title":"On the Last-Iterate Convergence of Shuffling Gradient Methods","abstract":"Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity. Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate.","sentences":["Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG).","Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time.","Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric).","However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization).","To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity.","Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate."],"url":"http://arxiv.org/abs/2403.07723v1","category":"cs.LG"}
{"created":"2024-03-12 14:58:51","title":"Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis","abstract":"Histopathological whole slide images (WSIs) classification has become a foundation task in medical microscopic imaging processing. Prevailing approaches involve learning WSIs as instance-bag representations, emphasizing significant instances but struggling to capture the interactions between instances. Additionally, conventional graph representation methods utilize explicit spatial positions to construct topological structures but restrict the flexible interaction capabilities between instances at arbitrary locations, particularly when spatially distant. In response, we propose a novel dynamic graph representation algorithm that conceptualizes WSIs as a form of the knowledge graph structure. Specifically, we dynamically construct neighbors and directed edge embeddings based on the head and tail relationships between instances. Then, we devise a knowledge-aware attention mechanism that can update the head node features by learning the joint attention score of each neighbor and edge. Finally, we obtain a graph-level embedding through the global pooling process of the updated head, serving as an implicit representation for the WSI classification. Our end-to-end graph representation learning approach has outperformed the state-of-the-art WSI analysis methods on three TCGA benchmark datasets and in-house test sets. Our code is available at https://github.com/WonderLandxD/WiKG.","sentences":["Histopathological whole slide images (WSIs) classification has become a foundation task in medical microscopic imaging processing.","Prevailing approaches involve learning WSIs as instance-bag representations, emphasizing significant instances but struggling to capture the interactions between instances.","Additionally, conventional graph representation methods utilize explicit spatial positions to construct topological structures but restrict the flexible interaction capabilities between instances at arbitrary locations, particularly when spatially distant.","In response, we propose a novel dynamic graph representation algorithm that conceptualizes WSIs as a form of the knowledge graph structure.","Specifically, we dynamically construct neighbors and directed edge embeddings based on the head and tail relationships between instances.","Then, we devise a knowledge-aware attention mechanism that can update the head node features by learning the joint attention score of each neighbor and edge.","Finally, we obtain a graph-level embedding through the global pooling process of the updated head, serving as an implicit representation for the WSI classification.","Our end-to-end graph representation learning approach has outperformed the state-of-the-art WSI analysis methods on three TCGA benchmark datasets and in-house test sets.","Our code is available at https://github.com/WonderLandxD/WiKG."],"url":"http://arxiv.org/abs/2403.07719v1","category":"cs.CV"}
{"created":"2024-03-12 14:57:57","title":"Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound","abstract":"Self-supervised learning (SSL) is one strategy for addressing the paucity of labelled data in medical imaging by learning representations from unlabelled images. Contrastive and non-contrastive SSL methods produce learned representations that are similar for pairs of related images. Such pairs are commonly constructed by randomly distorting the same image twice. The videographic nature of ultrasound offers flexibility for defining the similarity relationship between pairs of images. In this study, we investigated the effect of utilizing proximal, distinct images from the same B-mode ultrasound video as pairs for SSL. Additionally, we introduced a sample weighting scheme that increases the weight of closer image pairs and demonstrated how it can be integrated into SSL objectives. Named Intra-Video Positive Pairs (IVPP), the method surpassed previous ultrasound-specific contrastive learning methods' average test accuracy on COVID-19 classification with the POCUS dataset by $\\ge 1.3\\%$. Detailed investigations of IVPP's hyperparameters revealed that some combinations of IVPP hyperparameters can lead to improved or worsened performance, depending on the downstream task. Guidelines for practitioners were synthesized based on the results, such as the merit of IVPP with task-specific hyperparameters, and the improved performance of contrastive methods for ultrasound compared to non-contrastive counterparts.","sentences":["Self-supervised learning (SSL) is one strategy for addressing the paucity of labelled data in medical imaging by learning representations from unlabelled images.","Contrastive and non-contrastive SSL methods produce learned representations that are similar for pairs of related images.","Such pairs are commonly constructed by randomly distorting the same image twice.","The videographic nature of ultrasound offers flexibility for defining the similarity relationship between pairs of images.","In this study, we investigated the effect of utilizing proximal, distinct images from the same B-mode ultrasound video as pairs for SSL.","Additionally, we introduced a sample weighting scheme that increases the weight of closer image pairs and demonstrated how it can be integrated into SSL objectives.","Named Intra-Video Positive Pairs (IVPP), the method surpassed previous ultrasound-specific contrastive learning methods' average test accuracy on COVID-19 classification with the POCUS dataset by $\\ge 1.3\\%$. Detailed investigations of IVPP's hyperparameters revealed that some combinations of IVPP hyperparameters can lead to improved or worsened performance, depending on the downstream task.","Guidelines for practitioners were synthesized based on the results, such as the merit of IVPP with task-specific hyperparameters, and the improved performance of contrastive methods for ultrasound compared to non-contrastive counterparts."],"url":"http://arxiv.org/abs/2403.07715v1","category":"eess.IV"}
{"created":"2024-03-12 14:33:53","title":"SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt","abstract":"Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD \\textit{identification} (i.e., whether an instance is SATD or not) and \\textit{categorization} (i.e., which type of SATD is being classified) in existing datasets, we share the \\textit{SATDAUG} dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages. These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models.","sentences":["Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase.","Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages.","These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances.","However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD.","In order to address the scarcity of labeled data for SATD \\textit{identification} (i.e., whether an instance is SATD or not) and \\textit{categorization} (i.e., which type of SATD is being classified) in existing datasets, we share the \\textit{SATDAUG} dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages.","These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models."],"url":"http://arxiv.org/abs/2403.07690v1","category":"cs.SE"}
{"created":"2024-03-12 12:25:38","title":"Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features","abstract":"Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model's predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX's potential in advancing cancer diagnosis and treatment.","sentences":["Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts.","However, the high costs and methodological limitations of ST necessitate a more robust predictive model.","In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs).","TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization.","By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction.","Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC).","The model's predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX's potential in advancing cancer diagnosis and treatment."],"url":"http://arxiv.org/abs/2403.07592v1","category":"cs.CV"}
{"created":"2024-03-12 12:07:00","title":"AACP: Aesthetics assessment of children's paintings based on self-supervised learning","abstract":"The Aesthetics Assessment of Children's Paintings (AACP) is an important branch of the image aesthetics assessment (IAA), playing a significant role in children's education. This task presents unique challenges, such as limited available data and the requirement for evaluation metrics from multiple perspectives. However, previous approaches have relied on training large datasets and subsequently providing an aesthetics score to the image, which is not applicable to AACP. To solve this problem, we construct an aesthetics assessment dataset of children's paintings and a model based on self-supervised learning. 1) We build a novel dataset composed of two parts: the first part contains more than 20k unlabeled images of children's paintings; the second part contains 1.2k images of children's paintings, and each image contains eight attributes labeled by multiple design experts. 2) We design a pipeline that includes a feature extraction module, perception modules and a disentangled evaluation module. 3) We conduct both qualitative and quantitative experiments to compare our model's performance with five other methods using the AACP dataset. Our experiments reveal that our method can accurately capture aesthetic features and achieve state-of-the-art performance.","sentences":["The Aesthetics Assessment of Children's Paintings (AACP) is an important branch of the image aesthetics assessment (IAA), playing a significant role in children's education.","This task presents unique challenges, such as limited available data and the requirement for evaluation metrics from multiple perspectives.","However, previous approaches have relied on training large datasets and subsequently providing an aesthetics score to the image, which is not applicable to AACP.","To solve this problem, we construct an aesthetics assessment dataset of children's paintings and a model based on self-supervised learning.","1) We build a novel dataset composed of two parts: the first part contains more than 20k unlabeled images of children's paintings; the second part contains 1.2k images of children's paintings, and each image contains eight attributes labeled by multiple design experts.","2) We design a pipeline that includes a feature extraction module, perception modules and a disentangled evaluation module.","3) We conduct both qualitative and quantitative experiments to compare our model's performance with five other methods using the AACP dataset.","Our experiments reveal that our method can accurately capture aesthetic features and achieve state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.07578v1","category":"cs.CV"}
{"created":"2024-03-12 11:41:51","title":"SIFiD: Reassess Summary Factual Inconsistency Detection with LLM","abstract":"Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documents.","sentences":["Ensuring factual consistency between the summary and the original document is paramount in summarization tasks.","Consequently, considerable effort has been dedicated to detecting inconsistencies.","With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection.","However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology.","In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4.","To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documents."],"url":"http://arxiv.org/abs/2403.07557v1","category":"cs.CL"}
{"created":"2024-03-12 10:59:30","title":"Online Misogyny Against Female Candidates in the 2022 Brazilian Elections: A Threat to Women's Political Representation?","abstract":"Technology-facilitated gender-based violence has become a global threat to women's political representation and democracy. Understanding how online hate affects its targets is thus paramount. We analyse 10 million tweets directed at female candidates in the Brazilian election in 2022 and examine their reactions to online misogyny. Using a self-trained machine learning classifier to detect Portuguese misogynistic tweets and a quantitative analysis of the candidates' tweeting behaviour, we investigate how the number of misogynistic attacks received alters the online activity of the female candidates. We find that young and left-wing candidates and candidates with higher visibility online received significantly more attacks. Furthermore, we find that an increase in misogynistic attacks in the previous week is associated with a decrease in female candidates' tweets in the following week. This potentially threatens their equal participation in public opinion building and silences women's voices in political discourse.","sentences":["Technology-facilitated gender-based violence has become a global threat to women's political representation and democracy.","Understanding how online hate affects its targets is thus paramount.","We analyse 10 million tweets directed at female candidates in the Brazilian election in 2022 and examine their reactions to online misogyny.","Using a self-trained machine learning classifier to detect Portuguese misogynistic tweets and a quantitative analysis of the candidates' tweeting behaviour, we investigate how the number of misogynistic attacks received alters the online activity of the female candidates.","We find that young and left-wing candidates and candidates with higher visibility online received significantly more attacks.","Furthermore, we find that an increase in misogynistic attacks in the previous week is associated with a decrease in female candidates' tweets in the following week.","This potentially threatens their equal participation in public opinion building and silences women's voices in political discourse."],"url":"http://arxiv.org/abs/2403.07523v1","category":"cs.SI"}
{"created":"2024-03-12 10:08:36","title":"Imbalance-aware Presence-only Loss Function for Species Distribution Modeling","abstract":"In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences. Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives. However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts. To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets. We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in accurately modeling rare species with limited observations.","sentences":["In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences.","Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives.","However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts.","To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets.","We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in accurately modeling rare species with limited observations."],"url":"http://arxiv.org/abs/2403.07472v1","category":"cs.LG"}
{"created":"2024-03-12 10:00:00","title":"On Ranking-based Tests of Independence","abstract":"In this paper we develop a novel nonparametric framework to test the independence of two random variables $\\mathbf{X}$ and $\\mathbf{Y}$ with unknown respective marginals $H(dx)$ and $G(dy)$ and joint distribution $F(dx dy)$, based on {\\it Receiver Operating Characteristic} (ROC) analysis and bipartite ranking. The rationale behind our approach relies on the fact that, the independence hypothesis $\\mathcal{H}\\_0$ is necessarily false as soon as the optimal scoring function related to the pair of distributions $(H\\otimes G,\\; F)$, obtained from a bipartite ranking algorithm, has a ROC curve that deviates from the main diagonal of the unit square.We consider a wide class of rank statistics encompassing many ways of deviating from the diagonal in the ROC space to build tests of independence. Beyond its great flexibility, this new method has theoretical properties that far surpass those of its competitors. Nonasymptotic bounds for the two types of testing errors are established. From an empirical perspective, the novel procedure we promote in this paper exhibits a remarkable ability to detect small departures, of various types, from the null assumption $\\mathcal{H}_0$, even in high dimension, as supported by the numerical experiments presented here.","sentences":["In this paper we develop a novel nonparametric framework to test the independence of two random variables $\\mathbf{X}$ and $\\mathbf{Y}$ with unknown respective marginals $H(dx)$ and $G(dy)$ and joint distribution $F(dx dy)$, based on {\\it Receiver Operating Characteristic} (ROC) analysis and bipartite ranking.","The rationale behind our approach relies on the fact that, the independence hypothesis $\\mathcal{H}\\_0$ is necessarily false as soon as the optimal scoring function related to the pair of distributions $(H\\otimes G,\\; F)$, obtained from a bipartite ranking algorithm, has a ROC curve that deviates from the main diagonal of the unit square.","We consider a wide class of rank statistics encompassing many ways of deviating from the diagonal in the ROC space to build tests of independence.","Beyond its great flexibility, this new method has theoretical properties that far surpass those of its competitors.","Nonasymptotic bounds for the two types of testing errors are established.","From an empirical perspective, the novel procedure we promote in this paper exhibits a remarkable ability to detect small departures, of various types, from the null assumption $\\mathcal{H}_0$, even in high dimension, as supported by the numerical experiments presented here."],"url":"http://arxiv.org/abs/2403.07464v1","category":"math.ST"}
{"created":"2024-03-12 09:57:45","title":"Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index","abstract":"Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation. In this paper, we review and compare the performance of several prediction models for time-to-event analysis. These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches. Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index). Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance. We conclude the analysis with a simulation experiment in which we evaluate the factors influencing the performance ranking of the methods using both scores.","sentences":["Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation.","In this paper, we review and compare the performance of several prediction models for time-to-event analysis.","These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches.","Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index).","Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance.","We conclude the analysis with a simulation experiment in which we evaluate the factors influencing the performance ranking of the methods using both scores."],"url":"http://arxiv.org/abs/2403.07460v1","category":"cs.LG"}
{"created":"2024-03-12 09:43:27","title":"Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study","abstract":"In multiple federated learning schemes, a random subset of clients sends in each round their model updates to the server for aggregation. Although this client selection strategy aims to reduce communication overhead, it remains energy and computationally inefficient, especially when considering resource-constrained devices as clients. This is because conventional random client selection overlooks the content of exchanged information and falls short of providing a mechanism to reduce the transmission of semantically redundant data. To overcome this challenge, we propose clustering the clients with the aid of similarity metrics, where a single client from each of the formed clusters is selected in each round to participate in the federated training. To evaluate our approach, we perform an extensive feasibility study considering the use of nine statistical metrics in the clustering process. Simulation results reveal that, when considering a scenario with high data heterogeneity of clients, similarity-based clustering can reduce the number of required rounds compared to the baseline random client selection. In addition, energy consumption can be notably reduced from 23.93% to 41.61%, for those similarity metrics with an equivalent number of clients per round as the baseline random scheme.","sentences":["In multiple federated learning schemes, a random subset of clients sends in each round their model updates to the server for aggregation.","Although this client selection strategy aims to reduce communication overhead, it remains energy and computationally inefficient, especially when considering resource-constrained devices as clients.","This is because conventional random client selection overlooks the content of exchanged information and falls short of providing a mechanism to reduce the transmission of semantically redundant data.","To overcome this challenge, we propose clustering the clients with the aid of similarity metrics, where a single client from each of the formed clusters is selected in each round to participate in the federated training.","To evaluate our approach, we perform an extensive feasibility study considering the use of nine statistical metrics in the clustering process.","Simulation results reveal that, when considering a scenario with high data heterogeneity of clients, similarity-based clustering can reduce the number of required rounds compared to the baseline random client selection.","In addition, energy consumption can be notably reduced from 23.93% to 41.61%, for those similarity metrics with an equivalent number of clients per round as the baseline random scheme."],"url":"http://arxiv.org/abs/2403.07450v1","category":"cs.DC"}
{"created":"2024-03-12 08:40:21","title":"Learning-Augmented Algorithms with Explicit Predictors","abstract":"Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for). In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the black-box setting. For each of the problems we consider, we introduce new algorithms that take advantage of explicit learning algorithms which we carefully design towards optimizing the overall performance. We demonstrate the potential of our approach by deriving performance bounds which improve over those established in previous work.","sentences":["Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data.","These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail.","In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for).","In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge.","In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand.","Adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the black-box setting.","For each of the problems we consider, we introduce new algorithms that take advantage of explicit learning algorithms which we carefully design towards optimizing the overall performance.","We demonstrate the potential of our approach by deriving performance bounds which improve over those established in previous work."],"url":"http://arxiv.org/abs/2403.07413v1","category":"cs.LG"}
{"created":"2024-03-12 08:35:42","title":"NightHaze: Nighttime Image Dehazing via Self-Prior Learning","abstract":"Masked autoencoder (MAE) shows that severe augmentation during training produces robust representations for high-level tasks. This paper brings the MAE-like framework to nighttime image enhancement, demonstrating that severe augmentation during training produces strong network priors that are resilient to real-world night haze degradations. We propose a novel nighttime image dehazing method with self-prior learning. Our main novelty lies in the design of severe augmentation, which allows our model to learn robust priors. Unlike MAE that uses masking, we leverage two key challenging factors of nighttime images as augmentation: light effects and noise. During training, we intentionally degrade clear images by blending them with light effects as well as by adding noise, and subsequently restore the clear images. This enables our model to learn clear background priors. By increasing the noise values to approach as high as the pixel intensity values of the glow and light effect blended images, our augmentation becomes severe, resulting in stronger priors. While our self-prior learning is considerably effective in suppressing glow and revealing details of background scenes, in some cases, there are still some undesired artifacts that remain, particularly in the forms of over-suppression. To address these artifacts, we propose a self-refinement module based on the semi-supervised teacher-student framework. Our NightHaze, especially our MAE-like self-prior learning, shows that models trained with severe augmentation effectively improve the visibility of input haze images, approaching the clarity of clear nighttime images. Extensive experiments demonstrate that our NightHaze achieves state-of-the-art performance, outperforming existing nighttime image dehazing methods by a substantial margin of 15.5% for MUSIQ and 23.5% for ClipIQA.","sentences":["Masked autoencoder (MAE) shows that severe augmentation during training produces robust representations for high-level tasks.","This paper brings the MAE-like framework to nighttime image enhancement, demonstrating that severe augmentation during training produces strong network priors that are resilient to real-world night haze degradations.","We propose a novel nighttime image dehazing method with self-prior learning.","Our main novelty lies in the design of severe augmentation, which allows our model to learn robust priors.","Unlike MAE that uses masking, we leverage two key challenging factors of nighttime images as augmentation: light effects and noise.","During training, we intentionally degrade clear images by blending them with light effects as well as by adding noise, and subsequently restore the clear images.","This enables our model to learn clear background priors.","By increasing the noise values to approach as high as the pixel intensity values of the glow and light effect blended images, our augmentation becomes severe, resulting in stronger priors.","While our self-prior learning is considerably effective in suppressing glow and revealing details of background scenes, in some cases, there are still some undesired artifacts that remain, particularly in the forms of over-suppression.","To address these artifacts, we propose a self-refinement module based on the semi-supervised teacher-student framework.","Our NightHaze, especially our MAE-like self-prior learning, shows that models trained with severe augmentation effectively improve the visibility of input haze images, approaching the clarity of clear nighttime images.","Extensive experiments demonstrate that our NightHaze achieves state-of-the-art performance, outperforming existing nighttime image dehazing methods by a substantial margin of 15.5% for MUSIQ and 23.5% for ClipIQA."],"url":"http://arxiv.org/abs/2403.07408v1","category":"cs.CV"}
{"created":"2024-03-12 07:58:14","title":"Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution","abstract":"Previous approaches for blind image super-resolution (SR) have relied on degradation estimation to restore high-resolution (HR) images from their low-resolution (LR) counterparts. However, accurate degradation estimation poses significant challenges. The SR model's incompatibility with degradation estimation methods, particularly the Correction Filter, may significantly impair performance as a result of correction errors. In this paper, we introduce a novel blind SR approach that focuses on Learning Correction Errors (LCE). Our method employs a lightweight Corrector to obtain a corrected low-resolution (CLR) image. Subsequently, within an SR network, we jointly optimize SR performance by utilizing both the original LR image and the frequency learning of the CLR image. Additionally, we propose a new Frequency-Self Attention block (FSAB) that enhances the global information utilization ability of Transformer. This block integrates both self-attention and frequency spatial attention mechanisms. Extensive ablation and comparison experiments conducted across various settings demonstrate the superiority of our method in terms of visual quality and accuracy. Our approach effectively addresses the challenges associated with degradation estimation and correction errors, paving the way for more accurate blind image SR.","sentences":["Previous approaches for blind image super-resolution (SR) have relied on degradation estimation to restore high-resolution (HR) images from their low-resolution (LR) counterparts.","However, accurate degradation estimation poses significant challenges.","The SR model's incompatibility with degradation estimation methods, particularly the Correction Filter, may significantly impair performance as a result of correction errors.","In this paper, we introduce a novel blind SR approach that focuses on Learning Correction Errors (LCE).","Our method employs a lightweight Corrector to obtain a corrected low-resolution (CLR) image.","Subsequently, within an SR network, we jointly optimize SR performance by utilizing both the original LR image and the frequency learning of the CLR image.","Additionally, we propose a new Frequency-Self Attention block (FSAB) that enhances the global information utilization ability of Transformer.","This block integrates both self-attention and frequency spatial attention mechanisms.","Extensive ablation and comparison experiments conducted across various settings demonstrate the superiority of our method in terms of visual quality and accuracy.","Our approach effectively addresses the challenges associated with degradation estimation and correction errors, paving the way for more accurate blind image SR."],"url":"http://arxiv.org/abs/2403.07390v1","category":"eess.IV"}
{"created":"2024-03-12 05:32:33","title":"LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries","abstract":"With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that evaluates both spatial and textual relevance, have found many real-life applications. Existing geo-textual indexes for TkQs use traditional retrieval models like BM25 to compute text relevance and usually exploit a simple linear function to compute spatial relevance, but its effectiveness is limited. To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues. To the best of our knowledge, there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models.   To tackle these issues, we propose a novel technique, which Learns to Index the Spatio-Textual data for answering embedding based spatial keyword queries (called LIST). LIST is featured with two novel components. Firstly, we propose a lightweight and effective relevance model that is capable of learning both textual and spatial relevance. Secondly, we introduce a novel machine learning based Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new learning-to-cluster technique to group relevant queries and objects together while separating irrelevant queries and objects. Two key challenges in building an effective and efficient index are the absence of high-quality labels and unbalanced clustering results. We develop a novel pseudo-label generation technique to address the two challenges. Experimental results show that LIST significantly outperforms state-of-the-art methods on effectiveness, with improvements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is three orders of magnitude faster than the most effective baseline.","sentences":["With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that evaluates both spatial and textual relevance, have found many real-life applications.","Existing geo-textual indexes for TkQs use traditional retrieval models like BM25 to compute text relevance and usually exploit a simple linear function to compute spatial relevance, but its effectiveness is limited.","To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues.","To the best of our knowledge, there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models.   ","To tackle these issues, we propose a novel technique, which Learns to Index the Spatio-Textual data for answering embedding based spatial keyword queries (called LIST).","LIST is featured with two novel components.","Firstly, we propose a lightweight and effective relevance model that is capable of learning both textual and spatial relevance.","Secondly, we introduce a novel machine learning based Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new learning-to-cluster technique to group relevant queries and objects together while separating irrelevant queries and objects.","Two key challenges in building an effective and efficient index are the absence of high-quality labels and unbalanced clustering results.","We develop a novel pseudo-label generation technique to address the two challenges.","Experimental results show that LIST significantly outperforms state-of-the-art methods on effectiveness, with improvements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is three orders of magnitude faster than the most effective baseline."],"url":"http://arxiv.org/abs/2403.07331v1","category":"cs.IR"}
{"created":"2024-03-12 05:29:48","title":"Unknown Domain Inconsistency Minimization for Domain Generalization","abstract":"The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there's still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM's generalization capability in unseen domains. Our code is available at \\url{https://github.com/SJShin-AI/UDIM}.","sentences":["The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains.","To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness.","Although SAM variants have delivered significant improvements in DG, we highlight that there's still potential for improvement in generalizing to unknown domains through the exploration on data space.","This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM).","UDIM reduces the loss landscape inconsistency between source domain and unknown domains.","As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset.","In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains.","Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task.","In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets.","Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM's generalization capability in unseen domains.","Our code is available at \\url{https://github.com/SJShin-AI/UDIM}."],"url":"http://arxiv.org/abs/2403.07329v1","category":"cs.LG"}
{"created":"2024-03-12 04:47:29","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","abstract":"The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Our experimental findings discover that integrating ICL and CoT not only augments the performance of our approach but also significantly boosts the models' generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios.","sentences":["The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques.","This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs.","By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations.","To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation.","Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts.","Our experimental findings discover that integrating ICL and CoT not only augments the performance of our approach but also significantly boosts the models' generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios."],"url":"http://arxiv.org/abs/2403.07311v1","category":"cs.CL"}
{"created":"2024-03-12 04:04:38","title":"Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation","abstract":"Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with favorable performance as well as strong generalization abilities. Extensive experiments demonstrate the proposed method establishes a new state of the art for both long- and short-term forecasting. Code is available at \\url{https://github.com/Hank0626/LLaTA}.","sentences":["Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models.","However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization.","Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting.","Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data.","In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge.","Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs.","In this way, it empowers the forecasting model with favorable performance as well as strong generalization abilities.","Extensive experiments demonstrate the proposed method establishes a new state of the art for both long- and short-term forecasting.","Code is available at \\url{https://github.com/Hank0626/LLaTA}."],"url":"http://arxiv.org/abs/2403.07300v1","category":"cs.LG"}
{"created":"2024-03-12 02:52:17","title":"Self-supervised Contrastive Learning for Implicit Collaborative Filtering","abstract":"Contrastive learning-based recommendation algorithms have significantly advanced the field of self-supervised recommendation, particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering. However, the presence of false-positive and false-negative examples in recommendation systems hampers accurate preference learning. In this study, we propose a simple self-supervised contrastive learning framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal. Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers. Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augmentation in constant time complexity. Through validation on multiple datasets, we illustrate the significant improvements our method achieves over the widely used BPR optimization objective while maintaining comparable runtime.","sentences":["Contrastive learning-based recommendation algorithms have significantly advanced the field of self-supervised recommendation, particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering.","However, the presence of false-positive and false-negative examples in recommendation systems hampers accurate preference learning.","In this study, we propose a simple self-supervised contrastive learning framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal.","Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers.","Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augmentation in constant time complexity.","Through validation on multiple datasets, we illustrate the significant improvements our method achieves over the widely used BPR optimization objective while maintaining comparable runtime."],"url":"http://arxiv.org/abs/2403.07265v1","category":"cs.IR"}
{"created":"2024-03-12 02:26:30","title":"The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models","abstract":"Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This perspective paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts.   We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design-tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities.","sentences":["Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies.","These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits.","Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data.","This perspective paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process.","Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts.   ","We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies.","Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology.","The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design-tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities."],"url":"http://arxiv.org/abs/2403.07257v1","category":"cs.AR"}
{"created":"2024-03-12 02:07:23","title":"Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration","abstract":"Human-object interaction (HOI) detection aims to locate human-object pairs and identify their interaction categories in images. Most existing methods primarily focus on supervised learning, which relies on extensive manual HOI annotations. In this paper, we propose a novel framework, termed Knowledge Integration to HOI (KI2HOI), that effectively integrates the knowledge of visual-language model to improve zero-shot HOI detection. Specifically, the verb feature learning module is designed based on visual semantics, by employing the verb extraction decoder to convert corresponding verb queries into interaction-specific category representations. We develop an effective additive self-attention mechanism to generate more comprehensive visual representations. Moreover, the innovative interaction representation decoder effectively extracts informative regions by integrating spatial and visual feature information through a cross-attention mechanism. To deal with zero-shot learning in low-data, we leverage a priori knowledge from the CLIP text encoder to initialize the linear classifier for enhanced interaction understanding. Extensive experiments conducted on the mainstream HICO-DET and V-COCO datasets demonstrate that our model outperforms the previous methods in various zero-shot and full-supervised settings.","sentences":["Human-object interaction (HOI) detection aims to locate human-object pairs and identify their interaction categories in images.","Most existing methods primarily focus on supervised learning, which relies on extensive manual HOI annotations.","In this paper, we propose a novel framework, termed Knowledge Integration to HOI (KI2HOI), that effectively integrates the knowledge of visual-language model to improve zero-shot HOI detection.","Specifically, the verb feature learning module is designed based on visual semantics, by employing the verb extraction decoder to convert corresponding verb queries into interaction-specific category representations.","We develop an effective additive self-attention mechanism to generate more comprehensive visual representations.","Moreover, the innovative interaction representation decoder effectively extracts informative regions by integrating spatial and visual feature information through a cross-attention mechanism.","To deal with zero-shot learning in low-data, we leverage a priori knowledge from the CLIP text encoder to initialize the linear classifier for enhanced interaction understanding.","Extensive experiments conducted on the mainstream HICO-DET and V-COCO datasets demonstrate that our model outperforms the previous methods in various zero-shot and full-supervised settings."],"url":"http://arxiv.org/abs/2403.07246v1","category":"cs.CV"}
{"created":"2024-03-12 01:47:17","title":"Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations","abstract":"Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation. To this end, we systematically study the existence of spurious correlation on CLIP and CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP. In view of them, we advocate a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning, all without the need for group labels. Extensive experiments and in-depth visualizations on several benchmarks validate the effectiveness of our proposals, largely reducing reliance and significantly boosting the model generalization.","sentences":["Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks.","However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly.","Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications.","As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation.","To this end, we systematically study the existence of spurious correlation on CLIP and CILP+ERM.","We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP.","In view of them, we advocate a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning, all without the need for group labels.","Extensive experiments and in-depth visualizations on several benchmarks validate the effectiveness of our proposals, largely reducing reliance and significantly boosting the model generalization."],"url":"http://arxiv.org/abs/2403.07241v1","category":"cs.CV"}
{"created":"2024-03-12 01:28:00","title":"Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning","abstract":"This research addresses the challenge of developing a universal deepfake detector that can effectively identify unseen deepfake images despite limited training data. Existing frequency-based paradigms have relied on frequency-level artifacts introduced during the up-sampling in GAN pipelines to detect forgeries. However, the rapid advancements in synthesis technology have led to specific artifacts for each generation model. Consequently, these detectors have exhibited a lack of proficiency in learning the frequency domain and tend to overfit to the artifacts present in the training data, leading to suboptimal performance on unseen sources. To address this issue, we introduce a novel frequency-aware approach called FreqNet, centered around frequency domain learning, specifically designed to enhance the generalizability of deepfake detectors. Our method forces the detector to continuously focus on high-frequency information, exploiting high-frequency representation of features across spatial and channel dimensions. Additionally, we incorporate a straightforward frequency domain learning module to learn source-agnostic features. It involves convolutional layers applied to both the phase spectrum and amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (iFFT). Extensive experimentation involving 17 GANs demonstrates the effectiveness of our proposed method, showcasing state-of-the-art performance (+9.8\\%) while requiring fewer parameters. The code is available at {\\cred \\url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}.","sentences":["This research addresses the challenge of developing a universal deepfake detector that can effectively identify unseen deepfake images despite limited training data.","Existing frequency-based paradigms have relied on frequency-level artifacts introduced during the up-sampling in GAN pipelines to detect forgeries.","However, the rapid advancements in synthesis technology have led to specific artifacts for each generation model.","Consequently, these detectors have exhibited a lack of proficiency in learning the frequency domain and tend to overfit to the artifacts present in the training data, leading to suboptimal performance on unseen sources.","To address this issue, we introduce a novel frequency-aware approach called FreqNet, centered around frequency domain learning, specifically designed to enhance the generalizability of deepfake detectors.","Our method forces the detector to continuously focus on high-frequency information, exploiting high-frequency representation of features across spatial and channel dimensions.","Additionally, we incorporate a straightforward frequency domain learning module to learn source-agnostic features.","It involves convolutional layers applied to both the phase spectrum and amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (iFFT).","Extensive experimentation involving 17 GANs demonstrates the effectiveness of our proposed method, showcasing state-of-the-art performance (+9.8\\%) while requiring fewer parameters.","The code is available at {\\cred \\url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}."],"url":"http://arxiv.org/abs/2403.07240v1","category":"cs.CV"}
{"created":"2024-03-12 01:14:35","title":"Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model","abstract":"It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships, but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary, while imposing as few restrictions on the underlying data generating process as possible. I construct identified sets using an optimization program that allows for researchers to impose additional shape restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful. This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified.","sentences":["It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals.","This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships, but only has access to data that has been aggregated.","In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary, while imposing as few restrictions on the underlying data generating process as possible.","I construct identified sets using an optimization program that allows for researchers to impose additional shape restrictions.","I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable.","I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful.","This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified."],"url":"http://arxiv.org/abs/2403.07236v1","category":"econ.EM"}
{"created":"2024-03-12 01:00:52","title":"Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving","abstract":"Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approach avoids the $\\textit{frozen robot problem}$ which is pervasive in conventional planners. Our approach also outperforms the previous state-of-the-art in CARLA on challenging dense traffic scenarios when evaluated at realistic speeds.","sentences":["Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving.","However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem.","Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining.","We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors.","We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step.","We validate our approach on a suite of more dynamic merging scenarios, finding that our approach avoids the $\\textit{frozen robot problem}$ which is pervasive in conventional planners.","Our approach also outperforms the previous state-of-the-art in CARLA on challenging dense traffic scenarios when evaluated at realistic speeds."],"url":"http://arxiv.org/abs/2403.07232v1","category":"cs.RO"}
{"created":"2024-03-12 00:58:19","title":"Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning","abstract":"The rapid proliferation of digital content and the ever-growing need for precise object recognition and segmentation have driven the advancement of cutting-edge techniques in the field of object classification and segmentation. This paper introduces \"Learn and Search\", a novel approach for object lookup that leverages the power of contrastive learning to enhance the efficiency and effectiveness of retrieval systems.   In this study, we present an elegant and innovative methodology that integrates deep learning principles and contrastive learning to tackle the challenges of object search. Our extensive experimentation reveals compelling results, with \"Learn and Search\" achieving superior Similarity Grid Accuracy, showcasing its efficacy in discerning regions of utmost similarity within an image relative to a cropped image.   The seamless fusion of deep learning and contrastive learning to address the intricacies of object identification not only promises transformative applications in image recognition, recommendation systems, and content tagging but also revolutionizes content-based search and retrieval. The amalgamation of these techniques, as exemplified by \"Learn and Search,\" represents a significant stride in the ongoing evolution of methodologies in the dynamic realm of object classification and segmentation.","sentences":["The rapid proliferation of digital content and the ever-growing need for precise object recognition and segmentation have driven the advancement of cutting-edge techniques in the field of object classification and segmentation.","This paper introduces \"Learn and Search\", a novel approach for object lookup that leverages the power of contrastive learning to enhance the efficiency and effectiveness of retrieval systems.   ","In this study, we present an elegant and innovative methodology that integrates deep learning principles and contrastive learning to tackle the challenges of object search.","Our extensive experimentation reveals compelling results, with \"Learn and Search\" achieving superior Similarity Grid Accuracy, showcasing its efficacy in discerning regions of utmost similarity within an image relative to a cropped image.   ","The seamless fusion of deep learning and contrastive learning to address the intricacies of object identification not only promises transformative applications in image recognition, recommendation systems, and content tagging but also revolutionizes content-based search and retrieval.","The amalgamation of these techniques, as exemplified by \"Learn and Search,\" represents a significant stride in the ongoing evolution of methodologies in the dynamic realm of object classification and segmentation."],"url":"http://arxiv.org/abs/2403.07231v1","category":"cs.CV"}
{"created":"2024-03-12 17:57:56","title":"Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks","abstract":"For users requesting popular contents from content providers, edge caching can alleviate backhaul pressure and enhance the quality of experience of users. Recently there is also a growing concern about content freshness that is quantified by age of information (AoI). Therefore, AoI-aware online caching algorithms are required, which is challenging because the content popularity is usually unknown in advance and may vary over time. In this paper, we propose an online digital twin (DT) empowered content resale mechanism in AoI-aware edge caching networks. We aim to design an optimal two-timescale caching strategy to maximize the utility of an edge network service provider (ENSP). The formulated optimization problem is non-convex and NP-hard. To tackle this intractable problem, we propose a DT-assisted Online Caching Algorithm (DT-OCA). In specific, we first decompose our formulated problem into a series of subproblems, each handling a cache period. For each cache period, we use a DT-based prediction method to effectively capture future content popularity, and develop online caching strategy. Competitive ratio analysis and extensive experimental results demonstrate that our algorithm has promising performance, and outperforms other benchmark algorithms. Insightful observations are also found and discussed.","sentences":["For users requesting popular contents from content providers, edge caching can alleviate backhaul pressure and enhance the quality of experience of users.","Recently there is also a growing concern about content freshness that is quantified by age of information (AoI).","Therefore, AoI-aware online caching algorithms are required, which is challenging because the content popularity is usually unknown in advance and may vary over time.","In this paper, we propose an online digital twin (DT) empowered content resale mechanism in AoI-aware edge caching networks.","We aim to design an optimal two-timescale caching strategy to maximize the utility of an edge network service provider (ENSP).","The formulated optimization problem is non-convex and NP-hard.","To tackle this intractable problem, we propose a DT-assisted Online Caching Algorithm (DT-OCA).","In specific, we first decompose our formulated problem into a series of subproblems, each handling a cache period.","For each cache period, we use a DT-based prediction method to effectively capture future content popularity, and develop online caching strategy.","Competitive ratio analysis and extensive experimental results demonstrate that our algorithm has promising performance, and outperforms other benchmark algorithms.","Insightful observations are also found and discussed."],"url":"http://arxiv.org/abs/2403.07868v1","category":"cs.NI"}
{"created":"2024-03-12 17:57:35","title":"The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods","abstract":"In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation. This algorithm allows for the simultaneous exploration of a robot's state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications. Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories. The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches.","sentences":["In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation.","This algorithm allows for the simultaneous exploration of a robot's state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications.","Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories.","The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches."],"url":"http://arxiv.org/abs/2403.07867v1","category":"cs.RO"}
{"created":"2024-03-12 15:59:50","title":"The probabilistic p-center problem: Planning service for potential customers","abstract":"This work deals with the probabilistic p-center problem, which aims at minimizing the expected maximum distance between any site with demand and its center, considering that each site has demand with a specific probability. The problem is of interest when emergencies may occur at predefined sites with known probabilities. For this problem we propose and analyze different formulations as well as a Variable Neighborhood Search heuristic. Computational tests are reported, showing the potentials and limits of each formulation, the impact of their enhancements, and the effectiveness of the heuristic.","sentences":["This work deals with the probabilistic p-center problem, which aims at minimizing the expected maximum distance between any site with demand and its center, considering that each site has demand with a specific probability.","The problem is of interest when emergencies may occur at predefined sites with known probabilities.","For this problem we propose and analyze different formulations as well as a Variable Neighborhood Search heuristic.","Computational tests are reported, showing the potentials and limits of each formulation, the impact of their enhancements, and the effectiveness of the heuristic."],"url":"http://arxiv.org/abs/2403.07775v1","category":"math.OC"}
{"created":"2024-03-12 15:13:40","title":"Tightening big Ms in integer programming formulations for support vector machines with ramp loss","abstract":"This paper considers various models of support vector machines with ramp loss, these being an efficient and robust tool in supervised classification for the detection of outliers. The exact solution approaches for the resulting optimization problem are of high demand for large datasets. Hence, the goal of this paper is to develop algorithms that provide efficient methodologies to exactly solve these optimization problems. These approaches are based on three strategies for obtaining tightened values of the big M parameters included in the formulation of the problem. Two of them require solving a sequence of continuous problems, while the third uses the Lagrangian relaxation to tighten the bounds. The proposed resolution methods are valid for the l1-norm and l2-norm ramp loss formulations. They were tested and compared with existing solution methods in simulated and real-life datasets, showing the efficiency of the developed methodology.","sentences":["This paper considers various models of support vector machines with ramp loss, these being an efficient and robust tool in supervised classification for the detection of outliers.","The exact solution approaches for the resulting optimization problem are of high demand for large datasets.","Hence, the goal of this paper is to develop algorithms that provide efficient methodologies to exactly solve these optimization problems.","These approaches are based on three strategies for obtaining tightened values of the big M parameters included in the formulation of the problem.","Two of them require solving a sequence of continuous problems, while the third uses the Lagrangian relaxation to tighten the bounds.","The proposed resolution methods are valid for the l1-norm and l2-norm ramp loss formulations.","They were tested and compared with existing solution methods in simulated and real-life datasets, showing the efficiency of the developed methodology."],"url":"http://arxiv.org/abs/2403.07736v1","category":"math.OC"}
{"created":"2024-03-12 10:02:46","title":"Zero-Sum Stochastic Games with Vanishing Stage Duration and Public Signals","abstract":"We consider the behaviour of $\\lambda$-discounted zero-sum games as the discount factor $\\lambda$ tends to 0 (that is, the players are more and more patient), in the context of games with stage duration. In stochastic games with stage duration h, players act at times 0, h, 2h, ..., and the payoff and leaving probabilities are proportional to h. When h tends to 0, such games approximate games in continuous time. The asymptotic behavior of the values (when both $\\lambda$ and h tend to 0) was already studied in the case of stochastic games with perfect observation of the state and in the state-blind case. We consider the same question for the case of stochastic games with imperfect observation of the state. In such games, players are given at each stage a public signal that depends only on the current state. Our main result states that there exists a stochastic game with public signals, with no limit value (as the discount factor $\\lambda$ goes to 0) if stage duration is 1, but with a limit value when stage duration h and discount factor $\\lambda$ both tend to 0. Informally speaking, it means that the limit value in discrete time does not exist, but the limit value in continuous time (i.e., when h tends to 0) exists. Such a situation is impossible in the case of stochastic games with perfect observation of the state.","sentences":["We consider the behaviour of $\\lambda$-discounted zero-sum games as the discount factor $\\lambda$ tends to 0 (that is, the players are more and more patient), in the context of games with stage duration.","In stochastic games with stage duration h, players act at times 0, h, 2h, ..., and the payoff and leaving probabilities are proportional to h.","When h tends to 0, such games approximate games in continuous time.","The asymptotic behavior of the values (when both $\\lambda$ and h tend to 0) was already studied in the case of stochastic games with perfect observation of the state and in the state-blind case.","We consider the same question for the case of stochastic games with imperfect observation of the state.","In such games, players are given at each stage a public signal that depends only on the current state.","Our main result states that there exists a stochastic game with public signals, with no limit value (as the discount factor $\\lambda$ goes to 0) if stage duration is 1, but with a limit value when stage duration h and discount factor $\\lambda$ both tend to 0.","Informally speaking, it means that the limit value in discrete time does not exist, but the limit value in continuous time (i.e., when h tends to 0) exists.","Such a situation is impossible in the case of stochastic games with perfect observation of the state."],"url":"http://arxiv.org/abs/2403.07467v1","category":"math.OC"}
{"created":"2024-03-12 09:55:06","title":"Energy bounds for weighted spherical codes and designs via linear programming","abstract":"Universal bounds for the potential energy of weighted spherical codes are obtained by linear programming. The universality is in the sense of Cohn-Kumar -- every attaining code is optimal with respect to a large class of potential functions (absolutely monotone), in the sense of Levenshtein -- there is a bound for every weighted code, and in the sense of parameters (nodes and weights) -- they are independent of the potential function. We derive a necessary condition for optimality (in the linear programming framework) of our lower bounds which is also shown to be sufficient when the potential is strictly absolutely monotone. Bounds are also obtained for the weighted energy of weighted spherical designs. We explore our bounds for several previously studied weighted spherical codes.","sentences":["Universal bounds for the potential energy of weighted spherical codes are obtained by linear programming.","The universality is in the sense of Cohn-Kumar -- every attaining code is optimal with respect to a large class of potential functions (absolutely monotone), in the sense of Levenshtein -- there is a bound for every weighted code, and in the sense of parameters (nodes and weights) -- they are independent of the potential function.","We derive a necessary condition for optimality (in the linear programming framework) of our lower bounds which is also shown to be sufficient when the potential is strictly absolutely monotone.","Bounds are also obtained for the weighted energy of weighted spherical designs.","We explore our bounds for several previously studied weighted spherical codes."],"url":"http://arxiv.org/abs/2403.07457v1","category":"math.MG"}
{"created":"2024-03-12 08:38:09","title":"Polylog-Competitive Deterministic Local Routing and Scheduling","abstract":"This paper addresses point-to-point packet routing in undirected networks, which is the most important communication primitive in most networks. The main result proves the existence of routing tables that guarantee a polylog-competitive completion-time $\\textbf{deterministically}$: in any undirected network, it is possible to give each node simple stateless deterministic local forwarding rules, such that, any adversarially chosen set of packets are delivered as fast as possible, up to polylog factors.   All previous routing strategies crucially required randomization for both route selection and packet scheduling.   The core technical contribution of this paper is a new local packet scheduling result of independent interest. This scheduling strategy integrates well with recent sparse semi-oblivious path selection strategies. Such strategies deterministically select not one but several candidate paths for each packet and require a global coordinator to select a single good path from those candidates for each packet. Another challenge is that, even if a single path is selected for each packet, no strategy for scheduling packets along low-congestion paths that is both local and deterministic is known. Our novel scheduling strategy utilizes the fact that every semi-oblivious routing strategy uses only a small (polynomial) subset of candidate routes. It overcomes the issue of global coordination by furthermore being provably robust to adversarial noise. This avoids the issue of having to choose a single path per packet because congestion caused by ineffective candidate paths can be treated as noise.   Our results imply the first deterministic universally-optimal algorithms in the distributed supported-CONGEST model for many important global distributed tasks, including computing minimum spanning trees, approximate shortest paths, and part-wise aggregates.","sentences":["This paper addresses point-to-point packet routing in undirected networks, which is the most important communication primitive in most networks.","The main result proves the existence of routing tables that guarantee a polylog-competitive completion-time $\\textbf{deterministically}$: in any undirected network, it is possible to give each node simple stateless deterministic local forwarding rules, such that, any adversarially chosen set of packets are delivered as fast as possible, up to polylog factors.   ","All previous routing strategies crucially required randomization for both route selection and packet scheduling.   ","The core technical contribution of this paper is a new local packet scheduling result of independent interest.","This scheduling strategy integrates well with recent sparse semi-oblivious path selection strategies.","Such strategies deterministically select not one but several candidate paths for each packet and require a global coordinator to select a single good path from those candidates for each packet.","Another challenge is that, even if a single path is selected for each packet, no strategy for scheduling packets along low-congestion paths that is both local and deterministic is known.","Our novel scheduling strategy utilizes the fact that every semi-oblivious routing strategy uses only a small (polynomial) subset of candidate routes.","It overcomes the issue of global coordination by furthermore being provably robust to adversarial noise.","This avoids the issue of having to choose a single path per packet because congestion caused by ineffective candidate paths can be treated as noise.   ","Our results imply the first deterministic universally-optimal algorithms in the distributed supported-CONGEST model for many important global distributed tasks, including computing minimum spanning trees, approximate shortest paths, and part-wise aggregates."],"url":"http://arxiv.org/abs/2403.07410v1","category":"cs.DC"}
{"created":"2024-03-12 08:13:37","title":"Skyrmion flow in periodically modulated channels","abstract":"Magnetic skyrmions, topologically stabilized chiral magnetic textures with particle-like properties have so far primarily been studied statically. Here, we experimentally investigate the dynamics of skyrmion ensembles in metallic thin film conduits where they behave as quasi-particle fluids. By exploiting our access to the full trajectories of all fluid particles by means of time-resolved magneto-optical Kerr microscopy, we demonstrate that boundary conditions of skyrmion fluids can be tuned by modulation of the channel geometry. We observe as a function of channel width deviations from classical flow profiles even into the no- or partial-slip regime. Unlike conventional colloids, the skyrmion Hall effect can also introduce transversal flow-asymmetries and even local motion of single skyrmions against the driving force which we explore with particle-based simulations, demonstrating the unique properties of skyrmion liquid flow that uniquely deviates from previously known behavior of other quasi-particles.","sentences":["Magnetic skyrmions, topologically stabilized chiral magnetic textures with particle-like properties have so far primarily been studied statically.","Here, we experimentally investigate the dynamics of skyrmion ensembles in metallic thin film conduits where they behave as quasi-particle fluids.","By exploiting our access to the full trajectories of all fluid particles by means of time-resolved magneto-optical Kerr microscopy, we demonstrate that boundary conditions of skyrmion fluids can be tuned by modulation of the channel geometry.","We observe as a function of channel width deviations from classical flow profiles even into the no- or partial-slip regime.","Unlike conventional colloids, the skyrmion Hall effect can also introduce transversal flow-asymmetries and even local motion of single skyrmions against the driving force which we explore with particle-based simulations, demonstrating the unique properties of skyrmion liquid flow that uniquely deviates from previously known behavior of other quasi-particles."],"url":"http://arxiv.org/abs/2403.07397v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 05:03:55","title":"GMPC: Geometric Model Predictive Control for Wheeled Mobile Robot Trajectory Tracking","abstract":"The configuration of most robotic systems lies in continuous transformation groups. However, in mobile robot trajectory tracking, many recent works still naively utilize optimization methods for elements in vector space without considering the manifold constraint of the robot configuration. In this letter, we propose a geometric model predictive control (MPC) framework for wheeled mobile robot trajectory tracking. We first derive the error dynamics of the wheeled mobile robot trajectory tracking by considering its manifold constraint and kinematic constraint simultaneously. After that, we utilize the relationship between the Lie group and Lie algebra to convexify the tracking control problem, which enables us to solve the problem efficiently. Thanks to the Lie group formulation, our method tracks the trajectory more smoothly than existing nonlinear MPC. Simulations and physical experiments verify the effectiveness of our proposed methods. Our pure Python-based simulation platform is publicly available to benefit further research in the community.","sentences":["The configuration of most robotic systems lies in continuous transformation groups.","However, in mobile robot trajectory tracking, many recent works still naively utilize optimization methods for elements in vector space without considering the manifold constraint of the robot configuration.","In this letter, we propose a geometric model predictive control (MPC) framework for wheeled mobile robot trajectory tracking.","We first derive the error dynamics of the wheeled mobile robot trajectory tracking by considering its manifold constraint and kinematic constraint simultaneously.","After that, we utilize the relationship between the Lie group and Lie algebra to convexify the tracking control problem, which enables us to solve the problem efficiently.","Thanks to the Lie group formulation, our method tracks the trajectory more smoothly than existing nonlinear MPC.","Simulations and physical experiments verify the effectiveness of our proposed methods.","Our pure Python-based simulation platform is publicly available to benefit further research in the community."],"url":"http://arxiv.org/abs/2403.07317v1","category":"eess.SY"}
{"created":"2024-03-12 05:03:05","title":"Simplicial complexes with many facets are vertex decomposable","abstract":"Let $\\Delta$ be a pure simplicial complex on $n$ vertices having dimension $d$ and codimension $c = n-d-1$ in the simplex. Terai and Yoshida proved that if the number of facets of $\\Delta$ is at least $\\binom{n}{c}-2c+1$, then $\\Delta$ is Cohen-Macaulay. We improve this result by showing that these hypotheses imply the stronger condition that $\\Delta$ is vertex decomposable. We give examples to show that this bound is optimal, and that the conclusion cannot be strengthened to the class of matroids or shifted complexes. We explore an application to Simon's Conjecture and discuss connections to other results from the literature.","sentences":["Let $\\Delta$ be a pure simplicial complex on $n$ vertices having dimension $d$ and codimension $c = n-d-1$ in the simplex.","Terai and Yoshida proved that if the number of facets of $\\Delta$ is at least $\\binom{n}{c}-2c+1$, then $\\Delta$ is Cohen-Macaulay.","We improve this result by showing that these hypotheses imply the stronger condition that $\\Delta$ is vertex decomposable.","We give examples to show that this bound is optimal, and that the conclusion cannot be strengthened to the class of matroids or shifted complexes.","We explore an application to Simon's Conjecture and discuss connections to other results from the literature."],"url":"http://arxiv.org/abs/2403.07316v1","category":"math.CO"}
{"created":"2024-03-12 04:49:59","title":"Multi-task Manipulation Policy Modeling with Visuomotor Latent Diffusion","abstract":"Modeling a generalized visuomotor policy has been a longstanding challenge for both computer vision and robotics communities. Existing approaches often fail to efficiently leverage cross-dataset resources or rely on heavy Vision-Language models, which require substantial computational resources, thereby limiting their multi-task performance and application potential. In this paper, we introduce a novel paradigm that effectively utilizes latent modeling of manipulation skills and an efficient visuomotor latent diffusion policy, which enhances the utilizing of existing cross-embodiment and cross-environment datasets, thereby improving multi-task capabilities. Our methodology consists of two decoupled phases: action modeling and policy modeling. Firstly, we introduce a task-agnostic, embodiment-aware trajectory latent autoencoder for unified action skills modeling. This step condenses action data and observation into a condensed latent space, effectively benefiting from large-scale cross-datasets. Secondly, we propose to use a visuomotor latent diffusion policy that recovers target skill latent from noises for effective task execution. We conducted extensive experiments on two widely used benchmarks, and the results demonstrate the effectiveness of our proposed paradigms on multi-tasking and pre-training. Code is available at https://github.com/AlbertTan404/RoLD.","sentences":["Modeling a generalized visuomotor policy has been a longstanding challenge for both computer vision and robotics communities.","Existing approaches often fail to efficiently leverage cross-dataset resources or rely on heavy Vision-Language models, which require substantial computational resources, thereby limiting their multi-task performance and application potential.","In this paper, we introduce a novel paradigm that effectively utilizes latent modeling of manipulation skills and an efficient visuomotor latent diffusion policy, which enhances the utilizing of existing cross-embodiment and cross-environment datasets, thereby improving multi-task capabilities.","Our methodology consists of two decoupled phases: action modeling and policy modeling.","Firstly, we introduce a task-agnostic, embodiment-aware trajectory latent autoencoder for unified action skills modeling.","This step condenses action data and observation into a condensed latent space, effectively benefiting from large-scale cross-datasets.","Secondly, we propose to use a visuomotor latent diffusion policy that recovers target skill latent from noises for effective task execution.","We conducted extensive experiments on two widely used benchmarks, and the results demonstrate the effectiveness of our proposed paradigms on multi-tasking and pre-training.","Code is available at https://github.com/AlbertTan404/RoLD."],"url":"http://arxiv.org/abs/2403.07312v1","category":"cs.RO"}
{"created":"2024-03-12 03:56:13","title":"Tight error bounds for log-determinant cones without constraint qualifications","abstract":"In this paper, without requiring any constraint qualifications, we establish tight error bounds for the log-determinant cone, which is the closure of the hypograph of the perspective function of the log-determinant function. This error bound is obtained using the recently developed framework based on one-step facial residual functions.","sentences":["In this paper, without requiring any constraint qualifications, we establish tight error bounds for the log-determinant cone, which is the closure of the hypograph of the perspective function of the log-determinant function.","This error bound is obtained using the recently developed framework based on one-step facial residual functions."],"url":"http://arxiv.org/abs/2403.07295v1","category":"math.OC"}
{"created":"2024-03-12 02:56:31","title":"Long-term Hydrothermal Bid-based Market Simulator","abstract":"Simulating long-term hydrothermal bid-based markets considering strategic agents is a challenging task. The representation of strategic agents considering inter-temporal constraints within a stochastic framework brings additional complexity to the already difficult single-period bilevel, thus, non-convex, optimal bidding problem. Thus, we propose a simulation methodology that effectively addresses these challenges for large-scale hydrothermal power systems. We demonstrate the effectiveness of the framework through a case study with real data from the large-scale Brazilian power system. In the case studies, we show the effects of market concentration in power systems and how contracts can be used to mitigate them. In particular, we show how market power might affect the current setting in Brazil. The developed method can strongly benefit policy makers, market monitors, and market designers as simulations can be used to understand existing power systems and experiment with alternative designs.","sentences":["Simulating long-term hydrothermal bid-based markets considering strategic agents is a challenging task.","The representation of strategic agents considering inter-temporal constraints within a stochastic framework brings additional complexity to the already difficult single-period bilevel, thus, non-convex, optimal bidding problem.","Thus, we propose a simulation methodology that effectively addresses these challenges for large-scale hydrothermal power systems.","We demonstrate the effectiveness of the framework through a case study with real data from the large-scale Brazilian power system.","In the case studies, we show the effects of market concentration in power systems and how contracts can be used to mitigate them.","In particular, we show how market power might affect the current setting in Brazil.","The developed method can strongly benefit policy makers, market monitors, and market designers as simulations can be used to understand existing power systems and experiment with alternative designs."],"url":"http://arxiv.org/abs/2403.07270v1","category":"math.OC"}
{"created":"2024-03-12 02:29:39","title":"Near-optimal convergence of the full orthogonalization method","abstract":"We establish a near-optimality guarantee for the full orthogonalization method (FOM), showing that the overall convergence of FOM is nearly as good as GMRES. In particular, we prove that at every iteration $k$, there exists an iteration $j\\leq k$ for which the FOM residual norm at iteration $j$ is no more than $\\sqrt{k+1}$ times larger than the GMRES residual norm at iteration $k$. This bound is sharp, and it has implications for algorithms for approximating the action of a matrix function on a vector.","sentences":["We establish a near-optimality guarantee for the full orthogonalization method (FOM), showing that the overall convergence of FOM is nearly as good as GMRES.","In particular, we prove that at every iteration $k$, there exists an iteration $j\\leq k$ for which the FOM residual norm at iteration $j$ is no more than $\\sqrt{k+1}$ times larger than the GMRES residual norm at iteration $k$.","This bound is sharp, and it has implications for algorithms for approximating the action of a matrix function on a vector."],"url":"http://arxiv.org/abs/2403.07259v1","category":"math.NA"}
{"created":"2024-03-12 01:56:21","title":"Designing high-fidelity two-qubit gates between fluxonium qubits","abstract":"We take a bottom-up, first-principles approach to design a two-qubit gate between fluxonium qubits for minimal error, speed, and control simplicity. Our proposed architecture consists of two fluxoniums coupled via a linear resonator. Using a linear coupler introduces the possibility of material optimization for suppressing its loss, enables efficient driving of state-selective transitions through its large charge zero point fluctuation, reduces sensitivity to junction aging, and partially mitigates coherent coupling to two-level systems. Crucially, a resonator-as-coupler approach also suggests a clear path to increased connectivity between fluxonium qubits, by reducing capacitive loading when the coupler has a high impedance. After performing analytic and numeric analyses of the circuit Hamiltonian and gate dynamics, we tune circuit parameters to destructively interfere sources of coherent error, revealing an efficient, fourth-order scaling of coherent error with gate duration. For component properties from the literature, we predict an open-system average CZ gate infidelity of $1.86 \\times 10^{-4}$ in 70ns.","sentences":["We take a bottom-up, first-principles approach to design a two-qubit gate between fluxonium qubits for minimal error, speed, and control simplicity.","Our proposed architecture consists of two fluxoniums coupled via a linear resonator.","Using a linear coupler introduces the possibility of material optimization for suppressing its loss, enables efficient driving of state-selective transitions through its large charge zero point fluctuation, reduces sensitivity to junction aging, and partially mitigates coherent coupling to two-level systems.","Crucially, a resonator-as-coupler approach also suggests a clear path to increased connectivity between fluxonium qubits, by reducing capacitive loading when the coupler has a high impedance.","After performing analytic and numeric analyses of the circuit Hamiltonian and gate dynamics, we tune circuit parameters to destructively interfere sources of coherent error, revealing an efficient, fourth-order scaling of coherent error with gate duration.","For component properties from the literature, we predict an open-system average CZ gate infidelity of $1.86 \\times 10^{-4}$ in 70ns."],"url":"http://arxiv.org/abs/2403.07242v1","category":"quant-ph"}
{"created":"2024-03-12 00:33:19","title":"Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints","abstract":"We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems. Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution. We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy. This, in turn, affects trajectory accuracy due to the accumulation of translation errors. To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement. After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements. We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment. Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint. Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness. It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed. Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy.","sentences":["We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems.","Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution.","We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy.","This, in turn, affects trajectory accuracy due to the accumulation of translation errors.","To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement.","After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements.","We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment.","Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint.","Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness.","It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed.","Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy."],"url":"http://arxiv.org/abs/2403.07225v1","category":"cs.RO"}
{"created":"2024-03-12 00:26:08","title":"Monocular Microscope to CT Registration using Pose Estimation of the Incus for Augmented Reality Cochlear Implant Surgery","abstract":"For those experiencing severe-to-profound sensorineural hearing loss, the cochlear implant (CI) is the preferred treatment. Augmented reality (AR) aided surgery can potentially improve CI procedures and hearing outcomes. Typically, AR solutions for image-guided surgery rely on optical tracking systems to register pre-operative planning information to the display so that hidden anatomy or other important information can be overlayed and co-registered with the view of the surgical scene. In this paper, our goal is to develop a method that permits direct 2D-to-3D registration of the microscope video to the pre-operative Computed Tomography (CT) scan without the need for external tracking equipment. Our proposed solution involves using surface mapping of a portion of the incus in surgical recordings and determining the pose of this structure relative to the surgical microscope by performing pose estimation via the perspective-n-point (PnP) algorithm. This registration can then be applied to pre-operative segmentations of other anatomy-of-interest, as well as the planned electrode insertion trajectory to co-register this information for the AR display. Our results demonstrate the accuracy with an average rotation error of less than 25 degrees and a translation error of less than 2 mm, 3 mm, and 0.55% for the x, y, and z axes, respectively. Our proposed method has the potential to be applicable and generalized to other surgical procedures while only needing a monocular microscope during intra-operation.","sentences":["For those experiencing severe-to-profound sensorineural hearing loss, the cochlear implant (CI) is the preferred treatment.","Augmented reality (AR) aided surgery can potentially improve CI procedures and hearing outcomes.","Typically, AR solutions for image-guided surgery rely on optical tracking systems to register pre-operative planning information to the display so that hidden anatomy or other important information can be overlayed and co-registered with the view of the surgical scene.","In this paper, our goal is to develop a method that permits direct 2D-to-3D registration of the microscope video to the pre-operative Computed Tomography (CT) scan without the need for external tracking equipment.","Our proposed solution involves using surface mapping of a portion of the incus in surgical recordings and determining the pose of this structure relative to the surgical microscope by performing pose estimation via the perspective-n-point (PnP) algorithm.","This registration can then be applied to pre-operative segmentations of other anatomy-of-interest, as well as the planned electrode insertion trajectory to co-register this information for the AR display.","Our results demonstrate the accuracy with an average rotation error of less than 25 degrees and a translation error of less than 2 mm, 3 mm, and 0.55% for the x, y, and z axes, respectively.","Our proposed method has the potential to be applicable and generalized to other surgical procedures while only needing a monocular microscope during intra-operation."],"url":"http://arxiv.org/abs/2403.07219v1","category":"cs.CV"}
{"created":"2024-03-12 00:03:44","title":"Anomalous magnetic flux via junction twist-angle in a triplet-superconducting transmon qubit","abstract":"Superconducting transmon qubits with strong anharmonicity and insensitivity to offset charge are highly desirable for low-error implementation. In this work we propose a c-axis junction, comprising triplet superconductors, and set at a relative twist angle. Invoking spin-orbit coupling and spin polarization, which are known to occur in the material platform of choice, we examine the resulting transmon Hamiltonian. This junction allows for direct control of the single and double Cooper pair tunneling strength, and most remarkably, an anomalous magnetic flux -- i.e. a phase offset equivalent to magnetic flux, yet in zero magnetic field. Having control over these three parameters -- single and double pair tunneling and anomalous flux -- allows for optimal design of the transmon qubit. Interestingly, in this architecture, the anomalous flux is determined by the twist angle of the junction, thereby offering a novel zero-field functionality. Our key results rely on symmetry arguments, for concreteness we demonstrate the implementation of our concept using a model of moir\\'e graphene-based c-axis junctions.","sentences":["Superconducting transmon qubits with strong anharmonicity and insensitivity to offset charge are highly desirable for low-error implementation.","In this work we propose a c-axis junction, comprising triplet superconductors, and set at a relative twist angle.","Invoking spin-orbit coupling and spin polarization, which are known to occur in the material platform of choice, we examine the resulting transmon Hamiltonian.","This junction allows for direct control of the single and double Cooper pair tunneling strength, and most remarkably, an anomalous magnetic flux -- i.e. a phase offset equivalent to magnetic flux, yet in zero magnetic field.","Having control over these three parameters -- single and double pair tunneling and anomalous flux -- allows for optimal design of the transmon qubit.","Interestingly, in this architecture, the anomalous flux is determined by the twist angle of the junction, thereby offering a novel zero-field functionality.","Our key results rely on symmetry arguments, for concreteness we demonstrate the implementation of our concept using a model of moir\\'e graphene-based c-axis junctions."],"url":"http://arxiv.org/abs/2403.07215v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 00:02:03","title":"Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers","abstract":"This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model's feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements.","sentences":["This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR).","We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos.","This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies.","In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts.","For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained).","Then we employ visual and textual prompts to guide the model's feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations.","Extensive experiments on several benchmark datasets validate significant performance improvements."],"url":"http://arxiv.org/abs/2403.07214v1","category":"cs.CV"}
{"created":"2024-03-11 23:26:44","title":"Exploring iterative and non-iterative Fourier series-based methods of control optimization in application to a discontinuous capsule drive model","abstract":"The paper explains iterative and non-iterative approaches to control optimization with use of the Fourier series-based method. Both variants of the presented algorithm are used to numerically approximate optimal control of a discontinuous pendulum capsule drive. Firstly, the general algorithm and its two realizations (iterative and non-iterative) are presented. It is shown that the iterative variant assures non-decreasing quality of solutions in subsequent repetitions of the procedure and the background of such guarantees is explained. A numerical example follows: control of a self-propelled capsule drive is optimized using both approaches. Results are compared and discussed. It is expected that the presented methods can be useful in optimal control estimation for complex systems, particularly discontinuous ones.","sentences":["The paper explains iterative and non-iterative approaches to control optimization with use of the Fourier series-based method.","Both variants of the presented algorithm are used to numerically approximate optimal control of a discontinuous pendulum capsule drive.","Firstly, the general algorithm and its two realizations (iterative and non-iterative) are presented.","It is shown that the iterative variant assures non-decreasing quality of solutions in subsequent repetitions of the procedure and the background of such guarantees is explained.","A numerical example follows: control of a self-propelled capsule drive is optimized using both approaches.","Results are compared and discussed.","It is expected that the presented methods can be useful in optimal control estimation for complex systems, particularly discontinuous ones."],"url":"http://arxiv.org/abs/2403.07208v1","category":"math.OC"}
{"created":"2024-03-11 23:21:26","title":"Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach","abstract":"Dynamic density estimation is ubiquitous in many applications, including computer vision and signal processing. One popular method to tackle this problem is the \"sliding window\" kernel density estimator. There exist various implementations of this method that use heuristically defined weight sequences for the observed data. The weight sequence, however, is a key aspect of the estimator affecting the tracking performance significantly. In this work, we study the exact mean integrated squared error (MISE) of \"sliding window\" Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide a principled guide for choosing the optimal weight sequence by theoretically characterizing the exact MISE, which can be formulated as constrained quadratic programming. We present empirical evidence with synthetic datasets to show that our weighting scheme indeed improves the tracking performance compared to heuristic approaches.","sentences":["Dynamic density estimation is ubiquitous in many applications, including computer vision and signal processing.","One popular method to tackle this problem is the \"sliding window\" kernel density estimator.","There exist various implementations of this method that use heuristically defined weight sequences for the observed data.","The weight sequence, however, is a key aspect of the estimator affecting the tracking performance significantly.","In this work, we study the exact mean integrated squared error (MISE) of \"sliding window\" Gaussian Kernel Density Estimators for evolving Gaussian densities.","We provide a principled guide for choosing the optimal weight sequence by theoretically characterizing the exact MISE, which can be formulated as constrained quadratic programming.","We present empirical evidence with synthetic datasets to show that our weighting scheme indeed improves the tracking performance compared to heuristic approaches."],"url":"http://arxiv.org/abs/2403.07207v1","category":"stat.ML"}
{"created":"2024-03-11 20:36:35","title":"Advancing Hyperspectral Targeted Alpha Therapy with Adversarial Machine Learning","abstract":"Targeted Alpha Therapy (TAT) has emerged as a promising modality for the treatment of various malignancies, leveraging the high linear energy transfer (LET) and short range of alpha particles to selectively irradiate cancer cells while sparing healthy tissue. Monitoring and optimizing TAT delivery is crucial for its clinical success. Hyper-spectral Single Photon Imaging (HSPI) presents a novel and versatile approach for the real-time assessment of TAT in vivo. This study introduces a comprehensive framework for HSPI in TAT, encompassing spectral unmixing, quantitative dosimetry, and spatiotemporal visualization. We report the development of a dedicated HSPI system tailored to alpha-emitting radionuclides, enabling the simultaneous acquisition of high-resolution spectral data and single-photon localization. Utilizing advanced spectral unmixing algorithms, we demonstrate the discrimination of alpha-induced scintillation from background fluorescence, facilitating precise alpha particle tracking with adversarial machine learning.","sentences":["Targeted Alpha Therapy (TAT) has emerged as a promising modality for the treatment of various malignancies, leveraging the high linear energy transfer (LET) and short range of alpha particles to selectively irradiate cancer cells while sparing healthy tissue.","Monitoring and optimizing TAT delivery is crucial for its clinical success.","Hyper-spectral Single Photon Imaging (HSPI) presents a novel and versatile approach for the real-time assessment of TAT in vivo.","This study introduces a comprehensive framework for HSPI in TAT, encompassing spectral unmixing, quantitative dosimetry, and spatiotemporal visualization.","We report the development of a dedicated HSPI system tailored to alpha-emitting radionuclides, enabling the simultaneous acquisition of high-resolution spectral data and single-photon localization.","Utilizing advanced spectral unmixing algorithms, we demonstrate the discrimination of alpha-induced scintillation from background fluorescence, facilitating precise alpha particle tracking with adversarial machine learning."],"url":"http://arxiv.org/abs/2403.07149v1","category":"physics.med-ph"}
{"created":"2024-03-11 19:41:40","title":"Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris","abstract":"Maneuverable tether-net systems launched from an unmanned spacecraft offer a promising solution for the active removal of large space debris. Guaranteeing the successful capture of such space debris is dependent on the ability to reliably maneuver the tether-net system -- a flexible, many-DoF (thus complex) system -- for a wide range of launch scenarios. Here, scenarios are defined by the relative location of the debris with respect to the chaser spacecraft. This paper represents and solves this problem as a hierarchically decentralized implementation of robotic trajectory planning and control and demonstrates the effectiveness of the approach when applied to two different tether-net systems, with 4 and 8 maneuverable units (MUs), respectively. Reinforcement learning (policy gradient) is used to design the centralized trajectory planner that, based on the relative location of the target debris at the launch of the net, computes the final aiming positions of each MU, from which their trajectory can be derived. Each MU then seeks to follow its assigned trajectory by using a decentralized PID controller that outputs the MU's thrust vector and is informed by noisy sensor feedback (for realism) of its relative location. System performance is assessed in terms of capture success and overall fuel consumption by the MUs. Reward shaping and surrogate models are used to respectively guide and speed up the RL process. Simulation-based experiments show that this approach allows the successful capture of debris at fuel costs that are notably lower than nominal baselines, including in scenarios where the debris is significantly off-centered compared to the approaching chaser spacecraft.","sentences":["Maneuverable tether-net systems launched from an unmanned spacecraft offer a promising solution for the active removal of large space debris.","Guaranteeing the successful capture of such space debris is dependent on the ability to reliably maneuver the tether-net system -- a flexible, many-DoF (thus complex) system -- for a wide range of launch scenarios.","Here, scenarios are defined by the relative location of the debris with respect to the chaser spacecraft.","This paper represents and solves this problem as a hierarchically decentralized implementation of robotic trajectory planning and control and demonstrates the effectiveness of the approach when applied to two different tether-net systems, with 4 and 8 maneuverable units (MUs), respectively.","Reinforcement learning (policy gradient) is used to design the centralized trajectory planner that, based on the relative location of the target debris at the launch of the net, computes the final aiming positions of each MU, from which their trajectory can be derived.","Each MU then seeks to follow its assigned trajectory by using a decentralized PID controller that outputs the MU's thrust vector and is informed by noisy sensor feedback (for realism) of its relative location.","System performance is assessed in terms of capture success and overall fuel consumption by the MUs.","Reward shaping and surrogate models are used to respectively guide and speed up the RL process.","Simulation-based experiments show that this approach allows the successful capture of debris at fuel costs that are notably lower than nominal baselines, including in scenarios where the debris is significantly off-centered compared to the approaching chaser spacecraft."],"url":"http://arxiv.org/abs/2403.07125v1","category":"eess.SY"}
{"created":"2024-03-11 18:57:27","title":"Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions","abstract":"We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy.","sentences":["We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods.","These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off.","Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart.","Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely.","We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples.","In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators.","Introducing confidence weights into the statistics also enhance estimator robustness against outliers.","We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy."],"url":"http://arxiv.org/abs/2403.07104v1","category":"astro-ph.IM"}
{"created":"2024-03-11 18:07:54","title":"VLEIBot: A New 45-mg Swimming Microrobot Driven by a Bioinspired Anguilliform Propulsor","abstract":"This paper presents the VLEIBot^* (Very Little Eel-Inspired roBot), a 45-mg/23-mm^3 microrobotic swimmer that is propelled by a bioinspired anguilliform propulsor. The propulsor is excited by a single 6-mg high-work-density (HWD) microactuator and undulates periodically due to wave propagation phenomena generated by fluid-structure interaction (FSI) during swimming. The microactuator is composed of a carbon-fiber beam, which functions as a leaf spring, and shape-memory alloy (SMA) wires, which deform cyclically when excited periodically using Joule heating. The VLEIBot can swim at speeds as high as 15.1mm * s^{-1} (0.33 Bl * s^{-1}}) when driven with a heuristically-optimized propulsor. To improve maneuverability, we evolved the VLEIBot design into the 90-mg/47-mm^3 VLEIBot^+, which is driven by two propulsors and fully controllable in the two-dimensional (2D) space. The VLEIBot^+ can swim at speeds as high as 16.1mm * s^{-1} (0.35 Bl * s^{-1}), when driven with heuristically-optimized propulsors, and achieves turning rates as high as 0.28 rad * s^{-1}, when tracking path references. The measured root-mean-square (RMS) values of the tracking errors are as low as 4 mm.","sentences":["This paper presents the VLEIBot^* (Very Little Eel-Inspired roBot), a 45-mg/23-mm^3 microrobotic swimmer that is propelled by a bioinspired anguilliform propulsor.","The propulsor is excited by a single 6-mg high-work-density (HWD) microactuator and undulates periodically due to wave propagation phenomena generated by fluid-structure interaction (FSI) during swimming.","The microactuator is composed of a carbon-fiber beam, which functions as a leaf spring, and shape-memory alloy (SMA) wires, which deform cyclically when excited periodically using Joule heating.","The VLEIBot can swim at speeds as high as 15.1mm * s^{-1} (0.33 Bl * s^{-1}}) when driven with a heuristically-optimized propulsor.","To improve maneuverability, we evolved the VLEIBot design into the 90-mg/47-mm^3 VLEIBot^+, which is driven by two propulsors and fully controllable in the two-dimensional (2D) space.","The VLEIBot^+ can swim at speeds as high as 16.1mm * s^{-1} (0.35","Bl * s^{-1}), when driven with heuristically-optimized propulsors, and achieves turning rates as high as 0.28 rad * s^{-1}, when tracking path references.","The measured root-mean-square (RMS) values of the tracking errors are as low as 4 mm."],"url":"http://arxiv.org/abs/2403.07073v1","category":"eess.SY"}
{"created":"2024-03-11 18:01:48","title":"Multiset tomography: Optimizing quantum measurements by partitioning multisets of observables","abstract":"Quantum tomography approaches typically consider a set of observables which we wish to measure, design a measurement scheme which measures each of the observables and then repeats the measurements as many times as necessary. We show that instead of considering only the simple set of observables, one should consider a multiset of the observables taking into account the required repetitions, to minimize the number of measurements. This leads to a graph theoretic multicolouring problem. We show that multiset tomography offers at most quadratic improvement but it is achievable. Furthermore, despite the NP-hard optimal colouring problem, the multiset approach with greedy colouring algorithms already offers asymptotically quadratic improvement in test cases.","sentences":["Quantum tomography approaches typically consider a set of observables which we wish to measure, design a measurement scheme which measures each of the observables and then repeats the measurements as many times as necessary.","We show that instead of considering only the simple set of observables, one should consider a multiset of the observables taking into account the required repetitions, to minimize the number of measurements.","This leads to a graph theoretic multicolouring problem.","We show that multiset tomography offers at most quadratic improvement but it is achievable.","Furthermore, despite the NP-hard optimal colouring problem, the multiset approach with greedy colouring algorithms already offers asymptotically quadratic improvement in test cases."],"url":"http://arxiv.org/abs/2403.07068v1","category":"quant-ph"}
{"created":"2024-03-11 18:00:15","title":"Regge Limit of One-Loop String Amplitudes","abstract":"We study the high-energy limit of $2 \\to 2$ one-loop string amplitudes at fixed momentum transfer. For the closed string, the high-energy behaviour of the amplitudes can be determined from Regge theory just like in field theory, as was first discussed by Amati, Ciafaloni and Veneziano. However, field theory intuition partially breaks down for the open-string amplitude, where amplitudes can exhibit surprising asymptotics in the high-energy limit depending on the topology of the diagram. We call this phenomenon Regge attenuation. We extract Regge limits by a combination of unitarity cuts and saddle-point analysis. We show that the leading contribution of the planar open-string amplitude is sufficiently simple that we can extract it at any loop order. This allows us to resum the genus expansion in a certain limit and demonstrate that the leading Regge trajectory remains linear in that limit.","sentences":["We study the high-energy limit of $2 \\to 2$ one-loop string amplitudes at fixed momentum transfer.","For the closed string, the high-energy behaviour of the amplitudes can be determined from Regge theory just like in field theory, as was first discussed by Amati, Ciafaloni and Veneziano.","However, field theory intuition partially breaks down for the open-string amplitude, where amplitudes can exhibit surprising asymptotics in the high-energy limit depending on the topology of the diagram.","We call this phenomenon Regge attenuation.","We extract Regge limits by a combination of unitarity cuts and saddle-point analysis.","We show that the leading contribution of the planar open-string amplitude is sufficiently simple that we can extract it at any loop order.","This allows us to resum the genus expansion in a certain limit and demonstrate that the leading Regge trajectory remains linear in that limit."],"url":"http://arxiv.org/abs/2403.07064v1","category":"hep-th"}
{"created":"2024-03-11 17:49:18","title":"Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts","abstract":"Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills.","sentences":["Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy.","However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization.","We propose \\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive.","Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts.","The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space.","To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective.","We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills."],"url":"http://arxiv.org/abs/2403.06966v1","category":"cs.LG"}
{"created":"2024-03-11 17:47:40","title":"Decorrelation of a leader by the increasing number of followers","abstract":"We compute the connected two-time correlator of the maximum $M_N(t)$ of $N$ independent Gaussian stochastic processes (GSP) characterised by a common correlation coefficient $\\rho$ that depends on the two times $t_1$ and $t_2$. We show analytically that this correlator, for fixed times $t_1$ and $t_2$, decays for large $N$ as a power law $N^{-\\gamma}$ (with logarithmic corrections) with a decorrelation exponent $\\gamma = (1-\\rho)/(1+ \\rho)$ that depends only on $\\rho$, but otherwise is universal for any GSP. We study several examples of physical processes including the fractional Brownian motion (fBm) with Hurst exponent $H$ and the Ornstein-Uhlenbeck (OU) process. For the fBm, $\\rho$ is only a function of $\\tau = \\sqrt{t_1/t_2}$ and we find an interesting ``freezing'' transition at a critical value $\\tau= \\tau_c=(3-\\sqrt{5})/2$. For $\\tau < \\tau_c$, there is an optimal $H^*(\\tau) > 0$ that maximises the exponent $\\gamma$ and this maximal value freezes to $\\gamma= 1/3$ for $\\tau >\\tau_c$. For the OU process, we show that $\\gamma = {\\rm tanh}(\\mu \\,|t_1-t_2|/2)$ where $\\mu$ is the stiffness of the harmonic trap. Numerical simulations confirm our analytical predictions.","sentences":["We compute the connected two-time correlator of the maximum $M_N(t)$ of $N$ independent Gaussian stochastic processes (GSP) characterised by a common correlation coefficient $\\rho$ that depends on the two times $t_1$ and $t_2$. We show analytically that this correlator, for fixed times $t_1$ and $t_2$, decays for large $N$ as a power law $N^{-\\gamma}$ (with logarithmic corrections) with a decorrelation exponent $\\gamma = (1-\\rho)/(1+ \\rho)$ that depends only on $\\rho$, but otherwise is universal for any GSP.","We study several examples of physical processes including the fractional Brownian motion (fBm) with Hurst exponent $H$ and the Ornstein-Uhlenbeck (OU) process.","For the fBm, $\\rho$ is only a function of $\\tau = \\sqrt{t_1/t_2}$ and we find an interesting ``freezing'' transition at a critical value $\\tau= \\tau_c=(3-\\sqrt{5})/2$. For $\\tau < \\tau_c$, there is an optimal $H^*(\\tau) > 0$ that maximises the exponent $\\gamma$ and this maximal value freezes to $\\gamma= 1/3$ for $\\tau >\\tau_c$.","For the OU process, we show that $\\gamma = {\\rm tanh}(\\mu \\,|t_1-t_2|/2)$ where $\\mu$ is the stiffness of the harmonic trap.","Numerical simulations confirm our analytical predictions."],"url":"http://arxiv.org/abs/2403.06964v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 17:36:44","title":"Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping","abstract":"Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad. Video results can be found at https://youtu.be/SvfVNQ90k_w.","sentences":["Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping.","Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions.","Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics.","In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online.","We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization.","The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions.","After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad.","Video results can be found at https://youtu.be/SvfVNQ90k_w."],"url":"http://arxiv.org/abs/2403.06954v1","category":"cs.RO"}
{"created":"2024-03-11 17:36:11","title":"Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer","abstract":"Purpose: Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics. Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance. In this work, we conduct a multi-centric performance benchmark of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization.   Methods: We evaluate four object-centric approaches for domain generalization, establishing baseline performance. Next, leveraging the disentangled nature of object-centric representations, we dissect one of these methods through a series of ablations (e.g. ignoring either visual or semantic features for downstream classification). Finally, based on the results of these ablations, we develop an optimized method specifically tailored for domain generalization, LG-DG, that includes a novel disentanglement loss function.   Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over the best baseline approach. More broadly, we show that object-centric approaches are highly effective for domain generalization thanks to their modular approach to representation learning.   Conclusion: We investigate the use of object-centric methods for unseen domain generalization, identify method-agnostic factors critical for performance, and present an optimized approach that substantially outperforms existing methods.","sentences":["Purpose:","Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics.","Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance.","In this work, we conduct a multi-centric performance benchmark of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization.   ","Methods: We evaluate four object-centric approaches for domain generalization, establishing baseline performance.","Next, leveraging the disentangled nature of object-centric representations, we dissect one of these methods through a series of ablations (e.g. ignoring either visual or semantic features for downstream classification).","Finally, based on the results of these ablations, we develop an optimized method specifically tailored for domain generalization, LG-DG, that includes a novel disentanglement loss function.   ","Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over the best baseline approach.","More broadly, we show that object-centric approaches are highly effective for domain generalization thanks to their modular approach to representation learning.   ","Conclusion: We investigate the use of object-centric methods for unseen domain generalization, identify method-agnostic factors critical for performance, and present an optimized approach that substantially outperforms existing methods."],"url":"http://arxiv.org/abs/2403.06953v1","category":"cs.CV"}
{"created":"2024-03-11 17:35:23","title":"DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations","abstract":"The diffusion-based text-to-image model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles. In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is https://tianhao-qi.github.io/DEADiff/.","sentences":["The diffusion-based text-to-image model harbors immense potential in transferring reference style.","However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles.","In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images.","The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions.","Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement.","2) A non-reconstructive learning method.","The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics.","We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively.","Our project page is https://tianhao-qi.github.io/DEADiff/."],"url":"http://arxiv.org/abs/2403.06951v2","category":"cs.CV"}
{"created":"2024-03-11 17:18:50","title":"Optimizing sDTW for AMD GPUs","abstract":"Subsequence Dynamic Time Warping (sDTW) is the metric of choice when performing many sequence matching and alignment tasks. While sDTW is flexible and accurate, it is neither simple nor fast to compute; significant research effort has been spent devising parallel implementations on the GPU that leverage efficient memory access and computation patterns, as well as features offered by specific vendors and architectures (notably NVIDIA's). We present an implementation of sDTW on AMD hardware using HIP and ROCm. Our implementation employs well-known parallel patterns, as well as lower-level features offered by ROCm. We use shuffling for intra-wavefront communication and shared memory to transfer data between consecutive wavefronts. By constraining the input data to batches of 512 queries of length 2,000, we optimized for peak performance the width of reference elements operated on by a single thread.","sentences":["Subsequence Dynamic Time Warping (sDTW) is the metric of choice when performing many sequence matching and alignment tasks.","While sDTW is flexible and accurate, it is neither simple nor fast to compute; significant research effort has been spent devising parallel implementations on the GPU that leverage efficient memory access and computation patterns, as well as features offered by specific vendors and architectures (notably NVIDIA's).","We present an implementation of sDTW on AMD hardware using HIP and ROCm.","Our implementation employs well-known parallel patterns, as well as lower-level features offered by ROCm.","We use shuffling for intra-wavefront communication and shared memory to transfer data between consecutive wavefronts.","By constraining the input data to batches of 512 queries of length 2,000, we optimized for peak performance the width of reference elements operated on by a single thread."],"url":"http://arxiv.org/abs/2403.06931v1","category":"cs.DC"}
{"created":"2024-03-11 17:08:55","title":"Synthesis of Robust Optimal Strategies in Weighted Timed Games","abstract":"Weighted Timed Games (WTG for short) are the most widely used model to describe controller synthesis problems involving real-time issues. The synthesized strategies rely on a perfect measure of time elapse, which is not realistic in practice. In order to produce strategies tolerant to timing imprecisions, we rely on a notion of robustness first introduced for timed automata. More precisely, WTGs are two-player zero-sum games played in a timed automaton equipped with integer weights in which one of the players, that we call Min, wants to reach a target location while minimising the cumulated weight. In this work, we equip the underlying timed automaton with a semantics depending on some parameter (representing the maximal possible perturbation) in which the opponent of Min can in addition perturb delays chosen by Min.   The robust value problem can then be stated as follows: given some threshold, determine whether there exists a positive perturbation and a strategy for Min ensuring to reach the target, with an accumulated weight below the threshold, whatever the opponent does.   We provide the first decidability result for this robust value problem by computing the robust value function, in a parametric way, for the class of divergent WTGs (introduced to obtain decidability of the (classical) value problem in WTGs without bounding the number of clocks). To this end, we show that the robust value is the fixpoint of some operators, as is classically done for value iteration algorithms. We then combine in a very careful way two representations: piecewise affine functions introduced in [1] to analyse WTGs, and shrunk Difference Bound Matrices considered in [29] to analyse robustness in timed automata. Last, we also study qualitative decision problems and close an open problem on robust reachability, showing it is EXPTIME-complete for general WTGs.","sentences":["Weighted Timed Games (WTG for short) are the most widely used model to describe controller synthesis problems involving real-time issues.","The synthesized strategies rely on a perfect measure of time elapse, which is not realistic in practice.","In order to produce strategies tolerant to timing imprecisions, we rely on a notion of robustness first introduced for timed automata.","More precisely, WTGs are two-player zero-sum games played in a timed automaton equipped with integer weights in which one of the players, that we call Min, wants to reach a target location while minimising the cumulated weight.","In this work, we equip the underlying timed automaton with a semantics depending on some parameter (representing the maximal possible perturbation) in which the opponent of Min can in addition perturb delays chosen by Min.   ","The robust value problem can then be stated as follows: given some threshold, determine whether there exists a positive perturbation and a strategy for Min ensuring to reach the target, with an accumulated weight below the threshold, whatever the opponent does.   ","We provide the first decidability result for this robust value problem by computing the robust value function, in a parametric way, for the class of divergent WTGs (introduced to obtain decidability of the (classical) value problem in WTGs without bounding the number of clocks).","To this end, we show that the robust value is the fixpoint of some operators, as is classically done for value iteration algorithms.","We then combine in a very careful way two representations: piecewise affine functions introduced in [1] to analyse WTGs, and shrunk Difference Bound Matrices considered in [29] to analyse robustness in timed automata.","Last, we also study qualitative decision problems and close an open problem on robust reachability, showing it is EXPTIME-complete for general WTGs."],"url":"http://arxiv.org/abs/2403.06921v1","category":"cs.GT"}
{"created":"2024-03-11 17:02:11","title":"DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization","abstract":"Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$ faster rendering speed.","sentences":["Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed.","This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs.","Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease.","In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint.","Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance.","To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes.","Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$ faster rendering speed."],"url":"http://arxiv.org/abs/2403.06912v1","category":"cs.CV"}
{"created":"2024-03-11 16:56:01","title":"Benign overfitting in leaky ReLU networks with moderate input dimension","abstract":"The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require near orthogonality conditions on the training data: notably, for input dimension $d$ and training sample size $n$, while prior work shows asymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require only $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within $\\epsilon$ of optimal.","sentences":["The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well.","We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task.","We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another.","We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs.","We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property.","In contrast to prior work we do not require near orthogonality conditions on the training data: notably, for input dimension $d$ and training sample size $n$, while prior work shows asymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require only $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within $\\epsilon$ of optimal."],"url":"http://arxiv.org/abs/2403.06903v1","category":"cs.LG"}
{"created":"2024-03-11 16:42:46","title":"Model Predictive Control Strategies for Electric Endurance Race Cars Accounting for Competitors Interactions","abstract":"This paper presents model predictive control strategies for battery electric endurance race cars accounting for interactions with the competitors. In particular, we devise an optimization framework capturing the impact of the actions of the ego vehicle when interacting with competitors in a probabilistic fashion, jointly accounting for the optimal pit stop decision making, the charge times and the driving style in the course of the race. We showcase our method for a simulated 1h endurance race at the Zandvoort circuit, using real-life data of internal combustion engine race cars from a previous event. Our results show that optimizing both the race strategy as well as the decision making during the race is very important, resulting in a significant 21s advantage over an always overtake approach, whilst revealing the competitiveness of e-race cars w.r.t. conventional ones.","sentences":["This paper presents model predictive control strategies for battery electric endurance race cars accounting for interactions with the competitors.","In particular, we devise an optimization framework capturing the impact of the actions of the ego vehicle when interacting with competitors in a probabilistic fashion, jointly accounting for the optimal pit stop decision making, the charge times and the driving style in the course of the race.","We showcase our method for a simulated 1h endurance race at the Zandvoort circuit, using real-life data of internal combustion engine race cars from a previous event.","Our results show that optimizing both the race strategy as well as the decision making during the race is very important, resulting in a significant 21s advantage over an always overtake approach, whilst revealing the competitiveness of e-race cars w.r.t. conventional ones."],"url":"http://arxiv.org/abs/2403.06885v1","category":"eess.SY"}
{"created":"2024-03-11 16:42:29","title":"A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation","abstract":"Traffic signal control (TSC) is crucial for reducing traffic congestion that leads to smoother traffic flow, reduced idling time, and mitigated CO2 emissions. In this study, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation. Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals. Thus, we introduce a holistic traffic simulation framework called TrafficDojo towards vision-based TSC and its benchmarking by integrating the microscopic traffic flow provided in SUMO into the driving simulator MetaDrive. This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios. We establish and compare baseline algorithms including both traditional and Reinforecment Learning (RL) approaches. This work sheds insights into the design and development of vision-based TSC approaches and open up new research opportunities. All the code and baselines will be made publicly available.","sentences":["Traffic signal control (TSC) is crucial for reducing traffic congestion that leads to smoother traffic flow, reduced idling time, and mitigated CO2 emissions.","In this study, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation.","Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals.","Thus, we introduce a holistic traffic simulation framework called TrafficDojo towards vision-based TSC and its benchmarking by integrating the microscopic traffic flow provided in SUMO into the driving simulator MetaDrive.","This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios.","We establish and compare baseline algorithms including both traditional and Reinforecment Learning (RL) approaches.","This work sheds insights into the design and development of vision-based TSC approaches and open up new research opportunities.","All the code and baselines will be made publicly available."],"url":"http://arxiv.org/abs/2403.06884v1","category":"cs.CV"}
{"created":"2024-03-11 16:24:26","title":"Last Iterate Convergence of Incremental Methods and Applications in Continual Learning","abstract":"Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights, which can be seen as interpolating between the last iterate and the average iterate guarantees. Additionally, we discuss how our results can be generalized to variants of studied incremental methods with permuted ordering of updates. Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions.","sentences":["Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature.","Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate.","Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings.","Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods.","We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights, which can be seen as interpolating between the last iterate and the average iterate guarantees.","Additionally, we discuss how our results can be generalized to variants of studied incremental methods with permuted ordering of updates.","Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions."],"url":"http://arxiv.org/abs/2403.06873v1","category":"math.OC"}
{"created":"2024-03-11 16:09:39","title":"Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification","abstract":"Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\\pi$). To do this, we need a behavioural model of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold. In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g.\\ the discount rate). Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function.","sentences":["Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\\pi$).","To do this, we need a behavioural model of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation.","However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models.","This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data.","In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model.","Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold.","In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g.\\ the discount rate).","Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function."],"url":"http://arxiv.org/abs/2403.06854v1","category":"cs.LG"}
{"created":"2024-03-11 16:03:35","title":"DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation","abstract":"World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos. However, significant challenges still exist in generating customized driving videos. In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos. Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories. Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories. Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos. DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking). Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%.","sentences":["World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos.","However, significant challenges still exist in generating customized driving videos.","In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos.","Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories.","Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories.","Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos.","DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.","Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking).","Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%."],"url":"http://arxiv.org/abs/2403.06845v1","category":"cs.CV"}
{"created":"2024-03-11 16:02:01","title":"Hybrid optimal control with mixed-integer Lagrangian methods","abstract":"Models involving hybrid systems are versatile in their application, but difficult to handle and optimize efficiently due to their combinatorial nature. This work presents a method to cope with hybrid optimal control problems which, in contrast to decomposition techniques, does not require relaxing the integrality constraints. Based on the discretize-then-optimize approach, our scheme addresses mixed-integer nonlinear problems under mild assumptions. The proposed numerical algorithm builds upon the augmented Lagrangian framework, whose subproblems are handled using successive mixed-integer linearizations with trust regions. We validate the performance of the numerical routine with extensive investigations using several hybrid optimal control problems from different fields of application. Promising preliminary results are presented for a motion planning task with hysteresis and drag, a Lotka-Volterra fishing problem, and a facility location design problem.","sentences":["Models involving hybrid systems are versatile in their application, but difficult to handle and optimize efficiently due to their combinatorial nature.","This work presents a method to cope with hybrid optimal control problems which, in contrast to decomposition techniques, does not require relaxing the integrality constraints.","Based on the discretize-then-optimize approach, our scheme addresses mixed-integer nonlinear problems under mild assumptions.","The proposed numerical algorithm builds upon the augmented Lagrangian framework, whose subproblems are handled using successive mixed-integer linearizations with trust regions.","We validate the performance of the numerical routine with extensive investigations using several hybrid optimal control problems from different fields of application.","Promising preliminary results are presented for a motion planning task with hysteresis and drag, a Lotka-Volterra fishing problem, and a facility location design problem."],"url":"http://arxiv.org/abs/2403.06842v1","category":"math.OC"}
{"created":"2024-03-11 15:33:55","title":"Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science","abstract":"Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $n$ elements built from samples, each containing $m$ features, the stepsize parameters estimation and iterations in our algorithms scale on the order of $O(mn)$ operations and can be trivially parallelized. Moreover, the strong $\\ell_{1}$ convexity of the Kullback--Leibler divergence allows for larger stepsize parameters, thereby speeding up the convergence rate of our algorithms. To illustrate the efficiency of our novel algorithms, we consider the problem of estimating probabilities of fire occurrences as a function of ecological features in the Western US MTBS-Interagency wildfire data set. Our numerical results show that our algorithms outperform the state of the arts by one order of magnitude and yield results that agree with physical models of wildfire occurrence and previous statistical analyses of wildfire drivers.","sentences":["Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data.","Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications.","State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack.","In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models.","Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently.","For Maxent models with discrete probability distribution of $n$ elements built from samples, each containing $m$ features, the stepsize parameters estimation and iterations in our algorithms scale on the order of $O(mn)$ operations and can be trivially parallelized.","Moreover, the strong $\\ell_{1}$ convexity of the Kullback--Leibler divergence allows for larger stepsize parameters, thereby speeding up the convergence rate of our algorithms.","To illustrate the efficiency of our novel algorithms, we consider the problem of estimating probabilities of fire occurrences as a function of ecological features in the Western US MTBS-Interagency wildfire data set.","Our numerical results show that our algorithms outperform the state of the arts by one order of magnitude and yield results that agree with physical models of wildfire occurrence and previous statistical analyses of wildfire drivers."],"url":"http://arxiv.org/abs/2403.06816v1","category":"stat.ML"}
{"created":"2024-03-11 15:32:56","title":"Monotone Individual Fairness","abstract":"We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions. We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors. Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{1/2+2b}),\\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\\leq b \\leq 1/4$. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{2/3+2b}),\\mathcal{O}(T^{5/6-b}))$ for regret, number of fairness violations, for $0\\leq b \\leq 1/6$. In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms. Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to $\\tilde{\\mathcal{O}}(\\alpha^{-2})$ in the full information setting, and $\\tilde{\\mathcal{O}}(\\alpha^{-2} + k^2T^{1/3})$ in the partial information setting, where $\\alpha$ is the sensitivity for reporting fairness violations, and $k$ is the number of individuals in a round.","sentences":["We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly.","We first extend the frameworks of Gillen et al.","(2018);","Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions.","We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors.","Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{1/2+2b}),\\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\\leq b \\leq","1/4$. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{2/3+2b}),\\mathcal{O}(T^{5/6-b}))$ for regret, number of fairness violations, for $0\\leq","b \\leq 1/6$.","In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms.","Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to $\\tilde{\\mathcal{O}}(\\alpha^{-2})$ in the full information setting, and $\\tilde{\\mathcal{O}}(\\alpha^{-2} + k^2T^{1/3})$ in the partial information setting, where $\\alpha$ is the sensitivity for reporting fairness violations, and $k$ is the number of individuals in a round."],"url":"http://arxiv.org/abs/2403.06812v1","category":"cs.LG"}
{"created":"2024-03-11 15:25:03","title":"On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes","abstract":"We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP. Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs. We also present simulations to empirically evaluate the performance of average reward policy gradient algorithm.","sentences":["We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs).","Specifically, we focus on ergodic tabular MDPs with finite state and action spaces.","Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$ regret, where $T$ represents the number of iterations.","Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon.","Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees.","In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP.","Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs.","We also present simulations to empirically evaluate the performance of average reward policy gradient algorithm."],"url":"http://arxiv.org/abs/2403.06806v1","category":"cs.LG"}
{"created":"2024-03-11 15:11:57","title":"Boosting Image Restoration via Priors from Pre-trained Models","abstract":"Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions. Yet, their potential for low-level tasks such as image restoration remains relatively unexplored. In this paper, we explore such models to enhance image restoration. As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning. Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising.","sentences":["Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions.","Yet, their potential for low-level tasks such as image restoration remains relatively unexplored.","In this paper, we explore such models to enhance image restoration.","As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF.","PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA).","PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning.","Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising."],"url":"http://arxiv.org/abs/2403.06793v1","category":"cs.CV"}
{"created":"2024-03-11 14:51:34","title":"Domain-Independent Dynamic Programming and Constraint Programming Approaches for Assembly Line Balancing Problems with Setups","abstract":"We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type-1 and type-2 assembly line balancing problem with sequence-dependent setup times (SUALBP). The goal is to assign tasks to assembly stations and to sequence these tasks within each station, while satisfying precedence relations specified between a subset of task pairs. Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned. The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time. For type-1 SUALBP, the objective is to minimize the number of stations, given a maximum cycle time. For type-2 SUALBP, the objective is to minimize the cycle time, given the number of stations. On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1. For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition. By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems.","sentences":["We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type-1 and type-2 assembly line balancing problem with sequence-dependent setup times (SUALBP).","The goal is to assign tasks to assembly stations and to sequence these tasks within each station, while satisfying precedence relations specified between a subset of task pairs.","Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned.","The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time.","For type-1 SUALBP, the objective is to minimize the number of stations, given a maximum cycle time.","For type-2 SUALBP, the objective is to minimize the cycle time, given the number of stations.","On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1.","For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition.","By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems."],"url":"http://arxiv.org/abs/2403.06780v1","category":"math.OC"}
{"created":"2024-03-11 14:18:07","title":"Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges","abstract":"Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.","sentences":["Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results.","However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks.","This paper reviews the current implementations of LLMs in PM and reflects on three different questions.","1) What is the minimal set of capabilities required for PM on LLMs?","2) Which benchmark strategies help choose optimal LLMs for PM?","3) How do we evaluate the output of LLMs on specific PM tasks?","The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms."],"url":"http://arxiv.org/abs/2403.06749v1","category":"cs.DB"}
{"created":"2024-03-11 14:10:35","title":"Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot","abstract":"Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design. To address the limitations of manual tuning, we introduce a self-optimizing controller named fuzzyPID, leveraging the analysis of responses from various dynamic and static systems. The rule-based controller design is implemented using Matlab/Simulink, and trajectory tracking simulations are conducted within the CoppeliaSim environment. Similarly, a non-linear model predictive controller(NMPC) is proposed to compare tracking performance with fuzzyPID. We also assess the impact of tunable parameters of NMPC on its tracking accuracy. Simulation results validate the precision and effectiveness of NMPC over fuzzyPID controller while trading computational complexity.","sentences":["Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design.","To address the limitations of manual tuning, we introduce a self-optimizing controller named fuzzyPID, leveraging the analysis of responses from various dynamic and static systems.","The rule-based controller design is implemented using Matlab/Simulink, and trajectory tracking simulations are conducted within the CoppeliaSim environment.","Similarly, a non-linear model predictive controller(NMPC) is proposed to compare tracking performance with fuzzyPID.","We also assess the impact of tunable parameters of NMPC on its tracking accuracy.","Simulation results validate the precision and effectiveness of NMPC over fuzzyPID controller while trading computational complexity."],"url":"http://arxiv.org/abs/2403.06744v1","category":"cs.RO"}
{"created":"2024-03-11 14:07:53","title":"Distribution-Aware Data Expansion with Diffusion Models","abstract":"The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at https://github.com/haoweiz23/DistDiff","sentences":["The scale and quality of a dataset significantly impact the performance of deep models.","However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor.","To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models.","Current data expansion methods encompass image transformation-based and synthesis-based methods.","The transformation-based methods introduce only local variations, resulting in poor diversity.","While image synthesis-based methods can create entirely new content, significantly enhancing informativeness.","However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples.","In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model.","DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance.","We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks.","Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method.","Our code is available at https://github.com/haoweiz23/DistDiff"],"url":"http://arxiv.org/abs/2403.06741v1","category":"cs.CV"}
{"created":"2024-03-11 14:02:24","title":"Post-Training Attribute Unlearning in Recommender Systems","abstract":"With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \\textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers. We further extend this measurement to handle multi-class attribute cases with efficient computational overhead. The second component is regularization loss, where we explore a function-space measurement that effectively maintains recommendation performance compared to parameter-space regularization. We use stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods.","sentences":["With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention.","Existing studies predominantly use training data, i.e., model inputs, as unlearning target.","However, attackers can extract private information from the model even if it has not been explicitly encountered during training.","We name this unseen information as \\textit{attribute} and treat it as unlearning target.","To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable.","In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed.","To address the PoT-AU problem in recommender systems, we propose a two-component loss function.","The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers.","We further extend this measurement to handle multi-class attribute cases with efficient computational overhead.","The second component is regularization loss, where we explore a function-space measurement that effectively maintains recommendation performance compared to parameter-space regularization.","We use stochastic gradient descent algorithm to optimize our proposed loss.","Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods."],"url":"http://arxiv.org/abs/2403.06737v1","category":"cs.IR"}
{"created":"2024-03-11 13:42:12","title":"Bayesian prediction regions and density estimation with type-2 censored data","abstract":"For exponentially distributed lifetimes, we consider the prediction of future order statistics based on having observed the first $m$ order statistics. We focus on the previously less explored aspects of predicting: (i) an arbitrary pair of future order statistics such as the next and last ones, as well as (ii) the next $N$ future order statistics. We provide explicit and exact Bayesian credible regions associated with Gamma priors, and constructed by identifying a region with a given credibility $1-\\lambda$ under the Bayesian predictive density. For (ii), the HPD region is obtained, while a two-step algorithm is given for (i). The predictive distributions are represented as mixtures of bivariate Pareto distributions, as well as multivariate Pareto distributions. For the non-informative prior density choice, we demonstrate that a resulting Bayesian credible region has matching frequentist coverage probability, and that the resulting predictive density possesses the optimality properties of best invariance and minimaxity.","sentences":["For exponentially distributed lifetimes, we consider the prediction of future order statistics based on having observed the first $m$ order statistics.","We focus on the previously less explored aspects of predicting: (i) an arbitrary pair of future order statistics such as the next and last ones, as well as (ii) the next $N$ future order statistics.","We provide explicit and exact Bayesian credible regions associated with Gamma priors, and constructed by identifying a region with a given credibility $1-\\lambda$ under the Bayesian predictive density.","For (ii), the HPD region is obtained, while a two-step algorithm is given for (i).","The predictive distributions are represented as mixtures of bivariate Pareto distributions, as well as multivariate Pareto distributions.","For the non-informative prior density choice, we demonstrate that a resulting Bayesian credible region has matching frequentist coverage probability, and that the resulting predictive density possesses the optimality properties of best invariance and minimaxity."],"url":"http://arxiv.org/abs/2403.06718v1","category":"math.ST"}
{"created":"2024-03-11 13:33:31","title":"Tikhonov Regularization for Stochastic Non-Smooth Convex Optimization in Hilbert Spaces","abstract":"To solve non-smooth convex optimization problems with a noisy gradient input, we analyze the global behavior of subgradient-like flows under stochastic errors. The objective function is composite, being equal to the sum of two convex functions, one being differentiable and the other potentially non-smooth. We then use stochastic differential inclusions where the drift term is minus the subgradient of the objective function, and the diffusion term is either bounded or square-integrable. In this context, under Lipschitz's continuity of the differentiable term and a growth condition of the non-smooth term, our first main result shows almost sure weak convergence of the trajectory process towards a minimizer of the objective function. Then, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution. We find an explicit tuning of this parameter when our objective function satisfies a local error-bound inequality. We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex and strongly convex case.","sentences":["To solve non-smooth convex optimization problems with a noisy gradient input, we analyze the global behavior of subgradient-like flows under stochastic errors.","The objective function is composite, being equal to the sum of two convex functions, one being differentiable and the other potentially non-smooth.","We then use stochastic differential inclusions where the drift term is minus the subgradient of the objective function, and the diffusion term is either bounded or square-integrable.","In this context, under Lipschitz's continuity of the differentiable term and a growth condition of the non-smooth term, our first main result shows almost sure weak convergence of the trajectory process towards a minimizer of the objective function.","Then, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution.","We find an explicit tuning of this parameter when our objective function satisfies a local error-bound inequality.","We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex and strongly convex case."],"url":"http://arxiv.org/abs/2403.06708v1","category":"math.OC"}
{"created":"2024-03-11 13:29:23","title":"Propagation of Solar Energetic Particles in 3D MHD Simulations of the Solar Wind","abstract":"We propagate relativistic test particles in the field of a steady 3D MHD simulations of the solar wind. We use the MPI-AMRVAC code for the wind simulations and integrate the relativistic guiding center equations using a new third-order accurate time integration scheme to solve the particle trajectories. Diffusion in velocity space, given a particle-turbulence mean free path $\\lambda_\\parallel$ along the magnetic field, is also included. Preliminary results for $81\\:{\\rm keV}$ electrons injected at 0.139 AU heliocentric distance and mean free path $\\lambda_\\parallel =0.5\\:{\\rm AU}$ are in a good qualitative agreement with measurements at 1 AU.","sentences":["We propagate relativistic test particles in the field of a steady 3D MHD simulations of the solar wind.","We use the MPI-AMRVAC code for the wind simulations and integrate the relativistic guiding center equations using a new third-order accurate time integration scheme to solve the particle trajectories.","Diffusion in velocity space, given a particle-turbulence mean free path $\\lambda_\\parallel$ along the magnetic field, is also included.","Preliminary results for $81\\:{\\rm keV}$ electrons injected at 0.139 AU heliocentric distance and mean free path $\\lambda_\\parallel =0.5\\:{\\rm AU}$ are in a good qualitative agreement with measurements at 1 AU."],"url":"http://arxiv.org/abs/2403.06706v1","category":"astro-ph.SR"}
{"created":"2024-03-12 17:50:25","title":"Synthesis of epitaxial magnetic pyrochlore heterojunctions","abstract":"The synthesis of stoichiometric and epitaxial pyrochlore iridate thin films presents significant challenges yet is critical for unlocking experimental access to novel topological and magnetic states. Towards this goal, we unveil an in-situ two-stage growth mechanism that facilitates the synthesis of high-quality oriented pyrochlore iridate thin films. The growth starts with the deposition of a pyrochlore titanate as an active iso-structural template, followed by the application of an in-situ solid phase epitaxy technique in the second stage to accomplish the formation of single crystalline, large-area films. This novel protocol ensures the preservation of stoichiometry and structural homogeneity, leading to a marked improvement in surface and interface qualities over previously reported methods. The success of this synthesis approach is attributed to the application of directional laser-heat annealing, which effectively reorganizes the continuous random network of ions into a crystalline structure, as evidenced by our comprehensive analysis of the growth kinetics. This new synthesis approach advances our understanding of pyrochlore iridate film fabrication and opens a new perspective for investigating their unique physical properties.","sentences":["The synthesis of stoichiometric and epitaxial pyrochlore iridate thin films presents significant challenges yet is critical for unlocking experimental access to novel topological and magnetic states.","Towards this goal, we unveil an in-situ two-stage growth mechanism that facilitates the synthesis of high-quality oriented pyrochlore iridate thin films.","The growth starts with the deposition of a pyrochlore titanate as an active iso-structural template, followed by the application of an in-situ solid phase epitaxy technique in the second stage to accomplish the formation of single crystalline, large-area films.","This novel protocol ensures the preservation of stoichiometry and structural homogeneity, leading to a marked improvement in surface and interface qualities over previously reported methods.","The success of this synthesis approach is attributed to the application of directional laser-heat annealing, which effectively reorganizes the continuous random network of ions into a crystalline structure, as evidenced by our comprehensive analysis of the growth kinetics.","This new synthesis approach advances our understanding of pyrochlore iridate film fabrication and opens a new perspective for investigating their unique physical properties."],"url":"http://arxiv.org/abs/2403.07861v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 17:39:03","title":"On measuring the topological charge of anyons","abstract":"In this paper we discuss the principles of measuring topological charge or representation traveling in the set of anyons. We describe the procedure and analyze how it works for the different values of parameters of the theory. We also show how it can be modified to be more effective for different levels of Chern-Simons theory.","sentences":["In this paper we discuss the principles of measuring topological charge or representation traveling in the set of anyons.","We describe the procedure and analyze how it works for the different values of parameters of the theory.","We also show how it can be modified to be more effective for different levels of Chern-Simons theory."],"url":"http://arxiv.org/abs/2403.07847v1","category":"hep-th"}
{"created":"2024-03-12 17:16:25","title":"A Science4Peace initiative: Alleviating the consequences of sanctions in international scientific cooperation","abstract":"The armed invasion of Ukraine by the Russian Federation has adversely affected the relations between Russia and Western countries. Among other aspects, it has put scientific cooperation and collaboration into question and changed the scientific landscape significantly. Cooperation between some Western institutions and their Russian and Belarusian partners were put on hold after February 24, 2022. The CERN Council decided at its meeting in December 2023 to terminate cooperation agreements with Russia and Belarus that date back a decade. CERN is an international institution with UN observer status, and has so far played a role in international cooperation which was independent of national political strategies. We argue that the Science4Peace idea still has a great value and scientific collaboration between scientists must continue, since fundamental science is by its nature an international discipline. A ban of scientists participating in international cooperation and collaboration is against the traditions, requirements and understanding of science. We call for measures to reactivate the peaceful cooperation of individual scientists on fundamental research in order to stimulate international cooperation for a more peaceful world in the future. Specifically, we plead for finding ways to continue this cooperation through international organizations, such as CERN and JINR.","sentences":["The armed invasion of Ukraine by the Russian Federation has adversely affected the relations between Russia and Western countries.","Among other aspects, it has put scientific cooperation and collaboration into question and changed the scientific landscape significantly.","Cooperation between some Western institutions and their Russian and Belarusian partners were put on hold after February 24, 2022.","The CERN Council decided at its meeting in December 2023 to terminate cooperation agreements with Russia and Belarus that date back a decade.","CERN is an international institution with UN observer status, and has so far played a role in international cooperation which was independent of national political strategies.","We argue that the Science4Peace idea still has a great value and scientific collaboration between scientists must continue, since fundamental science is by its nature an international discipline.","A ban of scientists participating in international cooperation and collaboration is against the traditions, requirements and understanding of science.","We call for measures to reactivate the peaceful cooperation of individual scientists on fundamental research in order to stimulate international cooperation for a more peaceful world in the future.","Specifically, we plead for finding ways to continue this cooperation through international organizations, such as CERN and JINR."],"url":"http://arxiv.org/abs/2403.07833v1","category":"physics.soc-ph"}
{"created":"2024-03-12 16:40:27","title":"The Importance of Optical Wavelength Data on Atmospheric Retrievals of Exoplanet Transmission Spectra","abstract":"Exoplanet transmission spectra provide rich information about the chemical composition, clouds and temperature structure of exoplanet atmospheres. Most exoplanet transmission spectra only span infrared wavelengths ($\\gtrsim$ 1 $\\rm{\\mu m}$), which can preclude crucial atmospheric information from shorter wavelengths. Here, we explore how retrieved atmospheric parameters from exoplanet transmission spectra change with the addition of optical data. From a sample of 14 giant planets with transit spectra from 0.3-4.5 $\\rm{\\mu m}$, primarily from the Hubble and Spitzer space telescopes, we apply a free chemistry retrieval to planetary spectra for wavelength ranges of 0.3-4.5 $\\rm{\\mu m}$, 0.6-4.5 $\\rm{\\mu m}$, and 1.1-4.5 $\\rm{\\mu m}$. We analyse the posterior distributions of these retrievals and perform an information content analysis, finding wavelengths below 0.6 $\\rm{\\mu m}$ are necessary to constrain cloud scattering slope parameters ($\\log{a}$ and $\\gamma$) and alkali species Na and K. There is limited improvement in the constraints on the remaining atmospheric parameters. Across the population, we find limb temperatures are retrieved colder than planetary equilibrium temperatures but have an overall good agreement with Global Circulation Models. As JWST extends to a minimum wavelength of 0.6 $\\rm{\\mu m}$, we demonstrate that exploration into complementing JWST observations with optical HST data is important to further our understanding of aerosol properties and alkali abundances in exoplanet atmospheres.","sentences":["Exoplanet transmission spectra provide rich information about the chemical composition, clouds and temperature structure of exoplanet atmospheres.","Most exoplanet transmission spectra only span infrared wavelengths ($\\gtrsim$ 1 $\\rm{\\mu m}$), which can preclude crucial atmospheric information from shorter wavelengths.","Here, we explore how retrieved atmospheric parameters from exoplanet transmission spectra change with the addition of optical data.","From a sample of 14 giant planets with transit spectra from 0.3-4.5 $\\rm{\\mu m}$, primarily from the Hubble and Spitzer space telescopes, we apply a free chemistry retrieval to planetary spectra for wavelength ranges of 0.3-4.5 $\\rm{\\mu m}$, 0.6-4.5 $\\rm{\\mu m}$, and 1.1-4.5 $\\rm{\\mu m}$.","We analyse the posterior distributions of these retrievals and perform an information content analysis, finding wavelengths below 0.6 $\\rm{\\mu m}$ are necessary to constrain cloud scattering slope parameters ($\\log{a}$ and $\\gamma$) and alkali species Na and","K.","There is limited improvement in the constraints on the remaining atmospheric parameters.","Across the population, we find limb temperatures are retrieved colder than planetary equilibrium temperatures but have an overall good agreement with Global Circulation Models.","As JWST extends to a minimum wavelength of 0.6 $\\rm{\\mu m}$, we demonstrate that exploration into complementing JWST observations with optical HST data is important to further our understanding of aerosol properties and alkali abundances in exoplanet atmospheres."],"url":"http://arxiv.org/abs/2403.07801v1","category":"astro-ph.EP"}
{"created":"2024-03-12 16:30:33","title":"RADES axion search results with a High-Temperature Superconducting cavity in an 11.7 T magnet","abstract":"We describe the results of a haloscope axion search performed with an 11.7 T dipole magnet at CERN. The search used a custom-made radio-frequency cavity coated with high-temperature superconducting tape. A set of 27 h of data at a resonant frequency of around 8.84 GHz was analysed. In the range of axion mass 36.5676 $\\mu$eV to 36.5699 $\\mu$eV, corresponding to a width of 554 kHz, no signal excess hinting at an axion-like particle was found. Correspondingly, in this mass range, a limit on the axion to photon coupling-strength was set in the range between g$_{a\\gamma}\\gtrsim$ 6.2e-13 GeV$^{-1}$ and g$_{a\\gamma}\\gtrsim$ 1.54e-13 GeV$^{-1}$ with a 95% confidence level.","sentences":["We describe the results of a haloscope axion search performed with an 11.7 T dipole magnet at CERN.","The search used a custom-made radio-frequency cavity coated with high-temperature superconducting tape.","A set of 27 h of data at a resonant frequency of around 8.84 GHz was analysed.","In the range of axion mass 36.5676 $\\mu$eV to 36.5699 $\\mu$eV, corresponding to a width of 554 kHz, no signal excess hinting at an axion-like particle was found.","Correspondingly, in this mass range, a limit on the axion to photon coupling-strength was set in the range between g$_{a\\gamma}\\gtrsim$ 6.2e-13 GeV$^{-1}$ and g$_{a\\gamma}\\gtrsim$ 1.54e-13 GeV$^{-1}$ with a 95% confidence level."],"url":"http://arxiv.org/abs/2403.07790v1","category":"hep-ex"}
{"created":"2024-03-12 16:02:29","title":"Joint Modeling of Longitudinal Measurements and Time-to-event Outcomes Using BUGS","abstract":"The objective of this paper is to provide an introduction to the principles of Bayesian joint modeling of longitudinal measurements and time-to-event outcomes, as well as model implementation using the BUGS language syntax. This syntax can be executed directly using OpenBUGS or by utilizing convenient functions to invoke OpenBUGS and JAGS from R software. In this paper, all details of joint models are provided, ranging from simple to more advanced models. The presentation started with the joint modeling of a Gaussian longitudinal marker and time-to-event outcome. The implementation of the Bayesian paradigm of the model is reviewed. The strategies for simulating data from the JM are also discussed. A proportional hazard model with various forms of baseline hazards, along with the discussion of all possible association structures between the two sub-models are taken into consideration. The paper covers joint models with multivariate longitudinal measurements, zero-inflated longitudinal measurements, competing risks, and time-to-event with cure fraction. The models are illustrated by the analyses of several real data sets. All simulated and real data and code are available at \\url{https://github.com/tbaghfalaki/JM-with-BUGS-and-JAGS}.","sentences":["The objective of this paper is to provide an introduction to the principles of Bayesian joint modeling of longitudinal measurements and time-to-event outcomes, as well as model implementation using the BUGS language syntax.","This syntax can be executed directly using OpenBUGS or by utilizing convenient functions to invoke OpenBUGS and JAGS from R software.","In this paper, all details of joint models are provided, ranging from simple to more advanced models.","The presentation started with the joint modeling of a Gaussian longitudinal marker and time-to-event outcome.","The implementation of the Bayesian paradigm of the model is reviewed.","The strategies for simulating data from the JM are also discussed.","A proportional hazard model with various forms of baseline hazards, along with the discussion of all possible association structures between the two sub-models are taken into consideration.","The paper covers joint models with multivariate longitudinal measurements, zero-inflated longitudinal measurements, competing risks, and time-to-event with cure fraction.","The models are illustrated by the analyses of several real data sets.","All simulated and real data and code are available at \\url{https://github.com/tbaghfalaki/JM-with-BUGS-and-JAGS}."],"url":"http://arxiv.org/abs/2403.07778v1","category":"stat.ME"}
{"created":"2024-03-12 15:59:28","title":"Superexchange Mechanism in Coupled Triangulenes Forming Spin-1 Chains","abstract":"We show that the origin of the antiferromagnetic coupling in spin-1 triangulene chains, which were recently synthesized and measured by Mishra et al. Nature 598, 287-292 (2021) originates from a superexchange mechanism. This process, mediated by inter-triangulene states, opens the possibility to control parameters in the effective bilinear-biquadratic spin model. We start from the derivation of an effective tight-binding model for triangulene chains using a combination of tight-binding and Hartree-Fock methods fitted to hybrid density functional theory results. Next, correlation effects are investigated within the configuration interaction method. Our low-energy many-body spectrum for $N_{\\rm Tr}=2$ and $N_{\\rm Tr}=4$ triangulene chains agree well with the bilinear-biquadratic spin-1 chain antiferromagnetic model when indirect coupling processes, and superexchange coupling between triangulene spins are taken into account.","sentences":["We show that the origin of the antiferromagnetic coupling in spin-1 triangulene chains, which were recently synthesized and measured by Mishra et al.","Nature 598, 287-292 (2021) originates from a superexchange mechanism.","This process, mediated by inter-triangulene states, opens the possibility to control parameters in the effective bilinear-biquadratic spin model.","We start from the derivation of an effective tight-binding model for triangulene chains using a combination of tight-binding and Hartree-Fock methods fitted to hybrid density functional theory results.","Next, correlation effects are investigated within the configuration interaction method.","Our low-energy many-body spectrum for $N_{\\rm Tr}=2$ and $N_{\\rm Tr}=4$ triangulene chains agree well with the bilinear-biquadratic spin-1 chain antiferromagnetic model when indirect coupling processes, and superexchange coupling between triangulene spins are taken into account."],"url":"http://arxiv.org/abs/2403.07774v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 15:58:38","title":"A first principles study of the Stark shift effect on the zero-phonon line of the NV center in diamond","abstract":"Point defects in semiconductors are attractive candidates for quantum information science applications owing to their ability to act as spin-photon interface or single-photon emitters. However, the coupling between the change of dipole moment upon electronic excitation and stray electric fields in the vicinity of the defect, an effect known as Stark shift, can cause significant spectral diffusion in the emitted photons. In this work, using first principles computations, we revisit the methodology to compute the Stark shift of point defects up to the second order. The approach consists of applying an electric field on a defect in a slab and monitoring the changes in the computed zero-phonon line (i.e., difference in energy between the ground and excited state) obtained from constraining the orbital occupations (constrained-DFT). We study the Stark shift of the negatively charged nitrogen-vacancy (NV) center in diamond using this slab approach. We discuss and compare two approaches to ensure a negatively charged defect in a slab and we show that converged values of the Stark shift measured by the change in dipole moment between the ground and excited states ($\\Delta \\mu$) can be obtained. We obtain a Stark shift of $\\Delta \\mu$=2.68D using the semi-local GGA-PBE functional and of $\\Delta \\mu$=2.23D using the HSE hybrid-functional. The results of the slab computations are significantly different than those obtained with Modern Theory of Polarization ($\\Delta \\mu$=4.34D for GGA-PBE) indicating a potential issue with the combination of constrained-DFT and Modern Theory of Polarization, at least in certain codes.","sentences":["Point defects in semiconductors are attractive candidates for quantum information science applications owing to their ability to act as spin-photon interface or single-photon emitters.","However, the coupling between the change of dipole moment upon electronic excitation and stray electric fields in the vicinity of the defect, an effect known as Stark shift, can cause significant spectral diffusion in the emitted photons.","In this work, using first principles computations, we revisit the methodology to compute the Stark shift of point defects up to the second order.","The approach consists of applying an electric field on a defect in a slab and monitoring the changes in the computed zero-phonon line (i.e., difference in energy between the ground and excited state) obtained from constraining the orbital occupations (constrained-DFT).","We study the Stark shift of the negatively charged nitrogen-vacancy (NV) center in diamond using this slab approach.","We discuss and compare two approaches to ensure a negatively charged defect in a slab and we show that converged values of the Stark shift measured by the change in dipole moment between the ground and excited states ($\\Delta \\mu$) can be obtained.","We obtain a Stark shift of $\\Delta \\mu$=2.68D using the semi-local GGA-PBE functional and of $\\Delta \\mu$=2.23D using the HSE hybrid-functional.","The results of the slab computations are significantly different than those obtained with Modern Theory of Polarization ($\\Delta \\mu$=4.34D for GGA-PBE) indicating a potential issue with the combination of constrained-DFT and Modern Theory of Polarization, at least in certain codes."],"url":"http://arxiv.org/abs/2403.07771v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 15:56:53","title":"PROSKILL: A formal skill language for acting in robotics","abstract":"Acting is an important decisional function for autonomous robots. Acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. While sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. Following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. This language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. We illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission.","sentences":["Acting is an important decisional function for autonomous robots.","Acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online.","While sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills.","Following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills.","This language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification.","We illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission."],"url":"http://arxiv.org/abs/2403.07770v1","category":"cs.RO"}
{"created":"2024-03-12 15:55:44","title":"Higher Witt Groups for 2-Categories I: Centralizers","abstract":"In this article, we investigate monoidal, braided, sylleptic centralizers of monoidal, braided, sylleptic 2-functors. We specifically focus on multifusion 2-categories and show that monoidal, braided, sylleptic centralizers are multifusion again, via studying the corresponding enveloping algebras. We provide a characterization of the non-degeneracy condition for monoidal, braided, and sylleptic fusion 2-categories, via vanishing of their centers. Applying Double Centralizer Theorems, we establish the relationship between monoidal, braided, symmetric local modules and free modules. In particular, we obtain factorization properties of non-degenerate monoidal, braided, and sylleptic fusion 2-categories. Main results in this article will be used to study higher Witt equivalences of non-degenerate monoidal, braided, sylleptic 2-categories in the sequential articles.","sentences":["In this article, we investigate monoidal, braided, sylleptic centralizers of monoidal, braided, sylleptic 2-functors.","We specifically focus on multifusion 2-categories and show that monoidal, braided, sylleptic centralizers are multifusion again, via studying the corresponding enveloping algebras.","We provide a characterization of the non-degeneracy condition for monoidal, braided, and sylleptic fusion 2-categories, via vanishing of their centers.","Applying Double Centralizer Theorems, we establish the relationship between monoidal, braided, symmetric local modules and free modules.","In particular, we obtain factorization properties of non-degenerate monoidal, braided, and sylleptic fusion 2-categories.","Main results in this article will be used to study higher Witt equivalences of non-degenerate monoidal, braided, sylleptic 2-categories in the sequential articles."],"url":"http://arxiv.org/abs/2403.07768v1","category":"math.CT"}
{"created":"2024-03-12 15:43:32","title":"Complete Minimal Left-Right Symmetric Model File","abstract":"We develop a comprehensive implementation of the minimal Left-Right symmetric model within a FeynRules model file. We derive the complete set of mass spectra and mixings for the charged and neutral gauge bosons, would-be-Goldstones and gauge fixing, together with the ghost Lagrangian. In the scalar sector, we analytically re-derive all the massive states with mixings and devise a physical input scheme, which expresses the model couplings in terms of masses and mixing angles of all states. Fermion couplings are determined in closed form, including the Dirac mixing in the neutrino sector, evaluated explicitly with the Cayley-Hamilton method. We calculate the one loop next-to-leading QCD corrections and provide a complete UFO file for NLO studies, demonstrated on relevant benchmarks. We provide various restricted variants of the model file with different gauges, massless states, neutrino hierarchies and parity violating $g_L \\neq g_R$ gauge couplings.","sentences":["We develop a comprehensive implementation of the minimal Left-Right symmetric model within a FeynRules model file.","We derive the complete set of mass spectra and mixings for the charged and neutral gauge bosons, would-be-Goldstones and gauge fixing, together with the ghost Lagrangian.","In the scalar sector, we analytically re-derive all the massive states with mixings and devise a physical input scheme, which expresses the model couplings in terms of masses and mixing angles of all states.","Fermion couplings are determined in closed form, including the Dirac mixing in the neutrino sector, evaluated explicitly with the Cayley-Hamilton method.","We calculate the one loop next-to-leading QCD corrections and provide a complete UFO file for NLO studies, demonstrated on relevant benchmarks.","We provide various restricted variants of the model file with different gauges, massless states, neutrino hierarchies and parity violating $g_L \\neq g_R$ gauge couplings."],"url":"http://arxiv.org/abs/2403.07756v1","category":"hep-ph"}
{"created":"2024-03-12 15:16:40","title":"Topological Data Analysis of Monopoles in $U(1)$ Lattice Gauge Theory","abstract":"In $4$-dimensional pure compact $U(1)$ lattice gauge theory, we analyse topological aspects of the dynamics of monopoles across the deconfinement phase transition. We do this using tools from Topological Data Analysis (TDA). We demonstrate that observables constructed from the zeroth and first homology groups of monopole current networks may be used to quantitatively and robustly locate the critical inverse coupling $\\beta_{c}$ through finite-size scaling. Our method provides a mathematically robust framework for the characterisation of topological invariants related to monopole currents, putting on firmer ground earlier investigations. Moreover, our approach can be generalised to the study of Abelian monopoles in non-Abelian gauge theories.","sentences":["In $4$-dimensional pure compact $U(1)$ lattice gauge theory, we analyse topological aspects of the dynamics of monopoles across the deconfinement phase transition.","We do this using tools from Topological Data Analysis (TDA).","We demonstrate that observables constructed from the zeroth and first homology groups of monopole current networks may be used to quantitatively and robustly locate the critical inverse coupling $\\beta_{c}$ through finite-size scaling.","Our method provides a mathematically robust framework for the characterisation of topological invariants related to monopole currents, putting on firmer ground earlier investigations.","Moreover, our approach can be generalised to the study of Abelian monopoles in non-Abelian gauge theories."],"url":"http://arxiv.org/abs/2403.07739v1","category":"hep-lat"}
{"created":"2024-03-12 15:13:15","title":"On disintegration of lean hydrogen flames in narrow gaps","abstract":"The disintegration of near limit flames propagating through the gap of Hele-Shaw cells has recently become a subject of active research. In this paper, the flamelets resulting from the disintegration of the continuous front are interpreted in terms of the Zeldovich flame-balls stabilized by volumetric heat losses. A complicated free-boundary problem for 2D self-drifting near circular flamelets is reduced to a 1D model. The 1D formulation is then utilized to obtain the locus of the flamelet velocity, size, heat losses and Lewis numbers at which the self-drifting flamelets may exist.","sentences":["The disintegration of near limit flames propagating through the gap of Hele-Shaw cells has recently become a subject of active research.","In this paper, the flamelets resulting from the disintegration of the continuous front are interpreted in terms of the Zeldovich flame-balls stabilized by volumetric heat losses.","A complicated free-boundary problem for 2D self-drifting near circular flamelets is reduced to a 1D model.","The 1D formulation is then utilized to obtain the locus of the flamelet velocity, size, heat losses and Lewis numbers at which the self-drifting flamelets may exist."],"url":"http://arxiv.org/abs/2403.07734v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 15:10:43","title":"Mechanisms of Elevated Temperature Galling in Hardfacings","abstract":"The galling mechanism of Tristelle 5183, an Fe-based hardfacing alloy, was investigated at elevated temperature. The test was performed using a bespoke galling rig. Adhesive transfer and galling were found to occur, as a result of shear at the adhesion boundary and the activation of an internal shear plane within one of the tribosurfaces. During deformation, carbides were observed to have fractured, as a result of the shear train they were exposed to and their lack of ductility. In the case of niobium carbides, their fracture resulted in the formation of voids, which were found to coalesce and led to cracking and adhesive transfer. A tribologically affected zone (TAZ) was found to form, which contained nanocrystalline austenite, as a result of the shear exerted within 30{\\mu}m of the adhesion boundaries. The galling of Tristelle 5183 initiated from the formation of an adhesive boundary, followed by sub-surface shear in only one tribosurface, Following further sub-surface shear, an internal shear plane is activated. internal shear and shear at the adhesion boundary continues until fracture occur, resulting in adhesive transfer.","sentences":["The galling mechanism of Tristelle 5183, an Fe-based hardfacing alloy, was investigated at elevated temperature.","The test was performed using a bespoke galling rig.","Adhesive transfer and galling were found to occur, as a result of shear at the adhesion boundary and the activation of an internal shear plane within one of the tribosurfaces.","During deformation, carbides were observed to have fractured, as a result of the shear train they were exposed to and their lack of ductility.","In the case of niobium carbides, their fracture resulted in the formation of voids, which were found to coalesce and led to cracking and adhesive transfer.","A tribologically affected zone (TAZ) was found to form, which contained nanocrystalline austenite, as a result of the shear exerted within 30{\\mu}m of the adhesion boundaries.","The galling of Tristelle 5183 initiated from the formation of an adhesive boundary, followed by sub-surface shear in only one tribosurface, Following further sub-surface shear, an internal shear plane is activated.","internal shear and shear at the adhesion boundary continues until fracture occur, resulting in adhesive transfer."],"url":"http://arxiv.org/abs/2403.07730v1","category":"physics.app-ph"}
{"created":"2024-03-12 14:48:44","title":"Categorizing $SU(3)_f$ representations of scalar mesons by $J/\u03c8$ decays","abstract":"We explore the possibilities of categorizing $SU(3)_f$ representations of scalar mesons through $J/\\psi\\to SV$ and $\\gamma S$, with $S$ ($V$) being the scalar(vector) mesons. We find that $f_0(500)$ and $f_0(980)$ are singlet and octet states, respectively; which both belong to a nonet of the $SU(3)_f$ flavor symmetry. In addition, we determine the singlet-octet mixing angle of $\\theta = (84.2\\pm13.9)^{\\circ}$ between $f_0(500)$ and $f_0(980)$, which supports the quark-antiquark ($q\\bar{q}$) hypothesis. For the scalar mesons in the range of 1-2 GeV, containing two of $f_0(1370,\\ 1500,\\ 1700)$, we discuss the mixings between $q\\bar{q}$ and glueballs. Our numerical results suggest that $f_0(1370 (1500))$ has the a significant component of $n\\bar{n}$ ($s\\bar{s}$), while $f_0(1710)$ is likely composed of the scalar glueball.","sentences":["We explore the possibilities of categorizing $SU(3)_f$ representations of scalar mesons through $J/\\psi\\to SV$ and $\\gamma S$, with $S$ ($V$) being the scalar(vector) mesons.","We find that $f_0(500)$ and $f_0(980)$ are singlet and octet states, respectively; which both belong to a nonet of the $SU(3)_f$ flavor symmetry.","In addition, we determine the singlet-octet mixing angle of $\\theta = (84.2\\pm13.9)^{\\circ}$ between $f_0(500)$ and $f_0(980)$, which supports the quark-antiquark ($q\\bar{q}$) hypothesis.","For the scalar mesons in the range of 1-2 GeV, containing two of $f_0(1370,\\ 1500,\\ 1700)$, we discuss the mixings between $q\\bar{q}$ and glueballs.","Our numerical results suggest that $f_0(1370 (1500))$ has the a significant component of $n\\bar{n}$ ($s\\bar{s}$), while $f_0(1710)$ is likely composed of the scalar glueball."],"url":"http://arxiv.org/abs/2403.07701v1","category":"hep-ph"}
{"created":"2024-03-12 14:05:29","title":"Characterizing the diffuse continuum excitations in the classical spin liquid $h$-YMnO$_3$","abstract":"We extend previous inelastic neutron scattering results on the geometrically frustrated antiferromagnet hexagonal-YMnO$_3$, which has been suggested to belong to the class of classical spin liquids. We extend the energy transfer coverage of the diffuse signal up to 6.9 meV within a wide temperature range around the ordering temperature, $T_\\mathrm{N}$. The two distinct diffuse signals in the a-b plane, the signal localized at $\\Gamma$' and the scattering intensity connecting $\\Gamma$' points over the M', are shown to be only weakly energy dependent. In addition, an external magnetic field of up to 10.5 T applied along c is shown to have no effect on the diffuse signal. In the orthogonal scattering plane, the signals are shown to be dependent on l only through the magnetic form factor, showing that the correlations are purely two-dimensional, and supporting its origin to be the frustrated Mn$^{3+}$ triangles. This result is corroborated by atomistic spin dynamics simulations showing similar scattering vector and temperature behaviours. Lastly, data for the spin wave scattering in the (h, 0, l) plane allow for a discussion of the magnetic ground state where better agreement is found between the data and an ordered structure of the $\\Gamma_1$ or $\\Gamma_3$ symmetry, albeit crystal electric field arguments dismisses the $\\Gamma_1$ as possibility.","sentences":["We extend previous inelastic neutron scattering results on the geometrically frustrated antiferromagnet hexagonal-YMnO$_3$, which has been suggested to belong to the class of classical spin liquids.","We extend the energy transfer coverage of the diffuse signal up to 6.9 meV within a wide temperature range around the ordering temperature, $T_\\mathrm{N}$. The two distinct diffuse signals in the a-b plane, the signal localized at $\\Gamma$' and the scattering intensity connecting $\\Gamma$' points over the M', are shown to be only weakly energy dependent.","In addition, an external magnetic field of up to 10.5 T applied along c is shown to have no effect on the diffuse signal.","In the orthogonal scattering plane, the signals are shown to be dependent on l only through the magnetic form factor, showing that the correlations are purely two-dimensional, and supporting its origin to be the frustrated Mn$^{3+}$ triangles.","This result is corroborated by atomistic spin dynamics simulations showing similar scattering vector and temperature behaviours.","Lastly, data for the spin wave scattering in the (h, 0, l) plane allow for a discussion of the magnetic ground state where better agreement is found between the data and an ordered structure of the $\\Gamma_1$ or $\\Gamma_3$ symmetry, albeit crystal electric field arguments dismisses the $\\Gamma_1$ as possibility."],"url":"http://arxiv.org/abs/2403.07671v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 13:54:44","title":"Superconformal indices and localization in $N=2B$ quantum mechanics","abstract":"Superconformal `type B' quantum mechanical sigma models arise in a variety of interesting contexts, such as the description of D-brane bound states in an AdS$_2$ decoupling limit. Focusing on $N=2B$ models, we study superconformal indices which count short multiplets and provide an alternative to the standard Witten index, as the latter suffers from infrared issues. We show that the basic index receives contributions from lowest Landau level states in an effective magnetic field and that, due to the noncompactness of the target space, it is typically divergent. Fortunately, the models of interest possess an additional target space isometry which allows for the definition of a well-behaved refined index. We compute this index using localization of the functional integral and find that the result agrees with a naive application of the Atiyah-Bott fixed point formula outside of it's starting assumptions. In the simplest examples, this formula can also be directly verified by explicitly computing the short multiplet spectrum.","sentences":["Superconformal `type B' quantum mechanical sigma models arise in a variety of interesting contexts, such as the description of D-brane bound states in an AdS$_2$ decoupling limit.","Focusing on $N=2B$ models, we study superconformal indices which count short multiplets and provide an alternative to the standard Witten index, as the latter suffers from infrared issues.","We show that the basic index receives contributions from lowest Landau level states in an effective magnetic field and that, due to the noncompactness of the target space, it is typically divergent.","Fortunately, the models of interest possess an additional target space isometry which allows for the definition of a well-behaved refined index.","We compute this index using localization of the functional integral and find that the result agrees with a naive application of the Atiyah-Bott fixed point formula outside of it's starting assumptions.","In the simplest examples, this formula can also be directly verified by explicitly computing the short multiplet spectrum."],"url":"http://arxiv.org/abs/2403.07665v1","category":"hep-th"}
{"created":"2024-03-12 13:13:44","title":"AstroSat View of Transient Low-mass X-ray Binary XTE J1701-462: Spectral and Temporal Evolution along the Z-track","abstract":"AstroSat observed transient neutron star low-mass X-ray binary XTE J1701-462 for a total duration of 135 ks during its 2022 outburst. The source traced a complete 'Z' shaped structure in the hardness intensity diagram (HID) during the observation. The source exhibited an extended horizontal branch and a short-dipping flaring branch in the HID. We find that most suitable spectral model comprises emission from a standard multi-color accretion disk and Comptonized radiation from a hot central corona. The observed disk component is cool having temperature in the range of 0.3-0.4 keV and truncated far ( ~100 - 520 km) from the compact object. The Compton corona has optical depth in the range of 3.3-5 and temperature in the range of 3.3-4.9 keV. The disk and corona flux as well as truncation radius varies significantly along the HID. We discuss the possible scenarios to explain the relationship between spectral evolution and motion of the source along the HID. The timing analysis revealed horizontal branch oscillations (HBOs) in the frequency range 34-60 Hz. The frequency and rms strength of HBO vary systematically as the source moves along the horizontal branch (HB). The observed correlation of HBO properties with the position on the HB is similar to that reported previously in this source using \\textit{RXTE} data during the 2006 outburst of the source. The source also showed NBOs with frequency $\\sim$ 6.7 Hz in the middle and lower normal branch. The energy dependent study of the HBO properties suggests that the HBO is stronger in the higher energy band. We also observed very-low frequency noise (VLFN) and band limited noise (BLN) components in the power-density spectra. The break frequency of BLN component was found to be tightly correlated with the HBO frequency. We discuss possible models to explain the origin and nature of the observed features in the PDS.","sentences":["AstroSat observed transient neutron star low-mass X-ray binary XTE J1701-462 for a total duration of 135 ks during its 2022 outburst.","The source traced a complete 'Z' shaped structure in the hardness intensity diagram (HID) during the observation.","The source exhibited an extended horizontal branch and a short-dipping flaring branch in the HID.","We find that most suitable spectral model comprises emission from a standard multi-color accretion disk and Comptonized radiation from a hot central corona.","The observed disk component is cool having temperature in the range of 0.3-0.4 keV and truncated far ( ~100 - 520 km) from the compact object.","The Compton corona has optical depth in the range of 3.3-5 and temperature in the range of 3.3-4.9 keV. The disk and corona flux as well as truncation radius varies significantly along the HID.","We discuss the possible scenarios to explain the relationship between spectral evolution and motion of the source along the HID.","The timing analysis revealed horizontal branch oscillations (HBOs) in the frequency range 34-60 Hz.","The frequency and rms strength of HBO vary systematically as the source moves along the horizontal branch (HB).","The observed correlation of HBO properties with the position on the HB is similar to that reported previously in this source using \\textit{RXTE} data during the 2006 outburst of the source.","The source also showed NBOs with frequency $\\sim$ 6.7 Hz in the middle and lower normal branch.","The energy dependent study of the HBO properties suggests that the HBO is stronger in the higher energy band.","We also observed very-low frequency noise (VLFN) and band limited noise (BLN) components in the power-density spectra.","The break frequency of BLN component was found to be tightly correlated with the HBO frequency.","We discuss possible models to explain the origin and nature of the observed features in the PDS."],"url":"http://arxiv.org/abs/2403.07634v1","category":"astro-ph.HE"}
{"created":"2024-03-12 13:06:57","title":"Identifying a point-symmetric morphology in supernova remnant Cassiopeia A: explosion by jittering jets","abstract":"We identify a point-symmetric morphology of the supernova remnant (SNR) Cassiopeia A compatible with shaping by at least two, and more likely more than four, pairs of opposite jets, as expected in the jittering jets explosion mechanism (JJEM) of core-collapse supernovae. Using an old X-ray map of argon, we identify seven pairs of opposite morphological features that we connect with lines that cross each other at the same point on the plane of the sky. The opposite morphological features include protrusions, clumps, filaments, and funnels in the main SNR shell. In addition to these seven symmetry axes, we find two tentative symmetry axes (lines). These lines form a point-symmetric wind-rose. We place this point-symmetric wind-rose on a new JWST and X-ray images of Cassiopeia A. We find other morphological features and one more symmetry axis that strengthen the identified point-symmetric morphology. Not all symmetry axes correspond to jets; e.g., some clumps are formed by the compression of ejecta between two jet-inflated lobes (bubbles). The robust point-symmetric morphology in the iconic Cassiopeia A SNR strongly supports the JJEM and poses a severe challenge to the neutrino-driven explosion mechanism.","sentences":["We identify a point-symmetric morphology of the supernova remnant (SNR) Cassiopeia","A compatible with shaping by at least two, and more likely more than four, pairs of opposite jets, as expected in the jittering jets explosion mechanism (JJEM) of core-collapse supernovae.","Using an old X-ray map of argon, we identify seven pairs of opposite morphological features that we connect with lines that cross each other at the same point on the plane of the sky.","The opposite morphological features include protrusions, clumps, filaments, and funnels in the main SNR shell.","In addition to these seven symmetry axes, we find two tentative symmetry axes (lines).","These lines form a point-symmetric wind-rose.","We place this point-symmetric wind-rose on a new JWST and X-ray images of Cassiopeia A.","We find other morphological features and one more symmetry axis that strengthen the identified point-symmetric morphology.","Not all symmetry axes correspond to jets; e.g., some clumps are formed by the compression of ejecta between two jet-inflated lobes (bubbles).","The robust point-symmetric morphology in the iconic Cassiopeia A SNR strongly supports the JJEM and poses a severe challenge to the neutrino-driven explosion mechanism."],"url":"http://arxiv.org/abs/2403.07625v1","category":"astro-ph.HE"}
{"created":"2024-03-12 12:47:41","title":"Lattice dynamics of altermagnetic ruthenium oxide RuO$_{2}$","abstract":"Altermagnetic ruthenium oxide RuO$_{2}$ crystallizes with P4$_{2}$/mnm symmetry. Here we discuss the lattice dynamics of this structure. We show and discuss the phonon dispersion and density of states. The phonon dispersion curves contain several Dirac nodal lines and highly degenerate Dirac points. We present the characteristic frequencies and their irreducible representations at the $\\Gamma$ point. Theoretically obtained frequencies of the Raman active modes nicely reproduce the ones reported experimentally.","sentences":["Altermagnetic ruthenium oxide RuO$_{2}$ crystallizes with P4$_{2}$/mnm symmetry.","Here we discuss the lattice dynamics of this structure.","We show and discuss the phonon dispersion and density of states.","The phonon dispersion curves contain several Dirac nodal lines and highly degenerate Dirac points.","We present the characteristic frequencies and their irreducible representations at the $\\Gamma$ point.","Theoretically obtained frequencies of the Raman active modes nicely reproduce the ones reported experimentally."],"url":"http://arxiv.org/abs/2403.07609v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 11:31:22","title":"Gauss-Bonnet $\\boldsymbol{AdS}$ planar and spherical black hole thermodynamics and holography","abstract":"In this work, we extend the study in \\cite{Bilic:2022psx} incorporating the $AdS$/CFT duality to establish a relationship between the local temperatures of a large ($AdS$) spherical and a ($AdS$) planar Schwarzschild black hole near the $AdS$ boundary considering Gauss-Bonnet curvature correction in the gravitational action. We have shown the finite coupling corrections appear in the local temperature relationships due to the inclusion of Gauss-Bonnet term in the bulk. By transforming the metric into Fefferman-Graham form, we have calculated the energy of the conformal fluid at the boundary. Following the results of fluid/gravity duality, the energy of the conformal fluid at the boundary is then compared with the black body radiation energy which eventually leads us to establish the local temperature relationship between spherical and planar black holes in Gauss-Bonnet gravity near the $AdS$ boundary.","sentences":["In this work, we extend the study in \\cite{Bilic:2022psx} incorporating the $AdS$/CFT duality to establish a relationship between the local temperatures of a large ($AdS$) spherical and a ($AdS$) planar Schwarzschild black hole near the $AdS$ boundary considering Gauss-Bonnet curvature correction in the gravitational action.","We have shown the finite coupling corrections appear in the local temperature relationships due to the inclusion of Gauss-Bonnet term in the bulk.","By transforming the metric into Fefferman-Graham form, we have calculated the energy of the conformal fluid at the boundary.","Following the results of fluid/gravity duality, the energy of the conformal fluid at the boundary is then compared with the black body radiation energy which eventually leads us to establish the local temperature relationship between spherical and planar black holes in Gauss-Bonnet gravity near the $AdS$ boundary."],"url":"http://arxiv.org/abs/2403.07543v1","category":"hep-th"}
{"created":"2024-03-12 11:24:53","title":"Validation of electrodeposited 241Am alpha-particle sources for use in liquified gas detectors at cryogenic temperatures","abstract":"This paper describes a procedure for the validation of alpha-particle sources (exempt unsealed sources) to be used in experimental setups with liquefied gases at cryogenic temperatures (down to -196 C) and high vacuum. These setups are of interest for the development and characterization of neutrino and dark matter detectors based on liquid argon, among others. Due to the high purity requirements, the sources have to withstand high vacuum and cryogenic temperatures for extended periods. The validation procedure has been applied to 241Am sources produced by electrodeposition.","sentences":["This paper describes a procedure for the validation of alpha-particle sources (exempt unsealed sources) to be used in experimental setups with liquefied gases at cryogenic temperatures (down to -196 C) and high vacuum.","These setups are of interest for the development and characterization of neutrino and dark matter detectors based on liquid argon, among others.","Due to the high purity requirements, the sources have to withstand high vacuum and cryogenic temperatures for extended periods.","The validation procedure has been applied to 241Am sources produced by electrodeposition."],"url":"http://arxiv.org/abs/2403.07539v1","category":"physics.ins-det"}
{"created":"2024-03-12 11:05:39","title":"Enhanced Monochromatic Photon Emission from Millicharged Co-Interacting Dark Matter","abstract":"We study a millicharged co-interacting dark matter scenario, where the primary dark matter constituent is the dark photon $A'$ and the secondary component is the fermion $\\chi$. In this model, $\\chi$ interacts with $A'$ via a $U(1)'$ interaction while being millicharged with respect to normal photons. Our investigation focuses on the oscillation of $A'$ dark matter into photons within the background of $\\chi$ particles, revealing that the $A'-\\chi$ scattering rate benefits from a Bose enhancement of the $A'$ final state. As the oscillation production rate is directly linked to the scattering rate, the conversion of $A'$ dark matter into monochromatic photons experiences significant amplification owing to this Bose enhancement, especially when the scattering rate $\\Gamma_{\\rm sca}$ approaches the dark photon mass $m_{A'}$. These converted monochromatic photons are detectable through radio telescopes and can induce distortions in the Cosmic Microwave Background (CMB) spectrum. We find that the sensitivity of radio telescopes and the constraints imposed by CMB distortion on the kinetic mixing parameter are notably heightened compared to scenarios without the subdominant millicharged dark matter.","sentences":["We study a millicharged co-interacting dark matter scenario, where the primary dark matter constituent is the dark photon $A'$ and the secondary component is the fermion $\\chi$. In this model, $\\chi$ interacts with $A'$ via a $U(1)'$ interaction while being millicharged with respect to normal photons.","Our investigation focuses on the oscillation of $A'$ dark matter into photons within the background of $\\chi$ particles, revealing that the $A'-\\chi$ scattering rate benefits from a Bose enhancement of the $A'$ final state.","As the oscillation production rate is directly linked to the scattering rate, the conversion of $A'$ dark matter into monochromatic photons experiences significant amplification owing to this Bose enhancement, especially when the scattering rate $\\Gamma_{\\rm sca}$ approaches the dark photon mass $m_{A'}$. These converted monochromatic photons are detectable through radio telescopes and can induce distortions in the Cosmic Microwave Background (CMB) spectrum.","We find that the sensitivity of radio telescopes and the constraints imposed by CMB distortion on the kinetic mixing parameter are notably heightened compared to scenarios without the subdominant millicharged dark matter."],"url":"http://arxiv.org/abs/2403.07528v1","category":"hep-ph"}
{"created":"2024-03-12 10:37:43","title":"The low-mass enhancement of kaon pairs in $B^+\\to\\bar{D}^{(*)0}K^+\\bar{K}^0$ and $B^0\\to D^{(*)-}K^+\\bar{K}^0$ decays","abstract":"Very recently, the Belle~II collaboration presented a measurement for the decays $B^+\\to\\bar{D}^{(*)0} K^+\\bar{K}^0$ and $B^0\\to D^{(*)-}K^+\\bar{K}^0$, the bulk of observed $m(K^+ K_S^0)$ distributions showing low-mass structures in all four channels. In this work, we study the contributions of $\\rho(770,1450)^+$, $a_2(1320)^+$ and $a_0(980,1450)^+$ resonances to these decay processes. The intermediate states $\\rho(770,1450)^+$ are found to dominate the low-mass distribution of kaon pairs roughly contributing to half of the total branching fraction in each of the four decay channels. The contribution of the tensor $a_2(1320)^+$ meson is found to be negligible. Near the threshold of the kaon pair, the state $a_0(980)^+$ turns out to be much less important than expected, not being able to account for the enhancement of events in that energy region observed in the $B^+\\to\\bar{D}^{(*)0} K^+\\bar{K}^0$ decays. Further studies both from the theoretical and experimental sides are needed to elucidate the role of the non-resonant contributions governing the formation of $K^+\\bar{K}^0$ pairs near their threshold in these decay processes.","sentences":["Very recently, the Belle~II collaboration presented a measurement for the decays $B^+\\to\\bar{D}^{(*)0} K^+\\bar{K}^0$ and $B^0\\to D^{(*)-}K^+\\bar{K}^0$, the bulk of observed $m(K^+ K_S^0)$ distributions showing low-mass structures in all four channels.","In this work, we study the contributions of $\\rho(770,1450)^+$, $a_2(1320)^+$ and $a_0(980,1450)^+$ resonances to these decay processes.","The intermediate states $\\rho(770,1450)^+$ are found to dominate the low-mass distribution of kaon pairs roughly contributing to half of the total branching fraction in each of the four decay channels.","The contribution of the tensor $a_2(1320)^+$ meson is found to be negligible.","Near the threshold of the kaon pair, the state $a_0(980)^+$ turns out to be much less important than expected, not being able to account for the enhancement of events in that energy region observed in the $B^+\\to\\bar{D}^{(*)0} K^+\\bar{K}^0$ decays.","Further studies both from the theoretical and experimental sides are needed to elucidate the role of the non-resonant contributions governing the formation of $K^+\\bar{K}^0$ pairs near their threshold in these decay processes."],"url":"http://arxiv.org/abs/2403.07499v1","category":"hep-ph"}
{"created":"2024-03-12 10:37:00","title":"Reheated Sub-40000 Kelvin Neutron Stars at the JWST, ELT, and TMT","abstract":"Neutron stars cooling passively since their birth may be reheated in their late-stage evolution by a number of possible phenomena: rotochemical, vortex creep, crust cracking, magnetic field decay, or more exotic processes such as removal of neutrons from their Fermi seas (the nucleon Auger effect), baryon number-violating nucleon decay, and accretion of particle dark matter. Using Exposure Time Calculator tools, we show that reheating mechanisms imparting effective temperatures of 2000--40000 Kelvin may be uncovered with excellent sensitivities at the James Webb Space Telescope (JWST), the Extremely Large Telescope (ELT), and the Thirty Meter Telescope (TMT), with imaging instruments operating from visible-edge to near-infrared. With a day of exposure, they could constrain the reheating luminosity of a neutron star up to a distance of 500 pc, within which about $10^5$ (undiscovered) neutron stars lie. Detection in multiple filters could overconstrain a neutron star's surface temperature, distance from Earth, mass, and radius. Using publicly available catalogues of newly discovered pulsars at the FAST and CHIME radio telescopes and the Galactic electron distribution models YMW16 and NE2001, we estimate the pulsars' dispersion measure distance from Earth, and find that potentially 30$-$40 of these may be inspected for late-stage reheating within viable exposure times, in addition to a few hundred candidates already present in the ATNF catalogue. Whereas the coldest neutron star observed (PSR J2144$-$3933) has an upper limit on its effective temperature of about 33000 Kelvin with the Hubble Space Telescope, we show that the effective temperature may be constrained down to 20000 Kelvin with JWST-NIRCam, 15000 Kelvin at ELT-MICADO, and 9000 Kelvin with TMT-IRIS. Campaigns to measure thermal luminosities of old neutron stars would be transformative for astrophysics and fundamental physics.","sentences":["Neutron stars cooling passively since their birth may be reheated in their late-stage evolution by a number of possible phenomena: rotochemical, vortex creep, crust cracking, magnetic field decay, or more exotic processes such as removal of neutrons from their Fermi seas (the nucleon Auger effect), baryon number-violating nucleon decay, and accretion of particle dark matter.","Using Exposure Time Calculator tools, we show that reheating mechanisms imparting effective temperatures of 2000--40000 Kelvin may be uncovered with excellent sensitivities at the James Webb Space Telescope (JWST), the Extremely Large Telescope (ELT), and the Thirty Meter Telescope (TMT), with imaging instruments operating from visible-edge to near-infrared.","With a day of exposure, they could constrain the reheating luminosity of a neutron star up to a distance of 500 pc, within which about $10^5$ (undiscovered) neutron stars lie.","Detection in multiple filters could overconstrain a neutron star's surface temperature, distance from Earth, mass, and radius.","Using publicly available catalogues of newly discovered pulsars at the FAST and CHIME radio telescopes and the Galactic electron distribution models YMW16 and NE2001, we estimate the pulsars' dispersion measure distance from Earth, and find that potentially 30$-$40 of these may be inspected for late-stage reheating within viable exposure times, in addition to a few hundred candidates already present in the ATNF catalogue.","Whereas the coldest neutron star observed (PSR J2144$-$3933) has an upper limit on its effective temperature of about 33000 Kelvin with the Hubble Space Telescope, we show that the effective temperature may be constrained down to 20000 Kelvin with JWST-NIRCam, 15000 Kelvin at ELT-MICADO, and 9000 Kelvin with TMT-IRIS.","Campaigns to measure thermal luminosities of old neutron stars would be transformative for astrophysics and fundamental physics."],"url":"http://arxiv.org/abs/2403.07496v1","category":"astro-ph.HE"}
{"created":"2024-03-12 10:03:43","title":"Holographic spin alignment of $J/\u03c8$ meson in magnetized plasma","abstract":"We study the mass spectra and spin alignment of vector meson $J/\\psi$ in a thermal magnetized background using the gauge/gravity duality. Utilizing a soft wall model for the QGP background and a massive vector field for the $J/\\psi$ meson, we delve into the meson's spectral function and spin parameters $(\\lambda_{\\theta},\\, \\lambda_\\varphi,\\,\\lambda_{\\theta\\varphi})$ for different cases, assessing their response to variations in magnetic field strength, momentum, and temperature. We initially examine scenarios where a meson's momentum aligns parallel to the magnetic field in helicity frame. Our results reveal a magnetic field-induced positive $\\lambda_\\theta^\\text{H}$ for low meson momentum, transitioning to negative with increased momentum. As a comparison, we also study the case of momentum perpendicular to the magnetic field and find the direction of magnetic field does not affect the qualitative behavior for the $eB$-dependence of $\\lambda_\\theta^\\text{H}$. Moreover, we apply our model to real heavy-ion collisions for three different spin quantization directions. Further comparisons with experimental data show qualitative agreement for spin parameters $\\lambda_{\\theta}$ and $\\lambda_\\varphi$ in the helicity and Collins-Soper frames.","sentences":["We study the mass spectra and spin alignment of vector meson $J/\\psi$ in a thermal magnetized background using the gauge/gravity duality.","Utilizing a soft wall model for the QGP background and a massive vector field for the $J/\\psi$ meson, we delve into the meson's spectral function and spin parameters $(\\lambda_{\\theta},\\, \\lambda_\\varphi,\\,\\lambda_{\\theta\\varphi})$ for different cases, assessing their response to variations in magnetic field strength, momentum, and temperature.","We initially examine scenarios where a meson's momentum aligns parallel to the magnetic field in helicity frame.","Our results reveal a magnetic field-induced positive $\\lambda_\\theta^\\text{H}$ for low meson momentum, transitioning to negative with increased momentum.","As a comparison, we also study the case of momentum perpendicular to the magnetic field and find the direction of magnetic field does not affect the qualitative behavior for the $eB$-dependence of $\\lambda_\\theta^\\text{H}$. Moreover, we apply our model to real heavy-ion collisions for three different spin quantization directions.","Further comparisons with experimental data show qualitative agreement for spin parameters $\\lambda_{\\theta}$ and $\\lambda_\\varphi$ in the helicity and Collins-Soper frames."],"url":"http://arxiv.org/abs/2403.07468v1","category":"hep-ph"}
{"created":"2024-03-12 09:46:03","title":"A compact approach to higher-resolution resonant inelastic X-ray scattering detection using photoelectrons","abstract":"The detection of inelastically scattered soft X-rays with high energy resolution usually requires large grating spectrometers. Recently, photoelectron spectrometry for analysis of X-rays (PAX) has been rediscovered for modern spectroscopy experiments at synchrotron light sources. By converting scattered photons to electrons and using an electron energy analyser, the energy resolution for resonant inelastic X-ray scattering (RIXS) becomes decoupled from the X-ray spot size and instrument length. In this work, we develop PAX towards high energy resolution using a modern photoemission spectroscopy setup studying Ba2Cu3O4Cl2 at the Cu L3-edge. We measure a momentum transfer range of 24% of the first Brillouin zone simultaneously. Our results hint at the observation of a magnon excitation below 100 meV energy transfer and show intensity variations related to the dispersion of dd-excitations. With dedicated setups, PAX can become an alternative to the best and largest RIXS instruments, while at the same time opening new opportunities to acquire RIXS at a range of momentum transfers simultaneously and combine it with angle-resolved photoemission spectroscopy in a single instrument.","sentences":["The detection of inelastically scattered soft X-rays with high energy resolution usually requires large grating spectrometers.","Recently, photoelectron spectrometry for analysis of X-rays (PAX) has been rediscovered for modern spectroscopy experiments at synchrotron light sources.","By converting scattered photons to electrons and using an electron energy analyser, the energy resolution for resonant inelastic X-ray scattering (RIXS) becomes decoupled from the X-ray spot size and instrument length.","In this work, we develop PAX towards high energy resolution using a modern photoemission spectroscopy setup studying Ba2Cu3O4Cl2 at the Cu L3-edge.","We measure a momentum transfer range of 24% of the first Brillouin zone simultaneously.","Our results hint at the observation of a magnon excitation below 100 meV energy transfer and show intensity variations related to the dispersion of dd-excitations.","With dedicated setups, PAX can become an alternative to the best and largest RIXS instruments, while at the same time opening new opportunities to acquire RIXS at a range of momentum transfers simultaneously and combine it with angle-resolved photoemission spectroscopy in a single instrument."],"url":"http://arxiv.org/abs/2403.07452v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 09:32:29","title":"Exploring the Nuclear Shape Phase Transition in Ultra-Relativistic $^{129}$Xe+$^{129}$Xe Collisions at the LHC","abstract":"The shape phase transition for certain isotope or isotone chains, associated with the quantum phase transition of finite nuclei, is an intriguing phenomenon in nuclear physics. A notable case is the Xe isotope chain, where the structure transits from a $\\gamma$-soft rotor to a spherical vibrator, with the second-order shape phase transition occurring in the vicinity of $^{128-130}$Xe. In this letter, we focus on investigating the $\\gamma$-soft deformation of $^{129}$Xe associated with the second-order shape phase transition by constructing novel correlators for ultra-relativistic $^{129}$Xe+$^{129}$Xe collisions. In particular, our iEBE-VISHNU model calculations show that the $v_2^2-[p_T]$ correlation $\\rho_{2}$ and the mean transverse momentum fluctuation $\\Gamma_{p_T}$, which were previously interpreted as the evidence for the rigid triaxial deformation of $^{129}$Xe, can also be well explained by the $\\gamma$-soft deformation of $^{129}$Xe. We also propose two novel correlators $\\rho_{4,2}$ and $\\rho_{2,4}$, which carry non-trivial higher-order correlations and show unique capabilities to distinguish between the $\\gamma$-soft and the rigid triaxial deformation of $^{129}$Xe in $^{129}$Xe+$^{129}$Xe collisions at the LHC. The present study also provides a novel way to explore the second-order shape phase transition of finite nuclei with ultra-relativistic heavy ion collisions.","sentences":["The shape phase transition for certain isotope or isotone chains, associated with the quantum phase transition of finite nuclei, is an intriguing phenomenon in nuclear physics.","A notable case is the Xe isotope chain, where the structure transits from a $\\gamma$-soft rotor to a spherical vibrator, with the second-order shape phase transition occurring in the vicinity of $^{128-130}$Xe.","In this letter, we focus on investigating the $\\gamma$-soft deformation of $^{129}$Xe associated with the second-order shape phase transition by constructing novel correlators for ultra-relativistic $^{129}$Xe+$^{129}$Xe collisions.","In particular, our iEBE-VISHNU model calculations show that the $v_2^2-[p_T]$ correlation $\\rho_{2}$ and the mean transverse momentum fluctuation $\\Gamma_{p_T}$, which were previously interpreted as the evidence for the rigid triaxial deformation of $^{129}$Xe, can also be well explained by the $\\gamma$-soft deformation of $^{129}$Xe.","We also propose two novel correlators $\\rho_{4,2}$ and $\\rho_{2,4}$, which carry non-trivial higher-order correlations and show unique capabilities to distinguish between the $\\gamma$-soft and the rigid triaxial deformation of $^{129}$Xe in $^{129}$Xe+$^{129}$Xe collisions at the LHC.","The present study also provides a novel way to explore the second-order shape phase transition of finite nuclei with ultra-relativistic heavy ion collisions."],"url":"http://arxiv.org/abs/2403.07441v1","category":"nucl-th"}
{"created":"2024-03-12 09:13:50","title":"Spatially oscillating correlation functions in $\\left(2+1\\right)$-dimensional four-fermion models: The mixing of scalar and vector modes at finite density","abstract":"In this work, we demonstrate that the mixing of scalar and vector condensates produces spatially oscillating, but exponentially damped correlation functions in fermionic theories at finite density and temperature. We find a regime exhibiting this oscillatory behavior in a Gross-Neveu-type model that also features vector interactions within the mean-field approximation. The existence of this regime aligns with expectations based on symmetry arguments, that are also applicable to QCD at finite baryon density. We compute the phase diagram including both homogeneous phases and regions with spatially oscillating, exponentially damped correlation functions at finite temperature and chemical potential for different strengths of the vector coupling. Furthermore, we find that inhomogeneous condensates are disfavored compared to homogeneous ones akin to previous findings without vector interactions. We show that our results are valid for a broad class of $\\left(2+1\\right)$-dimensional models with local four-fermion interactions.","sentences":["In this work, we demonstrate that the mixing of scalar and vector condensates produces spatially oscillating, but exponentially damped correlation functions in fermionic theories at finite density and temperature.","We find a regime exhibiting this oscillatory behavior in a Gross-Neveu-type model that also features vector interactions within the mean-field approximation.","The existence of this regime aligns with expectations based on symmetry arguments, that are also applicable to QCD at finite baryon density.","We compute the phase diagram including both homogeneous phases and regions with spatially oscillating, exponentially damped correlation functions at finite temperature and chemical potential for different strengths of the vector coupling.","Furthermore, we find that inhomogeneous condensates are disfavored compared to homogeneous ones akin to previous findings without vector interactions.","We show that our results are valid for a broad class of $\\left(2+1\\right)$-dimensional models with local four-fermion interactions."],"url":"http://arxiv.org/abs/2403.07430v1","category":"hep-ph"}
{"created":"2024-03-12 08:59:37","title":"Persistent Upflows and Downflows at Active Region boundaries Observed by SUTRI and AIA","abstract":"Upflows and downflows at active region (AR) boundaries have been frequently observed with spectroscopic observations at extreme ultraviolet (EUV) passbands. In this paper, we report the coexistence of upflows and downflows at the AR boundaries with imaging observations from the Solar Upper Transition Region Imager (SUTRI) and the Atmospheric Imaging Assembly (AIA). With their observations from 2022 September 21 to 2022 September 30, we find 17 persistent opposite flows occurring along the AR coronal loops. The upflows are prominent in the AIA 193 \\AA images with a velocity of 50-200 km/s, while the downflows are best seen in the SUTRI 465 \\AA and AIA 131 \\AA images with a slower velocity of tens of kilometers per second (characteristic temperatures (log T(K)) for 193 \\AA, 465 \\AA and 131 \\AA are 6.2, 5.7, 5.6, respectively). We also analyze the center-to-limb variation of the velocities for both upflows and downflows. The simultaneous observations of downflows and upflows can be explained by the chromosphere-corona mass-cycling process, in which the localized chromospheric plasma is impulsively heated to coronal temperature forming a upflow and then these upflows experience radiative cooling producing a downflow with the previously heated plasma returning to the lower atmosphere. In particular, the persistent downflows seen by SUTRI provide strong evidence of the cooling process in the mass cycle. For upflows associated with open loops, part of the plasma is able to escape outward and into the heliosphere as solar wind.","sentences":["Upflows and downflows at active region (AR) boundaries have been frequently observed with spectroscopic observations at extreme ultraviolet (EUV) passbands.","In this paper, we report the coexistence of upflows and downflows at the AR boundaries with imaging observations from the Solar Upper Transition Region Imager (SUTRI) and the Atmospheric Imaging Assembly (AIA).","With their observations from 2022 September 21 to 2022 September 30, we find 17 persistent opposite flows occurring along the AR coronal loops.","The upflows are prominent in the AIA 193 \\AA images with a velocity of 50-200 km/s, while the downflows are best seen in the SUTRI 465 \\AA and AIA 131 \\AA images with a slower velocity of tens of kilometers per second (characteristic temperatures (log T(K)) for 193 \\AA, 465 \\AA and 131 \\AA are 6.2, 5.7, 5.6, respectively).","We also analyze the center-to-limb variation of the velocities for both upflows and downflows.","The simultaneous observations of downflows and upflows can be explained by the chromosphere-corona mass-cycling process, in which the localized chromospheric plasma is impulsively heated to coronal temperature forming a upflow and then these upflows experience radiative cooling producing a downflow with the previously heated plasma returning to the lower atmosphere.","In particular, the persistent downflows seen by SUTRI provide strong evidence of the cooling process in the mass cycle.","For upflows associated with open loops, part of the plasma is able to escape outward and into the heliosphere as solar wind."],"url":"http://arxiv.org/abs/2403.07422v1","category":"astro-ph.SR"}
{"created":"2024-03-12 08:51:21","title":"Quenching and flow of charm and bottom quarks via semi-leptonic decay of $D$ and $B$ mesons in Pb+Pb collisions at the LHC","abstract":"Heavy flavor particles provide important probes of the microscopic structure and thermodynamic properties of the quark-gluon plasma (QGP) produced in high-energy nucleus-nucleus collisions. We study the energy loss and flow of charm and bottom quarks inside the QGP via the nuclear modification factor ($R_\\mathrm{AA}$) and elliptic flow coefficient ($v_2$) of their decayed leptons in heavy-ion collisions at the LHC. The dynamical evolution of the QGP is performed using the (3+1)-dimensional viscous hydrodynamics model CLVisc; the evolution of heavy quarks inside the QGP is simulated with our improved Langevin model that takes into account both collisional and radiative energy loss of heavy quarks; the hadronization of heavy quarks is simulated via our hybrid coalescence-fragmentation model; and the semi-leptonic decay of $D$ and $B$ mesons is simulated via PYTHIA. By using the same spatial diffusion coefficient for charm and bottom quarks, we obtain smaller $R_\\mathrm{AA}$ and larger $v_2$ of charm decayed leptons than bottom decayed leptons, indicating stronger energy loss of charm quarks than bottom quarks inside the QGP within our current model setup.","sentences":["Heavy flavor particles provide important probes of the microscopic structure and thermodynamic properties of the quark-gluon plasma (QGP) produced in high-energy nucleus-nucleus collisions.","We study the energy loss and flow of charm and bottom quarks inside the QGP via the nuclear modification factor ($R_\\mathrm{AA}$) and elliptic flow coefficient ($v_2$) of their decayed leptons in heavy-ion collisions at the LHC.","The dynamical evolution of the QGP is performed using the (3+1)-dimensional viscous hydrodynamics model CLVisc; the evolution of heavy quarks inside the QGP is simulated with our improved Langevin model that takes into account both collisional and radiative energy loss of heavy quarks; the hadronization of heavy quarks is simulated via our hybrid coalescence-fragmentation model; and the semi-leptonic decay of $D$ and $B$ mesons is simulated via PYTHIA.","By using the same spatial diffusion coefficient for charm and bottom quarks, we obtain smaller $R_\\mathrm{AA}$ and larger $v_2$ of charm decayed leptons than bottom decayed leptons, indicating stronger energy loss of charm quarks than bottom quarks inside the QGP within our current model setup."],"url":"http://arxiv.org/abs/2403.07419v1","category":"hep-ph"}
{"created":"2024-03-12 08:49:41","title":"$\u03bb$-shaped random matrices, $\u03bb$-plane trees, and $\u03bb$-Dyck paths","abstract":"We consider random matrices whose shape is the dilation $N\\lambda$ of a self-conjugate Young diagram $\\lambda$. In the large-$N$ limit, the empirical distribution of the squared singular values converges almost surely to a probability distribution $F^{\\lambda}$. The moments of $F^{\\lambda}$ enumerate two combinatorial objects: $\\lambda$-plane trees and $\\lambda$-Dyck paths, which we introduce and show to be in bijection. We also prove that the distribution $F^{\\lambda}$ is algebraic, in the sense of Rao and Edelman. In the case of fat hook shapes we provide explicit formulae for $F^{\\lambda}$ and we express it as a free convolution of two measures involving a Marchenko-Pastur and a Bernoulli distribution.","sentences":["We consider random matrices whose shape is the dilation $N\\lambda$ of a self-conjugate Young diagram $\\lambda$. In the large-$N$ limit, the empirical distribution of the squared singular values converges almost surely to a probability distribution $F^{\\lambda}$. The moments of $F^{\\lambda}$ enumerate two combinatorial objects: $\\lambda$-plane trees and $\\lambda$-Dyck paths, which we introduce and show to be in bijection.","We also prove that the distribution $F^{\\lambda}$ is algebraic, in the sense of Rao and Edelman.","In the case of fat hook shapes we provide explicit formulae for $F^{\\lambda}$ and we express it as a free convolution of two measures involving a Marchenko-Pastur and a Bernoulli distribution."],"url":"http://arxiv.org/abs/2403.07418v1","category":"math.PR"}
{"created":"2024-03-12 08:27:15","title":"Constraining the Initial Mass function in the Epoch of Reionization from Astrophysical and Cosmological data","abstract":"[abridged] We aim to constrain the stellar initial mass function (IMF) during the epoch of reionization. To this purpose, we build up a semi-empirical model for the reionization history of the Universe, based on various ingredients: the latest determination of the UV galaxy luminosity function from JWST out to redshift $z\\lesssim 12$; data-inferred and simulation-driven assumptions on the redshift-dependent escape fraction of ionizing photons from primordial galaxies; a simple yet flexible parameterization of the IMF $\\phi(m_\\star)\\sim m_\\star^\\xi\\, e^{-m_{\\star,\\rm c}/m_\\star}$ in terms of a high-mass end slope $\\xi<0$ and of a characteristic mass $m_{\\star,\\rm c}$ below which a flattening or a bending sets in; the PARSEC stellar evolution code to compute the UV and ionizing emission from different star's masses as a function of age and metallicity; a few physical constraints related to stellar and galaxy formation in faint galaxies at the reionization redshifts. We compare our model outcomes with the reionization observables from different astrophysical and cosmological probes, and perform Bayesian inference on the IMF parameters. We find that the IMF slope $\\xi$ is within the range from $-2.8$ to $-2.3$, while appreciably flatter slopes are excluded at great significance. However, the bestfit value of the IMF characteristic mass $m_{\\star,\\rm c}\\sim$ a few $M_\\odot$ implies a suppression in the formation of small stellar masses, at variance with the IMF in the local Universe; this may be induced by the thermal background $\\sim 20-30$ K provided by CMB photons at the reionization redshifts. Finally, we investigate the implications of our reconstructed IMF on the recent JWST detections of massive galaxies at and beyond the reionization epoch, showing that any putative tension with the standard cosmological framework is substantially alleviated.","sentences":["[abridged] We aim to constrain the stellar initial mass function (IMF) during the epoch of reionization.","To this purpose, we build up a semi-empirical model for the reionization history of the Universe, based on various ingredients: the latest determination of the UV galaxy luminosity function from JWST out to redshift $z\\lesssim 12$; data-inferred and simulation-driven assumptions on the redshift-dependent escape fraction of ionizing photons from primordial galaxies; a simple yet flexible parameterization of the IMF $\\phi(m_\\star)\\sim m_\\star^\\xi\\, e^{-m_{\\star,\\rm c}/m_\\star}$ in terms of a high-mass end slope $\\xi<0$ and of a characteristic mass $m_{\\star,\\rm c}$ below which a flattening or a bending sets in; the PARSEC stellar evolution code to compute the UV and ionizing emission from different star's masses as a function of age and metallicity; a few physical constraints related to stellar and galaxy formation in faint galaxies at the reionization redshifts.","We compare our model outcomes with the reionization observables from different astrophysical and cosmological probes, and perform Bayesian inference on the IMF parameters.","We find that the IMF slope $\\xi$ is within the range from $-2.8$ to $-2.3$, while appreciably flatter slopes are excluded at great significance.","However, the bestfit value of the IMF characteristic mass $m_{\\star,\\rm c}\\sim$ a few $M_\\odot$ implies a suppression in the formation of small stellar masses, at variance with the IMF in the local Universe; this may be induced by the thermal background $\\sim 20-30$ K provided by CMB photons at the reionization redshifts.","Finally, we investigate the implications of our reconstructed IMF on the recent JWST detections of massive galaxies at and beyond the reionization epoch, showing that any putative tension with the standard cosmological framework is substantially alleviated."],"url":"http://arxiv.org/abs/2403.07401v1","category":"astro-ph.GA"}
{"created":"2024-03-12 07:59:41","title":"ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions","abstract":"Although Vision Transformer (ViT) has achieved significant success in computer vision, it does not perform well in dense prediction tasks due to the lack of inner-patch information interaction and the limited diversity of feature scale. Most existing studies are devoted to designing vision-specific transformers to solve the above problems, which introduce additional pre-training costs. Therefore, we present a plain, pre-training-free, and feature-enhanced ViT backbone with Convolutional Multi-scale feature interaction, named ViT-CoMer, which facilitates bidirectional interaction between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has the following advantages: (1) We inject spatial pyramid multi-receptive field convolutional features into the ViT architecture, which effectively alleviates the problems of limited local information interaction and single-feature representation in ViT. (2) We propose a simple and efficient CNN-Transformer bidirectional fusion interaction module that performs multi-scale fusion across hierarchical features, which is beneficial for handling dense prediction tasks. (3) We evaluate the performance of ViT-CoMer across various dense prediction tasks, different frameworks, and multiple advanced pre-training. Notably, our ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and 62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art methods. We hope ViT-CoMer can serve as a new backbone for dense prediction tasks to facilitate future research. The code will be released at https://github.com/Traffic-X/ViT-CoMer.","sentences":["Although Vision Transformer (ViT) has achieved significant success in computer vision, it does not perform well in dense prediction tasks due to the lack of inner-patch information interaction and the limited diversity of feature scale.","Most existing studies are devoted to designing vision-specific transformers to solve the above problems, which introduce additional pre-training costs.","Therefore, we present a plain, pre-training-free, and feature-enhanced ViT backbone with Convolutional Multi-scale feature interaction, named ViT-CoMer, which facilitates bidirectional interaction between CNN and transformer.","Compared to the state-of-the-art, ViT-CoMer has the following advantages: (1) We inject spatial pyramid multi-receptive field convolutional features into the ViT architecture, which effectively alleviates the problems of limited local information interaction and single-feature representation in ViT. (2) We propose a simple and efficient CNN-Transformer bidirectional fusion interaction module that performs multi-scale fusion across hierarchical features, which is beneficial for handling dense prediction tasks.","(3) We evaluate the performance of ViT-CoMer across various dense prediction tasks, different frameworks, and multiple advanced pre-training.","Notably, our ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and 62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art methods.","We hope ViT-CoMer can serve as a new backbone for dense prediction tasks to facilitate future research.","The code will be released at https://github.com/Traffic-X/ViT-CoMer."],"url":"http://arxiv.org/abs/2403.07392v1","category":"cs.CV"}
{"created":"2024-03-12 07:21:54","title":"Free submonoids of hyperbolic monoids","abstract":"In this paper, we prove that infinite cancellative finitely generated hyperbolic monoids never contain $\\mathbb N\\times\\mathbb N$ as a submonoid but that they contain an element of infinite order and, if they are elementary, then they also contain a free monoid of rank at least 2. As a corollary we obtain that the latter have exponential growth. We prove these results by analysing the monoid of self-embeddings of hyperbolic digraphs and proving fixed-point theorems for them.","sentences":["In this paper, we prove that infinite cancellative finitely generated hyperbolic monoids never contain $\\mathbb N\\times\\mathbb N$ as a submonoid but that they contain an element of infinite order and, if they are elementary, then they also contain a free monoid of rank at least 2.","As a corollary we obtain that the latter have exponential growth.","We prove these results by analysing the monoid of self-embeddings of hyperbolic digraphs and proving fixed-point theorems for them."],"url":"http://arxiv.org/abs/2403.07374v1","category":"math.GR"}
{"created":"2024-03-12 07:16:20","title":"Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection","abstract":"Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared bird's-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals. Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature. In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset. The code is released at https://github.com/fjhzhixi/ECFusion.","sentences":["Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared bird's-eye view (BEV) representation space.","However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts.","These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals.","Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features.","Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion.","Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature.","In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity.","Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset.","The code is released at https://github.com/fjhzhixi/ECFusion."],"url":"http://arxiv.org/abs/2403.07372v1","category":"cs.CV"}
{"created":"2024-03-12 07:15:29","title":"Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models","abstract":"This study discusses the critical issues of Virtual Try-On in contemporary e-commerce and the prospective metaverse, emphasizing the challenges of preserving intricate texture details and distinctive features of the target person and the clothes in various scenarios, such as clothing texture and identity characteristics like tattoos or accessories. In addition to the fidelity of the synthesized images, the efficiency of the synthesis process presents a significant hurdle. Various existing approaches are explored, highlighting the limitations and unresolved aspects, e.g., identity information omission, uncontrollable artifacts, and low synthesis speed. It then proposes a novel diffusion-based solution that addresses garment texture preservation and user identity retention during virtual try-on. The proposed network comprises two primary modules - a warping module aligning clothing with individual features and a try-on module refining the attire and generating missing parts integrated with a mask-aware post-processing technique ensuring the integrity of the individual's identity. It demonstrates impressive results, surpassing the state-of-the-art in speed by nearly 20 times during inference, with superior fidelity in qualitative assessments. Quantitative evaluations confirm comparable performance with the recent SOTA method on the VITON-HD and Dresscode datasets.","sentences":["This study discusses the critical issues of Virtual Try-On in contemporary e-commerce and the prospective metaverse, emphasizing the challenges of preserving intricate texture details and distinctive features of the target person and the clothes in various scenarios, such as clothing texture and identity characteristics like tattoos or accessories.","In addition to the fidelity of the synthesized images, the efficiency of the synthesis process presents a significant hurdle.","Various existing approaches are explored, highlighting the limitations and unresolved aspects, e.g., identity information omission, uncontrollable artifacts, and low synthesis speed.","It then proposes a novel diffusion-based solution that addresses garment texture preservation and user identity retention during virtual try-on.","The proposed network comprises two primary modules - a warping module aligning clothing with individual features and a try-on module refining the attire and generating missing parts integrated with a mask-aware post-processing technique ensuring the integrity of the individual's identity.","It demonstrates impressive results, surpassing the state-of-the-art in speed by nearly 20 times during inference, with superior fidelity in qualitative assessments.","Quantitative evaluations confirm comparable performance with the recent SOTA method on the VITON-HD and Dresscode datasets."],"url":"http://arxiv.org/abs/2403.07371v1","category":"cs.CV"}
{"created":"2024-03-12 07:01:14","title":"On exactly solvable Yang-Baxter models and enhanced symmetries","abstract":"We study Yang-Baxter deformations of the flat space string that result in exactly solvable models, finding the Nappi-Witten model and its higher dimensional generalizations. We then consider the spectra of these models obtained by canonical quantization in light-cone gauge, and match them with an integrability-based Bethe ansatz approach. By considering a generalized light-cone gauge we can describe the model by a nontrivially Drinfel'd twisted S matrix, explicitly verifying the twisted structure expected for such deformations. Next, the reformulation of the Nappi-Witten model as a Yang-Baxter deformation shows that Yang-Baxter models can have more symmetries than suggested by the $r$ matrix defining the deformation. We discuss these enhanced symmetries in more detail for some trivial and nontrivial examples. Finally, we observe that there are nonunimodular but Weyl-invariant Yang-Baxter models of a type not previously considered.","sentences":["We study Yang-Baxter deformations of the flat space string that result in exactly solvable models, finding the Nappi-Witten model and its higher dimensional generalizations.","We then consider the spectra of these models obtained by canonical quantization in light-cone gauge, and match them with an integrability-based Bethe ansatz approach.","By considering a generalized light-cone gauge we can describe the model by a nontrivially Drinfel'd twisted S matrix, explicitly verifying the twisted structure expected for such deformations.","Next, the reformulation of the Nappi-Witten model as a Yang-Baxter deformation shows that Yang-Baxter models can have more symmetries than suggested by the $r$ matrix defining the deformation.","We discuss these enhanced symmetries in more detail for some trivial and nontrivial examples.","Finally, we observe that there are nonunimodular but Weyl-invariant Yang-Baxter models of a type not previously considered."],"url":"http://arxiv.org/abs/2403.07365v1","category":"hep-th"}
{"created":"2024-03-12 06:03:45","title":"Electronic Structure of Superconducting Infinite-Layer Lanthanum Nickelates","abstract":"Revealing the momentum-resolved electronic structure of infinite-layer nickelates is essential for understanding this new class of unconventional superconductors, but has been hindered by the formidable challenges in improving the sample quality. In this work, we report for the first time the angle-resolved photoemission spectroscopy of superconducting La$_{0.8}$Sr$_{0.2}$NiO$_{2}$ films prepared by molecular beam epitaxy and ${\\mathrm{\\textit{in situ}}}$ atomic-hydrogen reduction. The measured Fermi topology closely matches theoretical calculations, showing a large Ni-$d_{x^2-y^2}$ derived Fermi sheet that evolves from hole-like to electron-like along $k_{z}$, and a three-dimensional (3D) electron pocket centered at Brillouin zone corner. The Ni-$d_{x^2-y^2}$ derived bands show a mass enhancement ($m^*/m_{\\rm{DFT}}$) of 2-3,while the 3D electron band shows negligible band renormalization. Moreover, the Ni-$d_{x^2-y^2}$ derived states also display a band dispersion anomaly at higher binding energy, reminiscent of the waterfall feature and kinks observed in cuprates.","sentences":["Revealing the momentum-resolved electronic structure of infinite-layer nickelates is essential for understanding this new class of unconventional superconductors, but has been hindered by the formidable challenges in improving the sample quality.","In this work, we report for the first time the angle-resolved photoemission spectroscopy of superconducting La$_{0.8}$Sr$_{0.2}$NiO$_{2}$ films prepared by molecular beam epitaxy and ${\\mathrm{\\textit{in situ}}}$ atomic-hydrogen reduction.","The measured Fermi topology closely matches theoretical calculations, showing a large Ni-$d_{x^2-y^2}$ derived Fermi sheet that evolves from hole-like to electron-like along $k_{z}$, and a three-dimensional (3D) electron pocket centered at Brillouin zone corner.","The Ni-$d_{x^2-y^2}$ derived bands show a mass enhancement ($m^*/m_{\\rm{DFT}}$) of 2-3,while the 3D electron band shows negligible band renormalization.","Moreover, the Ni-$d_{x^2-y^2}$ derived states also display a band dispersion anomaly at higher binding energy, reminiscent of the waterfall feature and kinks observed in cuprates."],"url":"http://arxiv.org/abs/2403.07344v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 06:02:49","title":"Large fluctuations in the Sky","abstract":"Renormalization of quantum loop effects generated from large fluctuations is a hugely debatable topic of research these days which rules out the Primordial Black Hole (PBH) formation within the framework of single-field inflation. In this article, we briefly discuss that the correct implementation of regularization, renormalization, and resummation techniques in a setup described by an ultra-slow-roll phase sandwiched between two slow-roll phases in the presence of smooth or sharp transitions can lead to a stringent constraint on the PBH mass (i.e. ${\\cal O}(10^{2}{\\rm gm}$)), which we advertise as a new No-go theorem. Finally, we will give some of the possible way-outs using which one can evade this proposed No-go theorem and produce solar/sub-solar mass PBHs.","sentences":["Renormalization of quantum loop effects generated from large fluctuations is a hugely debatable topic of research these days which rules out the Primordial Black Hole (PBH) formation within the framework of single-field inflation.","In this article, we briefly discuss that the correct implementation of regularization, renormalization, and resummation techniques in a setup described by an ultra-slow-roll phase sandwiched between two slow-roll phases in the presence of smooth or sharp transitions can lead to a stringent constraint on the PBH mass (i.e. ${\\cal O}(10^{2}{\\rm gm}$)), which we advertise as a new No-go theorem.","Finally, we will give some of the possible way-outs using which one can evade this proposed No-go theorem and produce solar/sub-solar mass PBHs."],"url":"http://arxiv.org/abs/2403.07343v1","category":"astro-ph.CO"}
{"created":"2024-03-12 05:38:21","title":"Out-of-time-order correlator as a detector of baryonic phase structure in holographic QCD with a theta angle","abstract":"We study the out-of-time-order correlators (OTOC) of Skyrmion as baryon in the D0-D4/D8 model which is holographically dual to QCD with an non-zero theta angle. The baryon state is identified to the excitation of the Skyrmion which is described by a quantum mechanical system in holography. By employing the definition of OTOC in quantum mechanics, we derive the formulas and demonstrate explicitly the numerical calculations of the OTOC. Our calculation illustrates the quantum OTOC with imaginary Lyapunov coefficient indicates the possibly metastable baryonic status in the presence of the theta angle while the classical OTOC can not, thus it reveals the theta-dependent features of QCD are dominated basically by its quantum properties. Furthermore, the OTOC also reveals the baryonic phase becomes really chaotic with real Lyapunov exponent if the theta angle increases sufficiently which agrees with the unstable baryon spectrum presented in this model. In this sense, we believe the OTOC may be treated as a tool to detect the baryonic phase structure of QCD.","sentences":["We study the out-of-time-order correlators (OTOC) of Skyrmion as baryon in the D0-D4/D8 model which is holographically dual to QCD with an non-zero theta angle.","The baryon state is identified to the excitation of the Skyrmion which is described by a quantum mechanical system in holography.","By employing the definition of OTOC in quantum mechanics, we derive the formulas and demonstrate explicitly the numerical calculations of the OTOC.","Our calculation illustrates the quantum OTOC with imaginary Lyapunov coefficient indicates the possibly metastable baryonic status in the presence of the theta angle","while the classical OTOC can not, thus it reveals the theta-dependent features of QCD are dominated basically by its quantum properties.","Furthermore, the OTOC also reveals the baryonic phase becomes really chaotic with real Lyapunov exponent if the theta angle increases sufficiently which agrees with the unstable baryon spectrum presented in this model.","In this sense, we believe the OTOC may be treated as a tool to detect the baryonic phase structure of QCD."],"url":"http://arxiv.org/abs/2403.07335v1","category":"hep-th"}
{"created":"2024-03-12 05:06:07","title":"Efficient Diffusion Model for Image Restoration by Residual Shifting","abstract":"While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \\textit{\\textbf{even only with four sampling steps}}. Our code and model are publicly available at \\url{https://github.com/zsyOAOA/ResShift}.","sentences":["While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps.","Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes.","To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps.","Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration.","Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency.","A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process.","Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \\textit{\\textbf{even only with four sampling steps}}.","Our code and model are publicly available at \\url{https://github.com/zsyOAOA/ResShift}."],"url":"http://arxiv.org/abs/2403.07319v1","category":"cs.CV"}
{"created":"2024-03-12 04:58:40","title":"Cosmology with shear ratios: a joint study of weak lensing and spectroscopic redshift datasets","abstract":"The ratio of the average tangential shear signal of different weak lensing source populations around the same lens galaxies, also known as a shear ratio, provides an important test of lensing systematics and a potential source of cosmological information. In this paper we measure shear ratios of three current weak lensing surveys -- KiDS, DES, and HSC -- using overlapping data from the Baryon Oscillation Spectroscopic Survey. We apply a Bayesian method to reduce bias in shear ratio measurement, and assess the degree to which shear ratio information improves the determination of important astrophysical parameters describing the source redshift distributions and intrinsic galaxy alignments, as well as cosmological parameters, in comparison with cosmic shear and full 3x2-pt correlations (cosmic shear, galaxy-galaxy lensing, and galaxy clustering). We consider both Fisher matrix forecasts, as well as full likelihood analyses of the data. We find that the addition of shear ratio information to cosmic shear allows the mean redshifts of the source samples and intrinsic alignment parameters to be determined significantly more accurately. Although the additional constraining power enabled by the shear ratio is less than that obtained by introducing an accurate prior in the mean source redshift using photometric redshift calibration, the shear ratio allows for a useful cross-check. The inclusion of shear ratio data consistently benefits the determination of cosmological parameters such as S_8, for which we obtain improvements up to 34%. However these improvements are less significant when shear ratio is combined with the full 3x2-pt correlations. We conclude that shear ratio tests will remain a useful source of cosmological information and cross-checks for lensing systematics, whose application will be further enhanced by upcoming datasets such as the Dark Energy Spectroscopic Instrument.","sentences":["The ratio of the average tangential shear signal of different weak lensing source populations around the same lens galaxies, also known as a shear ratio, provides an important test of lensing systematics and a potential source of cosmological information.","In this paper we measure shear ratios of three current weak lensing surveys -- KiDS, DES, and HSC -- using overlapping data from the Baryon Oscillation Spectroscopic Survey.","We apply a Bayesian method to reduce bias in shear ratio measurement, and assess the degree to which shear ratio information improves the determination of important astrophysical parameters describing the source redshift distributions and intrinsic galaxy alignments, as well as cosmological parameters, in comparison with cosmic shear and full 3x2-pt correlations (cosmic shear, galaxy-galaxy lensing, and galaxy clustering).","We consider both Fisher matrix forecasts, as well as full likelihood analyses of the data.","We find that the addition of shear ratio information to cosmic shear allows the mean redshifts of the source samples and intrinsic alignment parameters to be determined significantly more accurately.","Although the additional constraining power enabled by the shear ratio is less than that obtained by introducing an accurate prior in the mean source redshift using photometric redshift calibration, the shear ratio allows for a useful cross-check.","The inclusion of shear ratio data consistently benefits the determination of cosmological parameters such as S_8, for which we obtain improvements up to 34%.","However these improvements are less significant when shear ratio is combined with the full 3x2-pt correlations.","We conclude that shear ratio tests will remain a useful source of cosmological information and cross-checks for lensing systematics, whose application will be further enhanced by upcoming datasets such as the Dark Energy Spectroscopic Instrument."],"url":"http://arxiv.org/abs/2403.07313v1","category":"astro-ph.CO"}
{"created":"2024-03-12 04:27:34","title":"Asymptotic behavior of saxion-axion system in stringy quintessence model","abstract":"We study the late time behavior of the slow-roll parameter in the stringy quintessence model when axion as well as saxion is allowed to move. Even though the potential is independent of the axion at tree level, the axion can move through its coupling to the saxion and the background geometry. Then the contributions of the axion kinetic energy to the slow-roll parameter and the vacuum energy density are not negligible when the slow-roll approximation does not hold. As the dimension of the field space is doubled, the fixed point at which the time variation of the slow-roll parameter vanishes is not always stable. We find that the fixed point in the saxion-axion system is at most partially stable, in particular when the volume modulus and the axio-dilaton, the essential ingredients of the string compactification, are taken into account. It seems that as we consider more saxion-axion pairs, the stability of the fixed point becomes difficult to achieve.","sentences":["We study the late time behavior of the slow-roll parameter in the stringy quintessence model when axion as well as saxion is allowed to move.","Even though the potential is independent of the axion at tree level, the axion can move through its coupling to the saxion and the background geometry.","Then the contributions of the axion kinetic energy to the slow-roll parameter and the vacuum energy density are not negligible when the slow-roll approximation does not hold.","As the dimension of the field space is doubled, the fixed point at which the time variation of the slow-roll parameter vanishes is not always stable.","We find that the fixed point in the saxion-axion system is at most partially stable, in particular when the volume modulus and the axio-dilaton, the essential ingredients of the string compactification, are taken into account.","It seems that as we consider more saxion-axion pairs, the stability of the fixed point becomes difficult to achieve."],"url":"http://arxiv.org/abs/2403.07307v1","category":"hep-th"}
{"created":"2024-03-12 04:07:19","title":"The Kerr Memory Effect at Null Infinity","abstract":"We compute the memory effect due to a gravitational wave striking a Kerr black hole as seen by an observer at null infinity. This is done by working in Bondi--Sachs coordinates. It was shown by Hawking, Perry, and Strominger (HPS) that the memory effect due to a gravitational shockwave is seen as a pure BMS supertranslation from null infinity. Hence, it is of interest to compute the supertranslated Kerr solution in Bondi--Sachs coordinates. Finally, the gravitational wave is said to implant soft supertranslation hair on the event horizon of the black hole which carries superrotation charge. We will explicitly calculate the change in superrotation charge on the event horizon due to the supertranslation hair.","sentences":["We compute the memory effect due to a gravitational wave striking a Kerr black hole as seen by an observer at null infinity.","This is done by working in Bondi--Sachs coordinates.","It was shown by Hawking, Perry, and Strominger (HPS) that the memory effect due to a gravitational shockwave is seen as a pure BMS supertranslation from null infinity.","Hence, it is of interest to compute the supertranslated Kerr solution in Bondi--Sachs coordinates.","Finally, the gravitational wave is said to implant soft supertranslation hair on the event horizon of the black hole which carries superrotation charge.","We will explicitly calculate the change in superrotation charge on the event horizon due to the supertranslation hair."],"url":"http://arxiv.org/abs/2403.07302v1","category":"gr-qc"}
{"created":"2024-03-12 03:16:00","title":"Distance-Dependent Evolution of Electronic States in Kagome- Honeycomb Lateral Heterostructures in FeSn","abstract":"In this work, we demonstrate the formation and electronic influence of lateral heterointerfaces in FeSn containing Kagome and honeycomb layers. Lateral heterostructures offer spatially resolved property control, enabling the integration of dissimilar materials and promoting phenomena not typically observed in vertical heterostructures. Using the molecular beam epitaxy technique, we achieve a controllable synthesis of lateral heterostructures in the Kagome metal FeSn. With scanning tunneling microscopy/spectroscopy in conjunction with first-principles calculations, we provide a comprehensive understanding of the bonding motif connecting the Fe3Sn-terminated Kagome and Sn2-terminated honeycomb surfaces. More importantly, we reveal a distance-dependent evolution of the electronic states in the vicinity of the heterointerfaces. This evolution is significantly influenced by the orbital character of the flat bands. Our findings suggest an approach to modulate the electronic properties of the Kagome lattice, which should be beneficial for the development of future quantum devices.","sentences":["In this work, we demonstrate the formation and electronic influence of lateral heterointerfaces in FeSn containing Kagome and honeycomb layers.","Lateral heterostructures offer spatially resolved property control, enabling the integration of dissimilar materials and promoting phenomena not typically observed in vertical heterostructures.","Using the molecular beam epitaxy technique, we achieve a controllable synthesis of lateral heterostructures in the Kagome metal FeSn.","With scanning tunneling microscopy/spectroscopy in conjunction with first-principles calculations, we provide a comprehensive understanding of the bonding motif connecting the Fe3Sn-terminated Kagome and Sn2-terminated honeycomb surfaces.","More importantly, we reveal a distance-dependent evolution of the electronic states in the vicinity of the heterointerfaces.","This evolution is significantly influenced by the orbital character of the flat bands.","Our findings suggest an approach to modulate the electronic properties of the Kagome lattice, which should be beneficial for the development of future quantum devices."],"url":"http://arxiv.org/abs/2403.07278v1","category":"cond-mat.str-el"}
{"created":"2024-03-12 03:12:48","title":"Coarse-graining black holes out of equilibrium with boundary observables on time slice","abstract":"In black hole thermodynamics, defining coarse-grained entropy for dynamical black holes has long been a challenge, and various proposals, such as generalized entropy, have been explored. Guided by the AdS/CFT, we introduce a new definition of coarse-grained entropy for a dynamical black hole in Lorentzian Einstein gravity. On each time slice, this entropy is defined as the horizon area of an auxiliary Euclidean black hole that shares the same mass, (angular) momenta, and asymptotic normalizable matter modes with the original Lorentzian solution. The entropy is shown to satisfy a generalized first law and, through holography, the second law as well. Furthermore, by applying this thermodynamics to several Vaidya models in AdS and flat spacetime, we discover a connection between the second law and the null energy condition.","sentences":["In black hole thermodynamics, defining coarse-grained entropy for dynamical black holes has long been a challenge, and various proposals, such as generalized entropy, have been explored.","Guided by the AdS/CFT, we introduce a new definition of coarse-grained entropy for a dynamical black hole in Lorentzian Einstein gravity.","On each time slice, this entropy is defined as the horizon area of an auxiliary Euclidean black hole that shares the same mass, (angular) momenta, and asymptotic normalizable matter modes with the original Lorentzian solution.","The entropy is shown to satisfy a generalized first law and, through holography, the second law as well.","Furthermore, by applying this thermodynamics to several Vaidya models in AdS and flat spacetime, we discover a connection between the second law and the null energy condition."],"url":"http://arxiv.org/abs/2403.07275v1","category":"hep-th"}
{"created":"2024-03-12 03:03:36","title":"Lateral 2D superlattices in GaAs heterostructures with independent control of carrier density and modulation potential","abstract":"We present a new double-layer design for 2D surface superlattice systems in GaAs-AlGaAs heterostructures. Unlike previous studies, our device (1) uses an in-situ gate, which allows very short period superlattice in high mobility, shallow heterostructures; (2) enables independent control of the carrier density and the superlattice modulation potential amplitude over a wide range. We characterise this device design using low-temperature magneto-transport measurements and show that the fabrication process caused minimal damage to the system. We demonstrate the tuning of potential modulation from weak (much smaller than Fermi energy) to strong (larger than the Fermi energy) regimes.","sentences":["We present a new double-layer design for 2D surface superlattice systems in GaAs-AlGaAs heterostructures.","Unlike previous studies, our device (1) uses an in-situ gate, which allows very short period superlattice in high mobility, shallow heterostructures; (2) enables independent control of the carrier density and the superlattice modulation potential amplitude over a wide range.","We characterise this device design using low-temperature magneto-transport measurements and show that the fabrication process caused minimal damage to the system.","We demonstrate the tuning of potential modulation from weak (much smaller than Fermi energy) to strong (larger than the Fermi energy) regimes."],"url":"http://arxiv.org/abs/2403.07273v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 02:54:53","title":"Search for CP-violating Neutrino Non-Standard Interactions with the NOvA Experiment","abstract":"This Letter reports a search for charge-parity (CP) symmetry violating non-standard interactions (NSI) of neutrinos with matter using the NOvA Experiment, and examines their effects on the determination of the standard oscillation parameters. Data from $\\nu_{\\mu}(\\bar{\\nu}_{\\mu})\\rightarrow\\nu_{\\mu}(\\bar{\\nu}_{\\mu})$ and $\\nu_{\\mu}(\\bar{\\nu}_{\\mu})\\rightarrow\\nu_{e}(\\bar{\\nu}_{e})$ oscillation channels are used to measure the effect of the NSI parameters $\\varepsilon_{e\\mu}$ and $\\varepsilon_{e\\tau}$. With 90% C.L. the magnitudes of the NSI couplings are constrained to be $|\\varepsilon_{e\\mu}| \\, \\lesssim 0.3$ and $|\\varepsilon_{e\\tau}| \\, \\lesssim 0.4$. A degeneracy at $|\\varepsilon_{e\\tau}| \\, \\approx 1.8$ is reported, and we observe that the presence of NSI limits sensitivity to the standard CP phase $\\delta_{\\tiny\\text{CP}}$.","sentences":["This Letter reports a search for charge-parity (CP) symmetry violating non-standard interactions (NSI) of neutrinos with matter using the NOvA Experiment, and examines their effects on the determination of the standard oscillation parameters.","Data from $\\nu_{\\mu}(\\bar{\\nu}_{\\mu})\\rightarrow\\nu_{\\mu}(\\bar{\\nu}_{\\mu})$ and $\\nu_{\\mu}(\\bar{\\nu}_{\\mu})\\rightarrow\\nu_{e}(\\bar{\\nu}_{e})$ oscillation channels are used to measure the effect of the NSI parameters $\\varepsilon_{e\\mu}$ and $\\varepsilon_{e\\tau}$. With 90% C.L. the magnitudes of the NSI couplings are constrained to be $|\\varepsilon_{e\\mu}| \\, \\lesssim 0.3$ and $|\\varepsilon_{e\\tau}| \\,","\\lesssim 0.4$. A degeneracy at $|\\varepsilon_{e\\tau}| \\, \\approx 1.8$ is reported, and we observe that the presence of NSI limits sensitivity to the standard CP phase $\\delta_{\\tiny\\text{CP}}$."],"url":"http://arxiv.org/abs/2403.07266v1","category":"hep-ex"}
{"created":"2024-03-12 02:22:00","title":"Spatially resolved random telegraph fluctuations of a single trap at the Si/SiO2 interface","abstract":"We use electrostatic force microscopy to spatially resolve random telegraph noise at the Si/SiO$_2$ interface. Our measurements demonstrate that two-state fluctuations are localized at interfacial traps, with bias-dependent rates and amplitudes. These two-level systems lead to correlated carrier number and mobility fluctuations with a range of characteristic timescales; taken together as an ensemble, they give rise to a $1/f$ power spectral trend. Such individual defect fluctuations at the Si/SiO$_2$ interface impair the performance and reliability of nanoscale semiconductor devices, and will be a significant source of noise in semiconductor-based quantum sensors and computers. The fluctuations measured here are associated with a four-fold competition of rates, including slow two-state switching on the order of seconds and, in one state, fast switching on the order of nanoseconds which is associated with energy loss.","sentences":["We use electrostatic force microscopy to spatially resolve random telegraph noise at the Si/SiO$_2$ interface.","Our measurements demonstrate that two-state fluctuations are localized at interfacial traps, with bias-dependent rates and amplitudes.","These two-level systems lead to correlated carrier number and mobility fluctuations with a range of characteristic timescales; taken together as an ensemble, they give rise to a $1/f$ power spectral trend.","Such individual defect fluctuations at the Si/SiO$_2$ interface impair the performance and reliability of nanoscale semiconductor devices, and will be a significant source of noise in semiconductor-based quantum sensors and computers.","The fluctuations measured here are associated with a four-fold competition of rates, including slow two-state switching on the order of seconds and, in one state, fast switching on the order of nanoseconds which is associated with energy loss."],"url":"http://arxiv.org/abs/2403.07251v1","category":"cond-mat.mes-hall"}
