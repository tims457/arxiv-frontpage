{"created":"2024-05-31 17:59:47","title":"Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis","abstract":"In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 256 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page: https://video-mme.github.io","sentences":["In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements.","However, the predominant focus remains on developing their capabilities in static image understanding.","The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance.","In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis.","Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment.","900 videos with a total of 256 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs.","With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video.","Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models.","Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data.","Project Page: https://video-mme.github.io"],"url":"http://arxiv.org/abs/2405.21075v1","category":"cs.CV"}
{"created":"2024-05-31 17:58:06","title":"Fast inspirals and the treatment of orbital resonances","abstract":"Extreme mass ratio inspirals (EMRIs), where a compact object orbits a massive black hole, are a key source of gravitational waves for the future Laser Interferometer Space Antenna (LISA). Due to their small mass ratio, ($\\epsilon \\sim 10^{-4}$--$10^{-7}$), the binary evolves slowly and EMRI signals will be in-band for years. Additionally, astrophysical EMRIs are expected to have complex dynamics featuring both spin-precession and eccentricity. A standard approach to modelling these inspirals is via the method of osculating geodesics (OG) which we employ along with a toy model for the gravitational self-force. Using this method requires resolving tens of thousands radial and polar orbital librations over the long duration of the signal which makes the inspiral trajectory expensive to compute. In this work we accelerate these calculations by employing Near-Identity (averaging) Transformations. However, this averaging technique breaks down at orbital resonances where the radial and polar frequencies are an integer ratio of each other. Thus, we switch to a partial averaging transformation in the vicinity of the resonance where the dynamics are characterised by the slow evolution of the so-called \"resonant phase\". Additionally, we develop an optimal switching criterion to minimise the computation time while maximising accuracy. We find the error in the waveform phase is improved from $\\mathcal{O}(\\epsilon^{-1/2})$ in the fully averaged scheme to $\\mathcal{O}(\\epsilon^{4/7})$ in the switching scheme. At the same time, this scheme improves the scaling of the computation time from being inversely proportional to $\\epsilon$ using OG, to a very weak scaling with $\\epsilon$. This results in a speed-up of at least two orders of magnitude for LISA EMRIs with room for further optimisation.","sentences":["Extreme mass ratio inspirals (EMRIs), where a compact object orbits a massive black hole, are a key source of gravitational waves for the future Laser Interferometer Space Antenna (LISA).","Due to their small mass ratio, ($\\epsilon \\sim 10^{-4}$--$10^{-7}$), the binary evolves slowly and EMRI signals will be in-band for years.","Additionally, astrophysical EMRIs are expected to have complex dynamics featuring both spin-precession and eccentricity.","A standard approach to modelling these inspirals is via the method of osculating geodesics (OG) which we employ along with a toy model for the gravitational self-force.","Using this method requires resolving tens of thousands radial and polar orbital librations over the long duration of the signal which makes the inspiral trajectory expensive to compute.","In this work we accelerate these calculations by employing Near-Identity (averaging) Transformations.","However, this averaging technique breaks down at orbital resonances where the radial and polar frequencies are an integer ratio of each other.","Thus, we switch to a partial averaging transformation in the vicinity of the resonance where the dynamics are characterised by the slow evolution of the so-called \"resonant phase\".","Additionally, we develop an optimal switching criterion to minimise the computation time while maximising accuracy.","We find the error in the waveform phase is improved from $\\mathcal{O}(\\epsilon^{-1/2})$ in the fully averaged scheme to $\\mathcal{O}(\\epsilon^{4/7})$ in the switching scheme.","At the same time, this scheme improves the scaling of the computation time from being inversely proportional to $\\epsilon$ using OG, to a very weak scaling with $\\epsilon$. This results in a speed-up of at least two orders of magnitude for LISA EMRIs with room for further optimisation."],"url":"http://arxiv.org/abs/2405.21072v1","category":"gr-qc"}
{"created":"2024-05-31 17:57:30","title":"A Multi-wavelength, Multi-epoch Monitoring Campaign of Accretion Variability in T Tauri Stars from the ODYSSEUS Survey. II. Photometric Light Curves","abstract":"Classical T Tauri Stars (CTTSs) are young, low-mass stars which accrete material from their surrounding protoplanetary disk. To better understand accretion variability, we conducted a multi-epoch, multi-wavelength photometric monitoring campaign of four CTTSs: TW Hya, RU Lup, BP Tau, and GM Aur, in 2021 and 2022, contemporaneous with HST UV and optical spectra. We find that all four targets display significant variability in their light curves, generally on days-long timescales (but in some cases year-to-year) often due to periodicity associated with stellar rotation and to stochastic accretion variability. Their is a strong connection between mass accretion and photometric variability in all bands, but the relationship varies per target and epoch. Thus, photometry should be used with caution as a direct measure of accretion in CTTSs.","sentences":["Classical T Tauri Stars (CTTSs) are young, low-mass stars which accrete material from their surrounding protoplanetary disk.","To better understand accretion variability, we conducted a multi-epoch, multi-wavelength photometric monitoring campaign of four CTTSs: TW Hya, RU Lup, BP Tau, and GM Aur, in 2021 and 2022, contemporaneous with HST UV and optical spectra.","We find that all four targets display significant variability in their light curves, generally on days-long timescales (but in some cases year-to-year) often due to periodicity associated with stellar rotation and to stochastic accretion variability.","Their is a strong connection between mass accretion and photometric variability in all bands, but the relationship varies per target and epoch.","Thus, photometry should be used with caution as a direct measure of accretion in CTTSs."],"url":"http://arxiv.org/abs/2405.21071v1","category":"astro-ph.SR"}
{"created":"2024-05-31 17:57:24","title":"Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights","abstract":"Severe data imbalance naturally exists among web-scale vision-language datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations. With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training. This isolates the bias from dominant classes and implicitly balances the learning signal. Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning. Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community. The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks. Code will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.","sentences":["Severe data imbalance naturally exists among web-scale vision-language datasets.","Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations.","With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training.","This isolates the bias from dominant classes and implicitly balances the learning signal.","Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning.","Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community.","The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks.","Code will be available at: https://github.com/CVMI-Lab/clip-beyond-tail."],"url":"http://arxiv.org/abs/2405.21070v1","category":"cs.CV"}
{"created":"2024-05-31 17:56:33","title":"Code Pretraining Improves Entity Tracking Abilities of Language Models","abstract":"Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by comparing pairs of language models on their entity tracking performance. Critically, the pairs consist of base models and models trained on top of these base models with additional code data. We extend this analysis to additionally examine the effect of math training, another highly structured data type, and alignment tuning, an important step for enhancing the usability of models. We find clear evidence that models additionally trained on large amounts of code outperform the base models. On the other hand, we find no consistent benefit of additional math training or alignment tuning across various model families.","sentences":["Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language.","In this work, we systematically test this claim by comparing pairs of language models on their entity tracking performance.","Critically, the pairs consist of base models and models trained on top of these base models with additional code data.","We extend this analysis to additionally examine the effect of math training, another highly structured data type, and alignment tuning, an important step for enhancing the usability of models.","We find clear evidence that models additionally trained on large amounts of code outperform the base models.","On the other hand, we find no consistent benefit of additional math training or alignment tuning across various model families."],"url":"http://arxiv.org/abs/2405.21068v1","category":"cs.CL"}
{"created":"2024-05-31 17:53:00","title":"Recurrent neural networks: vanishing and exploding gradients are not the end of the story","abstract":"Recurrent neural networks (RNNs) notoriously struggle to learn long-term memories, primarily due to vanishing and exploding gradients. The recent success of state-space models (SSMs), a subclass of RNNs, to overcome such difficulties challenges our theoretical understanding. In this paper, we delve into the optimization challenges of RNNs and discover that, as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive, even without exploding gradients. Our analysis further reveals the importance of the element-wise recurrence design pattern combined with careful parametrizations in mitigating this effect. This feature is present in SSMs, as well as in other architectures, such as LSTMs. Overall, our insights provide a new explanation for some of the difficulties in gradient-based learning of RNNs and why some architectures perform better than others.","sentences":["Recurrent neural networks (RNNs) notoriously struggle to learn long-term memories, primarily due to vanishing and exploding gradients.","The recent success of state-space models (SSMs), a subclass of RNNs, to overcome such difficulties challenges our theoretical understanding.","In this paper, we delve into the optimization challenges of RNNs and discover that, as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive, even without exploding gradients.","Our analysis further reveals the importance of the element-wise recurrence design pattern combined with careful parametrizations in mitigating this effect.","This feature is present in SSMs, as well as in other architectures, such as LSTMs.","Overall, our insights provide a new explanation for some of the difficulties in gradient-based learning of RNNs and why some architectures perform better than others."],"url":"http://arxiv.org/abs/2405.21064v1","category":"cs.LG"}
{"created":"2024-05-31 17:51:07","title":"Neural Network Verification with Branch-and-Bound for General Nonlinearities","abstract":"Branch-and-bound (BaB) is among the most effective methods for neural network (NN) verification. However, existing works on BaB have mostly focused on NNs with piecewise linear activations, especially ReLU networks. In this paper, we develop a general framework, named GenBaB, to conduct BaB for general nonlinearities in general computational graphs based on linear bound propagation. To decide which neuron to branch, we design a new branching heuristic which leverages linear bounds as shortcuts to efficiently estimate the potential improvement after branching. To decide nontrivial branching points for general nonlinear functions, we propose to optimize branching points offline, which can be efficiently leveraged during verification with a lookup table. We demonstrate the effectiveness of our GenBaB on verifying a wide range of NNs, including networks with activation functions such as Sigmoid, Tanh, Sine and GeLU, as well as networks involving multi-dimensional nonlinear operations such as multiplications in LSTMs and Vision Transformers. Our framework also allows the verification of general nonlinear computation graphs and enables verification applications beyond simple neural networks, particularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest $\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of Neural Networks Competition (VNN-COMP 2023).","sentences":["Branch-and-bound (BaB) is among the most effective methods for neural network (NN) verification.","However, existing works on BaB have mostly focused on NNs with piecewise linear activations, especially ReLU networks.","In this paper, we develop a general framework, named GenBaB, to conduct BaB for general nonlinearities in general computational graphs based on linear bound propagation.","To decide which neuron to branch, we design a new branching heuristic which leverages linear bounds as shortcuts to efficiently estimate the potential improvement after branching.","To decide nontrivial branching points for general nonlinear functions, we propose to optimize branching points offline, which can be efficiently leveraged during verification with a lookup table.","We demonstrate the effectiveness of our GenBaB on verifying a wide range of NNs, including networks with activation functions such as Sigmoid, Tanh, Sine and GeLU, as well as networks involving multi-dimensional nonlinear operations such as multiplications in LSTMs and Vision Transformers.","Our framework also allows the verification of general nonlinear computation graphs and enables verification applications beyond simple neural networks, particularly for AC Optimal Power Flow (ACOPF).","GenBaB is part of the latest $\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of Neural Networks Competition (VNN-COMP 2023)."],"url":"http://arxiv.org/abs/2405.21063v1","category":"cs.LG"}
{"created":"2024-05-31 17:51:00","title":"Algebra of global sections of $\u03c8$-bundles on $\\bar{M}_{0,n}$","abstract":"We consider the ${\\mathbb Z}^n$-graded algebra of global sections of line bundles generated by the standard line bundles $L_1,\\ldots,L_n$ on $\\bar{M}_{0,n}$. We find a simple presentation of this algebra by generators and quadratic relations. As an application we prove that the moduli space $\\bar{M}_{0,n}[\\psi]$ of $\\psi$-stable curves of genus $0$ is Cohen-Macaulay and normal, and the natural map $\\bar{M}_{0,n}\\to \\bar{M}_{0,n}[\\psi]$ is a rational resolution.","sentences":["We consider the ${\\mathbb Z}^n$-graded algebra of global sections of line bundles generated by the standard line bundles $L_1,\\ldots,L_n$ on $\\bar{M}_{0,n}$. We find a simple presentation of this algebra by generators and quadratic relations.","As an application we prove that the moduli space $\\bar{M}_{0,n}[\\psi]$ of $\\psi$-stable curves of genus $0$ is Cohen-Macaulay and normal, and the natural map $\\bar{M}_{0,n}\\to \\bar{M}_{0,n}[\\psi]$ is a rational resolution."],"url":"http://arxiv.org/abs/2405.21062v1","category":"math.AG"}
{"created":"2024-05-31 17:50:01","title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","abstract":"While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.","sentences":["While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale.","We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.","Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling."],"url":"http://arxiv.org/abs/2405.21060v1","category":"cs.LG"}
{"created":"2024-05-31 17:49:51","title":"Unified Directly Denoising for Both Variance Preserving and Variance Exploding Diffusion Models","abstract":"Previous work has demonstrated that, in the Variance Preserving (VP) scenario, the nascent Directly Denoising Diffusion Models (DDDM) can generate high-quality images in one step while achieving even better performance in multistep sampling. However, the Pseudo-LPIPS loss used in DDDM leads to concerns about the bias in assessment. Here, we propose a unified DDDM (uDDDM) framework that generates images in one-step/multiple steps for both Variance Preserving (VP) and Variance Exploding (VE) cases. We provide theoretical proofs of the existence and uniqueness of the model's solution paths, as well as the non-intersecting property of the sampling paths. Additionally, we propose an adaptive Pseudo-Huber loss function to balance the convergence to the true solution and the stability of convergence process.Through a comprehensive evaluation, we demonstrate that uDDDMs achieve FID scores comparable to the best-performing methods available for CIFAR-10 in both VP and VE. Specifically, uDDDM achieves one-step generation on CIFAR10 with FID of 2.63 and 2.53 for VE and VP respectively. By extending the sampling to 1000 steps, we further reduce FID score to 1.71 and 1.65 for VE and VP respectively, setting state-of-the-art performance in both cases.","sentences":["Previous work has demonstrated that, in the Variance Preserving (VP) scenario, the nascent Directly Denoising Diffusion Models (DDDM) can generate high-quality images in one step while achieving even better performance in multistep sampling.","However, the Pseudo-LPIPS loss used in DDDM leads to concerns about the bias in assessment.","Here, we propose a unified DDDM (uDDDM) framework that generates images in one-step/multiple steps for both Variance Preserving (VP) and Variance Exploding (VE) cases.","We provide theoretical proofs of the existence and uniqueness of the model's solution paths, as well as the non-intersecting property of the sampling paths.","Additionally, we propose an adaptive Pseudo-Huber loss function to balance the convergence to the true solution and the stability of convergence process.","Through a comprehensive evaluation, we demonstrate that uDDDMs achieve FID scores comparable to the best-performing methods available for CIFAR-10 in both VP and VE.","Specifically, uDDDM achieves one-step generation on CIFAR10 with FID of 2.63 and 2.53 for VE and VP respectively.","By extending the sampling to 1000 steps, we further reduce FID score to 1.71 and 1.65 for VE and VP respectively, setting state-of-the-art performance in both cases."],"url":"http://arxiv.org/abs/2405.21059v1","category":"cs.CV"}
{"created":"2024-05-31 17:47:22","title":"An Organic Weed Control Prototype using Directed Energy and Deep Learning","abstract":"Organic weed control is a vital to improve crop yield with a sustainable approach. In this work, a directed energy weed control robot prototype specifically designed for organic farms is proposed. The robot uses a novel distributed array robot (DAR) unit for weed treatment. Soybean and corn databases are built to train deep learning neural nets to perform weed recognition. The initial deep learning neural nets show a high performance in classifying crops. The robot uses a patented directed energy plant eradication recipe that is completely organic and UV-C free, with no chemical damage or physical disturbance to the soil. The deep learning can classify 8 common weed species in a soybean field under natural environment with up to 98% accuracy.","sentences":["Organic weed control is a vital to improve crop yield with a sustainable approach.","In this work, a directed energy weed control robot prototype specifically designed for organic farms is proposed.","The robot uses a novel distributed array robot (DAR) unit for weed treatment.","Soybean and corn databases are built to train deep learning neural nets to perform weed recognition.","The initial deep learning neural nets show a high performance in classifying crops.","The robot uses a patented directed energy plant eradication recipe that is completely organic and UV-C free, with no chemical damage or physical disturbance to the soil.","The deep learning can classify 8 common weed species in a soybean field under natural environment with up to 98% accuracy."],"url":"http://arxiv.org/abs/2405.21056v1","category":"cs.RO"}
{"created":"2024-05-31 17:44:36","title":"RydbergGPT","abstract":"We introduce a generative pretained transformer (GPT) designed to learn the measurement outcomes of a neutral atom array quantum computer. Based on a vanilla transformer, our encoder-decoder architecture takes as input the interacting Hamiltonian, and outputs an autoregressive sequence of qubit measurement probabilities. Its performance is studied in the vicinity of a quantum phase transition in Rydberg atoms in a square lattice array. We explore the ability of the architecture to generalize, by producing groundstate measurements for Hamiltonian parameters not seen in the training set. We focus on examples of physical observables obtained from inference on three different models, trained in fixed compute time on a single NVIDIA A100 GPU. These can act as benchmarks for the scaling of larger RydbergGPT models in the future. Finally, we provide RydbergGPT open source, to aid in the development of foundation models based off of a wide variety of quantum computer interactions and data sets in the future.","sentences":["We introduce a generative pretained transformer (GPT) designed to learn the measurement outcomes of a neutral atom array quantum computer.","Based on a vanilla transformer, our encoder-decoder architecture takes as input the interacting Hamiltonian, and outputs an autoregressive sequence of qubit measurement probabilities.","Its performance is studied in the vicinity of a quantum phase transition in Rydberg atoms in a square lattice array.","We explore the ability of the architecture to generalize, by producing groundstate measurements for Hamiltonian parameters not seen in the training set.","We focus on examples of physical observables obtained from inference on three different models, trained in fixed compute time on a single NVIDIA A100 GPU.","These can act as benchmarks for the scaling of larger RydbergGPT models in the future.","Finally, we provide RydbergGPT open source, to aid in the development of foundation models based off of a wide variety of quantum computer interactions and data sets in the future."],"url":"http://arxiv.org/abs/2405.21052v1","category":"quant-ph"}
{"created":"2024-05-31 17:43:35","title":"Spectrum-Aware Parameter Efficient Fine-Tuning for Diffusion Models","abstract":"Adapting large-scale pre-trained generative models in a parameter-efficient manner is gaining traction. Traditional methods like low rank adaptation achieve parameter efficiency by imposing constraints but may not be optimal for tasks requiring high representation capacity. We propose a novel spectrum-aware adaptation framework for generative models. Our method adjusts both singular values and their basis vectors of pretrained weights. Using the Kronecker product and efficient Stiefel optimizers, we achieve parameter-efficient adaptation of orthogonal matrices. We introduce Spectral Orthogonal Decomposition Adaptation (SODA), which balances computational efficiency and representation capacity. Extensive evaluations on text-to-image diffusion models demonstrate SODA's effectiveness, offering a spectrum-aware alternative to existing fine-tuning methods.","sentences":["Adapting large-scale pre-trained generative models in a parameter-efficient manner is gaining traction.","Traditional methods like low rank adaptation achieve parameter efficiency by imposing constraints but may not be optimal for tasks requiring high representation capacity.","We propose a novel spectrum-aware adaptation framework for generative models.","Our method adjusts both singular values and their basis vectors of pretrained weights.","Using the Kronecker product and efficient Stiefel optimizers, we achieve parameter-efficient adaptation of orthogonal matrices.","We introduce Spectral Orthogonal Decomposition Adaptation (SODA), which balances computational efficiency and representation capacity.","Extensive evaluations on text-to-image diffusion models demonstrate SODA's effectiveness, offering a spectrum-aware alternative to existing fine-tuning methods."],"url":"http://arxiv.org/abs/2405.21050v1","category":"cs.CV"}
{"created":"2024-05-31 17:41:11","title":"Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling","abstract":"Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process. In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs. Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process.","sentences":["Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions.","Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight.","To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors.","Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process.","In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens.","These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs.","Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality.","Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process."],"url":"http://arxiv.org/abs/2405.21048v1","category":"cs.CV"}
{"created":"2024-05-31 17:39:15","title":"Grammar-Aligned Decoding","abstract":"Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.","sentences":["Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup.","Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint.","Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar.","In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality.","We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint.","Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes.","Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints."],"url":"http://arxiv.org/abs/2405.21047v1","category":"cs.AI"}
{"created":"2024-05-31 17:39:06","title":"Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF","abstract":"Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment. We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses. By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques. We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical -- a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data. In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage. Our analysis, which builds on the observation that DPO implicitly performs a form of $Q^{\\star}$-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of KL-regularized Markov decision processes. Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation.","sentences":["Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment.","We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses.","By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques.","We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical -- a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the strongest known provable guarantees and promising empirical performance.","XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data.","In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage.","Our analysis, which builds on the observation that DPO implicitly performs a form of $Q^{\\star}$-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of KL-regularized Markov decision processes.","Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation."],"url":"http://arxiv.org/abs/2405.21046v1","category":"cs.LG"}
{"created":"2024-05-31 17:36:16","title":"Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation","abstract":"We prove that the combination of a target network and over-parameterized linear function approximation establishes a weaker convergence condition for bootstrapped value estimation in certain cases, even with off-policy data. Our condition is naturally satisfied for expected updates over the entire state-action space or learning with a batch of complete trajectories from episodic Markov decision processes. Notably, using only a target network or an over-parameterized model does not provide such a convergence guarantee. Additionally, we extend our results to learning with truncated trajectories, showing that convergence is achievable for all tasks with minor modifications, akin to value truncation for the final states in trajectories. Our primary result focuses on temporal difference estimation for prediction, providing high-probability value estimation error bounds and empirical analysis on Baird's counterexample and a Four-room task. Furthermore, we explore the control setting, demonstrating that similar convergence conditions apply to Q-learning.","sentences":["We prove that the combination of a target network and over-parameterized linear function approximation establishes a weaker convergence condition for bootstrapped value estimation in certain cases, even with off-policy data.","Our condition is naturally satisfied for expected updates over the entire state-action space or learning with a batch of complete trajectories from episodic Markov decision processes.","Notably, using only a target network or an over-parameterized model does not provide such a convergence guarantee.","Additionally, we extend our results to learning with truncated trajectories, showing that convergence is achievable for all tasks with minor modifications, akin to value truncation for the final states in trajectories.","Our primary result focuses on temporal difference estimation for prediction, providing high-probability value estimation error bounds and empirical analysis on Baird's counterexample and a Four-room task.","Furthermore, we explore the control setting, demonstrating that similar convergence conditions apply to Q-learning."],"url":"http://arxiv.org/abs/2405.21043v1","category":"cs.LG"}
{"created":"2024-05-31 17:33:07","title":"Comparing information content of representation spaces for disentanglement with VAE ensembles","abstract":"Disentanglement is the endeavour to use machine learning to divide information about a dataset into meaningful fragments. In practice these fragments are representation (sub)spaces, often the set of channels in the latent space of a variational autoencoder (VAE). Assessments of disentanglement predominantly employ metrics that are coarse-grained at the model level, but this approach can obscure much about the process of information fragmentation. Here we propose to study the learned channels in aggregate, as the fragments of information learned by an ensemble of repeat training runs. Additionally, we depart from prior work where measures of similarity between individual subspaces neglected the nature of data embeddings as probability distributions. Instead, we view representation subspaces as communication channels that perform a soft clustering of the data; consequently, we generalize two classic information-theoretic measures of similarity between clustering assignments to compare representation spaces. We develop a lightweight method of estimation based on fingerprinting representation subspaces by their ability to distinguish dataset samples, allowing us to identify, analyze, and leverage meaningful structure in ensembles of VAEs trained on synthetic and natural datasets. Using this fully unsupervised pipeline we identify \"hotspots\" in the space of information fragments: groups of nearly identical representation subspaces that appear repeatedly in an ensemble of VAEs, particularly as regularization is increased. Finally, we leverage the proposed methodology to achieve ensemble learning with VAEs, boosting the information content of a set of weak learners -- a capability not possible with previous methods of assessing channel similarity.","sentences":["Disentanglement is the endeavour to use machine learning to divide information about a dataset into meaningful fragments.","In practice these fragments are representation (sub)spaces, often the set of channels in the latent space of a variational autoencoder (VAE).","Assessments of disentanglement predominantly employ metrics that are coarse-grained at the model level, but this approach can obscure much about the process of information fragmentation.","Here we propose to study the learned channels in aggregate, as the fragments of information learned by an ensemble of repeat training runs.","Additionally, we depart from prior work where measures of similarity between individual subspaces neglected the nature of data embeddings as probability distributions.","Instead, we view representation subspaces as communication channels that perform a soft clustering of the data; consequently, we generalize two classic information-theoretic measures of similarity between clustering assignments to compare representation spaces.","We develop a lightweight method of estimation based on fingerprinting representation subspaces by their ability to distinguish dataset samples, allowing us to identify, analyze, and leverage meaningful structure in ensembles of VAEs trained on synthetic and natural datasets.","Using this fully unsupervised pipeline we identify \"hotspots\" in the space of information fragments: groups of nearly identical representation subspaces that appear repeatedly in an ensemble of VAEs, particularly as regularization is increased.","Finally, we leverage the proposed methodology to achieve ensemble learning with VAEs, boosting the information content of a set of weak learners -- a capability not possible with previous methods of assessing channel similarity."],"url":"http://arxiv.org/abs/2405.21042v1","category":"cs.LG"}
{"created":"2024-05-31 17:31:18","title":"Direct Alignment of Language Models via Quality-Aware Self-Refinement","abstract":"Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences. Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces an LLM-based reward model with the policy itself, thus obviating the need for extra memory and training time to learn the reward model. However, DPO does not consider the relative qualities of the positive and negative responses, and can lead to sub-optimal training outcomes. To alleviate this problem, we investigate the use of intrinsic knowledge within the on-the-fly fine-tuning LLM to obtain relative qualities and help to refine the loss function. Specifically, we leverage the knowledge of the LLM to design a refinement function to estimate the quality of both the positive and negative responses. We show that the constructed refinement function can help self-refine the loss function under mild assumptions. The refinement function is integrated into DPO and its variant Identity Policy Optimization (IPO). Experiments across various evaluators indicate that they can improve the performance of the fine-tuned models over DPO and IPO.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences.","Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces an LLM-based reward model with the policy itself, thus obviating the need for extra memory and training time to learn the reward model.","However, DPO does not consider the relative qualities of the positive and negative responses, and can lead to sub-optimal training outcomes.","To alleviate this problem, we investigate the use of intrinsic knowledge within the on-the-fly fine-tuning LLM to obtain relative qualities and help to refine the loss function.","Specifically, we leverage the knowledge of the LLM to design a refinement function to estimate the quality of both the positive and negative responses.","We show that the constructed refinement function can help self-refine the loss function under mild assumptions.","The refinement function is integrated into DPO and its variant Identity Policy Optimization (IPO).","Experiments across various evaluators indicate that they can improve the performance of the fine-tuned models over DPO and IPO."],"url":"http://arxiv.org/abs/2405.21040v1","category":"cs.CL"}
{"created":"2024-05-31 17:31:05","title":"Fibonacci sequence and Pythagorean triples in the composition of functions for integer solutions from certain operator","abstract":"The following article summarizes research where theorems and their respective demonstrations are postulated based on quadratic equations with special properties given by the Pythagorean triplets and the Fibonacci sequence given the second order of equations where integer solutions are found an environment in number theory and its applications to calculus.","sentences":["The following article summarizes research where theorems and their respective demonstrations are postulated based on quadratic equations with special properties given by the Pythagorean triplets and the Fibonacci sequence given the second order of equations where integer solutions are found an environment in number theory and its applications to calculus."],"url":"http://arxiv.org/abs/2405.21039v1","category":"math.GM"}
{"created":"2024-05-31 17:30:20","title":"A Multi-wavelength, Multi-epoch Monitoring Campaign of Accretion Variability in T Tauri Stars from the ODYSSEUS Survey. I. HST FUV and NUV Spectra","abstract":"The Classical T Tauri Star (CTTS) stage is a critical phase of the star and planet formation process. In an effort to better understand the mass accretion process, which can dictate further stellar evolution and planet formation, a multi-epoch, multi-wavelength photometric and spectroscopic monitoring campaign of four CTTSs (TW Hya, RU Lup, BP Tau, and GM Aur) was carried out in 2021 and 2022/2023 as part of the Outflows and Disks Around Young Stars: Synergies for the Exploration of ULYSSES Spectra (ODYSSEUS) program. Here we focus on the HST UV spectra obtained by the HST Director's Discretionary Time UV Legacy Library of Young Stars as Essential Standards (ULLYSES) program. Using accretion shock modeling, we find that all targets exhibit accretion variability, varying from short increases in accretion rate by up to a factor of 3 within 48 hours, to longer decreases in accretion rate by a factor of 2.5 over the course of 1 year. This is despite the generally consistent accretion morphology within each target. Additionally, we test empirical relationships between accretion rate and UV luminosity and find stark differences, showing that these relationships should not be used to estimate the accretion rate for individual target. Our work reinforces that future multi-epoch and simultaneous multi-wavelength studies are critical in our understanding of the accretion process in low-mass star formation.","sentences":["The Classical T Tauri Star (CTTS) stage is a critical phase of the star and planet formation process.","In an effort to better understand the mass accretion process, which can dictate further stellar evolution and planet formation, a multi-epoch, multi-wavelength photometric and spectroscopic monitoring campaign of four CTTSs (TW Hya, RU Lup, BP Tau, and GM Aur) was carried out in 2021 and 2022/2023 as part of the Outflows and Disks Around Young Stars: Synergies for the Exploration of ULYSSES Spectra (ODYSSEUS) program.","Here we focus on the HST UV spectra obtained by the HST Director's Discretionary Time UV Legacy Library of Young Stars as Essential Standards (ULLYSES) program.","Using accretion shock modeling, we find that all targets exhibit accretion variability, varying from short increases in accretion rate by up to a factor of 3 within 48 hours, to longer decreases in accretion rate by a factor of 2.5 over the course of 1 year.","This is despite the generally consistent accretion morphology within each target.","Additionally, we test empirical relationships between accretion rate and UV luminosity and find stark differences, showing that these relationships should not be used to estimate the accretion rate for individual target.","Our work reinforces that future multi-epoch and simultaneous multi-wavelength studies are critical in our understanding of the accretion process in low-mass star formation."],"url":"http://arxiv.org/abs/2405.21038v1","category":"astro-ph.SR"}
{"created":"2024-05-31 17:28:08","title":"Photoinduced Charge Transfer in Transition Metal Dichalcogenide Quantum Dots","abstract":"In this paper, a charge transfer mechanism in transition metal dichalcogenide (TMDC) quantum dots (QDs) is studied. A common dye, Rhodamine 6G (R6G), has been used as fluorescent molecule and QDs of molybdenum disulfide ($\\rm{MoS_2}$) and tungsten disulfide ($\\rm{WS_2}$) have been used as acceptors of electrons in the photo-induced charge transfer process. The MoS$_2$ and WS$_2$ QDs have been synthesized using ultrasonication method. These QDs have been characterized using transmission electron microscope (TEM), UV-Vis spectrophotometer, and fluorimeter. The TEM images of these QDs show well dispersed particles of size $~$2 nm. These QDs show intense fluorescence emission when excited with light having wavelength, $\\lambda < $ 350 nm. In presence of light, the photons help in creating charges in fluorescent dye molecule, and TMDCs QDs felicitate the charge transfer in the process. The charge transfer phenomenon has been studied using time correlated single photon counting (TCSPC) set-up, which is a time resolved fluorescence spectroscopic technique. The time resolved fluorescence study shows a drastic change in the fluorescence (FL) lifetime of R6G molecules in the presence of QDs. This decrease in FL lifetime of R6G shows that the MoS$_2$, and WS$_2$ QDs provide additional path to the photo-generated electrons in the excited state of R6G. This study can be further extended to optoelectronic devices, where charge transfer plays an important role in the device efficiency and performance.","sentences":["In this paper, a charge transfer mechanism in transition metal dichalcogenide (TMDC) quantum dots (QDs) is studied.","A common dye, Rhodamine 6G (R6G), has been used as fluorescent molecule and QDs of molybdenum disulfide ($\\rm{MoS_2}$) and tungsten disulfide ($\\rm{WS_2}$) have been used as acceptors of electrons in the photo-induced charge transfer process.","The MoS$_2$ and WS$_2$ QDs have been synthesized using ultrasonication method.","These QDs have been characterized using transmission electron microscope (TEM), UV-Vis spectrophotometer, and fluorimeter.","The TEM images of these QDs show well dispersed particles of size $~$2 nm.","These QDs show intense fluorescence emission when excited with light having wavelength, $\\lambda < $ 350 nm.","In presence of light, the photons help in creating charges in fluorescent dye molecule, and TMDCs QDs felicitate the charge transfer in the process.","The charge transfer phenomenon has been studied using time correlated single photon counting (TCSPC) set-up, which is a time resolved fluorescence spectroscopic technique.","The time resolved fluorescence study shows a drastic change in the fluorescence (FL) lifetime of R6G molecules in the presence of QDs.","This decrease in FL lifetime of R6G shows that the MoS$_2$, and WS$_2$ QDs provide additional path to the photo-generated electrons in the excited state of R6G. This study can be further extended to optoelectronic devices, where charge transfer plays an important role in the device efficiency and performance."],"url":"http://arxiv.org/abs/2405.21035v1","category":"physics.app-ph"}
{"created":"2024-05-31 17:24:37","title":"3D simulations of convective shell Neon-burning in a massive star","abstract":"The treatment of convection remains a major weakness in the modelling of stellar evolution with one-dimensional (1D) codes. The ever increasing computing power makes now possible to simulate in 3D part of a star for a fraction of its life, allowing us to study the full complexity of convective zones with hydrodynamics codes. Here, we performed state-of-the-art hydrodynamics simulations of turbulence in a neon-burning convective zone, during the late stage of the life of a massive star. We produced a set of simulations varying the resolution of the computing domain (from 1283 to 10243 cells) and the efficiency of the nuclear reactions (by boosting the energy generation rate from nominal to a factor of 1000). We analysed our results by the mean of Fourier transform of the velocity field, and mean-field decomposition of the various transport equations. Our results are in line with previous studies, showing that the behaviour of the bulk of the convective zone is already well captured at a relatively low resolution (2563), while the details of the convective boundaries require higher resolutions. The different boosting factors used show how various quantities (velocity, buoyancy, abundances, abundance variances) depend on the energy generation rate. We found that for low boosting factors, convective zones are well mixed, validating the approach usually used in 1D stellar evolution codes. However, when nuclear burning and turbulent transport occur on the same timescale, a more sophisticated treatment would be needed. This is typically the case when shell mergers occur.","sentences":["The treatment of convection remains a major weakness in the modelling of stellar evolution with one-dimensional (1D) codes.","The ever increasing computing power makes now possible to simulate in 3D part of a star for a fraction of its life, allowing us to study the full complexity of convective zones with hydrodynamics codes.","Here, we performed state-of-the-art hydrodynamics simulations of turbulence in a neon-burning convective zone, during the late stage of the life of a massive star.","We produced a set of simulations varying the resolution of the computing domain (from 1283 to 10243 cells) and the efficiency of the nuclear reactions (by boosting the energy generation rate from nominal to a factor of 1000).","We analysed our results by the mean of Fourier transform of the velocity field, and mean-field decomposition of the various transport equations.","Our results are in line with previous studies, showing that the behaviour of the bulk of the convective zone is already well captured at a relatively low resolution (2563), while the details of the convective boundaries require higher resolutions.","The different boosting factors used show how various quantities (velocity, buoyancy, abundances, abundance variances) depend on the energy generation rate.","We found that for low boosting factors, convective zones are well mixed, validating the approach usually used in 1D stellar evolution codes.","However, when nuclear burning and turbulent transport occur on the same timescale, a more sophisticated treatment would be needed.","This is typically the case when shell mergers occur."],"url":"http://arxiv.org/abs/2405.21033v1","category":"astro-ph.SR"}
{"created":"2024-05-31 17:24:20","title":"Scale hierarchies near the conifold","abstract":"We study the axio-dilaton and complex structure stabilization for a generic one-parameter Calabi-Yau (CY) compactification. We focus on near the conifold regions and consider a strongly warped metric. We analyze numerically the case of the mirror of the quintic CY. The axio-dilaton $\\tau$ and complex structure $z$ moduli are stabilized simultaneously comparing with previous results, showing that generically $z$ is heavier than $\\tau$, and the axio-dilaton can not be fixed first. This is also the case when we add the warping correction to the potential; and in general this inclusion does not destabilize the moduli. We point out why in this setup non-supersymmetric vacua are much more dense than supersymmetric vacua. We study the dependence of the vacua stabilization by addition of an $\\overline{D3}$-brane, for a fixed volume. The vacuum state masses have divergent eigenvalues near the conifold singularity that gets softened by the warping correction. Generically the considered contributions allow to find stable vacua in this setup. The scale hierarchy between 6D and 4D; arising from the warping, as stated by Giddings Kachru and Polchinski (GKP), is preserved; while the masses in the vacua acquire physically reasonable values.","sentences":["We study the axio-dilaton and complex structure stabilization for a generic one-parameter Calabi-Yau (CY) compactification.","We focus on near the conifold regions and consider a strongly warped metric.","We analyze numerically the case of the mirror of the quintic CY.","The axio-dilaton $\\tau$ and complex structure $z$ moduli are stabilized simultaneously comparing with previous results, showing that generically $z$ is heavier than $\\tau$, and the axio-dilaton can not be fixed first.","This is also the case when we add the warping correction to the potential; and in general this inclusion does not destabilize the moduli.","We point out why in this setup non-supersymmetric vacua are much more dense than supersymmetric vacua.","We study the dependence of the vacua stabilization by addition of an $\\overline{D3}$-brane, for a fixed volume.","The vacuum state masses have divergent eigenvalues near the conifold singularity that gets softened by the warping correction.","Generically the considered contributions allow to find stable vacua in this setup.","The scale hierarchy between 6D and 4D; arising from the warping, as stated by Giddings Kachru and Polchinski (GKP), is preserved; while the masses in the vacua acquire physically reasonable values."],"url":"http://arxiv.org/abs/2405.21032v1","category":"hep-th"}
{"created":"2024-05-31 17:22:39","title":"To be or not to be, but where?","abstract":"The identification of physical subsystems in quantum mechanics as compared to classical mechanics poses significant conceptual challenges, especially in the context of quantum gravity. Traditional approaches associate quantum systems with classical ones localized in spacetime, using either Hilbert space factors for finite-dimensional systems or local operator algebras in algebraic quantum field theory. These methods ensure statistical independence for state preparations and measurements. However, canonical linearized quantum gravity disrupts this framework by preventing the formation of gauge-invariant local algebras, thereby undermining the statistical independence required in measurements. This presents a major obstacle for modeling early universe cosmology, gravity-induced-entanglement experiments, and poses a significant roadblock toward a comprehensive theory of quantum gravity. A pivotal shift is proposed: the identification of classical and quantum systems should be dynamically evolving rather than static, opening the possibility of a single-world unitary quantum mechanics. This perspective aligns with the broader aim of understanding how classical spatiotemporal existence emerges from quantum mechanics and connects the measurement problem with quantum gravity.","sentences":["The identification of physical subsystems in quantum mechanics as compared to classical mechanics poses significant conceptual challenges, especially in the context of quantum gravity.","Traditional approaches associate quantum systems with classical ones localized in spacetime, using either Hilbert space factors for finite-dimensional systems or local operator algebras in algebraic quantum field theory.","These methods ensure statistical independence for state preparations and measurements.","However, canonical linearized quantum gravity disrupts this framework by preventing the formation of gauge-invariant local algebras, thereby undermining the statistical independence required in measurements.","This presents a major obstacle for modeling early universe cosmology, gravity-induced-entanglement experiments, and poses a significant roadblock toward a comprehensive theory of quantum gravity.","A pivotal shift is proposed: the identification of classical and quantum systems should be dynamically evolving rather than static, opening the possibility of a single-world unitary quantum mechanics.","This perspective aligns with the broader aim of understanding how classical spatiotemporal existence emerges from quantum mechanics and connects the measurement problem with quantum gravity."],"url":"http://arxiv.org/abs/2405.21031v1","category":"quant-ph"}
{"created":"2024-05-31 17:21:52","title":"Standards for Belief Representations in LLMs","abstract":"As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.","sentences":["As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world.","However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs.","This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like.","We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief.","Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints.","Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs.","We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations."],"url":"http://arxiv.org/abs/2405.21030v1","category":"cs.AI"}
{"created":"2024-05-31 17:20:59","title":"Table-top nanodiamond interferometer enabling quantum gravity tests","abstract":"Unifying quantum theory and general relativity is the holy grail of contemporary physics. Nonetheless, the lack of experimental evidence driving this process led to a plethora of mathematical models with a substantial impossibility of discriminating among them or even establishing if gravity really needs to be quantized or if, vice versa, quantum mechanics must be \"gravitized\" at some scale. Recently, it has been proposed that the observation of the generation of entanglement by gravitational interaction, could represent a breakthrough demonstrating the quantum nature of gravity. A few experimental proposals have been advanced in this sense, but the extreme technological requirements (e.g., the need for free-falling gravitationally-interacting masses in a quantum superposition state) make their implementation still far ahead. Here we present a feasibility study for a table-top nanodiamond-based interferometer eventually enabling easier and less resource-demanding quantum gravity tests. With respect to the aforementioned proposals, by relying on quantum superpositions of steady massive (mesoscopic) objects our interferometer may allow exploiting just small-range electromagnetic fields (much easier to implement and control) and, at the same time, the re-utilization of the massive quantum probes exploited, inevitably lost in free-falling interferometric schemes.","sentences":["Unifying quantum theory and general relativity is the holy grail of contemporary physics.","Nonetheless, the lack of experimental evidence driving this process led to a plethora of mathematical models with a substantial impossibility of discriminating among them or even establishing if gravity really needs to be quantized or if, vice versa, quantum mechanics must be \"gravitized\" at some scale.","Recently, it has been proposed that the observation of the generation of entanglement by gravitational interaction, could represent a breakthrough demonstrating the quantum nature of gravity.","A few experimental proposals have been advanced in this sense, but the extreme technological requirements (e.g., the need for free-falling gravitationally-interacting masses in a quantum superposition state) make their implementation still far ahead.","Here we present a feasibility study for a table-top nanodiamond-based interferometer eventually enabling easier and less resource-demanding quantum gravity tests.","With respect to the aforementioned proposals, by relying on quantum superpositions of steady massive (mesoscopic) objects our interferometer may allow exploiting just small-range electromagnetic fields (much easier to implement and control) and, at the same time, the re-utilization of the massive quantum probes exploited, inevitably lost in free-falling interferometric schemes."],"url":"http://arxiv.org/abs/2405.21029v1","category":"quant-ph"}
{"created":"2024-05-31 17:16:38","title":"LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models","abstract":"When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying \"I don't know\") for answers that are likely wrong.","sentences":["When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct.","This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge.","For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence.","To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener.","We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener.","We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t.","a simulated listener.","Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers.","Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA.","Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples.","Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details.","Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying \"I don't know\") for answers that are likely wrong."],"url":"http://arxiv.org/abs/2405.21028v1","category":"cs.CL"}
{"created":"2024-05-31 17:16:29","title":"Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles","abstract":"For solving zero-sum games involving non-transitivity, a common approach is to maintain population policies to approximate the Nash Equilibrium (NE). Previous research has shown that the Policy Space Response Oracle (PSRO) is an effective multi-agent reinforcement learning framework for these games. However, repeatedly training new policies from scratch to approximate the Best Response (BR) to opponents' mixed policies at each iteration is inefficient and costly. While some PSRO methods initialize a new BR policy by inheriting from past BR policies, this approach limits the exploration of new policies, especially against challenging opponents.To address this issue, we propose Fusion-PSRO, which uses model fusion to initialize the policy for better approximation to BR. With Top-k probabilities from NE, we select high-quality base policies and fuse them into a new BR policy through model averaging. This approach allows the initialized policy to incorporate multiple expert policies, making it easier to handle difficult opponents compared to inheriting or initializing from scratch. Additionally, our method only modifies the policy initialization, enabling its application to nearly all PSRO variants without additional training overhead.Our experiments with non-transitive matrix games, Leduc poker, and the more complex Liars Dice demonstrate that Fusion-PSRO enhances the performance of nearly all PSRO variants, achieving lower exploitability.","sentences":["For solving zero-sum games involving non-transitivity, a common approach is to maintain population policies to approximate the Nash Equilibrium (NE).","Previous research has shown that the Policy Space Response Oracle (PSRO) is an effective multi-agent reinforcement learning framework for these games.","However, repeatedly training new policies from scratch to approximate the Best Response (BR) to opponents' mixed policies at each iteration is inefficient and costly.","While some PSRO methods initialize a new BR policy by inheriting from past BR policies, this approach limits the exploration of new policies, especially against challenging opponents.","To address this issue, we propose Fusion-PSRO, which uses model fusion to initialize the policy for better approximation to BR.","With Top-k probabilities from NE, we select high-quality base policies and fuse them into a new BR policy through model averaging.","This approach allows the initialized policy to incorporate multiple expert policies, making it easier to handle difficult opponents compared to inheriting or initializing from scratch.","Additionally, our method only modifies the policy initialization, enabling its application to nearly all PSRO variants without additional training overhead.","Our experiments with non-transitive matrix games, Leduc poker, and the more complex Liars Dice demonstrate that Fusion-PSRO enhances the performance of nearly all PSRO variants, achieving lower exploitability."],"url":"http://arxiv.org/abs/2405.21027v1","category":"cs.GT"}
{"created":"2024-05-31 17:13:44","title":"On reduction and parameter recovering of Petri's cycloids","abstract":"Cycloids are particular Petri nets for modelling processes of actions and events, belonging to the fundaments of Petri's general systems theory. Defined by four parameters they provide an algebraic formalism to describe strongly synchronized sequential processes. To further investigate their structure, reduction systems of cycloids are defined in the style of rewriting systems and properties of reduced cycloids are proved. In particular the recovering of cycloid parameters from their Petri net structure is derived.","sentences":["Cycloids are particular Petri nets for modelling processes of actions and events, belonging to the fundaments of Petri's general systems theory.","Defined by four parameters they provide an algebraic formalism to describe strongly synchronized sequential processes.","To further investigate their structure, reduction systems of cycloids are defined in the style of rewriting systems and properties of reduced cycloids are proved.","In particular the recovering of cycloid parameters from their Petri net structure is derived."],"url":"http://arxiv.org/abs/2405.21025v1","category":"cs.DC"}
{"created":"2024-05-31 17:13:32","title":"Programming evolution of geometry in shape-morphing sheets via spatiotemporal activation","abstract":"Shape-programmed sheets morph from one surface into another upon activation by stimuli such as illumination, and have attracted much interest for their potential engineering applications, especially in soft robotics. Complex shape changes can be achieved by patterning a simple local active deformation (e.g. isotropic swelling), to generate differential growth. Usually the material itself is designed $\\unicode{x2014}$ for example by patterning a molecular director $\\unicode{x2014}$ such that a particular shape change occurs upon exposure to a spatially uniform stimulus. A limitation of this paradigm is that typically only one target geometry can be attained as the stimulus is adjusted. Here we show that this limitation can be overcome by patterning the stimulus itself, thereby exercising spatiotemporal control over local deformation magnitudes. Thus a single physical sample can be induced to traverse a continuous family of target geometries, opening the door to precise shape adjustments, new functionalities, and designable non-reciprocal loops in shape space. We illustrate these possibilities with examples including active parabolic reflectors, chiral flow guides, and bending channels. Finding the necessary patterns of activation involves solving families of metric inverse problems; we solve these by reduction to ODEs in an axisymmetric setting, then present a novel numerical scheme to solve them in generality.","sentences":["Shape-programmed sheets morph from one surface into another upon activation by stimuli such as illumination, and have attracted much interest for their potential engineering applications, especially in soft robotics.","Complex shape changes can be achieved by patterning a simple local active deformation (e.g. isotropic swelling), to generate differential growth.","Usually the material itself is designed $\\unicode{x2014}$ for example by patterning a molecular director $\\unicode{x2014}$ such that a particular shape change occurs upon exposure to a spatially uniform stimulus.","A limitation of this paradigm is that typically only one target geometry can be attained as the stimulus is adjusted.","Here we show that this limitation can be overcome by patterning the stimulus itself, thereby exercising spatiotemporal control over local deformation magnitudes.","Thus a single physical sample can be induced to traverse a continuous family of target geometries, opening the door to precise shape adjustments, new functionalities, and designable non-reciprocal loops in shape space.","We illustrate these possibilities with examples including active parabolic reflectors, chiral flow guides, and bending channels.","Finding the necessary patterns of activation involves solving families of metric inverse problems; we solve these by reduction to ODEs in an axisymmetric setting, then present a novel numerical scheme to solve them in generality."],"url":"http://arxiv.org/abs/2405.21024v1","category":"cond-mat.soft"}
{"created":"2024-05-31 17:11:39","title":"Compact Optimality Verification for Optimization Proxies","abstract":"Recent years have witnessed increasing interest in optimization proxies, i.e., machine learning models that approximate the input-output mapping of parametric optimization problems and return near-optimal feasible solutions. Following recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper reconsiders the optimality verification problem for optimization proxies, i.e., the determination of the worst-case optimality gap over the instance distribution. The paper proposes a compact formulation for optimality verification and a gradient-based primal heuristic that brings substantial computational benefits to the original formulation. The compact formulation is also more general and applies to non-convex optimization problems. The benefits of the compact formulation are demonstrated on large-scale DC Optimal Power Flow and knapsack problems.","sentences":["Recent years have witnessed increasing interest in optimization proxies, i.e., machine learning models that approximate the input-output mapping of parametric optimization problems and return near-optimal feasible solutions.","Following recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper reconsiders the optimality verification problem for optimization proxies, i.e., the determination of the worst-case optimality gap over the instance distribution.","The paper proposes a compact formulation for optimality verification and a gradient-based primal heuristic that brings substantial computational benefits to the original formulation.","The compact formulation is also more general and applies to non-convex optimization problems.","The benefits of the compact formulation are demonstrated on large-scale DC Optimal Power Flow and knapsack problems."],"url":"http://arxiv.org/abs/2405.21023v1","category":"math.OC"}
{"created":"2024-05-31 17:09:16","title":"You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet","abstract":"Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.","sentences":["Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed.","However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning.","In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies.","This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.","We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence.","Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios.","Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling."],"url":"http://arxiv.org/abs/2405.21022v1","category":"cs.CL"}
{"created":"2024-05-31 17:06:14","title":"Generating Triangulations and Fibrations with Reinforcement Learning","abstract":"We apply reinforcement learning (RL) to generate fine regular star triangulations of reflexive polytopes, that give rise to smooth Calabi-Yau (CY) hypersurfaces. We demonstrate that, by simple modifications to the data encoding and reward function, one can search for CYs that satisfy a set of desirable string compactification conditions. For instance, we show that our RL algorithm can generate triangulations together with holomorphic vector bundles that satisfy anomaly cancellation and poly-stability conditions in heterotic compactification. Furthermore, we show that our algorithm can be used to search for reflexive subpolytopes together with compatible triangulations that define fibration structures of the CYs.","sentences":["We apply reinforcement learning (RL) to generate fine regular star triangulations of reflexive polytopes, that give rise to smooth Calabi-Yau (CY) hypersurfaces.","We demonstrate that, by simple modifications to the data encoding and reward function, one can search for CYs that satisfy a set of desirable string compactification conditions.","For instance, we show that our RL algorithm can generate triangulations together with holomorphic vector bundles that satisfy anomaly cancellation and poly-stability conditions in heterotic compactification.","Furthermore, we show that our algorithm can be used to search for reflexive subpolytopes together with compatible triangulations that define fibration structures of the CYs."],"url":"http://arxiv.org/abs/2405.21017v1","category":"hep-th"}
{"created":"2024-05-31 17:05:59","title":"MpoxSLDNet: A Novel CNN Model for Detecting Monkeypox Lesions and Performance Comparison with Pre-trained Models","abstract":"Monkeypox virus (MPXV) is a zoonotic virus that poses a significant threat to public health, particularly in remote parts of Central and West Africa. Early detection of monkeypox lesions is crucial for effective treatment. However, due to its similarity with other skin diseases, monkeypox lesion detection is a challenging task. To detect monkeypox, many researchers used various deep-learning models such as MobileNetv2, VGG16, ResNet50, InceptionV3, DenseNet121, EfficientNetB3, MobileNetV2, and Xception. However, these models often require high storage space due to their large size. This study aims to improve the existing challenges by introducing a CNN model named MpoxSLDNet (Monkeypox Skin Lesion Detector Network) to facilitate early detection and categorization of Monkeypox lesions and Non-Monkeypox lesions in digital images. Our model represents a significant advancement in the field of monkeypox lesion detection by offering superior performance metrics, including precision, recall, F1-score, accuracy, and AUC, compared to traditional pre-trained models such as VGG16, ResNet50, and DenseNet121. The key novelty of our approach lies in MpoxSLDNet's ability to achieve high detection accuracy while requiring significantly less storage space than existing models. By addressing the challenge of high storage requirements, MpoxSLDNet presents a practical solution for early detection and categorization of monkeypox lesions in resource-constrained healthcare settings. In this study, we have used \"Monkeypox Skin Lesion Dataset\" comprising 1428 skin images of monkeypox lesions and 1764 skin images of Non-Monkeypox lesions. Dataset's limitations could potentially impact the model's ability to generalize to unseen cases. However, the MpoxSLDNet model achieved a validation accuracy of 94.56%, compared to 86.25%, 84.38%, and 67.19% for VGG16, DenseNet121, and ResNet50, respectively.","sentences":["Monkeypox virus (MPXV) is a zoonotic virus that poses a significant threat to public health, particularly in remote parts of Central and West Africa.","Early detection of monkeypox lesions is crucial for effective treatment.","However, due to its similarity with other skin diseases, monkeypox lesion detection is a challenging task.","To detect monkeypox, many researchers used various deep-learning models such as MobileNetv2, VGG16, ResNet50, InceptionV3, DenseNet121, EfficientNetB3, MobileNetV2, and Xception.","However, these models often require high storage space due to their large size.","This study aims to improve the existing challenges by introducing a CNN model named MpoxSLDNet (Monkeypox Skin Lesion Detector Network) to facilitate early detection and categorization of Monkeypox lesions and Non-Monkeypox lesions in digital images.","Our model represents a significant advancement in the field of monkeypox lesion detection by offering superior performance metrics, including precision, recall, F1-score, accuracy, and AUC, compared to traditional pre-trained models such as VGG16, ResNet50, and DenseNet121.","The key novelty of our approach lies in MpoxSLDNet's ability to achieve high detection accuracy while requiring significantly less storage space than existing models.","By addressing the challenge of high storage requirements, MpoxSLDNet presents a practical solution for early detection and categorization of monkeypox lesions in resource-constrained healthcare settings.","In this study, we have used \"Monkeypox Skin Lesion Dataset\" comprising 1428 skin images of monkeypox lesions and 1764 skin images of Non-Monkeypox lesions.","Dataset's limitations could potentially impact the model's ability to generalize to unseen cases.","However, the MpoxSLDNet model achieved a validation accuracy of 94.56%, compared to 86.25%, 84.38%, and 67.19% for VGG16, DenseNet121, and ResNet50, respectively."],"url":"http://arxiv.org/abs/2405.21016v1","category":"cs.CV"}
{"created":"2024-05-31 16:55:04","title":"StrucTexTv3: An Efficient Vision-Language Model for Text-rich Image Perception, Comprehension, and Beyond","abstract":"Text-rich images have significant and extensive value, deeply integrated into various aspects of human life. Notably, both visual cues and linguistic symbols in text-rich images play crucial roles in information transmission but are accompanied by diverse challenges. Therefore, the efficient and effective understanding of text-rich images is a crucial litmus test for the capability of Vision-Language Models. We have crafted an efficient vision-language model, StrucTexTv3, tailored to tackle various intelligent tasks for text-rich images. The significant design of StrucTexTv3 is presented in the following aspects: Firstly, we adopt a combination of an effective multi-scale reduced visual transformer and a multi-granularity token sampler (MG-Sampler) as a visual token generator, successfully solving the challenges of high-resolution input and complex representation learning for text-rich images. Secondly, we enhance the perception and comprehension abilities of StrucTexTv3 through instruction learning, seamlessly integrating various text-oriented tasks into a unified framework. Thirdly, we have curated a comprehensive collection of high-quality text-rich images, abbreviated as TIM-30M, encompassing diverse scenarios like incidental scenes, office documents, web pages, and screenshots, thereby improving the robustness of our model. Our method achieved SOTA results in text-rich image perception tasks, and significantly improved performance in comprehension tasks. Among multimodal models with LLM decoder of approximately 1.8B parameters, it stands out as a leader, which also makes the deployment of edge devices feasible. In summary, the StrucTexTv3 model, featuring efficient structural design, outstanding performance, and broad adaptability, offers robust support for diverse intelligent application tasks involving text-rich images, thus exhibiting immense potential for widespread application.","sentences":["Text-rich images have significant and extensive value, deeply integrated into various aspects of human life.","Notably, both visual cues and linguistic symbols in text-rich images play crucial roles in information transmission but are accompanied by diverse challenges.","Therefore, the efficient and effective understanding of text-rich images is a crucial litmus test for the capability of Vision-Language Models.","We have crafted an efficient vision-language model, StrucTexTv3, tailored to tackle various intelligent tasks for text-rich images.","The significant design of StrucTexTv3 is presented in the following aspects: Firstly, we adopt a combination of an effective multi-scale reduced visual transformer and a multi-granularity token sampler (MG-Sampler) as a visual token generator, successfully solving the challenges of high-resolution input and complex representation learning for text-rich images.","Secondly, we enhance the perception and comprehension abilities of StrucTexTv3 through instruction learning, seamlessly integrating various text-oriented tasks into a unified framework.","Thirdly, we have curated a comprehensive collection of high-quality text-rich images, abbreviated as TIM-30M, encompassing diverse scenarios like incidental scenes, office documents, web pages, and screenshots, thereby improving the robustness of our model.","Our method achieved SOTA results in text-rich image perception tasks, and significantly improved performance in comprehension tasks.","Among multimodal models with LLM decoder of approximately 1.8B parameters, it stands out as a leader, which also makes the deployment of edge devices feasible.","In summary, the StrucTexTv3 model, featuring efficient structural design, outstanding performance, and broad adaptability, offers robust support for diverse intelligent application tasks involving text-rich images, thus exhibiting immense potential for widespread application."],"url":"http://arxiv.org/abs/2405.21013v1","category":"cs.CV"}
{"created":"2024-05-31 16:52:27","title":"Nash states versus eigenstates for many-body quantum systems","abstract":"Eigenstates of observables such as the Hamiltonian play a central role in quantum mechanics. Inspired by the pure Nash equilibria that arise in classical game theory, we propose ''Nash states'' of multiple observables as a generalization of eigenstates of single observables. This generalization is mathematically natural for many-body quantum systems, which possess an intrinsic tensor product structure. Every set of observables gives rise to algebraic varieties of Nash state vectors that we call ''Nash varieties''. We present analytical and numerical results on the existence of Nash states and on the geometry of Nash varieties. We relate these ideas to earlier, pioneering work on the Nash equilibria of few-body quantum games and discuss connections to the variational minimization of local Hamiltonians.","sentences":["Eigenstates of observables such as the Hamiltonian play a central role in quantum mechanics.","Inspired by the pure Nash equilibria that arise in classical game theory, we propose ''Nash states'' of multiple observables as a generalization of eigenstates of single observables.","This generalization is mathematically natural for many-body quantum systems, which possess an intrinsic tensor product structure.","Every set of observables gives rise to algebraic varieties of Nash state vectors that we call ''Nash varieties''.","We present analytical and numerical results on the existence of Nash states and on the geometry of Nash varieties.","We relate these ideas to earlier, pioneering work on the Nash equilibria of few-body quantum games and discuss connections to the variational minimization of local Hamiltonians."],"url":"http://arxiv.org/abs/2405.21011v1","category":"quant-ph"}
{"created":"2024-05-31 16:47:15","title":"Continuation of Bianchi Spacetimes Through The Big Bang","abstract":"In this paper we present a framework in which the relational description of General Relativity can be used to smoothly continue cosmological dynamical systems through the Big Bang without invoking quantum gravity effects. Cosmological spacetimes contain as a key dynamical variable a notion of scale through the volume factor $\\nu$. However no cosmological observer is ever able to separate their measuring apparatus from the system they are measuring, in that sense every measurement is a relative one and measurable dynamical variables are in fact dimensionless ratios. This is manifest in the identification of a scaling symmetry or ``Dynamical Similarity\" in the Einstein-Hilbert action associated with the volume factor. By quotienting out this scaling symmetry, we form a relational system defined on a contact manifold whose dynamical variables are decoupled from scale. When the phase space is reduced to shape space, we show that there exist unique solutions to the equations of motion that pass smoothly through the initial cosmological singularity in flat FLRW, Bianchi I and Quiescent Bianchi IX cosmologies.","sentences":["In this paper we present a framework in which the relational description of General Relativity can be used to smoothly continue cosmological dynamical systems through the Big Bang without invoking quantum gravity effects.","Cosmological spacetimes contain as a key dynamical variable a notion of scale through the volume factor $\\nu$. However no cosmological observer is ever able to separate their measuring apparatus from the system they are measuring, in that sense every measurement is a relative one and measurable dynamical variables are in fact dimensionless ratios.","This is manifest in the identification of a scaling symmetry or ``Dynamical Similarity\" in the Einstein-Hilbert action associated with the volume factor.","By quotienting out this scaling symmetry, we form a relational system defined on a contact manifold whose dynamical variables are decoupled from scale.","When the phase space is reduced to shape space, we show that there exist unique solutions to the equations of motion that pass smoothly through the initial cosmological singularity in flat FLRW, Bianchi I and Quiescent Bianchi IX cosmologies."],"url":"http://arxiv.org/abs/2405.21008v1","category":"gr-qc"}
{"created":"2024-05-31 16:44:40","title":"Explaining Predictions by Characteristic Rules","abstract":"Characteristic rules have been advocated for their ability to improve interpretability over discriminative rules within the area of rule learning. However, the former type of rule has not yet been used by techniques for explaining predictions. A novel explanation technique, called CEGA (Characteristic Explanatory General Association rules), is proposed, which employs association rule mining to aggregate multiple explanations generated by any standard local explanation technique into a set of characteristic rules. An empirical investigation is presented, in which CEGA is compared to two state-of-the-art methods, Anchors and GLocalX, for producing local and aggregated explanations in the form of discriminative rules. The results suggest that the proposed approach provides a better trade-off between fidelity and complexity compared to the two state-of-the-art approaches; CEGA and Anchors significantly outperform GLocalX with respect to fidelity, while CEGA and GLocalX significantly outperform Anchors with respect to the number of generated rules. The effect of changing the format of the explanations of CEGA to discriminative rules and using LIME and SHAP as local explanation techniques instead of Anchors are also investigated. The results show that the characteristic explanatory rules still compete favorably with rules in the standard discriminative format. The results also indicate that using CEGA in combination with either SHAP or Anchors consistently leads to a higher fidelity compared to using LIME as the local explanation technique.","sentences":["Characteristic rules have been advocated for their ability to improve interpretability over discriminative rules within the area of rule learning.","However, the former type of rule has not yet been used by techniques for explaining predictions.","A novel explanation technique, called CEGA (Characteristic Explanatory General Association rules), is proposed, which employs association rule mining to aggregate multiple explanations generated by any standard local explanation technique into a set of characteristic rules.","An empirical investigation is presented, in which CEGA is compared to two state-of-the-art methods, Anchors and GLocalX, for producing local and aggregated explanations in the form of discriminative rules.","The results suggest that the proposed approach provides a better trade-off between fidelity and complexity compared to the two state-of-the-art approaches; CEGA and Anchors significantly outperform GLocalX with respect to fidelity, while CEGA and GLocalX significantly outperform Anchors with respect to the number of generated rules.","The effect of changing the format of the explanations of CEGA to discriminative rules and using LIME and SHAP as local explanation techniques instead of Anchors are also investigated.","The results show that the characteristic explanatory rules still compete favorably with rules in the standard discriminative format.","The results also indicate that using CEGA in combination with either SHAP or Anchors consistently leads to a higher fidelity compared to using LIME as the local explanation technique."],"url":"http://arxiv.org/abs/2405.21003v1","category":"cs.LG"}
{"created":"2024-05-31 16:40:08","title":"Axial HoloTile: Extended Depth-of-Focus of Dynamic Holographic Light Projections","abstract":"This publication extends the HoloTile framework to three dimensions, introducing the ability to generate arbitrary dynamic patterns composed of extended depth-of-field non-diffractive beamlets with theoretically 100% diffraction efficiency. In particular, we demonstrate experimentally the generation of speckle-reduced reconstruction patterns, consisting of spatially multiplexed extended Bessel-like beamlets, implemented on a phase-only spatial light modulator (SLM). Due to the inherent separation of the tiled subhologram and the point spread function shaping hologram in HoloTile, we show that the reconstruction amplitude can be expressed as a simple convolution of the contributions from the two holograms. This results in a discretely sampled reconstruction, with each spatial frequency component exhibiting long DoF with characteristic Bessel beam properties. This separation facilitates spatial and temporal multiplexing of both contributions, and allows for real-time dynamic patterning with extended DoF. Additionally, a geometric analysis is included, allowing for the direct calculation of the propagation characteristics of the beamlets.","sentences":["This publication extends the HoloTile framework to three dimensions, introducing the ability to generate arbitrary dynamic patterns composed of extended depth-of-field non-diffractive beamlets with theoretically 100% diffraction efficiency.","In particular, we demonstrate experimentally the generation of speckle-reduced reconstruction patterns, consisting of spatially multiplexed extended Bessel-like beamlets, implemented on a phase-only spatial light modulator (SLM).","Due to the inherent separation of the tiled subhologram and the point spread function shaping hologram in HoloTile, we show that the reconstruction amplitude can be expressed as a simple convolution of the contributions from the two holograms.","This results in a discretely sampled reconstruction, with each spatial frequency component exhibiting long DoF with characteristic Bessel beam properties.","This separation facilitates spatial and temporal multiplexing of both contributions, and allows for real-time dynamic patterning with extended DoF.","Additionally, a geometric analysis is included, allowing for the direct calculation of the propagation characteristics of the beamlets."],"url":"http://arxiv.org/abs/2405.20997v1","category":"physics.optics"}
{"created":"2024-05-31 16:39:01","title":"Revisiting quantum field theory in Rindler spacetime with superselection rules","abstract":"Quantum field theory (QFT) in Rindler spacetime is a gateway to understanding unitarity and information loss paradoxes in curved spacetime. Rindler coordinates map Minkowski spacetime onto regions with horizons, effectively dividing accelerated observers into causally disconnected sectors. Employing standard quantum field theory techniques and Bogoliubov transformations between Minkowski and Rindler coordinates yields entanglement between states across these causally separated regions of spacetime. This results in a breakdown of unitarity, implying that information regarding the entangled partner may be irretrievably lost beyond the Rindler horizon. As a consequence, one has a situation of pure states evolving into mixed states. In this paper, we introduce a novel framework for comprehending this phenomenon using a recently proposed formulation of direct-sum quantum field theory (DQFT), which is grounded in superselection rules formulated by the parity and time reversal ($\\mathcal{P}\\mathcal{T}$) symmetry of Minkowski spacetime. In the context of DQFT applied to Rindler spacetime, we demonstrate that each Rindler observer can, in principle, access pure states within the horizon, thereby restoring unitarity. However, our analysis also reveals the emergence of a thermal spectrum of Unruh radiation. This prompts a reevaluation of entanglement in Rindler spacetime, where we propose a novel perspective on how Rindler observers may reconstruct complementary information beyond the horizon. Furthermore, we revisit the implications of the Reeh-Schlieder theorem within the framework of DQFT. Lastly, we underscore how our findings contribute to ongoing efforts aimed at elucidating the role of unitarity in quantum field theory within the context of de Sitter and black hole spacetimes.","sentences":["Quantum field theory (QFT) in Rindler spacetime is a gateway to understanding unitarity and information loss paradoxes in curved spacetime.","Rindler coordinates map Minkowski spacetime onto regions with horizons, effectively dividing accelerated observers into causally disconnected sectors.","Employing standard quantum field theory techniques and Bogoliubov transformations between Minkowski and Rindler coordinates yields entanglement between states across these causally separated regions of spacetime.","This results in a breakdown of unitarity, implying that information regarding the entangled partner may be irretrievably lost beyond the Rindler horizon.","As a consequence, one has a situation of pure states evolving into mixed states.","In this paper, we introduce a novel framework for comprehending this phenomenon using a recently proposed formulation of direct-sum quantum field theory (DQFT), which is grounded in superselection rules formulated by the parity and time reversal ($\\mathcal{P}\\mathcal{T}$) symmetry of Minkowski spacetime.","In the context of DQFT applied to Rindler spacetime, we demonstrate that each Rindler observer can, in principle, access pure states within the horizon, thereby restoring unitarity.","However, our analysis also reveals the emergence of a thermal spectrum of Unruh radiation.","This prompts a reevaluation of entanglement in Rindler spacetime, where we propose a novel perspective on how Rindler observers may reconstruct complementary information beyond the horizon.","Furthermore, we revisit the implications of the Reeh-Schlieder theorem within the framework of DQFT.","Lastly, we underscore how our findings contribute to ongoing efforts aimed at elucidating the role of unitarity in quantum field theory within the context of de Sitter and black hole spacetimes."],"url":"http://arxiv.org/abs/2405.20995v1","category":"gr-qc"}
{"created":"2024-05-31 16:38:35","title":"Information limits and Thouless-Anderson-Palmer equations for spiked matrix models with structured noise","abstract":"We consider a prototypical problem of Bayesian inference for a structured spiked model: a low-rank signal is corrupted by additive noise. While both information-theoretic and algorithmic limits are well understood when the noise is i.i.d. Gaussian, the more realistic case of structured noise still proves to be challenging. To capture the structure while maintaining mathematical tractability, a line of work has focused on rotationally invariant noise. However, existing studies either provide sub-optimal algorithms or they are limited to a special class of noise ensembles. In this paper, we establish the first characterization of the information-theoretic limits for a noise matrix drawn from a general trace ensemble. These limits are then achieved by an efficient algorithm inspired by the theory of adaptive Thouless-Anderson-Palmer (TAP) equations. Our approach leverages tools from statistical physics (replica method) and random matrix theory (generalized spherical integrals), and it unveils the equivalence between the rotationally invariant model and a surrogate Gaussian model.","sentences":["We consider a prototypical problem of Bayesian inference for a structured spiked model: a low-rank signal is corrupted by additive noise.","While both information-theoretic and algorithmic limits are well understood when the noise is i.i.d.","Gaussian, the more realistic case of structured noise still proves to be challenging.","To capture the structure while maintaining mathematical tractability, a line of work has focused on rotationally invariant noise.","However, existing studies either provide sub-optimal algorithms or they are limited to a special class of noise ensembles.","In this paper, we establish the first characterization of the information-theoretic limits for a noise matrix drawn from a general trace ensemble.","These limits are then achieved by an efficient algorithm inspired by the theory of adaptive Thouless-Anderson-Palmer (TAP) equations.","Our approach leverages tools from statistical physics (replica method) and random matrix theory (generalized spherical integrals), and it unveils the equivalence between the rotationally invariant model and a surrogate Gaussian model."],"url":"http://arxiv.org/abs/2405.20993v1","category":"cs.IT"}
{"created":"2024-05-31 16:35:41","title":"Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models","abstract":"Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges. To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems. However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models. Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets. This work explores the potential of VLMs in detecting hard cases in autonomous driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels. We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models. Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT. We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets. The code is accessible at https://github.com/KTH-RPL/Detect_VLM.","sentences":["Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges.","To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems.","However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models.","Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets.","This work explores the potential of VLMs in detecting hard cases in autonomous driving.","We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels.","We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models.","Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT.","We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets.","The code is accessible at https://github.com/KTH-RPL/Detect_VLM."],"url":"http://arxiv.org/abs/2405.20991v1","category":"cs.CV"}
{"created":"2024-05-31 16:35:29","title":"Locking Machine Learning Models into Hardware","abstract":"Modern Machine Learning models are expensive IP and business competitiveness often depends on keeping this IP confidential. This in turn restricts how these models are deployed -- for example it is unclear how to deploy a model on-device without inevitably leaking the underlying model. At the same time, confidential computing technologies such as Multi-Party Computation or Homomorphic encryption remain impractical for wide adoption. In this paper we take a different approach and investigate feasibility of ML-specific mechanisms that deter unauthorized model use by restricting the model to only be usable on specific hardware, making adoption on unauthorized hardware inconvenient. That way, even if IP is compromised, it cannot be trivially used without specialised hardware or major model adjustment. In a sense, we seek to enable cheap locking of machine learning models into specific hardware. We demonstrate that locking mechanisms are feasible by either targeting efficiency of model representations, such making models incompatible with quantisation, or tie the model's operation on specific characteristics of hardware, such as number of cycles for arithmetic operations. We demonstrate that locking comes with negligible work and latency overheads, while significantly restricting usability of the resultant model on unauthorized hardware.","sentences":["Modern Machine Learning models are expensive IP and business competitiveness often depends on keeping this IP confidential.","This in turn restricts how these models are deployed -- for example it is unclear how to deploy a model on-device without inevitably leaking the underlying model.","At the same time, confidential computing technologies such as Multi-Party Computation or Homomorphic encryption remain impractical for wide adoption.","In this paper we take a different approach and investigate feasibility of ML-specific mechanisms that deter unauthorized model use by restricting the model to only be usable on specific hardware, making adoption on unauthorized hardware inconvenient.","That way, even if IP is compromised, it cannot be trivially used without specialised hardware or major model adjustment.","In a sense, we seek to enable cheap locking of machine learning models into specific hardware.","We demonstrate that locking mechanisms are feasible by either targeting efficiency of model representations, such making models incompatible with quantisation, or tie the model's operation on specific characteristics of hardware, such as number of cycles for arithmetic operations.","We demonstrate that locking comes with negligible work and latency overheads, while significantly restricting usability of the resultant model on unauthorized hardware."],"url":"http://arxiv.org/abs/2405.20990v1","category":"cs.CR"}
{"created":"2024-05-31 16:33:20","title":"Early Stopping Criteria for Training Generative Adversarial Networks in Biomedical Imaging","abstract":"Generative Adversarial Networks (GANs) have high computational costs to train their complex architectures. Throughout the training process, GANs' output is analyzed qualitatively based on the loss and synthetic images' diversity and quality. Based on this qualitative analysis, training is manually halted once the desired synthetic images are generated. By utilizing an early stopping criterion, the computational cost and dependence on manual oversight can be reduced yet impacted by training problems such as mode collapse, non-convergence, and instability. This is particularly prevalent in biomedical imagery, where training problems degrade the diversity and quality of synthetic images, and the high computational cost associated with training makes complex architectures increasingly inaccessible. This work proposes a novel early stopping criteria to quantitatively detect training problems, halt training, and reduce the computational costs associated with synthesizing biomedical images. Firstly, the range of generator and discriminator loss values is investigated to assess whether mode collapse, non-convergence, and instability occur sequentially, concurrently, or interchangeably throughout the training of GANs. Secondly, utilizing these occurrences in conjunction with the Mean Structural Similarity Index (MS-SSIM) and Fr\\'echet Inception Distance (FID) scores of synthetic images forms the basis of the proposed early stopping criteria. This work helps identify the occurrence of training problems in GANs using low-resource computational cost and reduces training time to generate diversified and high-quality synthetic images.","sentences":["Generative Adversarial Networks (GANs) have high computational costs to train their complex architectures.","Throughout the training process, GANs' output is analyzed qualitatively based on the loss and synthetic images' diversity and quality.","Based on this qualitative analysis, training is manually halted once the desired synthetic images are generated.","By utilizing an early stopping criterion, the computational cost and dependence on manual oversight can be reduced yet impacted by training problems such as mode collapse, non-convergence, and instability.","This is particularly prevalent in biomedical imagery, where training problems degrade the diversity and quality of synthetic images, and the high computational cost associated with training makes complex architectures increasingly inaccessible.","This work proposes a novel early stopping criteria to quantitatively detect training problems, halt training, and reduce the computational costs associated with synthesizing biomedical images.","Firstly, the range of generator and discriminator loss values is investigated to assess whether mode collapse, non-convergence, and instability occur sequentially, concurrently, or interchangeably throughout the training of GANs.","Secondly, utilizing these occurrences in conjunction with the Mean Structural Similarity Index (MS-SSIM) and Fr\\'echet Inception Distance (FID) scores of synthetic images forms the basis of the proposed early stopping criteria.","This work helps identify the occurrence of training problems in GANs using low-resource computational cost and reduces training time to generate diversified and high-quality synthetic images."],"url":"http://arxiv.org/abs/2405.20987v1","category":"cs.CV"}
{"created":"2024-05-31 16:31:38","title":"DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models","abstract":"The visual projector, which bridges the vision and language modalities and facilitates cross-modal alignment, serves as a crucial component in MLLMs. However, measuring the effectiveness of projectors in vision-language alignment remains under-explored, which currently can only be inferred from the performance of MLLMs on downstream tasks. Motivated by the problem, this study examines the projector module by interpreting the vision-language semantic flow within MLLMs. Specifically, we trace back the semantic relevance flow from generated language tokens to raw visual encoder patches and the intermediate outputs produced by projectors. Our findings reveal that compressive projectors (e.g., QFormer), abstract visual patches into a limited set of semantic concepts, such as objects or attributes, resulting in a 'double abstraction' phenomenon. This involves a first visual semantic abstraction by the projector referring to pre-defined query tokens, and a second extraction by the LLM based on text instructions. The double abstraction is inefficient in training and will result in cumulative vision semantics deficiency. To mitigate this issue, we propose the key insight of 'Decouple Compression from Abstraction (DeCo), that is compressing the visual token number at the patch level by projectors and allowing the LLM to handle visual semantic abstraction entirely. Consequently, we adopt a simple compressor, i.e., 2D Adaptive Pooling, to downsample visual patches in a parameter-free manner. Empirical evaluation demonstrates that DeCo surpasses traditional compressive projectors regarding both performance and efficiency. It achieves performance gains of 0.9%, 7.1%, and 2.9% across the MLLM Benchmarks, Visual Localization, and Open-ended VQA tasks with fewer trainable parameters and faster convergence speed.","sentences":["The visual projector, which bridges the vision and language modalities and facilitates cross-modal alignment, serves as a crucial component in MLLMs.","However, measuring the effectiveness of projectors in vision-language alignment remains under-explored, which currently can only be inferred from the performance of MLLMs on downstream tasks.","Motivated by the problem, this study examines the projector module by interpreting the vision-language semantic flow within MLLMs.","Specifically, we trace back the semantic relevance flow from generated language tokens to raw visual encoder patches and the intermediate outputs produced by projectors.","Our findings reveal that compressive projectors (e.g., QFormer), abstract visual patches into a limited set of semantic concepts, such as objects or attributes, resulting in a 'double abstraction' phenomenon.","This involves a first visual semantic abstraction by the projector referring to pre-defined query tokens, and a second extraction by the LLM based on text instructions.","The double abstraction is inefficient in training and will result in cumulative vision semantics deficiency.","To mitigate this issue, we propose the key insight of 'Decouple Compression from Abstraction (DeCo), that is compressing the visual token number at the patch level by projectors and allowing the LLM to handle visual semantic abstraction entirely.","Consequently, we adopt a simple compressor, i.e., 2D Adaptive Pooling, to downsample visual patches in a parameter-free manner.","Empirical evaluation demonstrates that DeCo surpasses traditional compressive projectors regarding both performance and efficiency.","It achieves performance gains of 0.9%, 7.1%, and 2.9% across the MLLM Benchmarks, Visual Localization, and Open-ended VQA tasks with fewer trainable parameters and faster convergence speed."],"url":"http://arxiv.org/abs/2405.20985v1","category":"cs.CV"}
{"created":"2024-05-31 16:27:29","title":"Scaling Data Plane Verification with Intent-based Slicing","abstract":"Data plane verification has grown into a powerful tool to ensure network correctness. However, existing monolithic data plane models have high memory requirements with large networks, and the existing method of scaling out is too limited in expressiveness to capture practical network features. In this paper, we describe Scylla, a general data plane verifier that provides fine-grained scale-out without the need for a monolithic network model. Scylla creates models for what we call intent-based slices, each of which is constructed at a fine (rule-level) granularity with just enough to verify a given set of intents. The sliced models are retained in memory across a cluster and are incrementally updated in a distributed compute cluster in response to network updates. Our experiments show that Scylla makes the scaling problem more granular -- tied to the size of the intent-based slices rather than that of the overall network. This enables Scylla to verify large, complex networks in minimum units of work that are significantly smaller (in both memory and time) than past techniques, enabling fast scale-out verification with minimal resource requirement.","sentences":["Data plane verification has grown into a powerful tool to ensure network correctness.","However, existing monolithic data plane models have high memory requirements with large networks, and the existing method of scaling out is too limited in expressiveness to capture practical network features.","In this paper, we describe Scylla, a general data plane verifier that provides fine-grained scale-out without the need for a monolithic network model.","Scylla creates models for what we call intent-based slices, each of which is constructed at a fine (rule-level) granularity with just enough to verify a given set of intents.","The sliced models are retained in memory across a cluster and are incrementally updated in a distributed compute cluster in response to network updates.","Our experiments show that Scylla makes the scaling problem more granular -- tied to the size of the intent-based slices rather than that of the overall network.","This enables Scylla to verify large, complex networks in minimum units of work that are significantly smaller (in both memory and time) than past techniques, enabling fast scale-out verification with minimal resource requirement."],"url":"http://arxiv.org/abs/2405.20982v1","category":"cs.NI"}
{"created":"2024-05-31 16:26:30","title":"Generative Adversarial Networks in Ultrasound Imaging: Extending Field of View Beyond Conventional Limits","abstract":"Transthoracic Echocardiography (TTE) is a fundamental, non-invasive diagnostic tool in cardiovascular medicine, enabling detailed visualization of cardiac structures crucial for diagnosing various heart conditions. Despite its widespread use, TTE ultrasound imaging faces inherent limitations, notably the trade-off between field of view (FoV) and resolution. This paper introduces a novel application of conditional Generative Adversarial Networks (cGANs), specifically designed to extend the FoV in TTE ultrasound imaging while maintaining high resolution. Our proposed cGAN architecture, termed echoGAN, demonstrates the capability to generate realistic anatomical structures through outpainting, effectively broadening the viewable area in medical imaging. This advancement has the potential to enhance both automatic and manual ultrasound navigation, offering a more comprehensive view that could significantly reduce the learning curve associated with ultrasound imaging and aid in more accurate diagnoses. The results confirm that echoGAN reliably reproduce detailed cardiac features, thereby promising a significant step forward in the field of non-invasive cardiac naviagation and diagnostics.","sentences":["Transthoracic Echocardiography (TTE) is a fundamental, non-invasive diagnostic tool in cardiovascular medicine, enabling detailed visualization of cardiac structures crucial for diagnosing various heart conditions.","Despite its widespread use, TTE ultrasound imaging faces inherent limitations, notably the trade-off between field of view (FoV) and resolution.","This paper introduces a novel application of conditional Generative Adversarial Networks (cGANs), specifically designed to extend the FoV in TTE ultrasound imaging while maintaining high resolution.","Our proposed cGAN architecture, termed echoGAN, demonstrates the capability to generate realistic anatomical structures through outpainting, effectively broadening the viewable area in medical imaging.","This advancement has the potential to enhance both automatic and manual ultrasound navigation, offering a more comprehensive view that could significantly reduce the learning curve associated with ultrasound imaging and aid in more accurate diagnoses.","The results confirm that echoGAN reliably reproduce detailed cardiac features, thereby promising a significant step forward in the field of non-invasive cardiac naviagation and diagnostics."],"url":"http://arxiv.org/abs/2405.20981v1","category":"cs.AI"}
{"created":"2024-05-31 16:24:53","title":"Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training","abstract":"Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.","sentences":["Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes.","Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges.","However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses.","Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability.","In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments.","We analyze the impact of these various retrieval noises on the robustness of LLMs.","Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).","RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises.","Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts.","Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions.","For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."],"url":"http://arxiv.org/abs/2405.20978v1","category":"cs.AI"}
{"created":"2024-05-31 16:24:17","title":"A mathematical justification for nonlinear constitutive relations between stress and linearized strain","abstract":"We present an asymptotic framework that rigorously generates nonlinear constitutive relations between stress and linearized strain for elastic bodies. Each of these relations arises as the leading order relationship satisfied by a one-parameter family of nonlinear constitutive relations between stress and nonlinear strain. The asymptotic parameter limits the overall range of strains that satisfy the corresponding constitutive relation in the one-parameter family while the stresses can remain large (relative to a fixed stress scale). This differs from classical linearized elasticity where a fixed constitutive relation is assumed, and the magnitude of the displacement gradient serves as the asymptotic parameter. Also unlike classical approaches, the constitutive relations in our framework are expressed as implicit relationships between stress and strain rather than requiring stress explicitly expressed as a function of strain, adding conceptual simplicity and versatility. We demonstrate that our framework rigorously justifies nonlinear constitutive relations between stress and linearized strain including those with density-dependent Young's moduli or derived from strain energies beyond quadratic forms.","sentences":["We present an asymptotic framework that rigorously generates nonlinear constitutive relations between stress and linearized strain for elastic bodies.","Each of these relations arises as the leading order relationship satisfied by a one-parameter family of nonlinear constitutive relations between stress and nonlinear strain.","The asymptotic parameter limits the overall range of strains that satisfy the corresponding constitutive relation in the one-parameter family while the stresses can remain large (relative to a fixed stress scale).","This differs from classical linearized elasticity where a fixed constitutive relation is assumed, and the magnitude of the displacement gradient serves as the asymptotic parameter.","Also unlike classical approaches, the constitutive relations in our framework are expressed as implicit relationships between stress and strain rather than requiring stress explicitly expressed as a function of strain, adding conceptual simplicity and versatility.","We demonstrate that our framework rigorously justifies nonlinear constitutive relations between stress and linearized strain including those with density-dependent Young's moduli or derived from strain energies beyond quadratic forms."],"url":"http://arxiv.org/abs/2405.20977v1","category":"math-ph"}
{"created":"2024-05-31 16:22:05","title":"Matrix Rationalization via Partial Orders","abstract":"A preference matrix $M$ has an entry for each pair of candidates in an election whose value $p_{ij}$ represents the proportion of voters that prefer candidate $i$ over candidate $j$. The matrix is rationalizable if it is consistent with a set of voters whose preferences are total orders. A celebrated open problem asks for a concise characterization of rationalizable preference matrices. In this paper, we generalize this matrix rationalizability question and study when a preference matrix is consistent with a set of voters whose preferences are partial orders of width $\\alpha$. The width (the maximum cardinality of an antichain) of the partial order is a natural measure of the rationality of a voter; indeed, a partial order of width $1$ is a total order. Our primary focus concerns the rationality number, the minimum width required to rationalize a preference matrix. We present two main results. The first concerns the class of half-integral preference matrices, where we show the key parameter required in evaluating the rationality number is the chromatic number of the undirected unanimity graph associated with the preference matrix $M$. The second concerns the class of integral preference matrices, where we show the key parameter now is the dichromatic number of the directed voting graph associated with $M$.","sentences":["A preference matrix $M$ has an entry for each pair of candidates in an election whose value $p_{ij}$ represents the proportion of voters that prefer candidate $i$ over candidate $j$. The matrix is rationalizable if it is consistent with a set of voters whose preferences are total orders.","A celebrated open problem asks for a concise characterization of rationalizable preference matrices.","In this paper, we generalize this matrix rationalizability question and study when a preference matrix is consistent with a set of voters whose preferences are partial orders of width $\\alpha$. The width (the maximum cardinality of an antichain) of the partial order is a natural measure of the rationality of a voter; indeed, a partial order of width $1$ is a total order.","Our primary focus concerns the rationality number, the minimum width required to rationalize a preference matrix.","We present two main results.","The first concerns the class of half-integral preference matrices, where we show the key parameter required in evaluating the rationality number is the chromatic number of the undirected unanimity graph associated with the preference matrix $M$. The second concerns the class of integral preference matrices, where we show the key parameter now is the dichromatic number of the directed voting graph associated with $M$."],"url":"http://arxiv.org/abs/2405.20976v1","category":"cs.DM"}
{"created":"2024-05-31 16:21:55","title":"ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning","abstract":"In Federated Learning (FL), a set of clients collaboratively train a machine learning model (called global model) without sharing their local training data. The local training data of clients is typically non-i.i.d. and heterogeneous, resulting in varying contributions from individual clients to the final performance of the global model. In response, many contribution evaluation methods were proposed, where the server could evaluate the contribution made by each client and incentivize the high-contributing clients to sustain their long-term participation in FL. Existing studies mainly focus on developing new metrics or algorithms to better measure the contribution of each client. However, the security of contribution evaluation methods of FL operating in adversarial environments is largely unexplored. In this paper, we propose the first model poisoning attack on contribution evaluation methods in FL, termed ACE. Specifically, we show that any malicious client utilizing ACE could manipulate the parameters of its local model such that it is evaluated to have a high contribution by the server, even when its local training data is indeed of low quality. We perform both theoretical analysis and empirical evaluations of ACE. Theoretically, we show our design of ACE can effectively boost the malicious client's perceived contribution when the server employs the widely-used cosine distance metric to measure contribution. Empirically, our results show ACE effectively and efficiently deceive five state-of-the-art contribution evaluation methods. In addition, ACE preserves the accuracy of the final global models on testing inputs. We also explore six countermeasures to defend ACE. Our results show they are inadequate to thwart ACE, highlighting the urgent need for new defenses to safeguard the contribution evaluation methods in FL.","sentences":["In Federated Learning (FL), a set of clients collaboratively train a machine learning model (called global model) without sharing their local training data.","The local training data of clients is typically non-i.i.d. and heterogeneous, resulting in varying contributions from individual clients to the final performance of the global model.","In response, many contribution evaluation methods were proposed, where the server could evaluate the contribution made by each client and incentivize the high-contributing clients to sustain their long-term participation in FL.","Existing studies mainly focus on developing new metrics or algorithms to better measure the contribution of each client.","However, the security of contribution evaluation methods of FL operating in adversarial environments is largely unexplored.","In this paper, we propose the first model poisoning attack on contribution evaluation methods in FL, termed ACE.","Specifically, we show that any malicious client utilizing ACE could manipulate the parameters of its local model such that it is evaluated to have a high contribution by the server, even when its local training data is indeed of low quality.","We perform both theoretical analysis and empirical evaluations of ACE.","Theoretically, we show our design of ACE can effectively boost the malicious client's perceived contribution when the server employs the widely-used cosine distance metric to measure contribution.","Empirically, our results show ACE effectively and efficiently deceive five state-of-the-art contribution evaluation methods.","In addition, ACE preserves the accuracy of the final global models on testing inputs.","We also explore six countermeasures to defend ACE.","Our results show they are inadequate to thwart ACE, highlighting the urgent need for new defenses to safeguard the contribution evaluation methods in FL."],"url":"http://arxiv.org/abs/2405.20975v1","category":"cs.CR"}
{"created":"2024-05-31 16:21:16","title":"SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales","abstract":"Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at \\url{https://github.com/xu1868/SaySelf}.","sentences":["Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications.","Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning.","The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates.","In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates.","In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty.","This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language.","The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning.","Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs.","Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance.","We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration.","The code is made public at \\url{https://github.com/xu1868/SaySelf}."],"url":"http://arxiv.org/abs/2405.20974v1","category":"cs.CL"}
{"created":"2024-05-31 16:18:46","title":"Amortizing intractable inference in diffusion models for vision, language, and control","abstract":"Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm post}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\\mathbf{x})$ and a black-box constraint or likelihood function $r(\\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning.","sentences":["Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem.","This paper studies amortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm post}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\\mathbf{x})$ and a black-box constraint or likelihood function $r(\\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases.","Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage.","Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation).","Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning."],"url":"http://arxiv.org/abs/2405.20971v1","category":"cs.LG"}
{"created":"2024-05-31 16:18:06","title":"PUAL: A Classifier on Trifurcate Positive-Unlabeled Data","abstract":"Positive-unlabeled (PU) learning aims to train a classifier using the data containing only labeled-positive instances and unlabeled instances. However, existing PU learning methods are generally hard to achieve satisfactory performance on trifurcate data, where the positive instances distribute on both sides of the negative instances. To address this issue, firstly we propose a PU classifier with asymmetric loss (PUAL), by introducing a structure of asymmetric loss on positive instances into the objective function of the global and local learning classifier. Then we develop a kernel-based algorithm to enable PUAL to obtain non-linear decision boundary. We show that, through experiments on both simulated and real-world datasets, PUAL can achieve satisfactory classification on trifurcate data.","sentences":["Positive-unlabeled (PU) learning aims to train a classifier using the data containing only labeled-positive instances and unlabeled instances.","However, existing PU learning methods are generally hard to achieve satisfactory performance on trifurcate data, where the positive instances distribute on both sides of the negative instances.","To address this issue, firstly we propose a PU classifier with asymmetric loss (PUAL), by introducing a structure of asymmetric loss on positive instances into the objective function of the global and local learning classifier.","Then we develop a kernel-based algorithm to enable PUAL to obtain non-linear decision boundary.","We show that, through experiments on both simulated and real-world datasets, PUAL can achieve satisfactory classification on trifurcate data."],"url":"http://arxiv.org/abs/2405.20970v1","category":"stat.ML"}
{"created":"2024-05-31 16:15:02","title":"A new multivariate primitive from CCZ equivalence","abstract":"Multivariate Cryptography is one of the main candidates for Post-quantum Cryptography. Multivariate schemes are usually constructed by applying two secret affine invertible transformations $\\mathcal S,\\mathcal T$ to a set of multivariate polynomials $\\mathcal{F}$ (often quadratic). The secret polynomials $\\mathcal{F}$ posses a trapdoor that allows the legitimate user to find a solution of the corresponding system, while the public polynomials $\\mathcal G=\\mathcal S\\circ\\mathcal F\\circ\\mathcal T$ look like random polynomials. The polynomials $\\mathcal G$ and $\\mathcal F$ are said to be affine equivalent. In this article, we present a more general way of constructing a multivariate scheme by considering the CCZ equivalence, which has been introduced and studied in the context of vectorial Boolean functions.","sentences":["Multivariate Cryptography is one of the main candidates for Post-quantum Cryptography.","Multivariate schemes are usually constructed by applying two secret affine invertible transformations $\\mathcal S,\\mathcal T$ to a set of multivariate polynomials $\\mathcal{F}$ (often quadratic).","The secret polynomials $\\mathcal{F}$ posses a trapdoor that allows the legitimate user to find a solution of the corresponding system, while the public polynomials $\\mathcal G=\\mathcal S\\circ\\mathcal F\\circ\\mathcal T$ look like random polynomials.","The polynomials $\\mathcal G$ and $\\mathcal F$ are said to be affine equivalent.","In this article, we present a more general way of constructing a multivariate scheme by considering the CCZ equivalence, which has been introduced and studied in the context of vectorial Boolean functions."],"url":"http://arxiv.org/abs/2405.20968v1","category":"cs.CR"}
{"created":"2024-05-31 16:13:33","title":"Black hole solutions surrounded by anisotropic fluid in $f(\\mathbb{T},\\CMcal{T})$ gravity","abstract":"In this work, we investigate some extensions of the Kiselev black hole solutions in the context of $f(\\mathbb{T},\\CMcal{T})$ gravity. By mapping the components of the Kiselev energy-momentum tensor into the anisotropic energy-momentum tensor and assuming a particular form of $f(\\mathbb{T},\\CMcal{T})$, we obtain exact solutions for the field equation in this theory that carries dependence on the coupling constant and on the parameter of the equation of state of the fluid. We show that in this scenario of modified gravity some new structure is added to the geometry of spacetime as compared to the Kiselev black hole. We analyse the energy conditions, mass, horizons and the Hawking temperature considering particular values for the parameter of the equation of state.","sentences":["In this work, we investigate some extensions of the Kiselev black hole solutions in the context of $f(\\mathbb{T},\\CMcal{T})$ gravity.","By mapping the components of the Kiselev energy-momentum tensor into the anisotropic energy-momentum tensor and assuming a particular form of $f(\\mathbb{T},\\CMcal{T})$, we obtain exact solutions for the field equation in this theory that carries dependence on the coupling constant and on the parameter of the equation of state of the fluid.","We show that in this scenario of modified gravity some new structure is added to the geometry of spacetime as compared to the Kiselev black hole.","We analyse the energy conditions, mass, horizons and the Hawking temperature considering particular values for the parameter of the equation of state."],"url":"http://arxiv.org/abs/2405.20966v1","category":"gr-qc"}
{"created":"2024-05-31 16:07:33","title":"Large Language Models are Zero-Shot Next Location Predictors","abstract":"Predicting the locations an individual will visit in the future is crucial for solving many societal issues like disease diffusion and reduction of pollution among many others. The models designed to tackle next-location prediction, however, require a significant amount of individual-level information to be trained effectively. Such data may be scarce or even unavailable in some geographic regions or peculiar scenarios (e.g., cold-start in recommendation systems). Moreover, the design of a next-location predictor able to generalize or geographically transfer knowledge is still an open research challenge. Recent advances in natural language processing have led to a rapid diffusion of Large Language Models (LLMs) which have shown good generalization and reasoning capabilities. These insights, coupled with the recent findings that LLMs are rich in geographical knowledge, allowed us to believe that these models can act as zero-shot next-location predictors. This paper evaluates the capabilities of many popular LLMs in this role, specifically Llama, GPT-3.5 and Mistral 7B. After designing a proper prompt, we tested the models on three real-world mobility datasets. The results show that LLMs can obtain accuracies up to 32.4%, a significant relative improvement of over 600% when compared to sophisticated DL models specifically designed for human mobility. Moreover, we show that other LLMs are unable to perform the task properly. To prevent positively biased results, we also propose a framework inspired by other studies to test data contamination. Finally, we explored the possibility of using LLMs as text-based explainers for next-location prediction showing that can effectively provide an explanation for their decision. Notably, 7B models provide more generic, but still reliable, explanations compared to larger counterparts. Code: github.com/ssai-trento/LLM-zero-shot-NL","sentences":["Predicting the locations an individual will visit in the future is crucial for solving many societal issues like disease diffusion and reduction of pollution among many others.","The models designed to tackle next-location prediction, however, require a significant amount of individual-level information to be trained effectively.","Such data may be scarce or even unavailable in some geographic regions or peculiar scenarios (e.g., cold-start in recommendation systems).","Moreover, the design of a next-location predictor able to generalize or geographically transfer knowledge is still an open research challenge.","Recent advances in natural language processing have led to a rapid diffusion of Large Language Models (LLMs) which have shown good generalization and reasoning capabilities.","These insights, coupled with the recent findings that LLMs are rich in geographical knowledge, allowed us to believe that these models can act as zero-shot next-location predictors.","This paper evaluates the capabilities of many popular LLMs in this role, specifically Llama, GPT-3.5 and Mistral 7B. After designing a proper prompt, we tested the models on three real-world mobility datasets.","The results show that LLMs can obtain accuracies up to 32.4%, a significant relative improvement of over 600% when compared to sophisticated DL models specifically designed for human mobility.","Moreover, we show that other LLMs are unable to perform the task properly.","To prevent positively biased results, we also propose a framework inspired by other studies to test data contamination.","Finally, we explored the possibility of using LLMs as text-based explainers for next-location prediction showing that can effectively provide an explanation for their decision.","Notably, 7B models provide more generic, but still reliable, explanations compared to larger counterparts.","Code: github.com/ssai-trento/LLM-zero-shot-NL"],"url":"http://arxiv.org/abs/2405.20962v1","category":"cs.CY"}
{"created":"2024-05-31 16:02:52","title":"GGS-groups acting on trees of growing degrees","abstract":"We consider analogues of Grigorchuk-Gupta-Sidki (GGS-)groups acting on trees of growing degree; the so-called growing GGS-groups. These groups are not just infinite and do not possess the congruence subgroup property, but many of them are branch and have the $p$-congruence subgroup property, for a prime $p$. Among them, we find groups with maximal subgroups only of finite index, and with infinitely many such maximal subgroups. These give the first examples of finitely generated branch groups with infinitely many finite-index maximal subgroups. Additionally, we prove that congruence quotients of growing GGS-groups associated to a defining vector of zero sum give rise to Beauville groups.","sentences":["We consider analogues of Grigorchuk-Gupta-Sidki (GGS-)groups acting on trees of growing degree; the so-called growing GGS-groups.","These groups are not just infinite and do not possess the congruence subgroup property, but many of them are branch and have the $p$-congruence subgroup property, for a prime $p$. Among them, we find groups with maximal subgroups only of finite index, and with infinitely many such maximal subgroups.","These give the first examples of finitely generated branch groups with infinitely many finite-index maximal subgroups.","Additionally, we prove that congruence quotients of growing GGS-groups associated to a defining vector of zero sum give rise to Beauville groups."],"url":"http://arxiv.org/abs/2405.20961v1","category":"math.GR"}
{"created":"2024-05-31 16:00:43","title":"Navigating Tabular Data Synthesis Research: Understanding User Needs and Tool Capabilities","abstract":"In an era of rapidly advancing data-driven applications, there is a growing demand for data in both research and practice. Synthetic data have emerged as an alternative when no real data is available (e.g., due to privacy regulations). Synthesizing tabular data presents unique and complex challenges, especially handling (i) missing values, (ii) dataset imbalance, (iii) diverse column types, and (iv) complex data distributions, as well as preserving (i) column correlations, (ii) temporal dependencies, and (iii) integrity constraints (e.g., functional dependencies) present in the original dataset. While substantial progress has been made recently in the context of generational models, there is no one-size-fits-all solution for tabular data today, and choosing the right tool for a given task is therefore no trivial task. In this paper, we survey the state of the art in Tabular Data Synthesis (TDS), examine the needs of users by defining a set of functional and non-functional requirements, and compile the challenges associated with meeting those needs. In addition, we evaluate the reported performance of 36 popular research TDS tools about these requirements and develop a decision guide to help users find suitable TDS tools for their applications. The resulting decision guide also identifies significant research gaps.","sentences":["In an era of rapidly advancing data-driven applications, there is a growing demand for data in both research and practice.","Synthetic data have emerged as an alternative when no real data is available (e.g., due to privacy regulations).","Synthesizing tabular data presents unique and complex challenges, especially handling (i) missing values, (ii) dataset imbalance, (iii) diverse column types, and (iv) complex data distributions, as well as preserving (i) column correlations, (ii) temporal dependencies, and (iii) integrity constraints (e.g., functional dependencies) present in the original dataset.","While substantial progress has been made recently in the context of generational models, there is no one-size-fits-all solution for tabular data today, and choosing the right tool for a given task is therefore no trivial task.","In this paper, we survey the state of the art in Tabular Data Synthesis (TDS), examine the needs of users by defining a set of functional and non-functional requirements, and compile the challenges associated with meeting those needs.","In addition, we evaluate the reported performance of 36 popular research TDS tools about these requirements and develop a decision guide to help users find suitable TDS tools for their applications.","The resulting decision guide also identifies significant research gaps."],"url":"http://arxiv.org/abs/2405.20959v1","category":"cs.AI"}
{"created":"2024-05-31 15:55:51","title":"A Robot Walks into a Bar: Can Language Models Serve asCreativity Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with Comedians","abstract":"We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians' motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to ``cruise ship comedy material from the 1950s, but a bit less racist''. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, ``offensive'' language as a practice of resistance, satire and ``punching up''. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists' needs.","sentences":["We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival Fringe in August 2023 and online.","The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians' motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright.","Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship.","At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to ``cruise ship comedy material from the 1950s, but a bit less racist''.","Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, ``offensive'' language as a practice of resistance, satire and ``punching up''.","We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists' needs."],"url":"http://arxiv.org/abs/2405.20956v1","category":"cs.AI"}
{"created":"2024-05-31 15:50:46","title":"Monte Carlo Tree Search Satellite Scheduling Under Cloud Cover Uncertainty","abstract":"Efficient utilization of satellite resources in dynamic environments remains a challenging problem in satellite scheduling. This paper addresses the multi-satellite collection scheduling problem (m-SatCSP), aiming to optimize task scheduling over a constellation of satellites under uncertain conditions such as cloud cover. Leveraging Monte Carlo Tree Search (MCTS), a stochastic search algorithm, two versions of MCTS are explored to schedule satellites effectively. Hyperparameter tuning is conducted to optimize the algorithm's performance. Experimental results demonstrate the effectiveness of the MCTS approach, outperforming existing methods in both solution quality and efficiency. Comparative analysis against other scheduling algorithms showcases competitive performance, positioning MCTS as a promising solution for satellite task scheduling in dynamic environments.","sentences":["Efficient utilization of satellite resources in dynamic environments remains a challenging problem in satellite scheduling.","This paper addresses the multi-satellite collection scheduling problem (m-SatCSP), aiming to optimize task scheduling over a constellation of satellites under uncertain conditions such as cloud cover.","Leveraging Monte Carlo Tree Search (MCTS), a stochastic search algorithm, two versions of MCTS are explored to schedule satellites effectively.","Hyperparameter tuning is conducted to optimize the algorithm's performance.","Experimental results demonstrate the effectiveness of the MCTS approach, outperforming existing methods in both solution quality and efficiency.","Comparative analysis against other scheduling algorithms showcases competitive performance, positioning MCTS as a promising solution for satellite task scheduling in dynamic environments."],"url":"http://arxiv.org/abs/2405.20951v1","category":"cs.AI"}
{"created":"2024-05-31 15:44:33","title":"OR-Bench: An Over-Refusal Benchmark for Large Language Models","abstract":"Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where the LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that appear harmful but are benign. This study proposes a novel method for automatically generating large-scale sets of ``seemingly toxic prompts'' (benign prompts likely rejected by LLMs). Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 seemingly toxic prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 25 popular LLMs across 8 model families. Our datasets are available at https://huggingface.co/datasets/bench-llm/OR-Bench and the corresponding demo can be found at https://huggingface.co/spaces/bench-llm/or-bench. We hope this benchmark can help the community develop better safety aligned models.","sentences":["Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs.","While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where the LLMs may reject innocuous prompts and become less helpful.","Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that appear harmful but are benign.","This study proposes a novel method for automatically generating large-scale sets of ``seemingly toxic prompts'' (benign prompts likely rejected by LLMs).","Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark.","OR-Bench comprises 80,000 seemingly toxic prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses.","We then conduct a comprehensive study to measure the over-refusal of 25 popular LLMs across 8 model families.","Our datasets are available at https://huggingface.co/datasets/bench-llm/OR-Bench and the corresponding demo can be found at https://huggingface.co/spaces/bench-llm/or-bench.","We hope this benchmark can help the community develop better safety aligned models."],"url":"http://arxiv.org/abs/2405.20947v1","category":"cs.CL"}
{"created":"2024-05-31 15:41:48","title":"Two-dimensional solitons in second-harmonic-generating media with fractional diffraction","abstract":"We introduce a system of propagation equations for the fundamental-frequency (FF) and second-harmonic (SH) waves in the bulk waveguide with the effective fractional diffraction and quadratic (chi ^(2)) nonlinearity. The numerical solution produces families of ground-state (zero-vorticity) two-dimensional solitons in the free space, which are stable in exact agreement with the Vakhitov-Kolokolov criterion, while vortex solitons are completely unstable in that case. Mobility of the stable solitons and inelastic collisions between them are briefly considered too. In the presence of a harmonic-oscillator (HO) trapping potential, families of partially stable single- and two-color solitons (SH-only or FF-SH ones, respectively) are obtained, with zero and nonzero vorticities. The single-and two-color solitons are linked by a bifurcation which takes place withthe increase of the soliton's power.","sentences":["We introduce a system of propagation equations for the fundamental-frequency (FF) and second-harmonic (SH) waves in the bulk waveguide with the effective fractional diffraction and quadratic (chi ^(2))","nonlinearity.","The numerical solution produces families of ground-state (zero-vorticity) two-dimensional solitons in the free space, which are stable in exact agreement with the Vakhitov-Kolokolov criterion, while vortex solitons are completely unstable in that case.","Mobility of the stable solitons and inelastic collisions between them are briefly considered too.","In the presence of a harmonic-oscillator (HO) trapping potential, families of partially stable single- and two-color solitons (SH-only or FF-SH ones, respectively) are obtained, with zero and nonzero vorticities.","The single-and two-color solitons are linked by a bifurcation which takes place withthe increase of the soliton's power."],"url":"http://arxiv.org/abs/2405.20944v1","category":"nlin.PS"}
{"created":"2024-05-31 15:39:09","title":"Standard model of electromagnetism and chirality in crystals","abstract":"We present a general, systematic theory of electromagnetism and chirality in crystalline solids. Symmetry is its fundamental guiding principle. We use the formal similarity between space inversion $i$ and time inversion $\\theta$ to identify two complementary, comprehensive classification of crystals, based on five categories of electric and magnetic multipole order -- called polarizations -- and five categories of chirality. The five categories of polarizations (parapolar, electropolar, magnetopolar, antimagnetopolar, and multipolar) expand the familiar notion of electric dipolarization in ferroelectrics and magnetization in ferromagnets to higher-order multipole densities. The five categories of chirality (parachiral, electrochiral, magnetochiral, antimagnetochiral, and multichiral) expand the familiar notion of enantiomorphism due to non-superposable mirror images to the inversion symmetries $i$, $\\theta$, and $i\\theta$. In multichiral systems, all these inversion symmetries are absent so that these systems have four distinct enantiomorphs. Each category of chirality arises from distinct superpositions of electric and magnetic multipole densities. We provide a complete theory of minimal effective models characterizing the different categories of chirality in different systems. Jointly these two schemes yield a classification of all 122 magnetic crystallographic point groups into 15 types that treat the inversion symmetries $i$, $\\theta$, and $i\\theta$ on the same footing. The group types are characterized via distinct physical properties and characteristic features in the electronic band structure. At the same time, the formal similarities between the inversion symmetries $i$, $\\theta$, and $i\\theta$ imply striking correspondences between apparently dissimilar systems and their physical properties.","sentences":["We present a general, systematic theory of electromagnetism and chirality in crystalline solids.","Symmetry is its fundamental guiding principle.","We use the formal similarity between space inversion $i$ and time inversion $\\theta$ to identify two complementary, comprehensive classification of crystals, based on five categories of electric and magnetic multipole order -- called polarizations -- and five categories of chirality.","The five categories of polarizations (parapolar, electropolar, magnetopolar, antimagnetopolar, and multipolar) expand the familiar notion of electric dipolarization in ferroelectrics and magnetization in ferromagnets to higher-order multipole densities.","The five categories of chirality (parachiral, electrochiral, magnetochiral, antimagnetochiral, and multichiral) expand the familiar notion of enantiomorphism due to non-superposable mirror images to the inversion symmetries $i$, $\\theta$, and $i\\theta$. In multichiral systems, all these inversion symmetries are absent so that these systems have four distinct enantiomorphs.","Each category of chirality arises from distinct superpositions of electric and magnetic multipole densities.","We provide a complete theory of minimal effective models characterizing the different categories of chirality in different systems.","Jointly these two schemes yield a classification of all 122 magnetic crystallographic point groups into 15 types that treat the inversion symmetries $i$, $\\theta$, and $i\\theta$ on the same footing.","The group types are characterized via distinct physical properties and characteristic features in the electronic band structure.","At the same time, the formal similarities between the inversion symmetries $i$, $\\theta$, and $i\\theta$ imply striking correspondences between apparently dissimilar systems and their physical properties."],"url":"http://arxiv.org/abs/2405.20940v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 15:34:36","title":"Bayesian Deep Generative Models for Replicated Networks with Multiscale Overlapping Clusters","abstract":"Our interest is in replicated network data with multiple networks observed across the same set of nodes. Examples include brain connection networks, in which nodes corresponds to brain regions and replicates to different individuals, and ecological networks, in which nodes correspond to species and replicates to samples collected at different locations and/or times. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. In brain connectomics, the focus is on inferring common relationships among the brain regions, while characterizing inter-individual variability in an easily interpretable manner. To accomplish this, we propose a Bayesian hierarchical model, while providing theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our simulations and application to brain connectome data provide support for the proposed methodology.","sentences":["Our interest is in replicated network data with multiple networks observed across the same set of nodes.","Examples include brain connection networks, in which nodes corresponds to brain regions and replicates to different individuals, and ecological networks, in which nodes correspond to species and replicates to samples collected at different locations and/or times.","Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates.","In brain connectomics, the focus is on inferring common relationships among the brain regions, while characterizing inter-individual variability in an easily interpretable manner.","To accomplish this, we propose a Bayesian hierarchical model, while providing theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation.","We provide novel technical tools for proving model identifiability, which are of independent interest.","Our simulations and application to brain connectome data provide support for the proposed methodology."],"url":"http://arxiv.org/abs/2405.20936v1","category":"stat.ME"}
{"created":"2024-05-31 15:34:13","title":"Effective Interplay between Sparsity and Quantization: From Theory to Practice","abstract":"The increasing size of deep neural networks necessitates effective model compression to improve computational efficiency and reduce their memory footprint. Sparsity and quantization are two prominent compression methods that have individually demonstrated significant reduction in computational and memory footprints while preserving model accuracy. While effective, the interplay between these two methods remains an open question. In this paper, we investigate the interaction between these two methods and assess whether their combination impacts final model accuracy. We mathematically prove that applying sparsity before quantization is the optimal sequence for these operations, minimizing error in computation. Our empirical studies across a wide range of models, including OPT and Llama model families (125M-8B) and ViT corroborate these theoretical findings. In addition, through rigorous analysis, we demonstrate that sparsity and quantization are not orthogonal; their interaction can significantly harm model accuracy, with quantization error playing a dominant role in this degradation. Our findings extend to the efficient deployment of large models in resource-limited compute platforms and reduce serving cost, offering insights into best practices for applying these compression methods to maximize efficacy without compromising accuracy.","sentences":["The increasing size of deep neural networks necessitates effective model compression to improve computational efficiency and reduce their memory footprint.","Sparsity and quantization are two prominent compression methods that have individually demonstrated significant reduction in computational and memory footprints while preserving model accuracy.","While effective, the interplay between these two methods remains an open question.","In this paper, we investigate the interaction between these two methods and assess whether their combination impacts final model accuracy.","We mathematically prove that applying sparsity before quantization is the optimal sequence for these operations, minimizing error in computation.","Our empirical studies across a wide range of models, including OPT and Llama model families (125M-8B) and ViT corroborate these theoretical findings.","In addition, through rigorous analysis, we demonstrate that sparsity and quantization are not orthogonal; their interaction can significantly harm model accuracy, with quantization error playing a dominant role in this degradation.","Our findings extend to the efficient deployment of large models in resource-limited compute platforms and reduce serving cost, offering insights into best practices for applying these compression methods to maximize efficacy without compromising accuracy."],"url":"http://arxiv.org/abs/2405.20935v1","category":"cs.LG"}
{"created":"2024-05-31 15:33:10","title":"Spectroscopy of bumpy BHs: non-rotating case","abstract":"Recent detections of gravitational waves have made black hole quasinormal modes a powerful tool in testing predictions of general relativity. Understanding the spectrum of these quasinormal modes in a broad class of theories beyond general relativity and a variety of astrophysical environments around black holes remains vital. In this work, we study the quasinormal mode spectrum of parametrized deformations of a non-rotating black hole in the vacuum. Following Vigeland and Hughes, we model these parametrized deformations as axisymmetric multipole moments in the Weyl coordinates with amplitudes much less than the amplitude of the Schwarzschild potential. These tiny bumps in the black hole geometry satisfy the linearized vacuum Einstein equations and are asymptotically flat. We use the recently developed modified Teukolsky formalism to derive one decoupled differential equation for the radiative Weyl scalar $\\Psi_0$. We then use the eigenvalue perturbation method to compute the quasinormal mode frequency shifts of both even- and odd-parity modes with $\\ell=2,3$ and up to the overtone number $n=2$ for the Weyl multipoles with $\\ell_W=2,3$. Our calculation provides an avenue to directly connect the multipole moments of a modified black hole spacetime to the QNM frequency shifts in a parametric way.","sentences":["Recent detections of gravitational waves have made black hole quasinormal modes a powerful tool in testing predictions of general relativity.","Understanding the spectrum of these quasinormal modes in a broad class of theories beyond general relativity and a variety of astrophysical environments around black holes remains vital.","In this work, we study the quasinormal mode spectrum of parametrized deformations of a non-rotating black hole in the vacuum.","Following Vigeland and Hughes, we model these parametrized deformations as axisymmetric multipole moments in the Weyl coordinates with amplitudes much less than the amplitude of the Schwarzschild potential.","These tiny bumps in the black hole geometry satisfy the linearized vacuum Einstein equations and are asymptotically flat.","We use the recently developed modified Teukolsky formalism to derive one decoupled differential equation for the radiative Weyl scalar $\\Psi_0$. We then use the eigenvalue perturbation method to compute the quasinormal mode frequency shifts of both even- and odd-parity modes with $\\ell=2,3$ and up to the overtone number $n=2$ for the Weyl multipoles with $\\ell_W=2,3$. Our calculation provides an avenue to directly connect the multipole moments of a modified black hole spacetime to the QNM frequency shifts in a parametric way."],"url":"http://arxiv.org/abs/2405.20934v1","category":"gr-qc"}
{"created":"2024-05-31 15:29:56","title":"Finding Diverse Solutions Parameterized by Cliquewidth","abstract":"Finding a few solutions for a given problem that are diverse, as opposed to finding a single best solution to solve the problem, has recently become a notable topic in theoretical computer science. Recently, Baste, Fellows, Jaffke, Masa\\v{r}\\'ik, Oliveira, Philip, and Rosamond showed that under a standard structural parameterization by treewidth, one can find a set of diverse solutions for many problems with only a very small additional cost [Artificial Intelligence 2022]. In this paper, we investigate a much stronger graph parameter, the cliquewidth, which can additionally describe some dense graph classes. Broadly speaking, it describes graphs that can be recursively constructed by a few operations defined on graphs whose vertices are divided into a bounded number of groups while each such group behaves uniformly with respect to any operation.   We show that for any vertex problem, if we are given a dynamic program solving that problem on cliquewidth decomposition, we can modify it to produce a few solutions that are as diverse as possible with as little overhead as in the above-mentioned treewidth paper. As a consequence, we prove that a diverse version of any MSO$_1$ expressible problem can be solved in FPT time parameterized by cliquewidth, the number of sought solutions, and the number of quantifiers in the formula. That was an important missing piece in the complexity landscape of structural graph parameters and logic. We prove our results allowing for a more general natural collection of diversity functions compared to only two mostly studied diversity functions previously. That might be of independent interest as a larger pool of different diversity functions can highlight various aspects of different solutions to a problem.","sentences":["Finding a few solutions for a given problem that are diverse, as opposed to finding a single best solution to solve the problem, has recently become a notable topic in theoretical computer science.","Recently, Baste, Fellows, Jaffke, Masa\\v{r}\\'ik, Oliveira, Philip, and Rosamond showed that under a standard structural parameterization by treewidth, one can find a set of diverse solutions for many problems with only a very small additional cost [Artificial Intelligence 2022].","In this paper, we investigate a much stronger graph parameter, the cliquewidth, which can additionally describe some dense graph classes.","Broadly speaking, it describes graphs that can be recursively constructed by a few operations defined on graphs whose vertices are divided into a bounded number of groups while each such group behaves uniformly with respect to any operation.   ","We show that for any vertex problem, if we are given a dynamic program solving that problem on cliquewidth decomposition, we can modify it to produce a few solutions that are as diverse as possible with as little overhead as in the above-mentioned treewidth paper.","As a consequence, we prove that a diverse version of any MSO$_1$ expressible problem can be solved in FPT time parameterized by cliquewidth, the number of sought solutions, and the number of quantifiers in the formula.","That was an important missing piece in the complexity landscape of structural graph parameters and logic.","We prove our results allowing for a more general natural collection of diversity functions compared to only two mostly studied diversity functions previously.","That might be of independent interest as a larger pool of different diversity functions can highlight various aspects of different solutions to a problem."],"url":"http://arxiv.org/abs/2405.20931v1","category":"cs.DS"}
{"created":"2024-05-31 15:29:51","title":"Direct Laser Acceleration of Bethe-Heitler positrons in laser-channel interactions","abstract":"Positron creation and acceleration is one of the major challenges for constructing future lepton colliders. On the one hand, conventional technology can provide a solution, but at a prohibitive cost and scale. On the other hand, alternative, reduced-scale ideas for positron beam generation could bring this dream closer to reality. Here we propose a novel plasma-based positron acceleration method using a powerful laser propagating through a dense and narrow plasma channel. A large amount of electrons is injected within the channel during laser propagation. This electron loading creates static fields in the plasma, enabling positrons to be guided transversely while they directly gain energy from the laser field itself. Within this context, we present a theoretical model to describe how the laser injects the electrons and estimate the beam-loaded effective electron density. We validate our theoretical predictions through Quasi-3D PIC simulations and demonstrate the robustness of this guiding and direct laser acceleration process for positrons. Our approach could pave the way for testing this new positron acceleration scheme at ELI-Beamlines, showcasing unprecedentedly high average energy gain rate of a few TeV/m. The fireball jet produced contains GeV-level electrons, positrons, and x-rays, opening the path towards potential laboratory astrophysics experiments using these beams.","sentences":["Positron creation and acceleration is one of the major challenges for constructing future lepton colliders.","On the one hand, conventional technology can provide a solution, but at a prohibitive cost and scale.","On the other hand, alternative, reduced-scale ideas for positron beam generation could bring this dream closer to reality.","Here we propose a novel plasma-based positron acceleration method using a powerful laser propagating through a dense and narrow plasma channel.","A large amount of electrons is injected within the channel during laser propagation.","This electron loading creates static fields in the plasma, enabling positrons to be guided transversely while they directly gain energy from the laser field itself.","Within this context, we present a theoretical model to describe how the laser injects the electrons and estimate the beam-loaded effective electron density.","We validate our theoretical predictions through Quasi-3D PIC simulations and demonstrate the robustness of this guiding and direct laser acceleration process for positrons.","Our approach could pave the way for testing this new positron acceleration scheme at ELI-Beamlines, showcasing unprecedentedly high average energy gain rate of a few TeV/m. The fireball jet produced contains GeV-level electrons, positrons, and x-rays, opening the path towards potential laboratory astrophysics experiments using these beams."],"url":"http://arxiv.org/abs/2405.20930v1","category":"physics.plasm-ph"}
{"created":"2024-05-31 15:28:49","title":"Search of extended emission from HESS J1702-420 with eROSITA","abstract":"HESS J1702-420 is a peculiar TeV complex with a morphology changing from a diffuse (HESS J1702-420B source) at $\\lesssim 2$ TeV to point-like (HESS J1702-420A) at $\\gtrsim 10$ TeV energies. The morphology and the spectral properties of HESS J1702-420 could be understood in terms of a (diffusive) hadronic or leptonic models in which the observed TeV emission arises correpondingly from proton-proton or IC-radiation of relativistic particles present in the region. In this work we perform searches of the X-ray counterpart of HESS J1702-420B source originated from the synchrotron emission of the primary or secondary relativistic electrons produced within leptonic or hadronic models. Such an emission can be largely extent and remain beyond the detection capabilities of a narrow-FoV instruments such as XMM-Newton. We utilise the publicly available first 6-months eROSITA dataset (DR1) fully covering selected for the analysis region of $> 5^\\circ$-radius around HESS J1702-420. We discuss biases connected to variable plasma temperature/neutral hydrogen column density in the region and present results based on background modelling approach. The performed analysis does not allow us to detect the extended X-ray counterpart of HESS J1702-420 of $0.07^\\circ - 3^\\circ$-radii sizes. The derived upper limits are significantly higher than the expected hadronic model flux of the X-ray counterpart. For the leptonic model the derived limits indicate the magnetic field in the region $B\\lesssim 2\\mu$G. We argue, that the further advances in the diffuse X-ray counterpart searches could be achieved either with next generation missions or Msec-long observational campaigns with currently operating instruments.","sentences":["HESS J1702-420 is a peculiar TeV complex with a morphology changing from a diffuse (HESS J1702-420B source) at $\\lesssim 2$ TeV to point-like (HESS J1702-420A) at $\\gtrsim 10$ TeV energies.","The morphology and the spectral properties of HESS J1702-420 could be understood in terms of a (diffusive) hadronic or leptonic models in which the observed TeV emission arises correpondingly from proton-proton or IC-radiation of relativistic particles present in the region.","In this work we perform searches of the X-ray counterpart of HESS J1702-420B source originated from the synchrotron emission of the primary or secondary relativistic electrons produced within leptonic or hadronic models.","Such an emission can be largely extent and remain beyond the detection capabilities of a narrow-FoV instruments such as XMM-Newton.","We utilise the publicly available first 6-months eROSITA dataset (DR1) fully covering selected for the analysis region of $> 5^\\circ$-radius around HESS J1702-420.","We discuss biases connected to variable plasma temperature/neutral hydrogen column density in the region and present results based on background modelling approach.","The performed analysis does not allow us to detect the extended X-ray counterpart of HESS J1702-420 of $0.07^\\circ - 3^\\circ$-radii sizes.","The derived upper limits are significantly higher than the expected hadronic model flux of the X-ray counterpart.","For the leptonic model the derived limits indicate the magnetic field in the region $B\\lesssim 2\\mu$G.","We argue, that the further advances in the diffuse X-ray counterpart searches could be achieved either with next generation missions or Msec-long observational campaigns with currently operating instruments."],"url":"http://arxiv.org/abs/2405.20927v1","category":"astro-ph.HE"}
{"created":"2024-05-31 15:27:02","title":"Generalized uncertainty principle and neutrino phenomenology","abstract":"Generalized uncertainty principles are effective changes to the Heisenberg uncertainty principle that emerge in several quantum gravity models. In the present letter, we study the consequences that two classes of these modifications yield on the physics of neutrinos. Besides analyzing the change in the oscillation probabilities that the generalized uncertainty principles entail, we assess their impact on the neutrino coherence length and their possible interpretation as nonstandard neutrino interactions. Constraints cast by present and planned neutrino experiments on the generalized uncertainty principles parameters are also derived.","sentences":["Generalized uncertainty principles are effective changes to the Heisenberg uncertainty principle that emerge in several quantum gravity models.","In the present letter, we study the consequences that two classes of these modifications yield on the physics of neutrinos.","Besides analyzing the change in the oscillation probabilities that the generalized uncertainty principles entail, we assess their impact on the neutrino coherence length and their possible interpretation as nonstandard neutrino interactions.","Constraints cast by present and planned neutrino experiments on the generalized uncertainty principles parameters are also derived."],"url":"http://arxiv.org/abs/2405.20925v1","category":"hep-ph"}
{"created":"2024-05-31 15:25:21","title":"Atomistic spin dynamics simulations of magnonic spin Seebeck and spin Nernst effects in altermagnets","abstract":"Magnon band structures in altermagnets are characterized by an energy splitting of modes with opposite chirality, even in the absence of applied external fields and relativistic effects, due to an anisotropy in the Heisenberg exchange interactions. We perform quantitative atomistic spin dynamics simulations based on ab initio electronic structure calculations on rutile RuO$_2$, a prototypical \"d-wave\" altermagnet, to study magnon currents generated by thermal gradients. We report substantial spin Seebeck and spin Nernst effects, i.e., longitudinal or transverse spin currents, depending on the propagation direction of the magnons with respect to the crystal, together with a finite spin accumulation associated with non-linearities in the temperature profile. Our findings are consistent with the altermagnetic spin-group symmetry, as well as predictions from linear spin wave theory and semiclassical Boltzmann transport theory.","sentences":["Magnon band structures in altermagnets are characterized by an energy splitting of modes with opposite chirality, even in the absence of applied external fields and relativistic effects, due to an anisotropy in the Heisenberg exchange interactions.","We perform quantitative atomistic spin dynamics simulations based on ab initio electronic structure calculations on rutile RuO$_2$, a prototypical \"d-wave\" altermagnet, to study magnon currents generated by thermal gradients.","We report substantial spin Seebeck and spin Nernst effects, i.e., longitudinal or transverse spin currents, depending on the propagation direction of the magnons with respect to the crystal, together with a finite spin accumulation associated with non-linearities in the temperature profile.","Our findings are consistent with the altermagnetic spin-group symmetry, as well as predictions from linear spin wave theory and semiclassical Boltzmann transport theory."],"url":"http://arxiv.org/abs/2405.20921v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 15:23:31","title":"On the viscoelastic-electromagnetic-gravitational analogy","abstract":"The analogy between electromagnetism and gravitation was achieved by linearizing the tensorial gravitational equations of general relativity and converting them into a vector form corresponding to Maxwell's electromagnetic equations. On this basis, we use the equivalence with viscoelasticity (SH waves) and propose a theory of gravitational waves. We add a damping term to the differential equations, which is equivalent to Ohm's law in electromagnetism and Maxwell's viscosity in viscoelasticity, to describe the attenuation of the waves.   A plane-wave analysis gives the phase velocity, the energy velocity, the quality factor and the attenuation factor of the field as well as the energy balance. To obtain these properties, we use the analogy with viscoelasticity; the properties of electromagnetic and gravitational waves are similar to those of shear waves. The presence of attenuation means that the transient field is generally a composition of inhomogeneous (non-uniform) plane waves, where the propagation and attenuation vectors do not point in the same direction and the phase velocity vector and the energy flux (energy velocity) are not collinear. The polarization of cross-plane field is linear and perpendicular to the propagation-attenuation plane, while the polarization of the field within the plane is elliptical.   Transient wave fields in the space-time domain are analyzed with the Green function (in homogeneous media) and with a grid method (in heterogeneous media) based on the Fourier method for calculating the spatial derivatives and a Runge-Kutta scheme of order 4 for the time stepping. In the examples, wave propagation at the Sun-Earth and Earth-Moon distances using quadrupole sources is considered in comparison to viscoelastic waves. Finally, an example of propagation in heterogeneous media is presented.","sentences":["The analogy between electromagnetism and gravitation was achieved by linearizing the tensorial gravitational equations of general relativity and converting them into a vector form corresponding to Maxwell's electromagnetic equations.","On this basis, we use the equivalence with viscoelasticity (SH waves) and propose a theory of gravitational waves.","We add a damping term to the differential equations, which is equivalent to Ohm's law in electromagnetism and Maxwell's viscosity in viscoelasticity, to describe the attenuation of the waves.   ","A plane-wave analysis gives the phase velocity, the energy velocity, the quality factor and the attenuation factor of the field as well as the energy balance.","To obtain these properties, we use the analogy with viscoelasticity; the properties of electromagnetic and gravitational waves are similar to those of shear waves.","The presence of attenuation means that the transient field is generally a composition of inhomogeneous (non-uniform) plane waves, where the propagation and attenuation vectors do not point in the same direction and the phase velocity vector and the energy flux (energy velocity) are not collinear.","The polarization of cross-plane field is linear and perpendicular to the propagation-attenuation plane, while the polarization of the field within the plane is elliptical.   ","Transient wave fields in the space-time domain are analyzed with the Green function (in homogeneous media) and with a grid method (in heterogeneous media) based on the Fourier method for calculating the spatial derivatives and a Runge-Kutta scheme of order 4 for the time stepping.","In the examples, wave propagation at the Sun-Earth and Earth-Moon distances using quadrupole sources is considered in comparison to viscoelastic waves.","Finally, an example of propagation in heterogeneous media is presented."],"url":"http://arxiv.org/abs/2405.20920v1","category":"physics.geo-ph"}
{"created":"2024-05-31 15:21:59","title":"Flexible inference in heterogeneous and attributed multilayer networks","abstract":"Networked datasets are often enriched by different types of information about individual nodes or edges. However, most existing methods for analyzing such datasets struggle to handle the complexity of heterogeneous data, often requiring substantial model-specific analysis. In this paper, we develop a probabilistic generative model to perform inference in multilayer networks with arbitrary types of information. Our approach employs a Bayesian framework combined with the Laplace matching technique to ease interpretation of inferred parameters. Furthermore, the algorithmic implementation relies on automatic differentiation, avoiding the need for explicit derivations. This makes our model scalable and flexible to adapt to any combination of input data. We demonstrate the effectiveness of our method in detecting overlapping community structures and performing various prediction tasks on heterogeneous multilayer data, where nodes and edges have different types of attributes. Additionally, we showcase its ability to unveil a variety of patterns in a social support network among villagers in rural India by effectively utilizing all input information in a meaningful way.","sentences":["Networked datasets are often enriched by different types of information about individual nodes or edges.","However, most existing methods for analyzing such datasets struggle to handle the complexity of heterogeneous data, often requiring substantial model-specific analysis.","In this paper, we develop a probabilistic generative model to perform inference in multilayer networks with arbitrary types of information.","Our approach employs a Bayesian framework combined with the Laplace matching technique to ease interpretation of inferred parameters.","Furthermore, the algorithmic implementation relies on automatic differentiation, avoiding the need for explicit derivations.","This makes our model scalable and flexible to adapt to any combination of input data.","We demonstrate the effectiveness of our method in detecting overlapping community structures and performing various prediction tasks on heterogeneous multilayer data, where nodes and edges have different types of attributes.","Additionally, we showcase its ability to unveil a variety of patterns in a social support network among villagers in rural India by effectively utilizing all input information in a meaningful way."],"url":"http://arxiv.org/abs/2405.20918v1","category":"cs.SI"}
{"created":"2024-05-31 15:21:53","title":"Learning to Estimate System Specifications in Linear Temporal Logic using Transformers and Mamba","abstract":"Temporal logic is a framework for representing and reasoning about propositions that evolve over time. It is commonly used for specifying requirements in various domains, including hardware and software systems, as well as robotics. Specification mining or formula generation involves extracting temporal logic formulae from system traces and has numerous applications, such as detecting bugs and improving interpretability. Although there has been a surge of deep learning-based methods for temporal logic satisfiability checking in recent years, the specification mining literature has been lagging behind in adopting deep learning methods despite their many advantages, such as scalability. In this paper, we introduce autoregressive models that can generate linear temporal logic formulae from traces, towards addressing the specification mining problem. We propose multiple architectures for this task: transformer encoder-decoder, decoder-only transformer, and Mamba, which is an emerging alternative to transformer models. Additionally, we devise a metric for quantifying the distinctiveness of the generated formulae and a straightforward algorithm to enforce the syntax constraints. Our experiments show that the proposed architectures yield promising results, generating correct and distinct formulae at a fraction of the compute cost needed for the combinatorial baseline.","sentences":["Temporal logic is a framework for representing and reasoning about propositions that evolve over time.","It is commonly used for specifying requirements in various domains, including hardware and software systems, as well as robotics.","Specification mining or formula generation involves extracting temporal logic formulae from system traces and has numerous applications, such as detecting bugs and improving interpretability.","Although there has been a surge of deep learning-based methods for temporal logic satisfiability checking in recent years, the specification mining literature has been lagging behind in adopting deep learning methods despite their many advantages, such as scalability.","In this paper, we introduce autoregressive models that can generate linear temporal logic formulae from traces, towards addressing the specification mining problem.","We propose multiple architectures for this task: transformer encoder-decoder, decoder-only transformer, and Mamba, which is an emerging alternative to transformer models.","Additionally, we devise a metric for quantifying the distinctiveness of the generated formulae and a straightforward algorithm to enforce the syntax constraints.","Our experiments show that the proposed architectures yield promising results, generating correct and distinct formulae at a fraction of the compute cost needed for the combinatorial baseline."],"url":"http://arxiv.org/abs/2405.20917v1","category":"cs.CL"}
{"created":"2024-05-31 15:21:44","title":"Fast yet Safe: Early-Exiting with Risk Control","abstract":"Scaling machine learning models significantly improves their performance. However, such gains come at the cost of inference being slow and resource-intensive. Early-exit neural networks (EENNs) offer a promising solution: they accelerate inference by allowing intermediate layers to exit and produce a prediction early. Yet a fundamental issue with EENNs is how to determine when to exit without severely degrading performance. In other words, when is it 'safe' for an EENN to go 'fast'? To address this issue, we investigate how to adapt frameworks of risk control to EENNs. Risk control offers a distribution-free, post-hoc solution that tunes the EENN's exiting mechanism so that exits only occur when the output is of sufficient quality. We empirically validate our insights on a range of vision and language tasks, demonstrating that risk control can produce substantial computational savings, all the while preserving user-specified performance goals.","sentences":["Scaling machine learning models significantly improves their performance.","However, such gains come at the cost of inference being slow and resource-intensive.","Early-exit neural networks (EENNs) offer a promising solution: they accelerate inference by allowing intermediate layers to exit and produce a prediction early.","Yet a fundamental issue with EENNs is how to determine when to exit without severely degrading performance.","In other words, when is it 'safe' for an EENN to go 'fast'?","To address this issue, we investigate how to adapt frameworks of risk control to EENNs.","Risk control offers a distribution-free, post-hoc solution that tunes the EENN's exiting mechanism so that exits only occur when the output is of sufficient quality.","We empirically validate our insights on a range of vision and language tasks, demonstrating that risk control can produce substantial computational savings, all the while preserving user-specified performance goals."],"url":"http://arxiv.org/abs/2405.20915v1","category":"cs.LG"}
{"created":"2024-05-31 15:21:38","title":"RASE: Efficient Privacy-preserving Data Aggregation against Disclosure Attacks for IoTs","abstract":"The growing popular awareness of personal privacy raises the following quandary: what is the new paradigm for collecting and protecting the data produced by ever-increasing sensor devices. Most previous studies on co-design of data aggregation and privacy preservation assume that a trusted fusion center adheres to privacy regimes. Very recent work has taken steps towards relaxing the assumption by allowing data contributors to locally perturb their own data. Although these solutions withhold some data content to mitigate privacy risks, they have been shown to offer insufficient protection against disclosure attacks. Aiming at providing a more rigorous data safeguard for the Internet of Things (IoTs), this paper initiates the study of privacy-preserving data aggregation. We propose a novel paradigm (called RASE), which can be generalized into a 3-step sequential procedure, noise addition, followed by random permutation, and then parameter estimation. Specially, we design a differentially private randomizer, which carefully guides data contributors to obfuscate the truth. Then, a shuffler is employed to receive the noisy data from all data contributors. After that, it breaks the correct linkage between senders and receivers by applying a random permutation. The estimation phase involves using inaccurate data to calculate an approximate aggregate value. Extensive simulations are provided to explore the privacy-utility landscape of our RASE.","sentences":["The growing popular awareness of personal privacy raises the following quandary: what is the new paradigm for collecting and protecting the data produced by ever-increasing sensor devices.","Most previous studies on co-design of data aggregation and privacy preservation assume that a trusted fusion center adheres to privacy regimes.","Very recent work has taken steps towards relaxing the assumption by allowing data contributors to locally perturb their own data.","Although these solutions withhold some data content to mitigate privacy risks, they have been shown to offer insufficient protection against disclosure attacks.","Aiming at providing a more rigorous data safeguard for the Internet of Things (IoTs), this paper initiates the study of privacy-preserving data aggregation.","We propose a novel paradigm (called RASE), which can be generalized into a 3-step sequential procedure, noise addition, followed by random permutation, and then parameter estimation.","Specially, we design a differentially private randomizer, which carefully guides data contributors to obfuscate the truth.","Then, a shuffler is employed to receive the noisy data from all data contributors.","After that, it breaks the correct linkage between senders and receivers by applying a random permutation.","The estimation phase involves using inaccurate data to calculate an approximate aggregate value.","Extensive simulations are provided to explore the privacy-utility landscape of our RASE."],"url":"http://arxiv.org/abs/2405.20914v1","category":"cs.CR"}
{"created":"2024-05-31 15:21:20","title":"A Branch-Price-Cut-And-Switch Approach for Optimizing Team Formation and Routing for Airport Baggage Handling Tasks with Stochastic Travel Times","abstract":"In airport operations, optimally using dedicated personnel for baggage handling tasks plays a crucial role in the design of resource-efficient processes. Teams of workers with different qualifications must be formed, and loading or unloading tasks must be assigned to them. Each task has a time window within which it can be started and should be finished. Violating these temporal restrictions incurs severe financial penalties for the operator. In practice, various components of this process are subject to uncertainties. We consider the aforementioned problem under the assumption of stochastic travel times across the apron. We present two binary program formulations to model the problem at hand and solve it with a Branch-Price-Cut-and-Switch approach, in which we dynamically switch between two master problem formulations. Furthermore, we use an exact separation method to identify violated rank-1 Chv\\'atal-Gomory cuts and utilize an efficient branching rule relying on task finish times. We test the algorithm on instances generated based on real-world data from a major European hub airport with a planning horizon of up to two hours, 30 flights per hour, and three available task execution modes to choose from. Our results indicate that our algorithm is able to significantly outperform existing solution approaches. Moreover, an explicit consideration of stochastic travel times allows for solutions that utilize the available workforce more efficiently, while simultaneously guaranteeing a stable service level for the baggage handling operator.","sentences":["In airport operations, optimally using dedicated personnel for baggage handling tasks plays a crucial role in the design of resource-efficient processes.","Teams of workers with different qualifications must be formed, and loading or unloading tasks must be assigned to them.","Each task has a time window within which it can be started and should be finished.","Violating these temporal restrictions incurs severe financial penalties for the operator.","In practice, various components of this process are subject to uncertainties.","We consider the aforementioned problem under the assumption of stochastic travel times across the apron.","We present two binary program formulations to model the problem at hand and solve it with a Branch-Price-Cut-and-Switch approach, in which we dynamically switch between two master problem formulations.","Furthermore, we use an exact separation method to identify violated rank-1 Chv\\'atal-Gomory cuts and utilize an efficient branching rule relying on task finish times.","We test the algorithm on instances generated based on real-world data from a major European hub airport with a planning horizon of up to two hours, 30 flights per hour, and three available task execution modes to choose from.","Our results indicate that our algorithm is able to significantly outperform existing solution approaches.","Moreover, an explicit consideration of stochastic travel times allows for solutions that utilize the available workforce more efficiently, while simultaneously guaranteeing a stable service level for the baggage handling operator."],"url":"http://arxiv.org/abs/2405.20912v1","category":"econ.GN"}
{"created":"2024-05-31 15:21:06","title":"Predicting ptychography probe positions using single-shot phase retrieval neural network","abstract":"Ptychography is a powerful imaging technique that is used in a variety of fields, including materials science, biology, and nanotechnology. However, the accuracy of the reconstructed ptychography image is highly dependent on the accuracy of the recorded probe positions which often contain errors. These errors are typically corrected jointly with phase retrieval through numerical optimization approaches. When the error accumulates along the scan path or when the error magnitude is large, these approaches may not converge with satisfactory result. We propose a fundamentally new approach for ptychography probe position prediction for data with large position errors, where a neural network is used to make single-shot phase retrieval on individual diffraction patterns, yielding the object image at each scan point. The pairwise offsets among these images are then found using a robust image registration method, and the results are combined to yield the complete scan path by constructing and solving a linear equation. We show that our method can achieve good position prediction accuracy for data with large and accumulating errors on the order of $10^2$ pixels, a magnitude that often makes optimization-based algorithms fail to converge. For ptychography instruments without sophisticated position control equipment such as interferometers, our method is of significant practical potential.","sentences":["Ptychography is a powerful imaging technique that is used in a variety of fields, including materials science, biology, and nanotechnology.","However, the accuracy of the reconstructed ptychography image is highly dependent on the accuracy of the recorded probe positions which often contain errors.","These errors are typically corrected jointly with phase retrieval through numerical optimization approaches.","When the error accumulates along the scan path or when the error magnitude is large, these approaches may not converge with satisfactory result.","We propose a fundamentally new approach for ptychography probe position prediction for data with large position errors, where a neural network is used to make single-shot phase retrieval on individual diffraction patterns, yielding the object image at each scan point.","The pairwise offsets among these images are then found using a robust image registration method, and the results are combined to yield the complete scan path by constructing and solving a linear equation.","We show that our method can achieve good position prediction accuracy for data with large and accumulating errors on the order of $10^2$ pixels, a magnitude that often makes optimization-based algorithms fail to converge.","For ptychography instruments without sophisticated position control equipment such as interferometers, our method is of significant practical potential."],"url":"http://arxiv.org/abs/2405.20910v1","category":"physics.app-ph"}
{"created":"2024-05-31 15:18:08","title":"The Muckenhoupt condition","abstract":"The goal of this paper is to unify the theory of weights beyond the setting of weighted Lebesgue spaces in the general setting of quasi-Banach function spaces. We prove new characterizations for the boundedness of singular integrals, and pose several conjectures and partial results related to the duality of the Hardy-Littlewood maximal operator. Furthermore, we give an overview of the theory applied to weighted variable Lebesgue and Morrey spaces.","sentences":["The goal of this paper is to unify the theory of weights beyond the setting of weighted Lebesgue spaces in the general setting of quasi-Banach function spaces.","We prove new characterizations for the boundedness of singular integrals, and pose several conjectures and partial results related to the duality of the Hardy-Littlewood maximal operator.","Furthermore, we give an overview of the theory applied to weighted variable Lebesgue and Morrey spaces."],"url":"http://arxiv.org/abs/2405.20907v1","category":"math.FA"}
{"created":"2024-05-31 15:17:47","title":"Enhancing Vision Models for Text-Heavy Content Understanding and Interaction","abstract":"Interacting and understanding with text heavy visual content with multiple images is a major challenge for traditional vision models. This paper is on enhancing vision models' capability to comprehend or understand and learn from images containing a huge amount of textual information from the likes of textbooks and research papers which contain multiple images like graphs, etc and tables in them with different types of axes and scales. The approach involves dataset preprocessing, fine tuning which is by using instructional oriented data and evaluation. We also built a visual chat application integrating CLIP for image encoding and a model from the Massive Text Embedding Benchmark which is developed to consider both textual and visual inputs. An accuracy of 96.71% was obtained. The aim of the project is to increase and also enhance the advance vision models' capabilities in understanding complex visual textual data interconnected data, contributing to multimodal AI.","sentences":["Interacting and understanding with text heavy visual content with multiple images is a major challenge for traditional vision models.","This paper is on enhancing vision models' capability to comprehend or understand and learn from images containing a huge amount of textual information from the likes of textbooks and research papers which contain multiple images like graphs, etc and tables in them with different types of axes and scales.","The approach involves dataset preprocessing, fine tuning which is by using instructional oriented data and evaluation.","We also built a visual chat application integrating CLIP for image encoding and a model from the Massive Text Embedding Benchmark which is developed to consider both textual and visual inputs.","An accuracy of 96.71% was obtained.","The aim of the project is to increase and also enhance the advance vision models' capabilities in understanding complex visual textual data interconnected data, contributing to multimodal AI."],"url":"http://arxiv.org/abs/2405.20906v1","category":"cs.CV"}
{"created":"2024-05-31 15:15:04","title":"Preemptive Answer \"Attacks\" on Chain-of-Thought Reasoning","abstract":"Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent.","sentences":["Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting.","However, the robustness of this approach warrants further investigation.","In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning.","This situation can arise inadvertently or induced by malicious users by prompt injection attacks.","Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets.","To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent."],"url":"http://arxiv.org/abs/2405.20902v1","category":"cs.CL"}
{"created":"2024-05-31 15:03:41","title":"On the transitivity of Lie ideals and a characterization of perfect Lie algebras","abstract":"We explore general intrinsic and extrinsic conditions that allow the transitivity of the relation of being a Lie ideal, in the sense that if a Lie algebra $\\mathfrak{h}$ is a subideal of a Lie algebra $\\mathfrak{g}$ (i.e. there exist Lie subalgebras $\\mathfrak{l}_0,\\mathfrak{l}_1,\\dots,\\mathfrak{l}_n$ of $\\mathfrak{g}$ with $\\mathfrak{h}=\\mathfrak{l}_0\\unlhd \\mathfrak{l}_1 \\unlhd\\cdots \\unlhd \\mathfrak{l}_n=\\mathfrak{g}$), then $\\mathfrak{h}$ is an ideal of $\\mathfrak{g}$. We also prove that perfect Lie algebras of arbitrary dimension and over any field are intrinsically characterized by transitivity of this type; In particular, we show that a Lie algebra $\\mathfrak{h}$ is perfect (i.e. $\\mathfrak{h}=[\\mathfrak{h}, \\mathfrak{h}]$) if and only if for any Lie algebra $\\mathfrak{g}$ such that $\\mathfrak{h}$ is a subideal of $\\mathfrak{g}$, it follows that $\\mathfrak{h}$ is an ideal of $\\mathfrak{g}$.","sentences":["We explore general intrinsic and extrinsic conditions that allow the transitivity of the relation of being a Lie ideal, in the sense that if a Lie algebra $\\mathfrak{h}$ is a subideal of a Lie algebra $\\mathfrak{g}$ (i.e. there exist Lie subalgebras $\\mathfrak{l}_0,\\mathfrak{l}_1,\\dots,\\mathfrak{l}_n$ of $\\mathfrak{g}$ with $\\mathfrak{h}=\\mathfrak{l}_0\\unlhd \\mathfrak{l}_1 \\unlhd\\cdots \\unlhd \\mathfrak{l}_n=\\mathfrak{g}$), then $\\mathfrak{h}$ is an ideal of $\\mathfrak{g}$. We also prove that perfect Lie algebras of arbitrary dimension and over any field are intrinsically characterized by transitivity of this type; In particular, we show that a Lie algebra $\\mathfrak{h}$ is perfect (i.e. $\\mathfrak{h}=[\\mathfrak{h}, \\mathfrak{h}]$) if and only if for any Lie algebra $\\mathfrak{g}$ such that $\\mathfrak{h}$ is a subideal of $\\mathfrak{g}$, it follows that $\\mathfrak{h}$ is an ideal of $\\mathfrak{g}$."],"url":"http://arxiv.org/abs/2405.20893v1","category":"math.RA"}
{"created":"2024-05-31 15:03:35","title":"MALT: Multi-scale Action Learning Transformer for Online Action Detection","abstract":"Online action detection (OAD) aims to identify ongoing actions from streaming video in real-time, without access to future frames. Since these actions manifest at varying scales of granularity, ranging from coarse to fine, projecting an entire set of action frames to a single latent encoding may result in a lack of local information, necessitating the acquisition of action features across multiple scales. In this paper, we propose a multi-scale action learning transformer (MALT), which includes a novel recurrent decoder (used for feature fusion) that includes fewer parameters and can be trained more efficiently. A hierarchical encoder with multiple encoding branches is further proposed to capture multi-scale action features. The output from the preceding branch is then incrementally input to the subsequent branch as part of a cross-attention calculation. In this way, output features transition from coarse to fine as the branches deepen. We also introduce an explicit frame scoring mechanism employing sparse attention, which filters irrelevant frames more efficiently, without requiring an additional network. The proposed method achieved state-of-the-art performance on two benchmark datasets (THUMOS'14 and TVSeries), outperforming all existing models used for comparison, with an mAP of 0.2% for THUMOS'14 and an mcAP of 0.1% for TVseries.","sentences":["Online action detection (OAD) aims to identify ongoing actions from streaming video in real-time, without access to future frames.","Since these actions manifest at varying scales of granularity, ranging from coarse to fine, projecting an entire set of action frames to a single latent encoding may result in a lack of local information, necessitating the acquisition of action features across multiple scales.","In this paper, we propose a multi-scale action learning transformer (MALT), which includes a novel recurrent decoder (used for feature fusion) that includes fewer parameters and can be trained more efficiently.","A hierarchical encoder with multiple encoding branches is further proposed to capture multi-scale action features.","The output from the preceding branch is then incrementally input to the subsequent branch as part of a cross-attention calculation.","In this way, output features transition from coarse to fine as the branches deepen.","We also introduce an explicit frame scoring mechanism employing sparse attention, which filters irrelevant frames more efficiently, without requiring an additional network.","The proposed method achieved state-of-the-art performance on two benchmark datasets (THUMOS'14 and TVSeries), outperforming all existing models used for comparison, with an mAP of 0.2% for THUMOS'14 and an mcAP of 0.1% for TVseries."],"url":"http://arxiv.org/abs/2405.20892v1","category":"cs.CV"}
{"created":"2024-05-31 15:02:10","title":"On the largest independent sets in the Kneser graph on chambers of PG(4,q)","abstract":"Let $\\Gamma_4$ be the graph whose vertices are the chambers of the finite projective $4$-space PG(4,q), with two vertices being adjacent if the corresponding chambers are in general position. For $q\\geq 749 $ we show that $\\alpha:=(q^2+q+1)(q^3+2q^2+q+1)(q+1)^2$ is the independence number of $\\Gamma_4$ and the geometric structure of independent sets with $\\alpha$ vertices is described.","sentences":["Let $\\Gamma_4$ be the graph whose vertices are the chambers of the finite projective $4$-space PG(4,q), with two vertices being adjacent if the corresponding chambers are in general position.","For $q\\geq 749 $ we show that $\\alpha:=(q^2+q+1)(q^3+2q^2+q+1)(q+1)^2$ is the independence number of $\\Gamma_4$ and the geometric structure of independent sets with $\\alpha$ vertices is described."],"url":"http://arxiv.org/abs/2405.20891v1","category":"math.CO"}
{"created":"2024-05-31 14:58:24","title":"Interfaces of the two-dimensional voter model in the context of SLE","abstract":"This paper investigates various geometrical properties of interfaces of the two-dimensional voter model. Despite its simplicity, the model exhibits dual characteristics, resembling both a critical system with long-range correlations, while also showing a tendency towards order similar to the Ising-Glauber model at zero temperature. This duality is reflected in the geometrical properties of its interfaces, which are examined here from the perspective of Schramm-Loewner evolution. Recent studies have delved into the geometrical properties of these interfaces within different lattice geometries and boundary conditions. We revisit these findings, focusing on a system within a box of linear size $L$ with Dobrushin boundary conditions, where values of the spins are fixed to either $+1$ or $-1$ on two distinct halves of the boundary, in order to enforce the presence of a pinned interface with fixed endpoints (or chordal interface). We also expand the study to compare the geometrical properties of the interfaces of the voter model with those of the critical Ising model and other related models. Scaling arguments and numerical studies suggest that, while locally the chordal interface of the voter model has fractal dimension $d_{\\rm f}=3/2$, corresponding to a parameter $\\kappa=4$, it becomes straight at large scales, confirming a conjecture made by Holmes et al \\cite{holmes}, and ruling out the possibility of describing the chordal interface of the voter model by SLE$_{\\kappa}$, for any non zero value of $\\kappa$. This contrasts with the critical Ising model, which is described by SLE$_3$, and whose interface fluctuations remain of order $L$, and more generally with related critical models, which are in the same universality class.","sentences":["This paper investigates various geometrical properties of interfaces of the two-dimensional voter model.","Despite its simplicity, the model exhibits dual characteristics, resembling both a critical system with long-range correlations, while also showing a tendency towards order similar to the Ising-Glauber model at zero temperature.","This duality is reflected in the geometrical properties of its interfaces, which are examined here from the perspective of Schramm-Loewner evolution.","Recent studies have delved into the geometrical properties of these interfaces within different lattice geometries and boundary conditions.","We revisit these findings, focusing on a system within a box of linear size $L$ with Dobrushin boundary conditions, where values of the spins are fixed to either $+1$ or $-1$ on two distinct halves of the boundary, in order to enforce the presence of a pinned interface with fixed endpoints (or chordal interface).","We also expand the study to compare the geometrical properties of the interfaces of the voter model with those of the critical Ising model and other related models.","Scaling arguments and numerical studies suggest that, while locally the chordal interface of the voter model has fractal dimension $d_{\\rm f}=3/2$, corresponding to a parameter $\\kappa=4$, it becomes straight at large scales, confirming a conjecture made by Holmes et al \\cite{holmes}, and ruling out the possibility of describing the chordal interface of the voter model by SLE$_{\\kappa}$, for any non zero value of $\\kappa$. This contrasts with the critical Ising model, which is described by SLE$_3$, and whose interface fluctuations remain of order $L$, and more generally with related critical models, which are in the same universality class."],"url":"http://arxiv.org/abs/2405.20885v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-31 14:55:11","title":"Paying to Do Better: Games with Payments between Learning Agents","abstract":"In repeated games, such as auctions, players typically use learning algorithms to choose their actions. The use of such autonomous learning agents has become widespread on online platforms. In this paper, we explore the impact of players incorporating monetary transfers into their agents' algorithms, aiming to incentivize behavior in their favor. Our focus is on understanding when players have incentives to make use of monetary transfers, how these payments affect learning dynamics, and what the implications are for welfare and its distribution among the players. We propose a simple game-theoretic model to capture such scenarios. Our results on general games show that in a broad class of games, players benefit from letting their learning agents make payments to other learners during the game dynamics, and that in many cases, this kind of behavior improves welfare for all players. Our results on first- and second-price auctions show that in equilibria of the ``payment policy game,'' the agents' dynamics can reach strong collusive outcomes with low revenue for the auctioneer. These results highlight a challenge for mechanism design in systems where automated learning agents can benefit from interacting with their peers outside the boundaries of the mechanism.","sentences":["In repeated games, such as auctions, players typically use learning algorithms to choose their actions.","The use of such autonomous learning agents has become widespread on online platforms.","In this paper, we explore the impact of players incorporating monetary transfers into their agents' algorithms, aiming to incentivize behavior in their favor.","Our focus is on understanding when players have incentives to make use of monetary transfers, how these payments affect learning dynamics, and what the implications are for welfare and its distribution among the players.","We propose a simple game-theoretic model to capture such scenarios.","Our results on general games show that in a broad class of games, players benefit from letting their learning agents make payments to other learners during the game dynamics, and that in many cases, this kind of behavior improves welfare for all players.","Our results on first- and second-price auctions show that in equilibria of the ``payment policy game,'' the agents' dynamics can reach strong collusive outcomes with low revenue for the auctioneer.","These results highlight a challenge for mechanism design in systems where automated learning agents can benefit from interacting with their peers outside the boundaries of the mechanism."],"url":"http://arxiv.org/abs/2405.20880v1","category":"cs.GT"}
{"created":"2024-05-31 14:54:51","title":"Flow matching achieves minimax optimal convergence","abstract":"Flow matching (FM) has gained significant attention as a simulation-free generative model. Unlike diffusion models, which are based on stochastic differential equations, FM employs a simpler approach by solving an ordinary differential equation with an initial condition from a normal distribution, thus streamlining the sample generation process. This paper discusses the convergence properties of FM in terms of the $p$-Wasserstein distance, a measure of distributional discrepancy. We establish that FM can achieve the minmax optimal convergence rate for $1 \\leq p \\leq 2$, presenting the first theoretical evidence that FM can reach convergence rates comparable to those of diffusion models. Our analysis extends existing frameworks by examining a broader class of mean and variance functions for the vector fields and identifies specific conditions necessary to attain these optimal rates.","sentences":["Flow matching (FM) has gained significant attention as a simulation-free generative model.","Unlike diffusion models, which are based on stochastic differential equations, FM employs a simpler approach by solving an ordinary differential equation with an initial condition from a normal distribution, thus streamlining the sample generation process.","This paper discusses the convergence properties of FM in terms of the $p$-Wasserstein distance, a measure of distributional discrepancy.","We establish that FM can achieve the minmax optimal convergence rate for $1 \\leq p \\leq 2$, presenting the first theoretical evidence that FM can reach convergence rates comparable to those of diffusion models.","Our analysis extends existing frameworks by examining a broader class of mean and variance functions for the vector fields and identifies specific conditions necessary to attain these optimal rates."],"url":"http://arxiv.org/abs/2405.20879v1","category":"cs.LG"}
{"created":"2024-05-31 14:53:12","title":"SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation","abstract":"Sequential recommendation effectively addresses information overload by modeling users' temporal and sequential interaction patterns. To overcome the limitations of supervision signals, recent approaches have adopted self-supervised learning techniques in recommender systems. However, there are still two critical challenges that remain unsolved. Firstly, existing sequential models primarily focus on long-term modeling of individual interaction sequences, overlooking the valuable short-term collaborative relationships among the behaviors of different users. Secondly, real-world data often contain noise, particularly in users' short-term behaviors, which can arise from temporary intents or misclicks. Such noise negatively impacts the accuracy of both graph and sequence models, further complicating the modeling process. To address these challenges, we propose a novel framework called Self-Supervised Graph Neural Network (SelfGNN) for sequential recommendation. The SelfGNN framework encodes short-term graphs based on time intervals and utilizes Graph Neural Networks (GNNs) to learn short-term collaborative relationships. It captures long-term user and item representations at multiple granularity levels through interval fusion and dynamic behavior modeling. Importantly, our personalized self-augmented learning structure enhances model robustness by mitigating noise in short-term graphs based on long-term user interests and personal stability. Extensive experiments conducted on four real-world datasets demonstrate that SelfGNN outperforms various state-of-the-art baselines. Our model implementation codes are available at https://github.com/HKUDS/SelfGNN.","sentences":["Sequential recommendation effectively addresses information overload by modeling users' temporal and sequential interaction patterns.","To overcome the limitations of supervision signals, recent approaches have adopted self-supervised learning techniques in recommender systems.","However, there are still two critical challenges that remain unsolved.","Firstly, existing sequential models primarily focus on long-term modeling of individual interaction sequences, overlooking the valuable short-term collaborative relationships among the behaviors of different users.","Secondly, real-world data often contain noise, particularly in users' short-term behaviors, which can arise from temporary intents or misclicks.","Such noise negatively impacts the accuracy of both graph and sequence models, further complicating the modeling process.","To address these challenges, we propose a novel framework called Self-Supervised Graph Neural Network (SelfGNN) for sequential recommendation.","The SelfGNN framework encodes short-term graphs based on time intervals and utilizes Graph Neural Networks (GNNs) to learn short-term collaborative relationships.","It captures long-term user and item representations at multiple granularity levels through interval fusion and dynamic behavior modeling.","Importantly, our personalized self-augmented learning structure enhances model robustness by mitigating noise in short-term graphs based on long-term user interests and personal stability.","Extensive experiments conducted on four real-world datasets demonstrate that SelfGNN outperforms various state-of-the-art baselines.","Our model implementation codes are available at https://github.com/HKUDS/SelfGNN."],"url":"http://arxiv.org/abs/2405.20878v1","category":"cs.IR"}
{"created":"2024-05-31 14:52:58","title":"Waveform Design for Over-the-Air Computing","abstract":"In response to the increasing number of devices anticipated in next-generation networks, a shift toward over-the-air (OTA) computing has been proposed. Leveraging the superposition of multiple access channels, OTA computing enables efficient resource management by supporting simultaneous uncoded transmission in the time and the frequency domain. Thus, to advance the integration of OTA computing, our study presents a theoretical analysis addressing practical issues encountered in current digital communication transceivers, such as time sampling error and intersymbol interference (ISI). To this end, we examine the theoretical mean squared error (MSE) for OTA transmission under time sampling error and ISI, while also exploring methods for minimizing the MSE in the OTA transmission. Utilizing alternating optimization, we also derive optimal power policies for both the devices and the base station. Additionally, we propose a novel deep neural network (DNN)-based approach to design waveforms enhancing OTA transmission performance under time sampling error and ISI. To ensure fair comparison with existing waveforms like the raised cosine (RC) and the better-than-raised-cosine (BRTC), we incorporate a custom loss function integrating energy and bandwidth constraints, along with practical design considerations such as waveform symmetry. Simulation results validate our theoretical analysis and demonstrate performance gains of the designed pulse over RC and BTRC waveforms. To facilitate testing of our results without necessitating the DNN structure recreation, we provide curve fitting parameters for select DNN-based waveforms as well.","sentences":["In response to the increasing number of devices anticipated in next-generation networks, a shift toward over-the-air (OTA) computing has been proposed.","Leveraging the superposition of multiple access channels, OTA computing enables efficient resource management by supporting simultaneous uncoded transmission in the time and the frequency domain.","Thus, to advance the integration of OTA computing, our study presents a theoretical analysis addressing practical issues encountered in current digital communication transceivers, such as time sampling error and intersymbol interference (ISI).","To this end, we examine the theoretical mean squared error (MSE) for OTA transmission under time sampling error and ISI, while also exploring methods for minimizing the MSE in the OTA transmission.","Utilizing alternating optimization, we also derive optimal power policies for both the devices and the base station.","Additionally, we propose a novel deep neural network (DNN)-based approach to design waveforms enhancing OTA transmission performance under time sampling error and ISI.","To ensure fair comparison with existing waveforms like the raised cosine (RC) and the better-than-raised-cosine (BRTC), we incorporate a custom loss function integrating energy and bandwidth constraints, along with practical design considerations such as waveform symmetry.","Simulation results validate our theoretical analysis and demonstrate performance gains of the designed pulse over RC and BTRC waveforms.","To facilitate testing of our results without necessitating the DNN structure recreation, we provide curve fitting parameters for select DNN-based waveforms as well."],"url":"http://arxiv.org/abs/2405.20877v1","category":"cs.IT"}
{"created":"2024-05-31 14:52:49","title":"Investigating Calibration and Corruption Robustness of Post-hoc Pruned Perception CNNs: An Image Classification Benchmark Study","abstract":"Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in many computer vision tasks. However, high computational and storage demands hinder their deployment into resource-constrained environments, such as embedded devices. Model pruning helps to meet these restrictions by reducing the model size, while maintaining superior performance. Meanwhile, safety-critical applications pose more than just resource and performance constraints. In particular, predictions must not be overly confident, i.e., provide properly calibrated uncertainty estimations (proper uncertainty calibration), and CNNs must be robust against corruptions like naturally occurring input perturbations (natural corruption robustness). This work investigates the important trade-off between uncertainty calibration, natural corruption robustness, and performance for current state-of-research post-hoc CNN pruning techniques in the context of image classification tasks. Our study reveals that post-hoc pruning substantially improves the model's uncertainty calibration, performance, and natural corruption robustness, sparking hope for safe and robust embedded CNNs.Furthermore, uncertainty calibration and natural corruption robustness are not mutually exclusive targets under pruning, as evidenced by the improved safety aspects obtained by post-hoc unstructured pruning with increasing compression.","sentences":["Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in many computer vision tasks.","However, high computational and storage demands hinder their deployment into resource-constrained environments, such as embedded devices.","Model pruning helps to meet these restrictions by reducing the model size, while maintaining superior performance.","Meanwhile, safety-critical applications pose more than just resource and performance constraints.","In particular, predictions must not be overly confident, i.e., provide properly calibrated uncertainty estimations (proper uncertainty calibration), and CNNs must be robust against corruptions like naturally occurring input perturbations (natural corruption robustness).","This work investigates the important trade-off between uncertainty calibration, natural corruption robustness, and performance for current state-of-research post-hoc CNN pruning techniques in the context of image classification tasks.","Our study reveals that post-hoc pruning substantially improves the model's uncertainty calibration, performance, and natural corruption robustness, sparking hope for safe and robust embedded CNNs.","Furthermore, uncertainty calibration and natural corruption robustness are not mutually exclusive targets under pruning, as evidenced by the improved safety aspects obtained by post-hoc unstructured pruning with increasing compression."],"url":"http://arxiv.org/abs/2405.20876v1","category":"cs.CV"}
{"created":"2024-05-31 14:51:48","title":"Borromean Hypergraph Formation in Dense Random Rectangles","abstract":"We develop a minimal system to study the stochastic formation of Borromean links within topologically entangled networks without requiring the use of knot invariants. Borromean linkages may form in entangled solutions of open polymer chains or in Olympic gel systems such as kinetoplast DNA, but it is challenging to investigate this due to the difficulty of computing three-body link invariants. Here, we investigate randomly oriented rectangles densely packed within a volume, and evaluate them for Hopf linking and Borromean link formation. We show that dense packings of rectangles can form Borromean triplets and larger clusters, and that in high enough density the combination of Hopf and Borromean linking can create a percolating hypergraph through the network. We present data for the percolation threshold of Borromean hypergraphs, and discuss implications for the existence of Borromean connectivity within kinetoplast DNA.","sentences":["We develop a minimal system to study the stochastic formation of Borromean links within topologically entangled networks without requiring the use of knot invariants.","Borromean linkages may form in entangled solutions of open polymer chains or in Olympic gel systems such as kinetoplast DNA, but it is challenging to investigate this due to the difficulty of computing three-body link invariants.","Here, we investigate randomly oriented rectangles densely packed within a volume, and evaluate them for Hopf linking and Borromean link formation.","We show that dense packings of rectangles can form Borromean triplets and larger clusters, and that in high enough density the combination of Hopf and Borromean linking can create a percolating hypergraph through the network.","We present data for the percolation threshold of Borromean hypergraphs, and discuss implications for the existence of Borromean connectivity within kinetoplast DNA."],"url":"http://arxiv.org/abs/2405.20874v1","category":"cond-mat.soft"}
{"created":"2024-05-31 14:47:27","title":"Responsible AI for Earth Observation","abstract":"The convergence of artificial intelligence (AI) and Earth observation (EO) technologies has brought geoscience and remote sensing into an era of unparalleled capabilities. AI's transformative impact on data analysis, particularly derived from EO platforms, holds great promise in addressing global challenges such as environmental monitoring, disaster response and climate change analysis. However, the rapid integration of AI necessitates a careful examination of the responsible dimensions inherent in its application within these domains. In this paper, we represent a pioneering effort to systematically define the intersection of AI and EO, with a central focus on responsible AI practices. Specifically, we identify several critical components guiding this exploration from both academia and industry perspectives within the EO field: AI and EO for social good, mitigating unfair biases, AI security in EO, geo-privacy and privacy-preserving measures, as well as maintaining scientific excellence, open data, and guiding AI usage based on ethical principles. Furthermore, the paper explores potential opportunities and emerging trends, providing valuable insights for future research endeavors.","sentences":["The convergence of artificial intelligence (AI) and Earth observation (EO) technologies has brought geoscience and remote sensing into an era of unparalleled capabilities.","AI's transformative impact on data analysis, particularly derived from EO platforms, holds great promise in addressing global challenges such as environmental monitoring, disaster response and climate change analysis.","However, the rapid integration of AI necessitates a careful examination of the responsible dimensions inherent in its application within these domains.","In this paper, we represent a pioneering effort to systematically define the intersection of AI and EO, with a central focus on responsible AI practices.","Specifically, we identify several critical components guiding this exploration from both academia and industry perspectives within the EO field: AI and EO for social good, mitigating unfair biases, AI security in EO, geo-privacy and privacy-preserving measures, as well as maintaining scientific excellence, open data, and guiding AI usage based on ethical principles.","Furthermore, the paper explores potential opportunities and emerging trends, providing valuable insights for future research endeavors."],"url":"http://arxiv.org/abs/2405.20868v1","category":"cs.CV"}
{"created":"2024-05-31 14:47:20","title":"Automatic Channel Pruning for Multi-Head Attention","abstract":"Despite the strong performance of Transformers, their quadratic computation complexity presents challenges in applying them to vision tasks. Automatic pruning is one of effective methods for reducing computation complexity without heuristic approaches. However, directly applying it to multi-head attention is not straightforward due to channel misalignment. In this paper, we propose an automatic channel pruning method to take into account the multi-head attention mechanism. First, we incorporate channel similarity-based weights into the pruning indicator to preserve more informative channels in each head. Then, we adjust pruning indicator to enforce removal of channels in equal proportions across all heads, preventing the channel misalignment. We also add a reweight module to compensate for information loss resulting from channel removal, and an effective initialization step for pruning indicator based on difference of attention between original structure and each channel. Our proposed method can be used to not only original attention, but also linear attention, which is more efficient as linear complexity with respect to the number of tokens. On ImageNet-1K, applying our pruning method to the FLattenTransformer, which includes both attention mechanisms, shows outperformed accuracy for several MACs compared with previous state-of-the-art efficient models and pruned methods. Code will be available soon.","sentences":["Despite the strong performance of Transformers, their quadratic computation complexity presents challenges in applying them to vision tasks.","Automatic pruning is one of effective methods for reducing computation complexity without heuristic approaches.","However, directly applying it to multi-head attention is not straightforward due to channel misalignment.","In this paper, we propose an automatic channel pruning method to take into account the multi-head attention mechanism.","First, we incorporate channel similarity-based weights into the pruning indicator to preserve more informative channels in each head.","Then, we adjust pruning indicator to enforce removal of channels in equal proportions across all heads, preventing the channel misalignment.","We also add a reweight module to compensate for information loss resulting from channel removal, and an effective initialization step for pruning indicator based on difference of attention between original structure and each channel.","Our proposed method can be used to not only original attention, but also linear attention, which is more efficient as linear complexity with respect to the number of tokens.","On ImageNet-1K, applying our pruning method to the FLattenTransformer, which includes both attention mechanisms, shows outperformed accuracy for several MACs compared with previous state-of-the-art efficient models and pruned methods.","Code will be available soon."],"url":"http://arxiv.org/abs/2405.20867v1","category":"cs.CV"}
{"created":"2024-05-31 14:46:43","title":"Poynting flux transport channels formed in polar cap regions of neutron star magnetospheres","abstract":"Pair cascades in polar cap regions of neutron stars are considered to be an essential process in various models of coherent radio emissions of pulsars. The cascades produce pair plasma bunch discharges in quasi-periodic spark events. The cascade properties, and therefore also the coherent radiation, depend strongly on the magnetospheric plasma properties and vary significantly across and along the polar cap. It is furthermore still uncertain from where the radio emission emanates in polar cap region.   We investigate the generation of electromagnetic waves by pair cascades and their propagation in the polar cap for three representative inclination angles of a magnetic dipole, $0^\\circ$, $45^\\circ$, and $90^\\circ$.   2D particle-in-cell simulations that include quantum-electrodynamic pair cascades are used in a charge limited flow from the star surface.   We found that the discharge properties are strongly dependent on the magnetospheric current profile in the polar cap and that transport channels for high intensity Poynting flux are formed along magnetic field lines where the magnetospheric currents approach zero and where the plasma cannot carry the magnetospheric currents. There, the parallel Poynting flux component is efficiently transported away from the star and may eventually escape the magnetosphere as coherent radio waves. The Poynting flux decreases faster with the distance from the star in regions of high magnetospheric currents.   Our model shows that no process of energy conversion from particles to waves is necessary for the coherent radio wave emission. Moreover, the pulsar radio beam does not have a cone structure, but rather the radiation generated by the oscillating electric gap fields directly escapes along open magnetic field lines in which no pair creation occurs.","sentences":["Pair cascades in polar cap regions of neutron stars are considered to be an essential process in various models of coherent radio emissions of pulsars.","The cascades produce pair plasma bunch discharges in quasi-periodic spark events.","The cascade properties, and therefore also the coherent radiation, depend strongly on the magnetospheric plasma properties and vary significantly across and along the polar cap.","It is furthermore still uncertain from where the radio emission emanates in polar cap region.   ","We investigate the generation of electromagnetic waves by pair cascades and their propagation in the polar cap for three representative inclination angles of a magnetic dipole, $0^\\circ$, $45^\\circ$, and $90^\\circ$.   2D particle-in-cell simulations that include quantum-electrodynamic pair cascades are used in a charge limited flow from the star surface.   ","We found that the discharge properties are strongly dependent on the magnetospheric current profile in the polar cap and that transport channels for high intensity Poynting flux are formed along magnetic field lines where the magnetospheric currents approach zero and where the plasma cannot carry the magnetospheric currents.","There, the parallel Poynting flux component is efficiently transported away from the star and may eventually escape the magnetosphere as coherent radio waves.","The Poynting flux decreases faster with the distance from the star in regions of high magnetospheric currents.   ","Our model shows that no process of energy conversion from particles to waves is necessary for the coherent radio wave emission.","Moreover, the pulsar radio beam does not have a cone structure, but rather the radiation generated by the oscillating electric gap fields directly escapes along open magnetic field lines in which no pair creation occurs."],"url":"http://arxiv.org/abs/2405.20866v1","category":"astro-ph.HE"}
{"created":"2024-05-31 14:46:04","title":"Cartan Geometry and Infinite-Dimensional Kempf-Ness Theory","abstract":"We pioneer the development of a rigorous infinite-dimensional framework for the Kempf-Ness theorem, addressing the significant challenge posed by the absence of a complexification for the symmetry group in infinite dimensions, e.g, the diffeomorphism group. We propose a novel approach, based on Cartan bundles, to generalize Kempf-Ness theory to infinite dimensions, invoking the fundamental role played by the Maurer-Cartan form. This approach allows us to define and study objects essential for the Kempf-Ness theorem, such as the complex model for orbits and the Kempf-Ness function, as well as establishing its convexity properties and defining a generalized Futaki character. We show how our framework can be applied to the study of various problems in K\\\"ahler geometry, deformation quantization, and gauge theory.","sentences":["We pioneer the development of a rigorous infinite-dimensional framework for the Kempf-Ness theorem, addressing the significant challenge posed by the absence of a complexification for the symmetry group in infinite dimensions, e.g, the diffeomorphism group.","We propose a novel approach, based on Cartan bundles, to generalize Kempf-Ness theory to infinite dimensions, invoking the fundamental role played by the Maurer-Cartan form.","This approach allows us to define and study objects essential for the Kempf-Ness theorem, such as the complex model for orbits and the Kempf-Ness function, as well as establishing its convexity properties and defining a generalized Futaki character.","We show how our framework can be applied to the study of various problems in K\\\"ahler geometry, deformation quantization, and gauge theory."],"url":"http://arxiv.org/abs/2405.20864v1","category":"math.DG"}
{"created":"2024-05-31 14:46:04","title":"On alternative formulations of the thermodynamics of scalar-tensor theories","abstract":"We explore alternative formulations of the analogy between viable Horndeski gravity and Eckart's first-order thermodynamics. We single out a class of identifications for the effective stress-energy tensor of the scalar field fluid that, upon performing the imperfect fluid decomposition, yields constitutive relations that can be mapped onto Eckart's theory. We then investigate how different couplings to Einstein's gravity, at the level of the field equations, can affect the thermodynamic formalism overall. Lastly, we specialise the discussion to the case of ``traditional'' scalar-tensor theories and identify a specific choice of the coupling function that leads to a significant simplification of the formalism.","sentences":["We explore alternative formulations of the analogy between viable Horndeski gravity and Eckart's first-order thermodynamics.","We single out a class of identifications for the effective stress-energy tensor of the scalar field fluid that, upon performing the imperfect fluid decomposition, yields constitutive relations that can be mapped onto Eckart's theory.","We then investigate how different couplings to Einstein's gravity, at the level of the field equations, can affect the thermodynamic formalism overall.","Lastly, we specialise the discussion to the case of ``traditional'' scalar-tensor theories and identify a specific choice of the coupling function that leads to a significant simplification of the formalism."],"url":"http://arxiv.org/abs/2405.20865v1","category":"gr-qc"}
{"created":"2024-05-31 14:45:11","title":"ABodyBuilder3: Improved and scalable antibody structure predictions","abstract":"Accurate prediction of antibody structure is a central task in the design and development of monoclonal antibodies, notably to understand both their developability and their binding properties. In this article, we introduce ABodyBuilder3, an improved and scalable antibody structure prediction model based on ImmuneBuilder. We achieve a new state-of-the-art accuracy in the modelling of CDR loops by leveraging language model embeddings, and show how predicted structures can be further improved through careful relaxation strategies. Finally, we incorporate a predicted Local Distance Difference Test into the model output to allow for a more accurate estimation of uncertainties.","sentences":["Accurate prediction of antibody structure is a central task in the design and development of monoclonal antibodies, notably to understand both their developability and their binding properties.","In this article, we introduce ABodyBuilder3, an improved and scalable antibody structure prediction model based on ImmuneBuilder.","We achieve a new state-of-the-art accuracy in the modelling of CDR loops by leveraging language model embeddings, and show how predicted structures can be further improved through careful relaxation strategies.","Finally, we incorporate a predicted Local Distance Difference Test into the model output to allow for a more accurate estimation of uncertainties."],"url":"http://arxiv.org/abs/2405.20863v1","category":"q-bio.BM"}
{"created":"2024-05-31 14:43:31","title":"clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents","abstract":"It has been established in recent work that Large Language Models (LLMs) can be prompted to \"self-play\" conversational games that probe certain capabilities (general instruction following, strategic goal orientation, language understanding abilities), where the resulting interactive game play can be automatically scored. In this paper, we take one of the proposed frameworks for setting up such game-play environments, and further test its usefulness as an evaluation instrument, along a number of dimensions: We show that it can easily keep up with new developments while avoiding data contamination, we show that the tests implemented within it are not yet saturated (human performance is substantially higher than that of even the best models), and we show that it lends itself to investigating additional questions, such as the impact of the prompting language on performance. We believe that the approach forms a good basis for making decisions on model choice for building applied interactive systems, and perhaps ultimately setting up a closed-loop development environment of system and simulated evaluator.","sentences":["It has been established in recent work that Large Language Models (LLMs) can be prompted to \"self-play\" conversational games that probe certain capabilities (general instruction following, strategic goal orientation, language understanding abilities), where the resulting interactive game play can be automatically scored.","In this paper, we take one of the proposed frameworks for setting up such game-play environments, and further test its usefulness as an evaluation instrument, along a number of dimensions: We show that it can easily keep up with new developments while avoiding data contamination, we show that the tests implemented within it are not yet saturated (human performance is substantially higher than that of even the best models), and we show that it lends itself to investigating additional questions, such as the impact of the prompting language on performance.","We believe that the approach forms a good basis for making decisions on model choice for building applied interactive systems, and perhaps ultimately setting up a closed-loop development environment of system and simulated evaluator."],"url":"http://arxiv.org/abs/2405.20859v1","category":"cs.CL"}
{"created":"2024-05-31 14:39:14","title":"Parameter identification in linear non-Gaussian causal models under general confounding","abstract":"Linear non-Gaussian causal models postulate that each random variable is a linear function of parent variables and non-Gaussian exogenous error terms. We study identification of the linear coefficients when such models contain latent variables. Our focus is on the commonly studied acyclic setting, where each model corresponds to a directed acyclic graph (DAG). For this case, prior literature has demonstrated that connections to overcomplete independent component analysis yield effective criteria to decide parameter identifiability in latent variable models. However, this connection is based on the assumption that the observed variables linearly depend on the latent variables. Departing from this assumption, we treat models that allow for arbitrary non-linear latent confounding. Our main result is a graphical criterion that is necessary and sufficient for deciding the generic identifiability of direct causal effects. Moreover, we provide an algorithmic implementation of the criterion with a run time that is polynomial in the number of observed variables. Finally, we report on estimation heuristics based on the identification result, explore a generalization to models with feedback loops, and provide new results on the identifiability of the causal graph.","sentences":["Linear non-Gaussian causal models postulate that each random variable is a linear function of parent variables and non-Gaussian exogenous error terms.","We study identification of the linear coefficients when such models contain latent variables.","Our focus is on the commonly studied acyclic setting, where each model corresponds to a directed acyclic graph (DAG).","For this case, prior literature has demonstrated that connections to overcomplete independent component analysis yield effective criteria to decide parameter identifiability in latent variable models.","However, this connection is based on the assumption that the observed variables linearly depend on the latent variables.","Departing from this assumption, we treat models that allow for arbitrary non-linear latent confounding.","Our main result is a graphical criterion that is necessary and sufficient for deciding the generic identifiability of direct causal effects.","Moreover, we provide an algorithmic implementation of the criterion with a run time that is polynomial in the number of observed variables.","Finally, we report on estimation heuristics based on the identification result, explore a generalization to models with feedback loops, and provide new results on the identifiability of the causal graph."],"url":"http://arxiv.org/abs/2405.20856v1","category":"stat.ME"}
{"created":"2024-05-31 14:35:35","title":"MeshXL: Neural Coordinate Field for Generative 3D Foundation Models","abstract":"The polygon mesh representation of 3D data exhibits great flexibility, fast rendering speed, and storage efficiency, which is widely preferred in various applications. However, given its unstructured graph representation, the direct generation of high-fidelity 3D meshes is challenging. Fortunately, with a pre-defined ordering strategy, 3D meshes can be represented as sequences, and the generation process can be seamlessly treated as an auto-regressive problem. In this paper, we validate the Neural Coordinate Field (NeurCF), an explicit coordinate representation with implicit neural embeddings, is a simple-yet-effective representation for large-scale sequential mesh modeling. After that, we present MeshXL, a family of generative pre-trained auto-regressive models, which addresses the process of 3D mesh generation with modern large language model approaches. Extensive experiments show that MeshXL is able to generate high-quality 3D meshes, and can also serve as foundation models for various down-stream applications.","sentences":["The polygon mesh representation of 3D data exhibits great flexibility, fast rendering speed, and storage efficiency, which is widely preferred in various applications.","However, given its unstructured graph representation, the direct generation of high-fidelity 3D meshes is challenging.","Fortunately, with a pre-defined ordering strategy, 3D meshes can be represented as sequences, and the generation process can be seamlessly treated as an auto-regressive problem.","In this paper, we validate the Neural Coordinate Field (NeurCF), an explicit coordinate representation with implicit neural embeddings, is a simple-yet-effective representation for large-scale sequential mesh modeling.","After that, we present MeshXL, a family of generative pre-trained auto-regressive models, which addresses the process of 3D mesh generation with modern large language model approaches.","Extensive experiments show that MeshXL is able to generate high-quality 3D meshes, and can also serve as foundation models for various down-stream applications."],"url":"http://arxiv.org/abs/2405.20853v1","category":"cs.CV"}
{"created":"2024-05-31 14:33:13","title":"MegActor: Harness the Power of Raw Video for Vivid Portrait Animation","abstract":"Despite raw driving videos contain richer information on facial expressions than intermediate representations such as landmarks in the field of portrait animation, they are seldom the subject of research. This is due to two challenges inherent in portrait animation driven with raw videos: 1) significant identity leakage; 2) Irrelevant background and facial details such as wrinkles degrade performance. To harnesses the power of the raw videos for vivid portrait animation, we proposed a pioneering conditional diffusion model named as MegActor. First, we introduced a synthetic data generation framework for creating videos with consistent motion and expressions but inconsistent IDs to mitigate the issue of ID leakage. Second, we segmented the foreground and background of the reference image and employed CLIP to encode the background details. This encoded information is then integrated into the network via a text embedding module, thereby ensuring the stability of the background. Finally, we further style transfer the appearance of the reference image to the driving video to eliminate the influence of facial details in the driving videos. Our final model was trained solely on public datasets, achieving results comparable to commercial models. We hope this will help the open-source community.The code is available at https://github.com/megvii-research/MegFaceAnimate.","sentences":["Despite raw driving videos contain richer information on facial expressions than intermediate representations such as landmarks in the field of portrait animation, they are seldom the subject of research.","This is due to two challenges inherent in portrait animation driven with raw videos: 1) significant identity leakage; 2) Irrelevant background and facial details such as wrinkles degrade performance.","To harnesses the power of the raw videos for vivid portrait animation, we proposed a pioneering conditional diffusion model named as MegActor.","First, we introduced a synthetic data generation framework for creating videos with consistent motion and expressions but inconsistent IDs to mitigate the issue of ID leakage.","Second, we segmented the foreground and background of the reference image and employed CLIP to encode the background details.","This encoded information is then integrated into the network via a text embedding module, thereby ensuring the stability of the background.","Finally, we further style transfer the appearance of the reference image to the driving video to eliminate the influence of facial details in the driving videos.","Our final model was trained solely on public datasets, achieving results comparable to commercial models.","We hope this will help the open-source community.","The code is available at https://github.com/megvii-research/MegFaceAnimate."],"url":"http://arxiv.org/abs/2405.20851v1","category":"cs.CV"}
{"created":"2024-05-31 14:33:07","title":"Improving Reward Models with Synthetic Critiques","abstract":"Reward models (RM) play a critical role in aligning language models through the process of reinforcement learning from human feedback. RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation. Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions. We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. This offers richer signals and more robust features for RMs to assess and score on. We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models. Conversely, we also show that low-quality critiques negatively impact performance. Furthermore, incorporating critiques enhances the interpretability and robustness of RM training.","sentences":["Reward models (RM) play a critical role in aligning language models through the process of reinforcement learning from human feedback.","RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation.","Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions.","We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style.","This offers richer signals and more robust features for RMs to assess and score on.","We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models.","Conversely, we also show that low-quality critiques negatively impact performance.","Furthermore, incorporating critiques enhances the interpretability and robustness of RM training."],"url":"http://arxiv.org/abs/2405.20850v1","category":"cs.CL"}
{"created":"2024-05-31 14:32:31","title":"SLIM: a Scalable Light-weight Root Cause Analysis for Imbalanced Data in Microservice","abstract":"The newly deployed service -- one kind of change service, could lead to a new type of minority fault. Existing state-of-the-art methods for fault localization rarely consider the imbalanced fault classification in change service. This paper proposes a novel method that utilizes decision rule sets to deal with highly imbalanced data by optimizing the F1 score subject to cardinality constraints. The proposed method greedily generates the rule with maximal marginal gain and uses an efficient minorize-maximization (MM) approach to select rules iteratively, maximizing a non-monotone submodular lower bound. Compared with existing fault localization algorithms, our algorithm can adapt to the imbalanced fault scenario of change service, and provide interpretable fault causes which are easy to understand and verify. Our method can also be deployed in the online training setting, with only about 15% training overhead compared to the current SOTA methods. Empirical studies showcase that our algorithm outperforms existing fault localization algorithms in both accuracy and model interpretability.","sentences":["The newly deployed service -- one kind of change service, could lead to a new type of minority fault.","Existing state-of-the-art methods for fault localization rarely consider the imbalanced fault classification in change service.","This paper proposes a novel method that utilizes decision rule sets to deal with highly imbalanced data by optimizing the F1 score subject to cardinality constraints.","The proposed method greedily generates the rule with maximal marginal gain and uses an efficient minorize-maximization (MM) approach to select rules iteratively, maximizing a non-monotone submodular lower bound.","Compared with existing fault localization algorithms, our algorithm can adapt to the imbalanced fault scenario of change service, and provide interpretable fault causes which are easy to understand and verify.","Our method can also be deployed in the online training setting, with only about 15% training overhead compared to the current SOTA methods.","Empirical studies showcase that our algorithm outperforms existing fault localization algorithms in both accuracy and model interpretability."],"url":"http://arxiv.org/abs/2405.20848v1","category":"cs.SE"}
{"created":"2024-05-31 14:31:46","title":"Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models","abstract":"Image-based advertisements are complex multimodal stimuli that often contain unusual visual elements and figurative language. Previous research on automatic ad understanding has reported impressive zero-shot accuracy of contrastive vision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we examine the original task setup and show that contrastive VLMs can solve it by exploiting grounding heuristics. To control for this confound, we introduce TRADE, a new evaluation test set with adversarial grounded explanations. While these explanations look implausible to humans, we show that they \"fool\" four different contrastive VLMs. Our findings highlight the need for an improved operationalisation of automatic ad understanding that truly evaluates VLMs' multimodal reasoning abilities. We make our code and TRADE available at https://github.com/dmg-illc/trade .","sentences":["Image-based advertisements are complex multimodal stimuli that often contain unusual visual elements and figurative language.","Previous research on automatic ad understanding has reported impressive zero-shot accuracy of contrastive vision-and-language models (VLMs) on an ad-explanation retrieval task.","Here, we examine the original task setup and show that contrastive VLMs can solve it by exploiting grounding heuristics.","To control for this confound, we introduce TRADE, a new evaluation test set with adversarial grounded explanations.","While these explanations look implausible to humans, we show that they \"fool\" four different contrastive VLMs.","Our findings highlight the need for an improved operationalisation of automatic ad understanding that truly evaluates VLMs' multimodal reasoning abilities.","We make our code and TRADE available at https://github.com/dmg-illc/trade ."],"url":"http://arxiv.org/abs/2405.20846v1","category":"cs.CL"}
{"created":"2024-05-31 14:31:45","title":"Existence of solutions for a system with general Hardy--Sobolev singular criticalities","abstract":"In this paper we study a class of Hardy--Sobolev type systems defined in $\\mathbb{R}^N$ and coupled by a singular critical Hardy--Sobolev term. The main novelty of this work is that the orders of the singularities are independent and contained in a wide range. By means of variational techniques, we will prove the existence of positive bound and ground states for such a system. In particular, we find solutions as minimizers or Mountain--Pass critical points of the energy functional on the underlying Nehari manifold.","sentences":["In this paper we study a class of Hardy--Sobolev type systems defined in $\\mathbb{R}^N$ and coupled by a singular critical Hardy--Sobolev term.","The main novelty of this work is that the orders of the singularities are independent and contained in a wide range.","By means of variational techniques, we will prove the existence of positive bound and ground states for such a system.","In particular, we find solutions as minimizers or Mountain--Pass critical points of the energy functional on the underlying Nehari manifold."],"url":"http://arxiv.org/abs/2405.20845v1","category":"math.AP"}
{"created":"2024-05-31 14:29:39","title":"Charge Transport in Interband Cascade Lasers: An Ab-Initio Self-Consistent Model","abstract":"Interband cascade lasers (ICLs) stand out due to their low threshold current and minimal power consumption, rendering them viable sources for compact and mobile devices in the mid-infrared. Since their first demonstration, they experienced major performance improvements. Many of them originate, on one hand, from technological enhancements and, on the other hand, also from restricted numerical analysis. Encouraged by the impact of restricted models, an ICL-specific simulation tool can lead to performance breakthroughs and a better comprehension of governing mechanisms. Drawing from an evaluation of existing tools designed for quantum cascade structures, we implemented a self-consistent density matrix rate equation model generalized to simulate the transport in both conduction and valence band heterostructures. Albeit the extensive inclusion of the quantum effects, special care was taken to maintain a high numerical efficiency. Our charge transport model additionally considers optical field calculations, allowing for predictive calculations of light-current-voltage (LIV) curves. We benchmark the model against well-established ICL designs and demonstrate reliable performance predictability. Additionally, we give detailed insights into device characteristics extracted from our model. This ultimately allows us to deepen our understanding of ICLs and define existing and generate novel designs.","sentences":["Interband cascade lasers (ICLs) stand out due to their low threshold current and minimal power consumption, rendering them viable sources for compact and mobile devices in the mid-infrared.","Since their first demonstration, they experienced major performance improvements.","Many of them originate, on one hand, from technological enhancements and, on the other hand, also from restricted numerical analysis.","Encouraged by the impact of restricted models, an ICL-specific simulation tool can lead to performance breakthroughs and a better comprehension of governing mechanisms.","Drawing from an evaluation of existing tools designed for quantum cascade structures, we implemented a self-consistent density matrix rate equation model generalized to simulate the transport in both conduction and valence band heterostructures.","Albeit the extensive inclusion of the quantum effects, special care was taken to maintain a high numerical efficiency.","Our charge transport model additionally considers optical field calculations, allowing for predictive calculations of light-current-voltage (LIV) curves.","We benchmark the model against well-established ICL designs and demonstrate reliable performance predictability.","Additionally, we give detailed insights into device characteristics extracted from our model.","This ultimately allows us to deepen our understanding of ICLs and define existing and generate novel designs."],"url":"http://arxiv.org/abs/2405.20843v1","category":"physics.optics"}
{"created":"2024-05-31 14:25:45","title":"einspace: Searching for Neural Architectures from Fundamental Operations","abstract":"Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren't diverse enough to include such transformations a priori. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.","sentences":["Neural architecture search (NAS) finds high performing networks for a given task.","Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers.","This is not least because the search spaces in NAS often aren't diverse enough to include such transformations a priori.","Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations.","To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar.","Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more.","It contains many existing competitive architectures, and provides flexibility for discovering new ones.","Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets.","We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines.","We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles."],"url":"http://arxiv.org/abs/2405.20838v1","category":"cs.LG"}
{"created":"2024-05-31 14:25:13","title":"Isotope substitution and polytype control for point defects identification: the case of the ultraviolet color center in hexagonal boron nitride","abstract":"Defects in crystals can have a transformative effect on the properties and functionalities of solid-state systems. Dopants in semiconductors are core components in electronic and optoelectronic devices. The control of single color centers is at the basis of advanced applications for quantum technologies. Unintentional defects can also be detrimental to the crystalline structure and hinder the development of novel materials. Whatever the research perspective, the identification of defects is a key but complicated, and often long-standing issue. Here, we present a general methodology to identify point defects by combining isotope substitution and polytype control, with a systematic comparison between experiments and first-principles calculations. We apply this methodology to hexagonal boron nitride (hBN) and its ubiquitous color center emitting in the ultraviolet spectral range. From isotopic purification of the host hBN matrix, a local vibrational mode of the defect is uncovered, and isotope-selective carbon doping proves that this mode belongs to a carbon-based center. Then, by varying the stacking sequence of the host hBN matrix, we unveil different optical responses to hydrostatic pressure for the non-equivalent configurations of this ultraviolet color center. We conclude that this defect is a carbon dimer in the honeycomb lattice of hBN. Our results show that tuning the stacking sequence in different polytypes of a given crystal provides unique fingerprints contributing to the identification of defects in 2D materials.","sentences":["Defects in crystals can have a transformative effect on the properties and functionalities of solid-state systems.","Dopants in semiconductors are core components in electronic and optoelectronic devices.","The control of single color centers is at the basis of advanced applications for quantum technologies.","Unintentional defects can also be detrimental to the crystalline structure and hinder the development of novel materials.","Whatever the research perspective, the identification of defects is a key but complicated, and often long-standing issue.","Here, we present a general methodology to identify point defects by combining isotope substitution and polytype control, with a systematic comparison between experiments and first-principles calculations.","We apply this methodology to hexagonal boron nitride (hBN) and its ubiquitous color center emitting in the ultraviolet spectral range.","From isotopic purification of the host hBN matrix, a local vibrational mode of the defect is uncovered, and isotope-selective carbon doping proves that this mode belongs to a carbon-based center.","Then, by varying the stacking sequence of the host hBN matrix, we unveil different optical responses to hydrostatic pressure for the non-equivalent configurations of this ultraviolet color center.","We conclude that this defect is a carbon dimer in the honeycomb lattice of hBN.","Our results show that tuning the stacking sequence in different polytypes of a given crystal provides unique fingerprints contributing to the identification of defects in 2D materials."],"url":"http://arxiv.org/abs/2405.20837v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 14:24:33","title":"Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs","abstract":"Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various notable open-source LLMs. Calibration sets are crucial for evaluating activation magnitudes and identifying outliers, which can distort the quantization range and negatively impact performance. Our analysis reveals a marked contrast in quantization effectiveness across models. The older OPT model, which much of the quantization literature is based on, shows significant performance deterioration and high susceptibility to outliers with varying calibration sets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity to outliers and stable activations. These findings suggest a shift in PTQ strategies might be needed. As advancements in pre-training methods reduce the relevance of outliers, there is an emerging need to reassess the fundamentals of current quantization literature. The emphasis should pivot towards optimizing inference speed, rather than primarily focusing on outlier preservation, to align with the evolving characteristics of state-of-the-art LLMs.","sentences":["Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops.","We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various notable open-source LLMs.","Calibration sets are crucial for evaluating activation magnitudes and identifying outliers, which can distort the quantization range and negatively impact performance.","Our analysis reveals a marked contrast in quantization effectiveness across models.","The older OPT model, which much of the quantization literature is based on, shows significant performance deterioration and high susceptibility to outliers with varying calibration sets.","In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity to outliers and stable activations.","These findings suggest a shift in PTQ strategies might be needed.","As advancements in pre-training methods reduce the relevance of outliers, there is an emerging need to reassess the fundamentals of current quantization literature.","The emphasis should pivot towards optimizing inference speed, rather than primarily focusing on outlier preservation, to align with the evolving characteristics of state-of-the-art LLMs."],"url":"http://arxiv.org/abs/2405.20835v1","category":"cs.LG"}
{"created":"2024-05-31 14:23:49","title":"Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits Multimodal Reasoning","abstract":"Large language models equipped with retrieval-augmented generation (RAG) represent a burgeoning field aimed at enhancing answering capabilities by leveraging external knowledge bases. Although the application of RAG with language-only models has been extensively explored, its adaptation into multimodal vision-language models remains nascent. Going beyond mere answer generation, the primary goal of multimodal RAG is to cultivate the models' ability to reason in response to relevant queries. To this end, we introduce a novel multimodal RAG framework named RMR (Retrieval Meets Reasoning). The RMR framework employs a bi-modal retrieval module to identify the most relevant question-answer pairs, which then serve as scaffolds for the multimodal reasoning process. This training-free approach not only encourages the model to engage deeply with the reasoning processes inherent in the retrieved content but also facilitates the generation of answers that are precise and richly interpretable. Surprisingly, utilizing solely the ScienceQA dataset, collected from elementary and high school science curricula, RMR significantly boosts the performance of various vision-language models across a spectrum of benchmark datasets, including A-OKVQA, MMBench, and SEED. These outcomes highlight the substantial potential of our multimodal retrieval and reasoning mechanism to improve the reasoning capabilities of vision-language models.","sentences":["Large language models equipped with retrieval-augmented generation (RAG) represent a burgeoning field aimed at enhancing answering capabilities by leveraging external knowledge bases.","Although the application of RAG with language-only models has been extensively explored, its adaptation into multimodal vision-language models remains nascent.","Going beyond mere answer generation, the primary goal of multimodal RAG is to cultivate the models' ability to reason in response to relevant queries.","To this end, we introduce a novel multimodal RAG framework named RMR (Retrieval Meets Reasoning).","The RMR framework employs a bi-modal retrieval module to identify the most relevant question-answer pairs, which then serve as scaffolds for the multimodal reasoning process.","This training-free approach not only encourages the model to engage deeply with the reasoning processes inherent in the retrieved content but also facilitates the generation of answers that are precise and richly interpretable.","Surprisingly, utilizing solely the ScienceQA dataset, collected from elementary and high school science curricula, RMR significantly boosts the performance of various vision-language models across a spectrum of benchmark datasets, including A-OKVQA, MMBench, and SEED.","These outcomes highlight the substantial potential of our multimodal retrieval and reasoning mechanism to improve the reasoning capabilities of vision-language models."],"url":"http://arxiv.org/abs/2405.20834v1","category":"cs.CV"}
{"created":"2024-05-31 14:23:20","title":"Exact real time dynamics with free fermions in disguise","abstract":"We consider quantum spin chains with a hidden free fermionic structure, distinct from the Jordan-Wigner transformation and its generalizations. We express selected local operators with the hidden fermions. This way we can exactly solve the real time dynamics in various physical scenarios, including the computation of selected dynamical two point functions and Loschmidt amplitudes, in continuous or discrete time. In the latter case we build quantum circuits that can be implemented on a quantum computer. With this we extend the family of classically simulable quantum many body processes.","sentences":["We consider quantum spin chains with a hidden free fermionic structure, distinct from the Jordan-Wigner transformation and its generalizations.","We express selected local operators with the hidden fermions.","This way we can exactly solve the real time dynamics in various physical scenarios, including the computation of selected dynamical two point functions and Loschmidt amplitudes, in continuous or discrete time.","In the latter case we build quantum circuits that can be implemented on a quantum computer.","With this we extend the family of classically simulable quantum many body processes."],"url":"http://arxiv.org/abs/2405.20832v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-31 14:21:04","title":"Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment","abstract":"Traditional language model alignment methods, such as Direct Preference Optimization (DPO), are limited by their dependence on static, pre-collected paired preference data, which hampers their adaptability and practical applicability. To overcome this limitation, we introduce Self-Augmented Preference Optimization (SAPO), an effective and scalable training paradigm that does not require existing paired data. Building on the self-play concept, which autonomously generates negative responses, we further incorporate an off-policy learning pipeline to enhance data exploration and exploitation. Specifically, we employ an Exponential Moving Average (EMA) model in conjunction with a replay buffer to enable dynamic updates of response segments, effectively integrating real-time feedback with insights from historical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B models across benchmarks, including the Open LLM Leaderboard, IFEval, AlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses established offline contrastive baselines, such as DPO and Odds Ratio Preference Optimization, and outperforms offline self-play methods like SPIN. Our code is available at https://github.com/yinyueqin/SAPO","sentences":["Traditional language model alignment methods, such as Direct Preference Optimization (DPO), are limited by their dependence on static, pre-collected paired preference data, which hampers their adaptability and practical applicability.","To overcome this limitation, we introduce Self-Augmented Preference Optimization (SAPO), an effective and scalable training paradigm that does not require existing paired data.","Building on the self-play concept, which autonomously generates negative responses, we further incorporate an off-policy learning pipeline to enhance data exploration and exploitation.","Specifically, we employ an Exponential Moving Average (EMA) model in conjunction with a replay buffer to enable dynamic updates of response segments, effectively integrating real-time feedback with insights from historical data.","Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B models across benchmarks, including the Open LLM Leaderboard, IFEval, AlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses established offline contrastive baselines, such as DPO and Odds Ratio Preference Optimization, and outperforms offline self-play methods like SPIN.","Our code is available at https://github.com/yinyueqin/SAPO"],"url":"http://arxiv.org/abs/2405.20830v1","category":"cs.CL"}
{"created":"2024-05-31 14:21:00","title":"Rethinking Open-World Semi-Supervised Learning: Distribution Mismatch and Inductive Inference","abstract":"Open-world semi-supervised learning (OWSSL) extends conventional semi-supervised learning to open-world scenarios by taking account of novel categories in unlabeled datasets. Despite the recent advancements in OWSSL, the success often relies on the assumptions that 1) labeled and unlabeled datasets share the same balanced class prior distribution, which does not generally hold in real-world applications, and 2) unlabeled training datasets are utilized for evaluation, where such transductive inference might not adequately address challenges in the wild. In this paper, we aim to generalize OWSSL by addressing them. Our work suggests that practical OWSSL may require different training settings, evaluation methods, and learning strategies compared to those prevalent in the existing literature.","sentences":["Open-world semi-supervised learning (OWSSL) extends conventional semi-supervised learning to open-world scenarios by taking account of novel categories in unlabeled datasets.","Despite the recent advancements in OWSSL, the success often relies on the assumptions that 1) labeled and unlabeled datasets share the same balanced class prior distribution, which does not generally hold in real-world applications, and 2) unlabeled training datasets are utilized for evaluation, where such transductive inference might not adequately address challenges in the wild.","In this paper, we aim to generalize OWSSL by addressing them.","Our work suggests that practical OWSSL may require different training settings, evaluation methods, and learning strategies compared to those prevalent in the existing literature."],"url":"http://arxiv.org/abs/2405.20829v1","category":"cs.CV"}
{"created":"2024-05-31 14:15:49","title":"Inflation Determinants in Argentina (2004-2022)","abstract":"This paper analyzes the empirical relationship between the inflation rate and its proximate determinants in Argentina, using quarterly data over the period 2004-2022 and an error-correction vector model approach. Unlike previous literature, this paper uses a theoretical framework to motivate the inclusion of variables that are expected to contribute to explain inflation, thus reducing the risk of omitting relevant variables and formalizing key mechanisms. Inference is performed through Granger causality analysis, impulse response functions and forecast errors variance decomposition. The results suggest that an anti-inflationary plan for Argentina should take into consideration both the greater relevance of the inertial component, the exchange rate and the interest rate in the short-run dynamics of the price level, and the long-run relationship between prices, interest rate and activity level.","sentences":["This paper analyzes the empirical relationship between the inflation rate and its proximate determinants in Argentina, using quarterly data over the period 2004-2022 and an error-correction vector model approach.","Unlike previous literature, this paper uses a theoretical framework to motivate the inclusion of variables that are expected to contribute to explain inflation, thus reducing the risk of omitting relevant variables and formalizing key mechanisms.","Inference is performed through Granger causality analysis, impulse response functions and forecast errors variance decomposition.","The results suggest that an anti-inflationary plan for Argentina should take into consideration both the greater relevance of the inertial component, the exchange rate and the interest rate in the short-run dynamics of the price level, and the long-run relationship between prices, interest rate and activity level."],"url":"http://arxiv.org/abs/2405.20822v1","category":"econ.GN"}
{"created":"2024-05-31 14:14:01","title":"An iterated learning model of language change that mixes supervised and unsupervised learning","abstract":"The iterated learning model is an agent-based model of language change in which language is transmitted from a tutor to a pupil which itself becomes a tutor to a new pupil, and so on. Languages that are stable, expressive, and compositional arise spontaneously as a consequence of a language transmission bottleneck. Previous models have implemented an agent's mapping from signals to meanings using an artificial neural network decoder, but have relied on an unrealistic and computationally expensive process of obversion to implement the associated encoder, mapping from meanings to signals. Here, a new model is presented in which both decoder and encoder are neural networks, trained separately through supervised learning, and trained together through unsupervised learning in the form of an autoencoder. This avoids the substantial computational burden entailed in obversion and introduces a mixture of supervised and unsupervised learning as observed during human development.","sentences":["The iterated learning model is an agent-based model of language change in which language is transmitted from a tutor to a pupil which itself becomes a tutor to a new pupil, and so on.","Languages that are stable, expressive, and compositional arise spontaneously as a consequence of a language transmission bottleneck.","Previous models have implemented an agent's mapping from signals to meanings using an artificial neural network decoder, but have relied on an unrealistic and computationally expensive process of obversion to implement the associated encoder, mapping from meanings to signals.","Here, a new model is presented in which both decoder and encoder are neural networks, trained separately through supervised learning, and trained together through unsupervised learning in the form of an autoencoder.","This avoids the substantial computational burden entailed in obversion and introduces a mixture of supervised and unsupervised learning as observed during human development."],"url":"http://arxiv.org/abs/2405.20818v1","category":"cs.CL"}
{"created":"2024-05-31 14:12:59","title":"Extremile scalar-on-function regression with application to climate scenarios","abstract":"Extremiles provide a generalization of quantiles which are not only robust, but also have an intrinsic link with extreme value theory. This paper introduces an extremile regression model tailored for functional covariate spaces. The estimation procedure turns out to be a weighted version of local linear scalar-on-function regression, where now a double kernel approach plays a crucial role. Asymptotic expressions for the bias and variance are established, applicable to both decreasing bandwidth sequences and automatically selected bandwidths. The methodology is then investigated in detail through a simulation study. Furthermore, we highlight the applicability of the model through the analysis of data sourced from the CH2018 Swiss climate scenarios project, offering insights into its ability to serve as a modern tool to quantify climate behaviour.","sentences":["Extremiles provide a generalization of quantiles which are not only robust, but also have an intrinsic link with extreme value theory.","This paper introduces an extremile regression model tailored for functional covariate spaces.","The estimation procedure turns out to be a weighted version of local linear scalar-on-function regression, where now a double kernel approach plays a crucial role.","Asymptotic expressions for the bias and variance are established, applicable to both decreasing bandwidth sequences and automatically selected bandwidths.","The methodology is then investigated in detail through a simulation study.","Furthermore, we highlight the applicability of the model through the analysis of data sourced from the CH2018 Swiss climate scenarios project, offering insights into its ability to serve as a modern tool to quantify climate behaviour."],"url":"http://arxiv.org/abs/2405.20817v1","category":"stat.ME"}
{"created":"2024-05-31 14:11:13","title":"Can the symmetric Fermi and eROSITA bubbles be produced by tilted jets?","abstract":"The Fermi Gamma-Ray Space Telescope reveals two large bubbles in the Galaxy, extending nearly symmetrically $\\sim50^{\\circ}$ above and below the Galactic center (GC). Previous simulations of bubble formation invoking active galactic nucleus (AGN) jets have assumed that the jets are vertical to the Galactic disk; however, in general, the jet orientation does not necessarily correlate with the rotational axis of the Galactic disk. Using three-dimensional special relativistic hydrodynamic simulations including cosmic rays (CRs) and thermal gas, we show that the dense clumpy gas within the Galactic disk disrupts jet collimation (\"failed jets\" hereafter), which causes the failed jets to form hot bubbles. Subsequent buoyancy in the stratified atmosphere renders them vertical to form the symmetric Fermi and eROSITA bubbles (collectively, Galactic bubbles). We find that (1) despite the relativistic jets emanated from the GC are at various angles $\\le45^{\\circ}$ with respect to the rotational axis of the Galaxy, the Galactic bubbles nonetheless appear aligned with the axis; (2) the edge of the eROSITA bubbles corresponds to a forward shock driven by the hot bubbles; (3) followed by the forward shock is a tangling contact discontinuity corresponding to the edge of the Fermi bubbles; (4) assuming a leptonic model we find that the observed gamma-ray bubbles and microwave haze can be reproduced with a best-fit CR power-law spectral index of 2.4; The agreements between the simulated and the observed multi-wavelength features suggest that forming the Galactic bubbles by oblique AGN failed jets is a plausible scenario.","sentences":["The Fermi Gamma-Ray Space Telescope reveals two large bubbles in the Galaxy, extending nearly symmetrically $\\sim50^{\\circ}$ above and below the Galactic center (GC).","Previous simulations of bubble formation invoking active galactic nucleus (AGN) jets have assumed that the jets are vertical to the Galactic disk; however, in general, the jet orientation does not necessarily correlate with the rotational axis of the Galactic disk.","Using three-dimensional special relativistic hydrodynamic simulations including cosmic rays (CRs) and thermal gas, we show that the dense clumpy gas within the Galactic disk disrupts jet collimation (\"failed jets\" hereafter), which causes the failed jets to form hot bubbles.","Subsequent buoyancy in the stratified atmosphere renders them vertical to form the symmetric Fermi and eROSITA bubbles (collectively, Galactic bubbles).","We find that (1) despite the relativistic jets emanated from the GC are at various angles $\\le45^{\\circ}$ with respect to the rotational axis of the Galaxy, the Galactic bubbles nonetheless appear aligned with the axis; (2) the edge of the eROSITA bubbles corresponds to a forward shock driven by the hot bubbles; (3) followed by the forward shock is a tangling contact discontinuity corresponding to the edge of the Fermi bubbles; (4) assuming a leptonic model we find that the observed gamma-ray bubbles and microwave haze can be reproduced with a best-fit CR power-law spectral index of 2.4; The agreements between the simulated and the observed multi-wavelength features suggest that forming the Galactic bubbles by oblique AGN failed jets is a plausible scenario."],"url":"http://arxiv.org/abs/2405.20816v1","category":"astro-ph.HE"}
{"created":"2024-05-31 14:10:30","title":"Hyperrigidity I: operator moments and convergence of subnormal operators","abstract":"We show that, the concept of hyperrigidity can be expressed in many ways. We provide four main approaches to this issue. The first one is via semispectral measures in the spirit of the characterizations of spectral measures. The second approach is based on dilation theory and is written in terms of the Stone-von Neumann calculus for normal operators. The third is inspired by Brown's theorem and deals with the weak and strong convergence of sequences of subnormal (or normal) operators. Finally, the fourth approach concerns multiplicativity of UCP maps on $C^*$-subalgebras generated by normal elements. This is inspired by the Petz's theorem and its generalizations established first by Arveson in the finite-dimensional case and then by Brown in general. Next, we will examine, in the case of a unital commutative $C^*$-algebra $\\mathscr{A}$ generated by a single element $t$, which subsets $G$ of the set $\\{t^{*m}t^n : (m, n) \\in \\mathbb{Z}^2\\}$ are hyperrigid in $\\mathscr{A}$.","sentences":["We show that, the concept of hyperrigidity can be expressed in many ways.","We provide four main approaches to this issue.","The first one is via semispectral measures in the spirit of the characterizations of spectral measures.","The second approach is based on dilation theory and is written in terms of the Stone-von Neumann calculus for normal operators.","The third is inspired by Brown's theorem and deals with the weak and strong convergence of sequences of subnormal (or normal) operators.","Finally, the fourth approach concerns multiplicativity of UCP maps on $C^*$-subalgebras generated by normal elements.","This is inspired by the Petz's theorem and its generalizations established first by Arveson in the finite-dimensional case and then by Brown in general.","Next, we will examine, in the case of a unital commutative $C^*$-algebra $\\mathscr{A}$ generated by a single element $t$, which subsets $G$ of the set $\\{t^{*m}t^n : (m, n) \\in \\mathbb{Z}^2\\}$ are hyperrigid in $\\mathscr{A}$."],"url":"http://arxiv.org/abs/2405.20814v1","category":"math.OA"}
{"created":"2024-05-31 14:07:59","title":"Grothendieck-Verdier module categories, Frobenius algebras and relative Serre functors","abstract":"We develop the theory of module categories over a Grothendieck-Verdier category, i.e. a monoidal category with a dualizing object and hence a duality structure more general than rigidity. Such a category C comes with two monoidal structures which are related by non-invertible morphisms and which we treat on an equal footing. Quite generally, non-invertible structure morphisms play a dominant role in this theory.   In any Grothendieck-Verdier module category M we find two important subcategories M' and M''. The internal End of an object in M' that is a C-generator is an algebra such that its category of modules is equivalent to M as a module category. We also introduce a partially defined relative Serre functor S which furnishes an equivalence between M' and M''. Any isomorphism between an object m of M' and S(m) in M'' endows the internal End of m with the structure of a Grothendieck-Verdier Frobenius algebra.","sentences":["We develop the theory of module categories over a Grothendieck-Verdier category, i.e. a monoidal category with a dualizing object and hence a duality structure more general than rigidity.","Such a category C comes with two monoidal structures which are related by non-invertible morphisms and which we treat on an equal footing.","Quite generally, non-invertible structure morphisms play a dominant role in this theory.   ","In any Grothendieck-Verdier module category M we find two important subcategories M' and M''.","The internal End of an object in M' that is a C-generator is an algebra such that its category of modules is equivalent to M as a module category.","We also introduce a partially defined relative Serre functor S which furnishes an equivalence between M' and M''.","Any isomorphism between an object m of M' and S(m) in M'' endows the internal End of m with the structure of a Grothendieck-Verdier Frobenius algebra."],"url":"http://arxiv.org/abs/2405.20811v1","category":"math.CT"}
{"created":"2024-05-31 14:06:24","title":"There and Back Again: The AI Alignment Paradox","abstract":"The field of AI alignment aims to steer AI systems toward human goals, preferences, and ethical principles. Its contributions have been instrumental for improving the output quality, safety, and trustworthiness of today's AI models. This perspective article draws attention to a fundamental challenge inherent in all AI alignment endeavors, which we term the \"AI alignment paradox\": The better we align AI models with our values, the easier we make it for adversaries to misalign the models. We illustrate the paradox by sketching three concrete example incarnations for the case of language models, each corresponding to a distinct way in which adversaries can exploit the paradox. With AI's increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to break out of it, in order to ensure the beneficial use of AI for the good of humanity.","sentences":["The field of AI alignment aims to steer AI systems toward human goals, preferences, and ethical principles.","Its contributions have been instrumental for improving the output quality, safety, and trustworthiness of today's AI models.","This perspective article draws attention to a fundamental challenge inherent in all AI alignment endeavors, which we term the \"AI alignment paradox\": The better we align AI models with our values, the easier we make it for adversaries to misalign the models.","We illustrate the paradox by sketching three concrete example incarnations for the case of language models, each corresponding to a distinct way in which adversaries can exploit the paradox.","With AI's increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to break out of it, in order to ensure the beneficial use of AI for the good of humanity."],"url":"http://arxiv.org/abs/2405.20806v1","category":"cs.AI"}
{"created":"2024-05-31 14:01:12","title":"Shape Constraints in Symbolic Regression using Penalized Least Squares","abstract":"We study the addition of shape constraints and their consideration during the parameter estimation step of symbolic regression (SR). Shape constraints serve as a means to introduce prior knowledge about the shape of the otherwise unknown model function into SR. Unlike previous works that have explored shape constraints in SR, we propose minimizing shape constraint violations during parameter estimation using gradient-based numerical optimization.   We test three algorithm variants to evaluate their performance in identifying three symbolic expressions from a synthetically generated data set. This paper examines two benchmark scenarios: one with varying noise levels and another with reduced amounts of training data. The results indicate that incorporating shape constraints into the expression search is particularly beneficial when data is scarce. Compared to using shape constraints only in the selection process, our approach of minimizing violations during parameter estimation shows a statistically significant benefit in some of our test cases, without being significantly worse in any instance.","sentences":["We study the addition of shape constraints and their consideration during the parameter estimation step of symbolic regression (SR).","Shape constraints serve as a means to introduce prior knowledge about the shape of the otherwise unknown model function into SR.","Unlike previous works that have explored shape constraints in SR, we propose minimizing shape constraint violations during parameter estimation using gradient-based numerical optimization.   ","We test three algorithm variants to evaluate their performance in identifying three symbolic expressions from a synthetically generated data set.","This paper examines two benchmark scenarios: one with varying noise levels and another with reduced amounts of training data.","The results indicate that incorporating shape constraints into the expression search is particularly beneficial when data is scarce.","Compared to using shape constraints only in the selection process, our approach of minimizing violations during parameter estimation shows a statistically significant benefit in some of our test cases, without being significantly worse in any instance."],"url":"http://arxiv.org/abs/2405.20800v1","category":"cs.LG"}
{"created":"2024-05-31 13:59:56","title":"Quantum and classical magnetic Bloch points","abstract":"A Bloch point represents a three-dimensional hedgehog singularity of a magnetic vector field in which the magnetization vanishes. However, standard micromagnetic theory, developed for magnetic moments of fixed lengths, lacks full applicability in studying such singularities. To address this gap, we study a Bloch point in a quantum Heisenberg model for the case of spin-1/2 particles. Performing an exact diagonalization of the Hamiltonian as well as using density matrix renormalization group techniques, we obtain the ground state, which can be used to recover the corresponding magnetization profile. Our findings demonstrate a variation of the spin length in the quantum model, leading smoothly to zero magnetization at the Bloch point. Our results indicate the necessity of generalizing the classical micromagnetic model by adding the third degree of freedom of the spins: the ability to change its length. To this end, we introduce the micromagnetic $\\mathbb{S}_{3}$-model, which enables the description of magnets with and without Bloch point singularities.","sentences":["A Bloch point represents a three-dimensional hedgehog singularity of a magnetic vector field in which the magnetization vanishes.","However, standard micromagnetic theory, developed for magnetic moments of fixed lengths, lacks full applicability in studying such singularities.","To address this gap, we study a Bloch point in a quantum Heisenberg model for the case of spin-1/2 particles.","Performing an exact diagonalization of the Hamiltonian as well as using density matrix renormalization group techniques, we obtain the ground state, which can be used to recover the corresponding magnetization profile.","Our findings demonstrate a variation of the spin length in the quantum model, leading smoothly to zero magnetization at the Bloch point.","Our results indicate the necessity of generalizing the classical micromagnetic model by adding the third degree of freedom of the spins: the ability to change its length.","To this end, we introduce the micromagnetic $\\mathbb{S}_{3}$-model, which enables the description of magnets with and without Bloch point singularities."],"url":"http://arxiv.org/abs/2405.20798v1","category":"cond-mat.str-el"}
{"created":"2024-05-31 13:59:18","title":"Ovis: Structural Embedding Alignment for Multimodal Large Language Model","abstract":"Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings. This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks demonstrate that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Both the source code and the training dataset of Ovis will be made publicly available.","sentences":["Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities.","However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information.","We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings.","Ovis integrates an additional learnable visual embedding table into the visual encoder's process.","To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings.","This structural approach mirrors the method used for generating textual embeddings.","Empirical evaluations on various multimodal benchmarks demonstrate that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall.","These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning.","Both the source code and the training dataset of Ovis will be made publicly available."],"url":"http://arxiv.org/abs/2405.20797v1","category":"cs.CV"}
{"created":"2024-05-31 13:57:57","title":"Regularity of minimal surfaces with capillary boundary conditions","abstract":"We prove $\\varepsilon$-regularity theorems for varifolds with capillary boundary condition in a Riemannian manifold. These varifolds were first introduced by Kagaya-Tonegawa \\cite{KaTo}. We establish a uniform first variation control for all such varifolds (and free-boundary varifolds generally) satisfying a sharp density bound and prove that if a capillary varifold has bounded mean curvature and is close to a capillary half-plane with angle not equal to $\\tfrac{\\pi}{2}$, then it coincides with a $C^{1,\\alpha}$ properly embedded hypersurface. We apply our theorem to deduce regularity at a generic point along the boundary in the region where the density is strictly less than $1$.","sentences":["We prove $\\varepsilon$-regularity theorems for varifolds with capillary boundary condition in a Riemannian manifold.","These varifolds were first introduced by Kagaya-Tonegawa \\cite{KaTo}.","We establish a uniform first variation control for all such varifolds (and free-boundary varifolds generally) satisfying a sharp density bound and prove that if a capillary varifold has bounded mean curvature and is close to a capillary half-plane with angle not equal to $\\tfrac{\\pi}{2}$, then it coincides with a $C^{1,\\alpha}$ properly embedded hypersurface.","We apply our theorem to deduce regularity at a generic point along the boundary in the region where the density is strictly less than $1$."],"url":"http://arxiv.org/abs/2405.20796v1","category":"math.DG"}
{"created":"2024-05-31 13:56:55","title":"InsightSee: Advancing Multi-agent Vision-Language Models for Enhanced Visual Understanding","abstract":"Accurate visual understanding is imperative for advancing autonomous systems and intelligent robots. Despite the powerful capabilities of vision-language models (VLMs) in processing complex visual scenes, precisely recognizing obscured or ambiguously presented visual elements remains challenging. To tackle such issues, this paper proposes InsightSee, a multi-agent framework to enhance VLMs' interpretative capabilities in handling complex visual understanding scenarios. The framework comprises a description agent, two reasoning agents, and a decision agent, which are integrated to refine the process of visual information interpretation. The design of these agents and the mechanisms by which they can be enhanced in visual information processing are presented. Experimental results demonstrate that the InsightSee framework not only boosts performance on specific visual tasks but also retains the original models' strength. The proposed framework outperforms state-of-the-art algorithms in 6 out of 9 benchmark tests, with a substantial advancement in multimodal understanding.","sentences":["Accurate visual understanding is imperative for advancing autonomous systems and intelligent robots.","Despite the powerful capabilities of vision-language models (VLMs) in processing complex visual scenes, precisely recognizing obscured or ambiguously presented visual elements remains challenging.","To tackle such issues, this paper proposes InsightSee, a multi-agent framework to enhance VLMs' interpretative capabilities in handling complex visual understanding scenarios.","The framework comprises a description agent, two reasoning agents, and a decision agent, which are integrated to refine the process of visual information interpretation.","The design of these agents and the mechanisms by which they can be enhanced in visual information processing are presented.","Experimental results demonstrate that the InsightSee framework not only boosts performance on specific visual tasks but also retains the original models' strength.","The proposed framework outperforms state-of-the-art algorithms in 6 out of 9 benchmark tests, with a substantial advancement in multimodal understanding."],"url":"http://arxiv.org/abs/2405.20795v1","category":"cs.CV"}
{"created":"2024-05-31 13:48:54","title":"GS-Phong: Meta-Learned 3D Gaussians for Relightable Novel View Synthesis","abstract":"Decoupling the illumination in 3D scenes is crucial for novel view synthesis and relighting. In this paper, we propose a novel method for representing a scene illuminated by a point light using a set of relightable 3D Gaussian points. Inspired by the Blinn-Phong model, our approach decomposes the scene into ambient, diffuse, and specular components, enabling the synthesis of realistic lighting effects. To facilitate the decomposition of geometric information independent of lighting conditions, we introduce a novel bilevel optimization-based meta-learning framework. The fundamental idea is to view the rendering tasks under various lighting positions as a multi-task learning problem, which our meta-learning approach effectively addresses by generalizing the learned Gaussian geometries not only across different viewpoints but also across diverse light positions. Experimental results demonstrate the effectiveness of our approach in terms of training efficiency and rendering quality compared to existing methods for free-viewpoint relighting.","sentences":["Decoupling the illumination in 3D scenes is crucial for novel view synthesis and relighting.","In this paper, we propose a novel method for representing a scene illuminated by a point light using a set of relightable 3D Gaussian points.","Inspired by the Blinn-Phong model, our approach decomposes the scene into ambient, diffuse, and specular components, enabling the synthesis of realistic lighting effects.","To facilitate the decomposition of geometric information independent of lighting conditions, we introduce a novel bilevel optimization-based meta-learning framework.","The fundamental idea is to view the rendering tasks under various lighting positions as a multi-task learning problem, which our meta-learning approach effectively addresses by generalizing the learned Gaussian geometries not only across different viewpoints but also across diverse light positions.","Experimental results demonstrate the effectiveness of our approach in terms of training efficiency and rendering quality compared to existing methods for free-viewpoint relighting."],"url":"http://arxiv.org/abs/2405.20791v1","category":"cs.CV"}
{"created":"2024-05-31 13:45:52","title":"Intersectional Unfairness Discovery","abstract":"AI systems have been shown to produce unfair results for certain subgroups of population, highlighting the need to understand bias on certain sensitive attributes. Current research often falls short, primarily focusing on the subgroups characterized by a single sensitive attribute, while neglecting the nature of intersectional fairness of multiple sensitive attributes. This paper focuses on its one fundamental aspect by discovering diverse high-bias subgroups under intersectional sensitive attributes. Specifically, we propose a Bias-Guided Generative Network (BGGN). By treating each bias value as a reward, BGGN efficiently generates high-bias intersectional sensitive attributes. Experiments on real-world text and image datasets demonstrate a diverse and efficient discovery of BGGN. To further evaluate the generated unseen but possible unfair intersectional sensitive attributes, we formulate them as prompts and use modern generative AI to produce new texts and images. The results of frequently generating biased data provides new insights of discovering potential unfairness in popular modern generative AI systems. Warning: This paper contains generative examples that are offensive in nature.","sentences":["AI systems have been shown to produce unfair results for certain subgroups of population, highlighting the need to understand bias on certain sensitive attributes.","Current research often falls short, primarily focusing on the subgroups characterized by a single sensitive attribute, while neglecting the nature of intersectional fairness of multiple sensitive attributes.","This paper focuses on its one fundamental aspect by discovering diverse high-bias subgroups under intersectional sensitive attributes.","Specifically, we propose a Bias-Guided Generative Network (BGGN).","By treating each bias value as a reward, BGGN efficiently generates high-bias intersectional sensitive attributes.","Experiments on real-world text and image datasets demonstrate a diverse and efficient discovery of BGGN.","To further evaluate the generated unseen but possible unfair intersectional sensitive attributes, we formulate them as prompts and use modern generative AI to produce new texts and images.","The results of frequently generating biased data provides new insights of discovering potential unfairness in popular modern generative AI systems.","Warning:","This paper contains generative examples that are offensive in nature."],"url":"http://arxiv.org/abs/2405.20790v1","category":"cs.LG"}
{"created":"2024-05-31 12:35:06","title":"CoMoFusion: Fast and High-quality Fusion of Infrared and Visible Image with Consistency Model","abstract":"Generative models are widely utilized to model the distribution of fused images in the field of infrared and visible image fusion. However, current generative models based fusion methods often suffer from unstable training and slow inference speed. To tackle this problem, a novel fusion method based on consistency model is proposed, termed as CoMoFusion, which can generate the high-quality images and achieve fast image inference speed. In specific, the consistency model is used to construct multi-modal joint features in the latent space with the forward and reverse process. Then, the infrared and visible features extracted by the trained consistency model are fed into fusion module to generate the final fused image. In order to enhance the texture and salient information of fused images, a novel loss based on pixel value selection is also designed. Extensive experiments on public datasets illustrate that our method obtains the SOTA fusion performance compared with the existing fusion methods.","sentences":["Generative models are widely utilized to model the distribution of fused images in the field of infrared and visible image fusion.","However, current generative models based fusion methods often suffer from unstable training and slow inference speed.","To tackle this problem, a novel fusion method based on consistency model is proposed, termed as CoMoFusion, which can generate the high-quality images and achieve fast image inference speed.","In specific, the consistency model is used to construct multi-modal joint features in the latent space with the forward and reverse process.","Then, the infrared and visible features extracted by the trained consistency model are fed into fusion module to generate the final fused image.","In order to enhance the texture and salient information of fused images, a novel loss based on pixel value selection is also designed.","Extensive experiments on public datasets illustrate that our method obtains the SOTA fusion performance compared with the existing fusion methods."],"url":"http://arxiv.org/abs/2405.20764v1","category":"cs.CV"}
{"created":"2024-05-31 12:32:34","title":"Improving Generalization and Convergence by Enhancing Implicit Regularization","abstract":"In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with {\\em generic base optimizers} without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a $2\\times$ {\\em speed-up} compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in Sharpness-aware Minimization (SAM).","sentences":["In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence.","Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions.","We show that IRE can be practically incorporated with {\\em generic base optimizers} without introducing significant computational overload.","Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs).","Surprisingly, IRE also achieves a $2\\times$ {\\em speed-up} compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext.","Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in Sharpness-aware Minimization (SAM)."],"url":"http://arxiv.org/abs/2405.20763v1","category":"cs.LG"}
{"created":"2024-05-31 12:27:38","title":"Share Your Secrets for Privacy! Confidential Forecasting with Vertical Federated Learning","abstract":"Vertical federated learning (VFL) is a promising area for time series forecasting in industrial applications, such as predictive maintenance and machine control. Critical challenges to address in manufacturing include data privacy and over-fitting on small and noisy datasets during both training and inference. Additionally, to increase industry adaptability, such forecasting models must scale well with the number of parties while ensuring strong convergence and low-tuning complexity. We address those challenges and propose 'Secret-shared Time Series Forecasting with VFL' (STV), a novel framework that exhibits the following key features: i) a privacy-preserving algorithm for forecasting with SARIMAX and autoregressive trees on vertically partitioned data; ii) serverless forecasting using secret sharing and multi-party computation; iii) novel N-party algorithms for matrix multiplication and inverse operations for direct parameter optimization, giving strong convergence with minimal hyperparameter tuning complexity. We conduct evaluations on six representative datasets from public and industry-specific contexts. Our results demonstrate that STV's forecasting accuracy is comparable to those of centralized approaches. They also show that our direct optimization can outperform centralized methods, which include state-of-the-art diffusion models and long-short-term memory, by 23.81% on forecasting accuracy. We also conduct a scalability analysis by examining the communication costs of direct and iterative optimization to navigate the choice between the two. Code and appendix are available: https://github.com/adis98/STV","sentences":["Vertical federated learning (VFL) is a promising area for time series forecasting in industrial applications, such as predictive maintenance and machine control.","Critical challenges to address in manufacturing include data privacy and over-fitting on small and noisy datasets during both training and inference.","Additionally, to increase industry adaptability, such forecasting models must scale well with the number of parties while ensuring strong convergence and low-tuning complexity.","We address those challenges and propose 'Secret-shared Time Series Forecasting with VFL' (STV), a novel framework that exhibits the following key features: i) a privacy-preserving algorithm for forecasting with SARIMAX and autoregressive trees on vertically partitioned data; ii) serverless forecasting using secret sharing and multi-party computation; iii) novel N-party algorithms for matrix multiplication and inverse operations for direct parameter optimization, giving strong convergence with minimal hyperparameter tuning complexity.","We conduct evaluations on six representative datasets from public and industry-specific contexts.","Our results demonstrate that STV's forecasting accuracy is comparable to those of centralized approaches.","They also show that our direct optimization can outperform centralized methods, which include state-of-the-art diffusion models and long-short-term memory, by 23.81% on forecasting accuracy.","We also conduct a scalability analysis by examining the communication costs of direct and iterative optimization to navigate the choice between the two.","Code and appendix are available: https://github.com/adis98/STV"],"url":"http://arxiv.org/abs/2405.20761v1","category":"cs.LG"}
{"created":"2024-05-31 12:20:02","title":"Information Theoretic Text-to-Image Alignment","abstract":"Diffusion models for Text-to-Image (T2I) conditional generation have seen tremendous success recently. Despite their success, accurately capturing user intentions with these models still requires a laborious trial and error process. This challenge is commonly identified as a model alignment problem, an issue that has attracted considerable attention by the research community. Instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models to steer image generation, in this work we present a novel method that relies on an information-theoretic alignment measure. In a nutshell, our method uses self-supervised fine-tuning and relies on point-wise mutual information between prompts and images to define a synthetic training set to induce model alignment. Our comparative analysis shows that our method is on-par or superior to the state-of-the-art, yet requires nothing but a pre-trained denoising network to estimate MI and a lightweight fine-tuning strategy.","sentences":["Diffusion models for Text-to-Image (T2I) conditional generation have seen tremendous success recently.","Despite their success, accurately capturing user intentions with these models still requires a laborious trial and error process.","This challenge is commonly identified as a model alignment problem, an issue that has attracted considerable attention by the research community.","Instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models to steer image generation, in this work we present a novel method that relies on an information-theoretic alignment measure.","In a nutshell, our method uses self-supervised fine-tuning and relies on point-wise mutual information between prompts and images to define a synthetic training set to induce model alignment.","Our comparative analysis shows that our method is on-par or superior to the state-of-the-art, yet requires nothing but a pre-trained denoising network to estimate MI and a lightweight fine-tuning strategy."],"url":"http://arxiv.org/abs/2405.20759v1","category":"cs.LG"}
{"created":"2024-05-31 11:46:44","title":"Understanding magnetic hyperthermia performance within the \"Brezovich criterion\": beyond the uniaxial anisotropy description","abstract":"Careful determination of the heating performance of magnetic nanoparticles under AC fields is critical for magnetic hyperthermia applications. However, most interpretations of experimental data are based on the uniaxial anisotropy approximation, which in first instance can be correlated with particle aspect ratio. This is to say, the intrinsic magnetocrystalline anisotropy is discarded, under the assumption that the shape contribution dominates. We show in this work that such premise, generally valid for large field amplitudes, does not hold for describing hyperthermia experiments carried out under small field values. Specifically, given its relevance for \\textit{in vivo} applications, we focus our analysis on the so-called \"Brezovich criterion\", $H\\cdot{f}=4.85\\cdot{10^8}A/m\\cdot{s}$. By means of a computational model, we show that the intrinsic magnetocrystalline anisotropy plays a critical role in defining the heat output; otherwise, the only-uniaxial anisotropy description reports essentially negligible heat loss values. Our results call therefore for an improvement in the theoretical models used to interpret magnetic hyperthermia performance.","sentences":["Careful determination of the heating performance of magnetic nanoparticles under AC fields is critical for magnetic hyperthermia applications.","However, most interpretations of experimental data are based on the uniaxial anisotropy approximation, which in first instance can be correlated with particle aspect ratio.","This is to say, the intrinsic magnetocrystalline anisotropy is discarded, under the assumption that the shape contribution dominates.","We show in this work that such premise, generally valid for large field amplitudes, does not hold for describing hyperthermia experiments carried out under small field values.","Specifically, given its relevance for \\textit{in vivo} applications, we focus our analysis on the so-called \"Brezovich criterion\", $H\\cdot{f}=4.85\\cdot{10^8}A/m\\cdot{s}$.","By means of a computational model, we show that the intrinsic magnetocrystalline anisotropy plays a critical role in defining the heat output; otherwise, the only-uniaxial anisotropy description reports essentially negligible heat loss values.","Our results call therefore for an improvement in the theoretical models used to interpret magnetic hyperthermia performance."],"url":"http://arxiv.org/abs/2405.20756v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 11:43:31","title":"Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario","abstract":"Hate detection has long been a challenging task for the NLP community. The task becomes complex in a code-mixed environment because the models must understand the context and the hate expressed through language alteration. Compared to the monolingual setup, we see very less work on code-mixed hate as large-scale annotated hate corpora are unavailable to make the study. To overcome this bottleneck, we propose using native language hate samples. We hypothesise that in the era of multilingual language models (MLMs), hate in code-mixed settings can be detected by majorly relying on the native language samples. Even though the NLP literature reports the effectiveness of MLMs on hate detection in many cross-lingual settings, their extensive evaluation in a code-mixed scenario is yet to be done. This paper attempts to fill this gap through rigorous empirical experiments. We considered the Hindi-English code-mixed setup as a case study as we have the linguistic expertise for the same. Some of the interesting observations we got are: (i) adding native hate samples in the code-mixed training set, even in small quantity, improved the performance of MLMs for code-mixed hate detection, (ii) MLMs trained with native samples alone observed to be detecting code-mixed hate to a large extent, (iii) The visualisation of attention scores revealed that, when native samples were included in training, MLMs could better focus on the hate emitting words in the code-mixed context, and (iv) finally, when hate is subjective or sarcastic, naively mixing native samples doesn't help much to detect code-mixed hate. We will release the data and code repository to reproduce the reported results.","sentences":["Hate detection has long been a challenging task for the NLP community.","The task becomes complex in a code-mixed environment because the models must understand the context and the hate expressed through language alteration.","Compared to the monolingual setup, we see very less work on code-mixed hate as large-scale annotated hate corpora are unavailable to make the study.","To overcome this bottleneck, we propose using native language hate samples.","We hypothesise that in the era of multilingual language models (MLMs), hate in code-mixed settings can be detected by majorly relying on the native language samples.","Even though the NLP literature reports the effectiveness of MLMs on hate detection in many cross-lingual settings, their extensive evaluation in a code-mixed scenario is yet to be done.","This paper attempts to fill this gap through rigorous empirical experiments.","We considered the Hindi-English code-mixed setup as a case study as we have the linguistic expertise for the same.","Some of the interesting observations we got are: (i) adding native hate samples in the code-mixed training set, even in small quantity, improved the performance of MLMs for code-mixed hate detection, (ii) MLMs trained with native samples alone observed to be detecting code-mixed hate to a large extent, (iii) The visualisation of attention scores revealed that, when native samples were included in training, MLMs could better focus on the hate emitting words in the code-mixed context, and (iv) finally, when hate is subjective or sarcastic, naively mixing native samples doesn't help much to detect code-mixed hate.","We will release the data and code repository to reproduce the reported results."],"url":"http://arxiv.org/abs/2405.20755v1","category":"cs.CL"}
{"created":"2024-05-31 11:37:37","title":"Non-uniqueness of weak solutions to 2D generalized Navier-Stokes equations","abstract":"We study the non-uniqueness of weak solutions for the two-dimensional hyper-dissipative Navier-Stokes equations in the super-critical spaces $L_{t}^{\\gamma}L_{x}^{p}$ when $\\alpha\\in[1,\\frac{3}{2})$, and obtain the conclusion that the non-uniqueness of the weak solutions at the endpoint $(\\gamma,p)=(\\infty, \\frac{2}{2\\alpha-1})$ is sharp in view of the generalized Lady\\v{z}enskaja-Prodi-Serrin condition by using a different spatial-temporal building block from [Cheskidov-Luo, Ann. PDE, 9:13 (2023)] and taking advantage of the intermittency of the temporal concentrated function $g_{(k)}$ in an almost optimal way. Our results recover the above 2D non-uniqueness conclusion and extend to the hyper-dissipative case $\\alpha \\in(1,\\frac{3}{2})$.","sentences":["We study the non-uniqueness of weak solutions for the two-dimensional hyper-dissipative Navier-Stokes equations in the super-critical spaces $L_{t}^{\\gamma}L_{x}^{p}$ when $\\alpha\\in[1,\\frac{3}{2})$, and obtain the conclusion that the non-uniqueness of the weak solutions at the endpoint $(\\gamma,p)=(\\infty, \\frac{2}{2\\alpha-1})$ is sharp in view of the generalized Lady\\v{z}enskaja-Prodi-Serrin condition by using a different spatial-temporal building block from [Cheskidov-Luo, Ann. PDE, 9:13 (2023)] and taking advantage of the intermittency of the temporal concentrated function $g_{(k)}$ in an almost optimal way.","Our results recover the above 2D non-uniqueness conclusion and extend to the hyper-dissipative case $\\alpha \\in(1,\\frac{3}{2})$."],"url":"http://arxiv.org/abs/2405.20754v1","category":"math.AP"}
{"created":"2024-05-31 11:31:20","title":"The generating power of weighted tree automata with initial algebra semantics","abstract":"We consider the images of the initial algebra semantics of weighted tree automata over strong bimonoids (hence also over semirings). These images are subsets of the carrier set of the underlying strong bimonoid. We consider locally finite, weakly locally finite, and bi-locally finite strong bimonoids. We show that there exists a strong bimonoid which is weakly locally finite and not locally finite. We also show that if the ranked alphabet contains a binary symbol, then for any finitely generated strong bimonoid, weighted tree automata can generate, via their initial algebra semantics, all elements of the strong bimonoid. As a consequence of these results, for weakly locally finite strong bimonoids which are not locally finite, weighted tree automata can generate infinite images provided that the input ranked alphabet contains at least one binary symbol. This is in sharp contrast to the setting of weighted string automata, where each such image is known to be finite. As a further consequence, for any finitely generated semiring, there exists a weighted tree automaton which generates, via its run semantics, all elements of the semiring.","sentences":["We consider the images of the initial algebra semantics of weighted tree automata over strong bimonoids (hence also over semirings).","These images are subsets of the carrier set of the underlying strong bimonoid.","We consider locally finite, weakly locally finite, and bi-locally finite strong bimonoids.","We show that there exists a strong bimonoid which is weakly locally finite and not locally finite.","We also show that if the ranked alphabet contains a binary symbol, then for any finitely generated strong bimonoid, weighted tree automata can generate, via their initial algebra semantics, all elements of the strong bimonoid.","As a consequence of these results, for weakly locally finite strong bimonoids which are not locally finite, weighted tree automata can generate infinite images provided that the input ranked alphabet contains at least one binary symbol.","This is in sharp contrast to the setting of weighted string automata, where each such image is known to be finite.","As a further consequence, for any finitely generated semiring, there exists a weighted tree automaton which generates, via its run semantics, all elements of the semiring."],"url":"http://arxiv.org/abs/2405.20753v1","category":"cs.FL"}
{"created":"2024-05-31 11:14:52","title":"Linear models of strip-type roughness","abstract":"Prandtl's secondary flows of the second kind generated by laterally-varying roughness are studied in this work using the linearised Reynolds-Averaged Navier-Stokes approach recently proposed. The momentum equations are coupled to the SA transport model for the turbulent viscosity and the surface roughness is captured by adapting established strategies for homogeneous rough walls to spanwise inhomogeneous surfaces. Linearisation of the governing equations and of the roughness model yields a linear framework that elucidates the linear mechanisms that play a role in the generation of secondary flows. In addition, the framework allows exploring the parameter space associated with heterogeneous rough surfaces. Channel flow is considered, with high and low roughness strips arranged symmetrically on the two channel walls. The strip width is varied systematically to ascertain the role played by the length scale characterising the heterogeneity on the size and intensity of secondary flows. It is found that linear mechanisms, i.e. whereby secondary flows are interpreted as the output response of the turbulent mean flow subjected to a forcing localised at the wall, may be sufficient to explain such a role. In fact the model predicts that secondary flows are most intense when the strip width is about 0.7 times the half-channel height, in excellent agreement with available data. Further, a unified framework to analyse combinations of heterogeneous roughness properties and laterally-varying topographies, common in engineering applications, is discussed. This framework yields two separate secondary-flow inducing source mechanisms, i.e. the lateral variation of the virtual origin from which the turbulent structure is allowed to develop and the lateral variation of the streamwise velocity slip, capturing the acceleration/deceleration perceived by the bulk flow over troughs and crests of the topography.","sentences":["Prandtl's secondary flows of the second kind generated by laterally-varying roughness are studied in this work using the linearised Reynolds-Averaged Navier-Stokes approach recently proposed.","The momentum equations are coupled to the SA transport model for the turbulent viscosity and the surface roughness is captured by adapting established strategies for homogeneous rough walls to spanwise inhomogeneous surfaces.","Linearisation of the governing equations and of the roughness model yields a linear framework that elucidates the linear mechanisms that play a role in the generation of secondary flows.","In addition, the framework allows exploring the parameter space associated with heterogeneous rough surfaces.","Channel flow is considered, with high and low roughness strips arranged symmetrically on the two channel walls.","The strip width is varied systematically to ascertain the role played by the length scale characterising the heterogeneity on the size and intensity of secondary flows.","It is found that linear mechanisms, i.e. whereby secondary flows are interpreted as the output response of the turbulent mean flow subjected to a forcing localised at the wall, may be sufficient to explain such a role.","In fact the model predicts that secondary flows are most intense when the strip width is about 0.7 times the half-channel height, in excellent agreement with available data.","Further, a unified framework to analyse combinations of heterogeneous roughness properties and laterally-varying topographies, common in engineering applications, is discussed.","This framework yields two separate secondary-flow inducing source mechanisms, i.e. the lateral variation of the virtual origin from which the turbulent structure is allowed to develop and the lateral variation of the streamwise velocity slip, capturing the acceleration/deceleration perceived by the bulk flow over troughs and crests of the topography."],"url":"http://arxiv.org/abs/2405.20751v1","category":"physics.flu-dyn"}
{"created":"2024-05-31 11:14:12","title":"Diffusion Models Are Innate One-Step Generators","abstract":"Diffusion Models (DMs) have achieved great success in image generation and other fields. By fine sampling through the trajectory defined by the SDE/ODE solver based on a well-trained score model, DMs can generate remarkable high-quality results. However, this precise sampling often requires multiple steps and is computationally demanding. To address this problem, instance-based distillation methods have been proposed to distill a one-step generator from a DM by having a simpler student model mimic a more complex teacher model. Yet, our research reveals an inherent limitations in these methods: the teacher model, with more steps and more parameters, occupies different local minima compared to the student model, leading to suboptimal performance when the student model attempts to replicate the teacher. To avoid this problem, we introduce a novel distributional distillation method, which uses an exclusive distributional loss. This method exceeds state-of-the-art (SOTA) results while requiring significantly fewer training images. Additionally, we show that DMs' layers are activated differently at different time steps, leading to an inherent capability to generate images in a single step. Freezing most of the convolutional layers in a DM during distributional distillation leads to further performance improvements. Our method achieves the SOTA results on CIFAR-10 (FID 1.54), AFHQv2 64x64 (FID 1.23), FFHQ 64x64 (FID 0.85) and ImageNet 64x64 (FID 1.16) with great efficiency. Most of those results are obtained with only 5 million training images within 6 hours on 8 A100 GPUs. This breakthrough not only enhances the understanding of efficient image generation models but also offers a scalable framework for advancing the state of the art in various applications.","sentences":["Diffusion Models (DMs) have achieved great success in image generation and other fields.","By fine sampling through the trajectory defined by the SDE/ODE solver based on a well-trained score model, DMs can generate remarkable high-quality results.","However, this precise sampling often requires multiple steps and is computationally demanding.","To address this problem, instance-based distillation methods have been proposed to distill a one-step generator from a DM by having a simpler student model mimic a more complex teacher model.","Yet, our research reveals an inherent limitations in these methods: the teacher model, with more steps and more parameters, occupies different local minima compared to the student model, leading to suboptimal performance when the student model attempts to replicate the teacher.","To avoid this problem, we introduce a novel distributional distillation method, which uses an exclusive distributional loss.","This method exceeds state-of-the-art (SOTA) results while requiring significantly fewer training images.","Additionally, we show that DMs' layers are activated differently at different time steps, leading to an inherent capability to generate images in a single step.","Freezing most of the convolutional layers in a DM during distributional distillation leads to further performance improvements.","Our method achieves the SOTA results on CIFAR-10 (FID 1.54), AFHQv2 64x64 (FID 1.23), FFHQ 64x64 (FID 0.85) and ImageNet 64x64 (FID 1.16) with great efficiency.","Most of those results are obtained with only 5 million training images within 6 hours on 8 A100 GPUs.","This breakthrough not only enhances the understanding of efficient image generation models but also offers a scalable framework for advancing the state of the art in various applications."],"url":"http://arxiv.org/abs/2405.20750v1","category":"cs.CV"}
{"created":"2024-05-31 10:30:14","title":"OpenTensor: Reproducing Faster Matrix Multiplication Discovering Algorithms","abstract":"OpenTensor is a reproduction of AlphaTensor, which discovered a new algorithm that outperforms the state-of-the-art methods for matrix multiplication by Deep Reinforcement Learning (DRL). While AlphaTensor provides a promising framework for solving scientific problems, it is really hard to reproduce due to the massive tricks and lack of source codes. In this paper, we clean up the algorithm pipeline, clarify the technical details, and make some improvements to the training process. Computational results show that OpenTensor can successfully find efficient matrix multiplication algorithms.","sentences":["OpenTensor is a reproduction of AlphaTensor, which discovered a new algorithm that outperforms the state-of-the-art methods for matrix multiplication by Deep Reinforcement Learning (DRL).","While AlphaTensor provides a promising framework for solving scientific problems, it is really hard to reproduce due to the massive tricks and lack of source codes.","In this paper, we clean up the algorithm pipeline, clarify the technical details, and make some improvements to the training process.","Computational results show that OpenTensor can successfully find efficient matrix multiplication algorithms."],"url":"http://arxiv.org/abs/2405.20748v1","category":"cs.AI"}
{"created":"2024-05-31 10:23:47","title":"Generalized Inverse Optimal Control and its Application in Biology","abstract":"Living organisms exhibit remarkable adaptations across all scales, from molecules to ecosystems. We believe that many of these adaptations correspond to optimal solutions driven by evolution, training, and underlying physical and chemical laws and constraints. While some argue against such optimality principles due to their potential ambiguity, we propose generalized inverse optimal control to infer them directly from data. This novel approach incorporates multi-criteria optimality, nestedness of objective functions on different scales, the presence of active constraints, the possibility of switches of optimality principles during the observed time horizon, maximization of robustness, and minimization of time as important special cases, as well as uncertainties involved with the mathematical modeling of biological systems. This data-driven approach ensures that optimality principles are not merely theoretical constructs but are firmly rooted in experimental observations. Furthermore, the inferred principles can be used in forward optimal control to predict and manipulate biological systems, with possible applications in bio-medicine, biotechnology, and agriculture. As discussed and illustrated, the well-posed problem formulation and the inference are challenging and require a substantial interdisciplinary effort in the development of theory and robust numerical methods.","sentences":["Living organisms exhibit remarkable adaptations across all scales, from molecules to ecosystems.","We believe that many of these adaptations correspond to optimal solutions driven by evolution, training, and underlying physical and chemical laws and constraints.","While some argue against such optimality principles due to their potential ambiguity, we propose generalized inverse optimal control to infer them directly from data.","This novel approach incorporates multi-criteria optimality, nestedness of objective functions on different scales, the presence of active constraints, the possibility of switches of optimality principles during the observed time horizon, maximization of robustness, and minimization of time as important special cases, as well as uncertainties involved with the mathematical modeling of biological systems.","This data-driven approach ensures that optimality principles are not merely theoretical constructs but are firmly rooted in experimental observations.","Furthermore, the inferred principles can be used in forward optimal control to predict and manipulate biological systems, with possible applications in bio-medicine, biotechnology, and agriculture.","As discussed and illustrated, the well-posed problem formulation and the inference are challenging and require a substantial interdisciplinary effort in the development of theory and robust numerical methods."],"url":"http://arxiv.org/abs/2405.20747v1","category":"q-bio.QM"}
{"created":"2024-05-31 10:15:14","title":"On the sequential convergence of Lloyd's algorithms","abstract":"Lloyd's algorithm is an iterative method that solves the quantization problem, i.e. the approximation of a target probability measure by a discrete one, and is particularly used in digital applications.This algorithm can be interpreted as a gradient method on a certain quantization functional which is given by optimal transport. We study the sequential convergence (to a single accumulation point) for two variants of Lloyd's method: (i) optimal quantization with an arbitrary discrete measure and (ii) uniform quantization with a uniform discrete measure. For both cases, we prove sequential convergence of the iterates under an analiticity assumption on the density of the target measure. This includes for example analytic densities truncated to a compact semi-algebraic set. The argument leverages the log analytic nature of globally subanalytic integrals, the interpretation of Lloyd's method as a gradient method and the convergence analysis of gradient algorithms under Kurdyka-Lojasiewicz assumptions. As a by-product, we also obtain definability results for more general semi-discrete optimal transport losses such as transport distances with general costs, the max-sliced Wasserstein distance and the entropy regularized optimal transport loss.","sentences":["Lloyd's algorithm is an iterative method that solves the quantization problem, i.e. the approximation of a target probability measure by a discrete one, and is particularly used in digital applications.","This algorithm can be interpreted as a gradient method on a certain quantization functional which is given by optimal transport.","We study the sequential convergence (to a single accumulation point) for two variants of Lloyd's method: (i) optimal quantization with an arbitrary discrete measure and (ii) uniform quantization with a uniform discrete measure.","For both cases, we prove sequential convergence of the iterates under an analiticity assumption on the density of the target measure.","This includes for example analytic densities truncated to a compact semi-algebraic set.","The argument leverages the log analytic nature of globally subanalytic integrals, the interpretation of Lloyd's method as a gradient method and the convergence analysis of gradient algorithms under Kurdyka-Lojasiewicz assumptions.","As a by-product, we also obtain definability results for more general semi-discrete optimal transport losses such as transport distances with general costs, the max-sliced Wasserstein distance and the entropy regularized optimal transport loss."],"url":"http://arxiv.org/abs/2405.20744v1","category":"math.OC"}
{"created":"2024-05-31 10:13:17","title":"Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes","abstract":"Trajectory forecasting is crucial for video surveillance analytics, as it enables the anticipation of future movements for a set of agents, e.g. basketball players engaged in intricate interactions with long-term intentions. Deep generative models offer a natural learning approach for trajectory forecasting, yet they encounter difficulties in achieving an optimal balance between sampling fidelity and diversity. We address this challenge by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize a discrete latent space to tackle the issue of posterior collapse. Specifically, we introduce an instance-based codebook that allows tailored latent representations for each example. In a nutshell, the rows of the codebook are dynamically adjusted to reflect contextual information (i.e., past motion patterns extracted from the observed trajectories). In this way, the discretization process gains flexibility, leading to improved reconstructions. Notably, instance-level dynamics are injected into the codebook through low-rank updates, which restrict the customization of the codebook to a lower dimension space. The resulting discrete space serves as the basis of the subsequent step, which regards the training of a diffusion-based predictive model. We show that such a two-fold framework, augmented with instance-level discretization, leads to accurate and diverse forecasts, yielding state-of-the-art performance on three established benchmarks.","sentences":["Trajectory forecasting is crucial for video surveillance analytics, as it enables the anticipation of future movements for a set of agents, e.g. basketball players engaged in intricate interactions with long-term intentions.","Deep generative models offer a natural learning approach for trajectory forecasting, yet they encounter difficulties in achieving an optimal balance between sampling fidelity and diversity.","We address this challenge by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize a discrete latent space to tackle the issue of posterior collapse.","Specifically, we introduce an instance-based codebook that allows tailored latent representations for each example.","In a nutshell, the rows of the codebook are dynamically adjusted to reflect contextual information (i.e., past motion patterns extracted from the observed trajectories).","In this way, the discretization process gains flexibility, leading to improved reconstructions.","Notably, instance-level dynamics are injected into the codebook through low-rank updates, which restrict the customization of the codebook to a lower dimension space.","The resulting discrete space serves as the basis of the subsequent step, which regards the training of a diffusion-based predictive model.","We show that such a two-fold framework, augmented with instance-level discretization, leads to accurate and diverse forecasts, yielding state-of-the-art performance on three established benchmarks."],"url":"http://arxiv.org/abs/2405.20743v1","category":"cs.CV"}
{"created":"2024-05-31 10:12:47","title":"Terahertz emission from mutually synchronized standalone Bi2Sr2CaCu2O8+x intrinsic-Josephson-junction stacks","abstract":"Suitably patterned single crystals made of the cuprate superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+x}$ (BSCCO), intrinsically forming a stack of Josephson junctions, can generate electromagnetic radiation in the lower terahertz regime. Due to Joule heating the emission power of single stacks seems to be limited to values below 100 $\\mu$W. To increase the radiation power, mutually synchronized arrays situated on the same BSCCO base crystal have been studied. Mutual electromagnetic interactions via a connecting BSCCO base crystal have been considered essential for synchronization, but the approach still suffers from Joule heating, preventing the synchronization of more than three stacks. In the present paper we show, on the basis of two emitting stacks, that mutual synchronization can also be achieved by stand-alone stacks contacted by gold layers and sharing only a common gold layer. Compared to BSCCO base crystals, the gold layers have a much higher thermal conductivity and their patterning is not very problematic. We analyze our results in detail, showing that the two oscillators exhibit phase correlations over a range of $\\pm$0.4 GHz relative to their center frequencies, which we mainly studied between 745 GHz and 765 GHz. However, we also find that strong phase gradients in the beams radiated from both the mutually locked stacks and the unlocked ones play an important role and, presumably, diminish the detected emission power due to destructive interference. We speculate that the effect arises from higher-order cavity modes which are excited in the individual stacks. Our main message is that the mutual interaction provided by a common gold layer may open new possibilities for relaxing the Joule-heating-problem, allowing the synchronization of a higher number of stacks. Our findings may boost attempts to substantially increase the output power levels of the BSCCO terahertz oscillators.","sentences":["Suitably patterned single crystals made of the cuprate superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+x}$ (BSCCO), intrinsically forming a stack of Josephson junctions, can generate electromagnetic radiation in the lower terahertz regime.","Due to Joule heating the emission power of single stacks seems to be limited to values below 100 $\\mu$W. To increase the radiation power, mutually synchronized arrays situated on the same BSCCO base crystal have been studied.","Mutual electromagnetic interactions via a connecting BSCCO base crystal have been considered essential for synchronization, but the approach still suffers from Joule heating, preventing the synchronization of more than three stacks.","In the present paper we show, on the basis of two emitting stacks, that mutual synchronization can also be achieved by stand-alone stacks contacted by gold layers and sharing only a common gold layer.","Compared to BSCCO base crystals, the gold layers have a much higher thermal conductivity and their patterning is not very problematic.","We analyze our results in detail, showing that the two oscillators exhibit phase correlations over a range of $\\pm$0.4 GHz relative to their center frequencies, which we mainly studied between 745 GHz and 765 GHz.","However, we also find that strong phase gradients in the beams radiated from both the mutually locked stacks and the unlocked ones play an important role and, presumably, diminish the detected emission power due to destructive interference.","We speculate that the effect arises from higher-order cavity modes which are excited in the individual stacks.","Our main message is that the mutual interaction provided by a common gold layer may open new possibilities for relaxing the Joule-heating-problem, allowing the synchronization of a higher number of stacks.","Our findings may boost attempts to substantially increase the output power levels of the BSCCO terahertz oscillators."],"url":"http://arxiv.org/abs/2405.20742v1","category":"cond-mat.supr-con"}
{"created":"2024-05-31 09:59:49","title":"Regular Subgradients of Marginal Functions with Applications to Calculus and Bilevel Programming","abstract":"The paper addresses the study and applications of a broad class of extended-real-valued functions, known as optimal value or marginal functions, which are frequently appeared in variational analysis, parametric optimization, and a variety of applications. Functions of this type are intrinsically nonsmooth and require the usage of tools of generalized differentiation. The main results of this paper provide novel evaluations and exact calculations of regular/Fr\\'echet subgradients and their singular counterparts for general classes of marginal functions via their given data. The obtained results are applied to establishing new calculus rules for such subgradients and necessary optimality conditions in bilevel programming","sentences":["The paper addresses the study and applications of a broad class of extended-real-valued functions, known as optimal value or marginal functions, which are frequently appeared in variational analysis, parametric optimization, and a variety of applications.","Functions of this type are intrinsically nonsmooth and require the usage of tools of generalized differentiation.","The main results of this paper provide novel evaluations and exact calculations of regular/Fr\\'echet subgradients and their singular counterparts for general classes of marginal functions via their given data.","The obtained results are applied to establishing new calculus rules for such subgradients and necessary optimality conditions in bilevel programming"],"url":"http://arxiv.org/abs/2405.20737v1","category":"math.OC"}
{"created":"2024-05-31 09:59:42","title":"Progenitor Constraint Incorporating Shell Merger: The Case of Supernova Remnant G359.0-0.9","abstract":"It is generally hard to put robust constraints on progenitor masses of supernovae (SNe) and remnants (SNRs) observationally, while they offer tantalizing clues to understanding explosion mechanisms and mass distribution. Our recent study suggests that ``shell merger'', which is theoretically expected for stellar evolution, can appreciably affect final yields of inter-mediate mass elements (IMEs; such as Ne, Mg, and Si). In light of this, here we report results of X-ray spectral analysis of a Galactic SNR G359.0-0.9, whose abundance pattern may possibly be anomalous according to a previous study. Our spectroscopy using all the available data taken with XMM-Newton reveals that this remnant is classified as Mg-rich SNRs because of its high Mg-to-Ne ratio (Z_Mg/Z_Ne=1.90+0.27-0.19; mass ratio 0.66+0.09-0.07) and conclude that the result cannot be explained without the shell merger. By comparing the observation with theoretical calculations, we prefer the so-called Ne-burning shell intrusion and in this case the progenitor mass M_ZAMS is likely <15M_sun. We confirm the result also by our new molecular line observations with the NRO-45 m telescope: G359.0-0.9 is located in the Scutum-Centaurus arm (2.66--2.94 kpc) and in this case the resultant total ejecta mass ~6.8M_sun is indeed consistent with the above estimate. Our method using mass ratios of IMEs presented in this paper will become useful to distinguish the type of the shell merger, the Ne-burning shell intrusion and the O-burning shell merger, for future SNR studies.","sentences":["It is generally hard to put robust constraints on progenitor masses of supernovae (SNe) and remnants (SNRs) observationally, while they offer tantalizing clues to understanding explosion mechanisms and mass distribution.","Our recent study suggests that ``shell merger'', which is theoretically expected for stellar evolution, can appreciably affect final yields of inter-mediate mass elements (IMEs; such as Ne, Mg, and Si).","In light of this, here we report results of X-ray spectral analysis of a Galactic SNR G359.0-0.9, whose abundance pattern may possibly be anomalous according to a previous study.","Our spectroscopy using all the available data taken with XMM-Newton reveals that this remnant is classified as Mg-rich SNRs because of its high Mg-to-Ne ratio (Z_Mg/Z_Ne=1.90+0.27-0.19; mass ratio 0.66+0.09-0.07) and conclude that the result cannot be explained without the shell merger.","By comparing the observation with theoretical calculations, we prefer the so-called Ne-burning shell intrusion and in this case the progenitor mass M_ZAMS is likely <15M_sun.","We confirm the result also by our new molecular line observations with the NRO-45 m telescope: G359.0-0.9 is located in the Scutum-Centaurus arm (2.66--2.94 kpc) and in this case the resultant total ejecta mass ~6.8M_sun is indeed consistent with the above estimate.","Our method using mass ratios of IMEs presented in this paper will become useful to distinguish the type of the shell merger, the Ne-burning shell intrusion and the O-burning shell merger, for future SNR studies."],"url":"http://arxiv.org/abs/2405.20736v1","category":"astro-ph.HE"}
{"created":"2024-05-31 09:59:11","title":"Language Augmentation in CLIP for Improved Anatomy Detection on Multi-modal Medical Images","abstract":"Vision-language models have emerged as a powerful tool for previously challenging multi-modal classification problem in the medical domain. This development has led to the exploration of automated image description generation for multi-modal clinical scans, particularly for radiology report generation. Existing research has focused on clinical descriptions for specific modalities or body regions, leaving a gap for a model providing entire-body multi-modal descriptions. In this paper, we address this gap by automating the generation of standardized body station(s) and list of organ(s) across the whole body in multi-modal MR and CT radiological images. Leveraging the versatility of the Contrastive Language-Image Pre-training (CLIP), we refine and augment the existing approach through multiple experiments, including baseline model fine-tuning, adding station(s) as a superset for better correlation between organs, along with image and language augmentations. Our proposed approach demonstrates 47.6% performance improvement over baseline PubMedCLIP.","sentences":["Vision-language models have emerged as a powerful tool for previously challenging multi-modal classification problem in the medical domain.","This development has led to the exploration of automated image description generation for multi-modal clinical scans, particularly for radiology report generation.","Existing research has focused on clinical descriptions for specific modalities or body regions, leaving a gap for a model providing entire-body multi-modal descriptions.","In this paper, we address this gap by automating the generation of standardized body station(s) and list of organ(s) across the whole body in multi-modal MR and CT radiological images.","Leveraging the versatility of the Contrastive Language-Image Pre-training (CLIP), we refine and augment the existing approach through multiple experiments, including baseline model fine-tuning, adding station(s) as a superset for better correlation between organs, along with image and language augmentations.","Our proposed approach demonstrates 47.6% performance improvement over baseline PubMedCLIP."],"url":"http://arxiv.org/abs/2405.20735v1","category":"cs.CV"}
{"created":"2024-05-31 09:43:14","title":"Dynamic Microgrid Formation Considering Time-dependent Contingency: A Distributionally Robust Approach","abstract":"The increasing frequency of extreme weather events has posed significant risks to the operation of power grids. During long-duration extreme weather events, microgrid formation (MF) is an essential solution to enhance the resilience of the distribution systems by proactively partitioning the distribution system into several microgrids to mitigate the impact of contingencies. This paper proposes a distributionally robust dynamic microgrid formation (DR-DMF) approach to fully consider the temporal characteristics of line failure probability during long-duration extreme weather events like typhoons. The boundaries of each microgrid are dynamically adjusted to enhance the resilience of the system. Furthermore, the expected load shedding is minimized by a distributionally robust optimization model considering the uncertainty of line failure probability regarding the worst-case distribution of contingencies. The effectiveness of the proposed model is verified by numerical simulations on a modified IEEE 37-node system.","sentences":["The increasing frequency of extreme weather events has posed significant risks to the operation of power grids.","During long-duration extreme weather events, microgrid formation (MF) is an essential solution to enhance the resilience of the distribution systems by proactively partitioning the distribution system into several microgrids to mitigate the impact of contingencies.","This paper proposes a distributionally robust dynamic microgrid formation (DR-DMF) approach to fully consider the temporal characteristics of line failure probability during long-duration extreme weather events like typhoons.","The boundaries of each microgrid are dynamically adjusted to enhance the resilience of the system.","Furthermore, the expected load shedding is minimized by a distributionally robust optimization model considering the uncertainty of line failure probability regarding the worst-case distribution of contingencies.","The effectiveness of the proposed model is verified by numerical simulations on a modified IEEE 37-node system."],"url":"http://arxiv.org/abs/2405.20733v1","category":"eess.SY"}
{"created":"2024-05-31 09:39:41","title":"Maximum Temperature Prediction Using Remote Sensing Data Via Convolutional Neural Network","abstract":"Urban heat islands, defined as specific zones exhibiting substantially higher temperatures than their immediate environs, pose significant threats to environmental sustainability and public health. This study introduces a novel machine-learning model that amalgamates data from the Sentinel-3 satellite, meteorological predictions, and additional remote sensing inputs. The primary aim is to generate detailed spatiotemporal maps that forecast the peak temperatures within a 24-hour period in Turin. Experimental results validate the model's proficiency in predicting temperature patterns, achieving a Mean Absolute Error (MAE) of 2.09 degrees Celsius for the year 2023 at a resolution of 20 meters per pixel, thereby enriching our knowledge of urban climatic behavior. This investigation enhances the understanding of urban microclimates, emphasizing the importance of cross-disciplinary data integration, and laying the groundwork for informed policy-making aimed at alleviating the negative impacts of extreme urban temperatures.","sentences":["Urban heat islands, defined as specific zones exhibiting substantially higher temperatures than their immediate environs, pose significant threats to environmental sustainability and public health.","This study introduces a novel machine-learning model that amalgamates data from the Sentinel-3 satellite, meteorological predictions, and additional remote sensing inputs.","The primary aim is to generate detailed spatiotemporal maps that forecast the peak temperatures within a 24-hour period in Turin.","Experimental results validate the model's proficiency in predicting temperature patterns, achieving a Mean Absolute Error (MAE) of 2.09 degrees Celsius for the year 2023 at a resolution of 20 meters per pixel, thereby enriching our knowledge of urban climatic behavior.","This investigation enhances the understanding of urban microclimates, emphasizing the importance of cross-disciplinary data integration, and laying the groundwork for informed policy-making aimed at alleviating the negative impacts of extreme urban temperatures."],"url":"http://arxiv.org/abs/2405.20731v1","category":"cs.AI"}
{"created":"2024-05-31 09:39:10","title":"GRAVITY for MATISSE -- Improving the MATISSE performance with the GRAVITY fringe tracker","abstract":"Context: MATISSE, the mid-infrared spectro-imaging instrument of VLTI, was designed to deliver its advertised performance when paired with an external second generation fringe tracker. Science observation started in 2019, demonstrating imaging capabilities and faint science target observations. Now, The GRAVITY fringe tracker stabilizes the MATISSE fringes which allows using all spectroscopic modes and improves sensitivity and data accuracy. Aims: We present how the MATISSE and GRAVITY instruments were adapted to make the GRAVITY fringe tracker work with MATISSE, under the umbrella of the aptly-named GRA4MAT project, led by ESO in collaboration with the two instrument consortia. Methods: We detail the software modifications needed to implement an acquisition and observing sequence specific to GRA4MAT, including simultaneous fringe tracking and chopping and a narrow off-axis capability inspired by the galactic center and exoplanet capability of GRAVITY. We explain the modified data collection and reduction processes. We show how we leveraged the recent fringe tracker upgrade to implement features specific to its use with MATISSE, e.g. fringe jumps mitigation with an improved group delay control and simultaneous fringe tracking and chopping with a new state machine. Results: We successfully demonstrate significant improvements to the MATISSE instrument. Observations can now be performed at higher spectral resolutions of up to $R\\sim3300$ and across the full LM bands at once. Long detector integration times, made possible with stabilized fringes, have improved the LM-bands sensitivity by a factor of 10. Low flux biases in coherently-reduced N-band data have been eliminated. The L-band transfer function is now higher and more stable. We finally illustrate the scientific potential of GRA4MAT with a preview of the first exoplanet observation made by MATISSE on $\\beta$ Pictoris b.","sentences":["Context: MATISSE, the mid-infrared spectro-imaging instrument of VLTI, was designed to deliver its advertised performance when paired with an external second generation fringe tracker.","Science observation started in 2019, demonstrating imaging capabilities and faint science target observations.","Now, The GRAVITY fringe tracker stabilizes the MATISSE fringes which allows using all spectroscopic modes and improves sensitivity and data accuracy.","Aims:","We present how the MATISSE and GRAVITY instruments were adapted to make the GRAVITY fringe tracker work with MATISSE, under the umbrella of the aptly-named GRA4MAT project, led by ESO in collaboration with the two instrument consortia.","Methods: We detail the software modifications needed to implement an acquisition and observing sequence specific to GRA4MAT, including simultaneous fringe tracking and chopping and a narrow off-axis capability inspired by the galactic center and exoplanet capability of GRAVITY.","We explain the modified data collection and reduction processes.","We show how we leveraged the recent fringe tracker upgrade to implement features specific to its use with MATISSE, e.g. fringe jumps mitigation with an improved group delay control and simultaneous fringe tracking and chopping with a new state machine.","Results: We successfully demonstrate significant improvements to the MATISSE instrument.","Observations can now be performed at higher spectral resolutions of up to $R\\sim3300$ and across the full LM bands at once.","Long detector integration times, made possible with stabilized fringes, have improved the LM-bands sensitivity by a factor of 10.","Low flux biases in coherently-reduced N-band data have been eliminated.","The L-band transfer function is now higher and more stable.","We finally illustrate the scientific potential of GRA4MAT with a preview of the first exoplanet observation made by MATISSE on $\\beta$ Pictoris b."],"url":"http://arxiv.org/abs/2405.20730v1","category":"astro-ph.IM"}
{"created":"2024-05-31 09:37:39","title":"Extreme Point Supervised Instance Segmentation","abstract":"This paper introduces a novel approach to learning instance segmentation using extreme points, i.e., the topmost, leftmost, bottommost, and rightmost points, of each object. These points are readily available in the modern bounding box annotation process while offering strong clues for precise segmentation, and thus allows to improve performance at the same annotation cost with box-supervised methods. Our work considers extreme points as a part of the true instance mask and propagates them to identify potential foreground and background points, which are all together used for training a pseudo label generator. Then pseudo labels given by the generator are in turn used for supervised learning of our final model. On three public benchmarks, our method significantly outperforms existing box-supervised methods, further narrowing the gap with its fully supervised counterpart. In particular, our model generates high-quality masks when a target object is separated into multiple parts, where previous box-supervised methods often fail.","sentences":["This paper introduces a novel approach to learning instance segmentation using extreme points, i.e., the topmost, leftmost, bottommost, and rightmost points, of each object.","These points are readily available in the modern bounding box annotation process while offering strong clues for precise segmentation, and thus allows to improve performance at the same annotation cost with box-supervised methods.","Our work considers extreme points as a part of the true instance mask and propagates them to identify potential foreground and background points, which are all together used for training a pseudo label generator.","Then pseudo labels given by the generator are in turn used for supervised learning of our final model.","On three public benchmarks, our method significantly outperforms existing box-supervised methods, further narrowing the gap with its fully supervised counterpart.","In particular, our model generates high-quality masks when a target object is separated into multiple parts, where previous box-supervised methods often fail."],"url":"http://arxiv.org/abs/2405.20729v1","category":"cs.CV"}
{"created":"2024-05-31 09:33:16","title":"GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning","abstract":"With heightened awareness of data privacy protection, Federated Learning (FL) has attracted widespread attention as a privacy-preserving distributed machine learning method. However, the distributed nature of federated learning also provides opportunities for backdoor attacks, where attackers can guide the model to produce incorrect predictions without affecting the global model training process.   This paper introduces a novel defense mechanism against backdoor attacks in federated learning, named GANcrop. This approach leverages contrastive learning to deeply explore the disparities between malicious and benign models for attack identification, followed by the utilization of Generative Adversarial Networks (GAN) to recover backdoor triggers and implement targeted mitigation strategies. Experimental findings demonstrate that GANcrop effectively safeguards against backdoor attacks, particularly in non-IID scenarios, while maintaining satisfactory model accuracy, showcasing its remarkable defensive efficacy and practical utility.","sentences":["With heightened awareness of data privacy protection, Federated Learning (FL) has attracted widespread attention as a privacy-preserving distributed machine learning method.","However, the distributed nature of federated learning also provides opportunities for backdoor attacks, where attackers can guide the model to produce incorrect predictions without affecting the global model training process.   ","This paper introduces a novel defense mechanism against backdoor attacks in federated learning, named GANcrop.","This approach leverages contrastive learning to deeply explore the disparities between malicious and benign models for attack identification, followed by the utilization of Generative Adversarial Networks (GAN) to recover backdoor triggers and implement targeted mitigation strategies.","Experimental findings demonstrate that GANcrop effectively safeguards against backdoor attacks, particularly in non-IID scenarios, while maintaining satisfactory model accuracy, showcasing its remarkable defensive efficacy and practical utility."],"url":"http://arxiv.org/abs/2405.20727v1","category":"cs.CR"}
{"created":"2024-05-31 09:29:43","title":"GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search","abstract":"Gradient Inversion Attacks invert the transmitted gradients in Federated Learning (FL) systems to reconstruct the sensitive data of local clients and have raised considerable privacy concerns. A majority of gradient inversion methods rely heavily on explicit prior knowledge (e.g., a well pre-trained generative model), which is often unavailable in realistic scenarios. To alleviate this issue, researchers have proposed to leverage the implicit prior knowledge of an over-parameterized network. However, they only utilize a fixed neural architecture for all the attack settings. This would hinder the adaptive use of implicit architectural priors and consequently limit the generalizability. In this paper, we further exploit such implicit prior knowledge by proposing Gradient Inversion via Neural Architecture Search (GI-NAS), which adaptively searches the network and captures the implicit priors behind neural architectures. Extensive experiments verify that our proposed GI-NAS can achieve superior attack performance compared to state-of-the-art gradient inversion methods, even under more practical settings with high-resolution images, large-sized batches, and advanced defense strategies.","sentences":["Gradient Inversion Attacks invert the transmitted gradients in Federated Learning (FL) systems to reconstruct the sensitive data of local clients and have raised considerable privacy concerns.","A majority of gradient inversion methods rely heavily on explicit prior knowledge (e.g., a well pre-trained generative model), which is often unavailable in realistic scenarios.","To alleviate this issue, researchers have proposed to leverage the implicit prior knowledge of an over-parameterized network.","However, they only utilize a fixed neural architecture for all the attack settings.","This would hinder the adaptive use of implicit architectural priors and consequently limit the generalizability.","In this paper, we further exploit such implicit prior knowledge by proposing Gradient Inversion via Neural Architecture Search (GI-NAS), which adaptively searches the network and captures the implicit priors behind neural architectures.","Extensive experiments verify that our proposed GI-NAS can achieve superior attack performance compared to state-of-the-art gradient inversion methods, even under more practical settings with high-resolution images, large-sized batches, and advanced defense strategies."],"url":"http://arxiv.org/abs/2405.20725v1","category":"cs.AI"}
{"created":"2024-05-31 09:23:39","title":"ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model","abstract":"Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for novel view synthesis, offering fast rendering speeds and high fidelity. However, the large number of Gaussians and their associated attributes require effective compression techniques. Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence. Inspired by the effectiveness of the context model in image compression, we propose the first autoregressive model at the anchor level for 3DGS compression in this work. We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency. To further improve the efficiency of entropy coding, e.g., to code the coarsest level with no already coded anchors, we propose to introduce a low-dimensional quantized feature as the hyperprior for each anchor, which can be effectively compressed. Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for novel view synthesis, offering fast rendering speeds and high fidelity.","However, the large number of Gaussians and their associated attributes require effective compression techniques.","Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence.","Inspired by the effectiveness of the context model in image compression, we propose the first autoregressive model at the anchor level for 3DGS compression in this work.","We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency.","To further improve the efficiency of entropy coding, e.g., to code the coarsest level with no already coded anchors, we propose to introduce a low-dimensional quantized feature as the hyperprior for each anchor, which can be effectively compressed.","Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality."],"url":"http://arxiv.org/abs/2405.20721v1","category":"cs.CV"}
{"created":"2024-05-31 09:23:25","title":"Power of Cooperative Supervision: Multiple Teachers Framework for Enhanced 3D Semi-Supervised Object Detection","abstract":"To ensure safe urban driving for autonomous platforms, it is crucial not only to develop high-performance object detection techniques but also to establish a diverse and representative dataset that captures various urban environments and object characteristics. To address these two issues, we have constructed a multi-class 3D LiDAR dataset reflecting diverse urban environments and object characteristics, and developed a robust 3D semi-supervised object detection (SSOD) based on a multiple teachers framework. This SSOD framework categorizes similar classes and assigns specialized teachers to each category. Through collaborative supervision among these category-specialized teachers, the student network becomes increasingly proficient, leading to a highly effective object detector. We propose a simple yet effective augmentation technique, Pie-based Point Compensating Augmentation (PieAug), to enable the teacher network to generate high-quality pseudo-labels. Extensive experiments on the WOD, KITTI, and our datasets validate the effectiveness of our proposed method and the quality of our dataset. Experimental results demonstrate that our approach consistently outperforms existing state-of-the-art 3D semi-supervised object detection methods across all datasets. We plan to release our multi-class LiDAR dataset and the source code available on our Github repository in the near future.","sentences":["To ensure safe urban driving for autonomous platforms, it is crucial not only to develop high-performance object detection techniques but also to establish a diverse and representative dataset that captures various urban environments and object characteristics.","To address these two issues, we have constructed a multi-class 3D LiDAR","dataset reflecting diverse urban environments and object characteristics, and developed a robust 3D semi-supervised object detection (SSOD) based on a multiple teachers framework.","This SSOD framework categorizes similar classes and assigns specialized teachers to each category.","Through collaborative supervision among these category-specialized teachers, the student network becomes increasingly proficient, leading to a highly effective object detector.","We propose a simple yet effective augmentation technique, Pie-based Point Compensating Augmentation (PieAug), to enable the teacher network to generate high-quality pseudo-labels.","Extensive experiments on the WOD, KITTI, and our datasets validate the effectiveness of our proposed method and the quality of our dataset.","Experimental results demonstrate that our approach consistently outperforms existing state-of-the-art 3D semi-supervised object detection methods across all datasets.","We plan to release our multi-class LiDAR dataset and the source code available on our Github repository in the near future."],"url":"http://arxiv.org/abs/2405.20720v1","category":"cs.CV"}
{"created":"2024-05-31 09:20:33","title":"Climate Variable Downscaling with Conditional Normalizing Flows","abstract":"Predictions of global climate models typically operate on coarse spatial scales due to the large computational costs of climate simulations. This has led to a considerable interest in methods for statistical downscaling, a similar process to super-resolution in the computer vision context, to provide more local and regional climate information. In this work, we apply conditional normalizing flows to the task of climate variable downscaling. We showcase its successful performance on an ERA5 water content dataset for different upsampling factors. Additionally, we show that the method allows us to assess the predictive uncertainty in terms of standard deviation from the fitted conditional distribution mean.","sentences":["Predictions of global climate models typically operate on coarse spatial scales due to the large computational costs of climate simulations.","This has led to a considerable interest in methods for statistical downscaling, a similar process to super-resolution in the computer vision context, to provide more local and regional climate information.","In this work, we apply conditional normalizing flows to the task of climate variable downscaling.","We showcase its successful performance on an ERA5 water content dataset for different upsampling factors.","Additionally, we show that the method allows us to assess the predictive uncertainty in terms of standard deviation from the fitted conditional distribution mean."],"url":"http://arxiv.org/abs/2405.20719v1","category":"cs.AI"}
{"created":"2024-05-31 09:14:48","title":"Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias","abstract":"Collaborative Filtering (CF) typically suffers from the significant challenge of popularity bias due to the uneven distribution of items in real-world datasets. This bias leads to a significant accuracy gap between popular and unpopular items. It not only hinders accurate user preference understanding but also exacerbates the Matthew effect in recommendation systems. To alleviate popularity bias, existing efforts focus on emphasizing unpopular items or separating the correlation between item representations and their popularity. Despite the effectiveness, existing works still face two persistent challenges: (1) how to extract common supervision signals from popular items to improve the unpopular item representations, and (2) how to alleviate the representation separation caused by popularity bias. In this work, we conduct an empirical analysis of popularity bias and propose Popularity-Aware Alignment and Contrast (PAAC) to address two challenges. Specifically, we use the common supervisory signals modeled in popular item representations and propose a novel popularity-aware supervised alignment module to learn unpopular item representations. Additionally, we suggest re-weighting the contrastive learning loss to mitigate the representation separation from a popularity-centric perspective. Finally, we validate the effectiveness and rationale of PAAC in mitigating popularity bias through extensive experiments on three real-world datasets. Our code is available at https://github.com/miaomiao-cai2/KDD2024-PAAC.","sentences":["Collaborative Filtering (CF) typically suffers from the significant challenge of popularity bias due to the uneven distribution of items in real-world datasets.","This bias leads to a significant accuracy gap between popular and unpopular items.","It not only hinders accurate user preference understanding but also exacerbates the Matthew effect in recommendation systems.","To alleviate popularity bias, existing efforts focus on emphasizing unpopular items or separating the correlation between item representations and their popularity.","Despite the effectiveness, existing works still face two persistent challenges: (1) how to extract common supervision signals from popular items to improve the unpopular item representations, and (2) how to alleviate the representation separation caused by popularity bias.","In this work, we conduct an empirical analysis of popularity bias and propose Popularity-Aware Alignment and Contrast (PAAC) to address two challenges.","Specifically, we use the common supervisory signals modeled in popular item representations and propose a novel popularity-aware supervised alignment module to learn unpopular item representations.","Additionally, we suggest re-weighting the contrastive learning loss to mitigate the representation separation from a popularity-centric perspective.","Finally, we validate the effectiveness and rationale of PAAC in mitigating popularity bias through extensive experiments on three real-world datasets.","Our code is available at https://github.com/miaomiao-cai2/KDD2024-PAAC."],"url":"http://arxiv.org/abs/2405.20718v1","category":"cs.IR"}
{"created":"2024-05-31 09:14:36","title":"Cyclic image generation using chaotic dynamics","abstract":"Successive image generation using cyclic transformations is demonstrated by extending the CycleGAN model to transform images among three different categories. Repeated application of the trained generators produces sequences of images that transition among the different categories. The generated image sequences occupy a more limited region of the image space compared with the original training dataset. Quantitative evaluation using precision and recall metrics indicates that the generated images have high quality but reduced diversity relative to the training dataset. Such successive generation processes are characterized as chaotic dynamics in terms of dynamical system theory. Positive Lyapunov exponents estimated from the generated trajectories confirm the presence of chaotic dynamics, with the Lyapunov dimension of the attractor found to be comparable to the intrinsic dimension of the training data manifold. The results suggest that chaotic dynamics in the image space defined by the deep generative model contribute to the diversity of the generated images, constituting a novel approach for multi-class image generation. This model can be interpreted as an extension of classical associative memory to perform hetero-association among image categories.","sentences":["Successive image generation using cyclic transformations is demonstrated by extending the CycleGAN model to transform images among three different categories.","Repeated application of the trained generators produces sequences of images that transition among the different categories.","The generated image sequences occupy a more limited region of the image space compared with the original training dataset.","Quantitative evaluation using precision and recall metrics indicates that the generated images have high quality but reduced diversity relative to the training dataset.","Such successive generation processes are characterized as chaotic dynamics in terms of dynamical system theory.","Positive Lyapunov exponents estimated from the generated trajectories confirm the presence of chaotic dynamics, with the Lyapunov dimension of the attractor found to be comparable to the intrinsic dimension of the training data manifold.","The results suggest that chaotic dynamics in the image space defined by the deep generative model contribute to the diversity of the generated images, constituting a novel approach for multi-class image generation.","This model can be interpreted as an extension of classical associative memory to perform hetero-association among image categories."],"url":"http://arxiv.org/abs/2405.20717v1","category":"cs.CV"}
{"created":"2024-05-31 09:07:44","title":"Fast Evaluation of S-boxes with Garbled Circuits","abstract":"Garbling schemes are vital primitives for privacy-preserving protocols and secure two-party computation. This paper presents a projective garbling scheme that assigns $2^n$ values to wires in a circuit comprising XOR and unary projection gates. A generalization of FreeXOR allows the XOR of wires with $2^n$ values to be very efficient. We then analyze the performance of our scheme by evaluating substitution-permutation ciphers. Using our proposal, we measure high-speed evaluation of the ciphers with a moderately increased cost in garbling and bandwidth. Theoretical analysis suggests that for evaluating the nine examined ciphers, one can expect a 4- to 70-fold improvement in evaluation performance with, at most, a 4-fold increase in garbling cost and, at most, an 8-fold increase in communication cost compared to the Half-Gates (Zahur, Rosulek and Evans; Eurocrypt'15) and ThreeHalves (Rosulek and Roy; Crypto'21) garbling schemes. In an offline/online setting, such as secure function evaluation as a service, the circuit garbling and communication to the evaluator can proceed in the offline phase. Thus, our scheme offers a fast online phase. Furthermore, we present efficient Boolean circuits for the S-boxes of TWINE and Midori64 ciphers. To our knowledge, our formulas give the smallest number of AND gates for the S-boxes of these two ciphers.","sentences":["Garbling schemes are vital primitives for privacy-preserving protocols and secure two-party computation.","This paper presents a projective garbling scheme that assigns $2^n$ values to wires in a circuit comprising XOR and unary projection gates.","A generalization of FreeXOR allows the XOR of wires with $2^n$ values to be very efficient.","We then analyze the performance of our scheme by evaluating substitution-permutation ciphers.","Using our proposal, we measure high-speed evaluation of the ciphers with a moderately increased cost in garbling and bandwidth.","Theoretical analysis suggests that for evaluating the nine examined ciphers, one can expect a 4- to 70-fold improvement in evaluation performance with, at most, a 4-fold increase in garbling cost and, at most, an 8-fold increase in communication cost compared to the Half-Gates (Zahur, Rosulek and Evans; Eurocrypt'15) and ThreeHalves (Rosulek and Roy; Crypto'21) garbling schemes.","In an offline/online setting, such as secure function evaluation as a service, the circuit garbling and communication to the evaluator can proceed in the offline phase.","Thus, our scheme offers a fast online phase.","Furthermore, we present efficient Boolean circuits for the S-boxes of TWINE and Midori64 ciphers.","To our knowledge, our formulas give the smallest number of AND gates for the S-boxes of these two ciphers."],"url":"http://arxiv.org/abs/2405.20713v1","category":"cs.CR"}
{"created":"2024-05-31 09:07:15","title":"Revisiting Mutual Information Maximization for Generalized Category Discovery","abstract":"Generalized category discovery presents a challenge in a realistic scenario, which requires the model's generalization ability to recognize unlabeled samples from known and unknown categories. This paper revisits the challenge of generalized category discovery through the lens of information maximization (InfoMax) with a probabilistic parametric classifier. Our findings reveal that ensuring independence between known and unknown classes while concurrently assuming a uniform probability distribution across all classes, yields an enlarged margin among known and unknown classes that promotes the model's performance. To achieve the aforementioned independence, we propose a novel InfoMax-based method, Regularized Parametric InfoMax (RPIM), which adopts pseudo labels to supervise unlabeled samples during InfoMax, while proposing a regularization to ensure the quality of the pseudo labels. Additionally, we introduce novel semantic-bias transformation to refine the features from the pre-trained model instead of direct fine-tuning to rescue the computational costs. Extensive experiments on six benchmark datasets validate the effectiveness of our method. RPIM significantly improves the performance regarding unknown classes, surpassing the state-of-the-art method by an average margin of 3.5%.","sentences":["Generalized category discovery presents a challenge in a realistic scenario, which requires the model's generalization ability to recognize unlabeled samples from known and unknown categories.","This paper revisits the challenge of generalized category discovery through the lens of information maximization (InfoMax) with a probabilistic parametric classifier.","Our findings reveal that ensuring independence between known and unknown classes while concurrently assuming a uniform probability distribution across all classes, yields an enlarged margin among known and unknown classes that promotes the model's performance.","To achieve the aforementioned independence, we propose a novel InfoMax-based method, Regularized Parametric InfoMax (RPIM), which adopts pseudo labels to supervise unlabeled samples during InfoMax, while proposing a regularization to ensure the quality of the pseudo labels.","Additionally, we introduce novel semantic-bias transformation to refine the features from the pre-trained model instead of direct fine-tuning to rescue the computational costs.","Extensive experiments on six benchmark datasets validate the effectiveness of our method.","RPIM significantly improves the performance regarding unknown classes, surpassing the state-of-the-art method by an average margin of 3.5%."],"url":"http://arxiv.org/abs/2405.20711v1","category":"cs.CV"}
{"created":"2024-05-31 09:07:03","title":"Information Maximization via Variational Autoencoders for Cross-Domain Recommendation","abstract":"Cross-Domain Sequential Recommendation (CDSR) methods aim to address the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR). Existing CDSR methods typically rely on overlapping users, designing complex cross-domain modules to capture users' latent interests that can propagate across different domains. However, their propagated informative information is limited to the overlapping users and the users who have rich historical behavior records. As a result, these methods often underperform in real-world scenarios, where most users are non-overlapping (cold-start) and long-tailed. In this research, we introduce a new CDSR framework named Information Maximization Variational Autoencoder (\\textbf{\\texttt{IM-VAE}}). Here, we suggest using a Pseudo-Sequence Generator to enhance the user's interaction history input for downstream fine-grained CDSR models to alleviate the cold-start issues. We also propose a Generative Recommendation Framework combined with three regularizers inspired by the mutual information maximization (MIM) theory \\cite{mcgill1954multivariate} to capture the semantic differences between a user's interests shared across domains and those specific to certain domains, as well as address the informational gap between a user's actual interaction sequences and the pseudo-sequences generated. To the best of our knowledge, this paper is the first CDSR work that considers the information disentanglement and denoising of pseudo-sequences in the open-world recommendation scenario. Empirical experiments illustrate that \\texttt{IM-VAE} outperforms the state-of-the-art approaches on two real-world cross-domain datasets on all sorts of users, including cold-start and tailed users, demonstrating the effectiveness of \\texttt{IM-VAE} in open-world recommendation.","sentences":["Cross-Domain Sequential Recommendation (CDSR) methods aim to address the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR).","Existing CDSR methods typically rely on overlapping users, designing complex cross-domain modules to capture users' latent interests that can propagate across different domains.","However, their propagated informative information is limited to the overlapping users and the users who have rich historical behavior records.","As a result, these methods often underperform in real-world scenarios, where most users are non-overlapping (cold-start) and long-tailed.","In this research, we introduce a new CDSR framework named Information Maximization Variational Autoencoder (\\textbf{\\texttt{IM-VAE}}).","Here, we suggest using a Pseudo-Sequence Generator to enhance the user's interaction history input for downstream fine-grained CDSR models to alleviate the cold-start issues.","We also propose a Generative Recommendation Framework combined with three regularizers inspired by the mutual information maximization (MIM) theory \\cite{mcgill1954multivariate} to capture the semantic differences between a user's interests shared across domains and those specific to certain domains, as well as address the informational gap between a user's actual interaction sequences and the pseudo-sequences generated.","To the best of our knowledge, this paper is the first CDSR work that considers the information disentanglement and denoising of pseudo-sequences in the open-world recommendation scenario.","Empirical experiments illustrate that \\texttt{IM-VAE} outperforms the state-of-the-art approaches on two real-world cross-domain datasets on all sorts of users, including cold-start and tailed users, demonstrating the effectiveness of \\texttt{IM-VAE} in open-world recommendation."],"url":"http://arxiv.org/abs/2405.20710v1","category":"cs.IR"}
{"created":"2024-05-31 09:00:43","title":"FinGen: A Dataset for Argument Generation in Finance","abstract":"Thinking about the future is one of the important activities that people do in daily life. Futurists also pay a lot of effort into figuring out possible scenarios for the future. We argue that the exploration of this direction is still in an early stage in the NLP research. To this end, we propose three argument generation tasks in the financial application scenario. Our experimental results show these tasks are still big challenges for representative generation models. Based on our empirical results, we further point out several unresolved issues and challenges in this research direction.","sentences":["Thinking about the future is one of the important activities that people do in daily life.","Futurists also pay a lot of effort into figuring out possible scenarios for the future.","We argue that the exploration of this direction is still in an early stage in the NLP research.","To this end, we propose three argument generation tasks in the financial application scenario.","Our experimental results show these tasks are still big challenges for representative generation models.","Based on our empirical results, we further point out several unresolved issues and challenges in this research direction."],"url":"http://arxiv.org/abs/2405.20708v1","category":"cs.CL"}
{"created":"2024-05-31 08:59:20","title":"ADESSE: Advice Explanations in Complex Repeated Decision-Making Environments","abstract":"In the evolving landscape of human-centered AI, fostering a synergistic relationship between humans and AI agents in decision-making processes stands as a paramount challenge. This work considers a problem setup where an intelligent agent comprising a neural network-based prediction component and a deep reinforcement learning component provides advice to a human decision-maker in complex repeated decision-making environments. Whether the human decision-maker would follow the agent's advice depends on their beliefs and trust in the agent and on their understanding of the advice itself. To this end, we developed an approach named ADESSE to generate explanations about the adviser agent to improve human trust and decision-making. Computational experiments on a range of environments with varying model sizes demonstrate the applicability and scalability of ADESSE. Furthermore, an interactive game-based user study shows that participants were significantly more satisfied, achieved a higher reward in the game, and took less time to select an action when presented with explanations generated by ADESSE. These findings illuminate the critical role of tailored, human-centered explanations in AI-assisted decision-making.","sentences":["In the evolving landscape of human-centered AI, fostering a synergistic relationship between humans and AI agents in decision-making processes stands as a paramount challenge.","This work considers a problem setup where an intelligent agent comprising a neural network-based prediction component and a deep reinforcement learning component provides advice to a human decision-maker in complex repeated decision-making environments.","Whether the human decision-maker would follow the agent's advice depends on their beliefs and trust in the agent and on their understanding of the advice itself.","To this end, we developed an approach named ADESSE to generate explanations about the adviser agent to improve human trust and decision-making.","Computational experiments on a range of environments with varying model sizes demonstrate the applicability and scalability of ADESSE.","Furthermore, an interactive game-based user study shows that participants were significantly more satisfied, achieved a higher reward in the game, and took less time to select an action when presented with explanations generated by ADESSE.","These findings illuminate the critical role of tailored, human-centered explanations in AI-assisted decision-making."],"url":"http://arxiv.org/abs/2405.20705v1","category":"cs.AI"}
{"created":"2024-05-31 08:57:09","title":"It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance","abstract":"Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from textual data about specific entities and their corresponding aspects through various complementary subtasks. Several prior research has focused on developing ad hoc designs of varying complexities for these subtasks. In this paper, we present a generative framework extensible to any ABSA subtask. We build upon the instruction tuned model proposed by Scaria et al. (2023), who present an instruction-based model with task descriptions followed by in-context examples on ABSA subtasks. We propose PFInstruct, an extension to this instruction learning paradigm by appending an NLP-related task prefix to the task description. This simple approach leads to improved performance across all tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the ATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average of +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact of the prefix-enhanced prompt quality on the ABSA subtasks and find that even a noisy prefix enhances model performance compared to the baseline. Our method also achieves competitive results on a biomedical domain dataset (ERSA).","sentences":["Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from textual data about specific entities and their corresponding aspects through various complementary subtasks.","Several prior research has focused on developing ad hoc designs of varying complexities for these subtasks.","In this paper, we present a generative framework extensible to any ABSA subtask.","We build upon the instruction tuned model proposed by Scaria et al. (2023), who present an instruction-based model with task descriptions followed by in-context examples on ABSA subtasks.","We propose PFInstruct, an extension to this instruction learning paradigm by appending an NLP-related task prefix to the task description.","This simple approach leads to improved performance across all tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the ATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average of +5.43 F1-score across SemEval datasets.","Furthermore, we explore the impact of the prefix-enhanced prompt quality on the ABSA subtasks and find that even a noisy prefix enhances model performance compared to the baseline.","Our method also achieves competitive results on a biomedical domain dataset (ERSA)."],"url":"http://arxiv.org/abs/2405.20703v1","category":"cs.CL"}
{"created":"2024-05-31 08:53:59","title":"Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement","abstract":"Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.","sentences":["Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks.","Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions.","In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans.","By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different.","Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).","COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence.","Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks."],"url":"http://arxiv.org/abs/2405.20701v1","category":"cs.CL"}
{"created":"2024-05-31 08:51:57","title":"Self-degraded contrastive domain adaptation for industrial fault diagnosis with bi-imbalanced data","abstract":"Modern industrial fault diagnosis tasks often face the combined challenge of distribution discrepancy and bi-imbalance. Existing domain adaptation approaches pay little attention to the prevailing bi-imbalance, leading to poor domain adaptation performance or even negative transfer. In this work, we propose a self-degraded contrastive domain adaptation (Sd-CDA) diagnosis framework to handle the domain discrepancy under the bi-imbalanced data. It first pre-trains the feature extractor via imbalance-aware contrastive learning based on model pruning to learn the feature representation efficiently in a self-supervised manner. Then it forces the samples away from the domain boundary based on supervised contrastive domain adversarial learning (SupCon-DA) and ensures the features generated by the feature extractor are discriminative enough. Furthermore, we propose the pruned contrastive domain adversarial learning (PSupCon-DA) to pay automatically re-weighted attention to the minorities to enhance the performance towards bi-imbalanced data. We show the superiority of the proposed method via two experiments.","sentences":["Modern industrial fault diagnosis tasks often face the combined challenge of distribution discrepancy and bi-imbalance.","Existing domain adaptation approaches pay little attention to the prevailing bi-imbalance, leading to poor domain adaptation performance or even negative transfer.","In this work, we propose a self-degraded contrastive domain adaptation (Sd-CDA) diagnosis framework to handle the domain discrepancy under the bi-imbalanced data.","It first pre-trains the feature extractor via imbalance-aware contrastive learning based on model pruning to learn the feature representation efficiently in a self-supervised manner.","Then it forces the samples away from the domain boundary based on supervised contrastive domain adversarial learning (SupCon-DA) and ensures the features generated by the feature extractor are discriminative enough.","Furthermore, we propose the pruned contrastive domain adversarial learning (PSupCon-DA) to pay automatically re-weighted attention to the minorities to enhance the performance towards bi-imbalanced data.","We show the superiority of the proposed method via two experiments."],"url":"http://arxiv.org/abs/2405.20700v1","category":"cs.AI"}
{"created":"2024-05-31 08:47:35","title":"Isospin sum rules for the nonleptonic $B$ decays","abstract":"Isospin symmetry, as the most precise flavor symmetry, could be used to extract information of hadronic dynamics. The effective Hamiltonian of bottom quark weak decay is zero under the isospin lowering operators $I_-^n$, which permits us to generate isospin sum rules through several master formulas. In this work, we derive the master formulas of isospin sum rules for the two- and three-body non-leptonic decays of $B$ mesons. The isospin sum rules can be used to test of isospin symmetry and provide hints the isospin partners of exotic hadrons in the $B$ decays. It is found the isospin breaking could reach to be $\\mathcal{O}(10\\%)$ in some decay modes. And the charm tetraquark resonances might be observed in the $ B^- \\to J/\\Psi \\pi^-\\overline K^0$, $\\overline B^0 \\to J/\\Psi \\overline K^0\\phi$ and $\\overline B^0\\to D^0\\overline D^0\\overline K^0$ modes according to the isospin symmetry.","sentences":["Isospin symmetry, as the most precise flavor symmetry, could be used to extract information of hadronic dynamics.","The effective Hamiltonian of bottom quark weak decay is zero under the isospin lowering operators $I_-^n$, which permits us to generate isospin sum rules through several master formulas.","In this work, we derive the master formulas of isospin sum rules for the two- and three-body non-leptonic decays of $B$ mesons.","The isospin sum rules can be used to test of isospin symmetry and provide hints the isospin partners of exotic hadrons in the $B$ decays.","It is found the isospin breaking could reach to be $\\mathcal{O}(10\\%)$ in some decay modes.","And the charm tetraquark resonances might be observed in the $ B^- \\to J/\\Psi \\pi^-\\overline K^0$, $\\overline B^0 \\to J/\\Psi \\overline K^0\\phi$ and $\\overline B^0\\to D^0\\overline D^0\\overline K^0$ modes according to the isospin symmetry."],"url":"http://arxiv.org/abs/2405.20698v1","category":"hep-ph"}
{"created":"2024-05-31 08:38:25","title":"In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought","abstract":"In-context learning is a promising approach for offline reinforcement learning (RL) to handle online tasks, which can be achieved by providing task prompts. Recent works demonstrated that in-context RL could emerge with self-improvement in a trial-and-error manner when treating RL tasks as an across-episodic sequential prediction problem. Despite the self-improvement not requiring gradient updates, current works still suffer from high computational costs when the across-episodic sequence increases with task horizons. To this end, we propose an In-context Decision Transformer (IDT) to achieve self-improvement in a high-level trial-and-error manner. Specifically, IDT is inspired by the efficient hierarchical structure of human decision-making and thus reconstructs the sequence to consist of high-level decisions instead of low-level actions that interact with environments. As one high-level decision can guide multi-step low-level actions, IDT naturally avoids excessively long sequences and solves online tasks more efficiently. Experimental results show that IDT achieves state-of-the-art in long-horizon tasks over current in-context RL methods. In particular, the online evaluation time of our IDT is \\textbf{36$\\times$} times faster than baselines in the D4RL benchmark and \\textbf{27$\\times$} times faster in the Grid World benchmark.","sentences":["In-context learning is a promising approach for offline reinforcement learning (RL) to handle online tasks, which can be achieved by providing task prompts.","Recent works demonstrated that in-context RL could emerge with self-improvement in a trial-and-error manner when treating RL tasks as an across-episodic sequential prediction problem.","Despite the self-improvement not requiring gradient updates, current works still suffer from high computational costs when the across-episodic sequence increases with task horizons.","To this end, we propose an In-context Decision Transformer (IDT) to achieve self-improvement in a high-level trial-and-error manner.","Specifically, IDT is inspired by the efficient hierarchical structure of human decision-making and thus reconstructs the sequence to consist of high-level decisions instead of low-level actions that interact with environments.","As one high-level decision can guide multi-step low-level actions, IDT naturally avoids excessively long sequences and solves online tasks more efficiently.","Experimental results show that IDT achieves state-of-the-art in long-horizon tasks over current in-context RL methods.","In particular, the online evaluation time of our IDT is \\textbf{36$\\times$} times faster than baselines in the D4RL benchmark and \\textbf{27$\\times$} times faster in the Grid World benchmark."],"url":"http://arxiv.org/abs/2405.20692v1","category":"cs.LG"}
{"created":"2024-05-31 08:38:18","title":"Exploring run-and-tumble movement in confined settings through simulation","abstract":"Motion in bounded domains is a fundamental concept in various fields, including billiard dynamics and random walks on finite lattices, with important applications in physics, ecology and biology. An important universal property related to the average return time to the boundary, the Mean Path Length Theorem (MPLT), has been proposed theoretically and confirmed experimentally in various contexts. In this discussion, we investigate a wide range of mechanisms that lead to deviations from this universal behavior, such as boundary effects, reorientation and memory processes. In particular, this study investigates the dynamics of run-and-tumble particles within a confined two-dimensional circular domain. Through a combination of theoretical approaches and numerical simulations, we validate the MPLT under uniform and isotropic particle inflow conditions. The research demonstrates that although the MPLT is generally applicable for different step length distributions, deviations occur for non-uniform angular distributions, non-elastic boundary conditions or memory processes. These results underline the crucial influence of boundary interactions and angular dynamics on the behaviour of particles in confined spaces. Our results provide new insights into the geometry and dynamics of motion in confined spaces and contribute to a better understanding of a broad spectrum of phenomena ranging from the motion of bacteria to neutron transport. This type of analysis is crucial in situations where inhomogeneity occurs, such as multiple real-world scenarios within a limited domain. This bridges the gap between theoretical models and practical applications in biological and physical systems since the study of the statistics of movement in confined settings can bring some light in explaining the mobility mechanism of active agents.","sentences":["Motion in bounded domains is a fundamental concept in various fields, including billiard dynamics and random walks on finite lattices, with important applications in physics, ecology and biology.","An important universal property related to the average return time to the boundary, the Mean Path Length Theorem (MPLT), has been proposed theoretically and confirmed experimentally in various contexts.","In this discussion, we investigate a wide range of mechanisms that lead to deviations from this universal behavior, such as boundary effects, reorientation and memory processes.","In particular, this study investigates the dynamics of run-and-tumble particles within a confined two-dimensional circular domain.","Through a combination of theoretical approaches and numerical simulations, we validate the MPLT under uniform and isotropic particle inflow conditions.","The research demonstrates that although the MPLT is generally applicable for different step length distributions, deviations occur for non-uniform angular distributions, non-elastic boundary conditions or memory processes.","These results underline the crucial influence of boundary interactions and angular dynamics on the behaviour of particles in confined spaces.","Our results provide new insights into the geometry and dynamics of motion in confined spaces and contribute to a better understanding of a broad spectrum of phenomena ranging from the motion of bacteria to neutron transport.","This type of analysis is crucial in situations where inhomogeneity occurs, such as multiple real-world scenarios within a limited domain.","This bridges the gap between theoretical models and practical applications in biological and physical systems since the study of the statistics of movement in confined settings can bring some light in explaining the mobility mechanism of active agents."],"url":"http://arxiv.org/abs/2405.20691v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-31 08:31:26","title":"Conditioning GAN Without Training Dataset","abstract":"Deep learning algorithms have a large number of trainable parameters often with sizes of hundreds of thousands or more. Training this algorithm requires a large amount of training data and generating a sufficiently large dataset for these algorithms is costly\\cite{noguchi2019image}.   GANs are generative neural networks that use two deep learning networks that are competing with each other. The networks are generator and discriminator networks. The generator tries to generate realistic images which resemble the actual training dataset by approximating the training data distribution and the discriminator is trained to classify images as real or fake(generated)\\cite{goodfellow2016nips}. Training these GAN algorithms also requires a large amount of training dataset\\cite{noguchi2019image}.   In this study, the aim is to address the question, \"Given an unconditioned pretrained generator network and a pretrained classifier, is it feasible to develop a conditioned generator without relying on any training dataset?\"   The paper begins with a general introduction to the problem. The subsequent sections are structured as follows: Section 2 provides background information on the problem. Section 3 reviews relevant literature on the topic. Section 4 outlines the methodology employed in this study. Section 5 presents the experimental results. Section 6 discusses the findings and proposes potential future research directions. Finally, Section 7 offers concluding remarks.   The implementation can be accessed \\href{https://github.com/kidist-amde/BigGAN-PyTorch}{here}.","sentences":["Deep learning algorithms have a large number of trainable parameters often with sizes of hundreds of thousands or more.","Training this algorithm requires a large amount of training data and generating a sufficiently large dataset for these algorithms is costly\\cite{noguchi2019image}.   ","GANs are generative neural networks that use two deep learning networks that are competing with each other.","The networks are generator and discriminator networks.","The generator tries to generate realistic images which resemble the actual training dataset by approximating the training data distribution and the discriminator is trained to classify images as real or fake(generated)\\cite{goodfellow2016nips}.","Training these GAN algorithms also requires a large amount of training dataset\\cite{noguchi2019image}.   ","In this study, the aim is to address the question, \"Given an unconditioned pretrained generator network and a pretrained classifier, is it feasible to develop a conditioned generator without relying on any training dataset?\"   ","The paper begins with a general introduction to the problem.","The subsequent sections are structured as follows: Section 2 provides background information on the problem.","Section 3 reviews relevant literature on the topic.","Section 4 outlines the methodology employed in this study.","Section 5 presents the experimental results.","Section 6 discusses the findings and proposes potential future research directions.","Finally, Section 7 offers concluding remarks.   ","The implementation can be accessed \\href{https://github.com/kidist-amde/BigGAN-PyTorch}{here}."],"url":"http://arxiv.org/abs/2405.20687v1","category":"cs.CV"}
{"created":"2024-05-31 08:26:53","title":"Enhancing Counterfactual Image Generation Using Mahalanobis Distance with Distribution Preferences in Feature Space","abstract":"In the realm of Artificial Intelligence (AI), the importance of Explainable Artificial Intelligence (XAI) is increasingly recognized, particularly as AI models become more integral to our lives. One notable single-instance XAI approach is counterfactual explanation, which aids users in comprehending a model's decisions and offers guidance on altering these decisions. Specifically in the context of image classification models, effective image counterfactual explanations can significantly enhance user understanding. This paper introduces a novel method for computing feature importance within the feature space of a black-box model. By employing information fusion techniques, our method maximizes the use of data to address feature counterfactual explanations in the feature space. Subsequently, we utilize an image generation model to transform these feature counterfactual explanations into image counterfactual explanations. Our experiments demonstrate that the counterfactual explanations generated by our method closely resemble the original images in both pixel and feature spaces. Additionally, our method outperforms established baselines, achieving impressive experimental results.","sentences":["In the realm of Artificial Intelligence (AI), the importance of Explainable Artificial Intelligence (XAI) is increasingly recognized, particularly as AI models become more integral to our lives.","One notable single-instance XAI approach is counterfactual explanation, which aids users in comprehending a model's decisions and offers guidance on altering these decisions.","Specifically in the context of image classification models, effective image counterfactual explanations can significantly enhance user understanding.","This paper introduces a novel method for computing feature importance within the feature space of a black-box model.","By employing information fusion techniques, our method maximizes the use of data to address feature counterfactual explanations in the feature space.","Subsequently, we utilize an image generation model to transform these feature counterfactual explanations into image counterfactual explanations.","Our experiments demonstrate that the counterfactual explanations generated by our method closely resemble the original images in both pixel and feature spaces.","Additionally, our method outperforms established baselines, achieving impressive experimental results."],"url":"http://arxiv.org/abs/2405.20685v1","category":"cs.LG"}
{"created":"2024-05-31 08:26:47","title":"Joint Embeddings for Graph Instruction Tuning","abstract":"Large Language Models (LLMs) have achieved impressive performance in text understanding and have become an essential tool for building smart assistants. Originally focusing on text, they have been enhanced with multimodal capabilities in recent works that successfully built visual instruction following assistants. As far as the graph modality goes, however, no such assistants have yet been developed. Graph structures are complex in that they represent relation between different features and are permutation invariant. Moreover, representing them in purely textual form does not always lead to good LLM performance even for finetuned models. As a result, there is a need to develop a new method to integrate graphs in LLMs for general graph understanding. This work explores the integration of the graph modality in LLM for general graph instruction following tasks. It aims at producing a deep learning model that enhances an underlying LLM with graph embeddings and trains it to understand them and to produce, given an instruction, an answer grounded in the graph representation. The approach performs significantly better than a graph to text approach and remains consistent even for larger graphs.","sentences":["Large Language Models (LLMs) have achieved impressive performance in text understanding and have become an essential tool for building smart assistants.","Originally focusing on text, they have been enhanced with multimodal capabilities in recent works that successfully built visual instruction following assistants.","As far as the graph modality goes, however, no such assistants have yet been developed.","Graph structures are complex in that they represent relation between different features and are permutation invariant.","Moreover, representing them in purely textual form does not always lead to good LLM performance even for finetuned models.","As a result, there is a need to develop a new method to integrate graphs in LLMs for general graph understanding.","This work explores the integration of the graph modality in LLM for general graph instruction following tasks.","It aims at producing a deep learning model that enhances an underlying LLM with graph embeddings and trains it to understand them and to produce, given an instruction, an answer grounded in the graph representation.","The approach performs significantly better than a graph to text approach and remains consistent even for larger graphs."],"url":"http://arxiv.org/abs/2405.20684v1","category":"cs.SE"}
{"created":"2024-05-31 08:25:02","title":"Cosmological Constant from Equivalent Transformation in Quantum Cosmology","abstract":"We explore the introduction of the cosmological constant via equivalent transformations in cosmology. We consider the Wheeler-DeWitt equation for the CDM universe and we construct the Hamilton-Jacobi action for the $\\Lambda$CDM model. We discuss how this approach allows us to relate different physical systems, providing insights into the role of the cosmological constant in cosmology.","sentences":["We explore the introduction of the cosmological constant via equivalent transformations in cosmology.","We consider the Wheeler-DeWitt equation for the CDM universe and we construct the Hamilton-Jacobi action for the $\\Lambda$CDM model.","We discuss how this approach allows us to relate different physical systems, providing insights into the role of the cosmological constant in cosmology."],"url":"http://arxiv.org/abs/2405.20683v1","category":"gr-qc"}
{"created":"2024-05-31 08:22:53","title":"No Free Lunch Theorem for Privacy-Preserving LLM Inference","abstract":"Individuals and businesses have been significantly benefited by Large Language Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For example, LLMs enhance productivity, reduce costs, and enable us to focus on more valuable tasks. Furthermore, LLMs possess the capacity to sift through extensive datasets, uncover underlying patterns, and furnish critical insights that propel the frontiers of technology and science. However, LLMs also pose privacy concerns. Users' interactions with LLMs may expose their sensitive personal or company information. A lack of robust privacy safeguards and legal frameworks could permit the unwarranted intrusion or improper handling of individual data, thereby risking infringements of privacy and the theft of personal identities. To ensure privacy, it is essential to minimize the dependency between shared prompts and private information. Various randomization approaches have been proposed to protect prompts' privacy, but they may incur utility loss compared to unprotected LLMs prompting. Therefore, it is essential to evaluate the balance between the risk of privacy leakage and loss of utility when conducting effective protection mechanisms. The current study develops a framework for inferring privacy-protected Large Language Models (LLMs) and lays down a solid theoretical basis for examining the interplay between privacy preservation and utility. The core insight is encapsulated within a theorem that is called as the NFL (abbreviation of the word No-Free-Lunch) Theorem.","sentences":["Individuals and businesses have been significantly benefited by Large Language Models (LLMs) including PaLM, Gemini and ChatGPT in various ways.","For example, LLMs enhance productivity, reduce costs, and enable us to focus on more valuable tasks.","Furthermore, LLMs possess the capacity to sift through extensive datasets, uncover underlying patterns, and furnish critical insights that propel the frontiers of technology and science.","However, LLMs also pose privacy concerns.","Users' interactions with LLMs may expose their sensitive personal or company information.","A lack of robust privacy safeguards and legal frameworks could permit the unwarranted intrusion or improper handling of individual data, thereby risking infringements of privacy and the theft of personal identities.","To ensure privacy, it is essential to minimize the dependency between shared prompts and private information.","Various randomization approaches have been proposed to protect prompts' privacy, but they may incur utility loss compared to unprotected LLMs prompting.","Therefore, it is essential to evaluate the balance between the risk of privacy leakage and loss of utility when conducting effective protection mechanisms.","The current study develops a framework for inferring privacy-protected Large Language Models (LLMs) and lays down a solid theoretical basis for examining the interplay between privacy preservation and utility.","The core insight is encapsulated within a theorem that is called as the NFL (abbreviation of the word No-Free-Lunch) Theorem."],"url":"http://arxiv.org/abs/2405.20681v1","category":"cs.CR"}
{"created":"2024-05-31 08:22:49","title":"Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models","abstract":"Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.","sentences":["Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs).","Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers.","To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories.","Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency.","Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors.","Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors."],"url":"http://arxiv.org/abs/2405.20680v1","category":"cs.AI"}
{"created":"2024-05-31 08:21:09","title":"Provably Efficient Interactive-Grounded Learning with Personalized Reward","abstract":"Interactive-Grounded Learning (IGL) [Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions. To deal with personalized rewards that are ubiquitous in applications such as recommendation systems, Maghakian et al. [2022] study a version of IGL with context-dependent feedback, but their algorithm does not come with theoretical guarantees. In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability. Our analysis reveals that the step-function estimator of prior work can deviate uncontrollably due to finite-sample effects. Our solution is a novel Lipschitz reward estimator which underestimates the true reward and enjoys favorable generalization performances. Building on this estimator, we propose two algorithms, one based on explore-then-exploit and the other based on inverse-gap weighting. We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice. Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms.","sentences":["Interactive-Grounded Learning (IGL)","[Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions.","To deal with personalized rewards that are ubiquitous in applications such as recommendation systems, Maghakian et al.","[2022] study a version of IGL with context-dependent feedback, but their algorithm does not come with theoretical guarantees.","In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability.","Our analysis reveals that the step-function estimator of prior work can deviate uncontrollably due to finite-sample effects.","Our solution is a novel Lipschitz reward estimator which underestimates the true reward and enjoys favorable generalization performances.","Building on this estimator, we propose two algorithms, one based on explore-then-exploit and the other based on inverse-gap weighting.","We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice.","Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms."],"url":"http://arxiv.org/abs/2405.20677v1","category":"cs.LG"}
{"created":"2024-05-31 17:46:57","title":"Factors Influencing Performance of Students in Software Automated Test Tools Course","abstract":"Formal software testing education is important for building efficient QA professionals. Various aspects of quality assurance approaches are usually covered in courses for training software testing students. Automated Test Tools is one of the core courses in the software testing post-graduate curriculum due to the high demand for automated testers in the workforce. It is important to understand which factors are affecting student performance in the automated testing course to be able to assist the students early on based on their needs. Various metrics that are considered for predicting student performance in this testing course are student engagement, grades on individual deliverables, and prerequisite courses. This study identifies the impact of assessing students based on individual vs. group activities, theoretical vs. practical components, and the effect of having taken prerequisite courses in their final grade. To carry out this research, student data was collected from the automated test tools course of a community college-based postgraduate certificate program in software testing. The dataset contained student records from the years 2021 to 2022 and consisted of information from five different semesters. Various machine learning algorithms were applied to develop an effective model for predicting students performance in the automated software testing tools course, and finally, important features affecting the students performance were identified. The predictive performance model of the automated test tools course that was developed by applying the logistic regression technique, showed the best performance, with an accuracy score of 90%.","sentences":["Formal software testing education is important for building efficient QA professionals.","Various aspects of quality assurance approaches are usually covered in courses for training software testing students.","Automated Test Tools is one of the core courses in the software testing post-graduate curriculum due to the high demand for automated testers in the workforce.","It is important to understand which factors are affecting student performance in the automated testing course to be able to assist the students early on based on their needs.","Various metrics that are considered for predicting student performance in this testing course are student engagement, grades on individual deliverables, and prerequisite courses.","This study identifies the impact of assessing students based on individual vs. group activities, theoretical vs. practical components, and the effect of having taken prerequisite courses in their final grade.","To carry out this research, student data was collected from the automated test tools course of a community college-based postgraduate certificate program in software testing.","The dataset contained student records from the years 2021 to 2022 and consisted of information from five different semesters.","Various machine learning algorithms were applied to develop an effective model for predicting students performance in the automated software testing tools course, and finally, important features affecting the students performance were identified.","The predictive performance model of the automated test tools course that was developed by applying the logistic regression technique, showed the best performance, with an accuracy score of 90%."],"url":"http://arxiv.org/abs/2405.21055v1","category":"cs.SE"}
{"created":"2024-05-31 17:46:10","title":"The First Billion Years, According to JWST","abstract":"With stunning clarity, JWST has revealed the Universe's first billion years. The scientific community is analyzing a wealth of JWST imaging and spectroscopic data from that era, and is in the process of rewriting the astronomy textbooks. Here, 1.5 years into the JWST science mission, we provide a snapshot of the great progress made towards understanding the initial chapters of our cosmic history. We highlight discoveries and breakthroughs, topics and issues that are not yet understood, and questions that will be addressed in the coming years, as JWST continues its revolutionary observations of the Early Universe. While this compendium is written by a small number of authors, invited to ISSI Bern in March 2024 as part of the 2024 ISSI Breakthrough Workshop, we acknowledge the work of a large community that is advancing our collective understanding of the evolution of the Early Universe.","sentences":["With stunning clarity, JWST has revealed the Universe's first billion years.","The scientific community is analyzing a wealth of JWST imaging and spectroscopic data from that era, and is in the process of rewriting the astronomy textbooks.","Here, 1.5 years into the JWST science mission, we provide a snapshot of the great progress made towards understanding the initial chapters of our cosmic history.","We highlight discoveries and breakthroughs, topics and issues that are not yet understood, and questions that will be addressed in the coming years, as JWST continues its revolutionary observations of the Early Universe.","While this compendium is written by a small number of authors, invited to ISSI Bern in March 2024 as part of the 2024 ISSI Breakthrough Workshop, we acknowledge the work of a large community that is advancing our collective understanding of the evolution of the Early Universe."],"url":"http://arxiv.org/abs/2405.21054v1","category":"astro-ph.GA"}
{"created":"2024-05-31 17:25:22","title":"Multirobot Watchman Routes in a Simple Polygon","abstract":"The well-known \\textsc{Watchman Route} problem seeks a shortest route in a polygonal domain from which every point of the domain can be seen. In this paper, we study the cooperative variant of the problem, namely the \\textsc{$k$-Watchmen Routes} problem, in a simple polygon $P$. We look at both the version in which the $k$ watchmen must collectively see all of $P$, and the quota version in which they must see a predetermined fraction of $P$'s area.   We give an exact pseudopolynomial time algorithm for the \\textsc{$k$-Watchmen Routes} problem in a simple orthogonal polygon $P$ with the constraint that watchmen must move on axis-parallel segments, and there is a given common starting point on the boundary. Further, we give a fully polynomial-time approximation scheme and a constant-factor approximation for unconstrained movement. For the quota version, we give a constant-factor approximation in a simple polygon, utilizing the solution to the (single) \\textsc{Quota Watchman Route} problem.","sentences":["The well-known \\textsc{Watchman Route} problem seeks a shortest route in a polygonal domain from which every point of the domain can be seen.","In this paper, we study the cooperative variant of the problem, namely the \\textsc{$k$-Watchmen Routes} problem, in a simple polygon $P$. We look at both the version in which the $k$ watchmen must collectively see all of $P$, and the quota version in which they must see a predetermined fraction of $P$'s area.   ","We give an exact pseudopolynomial time algorithm for the \\textsc{$k$-Watchmen Routes} problem in a simple orthogonal polygon $P$ with the constraint that watchmen must move on axis-parallel segments, and there is a given common starting point on the boundary.","Further, we give a fully polynomial-time approximation scheme and a constant-factor approximation for unconstrained movement.","For the quota version, we give a constant-factor approximation in a simple polygon, utilizing the solution to the (single) \\textsc{Quota Watchman Route} problem."],"url":"http://arxiv.org/abs/2405.21034v1","category":"cs.CG"}
{"created":"2024-05-31 16:38:54","title":"CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web Relevance Ranking","abstract":"We present CWRCzech, Click Web Ranking dataset for Czech, a 100M query-document Czech click dataset for relevance ranking with user behavior data collected from search engine logs of Seznam.cz. To the best of our knowledge, CWRCzech is the largest click dataset with raw text published so far. It provides document positions in the search results as well as information about user behavior: 27.6M clicked documents and 10.8M dwell times. In addition, we also publish a manually annotated Czech test for the relevance task, containing nearly 50k query-document pairs, each annotated by at least 2 annotators. Finally, we analyze how the user behavior data improve relevance ranking and show that models trained on data automatically harnessed at sufficient scale can surpass the performance of models trained on human annotated data. CWRCzech is published under an academic non-commercial license and is available to the research community at https://github.com/seznam/CWRCzech.","sentences":["We present CWRCzech, Click Web Ranking dataset for Czech, a 100M query-document Czech click dataset for relevance ranking with user behavior data collected from search engine logs of Seznam.cz.","To the best of our knowledge, CWRCzech is the largest click dataset with raw text published so far.","It provides document positions in the search results as well as information about user behavior: 27.6M clicked documents and 10.8M dwell times.","In addition, we also publish a manually annotated Czech test for the relevance task, containing nearly 50k query-document pairs, each annotated by at least 2 annotators.","Finally, we analyze how the user behavior data improve relevance ranking and show that models trained on data automatically harnessed at sufficient scale can surpass the performance of models trained on human annotated data.","CWRCzech is published under an academic non-commercial license and is available to the research community at https://github.com/seznam/CWRCzech."],"url":"http://arxiv.org/abs/2405.20994v1","category":"cs.IR"}
{"created":"2024-05-31 15:51:48","title":"Final Physics Design of Proton Improvement Plan-II At Fermilab","abstract":"This paper presents the final physics design of the Proton Improvement Plan-II (PIP-II) at Fermilab, focusing on the linear accelerator (Linac) and its beam transfer line. We address the challenges in longitudinal and transverse lattice design, specifically targeting collective effects, parametric resonances, and space charge nonlinearities that impact beam stability and emittance control. The strategies implemented effectively mitigate space charge complexities, resulting in significant improvements in beam quality -- evidenced by reduced emittance growth, lower beam halo, decreased loss, and better energy spread management. This comprehensive study is pivotal for the PIP-II project's success, providing valuable insights and approaches for future accelerator designs, especially in managing nonlinearities and enhancing beam dynamics.","sentences":["This paper presents the final physics design of the Proton Improvement Plan-II (PIP-II) at Fermilab, focusing on the linear accelerator (Linac) and its beam transfer line.","We address the challenges in longitudinal and transverse lattice design, specifically targeting collective effects, parametric resonances, and space charge nonlinearities that impact beam stability and emittance control.","The strategies implemented effectively mitigate space charge complexities, resulting in significant improvements in beam quality -- evidenced by reduced emittance growth, lower beam halo, decreased loss, and better energy spread management.","This comprehensive study is pivotal for the PIP-II project's success, providing valuable insights and approaches for future accelerator designs, especially in managing nonlinearities and enhancing beam dynamics."],"url":"http://arxiv.org/abs/2405.20953v1","category":"physics.acc-ph"}
{"created":"2024-05-31 15:51:14","title":"Continuous momentum state lasing and cavity frequency-pinning with laser-cooled strontium atoms","abstract":"Laser-cooled gases of atoms interacting with the field of an optical cavity are a powerful tool for quantum sensing and the simulation of open and closed quantum systems. They can display spontaneous self-organisation phase transitions, time crystals, new lasing mechanisms, squeezed states for quantum sensing, protection of quantum coherence, and dynamical phase transitions. However, all of these phenomena are explored in a discontinuous manner due to the need to stop and reload a new ensemble of atoms. Here we report the observation of hours-long continuous lasing from laser-cooled $^{88}$Sr atoms continuously loaded into a ring cavity. The required inversion to produce lasing arises from inversion in the atomic momentum degree of freedom, a mechanism related directly to self-organization phase transitions and collective atomic recoil lasing, both of which were previously only observed in a cyclic fashion compared to the truly continuous behavior here. Further, the sensitivity of the lasing frequency to cavity frequency changes is 120 fold suppressed due to an atomic loss mechanism, opening an interesting new path to compensate cavity frequency noise for realizing narrow frequency references. This work opens the way for continuous cavity QED quantum simulation experiments as well as continuous superradiant lasers.","sentences":["Laser-cooled gases of atoms interacting with the field of an optical cavity are a powerful tool for quantum sensing and the simulation of open and closed quantum systems.","They can display spontaneous self-organisation phase transitions, time crystals, new lasing mechanisms, squeezed states for quantum sensing, protection of quantum coherence, and dynamical phase transitions.","However, all of these phenomena are explored in a discontinuous manner due to the need to stop and reload a new ensemble of atoms.","Here we report the observation of hours-long continuous lasing from laser-cooled $^{88}$Sr atoms continuously loaded into a ring cavity.","The required inversion to produce lasing arises from inversion in the atomic momentum degree of freedom, a mechanism related directly to self-organization phase transitions and collective atomic recoil lasing, both of which were previously only observed in a cyclic fashion compared to the truly continuous behavior here.","Further, the sensitivity of the lasing frequency to cavity frequency changes is 120 fold suppressed due to an atomic loss mechanism, opening an interesting new path to compensate cavity frequency noise for realizing narrow frequency references.","This work opens the way for continuous cavity QED quantum simulation experiments as well as continuous superradiant lasers."],"url":"http://arxiv.org/abs/2405.20952v1","category":"quant-ph"}
{"created":"2024-05-31 15:29:43","title":"Probing the nature of the QCD phase transition with higher-order net-proton number fluctuation and local parton density fluctuation measurements at RHIC-STAR","abstract":"The moments of proton and net-proton multiplicity distributions are observables expected to be sensitive to the QCD critical point and the nature of the QCD phase transition from QGP to hadron gas. Hyper-order cumulants are measured in wide centrality bins in STAR BES-I data and found to be qualitatively consistent with trends predicted by lattice QCD which finds a cross-over phase transition at low $\\mu_\\text{B}$. Data collected at $\\sqrt{s_{NN}}=3$ GeV in BES-II exhibit trends opposite of those observed in higher energy collisions which may suggest the dominance of hadronic interactions at this energy. The variance of proton multiplicity distributions in azimuthal partitions is measured to search for signals of clustering indicative of a first-order phase transition. A strong dependence on the event multiplicity is observed. This dependence is independent of energy in AMPT while in STAR data a significant trend with energy is observed.","sentences":["The moments of proton and net-proton multiplicity distributions are observables expected to be sensitive to the QCD critical point and the nature of the QCD phase transition from QGP to hadron gas.","Hyper-order cumulants are measured in wide centrality bins in STAR BES-I data and found to be qualitatively consistent with trends predicted by lattice QCD which finds a cross-over phase transition at low $\\mu_\\text{B}$. Data collected at $\\sqrt{s_{NN}}=3$ GeV in BES-II exhibit trends opposite of those observed in higher energy collisions which may suggest the dominance of hadronic interactions at this energy.","The variance of proton multiplicity distributions in azimuthal partitions is measured to search for signals of clustering indicative of a first-order phase transition.","A strong dependence on the event multiplicity is observed.","This dependence is independent of energy in AMPT while in STAR data a significant trend with energy is observed."],"url":"http://arxiv.org/abs/2405.20929v1","category":"nucl-ex"}
{"created":"2024-05-31 15:29:07","title":"Recent Highlights from STAR BES Phase II","abstract":"The second phase of the RHIC Beam Energy Scan (BES-II) was conducted between 2019 and 2021. High statistics data was collected by the STAR experiment for Au+Au collisions at $\\sqrt{s_{NN}}$ from 7.7 to 27 GeV in collider mode and from 3 to 13.7 GeV in fixed target mode. A selection of results from the various BES-II analyses are presented here to showcase the wide range of physics accessible.","sentences":["The second phase of the RHIC Beam Energy Scan (BES-II) was conducted between 2019 and 2021.","High statistics data was collected by the STAR experiment for Au+Au collisions at $\\sqrt{s_{NN}}$ from 7.7 to 27 GeV in collider mode and from 3 to 13.7 GeV in fixed target mode.","A selection of results from the various BES-II analyses are presented here to showcase the wide range of physics accessible."],"url":"http://arxiv.org/abs/2405.20928v1","category":"nucl-ex"}
{"created":"2024-05-31 15:25:49","title":"Addressing misconceptions in university physics: A review and experiences from quantum physics educators","abstract":"Students often enter physics classrooms with deeply ingrained misconceptions stemming from everyday experiences. These misconceptions challenge educators, as students often resist information that conflicts with their preconceptions. The first aim of this manuscript is to summarize the existing literature on misconceptions in university physics, reviewing misconceptions' sources, diagnoses, and remediation strategies. Most of this literature has concentrated on classical physics. However, quantum physics poses unique challenges because its concepts are removed from everyday experiences. This signals the need to ask how well existing strategies for addressing misconceptions apply to quantum physics. This is underscored by the recent surge of people from diverse backgrounds entering quantum physics because of the growing significance of quantum technologies. To help answer this question, we conducted in-depth interviews with quantum physics instructors at the University of Waterloo who have collectively taught over 100 quantum physics courses. These interviews explored common misconceptions in quantum physics, their origins, and effective instructional techniques to address them. We highlight specific misconceptions, such as misunderstanding of entanglement and spin, and successful teaching strategies, including ``misconception-trap quizzes.'' We integrate insights from the literature review with our interview data to provide an overview of current best practices in addressing physics misconceptions. Furthermore, we identify key research questions that warrant further exploration, such as the efficacy of multi-tier tests in quantum physics and developing a cohesive quantum curriculum. This paper aims to inform educators and curriculum developers, offering practical recommendations and setting a research agenda to improve conceptual understanding in classical and quantum physics.","sentences":["Students often enter physics classrooms with deeply ingrained misconceptions stemming from everyday experiences.","These misconceptions challenge educators, as students often resist information that conflicts with their preconceptions.","The first aim of this manuscript is to summarize the existing literature on misconceptions in university physics, reviewing misconceptions' sources, diagnoses, and remediation strategies.","Most of this literature has concentrated on classical physics.","However, quantum physics poses unique challenges because its concepts are removed from everyday experiences.","This signals the need to ask how well existing strategies for addressing misconceptions apply to quantum physics.","This is underscored by the recent surge of people from diverse backgrounds entering quantum physics because of the growing significance of quantum technologies.","To help answer this question, we conducted in-depth interviews with quantum physics instructors at the University of Waterloo who have collectively taught over 100 quantum physics courses.","These interviews explored common misconceptions in quantum physics, their origins, and effective instructional techniques to address them.","We highlight specific misconceptions, such as misunderstanding of entanglement and spin, and successful teaching strategies, including ``misconception-trap quizzes.''","We integrate insights from the literature review with our interview data to provide an overview of current best practices in addressing physics misconceptions.","Furthermore, we identify key research questions that warrant further exploration, such as the efficacy of multi-tier tests in quantum physics and developing a cohesive quantum curriculum.","This paper aims to inform educators and curriculum developers, offering practical recommendations and setting a research agenda to improve conceptual understanding in classical and quantum physics."],"url":"http://arxiv.org/abs/2405.20923v1","category":"physics.ed-ph"}
{"created":"2024-05-31 14:47:56","title":"Understanding the Throughput Bounds of Reconfigurable Datacenter Networks","abstract":"The increasing gap between the growth of datacenter traffic volume and the capacity of electrical switches led to the emergence of reconfigurable datacenter network designs based on optical circuit switching. A multitude of research works, ranging from demand-oblivious (e.g., RotorNet, Sirius) to demand-aware (e.g., Helios, ProjecToR) reconfigurable networks, demonstrate significant performance benefits. Unfortunately, little is formally known about the achievable throughput of such networks. Only recently have the throughput bounds of demand-oblivious networks been studied. In this paper, we tackle a fundamental question: Whether and to what extent can demand-aware reconfigurable networks improve the throughput of datacenters?   This paper attempts to understand the landscape of the throughput bounds of reconfigurable datacenter networks. Given the rise of machine learning workloads and collective communication in modern datacenters, we specifically focus on their typical communication patterns, namely uniform-residual demand matrices. We formally establish a separation bound of demand-aware networks over demand-oblivious networks, proving analytically that the former can provide at least $16\\%$ higher throughput. Our analysis further uncovers new design opportunities based on periodic, fixed-duration reconfigurations that can harness the throughput benefits of demand-aware networks while inheriting the simplicity and low reconfiguration overheads of demand-oblivious networks. Finally, our evaluations corroborate the theoretical results of this paper, demonstrating that demand-aware networks significantly outperform oblivious networks in terms of throughput. This work barely scratches the surface and unveils several intriguing open questions, which we discuss at the end of this paper.","sentences":["The increasing gap between the growth of datacenter traffic volume and the capacity of electrical switches led to the emergence of reconfigurable datacenter network designs based on optical circuit switching.","A multitude of research works, ranging from demand-oblivious (e.g., RotorNet, Sirius) to demand-aware (e.g., Helios, ProjecToR) reconfigurable networks, demonstrate significant performance benefits.","Unfortunately, little is formally known about the achievable throughput of such networks.","Only recently have the throughput bounds of demand-oblivious networks been studied.","In this paper, we tackle a fundamental question: Whether and to what extent can demand-aware reconfigurable networks improve the throughput of datacenters?   ","This paper attempts to understand the landscape of the throughput bounds of reconfigurable datacenter networks.","Given the rise of machine learning workloads and collective communication in modern datacenters, we specifically focus on their typical communication patterns, namely uniform-residual demand matrices.","We formally establish a separation bound of demand-aware networks over demand-oblivious networks, proving analytically that the former can provide at least $16\\%$ higher throughput.","Our analysis further uncovers new design opportunities based on periodic, fixed-duration reconfigurations that can harness the throughput benefits of demand-aware networks while inheriting the simplicity and low reconfiguration overheads of demand-oblivious networks.","Finally, our evaluations corroborate the theoretical results of this paper, demonstrating that demand-aware networks significantly outperform oblivious networks in terms of throughput.","This work barely scratches the surface and unveils several intriguing open questions, which we discuss at the end of this paper."],"url":"http://arxiv.org/abs/2405.20869v1","category":"cs.NI"}
{"created":"2024-05-31 14:07:33","title":"Optimally Improving Cooperative Learning in a Social Setting","abstract":"We consider a cooperative learning scenario where a collection of networked agents with individually owned classifiers dynamically update their predictions, for the same classification task, through communication or observations of each other's predictions. Clearly if highly influential vertices use erroneous classifiers, there will be a negative effect on the accuracy of all the agents in the network. We ask the following question: how can we optimally fix the prediction of a few classifiers so as maximize the overall accuracy in the entire network. To this end we consider an aggregate and an egalitarian objective function. We show a polynomial time algorithm for optimizing the aggregate objective function, and show that optimizing the egalitarian objective function is NP-hard. Furthermore, we develop approximation algorithms for the egalitarian improvement. The performance of all of our algorithms are guaranteed by mathematical analysis and backed by experiments on synthetic and real data.","sentences":["We consider a cooperative learning scenario where a collection of networked agents with individually owned classifiers dynamically update their predictions, for the same classification task, through communication or observations of each other's predictions.","Clearly if highly influential vertices use erroneous classifiers, there will be a negative effect on the accuracy of all the agents in the network.","We ask the following question: how can we optimally fix the prediction of a few classifiers so as maximize the overall accuracy in the entire network.","To this end we consider an aggregate and an egalitarian objective function.","We show a polynomial time algorithm for optimizing the aggregate objective function, and show that optimizing the egalitarian objective function is NP-hard.","Furthermore, we develop approximation algorithms for the egalitarian improvement.","The performance of all of our algorithms are guaranteed by mathematical analysis and backed by experiments on synthetic and real data."],"url":"http://arxiv.org/abs/2405.20808v1","category":"cs.DS"}
{"created":"2024-05-31 08:43:38","title":"Directly Estimating Mixed-State Entanglement with Bell Measurement Assistance","abstract":"Entanglement plays a fundamental role in quantum physics and information processing. Here, we directly estimate mixed-state entanglement using random unitary evolution in a photonic system. As a supplement to traditional projective measurements, we incorporate Bell measurements on qubit-pairs, enriching the previous randomized measurement scheme, which is no-go in this task with only local unitary evolution. The scheme is scalable to n-qubits via Bell measurements on qubit-pairs. Moreover, by introducing the few-shot postprocessing, the efficiency of the data collecting and processing is significantly improved. The estimator can be derived directly from a few consecutive outcomes while exhibiting greater robustness to system errors and noise compared to schemes based on shadow estimation. Our protocol and demonstration advance the direct characterization of quantum states in practice.","sentences":["Entanglement plays a fundamental role in quantum physics and information processing.","Here, we directly estimate mixed-state entanglement using random unitary evolution in a photonic system.","As a supplement to traditional projective measurements, we incorporate Bell measurements on qubit-pairs, enriching the previous randomized measurement scheme, which is no-go in this task with only local unitary evolution.","The scheme is scalable to n-qubits via Bell measurements on qubit-pairs.","Moreover, by introducing the few-shot postprocessing, the efficiency of the data collecting and processing is significantly improved.","The estimator can be derived directly from a few consecutive outcomes while exhibiting greater robustness to system errors and noise compared to schemes based on shadow estimation.","Our protocol and demonstration advance the direct characterization of quantum states in practice."],"url":"http://arxiv.org/abs/2405.20696v1","category":"quant-ph"}
{"created":"2024-05-31 17:58:15","title":"Toward Quantum Analogue Simulation of Many-Body Supersymmetry with Rydberg Atom Arrays","abstract":"A topological quantum number, the Witten index, characterizes supersymmetric models by probing for zero energy modes and the possibility of supersymmetry breaking. We propose an averaging method to infer the Witten index in quantum analogue simulators. Motivated by recent work on Rydberg atoms trapped in optical tweezer arrays, we consider a related supersymmetric XXZ spin model. We show how to infer the Witten index from open system averaging and numerically demonstrate its topological robustness in this model. Our work defines a route for quantum analogue simulators to directly identify many-body topological physics.","sentences":["A topological quantum number, the Witten index, characterizes supersymmetric models by probing for zero energy modes and the possibility of supersymmetry breaking.","We propose an averaging method to infer the Witten index in quantum analogue simulators.","Motivated by recent work on Rydberg atoms trapped in optical tweezer arrays, we consider a related supersymmetric XXZ spin model.","We show how to infer the Witten index from open system averaging and numerically demonstrate its topological robustness in this model.","Our work defines a route for quantum analogue simulators to directly identify many-body topological physics."],"url":"http://arxiv.org/abs/2405.21073v1","category":"quant-ph"}
{"created":"2024-05-31 17:56:37","title":"Very Low Complexity Speech Synthesis Using Framewise Autoregressive GAN (FARGAN) with Pitch Prediction","abstract":"Neural vocoders are now being used in a wide range of speech processing applications. In many of those applications, the vocoder can be the most complex component, so finding lower complexity algorithms can lead to significant practical benefits. In this work, we propose FARGAN, an autoregressive vocoder that takes advantage of long-term pitch prediction to synthesize high-quality speech in small subframes, without the need for teacher-forcing. Experimental results show that the proposed 600~MFLOPS FARGAN vocoder can achieve both higher quality and lower complexity than existing low-complexity vocoders. The quality even matches that of existing higher-complexity vocoders.","sentences":["Neural vocoders are now being used in a wide range of speech processing applications.","In many of those applications, the vocoder can be the most complex component, so finding lower complexity algorithms can lead to significant practical benefits.","In this work, we propose FARGAN, an autoregressive vocoder that takes advantage of long-term pitch prediction to synthesize high-quality speech in small subframes, without the need for teacher-forcing.","Experimental results show that the proposed 600~MFLOPS FARGAN vocoder can achieve both higher quality and lower complexity than existing low-complexity vocoders.","The quality even matches that of existing higher-complexity vocoders."],"url":"http://arxiv.org/abs/2405.21069v1","category":"eess.AS"}
{"created":"2024-05-31 17:45:58","title":"Dimension formulas for period spaces via motives and species","abstract":"We apply the structure theory of finite dimensional algebras in order to deduce dimension formulas for spaces of period numbers, i.e., complex numbers defined by integrals of algebraic nature. We get a complete and conceptually clear answer in the case of $1$-periods, generalising classical results like Baker's theorem on the logarithms of algebraic numbers and partial results in Huber--W{\\\"u}stholz \\cite{huber-wuestholz}.   The application to the case of Mixed Tate Motives (i.e., Multiple Zeta Values) recovers the dimension estimates of Deligne--Goncharov \\cite{deligne-goncharov}.","sentences":["We apply the structure theory of finite dimensional algebras in order to deduce dimension formulas for spaces of period numbers, i.e., complex numbers defined by integrals of algebraic nature.","We get a complete and conceptually clear answer in the case of $1$-periods, generalising classical results like Baker's theorem on the logarithms of algebraic numbers and partial results in Huber--W{\\\"u}stholz \\cite{huber-wuestholz}.   ","The application to the case of Mixed Tate Motives (i.e., Multiple Zeta Values) recovers the dimension estimates of Deligne--Goncharov \\cite{deligne-goncharov}."],"url":"http://arxiv.org/abs/2405.21053v1","category":"math.NT"}
{"created":"2024-05-31 17:43:59","title":"Good Modelling Software Practices","abstract":"In socio-environmental sciences, models are frequently used as tools to represent, understand, project and predict the behaviour of these complex systems. Along the modelling chain, Good Modelling Practices have been evolving that ensure -- amongst others -- that models are transparent and replicable. Whenever such models are represented in software, good modelling meets Good software Practices, such as a tractable development workflow, good code, collaborative development and governance, continuous integration and deployment, and Good Scientific Practices, such as attribution of copyrights and acknowledgement of intellectual property, publication of a software paper and archiving. Too often in existing socio-environmental model software, these practices have been regarded as an add-on to be considered at a later stage only; in fact, many modellers have shied away from publishing their model as open source out of fear that having to add good practices is too demanding. We here argue for making a habit of following a list of simple and not so simple practices early on in the implementation of the model life cycle. We contextualise cherry-picked and hands-on practices for supporting Good Modelling Practices, and we demonstrate their application in the example context of the Viable North Sea fisheries socio-ecological systems model.","sentences":["In socio-environmental sciences, models are frequently used as tools to represent, understand, project and predict the behaviour of these complex systems.","Along the modelling chain, Good Modelling Practices have been evolving that ensure -- amongst others -- that models are transparent and replicable.","Whenever such models are represented in software, good modelling meets Good software Practices, such as a tractable development workflow, good code, collaborative development and governance, continuous integration and deployment, and Good Scientific Practices, such as attribution of copyrights and acknowledgement of intellectual property, publication of a software paper and archiving.","Too often in existing socio-environmental model software, these practices have been regarded as an add-on to be considered at a later stage only; in fact, many modellers have shied away from publishing their model as open source out of fear that having to add good practices is too demanding.","We here argue for making a habit of following a list of simple and not so simple practices early on in the implementation of the model life cycle.","We contextualise cherry-picked and hands-on practices for supporting Good Modelling Practices, and we demonstrate their application in the example context of the Viable North Sea fisheries socio-ecological systems model."],"url":"http://arxiv.org/abs/2405.21051v1","category":"cs.SE"}
{"created":"2024-05-31 17:32:02","title":"Interferometry of quantum correlation functions to access quasiprobability distribution of work","abstract":"The Kirkwood-Dirac quasiprobability distribution emerges from the quantum correlation function of two observables measured at distinct times and is therefore relevant for fundamental physics and quantum technologies. These quasiprobabilities follow all but one of Kolmogorov axioms for joint probability distributions: they can take non-positive values. Their experimental reconstruction becomes challenging when expectation values of incompatible observables are involved. Previous strategies aimed to reconstruct them using weak measurements or combining strong measurements. Here, we use a more direct approach, an interferometric scheme aided by an auxiliary system, to reconstruct the Kirkwood-Dirac quasiprobability distribution. We experimentally demonstrate the interferometric scheme in an electron-nuclear spin system associated with a nitrogen-vacancy center in diamond. By measuring the characteristic function, we reconstruct the quasiprobability distribution of the work and analyze the behavior of the first and second moments of work. Our results clarify the physical meaning of the work quasiprobability distribution in the context of quantum thermodynamics. Finally, having measured the real and imaginary parts of the Kirkwood-Dirac quasiprobability of work, we are also able to study the uncertainty of measuring the Hamiltonian of the system at two times, via the Robertson-Schr{\\\"o}dinger uncertainty relation, for different initial states.","sentences":["The Kirkwood-Dirac quasiprobability distribution emerges from the quantum correlation function of two observables measured at distinct times and is therefore relevant for fundamental physics and quantum technologies.","These quasiprobabilities follow all but one of Kolmogorov axioms for joint probability distributions: they can take non-positive values.","Their experimental reconstruction becomes challenging when expectation values of incompatible observables are involved.","Previous strategies aimed to reconstruct them using weak measurements or combining strong measurements.","Here, we use a more direct approach, an interferometric scheme aided by an auxiliary system, to reconstruct the Kirkwood-Dirac quasiprobability distribution.","We experimentally demonstrate the interferometric scheme in an electron-nuclear spin system associated with a nitrogen-vacancy center in diamond.","By measuring the characteristic function, we reconstruct the quasiprobability distribution of the work and analyze the behavior of the first and second moments of work.","Our results clarify the physical meaning of the work quasiprobability distribution in the context of quantum thermodynamics.","Finally, having measured the real and imaginary parts of the Kirkwood-Dirac quasiprobability of work, we are also able to study the uncertainty of measuring the Hamiltonian of the system at two times, via the Robertson-Schr{\\\"o}dinger uncertainty relation, for different initial states."],"url":"http://arxiv.org/abs/2405.21041v1","category":"quant-ph"}
{"created":"2024-05-31 17:09:07","title":"Beyond Conventional Parametric Modeling: Data-Driven Framework for Estimation and Prediction of Time Activity Curves in Dynamic PET Imaging","abstract":"Dynamic Positron Emission Tomography (dPET) imaging and Time-Activity Curve (TAC) analyses are essential for understanding and quantifying the biodistribution of radiopharmaceuticals over time and space. Traditional compartmental modeling, while foundational, commonly struggles to fully capture the complexities of biological systems, including non-linear dynamics and variability. This study introduces an innovative data-driven neural network-based framework, inspired by Reaction Diffusion systems, designed to address these limitations. Our approach, which adaptively fits TACs from dPET, enables the direct calibration of diffusion coefficients and reaction terms from observed data, offering significant improvements in predictive accuracy and robustness over traditional methods, especially in complex biological scenarios. By more accurately modeling the spatio-temporal dynamics of radiopharmaceuticals, our method advances modeling of pharmacokinetic and pharmacodynamic processes, enabling new possibilities in quantitative nuclear medicine.","sentences":["Dynamic Positron Emission Tomography (dPET) imaging and Time-Activity Curve (TAC) analyses are essential for understanding and quantifying the biodistribution of radiopharmaceuticals over time and space.","Traditional compartmental modeling, while foundational, commonly struggles to fully capture the complexities of biological systems, including non-linear dynamics and variability.","This study introduces an innovative data-driven neural network-based framework, inspired by Reaction Diffusion systems, designed to address these limitations.","Our approach, which adaptively fits TACs from dPET, enables the direct calibration of diffusion coefficients and reaction terms from observed data, offering significant improvements in predictive accuracy and robustness over traditional methods, especially in complex biological scenarios.","By more accurately modeling the spatio-temporal dynamics of radiopharmaceuticals, our method advances modeling of pharmacokinetic and pharmacodynamic processes, enabling new possibilities in quantitative nuclear medicine."],"url":"http://arxiv.org/abs/2405.21021v1","category":"cs.LG"}
{"created":"2024-05-31 17:07:43","title":"Quantum quench dynamics as a shortcut to adiabaticity","abstract":"The ability to efficiently prepare ground states of quantum Hamiltonians via adiabatic protocols is typically limited by the smallest energy gap encountered during the quantum evolution. This presents a key obstacle for quantum simulation and realizations of adiabatic quantum algorithms in large systems, particularly when the adiabatic gap vanishes exponentially with system size. Using QuEra's Aquila programmable quantum simulator based on Rydberg atom arrays, we experimentally demonstrate a method to circumvent such limitations. Specifically, we develop and test a \"sweep-quench-sweep\" quantum algorithm in which the incorporation of a quench step serves as a remedy to the diverging adiabatic timescale. These quenches introduce a macroscopic reconfiguration between states separated by an extensively large Hamming distance, akin to quantum many-body scars. Our experiments show that this approach significantly outperforms the adiabatic algorithm, illustrating that such quantum quench algorithms can provide a shortcut to adiabaticity for large-scale many-body quantum systems.","sentences":["The ability to efficiently prepare ground states of quantum Hamiltonians via adiabatic protocols is typically limited by the smallest energy gap encountered during the quantum evolution.","This presents a key obstacle for quantum simulation and realizations of adiabatic quantum algorithms in large systems, particularly when the adiabatic gap vanishes exponentially with system size.","Using QuEra's Aquila programmable quantum simulator based on Rydberg atom arrays, we experimentally demonstrate a method to circumvent such limitations.","Specifically, we develop and test a \"sweep-quench-sweep\" quantum algorithm in which the incorporation of a quench step serves as a remedy to the diverging adiabatic timescale.","These quenches introduce a macroscopic reconfiguration between states separated by an extensively large Hamming distance, akin to quantum many-body scars.","Our experiments show that this approach significantly outperforms the adiabatic algorithm, illustrating that such quantum quench algorithms can provide a shortcut to adiabaticity for large-scale many-body quantum systems."],"url":"http://arxiv.org/abs/2405.21019v1","category":"quant-ph"}
{"created":"2024-05-31 16:52:05","title":"Likelihood Equilibria in the Ising Game","abstract":"A description of static equilibria in the noisy binary choice (Ising) game on complete and random graphs resulting from maximisation of the likelihood of system configurations is presented. An equivalence of such likelihood equilibria to the competitive Bayes-Nash quantal response expectation equilibria in the special case of consistent agents expectations is established. It is shown that the same likelihood equilibria are obtained by considering the system's partition function.","sentences":["A description of static equilibria in the noisy binary choice (Ising) game on complete and random graphs resulting from maximisation of the likelihood of system configurations is presented.","An equivalence of such likelihood equilibria to the competitive Bayes-Nash quantal response expectation equilibria in the special case of consistent agents expectations is established.","It is shown that the same likelihood equilibria are obtained by considering the system's partition function."],"url":"http://arxiv.org/abs/2405.21010v1","category":"cs.GT"}
{"created":"2024-05-31 16:47:42","title":"FunLess: Functions-as-a-Service for Private Edge Cloud Systems","abstract":"We present FunLess, a Function-as-a-Service (FaaS) platform tailored for the private edge cloud system. FunLess responds to recent trends that advocate for extending the coverage of serverless computing to private edge cloud systems and enhancing latency, security, and privacy while improving resource usage. Unlike existing solutions that rely on containers for function invocation, FunLess leverages WebAssembly (Wasm) as its runtime environment. Wasm's lightweight, sandboxed runtime is crucial to have functions run on constrained devices at the edge. Moreover, the advantages of using Wasm in FunLess include a consistent development and deployment environment for users and function portability (write once, run everywhere)   We validate FunLess under different deployment scenarios, characterised by the presence/absence of constrained-resource devices (Raspberry Pi 3B+) and the (in)accessibility of container orchestration technologies - Kubernetes. We compare FunLess with three production-ready, widely adopted open-source FaaS platforms - OpenFaaS, Fission, and Knative. Our benchmarks confirm that FunLess is a proper solution for FaaS private edge cloud systems since it achieves performance comparable to the considered FaaS alternatives while it is the only fully-deployable alternative on constrained-resource devices, thanks to its small memory footprint.","sentences":["We present FunLess, a Function-as-a-Service (FaaS) platform tailored for the private edge cloud system.","FunLess responds to recent trends that advocate for extending the coverage of serverless computing to private edge cloud systems and enhancing latency, security, and privacy while improving resource usage.","Unlike existing solutions that rely on containers for function invocation, FunLess leverages WebAssembly (Wasm) as its runtime environment.","Wasm's lightweight, sandboxed runtime is crucial to have functions run on constrained devices at the edge.","Moreover, the advantages of using Wasm in FunLess include a consistent development and deployment environment for users and function portability (write once, run everywhere)   ","We validate FunLess under different deployment scenarios, characterised by the presence/absence of constrained-resource devices (Raspberry Pi 3B+) and the (in)accessibility of container orchestration technologies - Kubernetes.","We compare FunLess with three production-ready, widely adopted open-source FaaS platforms - OpenFaaS, Fission, and Knative.","Our benchmarks confirm that FunLess is a proper solution for FaaS private edge cloud systems since it achieves performance comparable to the considered FaaS alternatives while it is the only fully-deployable alternative on constrained-resource devices, thanks to its small memory footprint."],"url":"http://arxiv.org/abs/2405.21009v1","category":"cs.DC"}
{"created":"2024-05-31 16:44:54","title":"MunchSonic: Tracking Fine-grained Dietary Actions through Active Acoustic Sensing on Eyeglasses","abstract":"We introduce MunchSonic, an AI-powered active acoustic sensing system integrated into eyeglasses, designed to track fine-grained dietary actions like hand-to-mouth movements for food intake, chewing, and drinking. MunchSonic emits inaudible ultrasonic waves from a commodity eyeglass frame. The reflected signals contain rich information about the position and movements of various body parts, including the mouth, jaw, arms, and hands, all of which are involved in eating activities. These signals are then processed by a custom deep-learning pipeline to classify six actions: food intake, chewing, drinking, talking, face-hand touching, and other activities (null). In an unconstrained user study with 12 participants, MunchSonic achieves a 93.5% macro F1-score in a user-independent evaluation with a 2-second time resolution, demonstrating its effectiveness. Additionally, MunchSonic accurately tracks eating episodes and the frequency of food intake within those episodes.","sentences":["We introduce MunchSonic, an AI-powered active acoustic sensing system integrated into eyeglasses, designed to track fine-grained dietary actions like hand-to-mouth movements for food intake, chewing, and drinking.","MunchSonic emits inaudible ultrasonic waves from a commodity eyeglass frame.","The reflected signals contain rich information about the position and movements of various body parts, including the mouth, jaw, arms, and hands, all of which are involved in eating activities.","These signals are then processed by a custom deep-learning pipeline to classify six actions: food intake, chewing, drinking, talking, face-hand touching, and other activities (null).","In an unconstrained user study with 12 participants, MunchSonic achieves a 93.5% macro F1-score in a user-independent evaluation with a 2-second time resolution, demonstrating its effectiveness.","Additionally, MunchSonic accurately tracks eating episodes and the frequency of food intake within those episodes."],"url":"http://arxiv.org/abs/2405.21004v1","category":"cs.HC"}
{"created":"2024-05-31 16:43:20","title":"Quantum Information Processing with Molecular Nanomagnets: an introduction","abstract":"Many problems intractable on classical devices could be solved by algorithms explicitly based on quantum mechanical laws, i.e. exploiting quantum information processing. As a result, increasing efforts from different fields are nowadays directed to the actual realization of quantum devices. Here we provide an introduction to Quantum Information Processing, focusing on a promising setup for its implementation, represented by molecular spin clusters known as Molecular Nanomagnets. We introduce the basic tools to understand and design quantum algorithms, always referring to their actual realization on a molecular spin architecture. We then examine the most important sources of noise in this class of systems and then one of their most peculiar features, i.e. the possibility to exploit many (more than two) available states to encode information and to self-correct it from errors via proper design of quantum error correction codes. Finally, we present some examples of quantum algorithms proposed and implemented on a molecular spin qudit hardware.","sentences":["Many problems intractable on classical devices could be solved by algorithms explicitly based on quantum mechanical laws, i.e. exploiting quantum information processing.","As a result, increasing efforts from different fields are nowadays directed to the actual realization of quantum devices.","Here we provide an introduction to Quantum Information Processing, focusing on a promising setup for its implementation, represented by molecular spin clusters known as Molecular Nanomagnets.","We introduce the basic tools to understand and design quantum algorithms, always referring to their actual realization on a molecular spin architecture.","We then examine the most important sources of noise in this class of systems and then one of their most peculiar features, i.e. the possibility to exploit many (more than two) available states to encode information and to self-correct it from errors via proper design of quantum error correction codes.","Finally, we present some examples of quantum algorithms proposed and implemented on a molecular spin qudit hardware."],"url":"http://arxiv.org/abs/2405.21000v1","category":"quant-ph"}
{"created":"2024-05-31 16:41:36","title":"Towards a Fluid computer","abstract":"In 1991, Moore [20] raised a question about whether hydrodynamics is capable of performing computations. Similarly, in 2016, Tao [25] asked whether a mechanical system, including a fluid flow, can simulate a universal Turing machine. In this expository article, we review the construction in [8] of a \"Fluid computer\" in dimension 3 that combines techniques in symbolic dynamics with the connection between steady Euler flows and contact geometry unveiled by Etnyre and Ghrist. In addition, we argue that the metric that renders the vector field Beltrami cannot be critical in the Chern-Hamilton sense [9]. We also sketch the completely different construction for the Euclidean metric in $\\mathbb R^3$ as given in [7]. These results reveal the existence of undecidable fluid particle paths. We conclude the article with a list of open problems.","sentences":["In 1991, Moore","[20] raised a question about whether hydrodynamics is capable of performing computations.","Similarly, in 2016, Tao [25] asked whether a mechanical system, including a fluid flow, can simulate a universal Turing machine.","In this expository article, we review the construction in [8] of a \"Fluid computer\" in dimension 3 that combines techniques in symbolic dynamics with the connection between steady Euler flows and contact geometry unveiled by Etnyre and Ghrist.","In addition, we argue that the metric that renders the vector field Beltrami cannot be critical in the Chern-Hamilton sense [9].","We also sketch the completely different construction for the Euclidean metric in $\\mathbb R^3$ as given in [7].","These results reveal the existence of undecidable fluid particle paths.","We conclude the article with a list of open problems."],"url":"http://arxiv.org/abs/2405.20999v1","category":"math.DS"}
{"created":"2024-05-31 16:40:00","title":"KeldyshQFT: A C++ codebase for real-frequency multiloop functional renormalization group and parquet computations of the single-impurity Anderson model","abstract":"We provide a detailed exposition of our computational framework designed for the accurate calculation of real-frequency dynamical correlation functions of the single-impurity Anderson model (AM) in the regime of weak to intermediate coupling. Using quantum field theory within the Keldysh formalism to directly access the self-energy and dynamical susceptibilities in real frequencies, as detailed in our recent publication (https://doi.org/10.1103/PhysRevB.109.115128), the primary computational challenge is the full three-dimensional real-frequency dependence of the four-point vertex. Our codebase provides a fully MPI+OpenMP parallelized implementation of the functional renormalization group (fRG) and the self-consistent parquet equations within the parquet approximation. It leverages vectorization to handle the additional complexity imposed by the Keldysh formalism, using optimized data structures and highly performant integration routines. Going beyond the results shown in the previous publication, the code includes functionality to perform fRG calculations in the multiloop framework, at arbitrary loop order, including self-consistent self-energy iterations. Moreover, implementations of various regulators, such as hybridization, interaction, frequency, and temperature are supplied.","sentences":["We provide a detailed exposition of our computational framework designed for the accurate calculation of real-frequency dynamical correlation functions of the single-impurity Anderson model (AM) in the regime of weak to intermediate coupling.","Using quantum field theory within the Keldysh formalism to directly access the self-energy and dynamical susceptibilities in real frequencies, as detailed in our recent publication (https://doi.org/10.1103/PhysRevB.109.115128), the primary computational challenge is the full three-dimensional real-frequency dependence of the four-point vertex.","Our codebase provides a fully MPI+OpenMP parallelized implementation of the functional renormalization group (fRG) and the self-consistent parquet equations within the parquet approximation.","It leverages vectorization to handle the additional complexity imposed by the Keldysh formalism, using optimized data structures and highly performant integration routines.","Going beyond the results shown in the previous publication, the code includes functionality to perform fRG calculations in the multiloop framework, at arbitrary loop order, including self-consistent self-energy iterations.","Moreover, implementations of various regulators, such as hybridization, interaction, frequency, and temperature are supplied."],"url":"http://arxiv.org/abs/2405.20996v1","category":"cond-mat.str-el"}
{"created":"2024-05-31 16:35:43","title":"A Novel Two-stage Deming Regression Framework with Applications to Association Analysis between Clinical Risks","abstract":"In healthcare, clinical risks are crucial for treatment decisions, yet the analysis of their associations is often overlooked. This gap is particularly significant when balancing risks that are weighed against each other, as in the case of atrial fibrillation (AF) patients facing stroke and bleeding risks with anticoagulant medication. While traditional regression models are ill-suited for this task due to standard errors in risk estimation, a novel two-stage Deming regression framework is proposed to address this issue, offering a more accurate tool for analyzing associations between variables observed with errors of known or estimated variances. The first stage is to obtain the variable values with variances of errors either by estimation or observation, followed by the second stage that fits a Deming regression model potentially subject to a transformation. The second stage accounts for the uncertainties associated with both independent and response variables, including known or estimated variances and additional unknown variances from the model. The complexity arising from different scenarios of uncertainty is handled by existing and advanced variations of Deming regression models. An important practical application is to support personalized treatment recommendations based on clinical risk associations that were identified by the proposed framework. The model's effectiveness is demonstrated by applying it to a real-world dataset of AF-diagnosed patients to explore the relationship between stroke and bleeding risks, providing crucial guidance for making informed decisions regarding anticoagulant medication. Furthermore, the model's versatility in addressing data containing multiple sources of uncertainty such as privacy-protected data suggests promising avenues for future research in regression analysis.","sentences":["In healthcare, clinical risks are crucial for treatment decisions, yet the analysis of their associations is often overlooked.","This gap is particularly significant when balancing risks that are weighed against each other, as in the case of atrial fibrillation (AF) patients facing stroke and bleeding risks with anticoagulant medication.","While traditional regression models are ill-suited for this task due to standard errors in risk estimation, a novel two-stage Deming regression framework is proposed to address this issue, offering a more accurate tool for analyzing associations between variables observed with errors of known or estimated variances.","The first stage is to obtain the variable values with variances of errors either by estimation or observation, followed by the second stage that fits a Deming regression model potentially subject to a transformation.","The second stage accounts for the uncertainties associated with both independent and response variables, including known or estimated variances and additional unknown variances from the model.","The complexity arising from different scenarios of uncertainty is handled by existing and advanced variations of Deming regression models.","An important practical application is to support personalized treatment recommendations based on clinical risk associations that were identified by the proposed framework.","The model's effectiveness is demonstrated by applying it to a real-world dataset of AF-diagnosed patients to explore the relationship between stroke and bleeding risks, providing crucial guidance for making informed decisions regarding anticoagulant medication.","Furthermore, the model's versatility in addressing data containing multiple sources of uncertainty such as privacy-protected data suggests promising avenues for future research in regression analysis."],"url":"http://arxiv.org/abs/2405.20992v1","category":"stat.AP"}
{"created":"2024-05-31 16:32:46","title":"Uncertainty Quantification for Bird's Eye View Semantic Segmentation: Methods and Benchmarks","abstract":"The fusion of raw features from multiple sensors on an autonomous vehicle to create a Bird's Eye View (BEV) representation is crucial for planning and control systems. There is growing interest in using deep learning models for BEV semantic segmentation. Anticipating segmentation errors and improving the explainability of DNNs is essential for autonomous driving, yet it is under-studied. This paper introduces a benchmark for predictive uncertainty quantification in BEV segmentation. The benchmark assesses various approaches across three popular datasets using two representative backbones and focuses on the effectiveness of predicted uncertainty in identifying misclassified and out-of-distribution (OOD) pixels, as well as calibration. Empirical findings highlight the challenges in uncertainty quantification. Our results find that evidential deep learning based approaches show the most promise by efficiently quantifying aleatoric and epistemic uncertainty. We propose the Uncertainty-Focal-Cross-Entropy (UFCE) loss, designed for highly imbalanced data, which consistently improves the segmentation quality and calibration. Additionally, we introduce a vacuity-scaled regularization term that enhances the model's focus on high uncertainty pixels, improving epistemic uncertainty quantification.","sentences":["The fusion of raw features from multiple sensors on an autonomous vehicle to create a Bird's Eye View (BEV) representation is crucial for planning and control systems.","There is growing interest in using deep learning models for BEV semantic segmentation.","Anticipating segmentation errors and improving the explainability of DNNs is essential for autonomous driving, yet it is under-studied.","This paper introduces a benchmark for predictive uncertainty quantification in BEV segmentation.","The benchmark assesses various approaches across three popular datasets using two representative backbones and focuses on the effectiveness of predicted uncertainty in identifying misclassified and out-of-distribution (OOD) pixels, as well as calibration.","Empirical findings highlight the challenges in uncertainty quantification.","Our results find that evidential deep learning based approaches show the most promise by efficiently quantifying aleatoric and epistemic uncertainty.","We propose the Uncertainty-Focal-Cross-Entropy (UFCE) loss, designed for highly imbalanced data, which consistently improves the segmentation quality and calibration.","Additionally, we introduce a vacuity-scaled regularization term that enhances the model's focus on high uncertainty pixels, improving epistemic uncertainty quantification."],"url":"http://arxiv.org/abs/2405.20986v1","category":"cs.LG"}
{"created":"2024-05-31 16:29:08","title":"Goal-Oriented Sensor Reporting Scheduling for Non-linear Dynamic System Monitoring","abstract":"Goal-oriented communication (GoC) is a form of semantic communication where the effectiveness of information transmission is measured by its impact on achieving the desired goal. In the context of the Internet of Things (IoT), GoC can make IoT sensors to selectively transmit data pertinent to the intended goals of the receiver. Therefore, GoC holds significant value for IoT networks as it facilitates timely decision-making at the receiver, reduces network congestion, and enhances spectral efficiency. In this paper, we consider a scenario where an edge node polls sensors monitoring the state of a non-linear dynamic system (NLDS) to respond to the queries of several clients. Our work delves into the foregoing GoC problem, which we term goal-oriented scheduling (GoS). Our proposed GoS utilizes deep reinforcement learning (DRL) with meticulously devised action space, state space, and reward function. The devised action space and reward function play a pivotal role in reducing the number of sensor transmissions. Meanwhile, the devised state space empowers our DRL scheduler to poll the sensor whose observation is expected to minimize the mean square error (MSE) of the query responses. Our numerical analysis demonstrates that the proposed GoS can either effectively minimize the query response MSE further or obtain a resembling MSE compared to benchmark scheduling methods, depending on the type of query. Furthermore, the proposed GoS proves to be energy-efficient for the sensors and of lower complexity compared to benchmark scheduling methods.","sentences":["Goal-oriented communication (GoC) is a form of semantic communication where the effectiveness of information transmission is measured by its impact on achieving the desired goal.","In the context of the Internet of Things (IoT), GoC can make IoT sensors to selectively transmit data pertinent to the intended goals of the receiver.","Therefore, GoC holds significant value for IoT networks as it facilitates timely decision-making at the receiver, reduces network congestion, and enhances spectral efficiency.","In this paper, we consider a scenario where an edge node polls sensors monitoring the state of a non-linear dynamic system (NLDS) to respond to the queries of several clients.","Our work delves into the foregoing GoC problem, which we term goal-oriented scheduling (GoS).","Our proposed GoS utilizes deep reinforcement learning (DRL) with meticulously devised action space, state space, and reward function.","The devised action space and reward function play a pivotal role in reducing the number of sensor transmissions.","Meanwhile, the devised state space empowers our DRL scheduler to poll the sensor whose observation is expected to minimize the mean square error (MSE) of the query responses.","Our numerical analysis demonstrates that the proposed GoS can either effectively minimize the query response MSE further or obtain a resembling MSE compared to benchmark scheduling methods, depending on the type of query.","Furthermore, the proposed GoS proves to be energy-efficient for the sensors and of lower complexity compared to benchmark scheduling methods."],"url":"http://arxiv.org/abs/2405.20983v1","category":"eess.SY"}
{"created":"2024-05-31 16:20:55","title":"Congestion-Aware Path Re-routing Strategy for Dense Urban Airspace","abstract":"Existing UAS Traffic Management (UTM) frameworks designate preplanned flight paths to uncrewed aircraft systems (UAS), enabling the UAS to deliver payloads. However, with increasing delivery demand between the source-destination pairs in the urban airspace, UAS will likely experience considerable congestion on the nominal paths. We propose a rule-based congestion mitigation strategy that improves UAS safety and airspace utilization in congested traffic streams. The strategy relies on nominal path information from the UTM and positional information of other UAS in the vicinity. Following the strategy, UAS opts for alternative local paths in the unoccupied airspace surrounding the nominal path and avoids congested regions. The strategy results in UAS traffic exploring and spreading to alternative adjacent routes on encountering congestion. The paper presents queuing models to estimate the expected traffic spread for varying stochastic delivery demand at the source, thus helping to reserve the airspace around the nominal path beforehand to accommodate any foreseen congestion. Simulations are presented to validate the queuing results in the presence of static obstacles and intersecting UAS streams.","sentences":["Existing UAS Traffic Management (UTM) frameworks designate preplanned flight paths to uncrewed aircraft systems (UAS), enabling the UAS to deliver payloads.","However, with increasing delivery demand between the source-destination pairs in the urban airspace, UAS will likely experience considerable congestion on the nominal paths.","We propose a rule-based congestion mitigation strategy that improves UAS safety and airspace utilization in congested traffic streams.","The strategy relies on nominal path information from the UTM and positional information of other UAS in the vicinity.","Following the strategy, UAS opts for alternative local paths in the unoccupied airspace surrounding the nominal path and avoids congested regions.","The strategy results in UAS traffic exploring and spreading to alternative adjacent routes on encountering congestion.","The paper presents queuing models to estimate the expected traffic spread for varying stochastic delivery demand at the source, thus helping to reserve the airspace around the nominal path beforehand to accommodate any foreseen congestion.","Simulations are presented to validate the queuing results in the presence of static obstacles and intersecting UAS streams."],"url":"http://arxiv.org/abs/2405.20972v1","category":"cs.MA"}
{"created":"2024-05-31 16:16:28","title":"Design, Calibration, and Control of Compliant Force-sensing Gripping Pads for Humanoid Robots","abstract":"This paper introduces a pair of low-cost, light-weight and compliant force-sensing gripping pads used for manipulating box-like objects with smaller-sized humanoid robots. These pads measure normal gripping forces and center of pressure (CoP). A calibration method is developed to improve the CoP measurement accuracy. A hybrid force-alignment-position control framework is proposed to regulate the gripping forces and to ensure the surface alignment between the grippers and the object. Limit surface theory is incorporated as a contact friction modeling approach to determine the magnitude of gripping forces for slippage avoidance. The integrated hardware and software system is demonstrated with a NAO humanoid robot. Experiments show the effectiveness of the overall approach.","sentences":["This paper introduces a pair of low-cost, light-weight and compliant force-sensing gripping pads used for manipulating box-like objects with smaller-sized humanoid robots.","These pads measure normal gripping forces and center of pressure (CoP).","A calibration method is developed to improve the CoP measurement accuracy.","A hybrid force-alignment-position control framework is proposed to regulate the gripping forces and to ensure the surface alignment between the grippers and the object.","Limit surface theory is incorporated as a contact friction modeling approach to determine the magnitude of gripping forces for slippage avoidance.","The integrated hardware and software system is demonstrated with a NAO humanoid robot.","Experiments show the effectiveness of the overall approach."],"url":"http://arxiv.org/abs/2405.20969v1","category":"cs.RO"}
{"created":"2024-05-31 15:50:46","title":"Quantum computation with hybrid parafermion-spin qubits","abstract":"We propose a universal set of single- and two-qubit quantum gates acting on a hybrid qubit formed by coupling a quantum dot spin qubit to a $\\mathbb{Z}_{2m}$ parafermion qubit with arbitrary integer $m$. The special case $m=1$ reproduces the results previously derived for Majorana qubits. Our formalism utilizes Fock parafermions, facilitating a transparent treatment of hybrid parafermion-spin systems. Furthermore, we highlight the previously overlooked importance of particle-hole symmetry in these systems. We give concrete examples how the hybrid qubit system could be realized experimentally for $\\mathbb{Z}_4$ and $\\mathbb{Z}_6$ parafermions. In addition, we discuss a simple readout scheme for the fractional parafermion charge via the measurement of the spin qubit resonant frequency.","sentences":["We propose a universal set of single- and two-qubit quantum gates acting on a hybrid qubit formed by coupling a quantum dot spin qubit to a $\\mathbb{Z}_{2m}$ parafermion qubit with arbitrary integer $m$. The special case $m=1$ reproduces the results previously derived for Majorana qubits.","Our formalism utilizes Fock parafermions, facilitating a transparent treatment of hybrid parafermion-spin systems.","Furthermore, we highlight the previously overlooked importance of particle-hole symmetry in these systems.","We give concrete examples how the hybrid qubit system could be realized experimentally for $\\mathbb{Z}_4$ and $\\mathbb{Z}_6$ parafermions.","In addition, we discuss a simple readout scheme for the fractional parafermion charge via the measurement of the spin qubit resonant frequency."],"url":"http://arxiv.org/abs/2405.20950v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-31 15:48:19","title":"Uniform asymptotic stability of a PDE's system arising from a flexible robotics model","abstract":"In this paper we investigate the asymptotic stability of a fourth-order PDE with a fading memory forcing term and boundary conditions arising from a flexible robotics model. We carry on our study by using an abstract formulation of the problem based on the $C_0$-semigroup. To achieve our objective, we first provide new results on the existence, uniqueness, continuous dependence on initial data of either mild and strong solutions for semilinear integro-differential equations in Banach spaces. Then, we also find sufficient conditions for the uniform asymptotic stability of solutions and for the existence of attactors. As an application of these abstract results, we can ensure existence, uniqueness and continuous dependence on initial data for the solutions of the boundary value problem under investigation and, finally, we prove the uniform asymptotic stability of solutions and the existence of attactors under suitable conditions on the nonlinear term.","sentences":["In this paper we investigate the asymptotic stability of a fourth-order PDE with a fading memory forcing term and boundary conditions arising from a flexible robotics model.","We carry on our study by using an abstract formulation of the problem based on the $C_0$-semigroup.","To achieve our objective, we first provide new results on the existence, uniqueness, continuous dependence on initial data of either mild and strong solutions for semilinear integro-differential equations in Banach spaces.","Then, we also find sufficient conditions for the uniform asymptotic stability of solutions and for the existence of attactors.","As an application of these abstract results, we can ensure existence, uniqueness and continuous dependence on initial data for the solutions of the boundary value problem under investigation and, finally, we prove the uniform asymptotic stability of solutions and the existence of attactors under suitable conditions on the nonlinear term."],"url":"http://arxiv.org/abs/2405.20949v1","category":"math.AP"}
{"created":"2024-05-31 15:43:29","title":"A criterion to detect a nontrivial homology of an invariant set of a flow in $\\mathbb{R}^3$","abstract":"Consider a flow in $\\mathbb{R}^3$ and let $K$ be the biggest invariant subset of some compact region of interest $N \\subseteq \\mathbb{R}^3$. The set $K$ is often not computable, but the way the flow crosses the boundary of $N$ can provide indirect information about it. For example, classical tools such as Wa\\.{z}ewski's principle or the Poincar\\'e-Hopf theorem can be used to detect whether $K$ is nonempty or contains rest points, respectively. We present a criterion that can establish whether $K$ has a nontrivial homology by looking at the subset of the boundary of $N$ along which the flow is tangent to $N$. We prove that the criterion is as sharp as possible with the information it uses as an input. We also show that it is algorithmically checkable.","sentences":["Consider a flow in $\\mathbb{R}^3$ and let $K$ be the biggest invariant subset of some compact region of interest $N \\subseteq \\mathbb{R}^3$. The set $K$ is often not computable, but the way the flow crosses the boundary of $N$ can provide indirect information about it.","For example, classical tools such as Wa\\.{z}ewski's principle or the Poincar\\'e-Hopf theorem can be used to detect whether $K$ is nonempty or contains rest points, respectively.","We present a criterion that can establish whether $K$ has a nontrivial homology by looking at the subset of the boundary of $N$ along which the flow is tangent to $N$. We prove that the criterion is as sharp as possible with the information it uses as an input.","We also show that it is algorithmically checkable."],"url":"http://arxiv.org/abs/2405.20945v1","category":"math.DS"}
{"created":"2024-05-31 15:37:19","title":"Activity-driven polymer knotting for macromolecular topology engineering","abstract":"Macromolecules can gain special properties by adopting knotted conformations, but engineering knotted macromolecules is a challenging task. Here we surprisingly observed that knotting can be very effectively produced in active polymers. When one end of an actively reptative polymer is anchored, it can undergo continual self-knotting as a result of intermittent giant conformation fluctuations and the outward reptative motion. Once a knot is formed, it migrates to the anchored point due to a non-equilibrium ratchet effect. Moreover, when the active polymer is grafted on the end of a passive polymer, it can function as a self-propelling soft needle to either transfer its own knots to the passive polymer or directly braid knots on the passive polymer. We further show that these active needles can create inter-molecular bridging knots between two passive polymers. Our finding highlights the non-equilibrium effects in modifying the dynamic pathways of polymer systems, which have potential applications in macromolecular topology engineering, e.g., manipulating topological states of proteins and nucleic acids, as well as macromolecular braiding.","sentences":["Macromolecules can gain special properties by adopting knotted conformations, but engineering knotted macromolecules is a challenging task.","Here we surprisingly observed that knotting can be very effectively produced in active polymers.","When one end of an actively reptative polymer is anchored, it can undergo continual self-knotting as a result of intermittent giant conformation fluctuations and the outward reptative motion.","Once a knot is formed, it migrates to the anchored point due to a non-equilibrium ratchet effect.","Moreover, when the active polymer is grafted on the end of a passive polymer, it can function as a self-propelling soft needle to either transfer its own knots to the passive polymer or directly braid knots on the passive polymer.","We further show that these active needles can create inter-molecular bridging knots between two passive polymers.","Our finding highlights the non-equilibrium effects in modifying the dynamic pathways of polymer systems, which have potential applications in macromolecular topology engineering, e.g., manipulating topological states of proteins and nucleic acids, as well as macromolecular braiding."],"url":"http://arxiv.org/abs/2405.20938v1","category":"cond-mat.soft"}
{"created":"2024-05-31 15:32:13","title":"Normalization and cut-elimination theorems for some logics of evidence and truth","abstract":"In this paper, we investigate proof-theoretic aspects of the logics of evidence and truth LETJ and LETF. These logics extend, respectively, Nelson's logic N and the logic of first-degree entailment FDE, also known as Belnap-Dunn four-valued logic, with a classicality operator that recovers classical logic for formulas in its scope. We will present natural deduction and sequent systems for LETJ and LETF, together with proofs of normalization and cut-elimination theorems, respectively. As a corollary, we obtain decidability for both logics.","sentences":["In this paper, we investigate proof-theoretic aspects of the logics of evidence and truth LETJ and LETF.","These logics extend, respectively, Nelson's logic N and the logic of first-degree entailment FDE, also known as Belnap-Dunn four-valued logic, with a classicality operator that recovers classical logic for formulas in its scope.","We will present natural deduction and sequent systems for LETJ and LETF, together with proofs of normalization and cut-elimination theorems, respectively.","As a corollary, we obtain decidability for both logics."],"url":"http://arxiv.org/abs/2405.20932v1","category":"math.LO"}
{"created":"2024-05-31 15:26:56","title":"Successive vanishing on curves","abstract":"We investigate the emptiness of adjoint linear systems associated to successive multiples of a given positive divisor with real coefficients","sentences":["We investigate the emptiness of adjoint linear systems associated to successive multiples of a given positive divisor with real coefficients"],"url":"http://arxiv.org/abs/2405.20924v1","category":"math.AG"}
{"created":"2024-05-31 15:25:25","title":"Demonstration of 0-pi transition in Josephson junctions containing unbalanced synthetic antiferromagnets","abstract":"Josephson junctions containing ferromagnetic (F) materials have been the subject of intense study over the past two decades. The ground state of such junctions oscillates between 0 and pi as the thickness of the ferromagnetic layer increases. For some applications, it might be beneficial to replace a very thin F layer with an unbalanced synthetic antiferromagnet (SAF) consisting of two F layers of different thicknesses whose magnetizations are coupled antiparallel to each other. According to theory, such a system should behave similarly to a single F layer whose thickness is equal to the difference of the two F-layer thicknesses in the SAF. We test that theoretical prediction with Josephson junctions containing unbalanced Ni/Ru/Ni SAFs, keeping the thickness of one layer fixed at 2.0 nm and varying the thickness of the other layer between 2.0 and 5.0 nm. We observe the first 0-pi transition at a thickness difference of 0.86 nm, which closely matches the position of the transition observed previously using single Ni layers.","sentences":["Josephson junctions containing ferromagnetic (F) materials have been the subject of intense study over the past two decades.","The ground state of such junctions oscillates between 0 and pi as the thickness of the ferromagnetic layer increases.","For some applications, it might be beneficial to replace a very thin F layer with an unbalanced synthetic antiferromagnet (SAF) consisting of two F layers of different thicknesses whose magnetizations are coupled antiparallel to each other.","According to theory, such a system should behave similarly to a single F layer whose thickness is equal to the difference of the two F-layer thicknesses in the SAF.","We test that theoretical prediction with Josephson junctions containing unbalanced Ni/Ru/Ni SAFs, keeping the thickness of one layer fixed at 2.0 nm and varying the thickness of the other layer between 2.0 and 5.0 nm.","We observe the first 0-pi transition at a thickness difference of 0.86 nm, which closely matches the position of the transition observed previously using single Ni layers."],"url":"http://arxiv.org/abs/2405.20922v1","category":"cond-mat.supr-con"}
{"created":"2024-05-31 15:22:58","title":"Finite-size spectrum of the staggered six-vertex model with antidiagonal boundary conditions","abstract":"The finite-size spectrum of the critical staggered six-vertex model with antidiagonal boundary conditions is studied. Similar to the case of periodic boundary conditions, we identify three different phases. In two of those, the underlying conformal field theory can be identified to be related to the twisted $U(1)$ Kac-Moody algebra. In contrast, the finite size scaling in the third regime, whose critical behaviour with the quasi-periodic BCs is described by the $SL(2,\\mathbb{R})_k/U(1)$ black hole CFT possessing a non-compact degree of freedom, is more subtle. Here with antidiagonal BCs imposed, the corrections to the scaling of the ground state grow logarithmically with the system size, while the energy gaps appear to close logarithmically. Moreover, we obtain an explicit formula for the Q-operator which is useful for numerical implementation.","sentences":["The finite-size spectrum of the critical staggered six-vertex model with antidiagonal boundary conditions is studied.","Similar to the case of periodic boundary conditions, we identify three different phases.","In two of those, the underlying conformal field theory can be identified to be related to the twisted $U(1)$ Kac-Moody algebra.","In contrast, the finite size scaling in the third regime, whose critical behaviour with the quasi-periodic BCs is described by the $SL(2,\\mathbb{R})_k/U(1)$ black hole CFT possessing a non-compact degree of freedom, is more subtle.","Here with antidiagonal BCs imposed, the corrections to the scaling of the ground state grow logarithmically with the system size, while the energy gaps appear to close logarithmically.","Moreover, we obtain an explicit formula for the Q-operator which is useful for numerical implementation."],"url":"http://arxiv.org/abs/2405.20919v1","category":"hep-th"}
{"created":"2024-05-31 15:21:26","title":"Coexisting charge density waves in twisted bilayer NbSe2","abstract":"Twisted bilayers of two-dimensional materials have emerged as a highly tunable platform for studying broken symmetry phases. While most interest has been focused on emergent states in systems whose constituent monolayers do not feature broken symmetry states, assembling monolayers that exhibit ordered states into twisted bilayers can also give rise to interesting phenomena. Here, we use large-scale first-principles density-functional theory calculations to study the atomic structure of twisted bilayer $\\mathrm{{N}b{S}e_2}$ whose constituent monolayers feature a charge density wave. We find that different charge density wave states coexist in the ground state of the twisted bilayer: monolayer-like $3\\times 3$ triangular and hexagonal charge density waves are observed in low-energy stacking regions, while stripe charge density waves are found in the domain walls surrounding the low-energy stacking regions. These predictions, which can be tested by scanning tunneling microscopy experiments, highlight the potential to create complex charge density wave ground states in twisted bilayer systems and can serve as a starting point for understanding superconductivity occurring at low temperatures.","sentences":["Twisted bilayers of two-dimensional materials have emerged as a highly tunable platform for studying broken symmetry phases.","While most interest has been focused on emergent states in systems whose constituent monolayers do not feature broken symmetry states, assembling monolayers that exhibit ordered states into twisted bilayers can also give rise to interesting phenomena.","Here, we use large-scale first-principles density-functional theory calculations to study the atomic structure of twisted bilayer $\\mathrm{{N}b{S}e_2}$ whose constituent monolayers feature a charge density wave.","We find that different charge density wave states coexist in the ground state of the twisted bilayer: monolayer-like $3\\times 3$ triangular and hexagonal charge density waves are observed in low-energy stacking regions, while stripe charge density waves are found in the domain walls surrounding the low-energy stacking regions.","These predictions, which can be tested by scanning tunneling microscopy experiments, highlight the potential to create complex charge density wave ground states in twisted bilayer systems and can serve as a starting point for understanding superconductivity occurring at low temperatures."],"url":"http://arxiv.org/abs/2405.20913v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 15:18:22","title":"Note on homoclinic solutions to nonautonomous Hamiltonian systems with sign-changing nonlinear part","abstract":"In the paper, we utilize the recent variational, abstract theorem to show the existence of homoclinic solutions to the Hamiltonian system $$ \\dot{z} = J D_z H(z, t), \\quad t \\in \\mathbb{R}, $$ where the Hamiltonian $H : \\mathbb{R}^{2N} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is of the form $$ H(z, t) = \\frac12 Az \\cdot z + \\Gamma(t) \\left( F(z) - \\lambda G(z) \\right) $$ for some symmetric matrix $A$.","sentences":["In the paper, we utilize the recent variational, abstract theorem to show the existence of homoclinic solutions to the Hamiltonian system $$ \\dot{z} = J D_z H(z, t), \\quad t \\in \\mathbb{R}, $$ where the Hamiltonian $H : \\mathbb{R}^{2N} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is of the form $$ H(z, t) = \\frac12 Az \\cdot z + \\Gamma(t)","\\left( F(z) - \\lambda G(z) \\right) $$ for some symmetric matrix $A$."],"url":"http://arxiv.org/abs/2405.20908v1","category":"math.CA"}
{"created":"2024-05-31 15:16:48","title":"VENI, VINDy, VICI: a variational reduced-order modeling framework with uncertainty quantification","abstract":"The simulation of many complex phenomena in engineering and science requires solving expensive, high-dimensional systems of partial differential equations (PDEs). To circumvent this, reduced-order models (ROMs) have been developed to speed up computations. However, when governing equations are unknown or partially known, typically ROMs lack interpretability and reliability of the predicted solutions.   In this work we present a data-driven, non-intrusive framework for building ROMs where the latent variables and dynamics are identified in an interpretable manner and uncertainty is quantified. Starting from a limited amount of high-dimensional, noisy data the proposed framework constructs an efficient ROM by leveraging variational autoencoders for dimensionality reduction along with a newly introduced, variational version of sparse identification of nonlinear dynamics (SINDy), which we refer to as Variational Identification of Nonlinear Dynamics (VINDy).   In detail, the method consists of Variational Encoding of Noisy Inputs (VENI) to identify the distribution of reduced coordinates. Simultaneously, we learn the distribution of the coefficients of a pre-determined set of candidate functions by VINDy. Once trained offline, the identified model can be queried for new parameter instances and new initial conditions to compute the corresponding full-time solutions. The probabilistic setup enables uncertainty quantification as the online testing consists of Variational Inference naturally providing Certainty Intervals (VICI). In this work we showcase the effectiveness of the newly proposed VINDy method in identifying interpretable and accurate dynamical system for the R\\\"ossler system with different noise intensities and sources. Then the performance of the overall method - named VENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics and fluid dynamics.","sentences":["The simulation of many complex phenomena in engineering and science requires solving expensive, high-dimensional systems of partial differential equations (PDEs).","To circumvent this, reduced-order models (ROMs) have been developed to speed up computations.","However, when governing equations are unknown or partially known, typically ROMs lack interpretability and reliability of the predicted solutions.   ","In this work we present a data-driven, non-intrusive framework for building ROMs where the latent variables and dynamics are identified in an interpretable manner and uncertainty is quantified.","Starting from a limited amount of high-dimensional, noisy data the proposed framework constructs an efficient ROM by leveraging variational autoencoders for dimensionality reduction along with a newly introduced, variational version of sparse identification of nonlinear dynamics (SINDy), which we refer to as Variational Identification of Nonlinear Dynamics (VINDy).   ","In detail, the method consists of Variational Encoding of Noisy Inputs (VENI) to identify the distribution of reduced coordinates.","Simultaneously, we learn the distribution of the coefficients of a pre-determined set of candidate functions by VINDy.","Once trained offline, the identified model can be queried for new parameter instances and new initial conditions to compute the corresponding full-time solutions.","The probabilistic setup enables uncertainty quantification as the online testing consists of Variational Inference naturally providing Certainty Intervals (VICI).","In this work we showcase the effectiveness of the newly proposed VINDy method in identifying interpretable and accurate dynamical system for the R\\\"ossler system with different noise intensities and sources.","Then the performance of the overall method - named VENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics and fluid dynamics."],"url":"http://arxiv.org/abs/2405.20905v1","category":"cs.LG"}
{"created":"2024-05-31 15:15:51","title":"Solving systems of equations on antichains for the computation of the ninth Dedekind Number","abstract":"We study three systems of equations, together with a way to count the number of solutions. One of the results was used in the recent computation of D(9), the others have potential to speed up existing techniques in the future.","sentences":["We study three systems of equations, together with a way to count the number of solutions.","One of the results was used in the recent computation of D(9), the others have potential to speed up existing techniques in the future."],"url":"http://arxiv.org/abs/2405.20904v1","category":"math.CO"}
{"created":"2024-05-31 15:15:29","title":"Gaussian Framework and Optimal Projection of Weather Fields for Prediction of Extreme Events","abstract":"Extreme events are the major weather related hazard for humanity. It is then of crucial importance to have a good understanding of their statistics and to be able to forecast them. However, lack of sufficient data makes their study particularly challenging.   In this work we provide a simple framework to study extreme events that tackles the lack of data issue by using the whole dataset available, rather than focusing on the extremes in the dataset. To do so, we make the assumption that the set of predictors and the observable used to define the extreme event follow a jointly Gaussian distribution. This naturally gives the notion of an optimal projection of the predictors for forecasting the event.   We take as a case study extreme heatwaves over France, and we test our method on an 8000-year-long intermediate complexity climate model time series and on the ERA5 reanalysis dataset.   For a-posteriori statistics, we observe and motivate the fact that composite maps of very extreme events look similar to less extreme ones.   For prediction, we show that our method is competitive with off-the-shelf neural networks on the long dataset and outperforms them on reanalysis.   The optimal projection pattern, which makes our forecast intrinsically interpretable, highlights the importance of soil moisture deficit and quasi-stationary Rossby waves as precursors to extreme heatwaves.","sentences":["Extreme events are the major weather related hazard for humanity.","It is then of crucial importance to have a good understanding of their statistics and to be able to forecast them.","However, lack of sufficient data makes their study particularly challenging.   ","In this work we provide a simple framework to study extreme events that tackles the lack of data issue by using the whole dataset available, rather than focusing on the extremes in the dataset.","To do so, we make the assumption that the set of predictors and the observable used to define the extreme event follow a jointly Gaussian distribution.","This naturally gives the notion of an optimal projection of the predictors for forecasting the event.   ","We take as a case study extreme heatwaves over France, and we test our method on an 8000-year-long intermediate complexity climate model time series and on the ERA5 reanalysis dataset.   ","For a-posteriori statistics, we observe and motivate the fact that composite maps of very extreme events look similar to less extreme ones.   ","For prediction, we show that our method is competitive with off-the-shelf neural networks on the long dataset and outperforms them on reanalysis.   ","The optimal projection pattern, which makes our forecast intrinsically interpretable, highlights the importance of soil moisture deficit and quasi-stationary Rossby waves as precursors to extreme heatwaves."],"url":"http://arxiv.org/abs/2405.20903v1","category":"physics.ao-ph"}
{"created":"2024-05-31 15:12:33","title":"Large Language Models: A New Approach for Privacy Policy Analysis at Scale","abstract":"The number and dynamic nature of web and mobile applications presents significant challenges for assessing their compliance with data protection laws. In this context, symbolic and statistical Natural Language Processing (NLP) techniques have been employed for the automated analysis of these systems' privacy policies. However, these techniques typically require labor-intensive and potentially error-prone manually annotated datasets for training and validation. This research proposes the application of Large Language Models (LLMs) as an alternative for effectively and efficiently extracting privacy practices from privacy policies at scale. Particularly, we leverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the optimal design of prompts, parameters, and models, incorporating advanced strategies such as few-shot learning. We further illustrate its capability to detect detailed and varied privacy practices accurately. Using several renowned datasets in the domain as a benchmark, our evaluation validates its exceptional performance, achieving an F1 score exceeding 93%. Besides, it does so with reduced costs, faster processing times, and fewer technical knowledge requirements. Consequently, we advocate for LLM-based solutions as a sound alternative to traditional NLP techniques for the automated analysis of privacy policies at scale.","sentences":["The number and dynamic nature of web and mobile applications presents significant challenges for assessing their compliance with data protection laws.","In this context, symbolic and statistical Natural Language Processing (NLP) techniques have been employed for the automated analysis of these systems' privacy policies.","However, these techniques typically require labor-intensive and potentially error-prone manually annotated datasets for training and validation.","This research proposes the application of Large Language Models (LLMs) as an alternative for effectively and efficiently extracting privacy practices from privacy policies at scale.","Particularly, we leverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the optimal design of prompts, parameters, and models, incorporating advanced strategies such as few-shot learning.","We further illustrate its capability to detect detailed and varied privacy practices accurately.","Using several renowned datasets in the domain as a benchmark, our evaluation validates its exceptional performance, achieving an F1 score exceeding 93%.","Besides, it does so with reduced costs, faster processing times, and fewer technical knowledge requirements.","Consequently, we advocate for LLM-based solutions as a sound alternative to traditional NLP techniques for the automated analysis of privacy policies at scale."],"url":"http://arxiv.org/abs/2405.20900v1","category":"cs.CL"}
{"created":"2024-05-31 15:05:05","title":"SPARROW: Smart Precision Agriculture Robot for Ridding of Weeds","abstract":"The advancements in precision agriculture are vital to support the increasing demand for global food supply. Precision spot spraying is a major step towards reducing chemical usage for pest and weed control in agriculture. A novel spot spraying algorithm that autonomously detects weeds and performs trajectory planning for the sprayer nozzle has been proposed. Furthermore, this research introduces a vision-based autonomous navigation system that operates through the detected crop row, effectively synchronizing with an autonomous spraying algorithm. This proposed system is characterized by its cost effectiveness that enable the autonomous spraying of herbicides onto detected weeds.","sentences":["The advancements in precision agriculture are vital to support the increasing demand for global food supply.","Precision spot spraying is a major step towards reducing chemical usage for pest and weed control in agriculture.","A novel spot spraying algorithm that autonomously detects weeds and performs trajectory planning for the sprayer nozzle has been proposed.","Furthermore, this research introduces a vision-based autonomous navigation system that operates through the detected crop row, effectively synchronizing with an autonomous spraying algorithm.","This proposed system is characterized by its cost effectiveness that enable the autonomous spraying of herbicides onto detected weeds."],"url":"http://arxiv.org/abs/2405.20896v1","category":"cs.RO"}
{"created":"2024-05-31 15:04:11","title":"Existence of solutions to k-Wave models of nonlinear ultrasound propagation in biological tissue","abstract":"We investigate models for nonlinear ultrasound propagation in soft biological tissue based on the one that serves as the core for the software package k-Wave. The systems are solved for the acoustic particle velocity, mass density, and acoustic pressure and involve a fractional absorption operator. We first consider a system that incorporates additional viscosity in the equation for momentum conservation. By constructing a Galerkin approximation procedure, we prove the local existence of its solutions. In view of inverse problems arising from imaging tasks, the theory allows for the variable background mass density, speed of sound, and the nonlinearity parameter in the systems. Secondly, under stronger conditions on the data, we take the vanishing viscosity limit of the problem, thereby rigorously establishing the existence of solutions for the limiting system as well.","sentences":["We investigate models for nonlinear ultrasound propagation in soft biological tissue based on the one that serves as the core for the software package k-Wave.","The systems are solved for the acoustic particle velocity, mass density, and acoustic pressure and involve a fractional absorption operator.","We first consider a system that incorporates additional viscosity in the equation for momentum conservation.","By constructing a Galerkin approximation procedure, we prove the local existence of its solutions.","In view of inverse problems arising from imaging tasks, the theory allows for the variable background mass density, speed of sound, and the nonlinearity parameter in the systems.","Secondly, under stronger conditions on the data, we take the vanishing viscosity limit of the problem, thereby rigorously establishing the existence of solutions for the limiting system as well."],"url":"http://arxiv.org/abs/2405.20894v1","category":"math.AP"}
{"created":"2024-05-31 15:01:54","title":"The Role of Bases in Quantum Optimal Control","abstract":"Quantum Optimal Control (QOC) supports the advance of quantum technologies by tackling its problems at the pulse level: Numerical approaches iteratively work towards a given target by parametrising the applied time-dependent fields with a finite set of variables. The effectiveness of the resulting optimisation depends on the complexity of the problem and the number of variables. We consider different parametrisations in terms of basis functions, asking whether the choice of the applied basis affects the quality of the optimisation. Furthermore, we consider strategies to choose the most suitable basis. For the comparison, we test three different randomisable bases - introducing the sinc and sigmoid bases as alternatives to the Fourier basis - on QOC problems of varying complexity. For each problem, the basis-specific convergence rates result in a unique ranking. Especially for expensive evaluations, e.g., in closed-loop, a potential speed-up by a factor of up to 10 may be crucial for the optimisation's feasibility. We conclude that a problem-dependent basis choice is an influential factor for QOC efficiency and provide advice for its approach.","sentences":["Quantum Optimal Control (QOC) supports the advance of quantum technologies by tackling its problems at the pulse level: Numerical approaches iteratively work towards a given target by parametrising the applied time-dependent fields with a finite set of variables.","The effectiveness of the resulting optimisation depends on the complexity of the problem and the number of variables.","We consider different parametrisations in terms of basis functions, asking whether the choice of the applied basis affects the quality of the optimisation.","Furthermore, we consider strategies to choose the most suitable basis.","For the comparison, we test three different randomisable bases - introducing the sinc and sigmoid bases as alternatives to the Fourier basis - on QOC problems of varying complexity.","For each problem, the basis-specific convergence rates result in a unique ranking.","Especially for expensive evaluations, e.g., in closed-loop, a potential speed-up by a factor of up to 10 may be crucial for the optimisation's feasibility.","We conclude that a problem-dependent basis choice is an influential factor for QOC efficiency and provide advice for its approach."],"url":"http://arxiv.org/abs/2405.20889v1","category":"quant-ph"}
{"created":"2024-05-31 14:55:44","title":"Scalable Distance-based Multi-Agent Relative State Estimation via Block Multiconvex Optimization","abstract":"This paper explores the distance-based relative state estimation problem in large-scale systems, which is hard to solve effectively due to its high-dimensionality and non-convexity. In this paper, we alleviate this inherent hardness to simultaneously achieve scalability and robustness of inference on this problem. Our idea is launched from a universal geometric formulation, called \\emph{generalized graph realization}, for the distance-based relative state estimation problem. Based on this formulation, we introduce two collaborative optimization models, one of which is convex and thus globally solvable, and the other enables fast searching on non-convex landscapes to refine the solution offered by the convex one. Importantly, both models enjoy \\emph{multiconvex} and \\emph{decomposable} structures, allowing efficient and safe solutions using \\emph{block coordinate descent} that enjoys scalability and a distributed nature. The proposed algorithms collaborate to demonstrate superior or comparable solution precision to the current centralized convex relaxation-based methods, which are known for their high optimality. Distinctly, the proposed methods demonstrate scalability beyond the reach of previous convex relaxation-based methods. We also demonstrate that the combination of the two proposed algorithms achieves a more robust pipeline than deploying the local search method alone in a continuous-time scenario.","sentences":["This paper explores the distance-based relative state estimation problem in large-scale systems, which is hard to solve effectively due to its high-dimensionality and non-convexity.","In this paper, we alleviate this inherent hardness to simultaneously achieve scalability and robustness of inference on this problem.","Our idea is launched from a universal geometric formulation, called \\emph{generalized graph realization}, for the distance-based relative state estimation problem.","Based on this formulation, we introduce two collaborative optimization models, one of which is convex and thus globally solvable, and the other enables fast searching on non-convex landscapes to refine the solution offered by the convex one.","Importantly, both models enjoy \\emph{multiconvex} and \\emph{decomposable} structures, allowing efficient and safe solutions using \\emph{block coordinate descent} that enjoys scalability and a distributed nature.","The proposed algorithms collaborate to demonstrate superior or comparable solution precision to the current centralized convex relaxation-based methods, which are known for their high optimality.","Distinctly, the proposed methods demonstrate scalability beyond the reach of previous convex relaxation-based methods.","We also demonstrate that the combination of the two proposed algorithms achieves a more robust pipeline than deploying the local search method alone in a continuous-time scenario."],"url":"http://arxiv.org/abs/2405.20883v1","category":"cs.RO"}
{"created":"2024-05-31 14:55:38","title":"Sheaf HyperNetworks for Personalized Federated Learning","abstract":"Graph hypernetworks (GHNs), constructed by combining graph neural networks (GNNs) with hypernetworks (HNs), leverage relational data across various domains such as neural architecture search, molecular property prediction and federated learning. Despite GNNs and HNs being individually successful, we show that GHNs present problems compromising their performance, such as over-smoothing and heterophily. Moreover, we cannot apply GHNs directly to personalized federated learning (PFL) scenarios, where a priori client relation graph may be absent, private, or inaccessible. To mitigate these limitations in the context of PFL, we propose a novel class of HNs, sheaf hypernetworks (SHNs), which combine cellular sheaf theory with HNs to improve parameter sharing for PFL. We thoroughly evaluate SHNs across diverse PFL tasks, including multi-class classification, traffic and weather forecasting. Additionally, we provide a methodology for constructing client relation graphs in scenarios where such graphs are unavailable. We show that SHNs consistently outperform existing PFL solutions in complex non-IID scenarios. While the baselines' performance fluctuates depending on the task, SHNs show improvements of up to 2.7% in accuracy and 5.3% in lower mean squared error over the best-performing baseline.","sentences":["Graph hypernetworks (GHNs), constructed by combining graph neural networks (GNNs) with hypernetworks (HNs), leverage relational data across various domains such as neural architecture search, molecular property prediction and federated learning.","Despite GNNs and HNs being individually successful, we show that GHNs present problems compromising their performance, such as over-smoothing and heterophily.","Moreover, we cannot apply GHNs directly to personalized federated learning (PFL) scenarios, where a priori client relation graph may be absent, private, or inaccessible.","To mitigate these limitations in the context of PFL, we propose a novel class of HNs, sheaf hypernetworks (SHNs), which combine cellular sheaf theory with HNs to improve parameter sharing for PFL.","We thoroughly evaluate SHNs across diverse PFL tasks, including multi-class classification, traffic and weather forecasting.","Additionally, we provide a methodology for constructing client relation graphs in scenarios where such graphs are unavailable.","We show that SHNs consistently outperform existing PFL solutions in complex non-IID scenarios.","While the baselines' performance fluctuates depending on the task, SHNs show improvements of up to 2.7% in accuracy and 5.3% in lower mean squared error over the best-performing baseline."],"url":"http://arxiv.org/abs/2405.20882v1","category":"cs.LG"}
{"created":"2024-05-31 14:50:58","title":"Mutually unbiased bases via complex projective trigonometry","abstract":"We give a synthetic construction of a complete system of mutually unbiased bases in $\\mathbb{C}^3$.","sentences":["We give a synthetic construction of a complete system of mutually unbiased bases in $\\mathbb{C}^3$."],"url":"http://arxiv.org/abs/2405.20873v1","category":"math.DG"}
{"created":"2024-05-31 14:48:43","title":"Ultrafast optical switching to a heterochiral charge-density wave state","abstract":"Optical control of correlated electronic states promises unprecedented tunability of novel functional materials. Tailored optical excitations can steer a system along non-equilibrium pathways to metastable states with specific structural or electronic properties. A much-desired feature is the reproducible and ultrafast switching to functional states. The light-induced hidden state of 1T-TaS$_{2}$, with its strongly enhanced conductivity and exceptionally long lifetime, represents a unique model system for studying the switching of correlated electronic states using femtosecond optical stimuli. However, despite intense investigation, the switching mechanism and the structural origins of the distinctive electronic properties of the hidden state have not been fully uncovered. Here, we use surface-sensitive electron diffraction in combination with a femtosecond optical quench to reveal coexistent charge-density wave chiralities as a new structural feature of the hidden state. We find that a single-pulse optical quench produces a state with long-range structural order and different weights of the two chiral enantiomorphs of the charge-density wave. Harnessing a double-pulse optical quench, we trace the origin of the mixed chirality to the transient electronic excitation of the host crystal. The coexistent long-range-order of both chiralities suggests the presence of extended heterochiral charge-density wave interfaces, which results in a higher-level, fractal-type moir\\'{e} superstructure. Density functional theory simulations for such a charge-density wave moir\\'{e} superstructure reveal multiple flat bands, Dirac cones, and a kagome electronic subsystem around the Fermi energy. Our findings shed light on novel electronic properties gained by chiral interface engineering, and create avenues for light-induced moir\\'{e} superstructures in quasi-two-dimensional materials.","sentences":["Optical control of correlated electronic states promises unprecedented tunability of novel functional materials.","Tailored optical excitations can steer a system along non-equilibrium pathways to metastable states with specific structural or electronic properties.","A much-desired feature is the reproducible and ultrafast switching to functional states.","The light-induced hidden state of 1T-TaS$_{2}$, with its strongly enhanced conductivity and exceptionally long lifetime, represents a unique model system for studying the switching of correlated electronic states using femtosecond optical stimuli.","However, despite intense investigation, the switching mechanism and the structural origins of the distinctive electronic properties of the hidden state have not been fully uncovered.","Here, we use surface-sensitive electron diffraction in combination with a femtosecond optical quench to reveal coexistent charge-density wave chiralities as a new structural feature of the hidden state.","We find that a single-pulse optical quench produces a state with long-range structural order and different weights of the two chiral enantiomorphs of the charge-density wave.","Harnessing a double-pulse optical quench, we trace the origin of the mixed chirality to the transient electronic excitation of the host crystal.","The coexistent long-range-order of both chiralities suggests the presence of extended heterochiral charge-density wave interfaces, which results in a higher-level, fractal-type moir\\'{e} superstructure.","Density functional theory simulations for such a charge-density wave moir\\'{e} superstructure reveal multiple flat bands, Dirac cones, and a kagome electronic subsystem around the Fermi energy.","Our findings shed light on novel electronic properties gained by chiral interface engineering, and create avenues for light-induced moir\\'{e} superstructures in quasi-two-dimensional materials."],"url":"http://arxiv.org/abs/2405.20872v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-31 14:44:57","title":"BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning","abstract":"In a federated learning (FL) system, decentralized data owners (clients) could upload their locally trained models to a central server, to jointly train a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing misclassification to a target class when encountering attacker-defined triggers. Existing backdoor defenses show inconsistent performance under different system and adversarial settings, especially when the malicious updates are made statistically close to the benign ones. In this paper, we first reveal the fact that planting subsequent backdoors with the same target label could significantly help to maintain the accuracy of previously planted backdoors, and then propose a novel proactive backdoor detection mechanism for FL named BackdoorIndicator, which has the server inject indicator tasks into the global model leveraging out-of-distribution (OOD) data, and then utilizing the fact that any backdoor samples are OOD samples with respect to benign samples, the server, who is completely agnostic of the potential backdoor types and target labels, can accurately detect the presence of backdoors in uploaded models, via evaluating the indicator tasks. We perform systematic and extensive empirical studies to demonstrate the consistently superior performance and practicality of BackdoorIndicator over baseline defenses, across a wide range of system and adversarial settings.","sentences":["In a federated learning (FL) system, decentralized data owners (clients) could upload their locally trained models to a central server, to jointly train a global model.","Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing misclassification to a target class when encountering attacker-defined triggers.","Existing backdoor defenses show inconsistent performance under different system and adversarial settings, especially when the malicious updates are made statistically close to the benign ones.","In this paper, we first reveal the fact that planting subsequent backdoors with the same target label could significantly help to maintain the accuracy of previously planted backdoors, and then propose a novel proactive backdoor detection mechanism for FL named BackdoorIndicator, which has the server inject indicator tasks into the global model leveraging out-of-distribution (OOD) data, and then utilizing the fact that any backdoor samples are OOD samples with respect to benign samples, the server, who is completely agnostic of the potential backdoor types and target labels, can accurately detect the presence of backdoors in uploaded models, via evaluating the indicator tasks.","We perform systematic and extensive empirical studies to demonstrate the consistently superior performance and practicality of BackdoorIndicator over baseline defenses, across a wide range of system and adversarial settings."],"url":"http://arxiv.org/abs/2405.20862v1","category":"cs.CR"}
{"created":"2024-05-31 14:44:05","title":"Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation","abstract":"Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29% fewer samples than baselines, and reduces training time by 21--38%.","sentences":["Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints.","However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy.","We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation.","ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two.","By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds.","Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction.","Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29% fewer samples than baselines, and reduces training time by 21--38%."],"url":"http://arxiv.org/abs/2405.20860v1","category":"cs.LG"}
{"created":"2024-05-31 14:39:46","title":"Machine Learning Conservation Laws of Dynamical systems","abstract":"Conservation laws are of great theoretical and practical interest. We describe a novel approach to machine learning conservation laws of finite-dimensional dynamical systems using trajectory data. It is the first such approach based on kernel methods instead of neural networks which leads to lower computational costs and requires a lower amount of training data. We propose the use of an \"indeterminate\" form of kernel ridge regression where the labels still have to be found by additional conditions. We use here a simple approach minimising the length of the coefficient vector to discover a single conservation law.","sentences":["Conservation laws are of great theoretical and practical interest.","We describe a novel approach to machine learning conservation laws of finite-dimensional dynamical systems using trajectory data.","It is the first such approach based on kernel methods instead of neural networks which leads to lower computational costs and requires a lower amount of training data.","We propose the use of an \"indeterminate\" form of kernel ridge regression where the labels still have to be found by additional conditions.","We use here a simple approach minimising the length of the coefficient vector to discover a single conservation law."],"url":"http://arxiv.org/abs/2405.20857v1","category":"physics.comp-ph"}
{"created":"2024-05-31 14:34:23","title":"Towards Spoken Language Understanding via Multi-level Multi-grained Contrastive Learning","abstract":"Spoken language understanding (SLU) is a core task in task-oriented dialogue systems, which aims at understanding the user's current goal through constructing semantic frames. SLU usually consists of two subtasks, including intent detection and slot filling. Although there are some SLU frameworks joint modeling the two subtasks and achieving high performance, most of them still overlook the inherent relationships between intents and slots and fail to achieve mutual guidance between the two subtasks. To solve the problem, we propose a multi-level multi-grained SLU framework MMCL to apply contrastive learning at three levels, including utterance level, slot level, and word level to enable intent and slot to mutually guide each other. For the utterance level, our framework implements coarse granularity contrastive learning and fine granularity contrastive learning simultaneously. Besides, we also apply the self-distillation method to improve the robustness of the model. Experimental results and further analysis demonstrate that our proposed model achieves new state-of-the-art results on two public multi-intent SLU datasets, obtaining a 2.6 overall accuracy improvement on the MixATIS dataset compared to previous best models.","sentences":["Spoken language understanding (SLU) is a core task in task-oriented dialogue systems, which aims at understanding the user's current goal through constructing semantic frames.","SLU usually consists of two subtasks, including intent detection and slot filling.","Although there are some SLU frameworks joint modeling the two subtasks and achieving high performance, most of them still overlook the inherent relationships between intents and slots and fail to achieve mutual guidance between the two subtasks.","To solve the problem, we propose a multi-level multi-grained SLU framework MMCL to apply contrastive learning at three levels, including utterance level, slot level, and word level to enable intent and slot to mutually guide each other.","For the utterance level, our framework implements coarse granularity contrastive learning and fine granularity contrastive learning simultaneously.","Besides, we also apply the self-distillation method to improve the robustness of the model.","Experimental results and further analysis demonstrate that our proposed model achieves new state-of-the-art results on two public multi-intent SLU datasets, obtaining a 2.6 overall accuracy improvement on the MixATIS dataset compared to previous best models."],"url":"http://arxiv.org/abs/2405.20852v1","category":"cs.CL"}
{"created":"2024-05-31 14:32:06","title":"Proportionally dense subgraphs of maximum size in degree-constrained graphs","abstract":"A proportionally dense subgraph (PDS) of a graph is an induced subgraph of size at least two such that every vertex in the subgraph has proportionally as many neighbors inside as outside of the subgraph. Then, maxPDS is the problem of determining a PDS of maximum size in a given graph. If we further require that a PDS induces a connected subgraph, we refer to such problem as connected maxPDS. In this paper, we study the complexity of maxPDS with respect to parameters representing the density of a graph and its complement. We consider $\\Delta$, representing the maximum degree, $h$, representing the $h$-index, and degen, representing the degeneracy of a graph. We show that maxPDS is NP-hard parameterized by $\\Delta,h$ and degen. More specifically, we show that maxPDS is NP-hard on graphs with $\\Delta=4$, $h=4$ and degen=2. Then, we show that maxPDS is NP-hard when restricted to dense graphs, more specifically graphs $G$ such that $\\Delta(\\overline{G})\\leq 6$, and graphs $G$ such that $degen(\\overline{G}) \\leq 2$ and $\\overline{G}$ is bipartite, where $\\overline{G}$ represents the complement of $G$. On the other hand, we show that maxPDS is polynomial-time solvable on graphs with $h\\le2$. Finally, we consider graphs $G$ such that $h(\\overline{G})\\le 2$ and show that there exists a polynomial-time algorithm for finding a PDS of maximum size in such graphs. This result implies polynomial-time complexity on graphs with $n$ vertices of minimum degree $n-3$, i.e. graphs $G$ such that $\\Delta(\\overline{G})\\le 2$. For each result presented in this paper, we consider connected maxPDS and explain how to extend it when we require connectivity.","sentences":["A proportionally dense subgraph (PDS) of a graph is an induced subgraph of size at least two such that every vertex in the subgraph has proportionally as many neighbors inside as outside of the subgraph.","Then, maxPDS is the problem of determining a PDS of maximum size in a given graph.","If we further require that a PDS induces a connected subgraph, we refer to such problem as connected maxPDS.","In this paper, we study the complexity of maxPDS with respect to parameters representing the density of a graph and its complement.","We consider $\\Delta$, representing the maximum degree, $h$, representing the $h$-index, and degen, representing the degeneracy of a graph.","We show that maxPDS is NP-hard parameterized by $\\Delta,h$ and degen.","More specifically, we show that maxPDS is NP-hard on graphs with $\\Delta=4$, $h=4$ and degen=2.","Then, we show that maxPDS is NP-hard when restricted to dense graphs, more specifically graphs $G$ such that $\\Delta(\\overline{G})\\leq 6$, and graphs $G$ such that $degen(\\overline{G}) \\leq 2$ and $\\overline{G}$ is bipartite, where $\\overline{G}$ represents the complement of $G$. On the other hand, we show that maxPDS is polynomial-time solvable on graphs with $h\\le2$. Finally, we consider graphs $G$ such that $h(\\overline{G})\\le 2$ and show that there exists a polynomial-time algorithm for finding a PDS of maximum size in such graphs.","This result implies polynomial-time complexity on graphs with $n$ vertices of minimum degree $n-3$, i.e. graphs $G$ such that $\\Delta(\\overline{G})\\le 2$.","For each result presented in this paper, we consider connected maxPDS and explain how to extend it when we require connectivity."],"url":"http://arxiv.org/abs/2405.20847v1","category":"cs.CC"}
{"created":"2024-05-31 14:31:23","title":"Metastable doubly-charged Rydberg molecules","abstract":"H$_3^{2+}$ is a one-electron system with three positive nuclei and is known to be unstable in its electronic ground-state. We examine an analogous one-electron system composed of a $^{87}$Rb Rydberg atom interacting with a pair of cations and predict the existence of metastable vibrationally-bound states of $^{87}$Rb$_3^{2+}$. These molecules are long-range trimers whose stability rests on the presence of core-shell electrons and favourable scaling of the Rydberg atom's quadrupole moment with the principal quantum number $n$. Unlike recently observed ion-Rydberg dimers, whose binding is due to internal flipping of the Rydberg atom's dipole moment, the binding of $^{87}$Rb$_3^{2+}$ arises from the interaction of the ions with the Rydberg atom's quadrupole moment. The stability of these trimers is highly sensitive to $n$. We do not expect these states to exist below $n=24$ and for $n \\leq 35$, their lifetime is limited by tunnelling of the Rydberg electron. In contrast, at very large $n$ the lifetime will be limited by tunnelling of the vibrational wavepacket. In between these limits, we expect a range of bound states at intermediate $n$ for which both tunnelling rates are smaller than the radiative decay rate of the Rydberg state.","sentences":["H$_3^{2+}$ is a one-electron system with three positive nuclei and is known to be unstable in its electronic ground-state.","We examine an analogous one-electron system composed of a $^{87}$Rb Rydberg atom interacting with a pair of cations and predict the existence of metastable vibrationally-bound states of $^{87}$Rb$_3^{2+}$. These molecules are long-range trimers whose stability rests on the presence of core-shell electrons and favourable scaling of the Rydberg atom's quadrupole moment with the principal quantum number $n$. Unlike recently observed ion-Rydberg dimers, whose binding is due to internal flipping of the Rydberg atom's dipole moment, the binding of $^{87}$Rb$_3^{2+}$ arises from the interaction of the ions with the Rydberg atom's quadrupole moment.","The stability of these trimers is highly sensitive to $n$. We do not expect these states to exist below $n=24$ and for $n \\leq 35$, their lifetime is limited by tunnelling of the Rydberg electron.","In contrast, at very large $n$ the lifetime will be limited by tunnelling of the vibrational wavepacket.","In between these limits, we expect a range of bound states at intermediate $n$ for which both tunnelling rates are smaller than the radiative decay rate of the Rydberg state."],"url":"http://arxiv.org/abs/2405.20844v1","category":"physics.atom-ph"}
{"created":"2024-05-31 14:23:05","title":"Strong propagation of chaos for systems of interacting particles with nearly stable jumps","abstract":"We consider a system of $N$ interacting particles, described by SDEs driven by Poisson random measures, where the coefficients depend on the empirical measure of the system. Every particle jumps with a jump rate depending on its position. When this happens, all the other particles of the system receive a small random kick which is distributed according to a heavy tailed random variable belonging to the domain of attraction of an $\\alpha-$ stable law and scaled by $N^{-1/\\alpha},$ where $0 < \\alpha <2 .$ We call these jumps collateral jumps. Moreover, in case $ 0 < \\alpha < 1, $ the jumping particle itself undergoes a macroscopic, main jump. Such systems appear in the modeling of large neural networks, such as the human brain.   The particular scaling of the collateral jumps implies that the limit of the empirical measures of the system is random and equals the conditional distribution of one typical particle in the limit system, given the source of common noise. Thus the system exhibits the conditional propagation of chaos property. The limit system turns out to be solution of a non-linear SDE, driven by an $ \\alpha-$stable process. We prove strong unique existence of the limit system and introduce a suitable coupling to obtain the strong convergence of the finite to the limit system, together with precise error bounds for finite time marginals.","sentences":["We consider a system of $N$ interacting particles, described by SDEs driven by Poisson random measures, where the coefficients depend on the empirical measure of the system.","Every particle jumps with a jump rate depending on its position.","When this happens, all the other particles of the system receive a small random kick which is distributed according to a heavy tailed random variable belonging to the domain of attraction of an $\\alpha-$ stable law and scaled by $N^{-1/\\alpha},$ where $0 < \\alpha <2 .$","We call these jumps collateral jumps.","Moreover, in case $ 0 < \\alpha < 1, $ the jumping particle itself undergoes a macroscopic, main jump.","Such systems appear in the modeling of large neural networks, such as the human brain.   ","The particular scaling of the collateral jumps implies that the limit of the empirical measures of the system is random and equals the conditional distribution of one typical particle in the limit system, given the source of common noise.","Thus the system exhibits the conditional propagation of chaos property.","The limit system turns out to be solution of a non-linear SDE, driven by an $ \\alpha-$stable process.","We prove strong unique existence of the limit system and introduce a suitable coupling to obtain the strong convergence of the finite to the limit system, together with precise error bounds for finite time marginals."],"url":"http://arxiv.org/abs/2405.20831v1","category":"math.PR"}
{"created":"2024-05-31 14:19:57","title":"Experimental demonstration of a fault-tolerant qubit encoded on a hyperfine-coupled qudit","abstract":"The realization of effective quantum error correction protocols remains a central challenge in the development of scalable quantum computers. Protocols employing redundancy over multiple physical qubits to encode a single error-protected logical qubit are theoretically effective, but imply a large resource overhead. Alternative, more hardware-efficient, approaches seek to deploy higher-dimensional quantum systems known as qudits. Recently, proposals have emerged for exploiting high-spin magnetic nuclei coupled to condensed matter electron spin qubits to implement fault-tolerant memories.   Here, we explore experimentally the simplest of these proposals, a logical qubit encoded on the four states of a I=3/2 nuclear spin hyperfine-coupled to a S=1/2 electron spin qubit; the encoding protects against the dominant decoherence mechanism in such systems, fluctuations of the quantizing magnetic field. We implement the encoding using electron-nuclear double resonance within a subspace of the spin levels in an ensemble of highly coherent manganese defects in zinc oxide. We explore the dynamics of the encoded state both under a controlled application of the fluctuation and under natural decoherence processes. Our results confirm the potential of these proposals for practical, implementable, fault tolerant quantum memories.","sentences":["The realization of effective quantum error correction protocols remains a central challenge in the development of scalable quantum computers.","Protocols employing redundancy over multiple physical qubits to encode a single error-protected logical qubit are theoretically effective, but imply a large resource overhead.","Alternative, more hardware-efficient, approaches seek to deploy higher-dimensional quantum systems known as qudits.","Recently, proposals have emerged for exploiting high-spin magnetic nuclei coupled to condensed matter electron spin qubits to implement fault-tolerant memories.   ","Here, we explore experimentally the simplest of these proposals, a logical qubit encoded on the four states of a I=3/2 nuclear spin hyperfine-coupled to a S=1/2 electron spin qubit; the encoding protects against the dominant decoherence mechanism in such systems, fluctuations of the quantizing magnetic field.","We implement the encoding using electron-nuclear double resonance within a subspace of the spin levels in an ensemble of highly coherent manganese defects in zinc oxide.","We explore the dynamics of the encoded state both under a controlled application of the fluctuation and under natural decoherence processes.","Our results confirm the potential of these proposals for practical, implementable, fault tolerant quantum memories."],"url":"http://arxiv.org/abs/2405.20827v1","category":"quant-ph"}
{"created":"2024-05-31 14:19:31","title":"Modulation theory of soliton-mean flow in KdV equation with box type initial data","abstract":"For the KdV equation with box type initial data, the interaction between a trial soliton and large-scale dispersive mean flow is studied theoretically and numerically. The pure box initial value can cause rarefaction wave and dispersive shock wave, and can create an area of soliton train. The key to the interaction of soliton and mean flow is that the dynamic evolutions of the mean flow and the local soliton can be described by the same modulation system. The soliton modulation system is derived from the degenerations of the two-genus Whitham modulation system. Considering the influence of rarefaction wave, dispersive shock wave and soliton train on the trial soliton, in the framework of Whitham modulation theory, the equation describing the soliton trajectory and the changes in amplitude and phase shift are given explicitly. The predicted results are compared with the numerical simulations, which verifies the corrections of the theoretical analysis. The exotic interaction phenomena between soliton and mean flow found in this work have broad applications to shallow water soliton propagations and real soliton experiments in fluid dynamics.","sentences":["For the KdV equation with box type initial data, the interaction between a trial soliton and large-scale dispersive mean flow is studied theoretically and numerically.","The pure box initial value can cause rarefaction wave and dispersive shock wave, and can create an area of soliton train.","The key to the interaction of soliton and mean flow is that the dynamic evolutions of the mean flow and the local soliton can be described by the same modulation system.","The soliton modulation system is derived from the degenerations of the two-genus Whitham modulation system.","Considering the influence of rarefaction wave, dispersive shock wave and soliton train on the trial soliton, in the framework of Whitham modulation theory, the equation describing the soliton trajectory and the changes in amplitude and phase shift are given explicitly.","The predicted results are compared with the numerical simulations, which verifies the corrections of the theoretical analysis.","The exotic interaction phenomena between soliton and mean flow found in this work have broad applications to shallow water soliton propagations and real soliton experiments in fluid dynamics."],"url":"http://arxiv.org/abs/2405.20826v1","category":"nlin.PS"}
{"created":"2024-05-31 14:18:37","title":"Analysis of clinical, dosimetric and radiomic features for predicting local failure after stereotactic radiotherapy of brain metastases in malignant melanoma","abstract":"Background: The aim of this study was to investigate the role of clinical, dosimetric and pretherapeutic magnetic resonance imaging (MRI) features for lesion-specific outcome prediction of stereotactic radiotherapy (SRT) in patients with brain metastases from malignant melanoma (MBM).   Methods: In this multicenter, retrospective analysis, we reviewed 517 MBM from 130 patients treated with SRT (single fraction or hypofractionated). For each gross tumor volume (GTV) 1576 radiomic features (RF) were calculated (788 each for the GTV and for a 3 mm margin around the GTV). Clinical parameters, radiation dose and RF from pretherapeutic contrast-enhanced T1-weighted MRI from different institutions were evaluated with a feature processing and elimination pipeline in a nested cross-validation scheme.   Results: Seventy-two (72) of 517 lesions (13.9%) showed a local failure (LF) after SRT. The processing pipeline showed clinical, dosimetric and radiomic features providing information for LF prediction. The most prominent ones were the correlation of the gray level co-occurrence matrix of the margin (hazard ratio (HR): 0.37, confidence interval (CI): 0.23-0.58) and systemic therapy before SRT (HR: 0.55, CI: 0.42-0.70). The majority of RF associated with LF was calculated in the margin around the GTV.   Conclusions: Pretherapeutic MRI based RF connected with lesion-specific outcome after SRT could be identified, despite multicentric data and minor differences in imaging protocols. Image data analysis of the surrounding metastatic environment may provide therapy-relevant information with the potential to further individualize radiotherapy strategies.","sentences":["Background: The aim of this study was to investigate the role of clinical, dosimetric and pretherapeutic magnetic resonance imaging (MRI) features for lesion-specific outcome prediction of stereotactic radiotherapy (SRT) in patients with brain metastases from malignant melanoma (MBM).   ","Methods: In this multicenter, retrospective analysis, we reviewed 517 MBM from 130 patients treated with SRT (single fraction or hypofractionated).","For each gross tumor volume (GTV) 1576 radiomic features (RF) were calculated (788 each for the GTV and for a 3 mm margin around the GTV).","Clinical parameters, radiation dose and RF from pretherapeutic contrast-enhanced T1-weighted MRI from different institutions were evaluated with a feature processing and elimination pipeline in a nested cross-validation scheme.   ","Results: Seventy-two (72) of 517 lesions (13.9%) showed a local failure (LF) after SRT.","The processing pipeline showed clinical, dosimetric and radiomic features providing information for LF prediction.","The most prominent ones were the correlation of the gray level co-occurrence matrix of the margin (hazard ratio (HR): 0.37, confidence interval (CI): 0.23-0.58) and systemic therapy before SRT (HR: 0.55, CI: 0.42-0.70).","The majority of RF associated with LF was calculated in the margin around the GTV.   ","Conclusions: Pretherapeutic MRI based RF connected with lesion-specific outcome after SRT could be identified, despite multicentric data and minor differences in imaging protocols.","Image data analysis of the surrounding metastatic environment may provide therapy-relevant information with the potential to further individualize radiotherapy strategies."],"url":"http://arxiv.org/abs/2405.20825v1","category":"physics.med-ph"}
{"created":"2024-05-31 14:16:52","title":"Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously","abstract":"We consider the classic problem of online convex optimisation. Whereas the notion of static regret is relevant for stationary problems, the notion of switching regret is more appropriate for non-stationary problems. A switching regret is defined relative to any segmentation of the trial sequence, and is equal to the sum of the static regrets of each segment. In this paper we show that, perhaps surprisingly, we can achieve the asymptotically optimal switching regret on every possible segmentation simultaneously. Our algorithm for doing so is very efficient: having a space and per-trial time complexity that is logarithmic in the time-horizon. Our algorithm also obtains novel bounds on its dynamic regret: being adaptive to variations in the rate of change of the comparator sequence.","sentences":["We consider the classic problem of online convex optimisation.","Whereas the notion of static regret is relevant for stationary problems, the notion of switching regret is more appropriate for non-stationary problems.","A switching regret is defined relative to any segmentation of the trial sequence, and is equal to the sum of the static regrets of each segment.","In this paper we show that, perhaps surprisingly, we can achieve the asymptotically optimal switching regret on every possible segmentation simultaneously.","Our algorithm for doing so is very efficient: having a space and per-trial time complexity that is logarithmic in the time-horizon.","Our algorithm also obtains novel bounds on its dynamic regret: being adaptive to variations in the rate of change of the comparator sequence."],"url":"http://arxiv.org/abs/2405.20824v1","category":"cs.LG"}
{"created":"2024-05-31 14:16:04","title":"On cohomological invariants of complex and almost complex manifolds","abstract":"Given a compact complex manifold, we develop Hodge theory for the elliptic complex of differential forms defined by Bigolin in 1969 and recently referred as the Schweitzer complex. We exhibits several $L^2$ orthogonal decompositions of spaces of forms and prove a Hodge decomposition for harmonic forms on compact K\\\"ahler manifolds. Then we compute the cohomology of this complex on the small deformations of the complex structure of the Iwasawa manifold, showing that this cohomology is as powerful as Aeppli and Bott-Chern cohomology, in order to distinguish classes of complex structures. Finally, we partially extend the definition of this complex on almost complex manifolds, providing a new cohomological invariant on 1-forms which is finite dimensional when the manifold is compact.","sentences":["Given a compact complex manifold, we develop Hodge theory for the elliptic complex of differential forms defined by Bigolin in 1969 and recently referred as the Schweitzer complex.","We exhibits several $L^2$ orthogonal decompositions of spaces of forms and prove a Hodge decomposition for harmonic forms on compact K\\\"ahler manifolds.","Then we compute the cohomology of this complex on the small deformations of the complex structure of the Iwasawa manifold, showing that this cohomology is as powerful as Aeppli and Bott-Chern cohomology, in order to distinguish classes of complex structures.","Finally, we partially extend the definition of this complex on almost complex manifolds, providing a new cohomological invariant on 1-forms which is finite dimensional when the manifold is compact."],"url":"http://arxiv.org/abs/2405.20823v1","category":"math.DG"}
{"created":"2024-05-31 14:15:44","title":"Pursuing Overall Welfare in Federated Learning through Sequential Decision Making","abstract":"In traditional federated learning, a single global model cannot perform equally well for all clients. Therefore, the need to achieve the client-level fairness in federated system has been emphasized, which can be realized by modifying the static aggregation scheme for updating the global model to an adaptive one, in response to the local signals of the participating clients. Our work reveals that existing fairness-aware aggregation strategies can be unified into an online convex optimization framework, in other words, a central server's sequential decision making process. To enhance the decision making capability, we propose simple and intuitive improvements for suboptimal designs within existing methods, presenting AAggFF. Considering practical requirements, we further subdivide our method tailored for the cross-device and the cross-silo settings, respectively. Theoretical analyses guarantee sublinear regret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the cross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting, with $K$ clients and $T$ federation rounds. Extensive experiments demonstrate that the federated system equipped with AAggFF achieves better degree of client-level fairness than existing methods in both practical settings. Code is available at https://github.com/vaseline555/AAggFF","sentences":["In traditional federated learning, a single global model cannot perform equally well for all clients.","Therefore, the need to achieve the client-level fairness in federated system has been emphasized, which can be realized by modifying the static aggregation scheme for updating the global model to an adaptive one, in response to the local signals of the participating clients.","Our work reveals that existing fairness-aware aggregation strategies can be unified into an online convex optimization framework, in other words, a central server's sequential decision making process.","To enhance the decision making capability, we propose simple and intuitive improvements for suboptimal designs within existing methods, presenting AAggFF.","Considering practical requirements, we further subdivide our method tailored for the cross-device and the cross-silo settings, respectively.","Theoretical analyses guarantee sublinear regret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the cross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting, with $K$ clients and $T$ federation rounds.","Extensive experiments demonstrate that the federated system equipped with AAggFF achieves better degree of client-level fairness than existing methods in both practical settings.","Code is available at https://github.com/vaseline555/AAggFF"],"url":"http://arxiv.org/abs/2405.20821v1","category":"cs.LG"}
{"created":"2024-05-31 14:10:36","title":"Distributed Simulation for Digital Twins of Large-Scale Real-World DiffServ-Based Networks","abstract":"Digital Twin technology facilitates the monitoring and online analysis of large-scale communication networks. Faster predictions of network performance thus become imperative, especially for analysing Quality of Service (QoS) parameters in large-scale city networks. Discrete Event Simulation (DES) is a standard network analysis technology, and can be further optimised with parallel and distributed execution for speedup, referred to as Parallel Discrete Event Simulation (PDES). However, modelling detailed QoS mechanisms such as DiffServ requires complex event handling for each network router, which can involve excessive simulation events. In addition, current PDES for network analysis mostly adopts conservative scheduling, which suffers from excessive global synchronisation to avoid causality problems. The performance analysis of optimistic PDES for real-world large-scale network topology and complex QoS mechanisms is still inadequate. To address these gaps, this paper proposes a simulation toolkit, Quaint, which leverages an optimistic PDES engine ROSS, for detailed modelling of DiffServ-based networks. A novel event-handling model for each network router is also proposed to significantly reduce the number of events in complex QoS modelling. Quaint has been evaluated using a real-world metropolitan-scale network topology with 5,000 routers/switches. Results show that compared to the conventional simulator OMNeT++/INET, even the sequential mode of Quaint can achieve a speedup of 53 times, and the distributed mode has a speedup of 232 times. Scalability characterisation is conducted to portray the efficiency of distributed execution, and the results indicate the future direction for workload-aware model partitioning.","sentences":["Digital Twin technology facilitates the monitoring and online analysis of large-scale communication networks.","Faster predictions of network performance thus become imperative, especially for analysing Quality of Service (QoS) parameters in large-scale city networks.","Discrete Event Simulation (DES) is a standard network analysis technology, and can be further optimised with parallel and distributed execution for speedup, referred to as Parallel Discrete Event Simulation (PDES).","However, modelling detailed QoS mechanisms such as DiffServ requires complex event handling for each network router, which can involve excessive simulation events.","In addition, current PDES for network analysis mostly adopts conservative scheduling, which suffers from excessive global synchronisation to avoid causality problems.","The performance analysis of optimistic PDES for real-world large-scale network topology and complex QoS mechanisms is still inadequate.","To address these gaps, this paper proposes a simulation toolkit, Quaint, which leverages an optimistic PDES engine ROSS, for detailed modelling of DiffServ-based networks.","A novel event-handling model for each network router is also proposed to significantly reduce the number of events in complex QoS modelling.","Quaint has been evaluated using a real-world metropolitan-scale network topology with 5,000 routers/switches.","Results show that compared to the conventional simulator OMNeT++/INET, even the sequential mode of Quaint can achieve a speedup of 53 times, and the distributed mode has a speedup of 232 times.","Scalability characterisation is conducted to portray the efficiency of distributed execution, and the results indicate the future direction for workload-aware model partitioning."],"url":"http://arxiv.org/abs/2405.20815v1","category":"cs.DC"}
{"created":"2024-05-31 14:09:48","title":"Unusual Diffusivity in Strongly Disordered Quantum Lattices: Random Dimer Model","abstract":"Recent advances in transport properties measurements of disordered materials and lattice simulations, using superconducting qubits, have rekindled interest in Anderson localization, motivating our study of highly disordered quantum lattices. Initially, our statistical analysis of localized eigenstates reveals a distinct transition between weak and strong disorder regimes, suggesting a random distribution of dimers in highly disordered systems. Subsequently, the random dimer model predicts an oscillating diffusivity that decays as $t^{-1/2}$, is inversely proportional to the disorder strength, and maintains a constant frequency with an initial phase shift of $\\pi/4$. The first peak exhibits a universal scaling of $\\sigma^{-1}$ both in peak time and amplitude. Finally, we find that stochastic noise suppresses these oscillations and induces hopping between localized eigenstates, resulting in constant diffusion over long times. Our predictions challenge the conventional understanding of incoherent hopping under strong disorder. This offers new insights to optimize disordered systems for optoelectrical and quantum information technologies.","sentences":["Recent advances in transport properties measurements of disordered materials and lattice simulations, using superconducting qubits, have rekindled interest in Anderson localization, motivating our study of highly disordered quantum lattices.","Initially, our statistical analysis of localized eigenstates reveals a distinct transition between weak and strong disorder regimes, suggesting a random distribution of dimers in highly disordered systems.","Subsequently, the random dimer model predicts an oscillating diffusivity that decays as $t^{-1/2}$, is inversely proportional to the disorder strength, and maintains a constant frequency with an initial phase shift of $\\pi/4$. The first peak exhibits a universal scaling of $\\sigma^{-1}$ both in peak time and amplitude.","Finally, we find that stochastic noise suppresses these oscillations and induces hopping between localized eigenstates, resulting in constant diffusion over long times.","Our predictions challenge the conventional understanding of incoherent hopping under strong disorder.","This offers new insights to optimize disordered systems for optoelectrical and quantum information technologies."],"url":"http://arxiv.org/abs/2405.20813v1","category":"quant-ph"}
{"created":"2024-05-31 14:07:39","title":"Context-aware Difference Distilling for Multi-change Captioning","abstract":"Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language. Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes. In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences. Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features. Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features. Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features. Next, the difference context features augment the locally difference features to ensure that all changes are distilled. In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder. Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods.The code is available at https://github.com/tuyunbin/CARD.","sentences":["Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language.","Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes.","In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences.","Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features.","Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features.","Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features.","Next, the difference context features augment the locally difference features to ensure that all changes are distilled.","In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder.","Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods.","The code is available at https://github.com/tuyunbin/CARD."],"url":"http://arxiv.org/abs/2405.20810v1","category":"cs.CV"}
{"created":"2024-05-31 14:07:36","title":"Equivariant parametrized topological complexity","abstract":"In this paper, we define and study an equivariant analogue of Cohen, Farber and Weinberger's parametrized topological complexity. We show that several results in the non-equivariant case can be extended to the equivariant case. For example, we establish the fibrewise equivariant homotopy invariance of the sequential equivariant parametrized topological complexity. We obtain several bounds on sequential equivariant topological complexity involving equivariant category. We also obtain the cohomological lower bound and the dimension-connectivity upper bound on the sequential equivariant parametrized topological complexity.","sentences":["In this paper, we define and study an equivariant analogue of Cohen, Farber and Weinberger's parametrized topological complexity.","We show that several results in the non-equivariant case can be extended to the equivariant case.","For example, we establish the fibrewise equivariant homotopy invariance of the sequential equivariant parametrized topological complexity.","We obtain several bounds on sequential equivariant topological complexity involving equivariant category.","We also obtain the cohomological lower bound and the dimension-connectivity upper bound on the sequential equivariant parametrized topological complexity."],"url":"http://arxiv.org/abs/2405.20809v1","category":"math.AT"}
{"created":"2024-05-31 14:04:45","title":"Reachability and Safety Games under TSO Semantics (Extended Version)","abstract":"We consider games played on the transtion graph of concurrent programs running under the TotalStore Order (TSO) weak memory model. Games are frequently used to model the interaction between a system and its environment, in this case between the concurrent processes and the nondeterminisitic TSO buffer updates. The game is played by two players, who alternatinglymake a move: Theprocess playercan execute any enabled instruction of the processes, while theupdate playertakes care of updating the messages in the buffers that are between each process andthe shared memory. We show that the reachability and safety problem of this game reduce to theanalysis of single-process (non-concurrent) programs. In particular, they exhibit only finite-statebehaviour. Because of this, we introduce different notions offairness, which force the two players tobehave in a more realistic way. Both the reachability and safety problem then become undecidable.","sentences":["We consider games played on the transtion graph of concurrent programs running under the TotalStore Order (TSO) weak memory model.","Games are frequently used to model the interaction between a system and its environment, in this case between the concurrent processes and the nondeterminisitic TSO buffer updates.","The game is played by two players, who alternatinglymake a move: Theprocess playercan execute any enabled instruction of the processes, while theupdate playertakes care of updating the messages in the buffers that are between each process andthe shared memory.","We show that the reachability and safety problem of this game reduce to theanalysis of single-process (non-concurrent) programs.","In particular, they exhibit only finite-statebehaviour.","Because of this, we introduce different notions offairness, which force the two players tobehave in a more realistic way.","Both the reachability and safety problem then become undecidable."],"url":"http://arxiv.org/abs/2405.20804v1","category":"cs.GT"}
{"created":"2024-05-31 13:50:15","title":"Operators in the Fock-Toeplitz algebra","abstract":"We consider various classes of bounded operators on the Fock space $F^2$ of Gaussian square integrable entire functions over the complex plane. These include Toeplitz (type) operators, weighted composition operators, singular integral operators, Volterra-type operators and Hausdorff operators and range from classical objects in harmonic analysis to more recently introduced classes. As a leading problem and closely linked to well-known compactness characterizations we pursue the question of when these operators are contained in the Toeplitz algebra. This paper combines a (certainly in-complete) survey of the classical and more recent literature including new ideas for proofs from the perspective of quantum harmonic analysis (QHA). Moreover, we have added a number of new theorems and links between known results.","sentences":["We consider various classes of bounded operators on the Fock space $F^2$ of Gaussian square integrable entire functions over the complex plane.","These include Toeplitz (type) operators, weighted composition operators, singular integral operators, Volterra-type operators and Hausdorff operators and range from classical objects in harmonic analysis to more recently introduced classes.","As a leading problem and closely linked to well-known compactness characterizations we pursue the question of when these operators are contained in the Toeplitz algebra.","This paper combines a (certainly in-complete) survey of the classical and more recent literature including new ideas for proofs from the perspective of quantum harmonic analysis (QHA).","Moreover, we have added a number of new theorems and links between known results."],"url":"http://arxiv.org/abs/2405.20792v1","category":"math.FA"}
{"created":"2024-05-31 12:36:14","title":"A transportable hyperspectral imaging setup based on fast, high-density spectral scanning for in situ quantitative biochemical mapping of fresh tissue biopsies","abstract":"Histopathological examination of surgical biopsies, such as in glioma and glioblastoma resection, is hindered in current clinical practice by the long times required for the laboratory analysis and pathological screening, typically taking several days or even weeks to be completed. We propose here a transportable, high-density, spectral-scanning based hyperspectral imaging setup, named HyperProbe1, that can provide in situ, fast biochemical analysis and mapping of fresh surgical tissue samples, right after excision, and without the need of fixing or staining. HyperProbe1 is based on spectral scanning via supercontinuum laser illumination filtered with acousto-optic tuneable filters. Such methodology allows the user to select any number and type of wavelength bands in the visible and near-infrared range between 510 and 900 nm (up to 79), and to reconstruct 3D hypercubes composed of high-resolution, widefield images of the surgical samples, where each pixel is associated with a complete spectrum. The system is applied on 11 fresh surgical biopsies of glioma from routine patients, including different grades of tumour classification. Quantitative analysis of the composition of the tissue is performed via fast spectral unmixing to reconstruct mapping of major biomarkers. We also provided a preliminary attempt to infer tumour classification based on differences of composition in the samples, suggesting the possibility to use lipid content and differential cytochrome-c-oxidase concentrations to distinguish between lower and higher grade gliomas. A proof-of-concept of the performances of HyperProbe1 for quantitative, biochemical mapping of surgical biopsies is demonstrated, paving the way for improving current post-surgical, histopathological practice via non-destructive, in situ streamlined screening of fresh tissue samples in a matter of minutes after excision.","sentences":["Histopathological examination of surgical biopsies, such as in glioma and glioblastoma resection, is hindered in current clinical practice by the long times required for the laboratory analysis and pathological screening, typically taking several days or even weeks to be completed.","We propose here a transportable, high-density, spectral-scanning based hyperspectral imaging setup, named HyperProbe1, that can provide in situ, fast biochemical analysis and mapping of fresh surgical tissue samples, right after excision, and without the need of fixing or staining.","HyperProbe1 is based on spectral scanning via supercontinuum laser illumination filtered with acousto-optic tuneable filters.","Such methodology allows the user to select any number and type of wavelength bands in the visible and near-infrared range between 510 and 900 nm (up to 79), and to reconstruct 3D hypercubes composed of high-resolution, widefield images of the surgical samples, where each pixel is associated with a complete spectrum.","The system is applied on 11 fresh surgical biopsies of glioma from routine patients, including different grades of tumour classification.","Quantitative analysis of the composition of the tissue is performed via fast spectral unmixing to reconstruct mapping of major biomarkers.","We also provided a preliminary attempt to infer tumour classification based on differences of composition in the samples, suggesting the possibility to use lipid content and differential cytochrome-c-oxidase concentrations to distinguish between lower and higher grade gliomas.","A proof-of-concept of the performances of HyperProbe1 for quantitative, biochemical mapping of surgical biopsies is demonstrated, paving the way for improving current post-surgical, histopathological practice via non-destructive, in situ streamlined screening of fresh tissue samples in a matter of minutes after excision."],"url":"http://arxiv.org/abs/2405.20765v1","category":"physics.med-ph"}
{"created":"2024-05-31 12:31:05","title":"Comparison of Access Control Approaches for Graph-Structured Data","abstract":"Access control is the enforcement of the authorization policy, which defines subjects, resources, and access rights. Graph-structured data requires advanced, flexible, and fine-grained access control due to its complex structure as sequences of alternating vertices and edges. Several research works focus on protecting property graph-structured data, enforcing fine-grained access control, and proving the feasibility and applicability of their concept. However, they differ conceptually and technically. We select works from our systematic literature review on authorization and access control for different database models in addition to recent ones. Based on defined criteria, we exclude research works with different objectives, such as no protection of graph-structured data, graph models other than the property graph, coarse-grained access control approaches, or no application in a graph datastore (i.e., no proof-of-concept implementation). The latest version of the remaining works are discussed in detail in terms of their access control approach as well as authorization policy definition and enforcement. Finally, we analyze the strengths and limitations of the selected works and provide a comparison with respect to different aspects, including the base access control model, open/closed policy, negative permission support, and datastore-independent enforcement.","sentences":["Access control is the enforcement of the authorization policy, which defines subjects, resources, and access rights.","Graph-structured data requires advanced, flexible, and fine-grained access control due to its complex structure as sequences of alternating vertices and edges.","Several research works focus on protecting property graph-structured data, enforcing fine-grained access control, and proving the feasibility and applicability of their concept.","However, they differ conceptually and technically.","We select works from our systematic literature review on authorization and access control for different database models in addition to recent ones.","Based on defined criteria, we exclude research works with different objectives, such as no protection of graph-structured data, graph models other than the property graph, coarse-grained access control approaches, or no application in a graph datastore (i.e., no proof-of-concept implementation).","The latest version of the remaining works are discussed in detail in terms of their access control approach as well as authorization policy definition and enforcement.","Finally, we analyze the strengths and limitations of the selected works and provide a comparison with respect to different aspects, including the base access control model, open/closed policy, negative permission support, and datastore-independent enforcement."],"url":"http://arxiv.org/abs/2405.20762v1","category":"cs.CR"}
{"created":"2024-05-31 10:37:32","title":"Bifurcation in correlation length of the Ising model on a \"Toblerone\" lattice","abstract":"The classical Ising chain is the paradigm for the non-existence of phase transitions in 1D systems and was solved by Ernst Ising one hundred years ago. More recently, a decorated two leg Ising ladder has received interest for the curious thermodynamics that resemble a phase transition; a sharp peak in the specific heat at low, but finite temperature. We use this model to reveal a bifurcation in the correlation lengths due to a crossing of the sub-leading eigenvalues of the transfer matrix, which results in two distinct length scales necessary to describe to the decay of correlations. We discuss this phenomenon in the context of the geometric frustration in the model. We also provide additional results to aid in the understanding of the curious thermodynamics of the model through a study of the magnetic susceptibilities.","sentences":["The classical Ising chain is the paradigm for the non-existence of phase transitions in 1D systems and was solved by Ernst Ising one hundred years ago.","More recently, a decorated two leg Ising ladder has received interest for the curious thermodynamics that resemble a phase transition; a sharp peak in the specific heat at low, but finite temperature.","We use this model to reveal a bifurcation in the correlation lengths due to a crossing of the sub-leading eigenvalues of the transfer matrix, which results in two distinct length scales necessary to describe to the decay of correlations.","We discuss this phenomenon in the context of the geometric frustration in the model.","We also provide additional results to aid in the understanding of the curious thermodynamics of the model through a study of the magnetic susceptibilities."],"url":"http://arxiv.org/abs/2405.20749v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-31 10:18:28","title":"UAV-Enabled Wireless Networks with Movable-Antenna Array: Flexible Beamforming and Trajectory Design","abstract":"Recently, movable antenna (MA) array becomes a promising technology for improving the communication quality in wireless communication systems. In this letter, an unmanned aerial vehicle (UAV) enabled multi-user multi-input-single-output system enhanced by the MA array is investigated. To enhance the throughput capacity, we aim to maximize the achievable data rate by jointly optimizing the transmit beamforming, the UAV trajectory, and the positions of the MA array antennas. The formulated data rate maximization problem is a highly coupled non-convex problem, for which an alternating optimization based algorithm is proposed to get a sub-optimal solution. Numerical results have demonstrated the performance gain of the proposed method compared with conventional method with fixed-position antenna array.","sentences":["Recently, movable antenna (MA) array becomes a promising technology for improving the communication quality in wireless communication systems.","In this letter, an unmanned aerial vehicle (UAV) enabled multi-user multi-input-single-output system enhanced by the MA array is investigated.","To enhance the throughput capacity, we aim to maximize the achievable data rate by jointly optimizing the transmit beamforming, the UAV trajectory, and the positions of the MA array antennas.","The formulated data rate maximization problem is a highly coupled non-convex problem, for which an alternating optimization based algorithm is proposed to get a sub-optimal solution.","Numerical results have demonstrated the performance gain of the proposed method compared with conventional method with fixed-position antenna array."],"url":"http://arxiv.org/abs/2405.20746v1","category":"eess.SP"}
{"created":"2024-05-31 10:17:46","title":"Practical Modelling with Bigraphs","abstract":"Bigraphs are a versatile modelling formalism that allows easy expression of placement and connectivity relations in a graphical format. System evolution is user defined as a set of rewrite rules. This paper presents a practical, yet detailed guide to developing, executing, and reasoning about bigraph models, including recent extensions such as parameterised, instantaneous, prioritised and conditional rules, and probabilistic and stochastic rewriting.","sentences":["Bigraphs are a versatile modelling formalism that allows easy expression of placement and connectivity relations in a graphical format.","System evolution is user defined as a set of rewrite rules.","This paper presents a practical, yet detailed guide to developing, executing, and reasoning about bigraph models, including recent extensions such as parameterised, instantaneous, prioritised and conditional rules, and probabilistic and stochastic rewriting."],"url":"http://arxiv.org/abs/2405.20745v1","category":"cs.LO"}
{"created":"2024-05-31 09:41:23","title":"Dynamical Moir\u00e9 Systems in Twisted Bilayer Optical Lattices","abstract":"Moir\\'e related physics in twisted bilayer two-dimensional (2D) materials has attracted widespread interest in condensed matter physics. Simulation of moir\\'e related physics in cold atom platform is expected to outperform the 2D materials thanks to its advantage of higher tunablility. Here, we point out, the cold atom platform enables a new mechanism of moir\\'e lattice formation, with intrinsic \"dynamical\" character arising from interlayer interaction, in contrast to conventional moir\\'e lattice induced by single-particle interlayer tunneling. Specifically, we consider a twisted bilayer Bose-Hubbard model with vanishing interlayer tunneling, and the bilayer is solely coupled through interlayer interaction that originates from contact interaction of atoms. We find that this system hosts a plethora of novel phases unique to this dynamical lattice, including a variety of Mott insulator (MI) and superfluid (SF) phases either preserving or breaking moir\\'e lattice symmetry, phases with one layer in SF and the other in MI, \"interlocked\" MI, and self-localized phases at commensurate twist angles. Our prediction can be readily observed in current experimental setup of twisted bilayer optical lattices, opening up new avenues for exploring the rich physics of dynamical moir\\'e systems in cold atoms.","sentences":["Moir\\'e related physics in twisted bilayer two-dimensional (2D) materials has attracted widespread interest in condensed matter physics.","Simulation of moir\\'e related physics in cold atom platform is expected to outperform the 2D materials thanks to its advantage of higher tunablility.","Here, we point out, the cold atom platform enables a new mechanism of moir\\'e lattice formation, with intrinsic \"dynamical\" character arising from interlayer interaction, in contrast to conventional moir\\'e lattice induced by single-particle interlayer tunneling.","Specifically, we consider a twisted bilayer Bose-Hubbard model with vanishing interlayer tunneling, and the bilayer is solely coupled through interlayer interaction that originates from contact interaction of atoms.","We find that this system hosts a plethora of novel phases unique to this dynamical lattice, including a variety of Mott insulator (MI) and superfluid (SF) phases either preserving or breaking moir\\'e lattice symmetry, phases with one layer in SF and the other in MI, \"interlocked\" MI, and self-localized phases at commensurate twist angles.","Our prediction can be readily observed in current experimental setup of twisted bilayer optical lattices, opening up new avenues for exploring the rich physics of dynamical moir\\'e systems in cold atoms."],"url":"http://arxiv.org/abs/2405.20732v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-31 09:37:15","title":"Kinetic theory of stellar systems and two-dimensional vortices","abstract":"We discuss the kinetic theory of stellar systems and two-dimensional vortices and stress their analogies. We recall the derivation of the Landau and Lenard-Balescu equations from the Klimontovich formalism. These equations take into account two-body correlations and are valid at the order $1/N$, where $N$ is the number of particles in the system. They have the structure of a Fokker-Planck equation involving a diffusion term and a drift term. The systematic drift of a vortex is the counterpart of the dynamical friction experienced by a star. At equilibrium, the diffusion and the drift terms balance each other establishing the Boltzmann distribution of statistical mechanics. We discuss the problem of kinetic blocking in certain cases and how it can be solved at the order $1/N^2$ by the consideration of three-body correlations. We also consider the behavior of the system close to the critical point following a recent suggestion by Hamilton and Heinemann (2023). We present a simple calculation, valid for spatially homogeneous systems with long-range interactions described by the Cauchy distribution, showing how the consideration of the Landau modes regularizes the divergence of the friction by polarization at the critical point.","sentences":["We discuss the kinetic theory of stellar systems and two-dimensional vortices and stress their analogies.","We recall the derivation of the Landau and Lenard-Balescu equations from the Klimontovich formalism.","These equations take into account two-body correlations and are valid at the order $1/N$, where $N$ is the number of particles in the system.","They have the structure of a Fokker-Planck equation involving a diffusion term and a drift term.","The systematic drift of a vortex is the counterpart of the dynamical friction experienced by a star.","At equilibrium, the diffusion and the drift terms balance each other establishing the Boltzmann distribution of statistical mechanics.","We discuss the problem of kinetic blocking in certain cases and how it can be solved at the order $1/N^2$ by the consideration of three-body correlations.","We also consider the behavior of the system close to the critical point following a recent suggestion by Hamilton and Heinemann (2023).","We present a simple calculation, valid for spatially homogeneous systems with long-range interactions described by the Cauchy distribution, showing how the consideration of the Landau modes regularizes the divergence of the friction by polarization at the critical point."],"url":"http://arxiv.org/abs/2405.20728v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-31 09:26:26","title":"Learning on Large Graphs using Intersecting Communities","abstract":"Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each node's representation in an input graph by aggregating messages from the node's neighbors, which necessitates a memory complexity of the order of the number of graph edges. This complexity might quickly become prohibitive for large graphs provided they are not very sparse. In this paper, we propose a novel approach to alleviate this problem by approximating the input graph as an intersecting community graph (ICG) -- a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph does not depend on the graph size. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the number of nodes (rather than edges). This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.","sentences":["Message Passing Neural Networks (MPNNs) are a staple of graph machine learning.","MPNNs iteratively update each node's representation in an input graph by aggregating messages from the node's neighbors, which necessitates a memory complexity of the order of the number of graph edges.","This complexity might quickly become prohibitive for large graphs provided they are not very sparse.","In this paper, we propose a novel approach to alleviate this problem by approximating the input graph as an intersecting community graph (ICG) -- a combination of intersecting cliques.","The key insight is that the number of communities required to approximate a graph does not depend on the graph size.","We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph.","We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the number of nodes (rather than edges).","This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing."],"url":"http://arxiv.org/abs/2405.20724v1","category":"cs.LG"}
{"created":"2024-05-31 09:26:13","title":"Beaconless Auto-Alignment for Single-Wavelength 5 Tbit/s Mode-Division Multiplexing Free-Space Optical Communications","abstract":"Mode-division multiplexing has shown its ability to significantly increase the capacity of free-space optical communications. An accurate alignment is crucial to enable such links due to possible performance degradation induced by mode crosstalk and narrow beam divergence. Conventionally, a beacon beam is necessary for system alignment due to multiple local maximums in the mode-division multiplexed beam profile. However, the beacon beam introduces excess system complexity, power consumption, and alignment errors. Here we demonstrate a beaconless system with significantly higher alignment accuracy and faster acquisition. This system also excludes excess complexity, power consumption, and alignment errors, facilitating simplified system calibration and supporting a record-high 5.14 Tbit/s line rate in a single-wavelength free-space optical link. We anticipate our paper to be a starting point for more sophisticated alignment scenarios in future multi-Terabit mode-division multiplexing free-space optical communications for long-distance applications with a generalised mode basis.","sentences":["Mode-division multiplexing has shown its ability to significantly increase the capacity of free-space optical communications.","An accurate alignment is crucial to enable such links due to possible performance degradation induced by mode crosstalk and narrow beam divergence.","Conventionally, a beacon beam is necessary for system alignment due to multiple local maximums in the mode-division multiplexed beam profile.","However, the beacon beam introduces excess system complexity, power consumption, and alignment errors.","Here we demonstrate a beaconless system with significantly higher alignment accuracy and faster acquisition.","This system also excludes excess complexity, power consumption, and alignment errors, facilitating simplified system calibration and supporting a record-high 5.14 Tbit/s line rate in a single-wavelength free-space optical link.","We anticipate our paper to be a starting point for more sophisticated alignment scenarios in future multi-Terabit mode-division multiplexing free-space optical communications for long-distance applications with a generalised mode basis."],"url":"http://arxiv.org/abs/2405.20723v1","category":"physics.optics"}
{"created":"2024-05-31 09:07:27","title":"Simulation of open quantum systems on universal quantum computers","abstract":"The rapid development of quantum computers has enabled demonstrations of quantum advantages on various tasks. However, real quantum systems are always dissipative due to their inevitable interaction with the environment, and the resulting non-unitary dynamics make quantum simulation challenging with only unitary quantum gates. In this work, we present an innovative and scalable method to simulate open quantum systems using quantum computers. We define an adjoint density matrix as a counterpart of the true density matrix, which reduces to a mixed-unitary quantum channel and thus can be effectively sampled using quantum computers. This method has several benefits, including no need for auxiliary qubits and noteworthy scalability. Moreover, accurate long-time simulation can also be achieved as the adjoint density matrix and the true dissipated one converge to the same state. Finally, we present deployments of this theory in the dissipative quantum $XY$ model for the evolution of correlation and entropy with short-time dynamics and the disordered Heisenberg model for many-body localization with long-time dynamics. This work promotes the study of real-world many-body dynamics with quantum computers, highlighting the potential to demonstrate practical quantum advantages.","sentences":["The rapid development of quantum computers has enabled demonstrations of quantum advantages on various tasks.","However, real quantum systems are always dissipative due to their inevitable interaction with the environment, and the resulting non-unitary dynamics make quantum simulation challenging with only unitary quantum gates.","In this work, we present an innovative and scalable method to simulate open quantum systems using quantum computers.","We define an adjoint density matrix as a counterpart of the true density matrix, which reduces to a mixed-unitary quantum channel and thus can be effectively sampled using quantum computers.","This method has several benefits, including no need for auxiliary qubits and noteworthy scalability.","Moreover, accurate long-time simulation can also be achieved as the adjoint density matrix and the true dissipated one converge to the same state.","Finally, we present deployments of this theory in the dissipative quantum $XY$ model for the evolution of correlation and entropy with short-time dynamics and the disordered Heisenberg model for many-body localization with long-time dynamics.","This work promotes the study of real-world many-body dynamics with quantum computers, highlighting the potential to demonstrate practical quantum advantages."],"url":"http://arxiv.org/abs/2405.20712v1","category":"quant-ph"}
{"created":"2024-05-31 09:06:28","title":"Testing dynamical stabilization of Complex Langevin simulations of QCD","abstract":"We study complex Langevin simulations of a toy model as well as QCD, supplemented with a dynamical stabilization (DS) term, which was proposed to regularize the complexified process at lower temperatures. We compare the results to reweghting from zero chemical potential to measure the bias that the inclusion of the stabilization term causes, depending on its strength. At high temperatures the stabilization term is not needed. At low temperatures (below deconfinement transition) the DS term has a beneficial stabilizing effect, but too strong DS term causes phase quenching on the system. We observed that the bias of the dynamical stabilization can be to a good accuracy removed by extrapolating to zero dynamical stabilization force using a sigmoid fit.","sentences":["We study complex Langevin simulations of a toy model as well as QCD, supplemented with a dynamical stabilization (DS) term, which was proposed to regularize the complexified process at lower temperatures.","We compare the results to reweghting from zero chemical potential to measure the bias that the inclusion of the stabilization term causes, depending on its strength.","At high temperatures the stabilization term is not needed.","At low temperatures (below deconfinement transition) the DS term has a beneficial stabilizing effect, but too strong DS term causes phase quenching on the system.","We observed that the bias of the dynamical stabilization can be to a good accuracy removed by extrapolating to zero dynamical stabilization force using a sigmoid fit."],"url":"http://arxiv.org/abs/2405.20709v1","category":"hep-lat"}
{"created":"2024-05-31 09:00:43","title":"Enhanced formation of interstellar complex organic molecules on carbon monoxide ice","abstract":"We investigate the role of carbon monoxide ice in the chemical evolution of prestellar cores using astrochemical rate equation models. We constrain the ratios of the binding energies on CO ice and H$_{2}$O ice for a series of adsorbates deemed important in diffusive chemistry on H$_{2}$O ices. We later include these ratios in our chemical reaction network model, where the binding and diffusion energies of icy species vary as a function of the surface composition. When the surface coverage of CO increases, the model shows an enhancement of O-bearing complex organic molecules, especially those formed from the intermediate products of CO hydrogenation (e.g. HCO) and CH$_{3}$/CH$_{2}$. Because the binding energy of CH$_{3}$/CH$_{2}$ is in the right range, its diffusion rate increases significantly with CO coverage. At $T>$14 K and with less influence, enhanced diffusion of HCO also contributes to the increase of the abundances of COM. We find, however, that chemistry is not always enhanced on CO ice and that the temperature and cosmic ray ionization rate of each astronomical object is crucial for this particular chemistry, revealing a highly non-trivial behavior that needs to be addressed on a per-case basis. Our results are highly relevant in the context of interstellar ice observations with JWST.","sentences":["We investigate the role of carbon monoxide ice in the chemical evolution of prestellar cores using astrochemical rate equation models.","We constrain the ratios of the binding energies on CO ice and H$_{2}$O ice for a series of adsorbates deemed important in diffusive chemistry on H$_{2}$O ices.","We later include these ratios in our chemical reaction network model, where the binding and diffusion energies of icy species vary as a function of the surface composition.","When the surface coverage of CO increases, the model shows an enhancement of O-bearing complex organic molecules, especially those formed from the intermediate products of CO hydrogenation (e.g. HCO) and CH$_{3}$/CH$_{2}$.","Because the binding energy of CH$_{3}$/CH$_{2}$ is in the right range, its diffusion rate increases significantly with CO coverage.","At $T>$14 K and with less influence, enhanced diffusion of HCO also contributes to the increase of the abundances of COM.","We find, however, that chemistry is not always enhanced on CO ice and that the temperature and cosmic ray ionization rate of each astronomical object is crucial for this particular chemistry, revealing a highly non-trivial behavior that needs to be addressed on a per-case basis.","Our results are highly relevant in the context of interstellar ice observations with JWST."],"url":"http://arxiv.org/abs/2405.20707v1","category":"astro-ph.GA"}
{"created":"2024-05-31 08:57:34","title":"A flexible numerical tool for large dynamic DC networks","abstract":"DC networks play an important role within the ongoing energy transition. In this context, simulations of designed and existing networks and their corresponding assets are a core tool to get insights and form a support to decision-making. Hereby, these simulations of DC networks are executed in the time domain. Due to the involved high frequencies and the used controllers, the equations that model these DC networks are stiff and highly oscillatory differential equations. By exploiting sparsity, we show that conventional adaptive time stepping schemes can be used efficiently for the time domain simulation of very large DC networks and that this scales linearly in the computational cost as the size of the networks increase.","sentences":["DC networks play an important role within the ongoing energy transition.","In this context, simulations of designed and existing networks and their corresponding assets are a core tool to get insights and form a support to decision-making.","Hereby, these simulations of DC networks are executed in the time domain.","Due to the involved high frequencies and the used controllers, the equations that model these DC networks are stiff and highly oscillatory differential equations.","By exploiting sparsity, we show that conventional adaptive time stepping schemes can be used efficiently for the time domain simulation of very large DC networks and that this scales linearly in the computational cost as the size of the networks increase."],"url":"http://arxiv.org/abs/2405.20704v1","category":"eess.SY"}
{"created":"2024-05-31 08:56:55","title":"Effect of antibody levels on the spread of disease in multiple infections","abstract":"There are complex interactions between antibody levels and epidemic propagation, the antibody level of an individual influences the probability of infection, and the spread of the virus influences the antibody level of each individual. There exist some viruses that, in their natural state, cause antibody levels in an infected individual to gradually decay. When these antibody levels decay to a certain point, the individual can be reinfected, such as with COVID 19. To describe their interaction, we introduce a novel mathematical model that incorporates the presence of an antibody retention rate to investigate the infection patterns of individuals who survive multiple infections. The model is composed of a system of stochastic differential equations to derive the equilibrium point and threshold of the model and presents rich experimental results of numerical simulations to further elucidate the propagation properties of the model. We find that the antibody decay rate strongly affects the propagation process, and also that different network structures have different sensitivities to the antibody decay rate, and that changes in the antibody decay rate cause stronger changes in the propagation process in Barabasi Albert networks. Furthermore, we investigate the stationary distribution of the number of infection states and the final antibody levels, and find that they both satisfy the normal distribution, but the standard deviation is small in the Barabasi Albert network. Finally, we explore the effect of individual antibody differences and decay rates on the final population antibody levels, and uncover that individual antibody differences do not affect the final mean antibody levels. The study offers valuable insights for epidemic prevention and control in practical applications.","sentences":["There are complex interactions between antibody levels and epidemic propagation, the antibody level of an individual influences the probability of infection, and the spread of the virus influences the antibody level of each individual.","There exist some viruses that, in their natural state, cause antibody levels in an infected individual to gradually decay.","When these antibody levels decay to a certain point, the individual can be reinfected, such as with COVID 19.","To describe their interaction, we introduce a novel mathematical model that incorporates the presence of an antibody retention rate to investigate the infection patterns of individuals who survive multiple infections.","The model is composed of a system of stochastic differential equations to derive the equilibrium point and threshold of the model and presents rich experimental results of numerical simulations to further elucidate the propagation properties of the model.","We find that the antibody decay rate strongly affects the propagation process, and also that different network structures have different sensitivities to the antibody decay rate, and that changes in the antibody decay rate cause stronger changes in the propagation process in Barabasi Albert networks.","Furthermore, we investigate the stationary distribution of the number of infection states and the final antibody levels, and find that they both satisfy the normal distribution, but the standard deviation is small in the Barabasi Albert network.","Finally, we explore the effect of individual antibody differences and decay rates on the final population antibody levels, and uncover that individual antibody differences do not affect the final mean antibody levels.","The study offers valuable insights for epidemic prevention and control in practical applications."],"url":"http://arxiv.org/abs/2405.20702v1","category":"q-bio.PE"}
{"created":"2024-05-31 08:49:43","title":"Equivariant Parabolic connections and stack of roots","abstract":"Let $X$ be a smooth complex projective variety equipped with an action of a linear algebraic group $G$ over $\\mathbb{C}$. Let $D$ be a reduced effective divisor on $X$ that is invariant under the $G$--action on $X$. Let $s_D$ be the canonical section of $\\mathcal{O}_X(D)$ vanishing along $D$. Given a positive integer $r$, consider the stack $\\mathfrak{X} := \\mathfrak{X}_{(\\mathcal{O}_X(D),\\, s_D,\\, r)}$ of $r$-th roots of $(\\mathcal{O}_X, s_D)$ together with the natural morphism $\\pi : \\mathfrak{X} \\to X$. Under the assumption that $G$ has no non-trivial characters, we show that the $G$--action on $X$ naturally lifts to a $G$--action on $\\mathfrak{X}$ such that $\\pi$ become $G$--equivariant, and the tautological invertible sheaf $\\mathscr{M}$ on $\\mathfrak{X}$ admits a linearization of this $G$--action. Finally, we define the notions of $G$--equivariant logarithmic connections on $\\mathfrak{X}$ and $G$--equivariant parabolic connections on $X$ with rational parabolic weights along $D$, and establish an equivalence between the category of $G$--equivariant logarithmic connections on $\\mathfrak{X}$ and the category of $G$--equivariant parabolic connections on $X$ with rational parabolic weights along $D$.","sentences":["Let $X$ be a smooth complex projective variety equipped with an action of a linear algebraic group $G$ over $\\mathbb{C}$. Let $D$ be a reduced effective divisor on $X$ that is invariant under the $G$--action on $X$. Let $s_D$ be the canonical section of $\\mathcal{O}_X(D)$ vanishing along $D$. Given a positive integer $r$, consider the stack $\\mathfrak{X} := \\mathfrak{X}_{(\\mathcal{O}_X(D),\\, s_D,\\, r)}$ of $r$-th roots of $(\\mathcal{O}_X, s_D)$ together with the natural morphism $\\pi : \\mathfrak{X} \\to X$. Under the assumption that $G$ has no non-trivial characters, we show that the $G$--action on $X$ naturally lifts to a $G$--action on $\\mathfrak{X}$ such that $\\pi$ become $G$--equivariant, and the tautological invertible sheaf $\\mathscr{M}$ on $\\mathfrak{X}$ admits a linearization of this $G$--action.","Finally, we define the notions of $G$--equivariant logarithmic connections on $\\mathfrak{X}$ and $G$--equivariant parabolic connections on $X$ with rational parabolic weights along $D$, and establish an equivalence between the category of $G$--equivariant logarithmic connections on $\\mathfrak{X}$ and the category of $G$--equivariant parabolic connections on $X$ with rational parabolic weights along $D$."],"url":"http://arxiv.org/abs/2405.20699v1","category":"math.AG"}
{"created":"2024-05-31 08:40:02","title":"Robust Stable Spiking Neural Networks","abstract":"Spiking neural networks (SNNs) are gaining popularity in deep learning due to their low energy budget on neuromorphic hardware. However, they still face challenges in lacking sufficient robustness to guard safety-critical applications such as autonomous driving. Many studies have been conducted to defend SNNs from the threat of adversarial attacks. This paper aims to uncover the robustness of SNN through the lens of the stability of nonlinear systems. We are inspired by the fact that searching for parameters altering the leaky integrate-and-fire dynamics can enhance their robustness. Thus, we dive into the dynamics of membrane potential perturbation and simplify the formulation of the dynamics. We present that membrane potential perturbation dynamics can reliably convey the intensity of perturbation. Our theoretical analyses imply that the simplified perturbation dynamics satisfy input-output stability. Thus, we propose a training framework with modified SNN neurons and to reduce the mean square of membrane potential perturbation aiming at enhancing the robustness of SNN. Finally, we experimentally verify the effectiveness of the framework in the setting of Gaussian noise training and adversarial training on the image classification task.","sentences":["Spiking neural networks (SNNs) are gaining popularity in deep learning due to their low energy budget on neuromorphic hardware.","However, they still face challenges in lacking sufficient robustness to guard safety-critical applications such as autonomous driving.","Many studies have been conducted to defend SNNs from the threat of adversarial attacks.","This paper aims to uncover the robustness of SNN through the lens of the stability of nonlinear systems.","We are inspired by the fact that searching for parameters altering the leaky integrate-and-fire dynamics can enhance their robustness.","Thus, we dive into the dynamics of membrane potential perturbation and simplify the formulation of the dynamics.","We present that membrane potential perturbation dynamics can reliably convey the intensity of perturbation.","Our theoretical analyses imply that the simplified perturbation dynamics satisfy input-output stability.","Thus, we propose a training framework with modified SNN neurons and to reduce the mean square of membrane potential perturbation aiming at enhancing the robustness of SNN.","Finally, we experimentally verify the effectiveness of the framework in the setting of Gaussian noise training and adversarial training on the image classification task."],"url":"http://arxiv.org/abs/2405.20694v1","category":"cs.NE"}
{"created":"2024-05-31 08:40:02","title":"Macroscopic Efimov effect of quantized vortex","abstract":"The three-body problem, from the chaotic motions of celestial bodies to complex microscopic particle interactions, has always been one of the most foundational yet intricate challenges in physics since its establishment. A key breakthrough in this domain is the Efimov effect, which represents a significant stride in what is now known as Efimov physics. Our study uncovers a macroscopic Efimov effect in a three-component Bose-Einstein Condensate (BEC) system. Through theoretical analysis and numerical simulation, it is verified that under certain conditions, three vortices form a bound state, while removing one vortex causes the others to unbind, demonstrating topological characteristics similar to the Borromean rings, hence termed the `vortex Efimov effect', signifying a novel topological phase transition. We propose several experimental approaches to realize this macroscopic Efimov effect, paving new paths not only in many-body physics but also in exploring quantum phase transitions and applications in quantum information.","sentences":["The three-body problem, from the chaotic motions of celestial bodies to complex microscopic particle interactions, has always been one of the most foundational yet intricate challenges in physics since its establishment.","A key breakthrough in this domain is the Efimov effect, which represents a significant stride in what is now known as Efimov physics.","Our study uncovers a macroscopic Efimov effect in a three-component Bose-Einstein Condensate (BEC) system.","Through theoretical analysis and numerical simulation, it is verified that under certain conditions, three vortices form a bound state, while removing one vortex causes the others to unbind, demonstrating topological characteristics similar to the Borromean rings, hence termed the `vortex Efimov effect', signifying a novel topological phase transition.","We propose several experimental approaches to realize this macroscopic Efimov effect, paving new paths not only in many-body physics but also in exploring quantum phase transitions and applications in quantum information."],"url":"http://arxiv.org/abs/2405.20695v1","category":"hep-th"}
{"created":"2024-05-31 08:34:44","title":"Analisis cuantitativo de riesgos utilizando \"MCSimulRisk\" como herramienta didactica","abstract":"Risk management is a fundamental discipline in project management, which includes, among others, quantitative risk analysis. Throughout several years of teaching, we have observed difficulties in students performing Monte Carlo Simulation within the quantitative analysis of risks. This article aims to present MCSimulRisk as a teaching tool that allows students to perform Monte Carlo simulation and apply it to projects of any complexity simply and intuitively. This tool allows for incorporating any uncertainty identified in the project into the model.","sentences":["Risk management is a fundamental discipline in project management, which includes, among others, quantitative risk analysis.","Throughout several years of teaching, we have observed difficulties in students performing Monte Carlo Simulation within the quantitative analysis of risks.","This article aims to present MCSimulRisk as a teaching tool that allows students to perform Monte Carlo simulation and apply it to projects of any complexity simply and intuitively.","This tool allows for incorporating any uncertainty identified in the project into the model."],"url":"http://arxiv.org/abs/2405.20688v1","category":"q-fin.RM"}
{"created":"2024-05-31 08:24:21","title":"Impact of Phase Selection on Accuracy and Scalability in Calculating Distributed Energy Resources Hosting Capacity","abstract":"Hosting capacity (HC) and dynamic operating envelopes (DOEs), defined as dynamic, time-varying HC, are calculated using three-phase optimal power flow (OPF) formulations. Due to the computational complexity of such optimisation problems, HC and DOE are often calculated by introducing certain assumptions and approximations, including the linearised OPF formulation, which we implement in the Python-based tool ppOPF. Furthermore, we investigate how assumptions of the distributed energy resource (DER) connection phase impact the objective function value and computational time in calculating HC and DOE in distribution networks of different sizes. The results are not unambiguous and show that it is not possible to determine the optimal connection phase without introducing binary variables since, no matter the case study, the highest objective function values are calculated with mixed integer OPF formulations. The difference is especially visible in a real-world low-voltage network in which the difference between different scenarios is up to 14 MW in a single day. However, binary variables make the problem computationally complex and increase computational time to several hours in the DOE calculation, even when the optimality gap different from zero is set.","sentences":["Hosting capacity (HC) and dynamic operating envelopes (DOEs), defined as dynamic, time-varying HC, are calculated using three-phase optimal power flow (OPF) formulations.","Due to the computational complexity of such optimisation problems, HC and DOE are often calculated by introducing certain assumptions and approximations, including the linearised OPF formulation, which we implement in the Python-based tool ppOPF.","Furthermore, we investigate how assumptions of the distributed energy resource (DER) connection phase impact the objective function value and computational time in calculating HC and DOE in distribution networks of different sizes.","The results are not unambiguous and show that it is not possible to determine the optimal connection phase without introducing binary variables since, no matter the case study, the highest objective function values are calculated with mixed integer OPF formulations.","The difference is especially visible in a real-world low-voltage network in which the difference between different scenarios is up to 14 MW in a single day.","However, binary variables make the problem computationally complex and increase computational time to several hours in the DOE calculation, even when the optimality gap different from zero is set."],"url":"http://arxiv.org/abs/2405.20682v1","category":"eess.SY"}
{"created":"2024-05-31 08:21:11","title":"No-Regret Learning for Fair Multi-Agent Social Welfare Optimization","abstract":"We consider the problem of online multi-agent Nash social welfare (NSW) maximization. While previous works of Hossain et al. [2021], Jones et al. [2023] study similar problems in stochastic multi-agent multi-armed bandits and show that $\\sqrt{T}$-regret is possible after $T$ rounds, their fairness measure is the product of all agents' rewards, instead of their NSW (that is, their geometric mean). Given the fundamental role of NSW in the fairness literature, it is more than natural to ask whether no-regret fair learning with NSW as the objective is possible. In this work, we provide a complete answer to this question in various settings. Specifically, in stochastic $N$-agent $K$-armed bandits, we develop an algorithm with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}\\right)$ regret and prove that the dependence on $T$ is tight, making it a sharp contrast to the $\\sqrt{T}$-regret bounds of Hossain et al. [2021], Jones et al. [2023]. We then consider a more challenging version of the problem with adversarial rewards. Somewhat surprisingly, despite NSW being a concave function, we prove that no algorithm can achieve sublinear regret. To circumvent such negative results, we further consider a setting with full-information feedback and design two algorithms with $\\sqrt{T}$-regret: the first one has no dependence on $N$ at all and is applicable to not just NSW but a broad class of welfare functions, while the second one has better dependence on $K$ and is preferable when $N$ is small. Finally, we also show that logarithmic regret is possible whenever there exists one agent who is indifferent about different arms.","sentences":["We consider the problem of online multi-agent Nash social welfare (NSW) maximization.","While previous works of Hossain et al.","[2021], Jones et al.","[2023] study similar problems in stochastic multi-agent multi-armed bandits and show that $\\sqrt{T}$-regret is possible after $T$ rounds, their fairness measure is the product of all agents' rewards, instead of their NSW (that is, their geometric mean).","Given the fundamental role of NSW in the fairness literature, it is more than natural to ask whether no-regret fair learning with NSW as the objective is possible.","In this work, we provide a complete answer to this question in various settings.","Specifically, in stochastic $N$-agent $K$-armed bandits, we develop an algorithm with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}\\right)$ regret and prove that the dependence on $T$ is tight, making it a sharp contrast to the $\\sqrt{T}$-regret bounds of Hossain et al.","[2021], Jones et al.","[2023].","We then consider a more challenging version of the problem with adversarial rewards.","Somewhat surprisingly, despite NSW being a concave function, we prove that no algorithm can achieve sublinear regret.","To circumvent such negative results, we further consider a setting with full-information feedback and design two algorithms with $\\sqrt{T}$-regret: the first one has no dependence on $N$ at all and is applicable to not just NSW but a broad class of welfare functions, while the second one has better dependence on $K$ and is preferable when $N$ is small.","Finally, we also show that logarithmic regret is possible whenever there exists one agent who is indifferent about different arms."],"url":"http://arxiv.org/abs/2405.20678v1","category":"cs.LG"}
{"created":"2024-05-31 17:54:52","title":"Mixed Diffusion for 3D Indoor Scene Synthesis","abstract":"Realistic conditional 3D scene synthesis significantly enhances and accelerates the creation of virtual environments, which can also provide extensive training data for computer vision and robotics research among other applications. Diffusion models have shown great performance in related applications, e.g., making precise arrangements of unordered sets. However, these models have not been fully explored in floor-conditioned scene synthesis problems. We present MiDiffusion, a novel mixed discrete-continuous diffusion model architecture, designed to synthesize plausible 3D indoor scenes from given room types, floor plans, and potentially pre-existing objects. We represent a scene layout by a 2D floor plan and a set of objects, each defined by its category, location, size, and orientation. Our approach uniquely implements structured corruption across the mixed discrete semantic and continuous geometric domains, resulting in a better conditioned problem for the reverse denoising step. We evaluate our approach on the 3D-FRONT dataset. Our experimental results demonstrate that MiDiffusion substantially outperforms state-of-the-art autoregressive and diffusion models in floor-conditioned 3D scene synthesis. In addition, our models can handle partial object constraints via a corruption-and-masking strategy without task specific training. We show MiDiffusion maintains clear advantages over existing approaches in scene completion and furniture arrangement experiments.","sentences":["Realistic conditional 3D scene synthesis significantly enhances and accelerates the creation of virtual environments, which can also provide extensive training data for computer vision and robotics research among other applications.","Diffusion models have shown great performance in related applications, e.g., making precise arrangements of unordered sets.","However, these models have not been fully explored in floor-conditioned scene synthesis problems.","We present MiDiffusion, a novel mixed discrete-continuous diffusion model architecture, designed to synthesize plausible 3D indoor scenes from given room types, floor plans, and potentially pre-existing objects.","We represent a scene layout by a 2D floor plan and a set of objects, each defined by its category, location, size, and orientation.","Our approach uniquely implements structured corruption across the mixed discrete semantic and continuous geometric domains, resulting in a better conditioned problem for the reverse denoising step.","We evaluate our approach on the 3D-FRONT dataset.","Our experimental results demonstrate that MiDiffusion substantially outperforms state-of-the-art autoregressive and diffusion models in floor-conditioned 3D scene synthesis.","In addition, our models can handle partial object constraints via a corruption-and-masking strategy without task specific training.","We show MiDiffusion maintains clear advantages over existing approaches in scene completion and furniture arrangement experiments."],"url":"http://arxiv.org/abs/2405.21066v1","category":"cs.CV"}
{"created":"2024-05-31 17:50:27","title":"Graph External Attention Enhanced Transformer","abstract":"The Transformer architecture has recently gained considerable attention in the field of graph representation learning, as it naturally overcomes several limitations of Graph Neural Networks (GNNs) with customized attention mechanisms or positional and structural encodings. Despite making some progress, existing works tend to overlook external information of graphs, specifically the correlation between graphs. Intuitively, graphs with similar structures should have similar representations. Therefore, we propose Graph External Attention (GEA) -- a novel attention mechanism that leverages multiple external node/edge key-value units to capture inter-graph correlations implicitly. On this basis, we design an effective architecture called Graph External Attention Enhanced Transformer (GEAET), which integrates local structure and global interaction information for more comprehensive graph representations. Extensive experiments on benchmark datasets demonstrate that GEAET achieves state-of-the-art empirical performance. The source code is available for reproducibility at: https://github.com/icm1018/GEAET.","sentences":["The Transformer architecture has recently gained considerable attention in the field of graph representation learning, as it naturally overcomes several limitations of Graph Neural Networks (GNNs) with customized attention mechanisms or positional and structural encodings.","Despite making some progress, existing works tend to overlook external information of graphs, specifically the correlation between graphs.","Intuitively, graphs with similar structures should have similar representations.","Therefore, we propose Graph External Attention (GEA) -- a novel attention mechanism that leverages multiple external node/edge key-value units to capture inter-graph correlations implicitly.","On this basis, we design an effective architecture called Graph External Attention Enhanced Transformer (GEAET), which integrates local structure and global interaction information for more comprehensive graph representations.","Extensive experiments on benchmark datasets demonstrate that GEAET achieves state-of-the-art empirical performance.","The source code is available for reproducibility at: https://github.com/icm1018/GEAET."],"url":"http://arxiv.org/abs/2405.21061v1","category":"cs.LG"}
{"created":"2024-05-31 17:38:19","title":"Designing for Fairness in Human-Robot Interactions","abstract":"The foundation of successful human collaboration is deeply rooted in the principles of fairness. As robots are increasingly prevalent in various parts of society where they are working alongside groups and teams of humans, their ability to understand and act according to principles of fairness becomes crucial for their effective integration. This is especially critical when robots are part of multi-human teams, where they must make continuous decisions regarding the allocation of resources. These resources can be material, such as tools, or communicative, such as gaze direction, and must be distributed fairly among team members to ensure optimal team performance and healthy group dynamics. Therefore, our research focuses on understanding how robots can effectively participate within human groups by making fair decisions while contributing positively to group dynamics and outcomes. In this paper, I discuss advances toward ensuring that robots are capable of considering human notions of fairness in their decision-making.","sentences":["The foundation of successful human collaboration is deeply rooted in the principles of fairness.","As robots are increasingly prevalent in various parts of society where they are working alongside groups and teams of humans, their ability to understand and act according to principles of fairness becomes crucial for their effective integration.","This is especially critical when robots are part of multi-human teams, where they must make continuous decisions regarding the allocation of resources.","These resources can be material, such as tools, or communicative, such as gaze direction, and must be distributed fairly among team members to ensure optimal team performance and healthy group dynamics.","Therefore, our research focuses on understanding how robots can effectively participate within human groups by making fair decisions while contributing positively to group dynamics and outcomes.","In this paper, I discuss advances toward ensuring that robots are capable of considering human notions of fairness in their decision-making."],"url":"http://arxiv.org/abs/2405.21044v1","category":"cs.RO"}
{"created":"2024-05-31 16:52:51","title":"G-Transformer for Conditional Average Potential Outcome Estimation over Time","abstract":"Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. Yet, existing neural methods for this task suffer from either (a) bias or (b) large variance. In order to address both limitations, we introduce the G-transformer (GT). Our GT is a novel, neural end-to-end model designed for unbiased, low-variance estimation of conditional average potential outcomes (CAPOs) over time. Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our GT across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.","sentences":["Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine.","Yet, existing neural methods for this task suffer from either (a) bias or (b) large variance.","In order to address both limitations, we introduce the G-transformer (GT).","Our GT is a novel, neural end-to-end model designed for unbiased, low-variance estimation of conditional average potential outcomes (CAPOs) over time.","Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting.","We evaluate the effectiveness of our GT across various experiments.","In sum, this work represents a significant step towards personalized decision-making from electronic health records."],"url":"http://arxiv.org/abs/2405.21012v1","category":"cs.LG"}
{"created":"2024-05-31 16:44:16","title":"The Renormalization Group for Large-Scale Structure: Primordial non-Gaussianities","abstract":"The renormalization group for large-scale structure (RG-LSS) describes the evolution of galaxy bias and stochastic parameters as a function of the cutoff $\\Lambda$. In this work, we introduce interaction vertices that describe primordial non-Gaussianity into the Wilson-Polchinski framework, thereby extending the free theory to the interacting case. The presence of these interactions forces us to include new operators and bias coefficients to the bias expansion to ensure closure under renormalization. We recover the previously-derived ``scale-dependent bias'' contributions, as well as a new (subdominant) stochastic contribution. We derive the renormalization group equations governing the RG-LSS for a large class of interactions which account for vertices at linear order in $f_{\\rm NL}$ that parametrize interacting scalar and massive spinning fields during inflation. Solving the RG equations, we show the evolution of the non-Gaussian contributions to galaxy clustering as a function of scale.","sentences":["The renormalization group for large-scale structure (RG-LSS) describes the evolution of galaxy bias and stochastic parameters as a function of the cutoff $\\Lambda$. In this work, we introduce interaction vertices that describe primordial non-Gaussianity into the Wilson-Polchinski framework, thereby extending the free theory to the interacting case.","The presence of these interactions forces us to include new operators and bias coefficients to the bias expansion to ensure closure under renormalization.","We recover the previously-derived ``scale-dependent bias'' contributions, as well as a new (subdominant) stochastic contribution.","We derive the renormalization group equations governing the RG-LSS for a large class of interactions which account for vertices at linear order in $f_{\\rm NL}$ that parametrize interacting scalar and massive spinning fields during inflation.","Solving the RG equations, we show the evolution of the non-Gaussian contributions to galaxy clustering as a function of scale."],"url":"http://arxiv.org/abs/2405.21002v1","category":"astro-ph.CO"}
{"created":"2024-05-31 16:40:41","title":"Full-Cycle Simulations of the Fermilab Booster","abstract":"The Proton Improvement Plan phase II (PIP-II) project currently under construction at FNAL will replace the existing 400 MeV normal conducting linac with a new 800 MeV superconducting linac. The beam power in the downstream rapid-cycling Booster synchrotron will be doubled by raising the machine cycle frequency from 15 to 20 Hz and by increasing the injected beam intensity by a factor 1.5. This has to be accomplished without raising uncontrolled losses beyond the administrative limit of 500 W. In addition, slip-stacking efficiency in the Recycler, the next machine in the accelerator chain, sets an upper limit on the longitudinal emittance of the beam delivered by the Booster. As part of an effort to better understand potential losses and emittance blow-up in the Booster, we have been conducting full cycle 6D simulations using the code PyORBIT. The simulations include space charge, wall impedance effects and transition crossing. In this paper, we discuss our experience with the code and present representative results for possible operational scenarios.","sentences":["The Proton Improvement Plan phase II (PIP-II) project currently under construction at FNAL will replace the existing 400 MeV normal conducting linac with a new 800 MeV superconducting linac.","The beam power in the downstream rapid-cycling Booster synchrotron will be doubled by raising the machine cycle frequency from 15 to 20 Hz and by increasing the injected beam intensity by a factor 1.5.","This has to be accomplished without raising uncontrolled losses beyond the administrative limit of 500 W.","In addition, slip-stacking efficiency in the Recycler, the next machine in the accelerator chain, sets an upper limit on the longitudinal emittance of the beam delivered by the Booster.","As part of an effort to better understand potential losses and emittance blow-up in the Booster, we have been conducting full cycle 6D simulations using the code PyORBIT.","The simulations include space charge, wall impedance effects and transition crossing.","In this paper, we discuss our experience with the code and present representative results for possible operational scenarios."],"url":"http://arxiv.org/abs/2405.20998v1","category":"physics.acc-ph"}
{"created":"2024-05-31 16:40:33","title":"Exact Algorithms for MaxCut on Split Graphs","abstract":"This paper presents an $O^{*}(1.42^{n})$ time algorithm for the Maximum Cut problem on split graphs, along with a subexponential time algorithm for its decision variant.","sentences":["This paper presents an $O^{*}(1.42^{n})$ time algorithm for the Maximum Cut problem on split graphs, along with a subexponential time algorithm for its decision variant."],"url":"http://arxiv.org/abs/2405.20599v1","category":"cs.DS"}
{"created":"2024-05-31 16:34:11","title":"Communication-Efficient Distributed Deep Learning via Federated Dynamic Averaging","abstract":"Driven by the ever-growing volume and decentralized nature of data, coupled with the escalating size of modern models, distributed deep learning (DDL) has been entrenched as the preferred paradigm for training. However, frequent synchronization of DL models, encompassing millions to many billions of parameters, creates a communication bottleneck, severely hindering scalability. Worse yet, DDL algorithms typically waste valuable bandwidth, and make themselves less practical in bandwidth-constrained federated settings, by relying on overly simplistic, periodic, and rigid synchronization schedules. To address these shortcomings, we propose Federated Dynamic Averaging (FDA), a communication-efficient DDL strategy that dynamically triggers synchronization based on the value of the model variance. Through extensive experiments across a wide range of learning tasks we demonstrate that FDA reduces communication cost by orders of magnitude, compared to both traditional and cutting-edge communication-efficient algorithms. Remarkably, FDA achieves this without sacrificing convergence speed - in stark contrast to the trade-offs encountered in the field. Additionally, we show that FDA maintains robust performance across diverse data heterogeneity settings.","sentences":["Driven by the ever-growing volume and decentralized nature of data, coupled with the escalating size of modern models, distributed deep learning (DDL) has been entrenched as the preferred paradigm for training.","However, frequent synchronization of DL models, encompassing millions to many billions of parameters, creates a communication bottleneck, severely hindering scalability.","Worse yet, DDL algorithms typically waste valuable bandwidth, and make themselves less practical in bandwidth-constrained federated settings, by relying on overly simplistic, periodic, and rigid synchronization schedules.","To address these shortcomings, we propose Federated Dynamic Averaging (FDA), a communication-efficient DDL strategy that dynamically triggers synchronization based on the value of the model variance.","Through extensive experiments across a wide range of learning tasks we demonstrate that FDA reduces communication cost by orders of magnitude, compared to both traditional and cutting-edge communication-efficient algorithms.","Remarkably, FDA achieves this without sacrificing convergence speed - in stark contrast to the trade-offs encountered in the field.","Additionally, we show that FDA maintains robust performance across diverse data heterogeneity settings."],"url":"http://arxiv.org/abs/2405.20988v1","category":"cs.LG"}
{"created":"2024-05-31 16:08:39","title":"Non-commutative Iwasawa theory of abelian varieties over global function fields","abstract":"Let $A$ be an abelian variety defined over a global function field $F$. We investigate the structure of the $p$-primary Selmer group $\\mathrm{Sel}(A/F_\\infty)$ for any prime number $p$ distinct from the characteristic of $F$, over $p$-adic Lie extensions $F_\\infty$ of $F$ which contain the cyclotomic $\\mathbb{Z}_p$-extension $F^{\\mathrm{cyc}}$. In particular, we prove that the Pontryagin dual of the Selmer group $\\mathrm{Sel}(A/F^\\mathrm{cyc})$ is a torsion $\\mathbb{Z}_p[[\\mathrm{Gal}(F^\\mathrm{cyc}/F)]]$-module with trivial $\\mu$-invariant, and we establish the $\\mathfrak{M}_H(G)$-conjecture of Coates-Fukaya-Kato-Sujatha-Venjakob for $A/F_\\infty$.   In view of the validity of the $\\mathfrak{M}_H(G)$-conjecture, it therefore makes sense to speak of the characteristic element (in the sense of Coates et al.) attached to the Pontryagin dual of $\\mathrm{Sel}(A/F_\\infty)$. We relate the order of vanishing of the characteristic elements, evaluated at Artin representations, to the corank of the Selmer group of the corresponding twist of $A$ over the base field $F$. Combining this with the deep results of Tate, Milne and Kato-Trihan, we show that the order of vanishing of the characteristic elements is equal to the order of vanishing of the $L$-function of $A/F$ at $s=1$ under appropriate assumptions.   Finally, we relate the generalised Euler characteristic of $\\mathrm{Sel}(A/F_\\infty)$ to the Euler characteristic of $\\mathrm{Sel}(A/F^{\\mathrm{cyc}})$. This is a natural analogue of Zerbes' result in the number field context and generalises previous results of Sechi and Valentino in the function field context.","sentences":["Let $A$ be an abelian variety defined over a global function field $F$. We investigate the structure of the $p$-primary Selmer group $\\mathrm{Sel}(A/F_\\infty)$ for any prime number $p$ distinct from the characteristic of $F$, over $p$-adic Lie extensions $F_\\infty$ of $F$ which contain the cyclotomic $\\mathbb{Z}_p$-extension $F^{\\mathrm{cyc}}$. In particular, we prove that the Pontryagin dual of the Selmer group $\\mathrm{Sel}(A/F^\\mathrm{cyc})$ is a torsion $\\mathbb{Z}_p[[\\mathrm{Gal}(F^\\mathrm{cyc}/F)]]$-module with trivial $\\mu$-invariant, and we establish the $\\mathfrak{M}_H(G)$-conjecture of Coates-Fukaya-Kato-Sujatha-Venjakob for $A/F_\\infty$.   ","In view of the validity of the $\\mathfrak{M}_H(G)$-conjecture, it therefore makes sense to speak of the characteristic element (in the sense of Coates et al.)","attached to the Pontryagin dual of $\\mathrm{Sel}(A/F_\\infty)$. We relate the order of vanishing of the characteristic elements, evaluated at Artin representations, to the corank of the Selmer group of the corresponding twist of $A$ over the base field $F$. Combining this with the deep results of Tate, Milne and Kato-Trihan, we show that the order of vanishing of the characteristic elements is equal to the order of vanishing of the $L$-function of $A/F$ at $s=1$ under appropriate assumptions.   ","Finally, we relate the generalised Euler characteristic of $\\mathrm{Sel}(A/F_\\infty)$ to the Euler characteristic of $\\mathrm{Sel}(A/F^{\\mathrm{cyc}})$.","This is a natural analogue of Zerbes' result in the number field context and generalises previous results of Sechi and Valentino in the function field context."],"url":"http://arxiv.org/abs/2405.20963v1","category":"math.NT"}
{"created":"2024-05-31 15:56:17","title":"Data Fusion for Heterogeneous Treatment Effect Estimation with Multi-Task Gaussian Processes","abstract":"Bridging the gap between internal and external validity is crucial for heterogeneous treatment effect estimation. Randomised controlled trials (RCTs), favoured for their internal validity due to randomisation, often encounter challenges in generalising findings due to strict eligibility criteria. Observational studies on the other hand, provide external validity advantages through larger and more representative samples but suffer from compromised internal validity due to unmeasured confounding. Motivated by these complementary characteristics, we propose a novel Bayesian nonparametric approach leveraging multi-task Gaussian processes to integrate data from both RCTs and observational studies. In particular, we introduce a parameter which controls the degree of borrowing between the datasets and prevents the observational dataset from dominating the estimation. The value of the parameter can be either user-set or chosen through a data-adaptive procedure. Our approach outperforms other methods in point predictions across the covariate support of the observational study, and furthermore provides a calibrated measure of uncertainty for the estimated treatment effects, which is crucial when extrapolating. We demonstrate the robust performance of our approach in diverse scenarios through multiple simulation studies and a real-world education randomised trial.","sentences":["Bridging the gap between internal and external validity is crucial for heterogeneous treatment effect estimation.","Randomised controlled trials (RCTs), favoured for their internal validity due to randomisation, often encounter challenges in generalising findings due to strict eligibility criteria.","Observational studies on the other hand, provide external validity advantages through larger and more representative samples but suffer from compromised internal validity due to unmeasured confounding.","Motivated by these complementary characteristics, we propose a novel Bayesian nonparametric approach leveraging multi-task Gaussian processes to integrate data from both RCTs and observational studies.","In particular, we introduce a parameter which controls the degree of borrowing between the datasets and prevents the observational dataset from dominating the estimation.","The value of the parameter can be either user-set or chosen through a data-adaptive procedure.","Our approach outperforms other methods in point predictions across the covariate support of the observational study, and furthermore provides a calibrated measure of uncertainty for the estimated treatment effects, which is crucial when extrapolating.","We demonstrate the robust performance of our approach in diverse scenarios through multiple simulation studies and a real-world education randomised trial."],"url":"http://arxiv.org/abs/2405.20957v1","category":"stat.ME"}
{"created":"2024-05-31 15:48:10","title":"snompy: a package for modelling scattering-type scanning near-field optical microscopy","abstract":"Scattering-type scanning near-field optical microscopy (s-SNOM) is a powerful technique for extreme subwavelength imaging and spectroscopy, with around 20 nm spatial resolution. But quantitative relationships between experiment and material properties requires modelling, which can be computationally and conceptually challenging. In this work, we present snompy an open-source Python library which contains implementations of two of the most common s-SNOM models, the finite dipole model (FDM) and the point dipole model (PDM). We show a series of typical uses for this package with demonstrations including simulating nano-Fourier transform infrared (FTIR) spectra and recovering permittivity from experimental s-SNOM data. We also discuss the challenges faced with this sort of modelling, such as competing descriptions of the models in literature, and finite size effects. We hope that snompy will make quantitative s-SNOM modelling more accessible to the wider research community, which will further empower the use of s-SNOM for investigating nanoscale material properties.","sentences":["Scattering-type scanning near-field optical microscopy (s-SNOM) is a powerful technique for extreme subwavelength imaging and spectroscopy, with around 20 nm spatial resolution.","But quantitative relationships between experiment and material properties requires modelling, which can be computationally and conceptually challenging.","In this work, we present snompy an open-source Python library which contains implementations of two of the most common s-SNOM models, the finite dipole model (FDM) and the point dipole model (PDM).","We show a series of typical uses for this package with demonstrations including simulating nano-Fourier transform infrared (FTIR) spectra and recovering permittivity from experimental s-SNOM data.","We also discuss the challenges faced with this sort of modelling, such as competing descriptions of the models in literature, and finite size effects.","We hope that snompy will make quantitative s-SNOM modelling more accessible to the wider research community, which will further empower the use of s-SNOM for investigating nanoscale material properties."],"url":"http://arxiv.org/abs/2405.20948v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 15:35:16","title":"Edible microlasers for monitoring authenticity and quality of food and pharmaceuticals","abstract":"Traceability, security and freshness monitoring are crucial to the food and pharmaceutical industries. Currently, barcodes and sensors are almost exclusively located on product packaging. Making them edible and introducing them into edible products could significantly enhance their functions. Here, several types of microlasers made entirely out of edible substances were developed. It is striking that olive oil already contains enough chlorophyll to be used as a laser when dispersed in water as droplets. The edible lasers can be embedded directly into edible products and serve as barcodes and sensors. Due to their much narrower spectral lines compared to fluorescent or color-changing sensors, they are significantly more sensitive to various environmental factors. The edible lasers were employed to sense sugar concentration, pH, the presence of bacteria, and exposure to too-high temperatures. They can also encode tens of data bits, such as manufacturer's information and expiration date. The microlasers are entirely safe for consumption, do not change the appearance and taste of food considerably, and are environmentally friendly. The developed barcodes and sensors could also be applied to non-edible items, such as cosmetic and agricultural products, for environmental monitoring and biomedical applications.","sentences":["Traceability, security and freshness monitoring are crucial to the food and pharmaceutical industries.","Currently, barcodes and sensors are almost exclusively located on product packaging.","Making them edible and introducing them into edible products could significantly enhance their functions.","Here, several types of microlasers made entirely out of edible substances were developed.","It is striking that olive oil already contains enough chlorophyll to be used as a laser when dispersed in water as droplets.","The edible lasers can be embedded directly into edible products and serve as barcodes and sensors.","Due to their much narrower spectral lines compared to fluorescent or color-changing sensors, they are significantly more sensitive to various environmental factors.","The edible lasers were employed to sense sugar concentration, pH, the presence of bacteria, and exposure to too-high temperatures.","They can also encode tens of data bits, such as manufacturer's information and expiration date.","The microlasers are entirely safe for consumption, do not change the appearance and taste of food considerably, and are environmentally friendly.","The developed barcodes and sensors could also be applied to non-edible items, such as cosmetic and agricultural products, for environmental monitoring and biomedical applications."],"url":"http://arxiv.org/abs/2405.20937v1","category":"physics.optics"}
{"created":"2024-05-31 15:21:51","title":"Unravelling the Use of Digital Twins to Assist Decision- and Policy-Making in Smart Cities","abstract":"This short paper represents a systematic literature review that sets the basis for the future development of a framework for digital twin-based decision support in the public sector, specifically for the smart city domain. The final aim of the research is to model context-specific digital twins for aiding the decision-making processes in smart cities and devise methods for defining the policy agenda. Overall, this short paper provides a foundation, based on the main concepts from existing literature, for further research in the role and applications of urban digital twins to assist decision- and policy-making in smart cities. The existing literature analyses common applications of digital twins in smart city development with a focus on supporting decision- and policy-making. Future work will centre on developing a digital-twin-based sustainable smart city and defining different scenarios concerning challenges of good governance, especially so-called wicked problems, in smaller-scale urban and non-urban contexts.","sentences":["This short paper represents a systematic literature review that sets the basis for the future development of a framework for digital twin-based decision support in the public sector, specifically for the smart city domain.","The final aim of the research is to model context-specific digital twins for aiding the decision-making processes in smart cities and devise methods for defining the policy agenda.","Overall, this short paper provides a foundation, based on the main concepts from existing literature, for further research in the role and applications of urban digital twins to assist decision- and policy-making in smart cities.","The existing literature analyses common applications of digital twins in smart city development with a focus on supporting decision- and policy-making.","Future work will centre on developing a digital-twin-based sustainable smart city and defining different scenarios concerning challenges of good governance, especially so-called wicked problems, in smaller-scale urban and non-urban contexts."],"url":"http://arxiv.org/abs/2405.20916v1","category":"cs.CY"}
{"created":"2024-05-31 15:18:44","title":"Nonparametric regression on random geometric graphs sampled from submanifolds","abstract":"We consider the nonparametric regression problem when the covariates are located on an unknown smooth compact submanifold of a Euclidean space. Under defining a random geometric graph structure over the covariates we analyze the asymptotic frequentist behaviour of the posterior distribution arising from Bayesian priors designed through random basis expansion in the graph Laplacian eigenbasis. Under Holder smoothness assumption on the regression function and the density of the covariates over the submanifold, we prove that the posterior contraction rates of such methods are minimax optimal (up to logarithmic factors) for any positive smoothness index.","sentences":["We consider the nonparametric regression problem when the covariates are located on an unknown smooth compact submanifold of a Euclidean space.","Under defining a random geometric graph structure over the covariates we analyze the asymptotic frequentist behaviour of the posterior distribution arising from Bayesian priors designed through random basis expansion in the graph Laplacian eigenbasis.","Under Holder smoothness assumption on the regression function and the density of the covariates over the submanifold, we prove that the posterior contraction rates of such methods are minimax optimal (up to logarithmic factors) for any positive smoothness index."],"url":"http://arxiv.org/abs/2405.20909v1","category":"math.ST"}
{"created":"2024-05-31 14:55:31","title":"S4Fusion: Saliency-aware Selective State Space Model for Infrared Visible Image Fusion","abstract":"As one of the tasks in Image Fusion, Infrared and Visible Image Fusion aims to integrate complementary information captured by sensors of different modalities into a single image. The Selective State Space Model (SSSM), known for its ability to capture long-range dependencies, has demonstrated its potential in the field of computer vision. However, in image fusion, current methods underestimate the potential of SSSM in capturing the global spatial information of both modalities. This limitation prevents the simultaneous consideration of the global spatial information from both modalities during interaction, leading to a lack of comprehensive perception of salient targets. Consequently, the fusion results tend to bias towards one modality instead of adaptively preserving salient targets. To address this issue, we propose the Saliency-aware Selective State Space Fusion Model (S4Fusion). In our S4Fusion, the designed Cross-Modal Spatial Awareness Module (CMSA) can simultaneously focus on global spatial information from both modalities while facilitating their interaction, thereby comprehensively capturing complementary information. Additionally, S4Fusion leverages a pre-trained network to perceive uncertainty in the fused images. By minimizing this uncertainty, S4Fusion adaptively highlights salient targets from both images. Extensive experiments demonstrate that our approach produces high-quality images and enhances performance in downstream tasks.","sentences":["As one of the tasks in Image Fusion, Infrared and Visible Image Fusion aims to integrate complementary information captured by sensors of different modalities into a single image.","The Selective State Space Model (SSSM), known for its ability to capture long-range dependencies, has demonstrated its potential in the field of computer vision.","However, in image fusion, current methods underestimate the potential of SSSM in capturing the global spatial information of both modalities.","This limitation prevents the simultaneous consideration of the global spatial information from both modalities during interaction, leading to a lack of comprehensive perception of salient targets.","Consequently, the fusion results tend to bias towards one modality instead of adaptively preserving salient targets.","To address this issue, we propose the Saliency-aware Selective State Space Fusion Model (S4Fusion).","In our S4Fusion, the designed Cross-Modal Spatial Awareness Module (CMSA) can simultaneously focus on global spatial information from both modalities while facilitating their interaction, thereby comprehensively capturing complementary information.","Additionally, S4Fusion leverages a pre-trained network to perceive uncertainty in the fused images.","By minimizing this uncertainty, S4Fusion adaptively highlights salient targets from both images.","Extensive experiments demonstrate that our approach produces high-quality images and enhances performance in downstream tasks."],"url":"http://arxiv.org/abs/2405.20881v1","category":"cs.CV"}
{"created":"2024-05-31 14:48:36","title":"Simultaneous Measurement of Thermal Conductivity and Heat Capacity Across Diverse Materials Using the Square-Pulsed Source (SPS) Technique","abstract":"State-of-the-art techniques like dual-frequency Time-Domain Thermoreflectance (TDTR) and Frequency-Domain Thermoreflectance (FDTR) offer superb capability for simultaneous measurements of thermal conductivity and heat capacity with a spatial resolution on the order of 10 {\\mu}m. However, their applicability is limited to highly conductive materials with an in-plane thermal conductivity exceeding 10 W/(m*K). In this paper, we introduce the Square-Pulsed Source (SPS) technique, offering a novel approach to concurrently measure thermal conductivity and heat capacity with a 10 {\\mu}m spatial resolution, while significantly extending the measurable thermal conductivity range to an unprecedented low of 0.1 W/(m*K), offering enhanced versatility. To demonstrate and validate its efficacy, we conducted measurements on various standard materials--PMMA, silica, sapphire, silicon, and diamond--spanning a wide thermal conductivity range from 0.1 to 2000 W/(m*K). The obtained results exhibit remarkable agreement with literature values, with a typical measurement uncertainty of less than 10% across the entire thermal conductivity range. By providing a unique capability to characterize both highly and lowly conductive materials with micron-scale spatial resolution, the SPS method opens new avenues for understanding and engineering thermal properties across diverse applications.","sentences":["State-of-the-art techniques like dual-frequency Time-Domain Thermoreflectance (TDTR) and Frequency-Domain Thermoreflectance (FDTR) offer superb capability for simultaneous measurements of thermal conductivity and heat capacity with a spatial resolution on the order of 10 {\\mu}m.","However, their applicability is limited to highly conductive materials with an in-plane thermal conductivity exceeding 10 W/(m*K).","In this paper, we introduce the Square-Pulsed Source (SPS) technique, offering a novel approach to concurrently measure thermal conductivity and heat capacity with a 10 {\\mu}m spatial resolution, while significantly extending the measurable thermal conductivity range to an unprecedented low of 0.1 W/(m*K), offering enhanced versatility.","To demonstrate and validate its efficacy, we conducted measurements on various standard materials--PMMA, silica, sapphire, silicon, and diamond--spanning a wide thermal conductivity range from 0.1 to 2000 W/(m*K).","The obtained results exhibit remarkable agreement with literature values, with a typical measurement uncertainty of less than 10% across the entire thermal conductivity range.","By providing a unique capability to characterize both highly and lowly conductive materials with micron-scale spatial resolution, the SPS method opens new avenues for understanding and engineering thermal properties across diverse applications."],"url":"http://arxiv.org/abs/2405.20870v1","category":"physics.app-ph"}
{"created":"2024-05-31 14:37:21","title":"Homotopy theory of pre-Calabi-Yau morphisms","abstract":"In this article we study the homotopy theory of pre-Calabi-Yau morphisms, viewing them as Maurer-Cartan elements of an $L_{\\infty}$-algebra. We give two different notions of homotopy: a notion of weak homotopy for morphisms between $d$-pre-Calabi-Yau categories whose underlying graded quivers on the domain (resp. codomain) are the same, and a notion of homotopy for morphisms between fixed pre-Calabi-Yau categories $(\\mathcal{A},s_{d+1}M_{\\mathcal{A}})$ and $(\\mathcal{B},s_{d+1}M_{\\mathcal{B}})$. Then, we show that the notion of homotopy is stable under composition and that homotopy equivalences are quasi-isomorphisms. Finally, we prove that the functor constructed by the author in a previous article between the category of pre-Calabi-Yau categories and the partial category of $A_{\\infty}$-categories of the form $\\mathcal{A}\\oplus\\mathcal{A}^*[d-1]$, for $\\mathcal{A}$ a graded quiver, together with hat morphisms sends homotopic $d$-pre-Calabi-Yau morphisms to weak homotopic $A_{\\infty}$-morphisms.","sentences":["In this article we study the homotopy theory of pre-Calabi-Yau morphisms, viewing them as Maurer-Cartan elements of an $L_{\\infty}$-algebra.","We give two different notions of homotopy: a notion of weak homotopy for morphisms between $d$-pre-Calabi-Yau categories whose underlying graded quivers on the domain (resp.","codomain) are the same, and a notion of homotopy for morphisms between fixed pre-Calabi-Yau categories $(\\mathcal{A},s_{d+1}M_{\\mathcal{A}})$ and $(\\mathcal{B},s_{d+1}M_{\\mathcal{B}})$. Then, we show that the notion of homotopy is stable under composition and that homotopy equivalences are quasi-isomorphisms.","Finally, we prove that the functor constructed by the author in a previous article between the category of pre-Calabi-Yau categories and the partial category of $A_{\\infty}$-categories of the form $\\mathcal{A}\\oplus\\mathcal{A}^*[d-1]$, for $\\mathcal{A}$ a graded quiver, together with hat morphisms sends homotopic $d$-pre-Calabi-Yau morphisms to weak homotopic $A_{\\infty}$-morphisms."],"url":"http://arxiv.org/abs/2405.20854v1","category":"math.KT"}
{"created":"2024-05-31 14:26:24","title":"Stability in quadratic variation","abstract":"Consider a sequence of cadlag processes $\\{X^n\\}_n$, and some fixed function $f$. If $f$ is continuous then under several modes of convergence $X^n\\to X$ implies corresponding convergence of $f(X^n)\\to f(X)$, due to continuous mapping. We study conditions (on $f$, $\\{X^n\\}_n$ and $X$) under which convergence of $X^n\\to X$ implies $\\left[f(X^n)-f(X)\\right]\\to 0$. While interesting in its own right, this also directly relates (through integration by parts and the Kunita-Watanabe inequality) to convergence of integrators in the sense $\\int_0^t Y_{s-}df(X^n_s)\\to\\int_0^t Y_{s-}df(X_s)$. We use two different types of quadratic variations, weak sense and strong sense which our two main results deal with. For weak sense quadratic variations we show stability when $f\\in C^1$, $\\{X^n\\}_n,X$ are Dirichlet processes defined as in \\cite{NonCont} $X^n\\xrightarrow{a.s.}X$, $[X^n-X]\\xrightarrow{a.s.}0$ and $\\{(X^n)^*_t\\}_n$ is bounded in probability. For strong sense quadratic variations we are able to relax the conditions on $f$ to being the primitive function of a cadlag function but with the additional assumption on $X$, that the continuous and discontinuous parts of $X$ are independent stochastic processes (this assumption is not imposed on $\\{X^n\\}_n$ however), and $\\{X^n\\}_n,X$ are Dirichlet processes with quadratic variations along any stopping time refining sequence. To prove the result regarding strong sense quadratic variation we prove a new It\\^o decomposition for this setting.","sentences":["Consider a sequence of cadlag processes $\\{X^n\\}_n$, and some fixed function $f$. If $f$ is continuous then under several modes of convergence $X^n\\to X$ implies corresponding convergence of $f(X^n)\\to f(X)$, due to continuous mapping.","We study conditions (on $f$, $\\{X^n\\}_n$ and $X$) under which convergence of $X^n\\to X$ implies $\\left[f(X^n)-f(X)\\right]\\to 0$.","While interesting in its own right, this also directly relates (through integration by parts and the Kunita-Watanabe inequality) to convergence of integrators in the sense $\\int_0^t Y_{s-}df(X^n_s)\\to\\int_0^t Y_{s-}df(X_s)$. We use two different types of quadratic variations, weak sense and strong sense which our two main results deal with.","For weak sense quadratic variations we show stability when $f\\in C^1$, $\\{X^n\\}_n,X$ are Dirichlet processes defined as in \\cite{NonCont} $X^n\\xrightarrow{a.s.}X$, $[X^n-X]\\xrightarrow{a.s.}0$ and $\\{(X^n)^*_t\\}_n$ is bounded in probability.","For strong sense quadratic variations we are able to relax the conditions on $f$ to being the primitive function of a cadlag function but with the additional assumption on $X$, that the continuous and discontinuous parts of $X$ are independent stochastic processes (this assumption is not imposed on $\\{X^n\\}_n$ however), and $\\{X^n\\}_n,X$ are Dirichlet processes with quadratic variations along any stopping time refining sequence.","To prove the result regarding strong sense quadratic variation we prove a new It\\^o decomposition for this setting."],"url":"http://arxiv.org/abs/2405.20839v1","category":"math.PR"}
{"created":"2024-05-31 14:14:14","title":"Heuristic evaluations of back support, shoulder support, hand grip strength support, and sit-stand support exoskeletons using universal design principles","abstract":"Occupational exoskeletons promise to reduce the incidence of musculoskeletal injuries; however, we do not know if their designs allow universal use by all workers. We also do not know how easy the tasks of assembling, donning, doffing, and disassembling exoskeletons are. The purpose of our study was to heuristically evaluate a back support, a shoulder support, a handgrip strength support, and a sit-stand exoskeleton for how well they are designed for universal use when assembling, donning, doffing, and disassembling the exoskeleton. Seven evaluators used universal design principles and associated criteria to independently evaluate and rate four exoskeletons when assembling, donning, doffing, and disassembling the devices. The rating scale was a Likert-type scale, where a rating of 1 represented not at all, and a rating of 5 represented an excellent design with respect to the universal design criteria for the task. The results indicate that providing perceptible information to the user, making the design equitable to use for a diverse set of users, making the design simple and intuitive to use with adequate feedback, and designing to prevent user errors, and when errors are made, allowing the user to recover quickly from the errors, were rated poorly. Assembling and donning tasks presented the most challenges.","sentences":["Occupational exoskeletons promise to reduce the incidence of musculoskeletal injuries; however, we do not know if their designs allow universal use by all workers.","We also do not know how easy the tasks of assembling, donning, doffing, and disassembling exoskeletons are.","The purpose of our study was to heuristically evaluate a back support, a shoulder support, a handgrip strength support, and a sit-stand exoskeleton for how well they are designed for universal use when assembling, donning, doffing, and disassembling the exoskeleton.","Seven evaluators used universal design principles and associated criteria to independently evaluate and rate four exoskeletons when assembling, donning, doffing, and disassembling the devices.","The rating scale was a Likert-type scale, where a rating of 1 represented not at all, and a rating of 5 represented an excellent design with respect to the universal design criteria for the task.","The results indicate that providing perceptible information to the user, making the design equitable to use for a diverse set of users, making the design simple and intuitive to use with adequate feedback, and designing to prevent user errors, and when errors are made, allowing the user to recover quickly from the errors, were rated poorly.","Assembling and donning tasks presented the most challenges."],"url":"http://arxiv.org/abs/2405.20819v1","category":"cs.HC"}
{"created":"2024-05-31 14:03:36","title":"Myths of German Graphite in World War II, with Original Translations","abstract":"We re-examine the common narrative that a 1941 experimental error by physicists Walther Bothe and Peter Jensen led Germany to abandon graphite as a reactor moderator during World War II. We first detail the history of both German and American graphite experiments, noting that the Americans faced similar setbacks but succeeded only at the end of a costly 18-month graphite purification program. We then use Monte Carlo N-Particle simulations to reconstruct Bothe's 1941 experiment. We find the thermal absorption cross section of Bothe's Siemens electrographite to be 12.2 mb, in contrast to his reported 7.9 mb. This discrepancy arises because the neutrons in Bothe's experiment did not reach thermal equilibrium, leading to an underestimation of neutron absorption. Additionally, despite misconceptions that the Germans were unaware of boron impurities, we share evidence that Wilhelm Hanle accurately measured boron and cadmium impurities in the electrographite. To support our findings, we provide 9 excerpted and 3 complete English translations of classified wartime reports by Heisenberg, Joos, Bothe, Jensen, H\\\"ocker, Hanle, and Kremer. Our work aims to illuminate the rational constraints behind Germany's decision to forgo graphite moderation.","sentences":["We re-examine the common narrative that a 1941 experimental error by physicists Walther Bothe and Peter Jensen led Germany to abandon graphite as a reactor moderator during World War II.","We first detail the history of both German and American graphite experiments, noting that the Americans faced similar setbacks but succeeded only at the end of a costly 18-month graphite purification program.","We then use Monte Carlo N-Particle simulations to reconstruct Bothe's 1941 experiment.","We find the thermal absorption cross section of Bothe's Siemens electrographite to be 12.2 mb, in contrast to his reported 7.9 mb.","This discrepancy arises because the neutrons in Bothe's experiment did not reach thermal equilibrium, leading to an underestimation of neutron absorption.","Additionally, despite misconceptions that the Germans were unaware of boron impurities, we share evidence that Wilhelm Hanle accurately measured boron and cadmium impurities in the electrographite.","To support our findings, we provide 9 excerpted and 3 complete English translations of classified wartime reports by Heisenberg, Joos, Bothe, Jensen, H\\\"ocker, Hanle, and Kremer.","Our work aims to illuminate the rational constraints behind Germany's decision to forgo graphite moderation."],"url":"http://arxiv.org/abs/2405.20801v1","category":"physics.hist-ph"}
{"created":"2024-05-31 14:00:44","title":"Rough Transformers: Lightweight Continuous-Time Sequence Modelling with Path Signatures","abstract":"Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In these settings, traditional sequence-based recurrent models struggle. To overcome this, researchers often replace recurrent architectures with Neural ODE-based models to account for irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length. To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs. In particular, we propose \\textit{multi-view signature attention}, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing. We find that, on a variety of time-series-related tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources.","sentences":["Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals.","In these settings, traditional sequence-based recurrent models struggle.","To overcome this, researchers often replace recurrent architectures with Neural ODE-based models to account for irregularly sampled data and use Transformer-based architectures to account for long-range dependencies.","Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length.","To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs.","In particular, we propose \\textit{multi-view signature attention}, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing.","We find that, on a variety of time-series-related tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources."],"url":"http://arxiv.org/abs/2405.20799v1","category":"stat.ML"}
{"created":"2024-05-31 13:54:25","title":"Model Interpretation and Explainability: Towards Creating Transparency in Prediction Models","abstract":"Explainable AI (XAI) has a counterpart in analytical modeling which we refer to as model explainability. We tackle the issue of model explainability in the context of prediction models. We analyze a dataset of loans from a credit card company and apply three stages: execute and compare four different prediction methods, apply the best known explainability techniques in the current literature to the model training sets to identify feature importance (FI) (static case), and finally to cross-check whether the FI set holds up under what if prediction scenarios for continuous and categorical variables (dynamic case). We found inconsistency in FI identification between the static and dynamic cases. We summarize the state of the art in model explainability and suggest further research to advance the field.","sentences":["Explainable AI (XAI) has a counterpart in analytical modeling which we refer to as model explainability.","We tackle the issue of model explainability in the context of prediction models.","We analyze a dataset of loans from a credit card company and apply three stages: execute and compare four different prediction methods, apply the best known explainability techniques in the current literature to the model training sets to identify feature importance (FI) (static case), and finally to cross-check whether the FI set holds up under what if prediction scenarios for continuous and categorical variables (dynamic case).","We found inconsistency in FI identification between the static and dynamic cases.","We summarize the state of the art in model explainability and suggest further research to advance the field."],"url":"http://arxiv.org/abs/2405.20794v1","category":"cs.LG"}
{"created":"2024-05-31 13:28:37","title":"Reinforcement Learning for Sociohydrology","abstract":"In this study, we discuss how reinforcement learning (RL) provides an effective and efficient framework for solving sociohydrology problems. The efficacy of RL for these types of problems is evident because of its ability to update policies in an iterative manner - something that is also foundational to sociohydrology, where we are interested in representing the co-evolution of human-water interactions. We present a simple case study to demonstrate the implementation of RL in a problem of runoff reduction through management decisions related to changes in land-use land-cover (LULC). We then discuss the benefits of RL for these types of problems and share our perspectives on the future research directions in this area.","sentences":["In this study, we discuss how reinforcement learning (RL) provides an effective and efficient framework for solving sociohydrology problems.","The efficacy of RL for these types of problems is evident because of its ability to update policies in an iterative manner - something that is also foundational to sociohydrology, where we are interested in representing the co-evolution of human-water interactions.","We present a simple case study to demonstrate the implementation of RL in a problem of runoff reduction through management decisions related to changes in land-use land-cover (LULC).","We then discuss the benefits of RL for these types of problems and share our perspectives on the future research directions in this area."],"url":"http://arxiv.org/abs/2405.20772v1","category":"cs.LG"}
{"created":"2024-05-31 12:22:07","title":"On $r$-primitive $k$-normal polynomials with two prescribed coefficients","abstract":"This article investigates the existence of an $r$-primitive $k$-normal polynomial, defined as the minimal polynomial of an $r$-primitive $k$-normal element in $\\mathbb{F}_{q^n}$, with a specified degree $n$ and two given coefficients over the finite field $\\mathbb{F}_{q}$. Here, $q$ represents an odd prime power, and $n$ is an integer. The article establishes a sufficient condition to ensure the existence of such a polynomial. Using this condition, it is demonstrated that a $2$-primitive $2$-normal polynomial of degree $n$ always exists over $\\mathbb{F}_{q}$ when both $q\\geq 11$ and $n\\geq 15$. However, for the range $10\\leq n\\leq 14$, uncertainty remains regarding the existence of such a polynomial for $71$ specific pairs of $(q,n)$. Moreover, when $q<11$, the number of uncertain pairs reduces to $16$. Furthermore, for the case of $n=9$, extensive computational power is employed using SageMath software, and it is found that the count of such uncertain pairs is reduced to $3988$.","sentences":["This article investigates the existence of an $r$-primitive $k$-normal polynomial, defined as the minimal polynomial of an $r$-primitive $k$-normal element in $\\mathbb{F}_{q^n}$, with a specified degree $n$ and two given coefficients over the finite field $\\mathbb{F}_{q}$. Here, $q$ represents an odd prime power, and $n$ is an integer.","The article establishes a sufficient condition to ensure the existence of such a polynomial.","Using this condition, it is demonstrated that a $2$-primitive $2$-normal polynomial of degree $n$ always exists over $\\mathbb{F}_{q}$ when both $q\\geq 11$ and $n\\geq 15$.","However, for the range $10\\leq n\\leq 14$, uncertainty remains regarding the existence of such a polynomial for $71$ specific pairs of $(q,n)$.","Moreover, when $q<11$, the number of uncertain pairs reduces to $16$. Furthermore, for the case of $n=9$, extensive computational power is employed using SageMath software, and it is found that the count of such uncertain pairs is reduced to $3988$."],"url":"http://arxiv.org/abs/2405.20760v1","category":"math.NT"}
{"created":"2024-05-31 09:12:55","title":"A Needle in a Cosmic Haystack: A Review of FRB Search Techniques","abstract":"Ephemeral Fast Radio Bursts (FRBs) must be powered by some of the most energetic processes in the Universe. That makes them highly interesting in their own right and as precise probes for estimating cosmological parameters. This field thus poses a unique challenge: FRBs must be detected promptly and immediately localised and studied based only on that single millisecond-duration flash. The problem is that the burst occurrence is highly unpredictable and that their distance strongly suppresses their brightness. Since the discovery of FRBs in single-dish archival data in 2007, detection software has evolved tremendously. Pipelines now detect bursts in real-time within a matter of seconds, operate on interferometers, buffer high-time and frequency resolution data, and issue real-time alerts to other observatories for rapid multi-wavelength follow-up. In this paper, we review the components that comprise a FRB search software pipeline, we discuss the proven techniques that were adopted from pulsar searches, we highlight newer, more efficient techniques for detecting FRBs, and we conclude by discussing the proposed novel future methodologies that may power the search for FRBs in the era of big data astronomy.","sentences":["Ephemeral Fast Radio Bursts (FRBs) must be powered by some of the most energetic processes in the Universe.","That makes them highly interesting in their own right and as precise probes for estimating cosmological parameters.","This field thus poses a unique challenge: FRBs must be detected promptly and immediately localised and studied based only on that single millisecond-duration flash.","The problem is that the burst occurrence is highly unpredictable and that their distance strongly suppresses their brightness.","Since the discovery of FRBs in single-dish archival data in 2007, detection software has evolved tremendously.","Pipelines now detect bursts in real-time within a matter of seconds, operate on interferometers, buffer high-time and frequency resolution data, and issue real-time alerts to other observatories for rapid multi-wavelength follow-up.","In this paper, we review the components that comprise a FRB search software pipeline, we discuss the proven techniques that were adopted from pulsar searches, we highlight newer, more efficient techniques for detecting FRBs, and we conclude by discussing the proposed novel future methodologies that may power the search for FRBs in the era of big data astronomy."],"url":"http://arxiv.org/abs/2405.20716v1","category":"astro-ph.HE"}
{"created":"2024-05-31 09:12:28","title":"Transforming Japan Real Estate","abstract":"The Japanese real estate market, valued over 35 trillion USD, offers significant investment opportunities. Accurate rent and price forecasting could provide a substantial competitive edge. This paper explores using alternative data variables to predict real estate performance in 1100 Japanese municipalities. A comprehensive house price index was created, covering all municipalities from 2005 to the present, using a dataset of over 5 million transactions. This core dataset was enriched with economic factors spanning decades, allowing for price trajectory predictions.   The findings show that alternative data variables can indeed forecast real estate performance effectively. Investment signals based on these variables yielded notable returns with low volatility. For example, the net migration ratio delivered an annualized return of 4.6% with a Sharpe ratio of 1.5. Taxable income growth and new dwellings ratio also performed well, with annualized returns of 4.1% (Sharpe ratio of 1.3) and 3.3% (Sharpe ratio of 0.9), respectively. When combined with transformer models to predict risk-adjusted returns 4 years in advance, the model achieved an R-squared score of 0.28, explaining nearly 30% of the variation in future municipality prices.   These results highlight the potential of alternative data variables in real estate investment. They underscore the need for further research to identify more predictive factors. Nonetheless, the evidence suggests that such data can provide valuable insights into real estate price drivers, enabling more informed investment decisions in the Japanese market.","sentences":["The Japanese real estate market, valued over 35 trillion USD, offers significant investment opportunities.","Accurate rent and price forecasting could provide a substantial competitive edge.","This paper explores using alternative data variables to predict real estate performance in 1100 Japanese municipalities.","A comprehensive house price index was created, covering all municipalities from 2005 to the present, using a dataset of over 5 million transactions.","This core dataset was enriched with economic factors spanning decades, allowing for price trajectory predictions.   ","The findings show that alternative data variables can indeed forecast real estate performance effectively.","Investment signals based on these variables yielded notable returns with low volatility.","For example, the net migration ratio delivered an annualized return of 4.6% with a Sharpe ratio of 1.5.","Taxable income growth and new dwellings ratio also performed well, with annualized returns of 4.1% (Sharpe ratio of 1.3) and 3.3% (Sharpe ratio of 0.9), respectively.","When combined with transformer models to predict risk-adjusted returns 4 years in advance, the model achieved an R-squared score of 0.28, explaining nearly 30% of the variation in future municipality prices.   ","These results highlight the potential of alternative data variables in real estate investment.","They underscore the need for further research to identify more predictive factors.","Nonetheless, the evidence suggests that such data can provide valuable insights into real estate price drivers, enabling more informed investment decisions in the Japanese market."],"url":"http://arxiv.org/abs/2405.20715v1","category":"cs.CE"}
{"created":"2024-05-31 09:08:58","title":"Large low-field magnetocaloric response in a ferromagnetic gadolinium orthophosphate","abstract":"Bulk magnetic and thermodynamic measurements, along with mean-field calculations, were conducted on the ferromagnetic K3Gd5(PO4)6 powders. No magnetic ordering was observed until 2 K, while the application of an external field B > 1 T resulted in the splitting of the Gd3+ ground state multiplet and induced a non-cooperative Schottky effect. The average nearest-neighbor exchange strength |J1/kB| is determined to be 0.017 K, which leads to a remarkably large low field magnetic entropy change {\\Delta}Sm = 36.2 J kg-1 K-1 under applied field change B = 2 T at temperature T = 2 K, as well as a maximum adiabatic temperature change Tad = 10.9 K. We contend that ferromagnetic gadolinium orthophosphates serve as a promising reservoir for exploring advanced magnetic refrigerants applicable under low magnetic fields.","sentences":["Bulk magnetic and thermodynamic measurements, along with mean-field calculations, were conducted on the ferromagnetic K3Gd5(PO4)6 powders.","No magnetic ordering was observed until 2 K, while the application of an external field B > 1 T resulted in the splitting of the Gd3+ ground state multiplet and induced a non-cooperative Schottky effect.","The average nearest-neighbor exchange strength |J1/kB| is determined to be 0.017 K, which leads to a remarkably large low field magnetic entropy change {\\Delta}Sm = 36.2 J kg-1 K-1 under applied field change B = 2 T at temperature T = 2 K, as well as a maximum adiabatic temperature change Tad","= 10.9 K. We contend that ferromagnetic gadolinium orthophosphates serve as a promising reservoir for exploring advanced magnetic refrigerants applicable under low magnetic fields."],"url":"http://arxiv.org/abs/2405.20714v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 08:47:24","title":"A Lightweight Method for Defending Against UAF Vulnerabilities","abstract":"The widespread presence of Use-After-Free (UAF) vulnerabilities poses a serious threat to software security, with dangling pointers being considered the primary cause of these vulnerabilities. However, existing methods for defending against UAF vulnerabilities by eliminating dangling pointers need to interrupt the program's execution when encountering pointer assignment operations to look up the objects pointed to by the pointers and store the memory addresses of the pointers in a specific data structure. This makes these methods not lightweight. To overcome this drawback, we propose a novel approach called LightDE. This method does not require storing the memory addresses of pointers or locating the objects pointed to by pointers during program execution. LightDE uses our proposed structure-sensitive pointer analysis method to determine the objects pointed to by pointers and stores the pointing relationships in the program's data segment during program compilation. Since LightDE only needs to check whether the pointers identified by the pointer analysis point to the released objects when the objects are released, LightDE is very lightweight. Our experimental results show that LightDE can effectively defend against UAF vulnerabilities, and the additional performance overhead it introduces is very low.","sentences":["The widespread presence of Use-After-Free (UAF) vulnerabilities poses a serious threat to software security, with dangling pointers being considered the primary cause of these vulnerabilities.","However, existing methods for defending against UAF vulnerabilities by eliminating dangling pointers need to interrupt the program's execution when encountering pointer assignment operations to look up the objects pointed to by the pointers and store the memory addresses of the pointers in a specific data structure.","This makes these methods not lightweight.","To overcome this drawback, we propose a novel approach called LightDE.","This method does not require storing the memory addresses of pointers or locating the objects pointed to by pointers during program execution.","LightDE uses our proposed structure-sensitive pointer analysis method to determine the objects pointed to by pointers and stores the pointing relationships in the program's data segment during program compilation.","Since LightDE only needs to check whether the pointers identified by the pointer analysis point to the released objects when the objects are released, LightDE is very lightweight.","Our experimental results show that LightDE can effectively defend against UAF vulnerabilities, and the additional performance overhead it introduces is very low."],"url":"http://arxiv.org/abs/2405.20697v1","category":"cs.CR"}
{"created":"2024-05-31 08:39:02","title":"R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction","abstract":"3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R2-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown integration bias in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches by 0.93 dB in PSNR and 0.014 in SSIM. Crucially, it delivers high-quality results in 3 minutes, which is 12x faster than NeRF-based methods and on par with traditional algorithms. The superior performance and rapid convergence of our method highlight its practical value.","sentences":["3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction.","However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored.","This paper introduces R2-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction.","By carefully deriving X-ray rasterization functions, we discover a previously unknown integration bias in the standard 3DGS formulation, which hampers accurate volume retrieval.","To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians.","Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer.","Extensive experiments demonstrate that our method outperforms state-of-the-art approaches by 0.93 dB in PSNR and 0.014 in SSIM.","Crucially, it delivers high-quality results in 3 minutes, which is 12x faster than NeRF-based methods and on par with traditional algorithms.","The superior performance and rapid convergence of our method highlight its practical value."],"url":"http://arxiv.org/abs/2405.20693v1","category":"eess.IV"}
{"created":"2024-05-31 17:29:39","title":"A-PETE: Adaptive Prototype Explanations of Tree Ensembles","abstract":"The need for interpreting machine learning models is addressed through prototype explanations within the context of tree ensembles. An algorithm named Adaptive Prototype Explanations of Tree Ensembles (A-PETE) is proposed to automatise the selection of prototypes for these classifiers. Its unique characteristics is using a specialised distance measure and a modified k-medoid approach. Experiments demonstrated its competitive predictive accuracy with respect to earlier explanation algorithms. It also provides a a sufficient number of prototypes for the purpose of interpreting the random forest classifier.","sentences":["The need for interpreting machine learning models is addressed through prototype explanations within the context of tree ensembles.","An algorithm named Adaptive Prototype Explanations of Tree Ensembles (A-PETE) is proposed to automatise the selection of prototypes for these classifiers.","Its unique characteristics is using a specialised distance measure and a modified k-medoid approach.","Experiments demonstrated its competitive predictive accuracy with respect to earlier explanation algorithms.","It also provides a a sufficient number of prototypes for the purpose of interpreting the random forest classifier."],"url":"http://arxiv.org/abs/2405.21036v1","category":"cs.LG"}
{"created":"2024-05-31 17:07:15","title":"Improved Techniques for Optimization-Based Jailbreaking on Large Language Models","abstract":"Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of \"Sure\" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed $\\mathcal{I}$-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate. The code is released at https://github.com/jiaxiaojunQAQ/I-GCG.","sentences":["Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment.","Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques.","Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory.","In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG.","We first observe that the single target template of \"Sure\" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs.","Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation.","Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed $\\mathcal{I}$-GCG.","In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track).","The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate.","The code is released at https://github.com/jiaxiaojunQAQ/I-GCG."],"url":"http://arxiv.org/abs/2405.21018v1","category":"cs.LG"}
{"created":"2024-05-31 16:01:46","title":"Reiterated Periodic Homogenization of Parabolic Monotone Operators with Nonstandard Growth","abstract":"In this paper, we are interested in reiterated periodic homogenization for a family of parabolic problems with nonstandard growth monotone operators leading to Orlicz spaces. The aim of this work is the determination of the global homogenized problem on the one hand and the macroscopic homogenized problem on the other hand, via the reiterated two-scale convergence method adapted to this type of spaces.","sentences":["In this paper, we are interested in reiterated periodic homogenization for a family of parabolic problems with nonstandard growth monotone operators leading to Orlicz spaces.","The aim of this work is the determination of the global homogenized problem on the one hand and the macroscopic homogenized problem on the other hand, via the reiterated two-scale convergence method adapted to this type of spaces."],"url":"http://arxiv.org/abs/2405.20960v1","category":"math.AP"}
{"created":"2024-05-31 15:54:01","title":"Aligning Multiclass Neural Network Classifier Criterion with Task Performance via $F_\u03b2$-Score","abstract":"Multiclass neural network classifiers are typically trained using cross-entropy loss. Following training, the performance of this same neural network is evaluated using an application-specific metric based on the multiclass confusion matrix, such as the Macro $F_\\beta$-Score. It is questionable whether the use of cross-entropy will yield a classifier that aligns with the intended application-specific performance criteria, particularly in scenarios where there is a need to emphasize one aspect of classifier performance. For example, if greater precision is preferred over recall, the $\\beta$ value in the $F_\\beta$ evaluation metric can be adjusted accordingly, but the cross-entropy objective remains unaware of this preference during training. We propose a method that addresses this training-evaluation gap for multiclass neural network classifiers such that users can train these models informed by the desired final $F_\\beta$-Score. Following prior work in binary classification, we utilize the concepts of the soft-set confusion matrices and a piecewise-linear approximation of the Heaviside step function. Our method extends the $2 \\times 2$ binary soft-set confusion matrix to a multiclass $d \\times d$ confusion matrix and proposes dynamic adaptation of the threshold value $\\tau$, which parameterizes the piecewise-linear Heaviside approximation during run-time. We present a theoretical analysis that shows that our method can be used to optimize for a soft-set based approximation of Macro-$F_\\beta$ that is a consistent estimator of Macro-$F_\\beta$, and our extensive experiments show the practical effectiveness of our approach.","sentences":["Multiclass neural network classifiers are typically trained using cross-entropy loss.","Following training, the performance of this same neural network is evaluated using an application-specific metric based on the multiclass confusion matrix, such as the Macro $F_\\beta$-Score.","It is questionable whether the use of cross-entropy will yield a classifier that aligns with the intended application-specific performance criteria, particularly in scenarios where there is a need to emphasize one aspect of classifier performance.","For example, if greater precision is preferred over recall, the $\\beta$ value in the $F_\\beta$ evaluation metric can be adjusted accordingly, but the cross-entropy objective remains unaware of this preference during training.","We propose a method that addresses this training-evaluation gap for multiclass neural network classifiers such that users can train these models informed by the desired final $F_\\beta$-Score.","Following prior work in binary classification, we utilize the concepts of the soft-set confusion matrices and a piecewise-linear approximation of the Heaviside step function.","Our method extends the $2 \\times 2$ binary soft-set confusion matrix to a multiclass $d \\times d$ confusion matrix and proposes dynamic adaptation of the threshold value $\\tau$, which parameterizes the piecewise-linear Heaviside approximation during run-time.","We present a theoretical analysis that shows that our method can be used to optimize for a soft-set based approximation of Macro-$F_\\beta$ that is a consistent estimator of Macro-$F_\\beta$, and our extensive experiments show the practical effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.20954v1","category":"cs.LG"}
{"created":"2024-05-31 15:21:12","title":"Josephson junctions, superconducting circuits, and qubit for quantum technologies","abstract":"In the realm of physics, a pivotal moment occurred six decades ago when Brian Josephson made a groundbreaking prediction, setting in motion a series of events that would eventually earn him the prestigious Nobel Prize eleven years later. This prediction centered around what is now known as the Josephson effect, a phenomenon with far-reaching implications. At the heart of this effect lies the Josephson junction (JJ), a device that has become a linchpin in various scientific applications. This chapter delves into the foundational principles of the Josephson effect and the remarkable properties of JJs. From their role in metrology to their application in radiation detectors, these junctions have ushered in a new era of electronics. Exploiting the unique features of superconductive devices, such as high speed, low dissipation, and dispersion, JJs find today practical implementation in the development of superconductive qubits and nanotechnology applications.","sentences":["In the realm of physics, a pivotal moment occurred six decades ago when Brian Josephson made a groundbreaking prediction, setting in motion a series of events that would eventually earn him the prestigious Nobel Prize eleven years later.","This prediction centered around what is now known as the Josephson effect, a phenomenon with far-reaching implications.","At the heart of this effect lies the Josephson junction (JJ), a device that has become a linchpin in various scientific applications.","This chapter delves into the foundational principles of the Josephson effect and the remarkable properties of JJs.","From their role in metrology to their application in radiation detectors, these junctions have ushered in a new era of electronics.","Exploiting the unique features of superconductive devices, such as high speed, low dissipation, and dispersion, JJs find today practical implementation in the development of superconductive qubits and nanotechnology applications."],"url":"http://arxiv.org/abs/2405.20911v1","category":"cond-mat.supr-con"}
{"created":"2024-05-31 15:01:40","title":"Upper Bounds on Large Deviations of Dirichlet $L$-functions in the $q$-aspect","abstract":"We prove a result on the large deviations of the central values of even primitive Dirichlet $L$-functions with a given modulus. For $V\\sim \\alpha\\log\\log q$ with $0<\\alpha<1$, we show that \\begin{equation}\\nonumber\\frac{1}{\\varphi(q)} \\# \\left\\{\\chi \\text{ even, primitive mod }q: \\log \\left|L\\left(\\chi,\\frac{1}{2}\\right)\\right| >V\\right\\}\\ll \\frac{e^{-\\frac{V^2}{\\log\\log q}}}{\\sqrt{\\log\\log q}}.\\end{equation} This yields the sharp upper bound for the fractional moments of central values of Dirichlet $L$-functions proved by Gao, upon noting that the number of even, primitive characters with modulus $q$ is $\\frac{\\varphi(q)}{2}+O(1).$ The proof is an adaptation to the $q$-aspect of the recursive scheme developed by Arguin, Bourgade and Radziwill for the local maxima of the Riemann zeta function, and applied by Arguin and Bailey to the large deviations in the $t$-aspect. We go further and get bounds on the case where $V=o(\\log\\log q)$. These bounds are not expected to be sharp, but the discrepancy from the Central Limit Theorem estimate grows very slowly with $q$. The method involves a formula for the twisted mollified second moment of central values of Dirichlet $L$-functions, building on the work of Iwaniec and Sarnak.","sentences":["We prove a result on the large deviations of the central values of even primitive Dirichlet $L$-functions with a given modulus.","For $V\\sim \\alpha\\log\\log q$ with $0<\\alpha<1$, we show that \\begin{equation}\\nonumber\\frac{1}{\\varphi(q)} \\# \\left\\{\\chi \\text{ even, primitive mod }q: \\log \\left|L\\left(\\chi,\\frac{1}{2}\\right)\\right| >V\\right\\}\\ll \\frac{e^{-\\frac{V^2}{\\log\\log q}}}{\\sqrt{\\log\\log q}}.\\end{equation} This yields the sharp upper bound for the fractional moments of central values of Dirichlet $L$-functions proved by Gao, upon noting that the number of even, primitive characters with modulus $q$ is $\\frac{\\varphi(q)}{2}+O(1).$","The proof is an adaptation to the $q$-aspect of the recursive scheme developed by Arguin, Bourgade and Radziwill for the local maxima of the Riemann zeta function, and applied by Arguin and Bailey to the large deviations in the $t$-aspect.","We go further and get bounds on the case where $V=o(\\log\\log q)$.","These bounds are not expected to be sharp, but the discrepancy from the Central Limit Theorem estimate grows very slowly with $q$. The method involves a formula for the twisted mollified second moment of central values of Dirichlet $L$-functions, building on the work of Iwaniec and Sarnak."],"url":"http://arxiv.org/abs/2405.20888v1","category":"math.NT"}
{"created":"2024-05-31 10:07:24","title":"Federated Random Forest for Partially Overlapping Clinical Data","abstract":"In the healthcare sector, a consciousness surrounding data privacy and corresponding data protection regulations, as well as heterogeneous and non-harmonized data, pose huge challenges to large-scale data analysis. Moreover, clinical data often involves partially overlapping features, as some observations may be missing due to various reasons, such as differences in procedures, diagnostic tests, or other recorded patient history information across hospitals or institutes. To address the challenges posed by partially overlapping features and incomplete data in clinical datasets, a comprehensive approach is required. Particularly in the domain of medical data, promising outcomes are achieved by federated random forests whenever features align. However, for most standard algorithms, like random forest, it is essential that all data sets have identical parameters. Therefore, in this work the concept of federated random forest is adapted to a setting with partially overlapping features. Moreover, our research assesses the effectiveness of the newly developed federated random forest models for partially overlapping clinical data. For aggregating the federated, globally optimized model, only features available locally at each site can be used. We tackled two issues in federation: (i) the quantity of involved parties, (ii) the varying overlap of features. This evaluation was conducted across three clinical datasets. The federated random forest model even in cases where only a subset of features overlaps consistently demonstrates superior performance compared to its local counterpart. This holds true across various scenarios, including datasets with imbalanced classes. Consequently, federated random forests for partially overlapped data offer a promising solution to transcend barriers in collaborative research and corporate cooperation.","sentences":["In the healthcare sector, a consciousness surrounding data privacy and corresponding data protection regulations, as well as heterogeneous and non-harmonized data, pose huge challenges to large-scale data analysis.","Moreover, clinical data often involves partially overlapping features, as some observations may be missing due to various reasons, such as differences in procedures, diagnostic tests, or other recorded patient history information across hospitals or institutes.","To address the challenges posed by partially overlapping features and incomplete data in clinical datasets, a comprehensive approach is required.","Particularly in the domain of medical data, promising outcomes are achieved by federated random forests whenever features align.","However, for most standard algorithms, like random forest, it is essential that all data sets have identical parameters.","Therefore, in this work the concept of federated random forest is adapted to a setting with partially overlapping features.","Moreover, our research assesses the effectiveness of the newly developed federated random forest models for partially overlapping clinical data.","For aggregating the federated, globally optimized model, only features available locally at each site can be used.","We tackled two issues in federation: (i) the quantity of involved parties, (ii) the varying overlap of features.","This evaluation was conducted across three clinical datasets.","The federated random forest model even in cases where only a subset of features overlaps consistently demonstrates superior performance compared to its local counterpart.","This holds true across various scenarios, including datasets with imbalanced classes.","Consequently, federated random forests for partially overlapped data offer a promising solution to transcend barriers in collaborative research and corporate cooperation."],"url":"http://arxiv.org/abs/2405.20738v1","category":"cs.LG"}
{"created":"2024-05-31 17:38:49","title":"An Attention-Based Multi-Context Convolutional Encoder-Decoder Neural Network for Work Zone Traffic Impact Prediction","abstract":"Work zone is one of the major causes of non-recurrent traffic congestion and road incidents. Despite the significance of its impact, studies on predicting the traffic impact of work zones remain scarce. In this paper, we propose a data integration pipeline that enhances the utilization of work zone and traffic data from diversified platforms, and introduce a novel deep learning model to predict the traffic speed and incident likelihood during planned work zone events. The proposed model transforms traffic patterns into 2D space-time images for both model input and output and employs an attention-based multi-context convolutional encoder-decoder architecture to capture the spatial-temporal dependencies between work zone events and traffic variations. Trained and validated on four years of archived work zone traffic data from Maryland, USA, the model demonstrates superior performance over baseline models in predicting traffic speed, incident likelihood, and inferred traffic attributes such as queue length and congestion timings (i.e., start time and duration). Specifically, the proposed model outperforms the baseline models by reducing the prediction error of traffic speed by 5% to 34%, queue length by 11% to 29%, congestion timing by 6% to 17%, and increasing the accuracy of incident predictions by 5% to 7%. Consequently, this model offers substantial promise for enhancing the planning and traffic management of work zones.","sentences":["Work zone is one of the major causes of non-recurrent traffic congestion and road incidents.","Despite the significance of its impact, studies on predicting the traffic impact of work zones remain scarce.","In this paper, we propose a data integration pipeline that enhances the utilization of work zone and traffic data from diversified platforms, and introduce a novel deep learning model to predict the traffic speed and incident likelihood during planned work zone events.","The proposed model transforms traffic patterns into 2D space-time images for both model input and output and employs an attention-based multi-context convolutional encoder-decoder architecture to capture the spatial-temporal dependencies between work zone events and traffic variations.","Trained and validated on four years of archived work zone traffic data from Maryland, USA, the model demonstrates superior performance over baseline models in predicting traffic speed, incident likelihood, and inferred traffic attributes such as queue length and congestion timings (i.e., start time and duration).","Specifically, the proposed model outperforms the baseline models by reducing the prediction error of traffic speed by 5% to 34%, queue length by 11% to 29%, congestion timing by 6% to 17%, and increasing the accuracy of incident predictions by 5% to 7%.","Consequently, this model offers substantial promise for enhancing the planning and traffic management of work zones."],"url":"http://arxiv.org/abs/2405.21045v1","category":"cs.LG"}
{"created":"2024-05-31 16:26:08","title":"Neural Gaussian Scale-Space Fields","abstract":"Gaussian scale spaces are a cornerstone of signal representation and processing, with applications in filtering, multiscale analysis, anti-aliasing, and many more. However, obtaining such a scale space is costly and cumbersome, in particular for continuous representations such as neural fields. We present an efficient and lightweight method to learn the fully continuous, anisotropic Gaussian scale space of an arbitrary signal. Based on Fourier feature modulation and Lipschitz bounding, our approach is trained self-supervised, i.e., training does not require any manual filtering. Our neural Gaussian scale-space fields faithfully capture multiscale representations across a broad range of modalities, and support a diverse set of applications. These include images, geometry, light-stage data, texture anti-aliasing, and multiscale optimization.","sentences":["Gaussian scale spaces are a cornerstone of signal representation and processing, with applications in filtering, multiscale analysis, anti-aliasing, and many more.","However, obtaining such a scale space is costly and cumbersome, in particular for continuous representations such as neural fields.","We present an efficient and lightweight method to learn the fully continuous, anisotropic Gaussian scale space of an arbitrary signal.","Based on Fourier feature modulation and Lipschitz bounding, our approach is trained self-supervised, i.e., training does not require any manual filtering.","Our neural Gaussian scale-space fields faithfully capture multiscale representations across a broad range of modalities, and support a diverse set of applications.","These include images, geometry, light-stage data, texture anti-aliasing, and multiscale optimization."],"url":"http://arxiv.org/abs/2405.20980v1","category":"cs.CV"}
{"created":"2024-05-31 15:41:19","title":"Optimizing EPR pulses for broadband excitation and refocusing","abstract":"In this paper, we numerically optimize broadband pulse shapes that maximize Hahn echo amplitudes. Pulses are parameterized as neural networks (NN), nonlinear amplitude limited Fourier series (FS), and discrete time series (DT). These are compared to an optimized choice of the conventional hyperbolic secant (HS) pulse shape. A power constraint is included, as are realistic shape distortions due to power amplifier nonlinearity and the transfer function of the microwave resonator. We find that the NN, FS, and DT parameterizations perform equivalently, offer improvements over the best HS pulses, and contain a large number of equivalent optimal solutions, implying the flexibility to include further constraints or optimization goals in future designs.","sentences":["In this paper, we numerically optimize broadband pulse shapes that maximize Hahn echo amplitudes.","Pulses are parameterized as neural networks (NN), nonlinear amplitude limited Fourier series (FS), and discrete time series (DT).","These are compared to an optimized choice of the conventional hyperbolic secant (HS) pulse shape.","A power constraint is included, as are realistic shape distortions due to power amplifier nonlinearity and the transfer function of the microwave resonator.","We find that the NN, FS, and DT parameterizations perform equivalently, offer improvements over the best HS pulses, and contain a large number of equivalent optimal solutions, implying the flexibility to include further constraints or optimization goals in future designs."],"url":"http://arxiv.org/abs/2405.20943v1","category":"physics.chem-ph"}
{"created":"2024-05-31 15:40:58","title":"$G$-tables and the Poisson structure of the even cohomology of cotangent bundle of the Heisenberg Lie group","abstract":"In the first part of the paper, we define the concept of a $G$-table of a $G$-(co)algebra and we compute the $G$-table of some $G$-(co)algebras (here a $G$-algebra is an algebra on which $G$ acts, semisimply, by algebra automorphisms). The $G$-table of a $G$-(co)algebra $A$ is a set of scalars that provides very precise and concise information about both the algebra structure and the $G$-module structure of $A$. In particular, the ordinary multiplication table of $A$ can be derived from the $G$-table of $A$. From the $G$-table of a $G$-algebra $A$ we define a plain algebra $P(A)$ associated to it and we present some basic functoriality results about $P$. Obtaining the $G$-table of a given $G$-algebra $A$ requires a considerable amount of work but, the result, is a very powerful tool as shown in the second part of the paper. Here we compute the $SL(2)$-tables of the Poisson algebra structure of the even-degree part of the cohomology associated to the cotangent bundle of the 3-dimensional Heisenberg Lie group with Lie algebra $h$, that is $H_E(h)=H_E^{\\bullet}(h,\\bigwedge^{\\bullet}h)$. This Poisson $SL(2)$-algebra has dimension 18. From these $SL(2)$-tables we deduce that the underlying Lie algebra of $H_E(h)$ is isomorphic to $gl(3)\\ltimes gl(3)_{ab}$ with the first factor acting on the second (abelian) one by the adjoint representation. We find it remarkable that the Lie algebra structure on $H_{E}(h)$ contains a semisimple Lie subalgebra (in this case $sl(3)$) strictly larger than the Levi factor of $\\text{Der}(h)$, which in this case is $sl(2)\\subset H^{1}(h,h)$. This means that the Levi factor of the Lie algebra $H_{E}(h)$ has nontrivial elements outside $H^{1}(h,h)$. Finally, this leads us to find a family of commutative Poisson algebras whose underlying Lie structure is $gl(n)\\ltimes gl(n)_{ab}$ (arbitrary $n$) such that, for $n=3$, is isomorphic to $H_E(h)$.","sentences":["In the first part of the paper, we define the concept of a $G$-table of a $G$-(co)algebra and we compute the $G$-table of some $G$-(co)algebras (here a $G$-algebra is an algebra on which $G$ acts, semisimply, by algebra automorphisms).","The $G$-table of a $G$-(co)algebra $A$ is a set of scalars that provides very precise and concise information about both the algebra structure and the $G$-module structure of $A$.","In particular, the ordinary multiplication table of $A$ can be derived from the $G$-table of $A$.","From the $G$-table of a $G$-algebra $A$ we define a plain algebra $P(A)$ associated to it and we present some basic functoriality results about $P$. Obtaining the $G$-table of a given $G$-algebra $A$ requires a considerable amount of work but, the result, is a very powerful tool as shown in the second part of the paper.","Here we compute the $SL(2)$-tables of the Poisson algebra structure of the even-degree part of the cohomology associated to the cotangent bundle of the 3-dimensional Heisenberg Lie group with Lie algebra $h$, that is $H_E(h)=H_E^{\\bullet}(h,\\bigwedge^{\\bullet}h)$.","This Poisson $SL(2)$-algebra has dimension 18.","From these $SL(2)$-tables we deduce that the underlying Lie algebra of $H_E(h)$ is isomorphic to $gl(3)\\ltimes gl(3)_{ab}$ with the first factor acting on the second (abelian) one by the adjoint representation.","We find it remarkable that the Lie algebra structure on $H_{E}(h)$ contains a semisimple Lie subalgebra (in this case $sl(3)$) strictly larger than the Levi factor of $\\text{Der}(h)$, which in this case is $sl(2)\\subset H^{1}(h,h)$.","This means that the Levi factor of the Lie algebra $H_{E}(h)$ has nontrivial elements outside $H^{1}(h,h)$. Finally, this leads us to find a family of commutative Poisson algebras whose underlying Lie structure is $gl(n)\\ltimes gl(n)_{ab}$ (arbitrary $n$) such that, for $n=3$, is isomorphic to $H_E(h)$."],"url":"http://arxiv.org/abs/2405.20942v1","category":"math.RT"}
{"created":"2024-05-31 15:39:21","title":"Lecture on the combinatorial algebraic method for computing algebraic integrals","abstract":"Consider an algebraic equation $P(x,y)=0$ where $P\\in \\mathbb C[x,y] $ (or $\\mathbb F[x,y]$ with $\\mathbb F\\subset \\mathbb C$ a subfield) is a bivariate polynomial, it defines a plane algebraic curve. We provide an efficient method for computing integrals of the type $ \\int_\\gamma R(x,y)dx $ where $R(x,y)\\in \\mathbb C(x,y) $ is any rational fraction, and $y$ is solution of $P(x,y)=0$, and $\\gamma$ any Jordan arc open or closed on the plane algebraic curve. The method uses only algebraic and combinatorial manipulations, it rests on the combinatorics of the Newton's polygon. We illustrate it with many practical examples.","sentences":["Consider an algebraic equation $P(x,y)=0$ where $P\\in \\mathbb C[x,y] $ (or $\\mathbb F[x,y]$ with $\\mathbb F\\subset \\mathbb C$ a subfield) is a bivariate polynomial, it defines a plane algebraic curve.","We provide an efficient method for computing integrals of the type $ \\int_\\gamma R(x,y)dx $ where $R(x,y)\\in \\mathbb C(x,y) $ is any rational fraction, and $y$ is solution of $P(x,y)=0$, and $\\gamma$ any Jordan arc open or closed on the plane algebraic curve.","The method uses only algebraic and combinatorial manipulations, it rests on the combinatorics of the Newton's polygon.","We illustrate it with many practical examples."],"url":"http://arxiv.org/abs/2405.20941v1","category":"math-ph"}
{"created":"2024-05-31 14:28:28","title":"Convergence rate of the Euler-Maruyama scheme to density dependent SDEs driven by $\u03b1$-stable additive noise","abstract":"In this paper, we establish the weak convergence rate of density-dependent stochastic differential equations with bounded drift driven by $\\alpha$-stable processes with $\\alpha\\in(1,2)$. The well-posedness of these equations has been previously obtained in \\cite{wu2023well}. We derive an explicit convergence rate in total variation for the Euler-Maruyama scheme, employing a technique rooted in \\cite{hao2023}.","sentences":["In this paper, we establish the weak convergence rate of density-dependent stochastic differential equations with bounded drift driven by $\\alpha$-stable processes with $\\alpha\\in(1,2)$. The well-posedness of these equations has been previously obtained in \\cite{wu2023well}.","We derive an explicit convergence rate in total variation for the Euler-Maruyama scheme, employing a technique rooted in \\cite{hao2023}."],"url":"http://arxiv.org/abs/2405.20840v1","category":"math.PR"}
{"created":"2024-05-31 14:24:39","title":"Solving partial differential equations with sampled neural networks","abstract":"Approximation of solutions to partial differential equations (PDE) is an important problem in computational science and engineering. Using neural networks as an ansatz for the solution has proven a challenge in terms of training time and approximation accuracy. In this contribution, we discuss how sampling the hidden weights and biases of the ansatz network from data-agnostic and data-dependent probability distributions allows us to progress on both challenges. In most examples, the random sampling schemes outperform iterative, gradient-based optimization of physics-informed neural networks regarding training time and accuracy by several orders of magnitude. For time-dependent PDE, we construct neural basis functions only in the spatial domain and then solve the associated ordinary differential equation with classical methods from scientific computing over a long time horizon. This alleviates one of the greatest challenges for neural PDE solvers because it does not require us to parameterize the solution in time. For second-order elliptic PDE in Barron spaces, we prove the existence of sampled networks with $L^2$ convergence to the solution. We demonstrate our approach on several time-dependent and static PDEs. We also illustrate how sampled networks can effectively solve inverse problems in this setting. Benefits compared to common numerical schemes include spectral convergence and mesh-free construction of basis functions.","sentences":["Approximation of solutions to partial differential equations (PDE) is an important problem in computational science and engineering.","Using neural networks as an ansatz for the solution has proven a challenge in terms of training time and approximation accuracy.","In this contribution, we discuss how sampling the hidden weights and biases of the ansatz network from data-agnostic and data-dependent probability distributions allows us to progress on both challenges.","In most examples, the random sampling schemes outperform iterative, gradient-based optimization of physics-informed neural networks regarding training time and accuracy by several orders of magnitude.","For time-dependent PDE, we construct neural basis functions only in the spatial domain and then solve the associated ordinary differential equation with classical methods from scientific computing over a long time horizon.","This alleviates one of the greatest challenges for neural PDE solvers because it does not require us to parameterize the solution in time.","For second-order elliptic PDE in Barron spaces, we prove the existence of sampled networks with $L^2$ convergence to the solution.","We demonstrate our approach on several time-dependent and static PDEs.","We also illustrate how sampled networks can effectively solve inverse problems in this setting.","Benefits compared to common numerical schemes include spectral convergence and mesh-free construction of basis functions."],"url":"http://arxiv.org/abs/2405.20836v1","category":"math.NA"}
{"created":"2024-05-31 14:06:24","title":"On the Cahn-Hilliard equation with kinetic rate dependent dynamic boundary condition and non-smooth potential: separation property and long-time behavior","abstract":"We consider a class of Cahn-Hilliard equation that characterizes phase separation phenomena of binary mixtures in a bounded domain $\\Omega \\subset \\mathbb{R}^d$ $(d\\in \\{2,3\\})$ with non-permeable boundary. The equations in the bulk are subject to kinetic rate dependent dynamic boundary conditions with possible boundary diffusion acting on the boundary chemical potential. For the initial boundary value problem with singular potentials, we prove that any global weak solution exhibits a propagation of regularity in time. In the two dimensional case, we establish the instantaneous strict separation property by a suitable De Giorgi's iteration scheme, which yields that the weak solution stays uniformly away from the pure phases $\\pm 1$ from any positive time on. In particular, when the bulk and boundary chemical potentials are in equilibrium, we obtain the instantaneous separation property with or without possible boundary diffusion acting on the boundary chemical potential. Next, in the three dimensional case, we show the eventual strict separation property that holds after a sufficiently large time. These separation properties are obtained in an unified way with respect to the structural parameters. Moreover, they allow us to achieve higher-order regularity of the global weak solution and prove the convergence to a single equilibrium as $t \\rightarrow \\infty$.","sentences":["We consider a class of Cahn-Hilliard equation that characterizes phase separation phenomena of binary mixtures in a bounded domain $\\Omega \\subset \\mathbb{R}^d$ $(d\\in \\{2,3\\})$ with non-permeable boundary.","The equations in the bulk are subject to kinetic rate dependent dynamic boundary conditions with possible boundary diffusion acting on the boundary chemical potential.","For the initial boundary value problem with singular potentials, we prove that any global weak solution exhibits a propagation of regularity in time.","In the two dimensional case, we establish the instantaneous strict separation property by a suitable De Giorgi's iteration scheme, which yields that the weak solution stays uniformly away from the pure phases $\\pm 1$ from any positive time on.","In particular, when the bulk and boundary chemical potentials are in equilibrium, we obtain the instantaneous separation property with or without possible boundary diffusion acting on the boundary chemical potential.","Next, in the three dimensional case, we show the eventual strict separation property that holds after a sufficiently large time.","These separation properties are obtained in an unified way with respect to the structural parameters.","Moreover, they allow us to achieve higher-order regularity of the global weak solution and prove the convergence to a single equilibrium as $t \\rightarrow \\infty$."],"url":"http://arxiv.org/abs/2405.20807v1","category":"math.AP"}
{"created":"2024-05-31 12:15:07","title":"Fast Bayesian Basis Selection for Functional Data Representation with Correlated Errors","abstract":"Functional data analysis (FDA) finds widespread application across various fields, due to data being recorded continuously over a time interval or at several discrete points. Since the data is not observed at every point but rather across a dense grid, smoothing techniques are often employed to convert the observed data into functions. In this work, we propose a novel Bayesian approach for selecting basis functions for smoothing one or multiple curves simultaneously. Our method differentiates from other Bayesian approaches in two key ways: (i) by accounting for correlated errors and (ii) by developing a variational EM algorithm instead of a Gibbs sampler. Simulation studies demonstrate that our method effectively identifies the true underlying structure of the data across various scenarios and it is applicable to different types of functional data. Our variational EM algorithm not only recovers the basis coefficients and the correct set of basis functions but also estimates the existing within-curve correlation. When applied to the motorcycle dataset, our method demonstrates comparable, and in some cases superior, performance in terms of adjusted $R^2$ compared to other techniques such as regression splines, Bayesian LASSO and LASSO. Additionally, when assuming independence among observations within a curve, our method, utilizing only a variational Bayes algorithm, is in the order of thousands faster than a Gibbs sampler on average. Our proposed method is implemented in R and codes are available at https://github.com/acarolcruz/VB-Bases-Selection.","sentences":["Functional data analysis (FDA) finds widespread application across various fields, due to data being recorded continuously over a time interval or at several discrete points.","Since the data is not observed at every point but rather across a dense grid, smoothing techniques are often employed to convert the observed data into functions.","In this work, we propose a novel Bayesian approach for selecting basis functions for smoothing one or multiple curves simultaneously.","Our method differentiates from other Bayesian approaches in two key ways: (i) by accounting for correlated errors and (ii) by developing a variational EM algorithm instead of a Gibbs sampler.","Simulation studies demonstrate that our method effectively identifies the true underlying structure of the data across various scenarios and it is applicable to different types of functional data.","Our variational EM algorithm not only recovers the basis coefficients and the correct set of basis functions but also estimates the existing within-curve correlation.","When applied to the motorcycle dataset, our method demonstrates comparable, and in some cases superior, performance in terms of adjusted $R^2$ compared to other techniques such as regression splines, Bayesian LASSO and LASSO.","Additionally, when assuming independence among observations within a curve, our method, utilizing only a variational Bayes algorithm, is in the order of thousands faster than a Gibbs sampler on average.","Our proposed method is implemented in R and codes are available at https://github.com/acarolcruz/VB-Bases-Selection."],"url":"http://arxiv.org/abs/2405.20758v1","category":"stat.ME"}
{"created":"2024-05-31 10:12:27","title":"Upscaled equations for the Fokker-Planck diffusion through arrays of permeable and of impermeable inclusions","abstract":"We study the Fokker-Planck diffusion equation with diffusion coefficient depending periodically on the space variable. Inside a periodic array of inclusions the diffusion coefficient is reduced by a factor called the diffusion magnitude. We find the upscaled equations obtained by taking both the degeneration and the homogenization limits in which the diffusion magnitude and the scale of the periodicity tends, respectively, to zero. Different behaviors, classified as pure diffusion, diffusion with mass deposition, and absence of diffusion, are found depending on the order in which the two limits are taken and on the ratio between the size of the inclusions and the scale of the periodicity.","sentences":["We study the Fokker-Planck diffusion equation with diffusion coefficient depending periodically on the space variable.","Inside a periodic array of inclusions the diffusion coefficient is reduced by a factor called the diffusion magnitude.","We find the upscaled equations obtained by taking both the degeneration and the homogenization limits in which the diffusion magnitude and the scale of the periodicity tends, respectively, to zero.","Different behaviors, classified as pure diffusion, diffusion with mass deposition, and absence of diffusion, are found depending on the order in which the two limits are taken and on the ratio between the size of the inclusions and the scale of the periodicity."],"url":"http://arxiv.org/abs/2405.20741v1","category":"math.AP"}
{"created":"2024-05-31 08:21:47","title":"Beyond probability-impact matrices in project risk management: A quantitative methodology for risk prioritisation","abstract":"The project managers who deal with risk management are often faced with the difficult task of determining the relative importance of the various sources of risk that affect the project. This prioritisation is crucial to direct management efforts to ensure higher project profitability. Risk matrices are widely recognised tools by academics and practitioners in various sectors to assess and rank risks according to their likelihood of occurrence and impact on project objectives. However, the existing literature highlights several limitations to use the risk matrix. In response to the weaknesses of its use, this paper proposes a novel approach for prioritising project risks. Monte Carlo Simulation (MCS) is used to perform a quantitative prioritisation of risks with the simulation software MCSimulRisk. Together with the definition of project activities, the simulation includes the identified risks by modelling their probability and impact on cost and duration. With this novel methodology, a quantitative assessment of the impact of each risk is provided, as measured by the effect that it would have on project duration and its total cost. This allows the differentiation of critical risks according to their impact on project duration, which may differ if cost is taken as a priority objective. This proposal is interesting for project managers because they will, on the one hand, know the absolute impact of each risk on their project duration and cost objectives and, on the other hand, be able to discriminate the impacts of each risk independently on the duration objective and the cost objective.","sentences":["The project managers who deal with risk management are often faced with the difficult task of determining the relative importance of the various sources of risk that affect the project.","This prioritisation is crucial to direct management efforts to ensure higher project profitability.","Risk matrices are widely recognised tools by academics and practitioners in various sectors to assess and rank risks according to their likelihood of occurrence and impact on project objectives.","However, the existing literature highlights several limitations to use the risk matrix.","In response to the weaknesses of its use, this paper proposes a novel approach for prioritising project risks.","Monte Carlo Simulation (MCS) is used to perform a quantitative prioritisation of risks with the simulation software MCSimulRisk.","Together with the definition of project activities, the simulation includes the identified risks by modelling their probability and impact on cost and duration.","With this novel methodology, a quantitative assessment of the impact of each risk is provided, as measured by the effect that it would have on project duration and its total cost.","This allows the differentiation of critical risks according to their impact on project duration, which may differ if cost is taken as a priority objective.","This proposal is interesting for project managers because they will, on the one hand, know the absolute impact of each risk on their project duration and cost objectives and, on the other hand, be able to discriminate the impacts of each risk independently on the duration objective and the cost objective."],"url":"http://arxiv.org/abs/2405.20679v1","category":"q-fin.RM"}
{"created":"2024-05-31 17:29:51","title":"Introducing sgboost: A Practical Guide and Implementation of sparse-group boosting in R","abstract":"This paper introduces the sgboost package in R, which implements sparse-group boosting for modeling high-dimensional data with natural groupings in covariates. Sparse-group boosting offers a flexible approach for both group and individual variable selection, reducing overfitting and enhancing model interpretability. The package uses regularization techniques based on the degrees of freedom of individual and group base-learners, and is designed to be used in conjunction with the mboost package. Through comparisons with existing methods and demonstration of its unique functionalities, this paper provides a practical guide on utilizing sparse-group boosting in R, accompanied by code examples to facilitate its application in various research domains. Overall, this paper serves as a valuable resource for researchers and practitioners seeking to use sparse-group boosting for efficient and interpretable high-dimensional data analysis.","sentences":["This paper introduces the sgboost package in R, which implements sparse-group boosting for modeling high-dimensional data with natural groupings in covariates.","Sparse-group boosting offers a flexible approach for both group and individual variable selection, reducing overfitting and enhancing model interpretability.","The package uses regularization techniques based on the degrees of freedom of individual and group base-learners, and is designed to be used in conjunction with the mboost package.","Through comparisons with existing methods and demonstration of its unique functionalities, this paper provides a practical guide on utilizing sparse-group boosting in R, accompanied by code examples to facilitate its application in various research domains.","Overall, this paper serves as a valuable resource for researchers and practitioners seeking to use sparse-group boosting for efficient and interpretable high-dimensional data analysis."],"url":"http://arxiv.org/abs/2405.21037v1","category":"stat.AP"}
{"created":"2024-05-31 16:31:07","title":"Bayesian Design Principles for Offline-to-Online Reinforcement Learning","abstract":"Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies.   Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.","sentences":["Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe.","However, offline learned policies are often suboptimal, and further online fine-tuning is required.","In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop.","We show that Bayesian design principles are crucial in solving such a dilemma.","Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies.   ","Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy.","Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach.","Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data."],"url":"http://arxiv.org/abs/2405.20984v1","category":"cs.LG"}
{"created":"2024-05-31 16:21:05","title":"LCQ: Low-Rank Codebook based Quantization for Large Language Models","abstract":"Large language models~(LLMs) have recently demonstrated promising performance in many tasks. However, the high storage and computational cost of LLMs has become a challenge for deploying LLMs. Weight quantization has been widely used for model compression, which can reduce both storage and computational cost. Most existing weight quantization methods for LLMs use a rank-one codebook for quantization, which results in substantial accuracy loss when the compression ratio is high. In this paper, we propose a novel weight quantization method, called low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a low-rank codebook, the rank of which can be larger than one, for quantization. Experiments show that LCQ can achieve better accuracy than existing methods with a negligibly extra storage cost.","sentences":["Large language models~(LLMs) have recently demonstrated promising performance in many tasks.","However, the high storage and computational cost of LLMs has become a challenge for deploying LLMs.","Weight quantization has been widely used for model compression, which can reduce both storage and computational cost.","Most existing weight quantization methods for LLMs use a rank-one codebook for quantization, which results in substantial accuracy loss when the compression ratio is high.","In this paper, we propose a novel weight quantization method, called low-rank codebook based quantization~(LCQ), for LLMs.","LCQ adopts a low-rank codebook, the rank of which can be larger than one, for quantization.","Experiments show that LCQ can achieve better accuracy than existing methods with a negligibly extra storage cost."],"url":"http://arxiv.org/abs/2405.20973v1","category":"cs.LG"}
{"created":"2024-05-31 15:44:33","title":"Fast characterization of multiplexed single-electron pumps with machine learning","abstract":"We present an efficient machine learning based automated framework for the fast tuning of single-electron pump devices into current quantization regimes. It uses a sparse measurement approach based on an iterative active learning algorithm to take targeted measurements in the gate voltage parameter space. When compared to conventional parameter scans, our automated framework allows us to decrease the number of measurement points by about an order of magnitude. This corresponds to an eight-fold decrease in the time required to determine quantization errors, which are estimated via an exponential extrapolation of the first current plateau embedded into the algorithm. We show the robustness of the framework by characterizing 28 individual devices arranged in a GaAs/AlGaAs multiplexer array, which we use to identify a subset of devices suitable for parallel operation at communal gate voltages. The method opens up the possibility to efficiently scale the characterization of such multiplexed devices to a large number of pumps.","sentences":["We present an efficient machine learning based automated framework for the fast tuning of single-electron pump devices into current quantization regimes.","It uses a sparse measurement approach based on an iterative active learning algorithm to take targeted measurements in the gate voltage parameter space.","When compared to conventional parameter scans, our automated framework allows us to decrease the number of measurement points by about an order of magnitude.","This corresponds to an eight-fold decrease in the time required to determine quantization errors, which are estimated via an exponential extrapolation of the first current plateau embedded into the algorithm.","We show the robustness of the framework by characterizing 28 individual devices arranged in a GaAs/AlGaAs multiplexer array, which we use to identify a subset of devices suitable for parallel operation at communal gate voltages.","The method opens up the possibility to efficiently scale the characterization of such multiplexed devices to a large number of pumps."],"url":"http://arxiv.org/abs/2405.20946v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-31 15:32:43","title":"Concentration Bounds for Optimized Certainty Equivalent Risk Estimation","abstract":"We consider the problem of estimating the Optimized Certainty Equivalent (OCE) risk from independent and identically distributed (i.i.d.) samples. For the classic sample average approximation (SAA) of OCE, we derive mean-squared error as well as concentration bounds (assuming sub-Gaussianity). Further, we analyze an efficient stochastic approximation-based OCE estimator, and derive finite sample bounds for the same. To show the applicability of our bounds, we consider a risk-aware bandit problem, with OCE as the risk. For this problem, we derive bound on the probability of mis-identification. Finally, we conduct numerical experiments to validate the theoretical findings.","sentences":["We consider the problem of estimating the Optimized Certainty Equivalent (OCE) risk from independent and identically distributed (i.i.d.) samples.","For the classic sample average approximation (SAA) of OCE, we derive mean-squared error as well as concentration bounds (assuming sub-Gaussianity).","Further, we analyze an efficient stochastic approximation-based OCE estimator, and derive finite sample bounds for the same.","To show the applicability of our bounds, we consider a risk-aware bandit problem, with OCE as the risk.","For this problem, we derive bound on the probability of mis-identification.","Finally, we conduct numerical experiments to validate the theoretical findings."],"url":"http://arxiv.org/abs/2405.20933v1","category":"cs.LG"}
{"created":"2024-05-31 14:05:27","title":"Multilingual Text Style Transfer: Datasets & Models for Indian Languages","abstract":"Text style transfer (TST) involves altering the linguistic style of a text while preserving its core content. This paper focuses on sentiment transfer, a vital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian languages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous work on English-Bangla sentiment transfer (Mukherjee et al., 2023). We introduce dedicated datasets of 1,000 positive and 1,000 negative style-parallel sentences for each of these eight languages. We then evaluate the performance of various benchmark models categorized into parallel, non-parallel, cross-lingual, and shared learning approaches, including the Llama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the significance of parallel data in TST and demonstrate the effectiveness of the Masked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel techniques. Moreover, cross-lingual and joint multilingual learning methods show promise, offering insights into selecting optimal models tailored to the specific language and task requirements. To the best of our knowledge, this work represents the first comprehensive exploration of the TST task as sentiment transfer across a diverse set of languages.","sentences":["Text style transfer (TST) involves altering the linguistic style of a text while preserving its core content.","This paper focuses on sentiment transfer, a vital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian languages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous work on English-Bangla sentiment transfer (Mukherjee et al., 2023).","We introduce dedicated datasets of 1,000 positive and 1,000 negative style-parallel sentences for each of these eight languages.","We then evaluate the performance of various benchmark models categorized into parallel, non-parallel, cross-lingual, and shared learning approaches, including the Llama2 and GPT-3.5 large language models (LLMs).","Our experiments highlight the significance of parallel data in TST and demonstrate the effectiveness of the Masked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel techniques.","Moreover, cross-lingual and joint multilingual learning methods show promise, offering insights into selecting optimal models tailored to the specific language and task requirements.","To the best of our knowledge, this work represents the first comprehensive exploration of the TST task as sentiment transfer across a diverse set of languages."],"url":"http://arxiv.org/abs/2405.20805v1","category":"cs.CL"}
{"created":"2024-05-31 08:35:56","title":"Unleashing the Potential of Diffusion Models for Incomplete Data Imputation","abstract":"This paper introduces DiffPuter, an iterative method for missing data imputation that leverages the Expectation-Maximization (EM) algorithm and Diffusion Models. By treating missing data as hidden variables that can be updated during model training, we frame the missing data imputation task as an EM problem. During the M-step, DiffPuter employs a diffusion model to learn the joint distribution of both the observed and currently estimated missing data. In the E-step, DiffPuter re-estimates the missing data based on the conditional probability given the observed data, utilizing the diffusion model learned in the M-step. Starting with an initial imputation, DiffPuter alternates between the M-step and E-step until convergence. Through this iterative process, DiffPuter progressively refines the complete data distribution, yielding increasingly accurate estimations of the missing data. Our theoretical analysis demonstrates that the unconditional training and conditional sampling processes of the diffusion model align precisely with the objectives of the M-step and E-step, respectively. Empirical evaluations across 10 diverse datasets and comparisons with 16 different imputation methods highlight DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 8.10% in MAE and 5.64% in RMSE compared to the most competitive existing method.","sentences":["This paper introduces DiffPuter, an iterative method for missing data imputation that leverages the Expectation-Maximization (EM) algorithm and Diffusion Models.","By treating missing data as hidden variables that can be updated during model training, we frame the missing data imputation task as an EM problem.","During the M-step, DiffPuter employs a diffusion model to learn the joint distribution of both the observed and currently estimated missing data.","In the E-step, DiffPuter re-estimates the missing data based on the conditional probability given the observed data, utilizing the diffusion model learned in the M-step.","Starting with an initial imputation, DiffPuter alternates between the M-step and E-step until convergence.","Through this iterative process, DiffPuter progressively refines the complete data distribution, yielding increasingly accurate estimations of the missing data.","Our theoretical analysis demonstrates that the unconditional training and conditional sampling processes of the diffusion model align precisely with the objectives of the M-step and E-step, respectively.","Empirical evaluations across 10 diverse datasets and comparisons with 16 different imputation methods highlight DiffPuter's superior performance.","Notably, DiffPuter achieves an average improvement of 8.10% in MAE and 5.64% in RMSE compared to the most competitive existing method."],"url":"http://arxiv.org/abs/2405.20690v1","category":"cs.LG"}
{"created":"2024-05-31 17:48:50","title":"On the number of Regge trajectories for dual amplitudes","abstract":"Regge poles connect the analytic structure of scattering amplitudes, analytically continued in spin, to the high-energy limit in momentum space. Dual models are expected to have only Regge poles, and string theory suggests there should be an infinite number of them. In this study, we investigate the number of Regge trajectories these models may have. We prove, based solely on crossing symmetry and unitarity, that meromorphic amplitudes, with or without subtractions, cannot produce a reggeizing amplitude if they contain any finite number of Regge trajectories. We argue that this should exclude the existence of such amplitudes altogether. Additionally, we develop and apply a linear programming dual bootstrap method to exclude these amplitudes directly in momentum space.","sentences":["Regge poles connect the analytic structure of scattering amplitudes, analytically continued in spin, to the high-energy limit in momentum space.","Dual models are expected to have only Regge poles, and string theory suggests there should be an infinite number of them.","In this study, we investigate the number of Regge trajectories these models may have.","We prove, based solely on crossing symmetry and unitarity, that meromorphic amplitudes, with or without subtractions, cannot produce a reggeizing amplitude if they contain any finite number of Regge trajectories.","We argue that this should exclude the existence of such amplitudes altogether.","Additionally, we develop and apply a linear programming dual bootstrap method to exclude these amplitudes directly in momentum space."],"url":"http://arxiv.org/abs/2405.21057v1","category":"hep-th"}
{"created":"2024-05-31 15:56:56","title":"Optimized reinitialization based level-set method within industrial context","abstract":"This paper presents a solver using the Level-Set method for incompressible two phase flows with surface tension. A one fluid approach is adopted where both phases share the same velocity and pressure field. The Level Set method has been coupled with the Ghost Fluid Method. An efficient and pragmatic solution is proposed to avoid interface displacement during the reinitialization of the level-Set fields. A solver called LSFoam has been implemented in the OpenFOAM framework with consistent Rhie & Chow interpolation. This solver has been tested on several test cases covering different scales and flow configurations: rising bubble test case, Rayleigh-Taylor instability simulations, Ogee spillway flow, 3D dambreak simulation with a square cylinder obstacle and KVLCC2 steady resistance calculations. Overall results are in excellent agreement with reference data and the present approach is very promising for moderate free surface deformations.","sentences":["This paper presents a solver using the Level-Set method for incompressible two phase flows with surface tension.","A one fluid approach is adopted where both phases share the same velocity and pressure field.","The Level Set method has been coupled with the Ghost Fluid Method.","An efficient and pragmatic solution is proposed to avoid interface displacement during the reinitialization of the level-Set fields.","A solver called LSFoam has been implemented in the OpenFOAM framework with consistent Rhie & Chow interpolation.","This solver has been tested on several test cases covering different scales and flow configurations: rising bubble test case, Rayleigh-Taylor instability simulations, Ogee spillway flow, 3D dambreak simulation with a square cylinder obstacle and KVLCC2 steady resistance calculations.","Overall results are in excellent agreement with reference data and the present approach is very promising for moderate free surface deformations."],"url":"http://arxiv.org/abs/2405.20958v1","category":"physics.flu-dyn"}
{"created":"2024-05-31 14:44:23","title":"Maximum Bipartite Matching in $n^{2+o(1)}$ Time via a Combinatorial Algorithm","abstract":"Maximum bipartite matching (MBM) is a fundamental problem in combinatorial optimization with a long and rich history. A classic result of Hopcroft and Karp (1973) provides an $O(m \\sqrt{n})$-time algorithm for the problem, where $n$ and $m$ are the number of vertices and edges in the input graph, respectively. For dense graphs, an approach based on fast matrix multiplication achieves a running time of $O(n^{2.371})$. For several decades, these results represented state-of-the-art algorithms, until, in 2013, Madry introduced a powerful new approach for solving MBM using continuous optimization techniques. This line of research led to several spectacular results, culminating in a breakthrough $m^{1+o(1)}$-time algorithm for min-cost flow, that implies an $m^{1+o(1)}$-time algorithm for MBM as well.   These striking advances naturally raise the question of whether combinatorial algorithms can match the performance of the algorithms that are based on continuous techniques for MBM. A recent work of the authors (2024) made progress on this question by giving a combinatorial $\\tilde{O}(m^{1/3}n^{5/3})$-time algorithm for MBM, thus outperforming both the Hopcroft-Karp algorithm and matrix multiplication based approaches, on sufficiently dense graphs. Still, a large gap remains between the running time of their algorithm and the almost linear-time achievable by algorithms based on continuous techniques. In this work, we take another step towards narrowing this gap, and present a randomized $n^{2+o(1)}$-time combinatorial algorithm for MBM. Thus in dense graphs, our algorithm essentially matches the performance of algorithms that are based on continuous methods. We also obtain a randomized $n^{2+o(1)}$-time combinatorial algorithm for maximum vertex-capacitated $s$-$t$ flow in directed graphs when all vertex capacities are identical, using a standard reduction from this problem to MBM.","sentences":["Maximum bipartite matching (MBM) is a fundamental problem in combinatorial optimization with a long and rich history.","A classic result of Hopcroft and Karp (1973) provides an $O(m \\sqrt{n})$-time algorithm for the problem, where $n$ and $m$ are the number of vertices and edges in the input graph, respectively.","For dense graphs, an approach based on fast matrix multiplication achieves a running time of $O(n^{2.371})$. For several decades, these results represented state-of-the-art algorithms, until, in 2013, Madry introduced a powerful new approach for solving MBM using continuous optimization techniques.","This line of research led to several spectacular results, culminating in a breakthrough $m^{1+o(1)}$-time algorithm for min-cost flow, that implies an $m^{1+o(1)}$-time algorithm for MBM as well.   ","These striking advances naturally raise the question of whether combinatorial algorithms can match the performance of the algorithms that are based on continuous techniques for MBM.","A recent work of the authors (2024) made progress on this question by giving a combinatorial $\\tilde{O}(m^{1/3}n^{5/3})$-time algorithm for MBM, thus outperforming both the Hopcroft-Karp algorithm and matrix multiplication based approaches, on sufficiently dense graphs.","Still, a large gap remains between the running time of their algorithm and the almost linear-time achievable by algorithms based on continuous techniques.","In this work, we take another step towards narrowing this gap, and present a randomized $n^{2+o(1)}$-time combinatorial algorithm for MBM.","Thus in dense graphs, our algorithm essentially matches the performance of algorithms that are based on continuous methods.","We also obtain a randomized $n^{2+o(1)}$-time combinatorial algorithm for maximum vertex-capacitated $s$-$t$ flow in directed graphs when all vertex capacities are identical, using a standard reduction from this problem to MBM."],"url":"http://arxiv.org/abs/2405.20861v1","category":"cs.DS"}
{"created":"2024-05-31 14:42:36","title":"CSDO: Enhancing Efficiency and Success in Large-Scale Multi-Vehicle Trajectory Planning","abstract":"This paper presents an efficient algorithm, naming Centralized Searching and Decentralized Optimization (CSDO), to find feasible solution for large-scale Multi-Vehicle Trajectory Planning (MVTP) problem. Due to the intractable growth of non-convex constraints with the number of agents, exploring various homotopy classes that imply different convex domains, is crucial for finding a feasible solution. However, existing methods struggle to explore various homotopy classes efficiently due to combining it with time-consuming precise trajectory solution finding. CSDO, addresses this limitation by separating them into different levels and integrating an efficient Multi-Agent Path Finding (MAPF) algorithm to search homotopy classes. It first searches for a coarse initial guess using a large search step, identifying a specific homotopy class. Subsequent decentralized Quadratic Programming (QP) refinement processes this guess, resolving minor collisions efficiently. Experimental results demonstrate that CSDO outperforms existing MVTP algorithms in large-scale, high-density scenarios, achieving up to 95% success rate in 50m $\\times$ 50m random scenarios around one second. Source codes are released in https://github.com/YangSVM/CSDOTrajectoryPlanning.","sentences":["This paper presents an efficient algorithm, naming Centralized Searching and Decentralized Optimization (CSDO), to find feasible solution for large-scale Multi-Vehicle Trajectory Planning (MVTP) problem.","Due to the intractable growth of non-convex constraints with the number of agents, exploring various homotopy classes that imply different convex domains, is crucial for finding a feasible solution.","However, existing methods struggle to explore various homotopy classes efficiently due to combining it with time-consuming precise trajectory solution finding.","CSDO, addresses this limitation by separating them into different levels and integrating an efficient Multi-Agent Path Finding (MAPF) algorithm to search homotopy classes.","It first searches for a coarse initial guess using a large search step, identifying a specific homotopy class.","Subsequent decentralized Quadratic Programming (QP) refinement processes this guess, resolving minor collisions efficiently.","Experimental results demonstrate that CSDO outperforms existing MVTP algorithms in large-scale, high-density scenarios, achieving up to 95% success rate in 50m $\\times$ 50m random scenarios around one second.","Source codes are released in https://github.com/YangSVM/CSDOTrajectoryPlanning."],"url":"http://arxiv.org/abs/2405.20858v1","category":"cs.RO"}
{"created":"2024-05-31 14:32:32","title":"Locally Stationary Distributions: A Framework for Analyzing Slow-Mixing Markov Chains","abstract":"Many natural Markov chains fail to mix to their stationary distribution in polynomially many steps. Often, this slow mixing is inevitable since it is computationally intractable to sample from their stationary measure.   Nevertheless, Markov chains can be shown to always converge quickly to measures that are *locally stationary*, i.e., measures that don't change over a small number of steps. These locally stationary measures are analogous to local minima in continuous optimization, while stationary measures correspond to global minima.   While locally stationary measures can be statistically far from stationary measures, do they enjoy provable theoretical guarantees that have algorithmic implications? We study this question in this work and demonstrate three algorithmic applications of locally stationary measures:   1. We show that Glauber dynamics on the hardcore model can be used to find independent sets of size $\\Omega\\left(\\frac{\\log d}{d} \\cdot n\\right)$ in triangle-free graphs of degree at most $d$.   2. Let $W$ be a symmetric real matrix with bounded spectral diameter and $v$ be a unit vector. Given the matrix $M = \\lambda vv^\\top + W$ with a planted rank-one spike along vector $v$, for sufficiently large constant $\\lambda$, Glauber dynamics on the Ising model defined by $M$ samples vectors $x \\in \\{\\pm 1\\}^n$ that have constant correlation with the vector $v$.   3. Let $M = A_{\\mathbf{G}} - \\frac{d}{n}\\mathbf{1}\\mathbf{1}^\\top$ be a centered version of the adjacency matrix where the graph $\\mathbf{G}$ is drawn from a sparse 2-community stochastic block model.   We show that for sufficiently large constant $\\lambda$, Glauber dynamics on the Ising model defined by $M$ samples vectors $x \\in \\{\\pm 1\\}^n$ that have constant correlation with the hidden community vector $\\mathbf{\\sigma}$.","sentences":["Many natural Markov chains fail to mix to their stationary distribution in polynomially many steps.","Often, this slow mixing is inevitable since it is computationally intractable to sample from their stationary measure.   ","Nevertheless, Markov chains can be shown to always converge quickly to measures that are *locally stationary*, i.e., measures that don't change over a small number of steps.","These locally stationary measures are analogous to local minima in continuous optimization, while stationary measures correspond to global minima.   ","While locally stationary measures can be statistically far from stationary measures, do they enjoy provable theoretical guarantees that have algorithmic implications?","We study this question in this work and demonstrate three algorithmic applications of locally stationary measures:   1.","We show that Glauber dynamics on the hardcore model can be used to find independent sets of size $\\Omega\\left(\\frac{\\log d}{d} \\cdot n\\right)$ in triangle-free graphs of degree at most $d$.   2.","Let $W$ be a symmetric real matrix with bounded spectral diameter and $v$ be a unit vector.","Given the matrix $M = \\lambda vv^\\top + W$ with a planted rank-one spike along vector $v$, for sufficiently large constant $\\lambda$, Glauber dynamics on the Ising model defined by $M$ samples vectors $x \\in \\{\\pm 1\\}^n$ that have constant correlation with the vector $v$.   3.","Let $M = A_{\\mathbf{G}} - \\frac{d}{n}\\mathbf{1}\\mathbf{1}^\\top$ be a centered version of the adjacency matrix where the graph $\\mathbf{G}$ is drawn from a sparse 2-community stochastic block model.   ","We show that for sufficiently large constant $\\lambda$, Glauber dynamics on the Ising model defined by $M$ samples vectors $x \\in \\{\\pm 1\\}^n$ that have constant correlation with the hidden community vector $\\mathbf{\\sigma}$."],"url":"http://arxiv.org/abs/2405.20849v1","category":"cs.DS"}
{"created":"2024-05-31 14:23:30","title":"That's Optional: A Contemporary Exploration of \"that\" Omission in English Subordinate Clauses","abstract":"The Uniform Information Density (UID) hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. This paper investigates the impact of UID principles on syntactic reduction, specifically focusing on the optional omission of the connector \"that\" in English subordinate clauses. Building upon previous research, we extend our investigation to a larger corpus of written English, utilize contemporary large language models (LLMs) and extend the information-uniformity principles by the notion of entropy, to estimate the UID manifestations in the usecase of syntactic reduction choices.","sentences":["The Uniform Information Density (UID) hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time.","This paper investigates the impact of UID principles on syntactic reduction, specifically focusing on the optional omission of the connector \"that\" in English subordinate clauses.","Building upon previous research, we extend our investigation to a larger corpus of written English, utilize contemporary large language models (LLMs) and extend the information-uniformity principles by the notion of entropy, to estimate the UID manifestations in the usecase of syntactic reduction choices."],"url":"http://arxiv.org/abs/2405.20833v1","category":"cs.CL"}
{"created":"2024-05-31 17:56:29","title":"Brightening and Fading in the Youngest Galactic Supernova Remnant G1.9+0.3: 13 years of monitoring with the Chandra X-ray Observatory","abstract":"We report results from 13 years of Chandra monitoring of nonthermal X-ray emission from the youngest Galactic supernova remnant G1.9+0.3, the only remnant known to be increasing in brightness. We confirm the spatially-integrated flux increase rate of $(1.2 \\pm 0.2)$% yr$^{-1}$ between 1 and 7 keV, but find large spatial variations, from decreases of $-3$% yr$^{-1}$ to increases of 7% yr$^{-1}$, over length scales as small as $10''$ or smaller. We observe relatively little change in spectral slope, though one region shows significant hardening (photon index $\\Delta \\Gamma \\sim 0.4$) as it brightens by 1% yr$^{-1}$. Such rates of change can be accommodated by any of several explanations, including steady evolution of the blast wave, expansion or compression of discrete plasma blobs, strong magnetic turbulence, or variations in magnetic-field aspect angle. Our results do not constrain the mean magnetic-field strength, but a self-consistent picture of the spatially averaged rate of increase can be produced in which the maximum energies of accelerated particles are limited by the remnant age (applying both to electrons and to ions) to about 20 TeV, and the remnant-averaged magnetic field strength is about 30 $\\mu$G. The deceleration parameter $m$ (average shock radius varying as $t^m$) is about 0.7, consistent with estimates from overall expansion dynamics, and confirming an explosion date of about 1900 CE. Shock-efficiency factors $\\epsilon_e$ and $\\epsilon_B$ (fractions of shock energy in relativistic electrons and magnetic field) are 0.003 and 0.0002 in this picture. However, the large range of rates of brightness change indicates that such a global model is oversimplified. Temporal variations of photon index, expected to be small but measurable with longer time baselines, can discriminate among possible models.","sentences":["We report results from 13 years of Chandra monitoring of nonthermal X-ray emission from the youngest Galactic supernova remnant G1.9+0.3, the only remnant known to be increasing in brightness.","We confirm the spatially-integrated flux increase rate of $(1.2 \\pm 0.2)$% yr$^{-1}$","between 1 and 7 keV, but find large spatial variations, from decreases of $-3$% yr$^{-1}$ to increases of 7% yr$^{-1}$, over length scales as small as $10''$ or smaller.","We observe relatively little change in spectral slope, though one region shows significant hardening (photon index $\\Delta \\Gamma \\sim 0.4$) as it brightens by 1% yr$^{-1}$. Such rates of change can be accommodated by any of several explanations, including steady evolution of the blast wave, expansion or compression of discrete plasma blobs, strong magnetic turbulence, or variations in magnetic-field aspect angle.","Our results do not constrain the mean magnetic-field strength, but a self-consistent picture of the spatially averaged rate of increase can be produced in which the maximum energies of accelerated particles are limited by the remnant age (applying both to electrons and to ions) to about 20 TeV, and the remnant-averaged magnetic field strength is about 30 $\\mu$G. The deceleration parameter $m$ (average shock radius varying as $t^m$) is about 0.7, consistent with estimates from overall expansion dynamics, and confirming an explosion date of about 1900 CE.","Shock-efficiency factors $\\epsilon_e$ and $\\epsilon_B$ (fractions of shock energy in relativistic electrons and magnetic field) are 0.003 and 0.0002 in this picture.","However, the large range of rates of brightness change indicates that such a global model is oversimplified.","Temporal variations of photon index, expected to be small but measurable with longer time baselines, can discriminate among possible models."],"url":"http://arxiv.org/abs/2405.21067v1","category":"astro-ph.HE"}
{"created":"2024-05-31 17:42:48","title":"Boil-off of red supergiants: mass loss and type II-P supernovae","abstract":"The mass loss mechanism of red supergiant stars is not well understood, even though it has crucial consequences for their stellar evolution and the appearance of supernovae that occur upon core-collapse. We argue that outgoing shock waves launched near the photosphere can support a dense chromosphere between the star's surface and the dust formation radius at several stellar radii. We derive analytic expressions for the time-averaged density profile of the chromosphere, and we use these to estimate mass loss rates due to winds launched by radiation pressure at the dust formation radius. These mass loss rates are similar to recent observations, possibly explaining the upward kink in mass loss rates of luminous red supergiants. Our models predict that low-mass red supergiants lose less mass than commonly assumed, while high-mass red supergiants lose more. The chromospheric mass of our models is $\\sim$0.01 solar masses, most of which lies within a few stellar radii. This may explain the early light curves and spectra of type-II P supernovae without requiring extreme pre-supernova mass loss. We discuss implications for stellar evolution, type II-P supernovae, SN 2023ixf, and Betelgeuse.","sentences":["The mass loss mechanism of red supergiant stars is not well understood, even though it has crucial consequences for their stellar evolution and the appearance of supernovae that occur upon core-collapse.","We argue that outgoing shock waves launched near the photosphere can support a dense chromosphere between the star's surface and the dust formation radius at several stellar radii.","We derive analytic expressions for the time-averaged density profile of the chromosphere, and we use these to estimate mass loss rates due to winds launched by radiation pressure at the dust formation radius.","These mass loss rates are similar to recent observations, possibly explaining the upward kink in mass loss rates of luminous red supergiants.","Our models predict that low-mass red supergiants lose less mass than commonly assumed, while high-mass red supergiants lose more.","The chromospheric mass of our models is $\\sim$0.01 solar masses, most of which lies within a few stellar radii.","This may explain the early light curves and spectra of type-II P supernovae without requiring extreme pre-supernova mass loss.","We discuss implications for stellar evolution, type II-P supernovae, SN 2023ixf, and Betelgeuse."],"url":"http://arxiv.org/abs/2405.21049v1","category":"astro-ph.SR"}
{"created":"2024-05-31 17:04:18","title":"The rising costs of training frontier AI models","abstract":"The costs of training frontier AI models have grown dramatically in recent years, but there is limited public data on the magnitude and growth of these expenses. This paper develops a detailed cost model to address this gap, estimating training costs using three approaches that account for hardware, energy, cloud rental, and staff expenses. The analysis reveals that the amortized cost to train the most compute-intensive models has grown precipitously at a rate of 2.4x per year since 2016 (95% CI: 2.0x to 3.1x). For key frontier models, such as GPT-4 and Gemini, the most significant expenses are AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs include server components (15-22%), cluster-level interconnect (9-13%), and energy consumption (2-6%). If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","sentences":["The costs of training frontier AI models have grown dramatically in recent years, but there is limited public data on the magnitude and growth of these expenses.","This paper develops a detailed cost model to address this gap, estimating training costs using three approaches that account for hardware, energy, cloud rental, and staff expenses.","The analysis reveals that the amortized cost to train the most compute-intensive models has grown precipitously at a rate of 2.4x per year since 2016 (95% CI: 2.0x to 3.1x).","For key frontier models, such as GPT-4 and Gemini, the most significant expenses are AI accelerator chips and staff costs, each costing tens of millions of dollars.","Other notable costs include server components (15-22%), cluster-level interconnect (9-13%), and energy consumption (2-6%).","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models."],"url":"http://arxiv.org/abs/2405.21015v1","category":"cs.CY"}
{"created":"2024-05-31 16:45:26","title":"Influx ratio preserving coupling conditions for the networked Lighthill-Whitham-Richards model","abstract":"A new coupling rule for the Lighthill-Whitham-Richards model at merging junctions is introduced that imposes the preservation of the ratio between inflow from a given road to the total inflow into the junction. This rule is considered both in the context of the original traffic flow model and a relaxation setting giving rise to two different Riemann solvers that are discussed for merging 2-to-1 junctions. Numerical experiments are shown suggesting that the relaxation based Riemann solver is capable of suitable predictions of both, free-flow and congestion scenarios without relying on flow maximization.","sentences":["A new coupling rule for the Lighthill-Whitham-Richards model at merging junctions is introduced that imposes the preservation of the ratio between inflow from a given road to the total inflow into the junction.","This rule is considered both in the context of the original traffic flow model and a relaxation setting giving rise to two different Riemann solvers that are discussed for merging 2-to-1 junctions.","Numerical experiments are shown suggesting that the relaxation based Riemann solver is capable of suitable predictions of both, free-flow and congestion scenarios without relying on flow maximization."],"url":"http://arxiv.org/abs/2405.21005v1","category":"math.NA"}
{"created":"2024-05-31 16:44:01","title":"Progress in patterned wax stamp for prototyping of paper-based microfluidic analytical devices via injection molding","abstract":"In this study, we successfully developed two-dimensional paper-based analytical devices using a hybrid technique of injection molding and embossing. This innovative approach involves passive or active delivery of molten wax onto a glass substrate through a sealed chip, facilitating wax stamp creation.","sentences":["In this study, we successfully developed two-dimensional paper-based analytical devices using a hybrid technique of injection molding and embossing.","This innovative approach involves passive or active delivery of molten wax onto a glass substrate through a sealed chip, facilitating wax stamp creation."],"url":"http://arxiv.org/abs/2405.21001v1","category":"physics.ins-det"}
{"created":"2024-05-31 16:34:50","title":"Unravelling the asphericities in the explosion and multi-faceted circumstellar matter of SN 2023ixf","abstract":"We present a detailed investigation of photometric, spectroscopic, and polarimetric observations of the Type II SN 2023ixf. The early detection of highly-ionized flash features, rapid ascent in ultraviolet flux coupled with the blueward shift in near-ultraviolet colors and temperature provides compelling evidence for a delayed shock breakout from a confined dense circumstellar matter (CSM) enveloping the progenitor star. The temporal evolution of polarization in the SN 2023ixf phase revealed three distinct peaks in polarization evolution at 1.4 d, 6.4 d, and 79.2 d, indicating an asymmetric dense CSM, an aspherical shock front and clumpiness in the low-density extended CSM, and an aspherical inner ejecta/He-core. SN 2023ixf displayed two dominant axes, one along the CSM-outer ejecta and the other along the inner ejecta/He-core, showcasing the independent origin of asymmetry in the early and late evolution. The argument for an aspherical shock front is further strengthened by the presence of a high-velocity broad absorption feature in the blue wing of the Balmer features in addition to the P-Cygni absorption post 16 d. Hydrodynamical light curve modeling indicated a progenitor mass of 10 solar mass with a radius of 470 solar radius, explosion energy of 2e51 erg, and 0.06 solar mass of 56Ni. The modeling also indicated a two-zone CSM: a confined dense CSM extending up to 5e14 cm, with a mass-loss rate of 1e-2 solar mass per year, and an extended CSM spanning from 5e14 cm to 1e16 cm with a mass-loss rate of 1e-4 solar mass per year. The early nebular phase observations display an axisymmetric line profile of [OI] and red-ward attenuation of the emission of Halpha post 125 days, marking the onset of dust formation.","sentences":["We present a detailed investigation of photometric, spectroscopic, and polarimetric observations of the Type II SN 2023ixf.","The early detection of highly-ionized flash features, rapid ascent in ultraviolet flux coupled with the blueward shift in near-ultraviolet colors and temperature provides compelling evidence for a delayed shock breakout from a confined dense circumstellar matter (CSM) enveloping the progenitor star.","The temporal evolution of polarization in the SN 2023ixf phase revealed three distinct peaks in polarization evolution at 1.4 d, 6.4 d, and 79.2 d, indicating an asymmetric dense CSM, an aspherical shock front and clumpiness in the low-density extended CSM, and an aspherical inner ejecta/He-core.","SN 2023ixf displayed two dominant axes, one along the CSM-outer ejecta and the other along the inner ejecta/He-core, showcasing the independent origin of asymmetry in the early and late evolution.","The argument for an aspherical shock front is further strengthened by the presence of a high-velocity broad absorption feature in the blue wing of the Balmer features in addition to the P-Cygni absorption post 16 d. Hydrodynamical light curve modeling indicated a progenitor mass of 10 solar mass with a radius of 470 solar radius, explosion energy of 2e51 erg, and 0.06 solar mass of 56Ni.","The modeling also indicated a two-zone CSM: a confined dense CSM extending up to 5e14 cm, with a mass-loss rate of 1e-2 solar mass per year, and an extended CSM spanning from 5e14 cm to 1e16 cm with a mass-loss rate of 1e-4 solar mass per year.","The early nebular phase observations display an axisymmetric line profile of [OI] and red-ward attenuation of the emission of Halpha post 125 days, marking the onset of dust formation."],"url":"http://arxiv.org/abs/2405.20989v1","category":"astro-ph.HE"}
{"created":"2024-05-31 16:14:06","title":"Superlatives in Context: Explicit and Implicit Domain Restrictions for Superlative Frames","abstract":"Superlatives are used to single out elements with a maximal/minimal property. Semantically, superlatives perform a set comparison: something (or some things) has the min/max property out of a set. As such, superlatives provide an ideal phenomenon for studying implicit phenomena and discourse restrictions. While this comparison set is often not explicitly defined, its (implicit) restrictions can be inferred from the discourse context the expression appears in. In this work we provide an extensive computational study on the semantics of superlatives. We propose a unified account of superlative semantics which allows us to derive a broad-coverage annotation schema. Using this unified schema we annotated a multi-domain dataset of superlatives and their semantic interpretations. We specifically focus on interpreting implicit or ambiguous superlative expressions, by analyzing how the discourse context restricts the set of interpretations. In a set of experiments we then analyze how well models perform at variations of predicting superlative semantics, with and without context. We show that the fine-grained semantics of superlatives in context can be challenging for contemporary models, including GPT-4.","sentences":["Superlatives are used to single out elements with a maximal/minimal property.","Semantically, superlatives perform a set comparison: something (or some things) has the min/max property out of a set.","As such, superlatives provide an ideal phenomenon for studying implicit phenomena and discourse restrictions.","While this comparison set is often not explicitly defined, its (implicit) restrictions can be inferred from the discourse context the expression appears in.","In this work we provide an extensive computational study on the semantics of superlatives.","We propose a unified account of superlative semantics which allows us to derive a broad-coverage annotation schema.","Using this unified schema we annotated a multi-domain dataset of superlatives and their semantic interpretations.","We specifically focus on interpreting implicit or ambiguous superlative expressions, by analyzing how the discourse context restricts the set of interpretations.","In a set of experiments we then analyze how well models perform at variations of predicting superlative semantics, with and without context.","We show that the fine-grained semantics of superlatives in context can be challenging for contemporary models, including GPT-4."],"url":"http://arxiv.org/abs/2405.20967v1","category":"cs.CL"}
{"created":"2024-05-31 15:37:55","title":"Twenty-five years of greedy bases","abstract":"Although the basic idea behind the concept of a greedy basis had been around for some time, the formal development of a theory of greedy bases was initiated in 1999 with the publication of the article [S.~V.~Konyagin and V.~N.~Temlyakov, A remark on greedy approximation in Banach spaces, East J. Approx. 5 (1999), no. 3, 365--379]. The theoretical simplicity of the thresholding greedy algorithm became a model for a procedure widely used in numerical applications and the subject of greedy bases evolved very rapidly from the point of view of approximation theory. The idea of studying greedy bases and related greedy algorithms attracted also the attention of researchers with a classical Banach space theory background. From the more abstract point of functional analysis, the theory of greedy bases and its derivates evolved very fast as many fundamental results were discovered and new ramifications branched out. Hundreds of papers on greedy-like bases and several monographs have been written since the foundational paper mentioned above appeared. After twenty-five years, the theory is very much alive and it continues to be a very active research topic both for functional analysts and for researchers interested in the applied nature of nonlinear approximation alike. This is why we believe it is a good moment to gather a selection of 25 open problems (one per year since 1999!) whose solution would contribute to advance the state of art of this beautiful topic.","sentences":["Although the basic idea behind the concept of a greedy basis had been around for some time, the formal development of a theory of greedy bases was initiated in 1999 with the publication of the article [S.~V.~Konyagin and V.~N.~Temlyakov, A remark on greedy approximation in Banach spaces, East J. Approx.","5 (1999), no. 3, 365--379].","The theoretical simplicity of the thresholding greedy algorithm became a model for a procedure widely used in numerical applications and the subject of greedy bases evolved very rapidly from the point of view of approximation theory.","The idea of studying greedy bases and related greedy algorithms attracted also the attention of researchers with a classical Banach space theory background.","From the more abstract point of functional analysis, the theory of greedy bases and its derivates evolved very fast as many fundamental results were discovered and new ramifications branched out.","Hundreds of papers on greedy-like bases and several monographs have been written since the foundational paper mentioned above appeared.","After twenty-five years, the theory is very much alive and it continues to be a very active research topic both for functional analysts and for researchers interested in the applied nature of nonlinear approximation alike.","This is why we believe it is a good moment to gather a selection of 25 open problems (one per year since 1999!)","whose solution would contribute to advance the state of art of this beautiful topic."],"url":"http://arxiv.org/abs/2405.20939v1","category":"math.FA"}
{"created":"2024-05-31 15:27:49","title":"Search for a resonance decaying into a scalar particle and a Higgs boson in final states with leptons and two photons in proton-proton collisions at $\\sqrt s=13$ TeV with the ATLAS detector","abstract":"A search for a hypothetical heavy scalar particle, $X$, decaying into a singlet scalar particle, $S$, and a Standard Model Higgs boson, $H$, using 140 fb$^{-1}$ of proton-proton collision data at the centre-of-mass energy of 13 TeV recorded with the ATLAS detector at the LHC is presented. The explored mass range is $300 \\leq m_X \\leq 1000$ GeV and $170 \\leq m_S \\leq 500$ GeV. The signature of this search is one or two leptons ($e$ or $\\mu$) from the decay of vector bosons originating from the $S$ particle, $S \\rightarrow W^{\\pm}W^{\\mp}/ZZ$, and two photons from the Higgs boson decay, $H\\rightarrow \\gamma\\gamma$. No significant excess is observed above the expected Standard Model background. The observed (expected) upper limits at the 95% confidence level on the cross-section for $gg\\rightarrow X\\rightarrow SH$, assuming the same $S\\rightarrow WW/ZZ$ branching ratios as for a SM-like heavy Higgs boson, are between 530 (800) fb and 120 (170) fb.","sentences":["A search for a hypothetical heavy scalar particle, $X$, decaying into a singlet scalar particle, $S$, and a Standard Model Higgs boson, $H$, using 140 fb$^{-1}$ of proton-proton collision data at the centre-of-mass energy of 13 TeV recorded with the ATLAS detector at the LHC is presented.","The explored mass range is $300 \\leq m_X \\leq 1000$ GeV and $170 \\leq m_S","\\leq 500$ GeV.","The signature of this search is one or two leptons ($e$ or $\\mu$) from the decay of vector bosons originating from the $S$ particle, $S \\rightarrow W^{\\pm}W^{\\mp}/ZZ$, and two photons from the Higgs boson decay, $H\\rightarrow \\gamma\\gamma$.","No significant excess is observed above the expected Standard Model background.","The observed (expected) upper limits at the 95% confidence level on the cross-section for $gg\\rightarrow X\\rightarrow SH$, assuming the same $S\\rightarrow WW/ZZ$ branching ratios as for a SM-like heavy Higgs boson, are between 530 (800) fb and 120 (170) fb."],"url":"http://arxiv.org/abs/2405.20926v1","category":"hep-ex"}
{"created":"2024-05-31 15:13:09","title":"Galaxy Rest-Frame UV Colors at z ~ 2-4 with HST UVCANDELS","abstract":"We present an analysis of rest-frame UV colors of 17,243 galaxies at $z\\sim2-4$ in the HST UVCANDELS fields: GOODS-N, GOODS-S, COSMOS, and EGS. Here, we study the rest-frame UV spectral slope, $\\beta$, measured via model spectra obtained via spectral energy distribution (SED) fitting, $\\beta_{SED}$, and explore its correlation with various galaxy parameters (photometric redshift, UV magnitude, stellar mass, dust attenuation, star formation rate [SFR], and specific SFR) obtained via SED fitting with Dense Basis. We also obtain measurements for $\\beta$ via photometric power-law fitting and compare them to our SED-fit-based results, finding good agreement on average. While we find little evolution in $\\beta$ with redshift from $z=2-4$ for the full population, there are clear correlations between $\\beta$ (and related parameters) when binned by stellar mass. For this sample, lower stellar mass galaxies (log[$M_*$] = 7.5-8.5 $M_\\odot$) are typically bluer ($\\beta_{SED}=-2.0\\pm 0.2$ / $\\beta_{PL} = -2.1\\pm0.4$), fainter ($MUV = -17.8^{+0.7}_{-0.6}$) less dusty ($A{v}=0.4\\pm0.1$ mag), exhibit lower rates of star formation (log[SFR]=$0.1\\pm0.2 M_\\odot/$ yr) and higher specific star formation rates (log[sSFR]=$-8.2\\pm0.2 \\ \\mathrm{yr}^{-1}$) than their high-mass counterparts. Higher-mass galaxies (log[$M_*$] $=10.0-12.0 \\ M_\\odot$) are on average redder ($\\beta_{SED}=-0.9^{+0.8}_{-0.5}$ / $\\beta_{PL}=-1.0^{+0.8}_{-0.5}$), brighter ($MUV=-19.6^{+1.0}_{-1.2}$), dustier ($Av = 0.9^{+0.5}_{-0.4}$ mag), have higher SFRs (log[SFR]=$1.2^{+0.6}_{-1.1} M_\\odot$ yr), and lower sSFRs (log[sSFR]=$-9.1^{+0.5}_{-1.1} {yr}^{-1}$). This study's substantial sample size provides a benchmark for demonstrating that the rest-frame UV spectral slope correlates with stellar mass-dependent galaxy characteristics at $z\\sim2-4$, a relationship less discernible with smaller datasets typically available at higher redshifts.","sentences":["We present an analysis of rest-frame UV colors of 17,243 galaxies at $z\\sim2-4$ in the HST UVCANDELS fields: GOODS-N, GOODS-S, COSMOS, and EGS.","Here, we study the rest-frame UV spectral slope, $\\beta$, measured via model spectra obtained via spectral energy distribution (SED) fitting, $\\beta_{SED}$, and explore its correlation with various galaxy parameters (photometric redshift, UV magnitude, stellar mass, dust attenuation, star formation rate [SFR], and specific SFR) obtained via SED fitting with Dense Basis.","We also obtain measurements for $\\beta$ via photometric power-law fitting and compare them to our SED-fit-based results, finding good agreement on average.","While we find little evolution in $\\beta$ with redshift from $z=2-4$ for the full population, there are clear correlations between $\\beta$ (and related parameters) when binned by stellar mass.","For this sample, lower stellar mass galaxies (log[$M_*$] = 7.5-8.5 $M_\\odot$) are typically bluer ($\\beta_{SED}=-2.0\\pm 0.2$ / $\\beta_{PL} = -2.1\\pm0.4$), fainter ($MUV = -17.8^{+0.7}_{-0.6}$) less dusty ($A{v}=0.4\\pm0.1$ mag), exhibit lower rates of star formation (log[SFR]=$0.1\\pm0.2 M_\\odot/$ yr) and higher specific star formation rates (log[sSFR]=$-8.2\\pm0.2 \\ \\mathrm{yr}^{-1}$) than their high-mass counterparts.","Higher-mass galaxies (log[$M_*$] $=10.0-12.0 \\ M_\\odot$) are on average redder ($\\beta_{SED}=-0.9^{+0.8}_{-0.5}$ / $\\beta_{PL}=-1.0^{+0.8}_{-0.5}$), brighter ($MUV=-19.6^{+1.0}_{-1.2}$), dustier ($Av = 0.9^{+0.5}_{-0.4}$ mag), have higher SFRs (log[SFR]=$1.2^{+0.6}_{-1.1} M_\\odot$ yr), and lower sSFRs (log[sSFR]=$-9.1^{+0.5}_{-1.1} {yr}^{-1}$).","This study's substantial sample size provides a benchmark for demonstrating that the rest-frame UV spectral slope correlates with stellar mass-dependent galaxy characteristics at $z\\sim2-4$, a relationship less discernible with smaller datasets typically available at higher redshifts."],"url":"http://arxiv.org/abs/2405.20901v1","category":"astro-ph.GA"}
{"created":"2024-05-31 15:02:05","title":"Constraining Gluonic Contact Interaction of a Neutrino-philic Dark Fermion at Hadron Colliders and Direct Detection Experiments","abstract":"Weakly interacting fermion with the Standard Model particles is a promising candidate of the genuine dark matter. In this paper, we study signatures of the gluonic interactions of a dark fermion and a neutrino at hadron colliders and direct detection experiments. The lowest order interactions are described by contact operators in dimension 7. At hadron colliders, the mono-jet production is the most sensitive channel. And these operators can also induce both spin-independent and spin dependent absorption of the dark fermion at nuclear target. We show that for a nearly massless dark fermion, the energy scales are constrained to be higher than 500 GeV and 1.2 TeV by the current LHC and HE-LHC searches, respectively. Furthermore, we also find that almost all the parameter space accessible by the spin-independent absorption has been excluded by the current LHC constraints. In contrast, for spin-dependent absorption at light nuclear target there is still some parameter space which can not be reached by current and upcoming LHC searches.","sentences":["Weakly interacting fermion with the Standard Model particles is a promising candidate of the genuine dark matter.","In this paper, we study signatures of the gluonic interactions of a dark fermion and a neutrino at hadron colliders and direct detection experiments.","The lowest order interactions are described by contact operators in dimension 7.","At hadron colliders, the mono-jet production is the most sensitive channel.","And these operators can also induce both spin-independent and spin dependent absorption of the dark fermion at nuclear target.","We show that for a nearly massless dark fermion, the energy scales are constrained to be higher than 500 GeV and 1.2 TeV by the current LHC and HE-LHC searches, respectively.","Furthermore, we also find that almost all the parameter space accessible by the spin-independent absorption has been excluded by the current LHC constraints.","In contrast, for spin-dependent absorption at light nuclear target there is still some parameter space which can not be reached by current and upcoming LHC searches."],"url":"http://arxiv.org/abs/2405.20890v1","category":"hep-ph"}
{"created":"2024-05-31 14:58:34","title":"One-dimensional magnetic conduction channels across zigzag graphene nanoribbon/hexagonal boron nitride heterojunctions","abstract":"We examine the electronic structure of recently fabricated in-plane heterojunctions of zigzag graphene nanoribbons embedded in hexagonal boron nitride. We focus on hitherto unexplored interface configurations in which both edges of the nanoribbon are bonded to the same chemical species, either boron or nitrogen atoms. Using ab initio and mean-field Hubbard model calculations, we reveal the emergence of one-dimensional magnetic conducting channels at these interfaces. These channels originate from the energy shift of the magnetic interface states that is induced by charge transfer between the nanoribbon and hexagonal boron nitride. We further address the response of these heterojunctions to external electric and magnetic fields, demonstrating the tunability of energy and spin splittings in the electronic structure. Our findings establish that zigzag graphene nanoribbon/hexagonal boron nitride heterojunctions are a suitable platform for exploring and engineering spin transport in the atomically thin limit, with potential applications in integrated spintronic devices","sentences":["We examine the electronic structure of recently fabricated in-plane heterojunctions of zigzag graphene nanoribbons embedded in hexagonal boron nitride.","We focus on hitherto unexplored interface configurations in which both edges of the nanoribbon are bonded to the same chemical species, either boron or nitrogen atoms.","Using ab initio and mean-field Hubbard model calculations, we reveal the emergence of one-dimensional magnetic conducting channels at these interfaces.","These channels originate from the energy shift of the magnetic interface states that is induced by charge transfer between the nanoribbon and hexagonal boron nitride.","We further address the response of these heterojunctions to external electric and magnetic fields, demonstrating the tunability of energy and spin splittings in the electronic structure.","Our findings establish that zigzag graphene nanoribbon/hexagonal boron nitride heterojunctions are a suitable platform for exploring and engineering spin transport in the atomically thin limit, with potential applications in integrated spintronic devices"],"url":"http://arxiv.org/abs/2405.20886v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-31 14:52:34","title":"Progress on nucleon transition matrix elements with a lattice QCD variational analysis","abstract":"Nucleon weak matrix elements can be extracted from nucleon correlation functions with lattice QCD simulations. The signal-to-noise ratio prohibits the analysis at large source-sink separations and as a consequence, excited state contamination affects the extraction of the nucleon matrix elements. Chiral perturbation theory (ChPT) suggests that the dominant contamination in some of these channels is due to $N\\pi$ states where the pion carries the same momentum of the current. In this talk, we report updates on the variational analysis with $qqq$-operators (nucleon-like) and $(qqq)(\\bar{q}q)$-operators (nucleon-pion-like) where we report for the first time some preliminary results of $\\langle N\\pi| \\mathcal{J}| N \\rangle $, modulo some kinematic and volume factors, and we compare the results against ChPT. This pilot study is performed on a CLS ensemble with $N_f=3$, $m_\\pi \\approx 420~\\mathrm{MeV}$, $a\\approx 0.1~\\mathrm{fm}$ and $T=2L\\approx 4.8~\\mathrm{fm}$.","sentences":["Nucleon weak matrix elements can be extracted from nucleon correlation functions with lattice QCD simulations.","The signal-to-noise ratio prohibits the analysis at large source-sink separations and as a consequence, excited state contamination affects the extraction of the nucleon matrix elements.","Chiral perturbation theory (ChPT) suggests that the dominant contamination in some of these channels is due to $N\\pi$ states where the pion carries the same momentum of the current.","In this talk, we report updates on the variational analysis with $qqq$-operators (nucleon-like) and $(qqq)(\\bar{q}q)$-operators (nucleon-pion-like) where we report for the first time some preliminary results of $\\langle N\\pi| \\mathcal{J}|","N \\rangle $, modulo some kinematic and volume factors, and we compare the results against ChPT.","This pilot study is performed on a CLS ensemble with $N_f=3$, $m_\\pi \\approx 420~\\mathrm{MeV}$, $a\\approx 0.1~\\mathrm{fm}$ and $T=2L\\approx 4.8~\\mathrm{fm}$."],"url":"http://arxiv.org/abs/2405.20875v1","category":"hep-lat"}
{"created":"2024-05-31 14:48:41","title":"A Nearest-neighbor Expansion of Lepton Flavor Mixing in Powers of the $\u03bc$-$\u03c4$ Permutation Symmetry Breaking Effect","abstract":"We point out that the observed pattern of lepton flavor mixing can be well described by a proper nearest-neighbor expansion of a constant $3\\times 3$ unitary matrix in powers of a small parameter characterizing the fine effect of $\\mu$-$\\tau$ permutation symmetry breaking. We take an example of this kind for illustration, and provide complete discussions on the usefulness in the study of leptonic CP violation and unitarity triangles in matter.","sentences":["We point out that the observed pattern of lepton flavor mixing can be well described by a proper nearest-neighbor expansion of a constant $3\\times 3$ unitary matrix in powers of a small parameter characterizing the fine effect of $\\mu$-$\\tau$ permutation symmetry breaking.","We take an example of this kind for illustration, and provide complete discussions on the usefulness in the study of leptonic CP violation and unitarity triangles in matter."],"url":"http://arxiv.org/abs/2405.20871v1","category":"hep-ph"}
{"created":"2024-05-31 14:37:38","title":"Modified Euler-Heisenberg effective action and Proper-Time Method in Lorentz-Violating Scalar QED","abstract":"Quantum photon effects in vacuum provide an interesting setting to test quantum electrodynamics, serving as a source for predictions about physics beyond the Standard Model. In this paper, we investigate these effects by calculating the one-loop Euler-Heisenberg-like effective action within a Lorentz-violating scalar quantum electrodynamics framework. In both CPT-even and CPT-odd scenarios, we obtain the exact result in all orders of the stress tensor $F_{\\mu\\nu}$ and evaluate explicitly the lower orders of this effective action. We identify the quantum effects coming from Lorentz violation in an explicitly gauge invariant way. Nonlinear Lorentz-violating contributions that may affect photon-photon scattering are explicitly evaluated.","sentences":["Quantum photon effects in vacuum provide an interesting setting to test quantum electrodynamics, serving as a source for predictions about physics beyond the Standard Model.","In this paper, we investigate these effects by calculating the one-loop Euler-Heisenberg-like effective action within a Lorentz-violating scalar quantum electrodynamics framework.","In both CPT-even and CPT-odd scenarios, we obtain the exact result in all orders of the stress tensor $F_{\\mu\\nu}$ and evaluate explicitly the lower orders of this effective action.","We identify the quantum effects coming from Lorentz violation in an explicitly gauge invariant way.","Nonlinear Lorentz-violating contributions that may affect photon-photon scattering are explicitly evaluated."],"url":"http://arxiv.org/abs/2405.20855v1","category":"hep-th"}
{"created":"2024-05-31 14:29:36","title":"Compositional Reversible Computation","abstract":"Reversible computing is motivated by both pragmatic and foundational considerations arising from a variety of disciplines. We take a particular path through the development of reversible computation, emphasizing compositional reversible computation. We start from a historical perspective, by reviewing those approaches that developed reversible extensions of lambda-calculi, Turing machines, and communicating process calculi. These approaches share a common challenge: computations made reversible in this way do not naturally compose locally.   We then turn our attention to computational models that eschew the detour via existing irreversible models. Building on an original analysis by Landauer, the insights of Bennett, Fredkin, and Toffoli introduced a fresh approach to reversible computing in which reversibility is elevated to the status of the main design principle. These initial models are expressed using low-level bit manipulations, however.   Abstracting from the low-level of the Bennett-Fredkin-Toffoli models and pursuing more intrinsic, typed, and algebraic models, naturally leads to rig categories as the canonical model for compositional reversible programming. The categorical model reveals connections to type isomorphisms, symmetries, permutations, groups, and univalent universes. This, in turn, paves the way for extensions to reversible programming based on monads and arrows. These extensions are shown to recover conventional irreversible programming, a variety of reversible computational effects, and more interestingly both pure (measurement-free) and measurement-based quantum programming.","sentences":["Reversible computing is motivated by both pragmatic and foundational considerations arising from a variety of disciplines.","We take a particular path through the development of reversible computation, emphasizing compositional reversible computation.","We start from a historical perspective, by reviewing those approaches that developed reversible extensions of lambda-calculi, Turing machines, and communicating process calculi.","These approaches share a common challenge: computations made reversible in this way do not naturally compose locally.   ","We then turn our attention to computational models that eschew the detour via existing irreversible models.","Building on an original analysis by Landauer, the insights of Bennett, Fredkin, and Toffoli introduced a fresh approach to reversible computing in which reversibility is elevated to the status of the main design principle.","These initial models are expressed using low-level bit manipulations, however.   ","Abstracting from the low-level of the Bennett-Fredkin-Toffoli models and pursuing more intrinsic, typed, and algebraic models, naturally leads to rig categories as the canonical model for compositional reversible programming.","The categorical model reveals connections to type isomorphisms, symmetries, permutations, groups, and univalent universes.","This, in turn, paves the way for extensions to reversible programming based on monads and arrows.","These extensions are shown to recover conventional irreversible programming, a variety of reversible computational effects, and more interestingly both pure (measurement-free) and measurement-based quantum programming."],"url":"http://arxiv.org/abs/2405.20842v1","category":"cs.LO"}
{"created":"2024-05-31 14:08:55","title":"Projective holonomic quantum computation","abstract":"Nonadiabatic holonomic quantum computing is a novel and promising framework for the implementation and efficient and robust execution of quantum gates based on purely geometric principles. However, the parallel transport condition that is central to nonadiabatic holonomic quantum computing has shortcomings. In this paper, we address some of these shortcomings and show that a projectivization of the standard gauge theory of nonadiabatic holonomic quantum computation eliminates them. In addition, we extend the isoholonomic inequality to projective gates and establish a minimum execution time-a quantum speed limit-for projective holonomic quantum gates.","sentences":["Nonadiabatic holonomic quantum computing is a novel and promising framework for the implementation and efficient and robust execution of quantum gates based on purely geometric principles.","However, the parallel transport condition that is central to nonadiabatic holonomic quantum computing has shortcomings.","In this paper, we address some of these shortcomings and show that a projectivization of the standard gauge theory of nonadiabatic holonomic quantum computation eliminates them.","In addition, we extend the isoholonomic inequality to projective gates and establish a minimum execution time-a quantum speed limit-for projective holonomic quantum gates."],"url":"http://arxiv.org/abs/2405.20812v1","category":"quant-ph"}
{"created":"2024-05-31 11:51:01","title":"Compressibility of dense nuclear matter in the $\u03c1$-meson variant of the Skyrme model","abstract":"We show that coupling the $\\textrm{SU}(2)$-valued Skyrme field to the $\\rho$-meson solves the long-standing issue of (in)compressibility in the solitonic Skyrme model. Even by including only one $\\rho\\pi$ interaction term, motivated by a holographic-like reduction of Yang-Mills action by Sutcliffe, reduces the compression modulus from $K_0 \\simeq 1080$ MeV, in the massive Skyrme model, to $K_0\\simeq 351$ MeV.","sentences":["We show that coupling the $\\textrm{SU}(2)$-valued Skyrme field to the $\\rho$-meson solves the long-standing issue of (in)compressibility in the solitonic Skyrme model.","Even by including only one $\\rho\\pi$ interaction term, motivated by a holographic-like reduction of Yang-Mills action by Sutcliffe, reduces the compression modulus from $K_0 \\simeq 1080$ MeV, in the massive Skyrme model, to $K_0\\simeq 351$ MeV."],"url":"http://arxiv.org/abs/2405.20757v1","category":"hep-th"}
{"created":"2024-05-31 11:24:47","title":"Experimental evidence of crystal-field, Zeeman splitting, and spin-phonon excitations in the quantum supersolid Na2BaCo(PO4)2","abstract":"Drawing inspiration from the recent breakthroughs in the \\ce{Na_{2}BaCo(PO_{4})_{2}} quantum magnet, renowned for its spin supersolidity phase and its potential for revolutionary cooling applications, our study delves into the intricate interplay among lattice, spin, and orbital degrees of freedom within this intriguing compound. Using meticulous temperature, field, and pressure-dependent Raman scattering techniques, we present compelling experimental evidence revealing pronounced crystal-electric field (CEF) excitations, alongside the interplay of CEF-phonon interactions. Notably, our experiments elucidate all electronic transitions from $j_{1 / 2}$ to $j_{3 / 2}$ and from $j_{1 / 2}$ to $j_{5 / 2}$, with energy level patterns closely aligned with theoretical predictions based on point-charge models. Furthermore, the application of a magnetic field and pressure reveals Zeeman splittings characterized by Land\\'e-g factors as well as the CEF-phonon resonances. The anomalous shift in coupled peak at low temperatures originates from the hybridization of CEF and phonon excitations due to their close energy proximity. These findings constitute a significant step towards unraveling the fundamental properties of this exotic quantum material for future research in fundamental physics or engineering application.","sentences":["Drawing inspiration from the recent breakthroughs in the \\ce{Na_{2}BaCo(PO_{4})_{2}} quantum magnet, renowned for its spin supersolidity phase and its potential for revolutionary cooling applications, our study delves into the intricate interplay among lattice, spin, and orbital degrees of freedom within this intriguing compound.","Using meticulous temperature, field, and pressure-dependent Raman scattering techniques, we present compelling experimental evidence revealing pronounced crystal-electric field (CEF) excitations, alongside the interplay of CEF-phonon interactions.","Notably, our experiments elucidate all electronic transitions from $j_{1 / 2}$ to $j_{3 / 2}$ and from $j_{1 / 2}$ to $j_{5 / 2}$, with energy level patterns closely aligned with theoretical predictions based on point-charge models.","Furthermore, the application of a magnetic field and pressure reveals Zeeman splittings characterized by Land\\'e-g factors as well as the CEF-phonon resonances.","The anomalous shift in coupled peak at low temperatures originates from the hybridization of CEF and phonon excitations due to their close energy proximity.","These findings constitute a significant step towards unraveling the fundamental properties of this exotic quantum material for future research in fundamental physics or engineering application."],"url":"http://arxiv.org/abs/2405.20752v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 10:07:26","title":"Elucidating the Role of Stacking Faults in TlGaSe$_{2}$ on its Thermoelectric Properties","abstract":"Thermoelectric materials are of great interest for heat energy harvesting applications. One such promising material is TlGaSe$_{2}$, a p-type semiconducting ternary chalcogenide. Recent reports show it can be processed as a thin film, opening the door for large-scale commercialization. However, TlGaSe$_{2}$ is prone to stacking faults along the [001] stacking direction and their role in its thermoelectric properties has not been understood to date. Herein, TlGaSe$_{2}$ is investigated via (scanning) transmission electron microscopy and first-principles calculations. Stacking faults are found to be present throughout the material, as density functional theory calculations reveal a lack of preferential stacking order. Electron transport calculations show an enhancement of thermoelectric power factors when stacking faults are present. This implies the presence of stacking faults is key to the material's excellent thermoelectric properties along the [001] stacking direction, which can be further enhanced by doping the material to hole carrier concentrations to approx. 10$^{19}$ cm$^{-3}$.","sentences":["Thermoelectric materials are of great interest for heat energy harvesting applications.","One such promising material is TlGaSe$_{2}$, a p-type semiconducting ternary chalcogenide.","Recent reports show it can be processed as a thin film, opening the door for large-scale commercialization.","However, TlGaSe$_{2}$ is prone to stacking faults along the [001] stacking direction and their role in its thermoelectric properties has not been understood to date.","Herein, TlGaSe$_{2}$ is investigated via (scanning) transmission electron microscopy and first-principles calculations.","Stacking faults are found to be present throughout the material, as density functional theory calculations reveal a lack of preferential stacking order.","Electron transport calculations show an enhancement of thermoelectric power factors when stacking faults are present.","This implies the presence of stacking faults is key to the material's excellent thermoelectric properties along the [001] stacking direction, which can be further enhanced by doping the material to hole carrier concentrations to approx.","10$^{19}$ cm$^{-3}$."],"url":"http://arxiv.org/abs/2405.20739v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-31 09:52:54","title":"The giant outburst of EXO 2030+375 I: Spectral and pulse profile evolution","abstract":"The Be X-ray binary EXO 2030+375 went through its third recorded giant outburst from June 2021 to early 2022. We present the results of both spectral and timing analysis based on NICER monitoring, covering the 2-10 keV flux range from 20 to 310 mCrab. Dense monitoring with observations carried out about every second day and a total exposure time of 160 ks allowed us to closely track the source evolution over the outburst. Changes in spectral shape and pulse profiles showed a stable luminosity dependence during the rise and decline. The same type of dependence has been seen in past outbursts. The pulse profile is characterized by several distinct peaks and dips. The profiles show a clear dependence on luminosity with a stark transition at a luminosity of 2x10^36 erg/s, indicating a change in the emission pattern. Using relativistic ray-tracing, we demonstrate how anisotropic beaming of emission from an accretion channel with constant geometrical configuration can give rise to the observed pulse profiles over a range of luminosities.","sentences":["The Be X-ray binary EXO 2030+375 went through its third recorded giant outburst from June 2021 to early 2022.","We present the results of both spectral and timing analysis based on NICER monitoring, covering the 2-10 keV flux range from 20 to 310 mCrab.","Dense monitoring with observations carried out about every second day and a total exposure time of 160 ks allowed us to closely track the source evolution over the outburst.","Changes in spectral shape and pulse profiles showed a stable luminosity dependence during the rise and decline.","The same type of dependence has been seen in past outbursts.","The pulse profile is characterized by several distinct peaks and dips.","The profiles show a clear dependence on luminosity with a stark transition at a luminosity of 2x10^36 erg/s, indicating a change in the emission pattern.","Using relativistic ray-tracing, we demonstrate how anisotropic beaming of emission from an accretion channel with constant geometrical configuration can give rise to the observed pulse profiles over a range of luminosities."],"url":"http://arxiv.org/abs/2405.20734v1","category":"astro-ph.HE"}
{"created":"2024-05-31 09:25:40","title":"Formal Verification of Ecosystem Restoration Requirements using UML and Alloy","abstract":"United Nations have declared the current decade (2021-2030) as the \"UN Decade on Ecosystem Restoration\" to join R\\&D forces to fight against the ongoing environmental crisis. Given the ongoing degradation of earth ecosystems and the related crucial services that they offer to the human society, ecosystem restoration has become a major society-critical issue. It is required to develop rigorously software applications managing ecosystem restoration. Reliable models of ecosystems and restoration goals are necessary. This paper proposes a rigorous approach for ecosystem requirements modeling using formal methods from a model-driven software engineering point of view. The authors describe the main concepts at stake with a metamodel in UML and introduce a formalization of this metamodel in Alloy. The formal model is executed with Alloy Analyzer, and safety and liveness properties are checked against it. This approach helps ensuring that ecosystem specifications are reliable and that the specified ecosystem meets the desired restoration goals, seen in our approach as liveness and safety properties. The concepts and activities of the approach are illustrated with CRESTO, a real-world running example of a restored Costa Rican ecosystem.","sentences":["United Nations have declared the current decade (2021-2030) as the \"UN Decade on Ecosystem Restoration\" to join R\\&D forces to fight against the ongoing environmental crisis.","Given the ongoing degradation of earth ecosystems and the related crucial services that they offer to the human society, ecosystem restoration has become a major society-critical issue.","It is required to develop rigorously software applications managing ecosystem restoration.","Reliable models of ecosystems and restoration goals are necessary.","This paper proposes a rigorous approach for ecosystem requirements modeling using formal methods from a model-driven software engineering point of view.","The authors describe the main concepts at stake with a metamodel in UML and introduce a formalization of this metamodel in Alloy.","The formal model is executed with Alloy Analyzer, and safety and liveness properties are checked against it.","This approach helps ensuring that ecosystem specifications are reliable and that the specified ecosystem meets the desired restoration goals, seen in our approach as liveness and safety properties.","The concepts and activities of the approach are illustrated with CRESTO, a real-world running example of a restored Costa Rican ecosystem."],"url":"http://arxiv.org/abs/2405.20722v1","category":"cs.SE"}
{"created":"2024-05-31 08:59:38","title":"IoT on the Road to Sustainability: Vehicle or Bandit?","abstract":"The Internet of Things (IoT) can support the evolution towards a digital and green future. However, the introduction of the technology clearly has in itself a direct adverse ecological impact. This paper assesses this impact at both the IoT-node and at the network side. For the nodes, we show that the electronics production of devices comes with a carbon footprint that can be much higher than during operation phase. We highlight that the inclusion of IoT support in existing cellular networks comes with a significant ecological penalty, raising overall energy consumption by more than 15%. These results call for novel design approaches for the nodes and for early consideration of the support for IoT in future networks. Raising the 'Vehicle or bandit?' question on the nature of IoT in the broader sense of sustainability, we illustrate the need for multidisciplinary cooperation to steer applications in desirable directions.","sentences":["The Internet of Things (IoT) can support the evolution towards a digital and green future.","However, the introduction of the technology clearly has in itself a direct adverse ecological impact.","This paper assesses this impact at both the IoT-node and at the network side.","For the nodes, we show that the electronics production of devices comes with a carbon footprint that can be much higher than during operation phase.","We highlight that the inclusion of IoT support in existing cellular networks comes with a significant ecological penalty, raising overall energy consumption by more than 15%.","These results call for novel design approaches for the nodes and for early consideration of the support for IoT in future networks.","Raising the 'Vehicle or bandit?'","question on the nature of IoT in the broader sense of sustainability, we illustrate the need for multidisciplinary cooperation to steer applications in desirable directions."],"url":"http://arxiv.org/abs/2405.20706v1","category":"eess.SP"}
