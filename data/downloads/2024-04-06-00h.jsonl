{"created":"2024-04-03 17:59:59","title":"Hitting the Thermal Target for Leptophilic Dark Matter","abstract":"We study future lepton collider prospects for testing predictive models of leptophilic dark matter candidates with a thermal origin. We calculate experimental milestones for testing the parameter space compatible with freeze-out and the associated collider signals at past, present, and future facilities. This analysis places new limits on such models by leveraging the utility of lepton colliders. At $e^+e^-$ machines, we make projections using precision $Z$-pole observables from $e^+e^-\\to l^+l^- + $ missing energy signatures at LEP and future projections for FCC-ee in these channels. Additionally, a muon collider could also probe new thermal relic parameter space in this scenario via $\\mu^+\\mu^- \\to X + $ missing energy where $X$ is any easy identifiable SM object. Collectively, these processes can probe much all of the parameter space for which DM direct annihilation to $l^+l^-$ yields the observed relic density in Higgs-like models with mass-proportional couplings to charged leptons.","sentences":["We study future lepton collider prospects for testing predictive models of leptophilic dark matter candidates with a thermal origin.","We calculate experimental milestones for testing the parameter space compatible with freeze-out and the associated collider signals at past, present, and future facilities.","This analysis places new limits on such models by leveraging the utility of lepton colliders.","At $e^+e^-$ machines, we make projections using precision $Z$-pole observables from $e^+e^-\\to l^+l^- + $ missing energy signatures at LEP and future projections for FCC-ee in these channels.","Additionally, a muon collider could also probe new thermal relic parameter space in this scenario via $\\mu^+\\mu^- \\to X + $ missing energy where $X$ is any easy identifiable SM object.","Collectively, these processes can probe much all of the parameter space for which DM direct annihilation to $l^+l^-$ yields the observed relic density in Higgs-like models with mass-proportional couplings to charged leptons."],"url":"http://arxiv.org/abs/2404.02906v1","category":"hep-ph"}
{"created":"2024-04-03 17:59:53","title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","abstract":"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.","sentences":["We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\".","This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation.","On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed.","It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability.","Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence.","VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing.","These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization.","We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning."],"url":"http://arxiv.org/abs/2404.02905v1","category":"cs.CV"}
{"created":"2024-04-03 17:59:36","title":"ALOHa: A New Measure for Hallucination in Captioning Models","abstract":"Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories. Our code is available at https://davidmchan.github.io/aloha/.","sentences":["Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene.","The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms.","In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations.","Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score.","We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.","Our code is available at https://davidmchan.github.io/aloha/."],"url":"http://arxiv.org/abs/2404.02904v1","category":"cs.CV"}
{"created":"2024-04-03 17:58:21","title":"DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets","abstract":"Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes. Further, to mitigate overfitting, we propose distilling from a flat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks. With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes. The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture. We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.","sentences":["Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks.","In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks.","However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ).","Due to this, ViT requires a large amount of data for pre-training.","Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively.","However, limited literature discusses the use of ViT for datasets with long-tailed imbalances.","In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets.","In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes.","This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes.","Further, to mitigate overfitting, we propose distilling from a flat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks.","With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes.","The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture.","We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018."],"url":"http://arxiv.org/abs/2404.02900v1","category":"cs.CV"}
{"created":"2024-04-03 17:51:18","title":"ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline","abstract":"Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM. Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}.","sentences":["Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving.","While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.","In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment.","We first train a general Math-Critique model from the LLM itself to provide feedback signals.","Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection.","Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval.","Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger.","Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM.","Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}."],"url":"http://arxiv.org/abs/2404.02893v1","category":"cs.CL"}
{"created":"2024-04-03 17:44:02","title":"Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining","abstract":"Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation. Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users. They are also vulnerable to advanced Expanded Residual Block ambiguity attacks. We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports. An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature. To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model. By jointly training the verification and deployment branches, their weights become tightly coupled. The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user. Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks.","sentences":["Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation.","Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users.","They are also vulnerable to advanced Expanded Residual Block ambiguity attacks.","We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports.","An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature.","To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model.","By jointly training the verification and deployment branches, their weights become tightly coupled.","The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user.","Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks."],"url":"http://arxiv.org/abs/2404.02889v1","category":"cs.CR"}
{"created":"2024-04-03 17:34:28","title":"On the Scalability of Diffusion-based Text-to-Image Generation","abstract":"Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers. We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL's UNet. On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size. Increasing caption density and diversity improves text-image alignment performance and the learning efficiency. Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size.","sentences":["Scaling up model and data size has been quite successful for the evolution of LLMs.","However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored.","It is also unclear how to efficiently scale the model for better performance at reduced cost.","The different training settings and expensive training cost make a fair model comparison extremely difficult.","In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images.","For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs.","And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers.","We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL's UNet.","On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size.","Increasing caption density and diversity improves text-image alignment performance and the learning efficiency.","Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size."],"url":"http://arxiv.org/abs/2404.02883v1","category":"cs.CV"}
{"created":"2024-04-03 17:24:27","title":"FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery","abstract":"Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores. The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy. The benchmark toolkit and codes, available via https://github.com/toelt-llc/FlightScope_Bench, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications.","sentences":["Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring.","While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos.","This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery.","Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch.","This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions.","This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores.","The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy.","The benchmark toolkit and codes, available via https://github.com/toelt-llc/FlightScope_Bench, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications."],"url":"http://arxiv.org/abs/2404.02877v1","category":"cs.CV"}
{"created":"2024-04-03 17:09:00","title":"Integrating Explanations in Learning LTL Specifications from Demonstrations","abstract":"This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations. Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations. LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains. On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges. We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications. We have implemented a tool called Janaka based on our approach. Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies.","sentences":["This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations.","Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations.","LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains.","On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges.","We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications.","We have implemented a tool called Janaka based on our approach.","Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies."],"url":"http://arxiv.org/abs/2404.02872v1","category":"cs.AI"}
{"created":"2024-04-03 17:05:41","title":"Human Activity Recognition using Smartphones","abstract":"Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc. In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time. We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer. These readings were preprocessed using a median filter. 42 features were extracted using various methods. We then tested various machine learning algorithms along with dimensionality reduction. Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time. This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metabolic Equivalent.","sentences":["Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc.","In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time.","We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer.","These readings were preprocessed using a median filter.","42 features were extracted using various methods.","We then tested various machine learning algorithms along with dimensionality reduction.","Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time.","This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metabolic Equivalent."],"url":"http://arxiv.org/abs/2404.02869v1","category":"cs.LG"}
{"created":"2024-04-03 16:19:47","title":"AI-augmented Automation for Real Driving Prediction: an Industrial Use Case","abstract":"The risen complexity of automotive systems requires new development strategies and methods to master the upcoming challenges. Traditional methods need thus to be changed by an increased level of automation, and a faster continuous improvement cycle. In this context, current vehicle performance tests represent a very time-consuming and expensive task due to the need to perform the tests in real driving conditions. As a consequence, agile/iterative processes like DevOps are largely hindered by the necessity of triggering frequent tests. This paper reports on a practical experience of developing an AI-augmented solution based on Machine Learning and Model-based Engineering to support continuous vehicle development and testing. In particular, historical data collected in real driving conditions is leveraged to synthesize a high-fidelity driving simulator and hence enable performance tests in virtual environments. Based on this practical experience, this paper also proposes a conceptual framework to support predictions based on real driving behavior.","sentences":["The risen complexity of automotive systems requires new development strategies and methods to master the upcoming challenges.","Traditional methods need thus to be changed by an increased level of automation, and a faster continuous improvement cycle.","In this context, current vehicle performance tests represent a very time-consuming and expensive task due to the need to perform the tests in real driving conditions.","As a consequence, agile/iterative processes like DevOps are largely hindered by the necessity of triggering frequent tests.","This paper reports on a practical experience of developing an AI-augmented solution based on Machine Learning and Model-based Engineering to support continuous vehicle development and testing.","In particular, historical data collected in real driving conditions is leveraged to synthesize a high-fidelity driving simulator and hence enable performance tests in virtual environments.","Based on this practical experience, this paper also proposes a conceptual framework to support predictions based on real driving behavior."],"url":"http://arxiv.org/abs/2404.02841v1","category":"cs.SE"}
{"created":"2024-04-03 16:17:53","title":"I-Design: Personalized LLM Interior Designer","abstract":"Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.","sentences":["Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality.","However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury.","To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication.","I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships.","Subsequently, an effective placement algorithm determines optimal locations for each object within the scene.","The final design is then constructed in 3D by retrieving and integrating assets from an existing object database.","Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline.","Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity."],"url":"http://arxiv.org/abs/2404.02838v1","category":"cs.AI"}
{"created":"2024-04-03 16:08:01","title":"Empowering Biomedical Discovery with AI Agents","abstract":"We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.","sentences":["We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms.","Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks.","AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows.","These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories.","AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies."],"url":"http://arxiv.org/abs/2404.02831v1","category":"cs.AI"}
{"created":"2024-04-03 16:04:59","title":"Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes","abstract":"Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models. Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis. Moreover, such models either rely on post-hoc methods or additional annotations. In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way. Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics. We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method. Further, our model provides superior interpretability against the post-hoc method. Importantly, expert radiologists validated the visual interpretability of our results, showing clinical applicability.","sentences":["Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models.","Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis.","Moreover, such models either rely on post-hoc methods or additional annotations.","In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way.","Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics.","We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method.","Further, our model provides superior interpretability against the post-hoc method.","Importantly, expert radiologists validated the visual interpretability of our results, showing clinical applicability."],"url":"http://arxiv.org/abs/2404.02830v1","category":"cs.CV"}
{"created":"2024-04-03 15:59:00","title":"Control of high-dimensional collective dynamics by deep neural feedback laws and kinetic modelling","abstract":"Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large. Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces. To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation. Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles. Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics.","sentences":["Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large.","Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces.","To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation.","Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles.","Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics."],"url":"http://arxiv.org/abs/2404.02825v1","category":"math.OC"}
{"created":"2024-04-03 15:55:39","title":"Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models","abstract":"The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer.","sentences":["The ability of large language models (LLMs) to follow instructions is crucial to real-world applications.","Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks.","To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints.","Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality.","We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback.","Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints.","On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics.","All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer."],"url":"http://arxiv.org/abs/2404.02823v1","category":"cs.CL"}
{"created":"2024-04-03 15:55:27","title":"Identifying Climate Targets in National Laws and Policies using Machine Learning","abstract":"Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problematic features. Finally, we investigate the characteristics of the dataset produced by running this classifier on the Climate Policy Radar (CPR) dataset of global national climate laws and policies and UNFCCC submissions, highlighting the potential of automated and scalable data collection for existing climate policy databases and supporting further research. Our work represents a significant upgrade in the accessibility of these key climate policy elements for policymakers and researchers. We publish our model at https://huggingface.co/ClimatePolicyRadar/national-climate-targets and related dataset at https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets.","sentences":["Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language.","Current methods for curating comprehensive views of global climate policy targets entail significant manual effort.","At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions.","In this paper we present an approach for extracting mentions of climate targets from national laws and policies.","We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text.","We investigate bias and equity impacts related to our model and identify specific years and country names as problematic features.","Finally, we investigate the characteristics of the dataset produced by running this classifier on the Climate Policy Radar (CPR) dataset of global national climate laws and policies and UNFCCC submissions, highlighting the potential of automated and scalable data collection for existing climate policy databases and supporting further research.","Our work represents a significant upgrade in the accessibility of these key climate policy elements for policymakers and researchers.","We publish our model at https://huggingface.co/ClimatePolicyRadar/national-climate-targets and related dataset at https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets."],"url":"http://arxiv.org/abs/2404.02822v2","category":"cs.CY"}
{"created":"2024-04-03 15:38:36","title":"A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches","abstract":"Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models. Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.","sentences":["Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks.","Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment.","Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems.","This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO.","A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches.","Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models.","Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges."],"url":"http://arxiv.org/abs/2404.02817v1","category":"cs.RO"}
{"created":"2024-04-03 15:37:07","title":"Anyonic quantum multipartite maskers in the Kitaev model","abstract":"The structure of quantum mechanics forbids a bipartite scenario for masking quantum information, however, it allows multipartite maskers. The Latin squares are found to be closely related to a series of tripartite maskers. This adds another item, significantly different from the original no-cloning theorem, to the no-go theorems. On the other hand, anyonic excitations in two dimensions exhibit exotic collective behaviors of quantum physics, and open the avenue of fault-tolerant topological quantum computing. Here, we give the Latin-square construction of Abelian and Ising anyons %of in the Kitaev model and study the maskable space configuration in anyonic space. The circling and braiding of Kitaev anyons are masking operations on extended hyperdisks in anyonic space. We also realize quantum information masking in a teleportation way in the Kitaev Ising anyon model.","sentences":["The structure of quantum mechanics forbids a bipartite scenario for masking quantum information, however, it allows multipartite maskers.","The Latin squares are found to be closely related to a series of tripartite maskers.","This adds another item, significantly different from the original no-cloning theorem, to the no-go theorems.","On the other hand, anyonic excitations in two dimensions exhibit exotic collective behaviors of quantum physics, and open the avenue of fault-tolerant topological quantum computing.","Here, we give the Latin-square construction of Abelian and Ising anyons %of in the Kitaev model and study the maskable space configuration in anyonic space.","The circling and braiding of Kitaev anyons are masking operations on extended hyperdisks in anyonic space.","We also realize quantum information masking in a teleportation way in the Kitaev Ising anyon model."],"url":"http://arxiv.org/abs/2404.02814v1","category":"quant-ph"}
{"created":"2024-04-03 15:23:17","title":"An Optimization Framework to Personalize Passive Cardiac Mechanics","abstract":"Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning. However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions. This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data. The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration. With a focus on characterizing the passive mechanical behavior, the framework employs structurally based anisotropic hyperelastic constitutive models and physiologically relevant boundary conditions to simulate myocardial mechanics. We use a stabilized variational multiscale formulation for solving the governing nonlinear elastodynamics equations, verified for cardiac mechanics applications. The framework is tested in myocardium models of biventricle and left atrium derived from cardiac phase-resolved computed tomographic (CT) images of a healthy subject and three patients with hypertrophic obstructive cardiomyopathy (HOCM). The impact of the choice of optimization methods and other numerical settings, including fiber direction parameters, mesh size, initial parameters for optimization, and perturbations to optimal material parameters, is assessed using a rigorous sensitivity analysis. The performance of the current iFEA is compared against an assumed power-law-based pressure-volume relation, typically used for single-phase image acquisition.","sentences":["Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning.","However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions.","This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data.","The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration.","With a focus on characterizing the passive mechanical behavior, the framework employs structurally based anisotropic hyperelastic constitutive models and physiologically relevant boundary conditions to simulate myocardial mechanics.","We use a stabilized variational multiscale formulation for solving the governing nonlinear elastodynamics equations, verified for cardiac mechanics applications.","The framework is tested in myocardium models of biventricle and left atrium derived from cardiac phase-resolved computed tomographic (CT) images of a healthy subject and three patients with hypertrophic obstructive cardiomyopathy (HOCM).","The impact of the choice of optimization methods and other numerical settings, including fiber direction parameters, mesh size, initial parameters for optimization, and perturbations to optimal material parameters, is assessed using a rigorous sensitivity analysis.","The performance of the current iFEA is compared against an assumed power-law-based pressure-volume relation, typically used for single-phase image acquisition."],"url":"http://arxiv.org/abs/2404.02807v2","category":"physics.med-ph"}
{"created":"2024-04-03 15:20:57","title":"The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers","abstract":"Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.","sentences":["Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests.","As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding.","In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates.","To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support.","We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance.","Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support.","In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals.","We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models."],"url":"http://arxiv.org/abs/2404.02806v1","category":"cs.SE"}
{"created":"2024-04-03 15:17:21","title":"On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension","abstract":"Question Generation aims to automatically generate questions based on a given input provided as context. A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control. In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts. We aim to control two attributes: the question's explicitness and underlying narrative elements. With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model. Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs. However, these improvements are not always statistically significant. The code is publicly available at github.com/bernardoleite/few-shot-prompting-qg-control.","sentences":["Question Generation aims to automatically generate questions based on a given input provided as context.","A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control.","In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts.","We aim to control two attributes: the question's explicitness and underlying narrative elements.","With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model.","Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs.","However, these improvements are not always statistically significant.","The code is publicly available at github.com/bernardoleite/few-shot-prompting-qg-control."],"url":"http://arxiv.org/abs/2404.02800v1","category":"cs.CL"}
{"created":"2024-04-03 15:07:00","title":"AI and personalized learning: bridging the gap with modern educational goals","abstract":"Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our analysis indicates a gap between the objectives of modern education and the current direction of PL. We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies. While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.","sentences":["Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education.","Technology-based PL solutions have shown notable effectiveness in enhancing learning performance.","However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas.","In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals.","Our analysis indicates a gap between the objectives of modern education and the current direction of PL.","We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies.","While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system.","Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning."],"url":"http://arxiv.org/abs/2404.02798v1","category":"cs.CY"}
{"created":"2024-04-03 14:55:58","title":"The Steinberg Tensor Product Theorem for General Linear Group Schemes in the Verlinde Category","abstract":"The Steinberg tensor product theorem is a fundamental result in the modular representation theory of reductive algebraic groups. It describes any finite-dimensional simple module of highest weight $\\lambda$ over such a group as the tensor product of Frobenius twists of simple modules with highest weights the weights appearing in a $p$-adic decomposition of $\\lambda$, thereby reducing the character problem to a a finite collection of weights. In recent years this theorem has been extended to various quasi-reductive supergroup schemes. In this paper, we prove the analogous result for the general linear group scheme $GL(X)$ for any object $X$ in the Verlinde category $\\mathrm{Ver}_p$.","sentences":["The Steinberg tensor product theorem is a fundamental result in the modular representation theory of reductive algebraic groups.","It describes any finite-dimensional simple module of highest weight $\\lambda$ over such a group as the tensor product of Frobenius twists of simple modules with highest weights the weights appearing in a $p$-adic decomposition of $\\lambda$, thereby reducing the character problem to a a finite collection of weights.","In recent years this theorem has been extended to various quasi-reductive supergroup schemes.","In this paper, we prove the analogous result for the general linear group scheme $GL(X)$ for any object $X$ in the Verlinde category $\\mathrm{Ver}_p$."],"url":"http://arxiv.org/abs/2404.02786v1","category":"math.RT"}
{"created":"2024-04-03 14:55:17","title":"Domain Generalization through Meta-Learning: A Survey","abstract":"Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions, paving the way for future innovation in meta-learning for domain generalization.","sentences":["Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications.","This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice.","Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains.","Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch.","This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization.","We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies.","Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field.","Our survey provides practical insights and an informed discussion on promising research directions, paving the way for future innovation in meta-learning for domain generalization."],"url":"http://arxiv.org/abs/2404.02785v1","category":"cs.LG"}
{"created":"2024-04-03 14:52:20","title":"CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech","abstract":"With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.","sentences":["With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis.","Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences.","To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams.","Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed.","In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances."],"url":"http://arxiv.org/abs/2404.02781v1","category":"eess.AS"}
{"created":"2024-04-03 14:47:48","title":"Chain event graphs for assessing activity-level propositions in forensic science in relation to drug traces on banknotes","abstract":"Graphical models and likelihood ratios can be used by forensic scientists to compare support given by evidence to propositions put forward by competing parties during court proceedings. Such models can also be used to evaluate support for activity-level propositions, i.e. propositions that refer to the nature of activities associated with evidence and how this evidence came to be at a crime scene. Graphical methods can be used to show explicitly different scenarios that might explain the evidence in a case and to distinguish between evidence requiring evaluation by a jury and quantifiable evidence from the crime scene. Such visual representations can be helpful for forensic practitioners, the police and lawyers who may need to assess the value that different pieces of evidence make to their arguments in a case. In this paper we demonstrate for the first time how chain event graphs can be applied to a criminal case involving drug trafficking. We show how different types of evidence (i.e. expert judgement and data collected from a crime scene) can be combined using a chain event graph and show how the hierarchical model deriving from the graph can be used to evaluate the degree of support for different activity-level propositions in the case. We also develop a modification of the standard chain event graph to simplify their use in forensic applications.","sentences":["Graphical models and likelihood ratios can be used by forensic scientists to compare support given by evidence to propositions put forward by competing parties during court proceedings.","Such models can also be used to evaluate support for activity-level propositions, i.e. propositions that refer to the nature of activities associated with evidence and how this evidence came to be at a crime scene.","Graphical methods can be used to show explicitly different scenarios that might explain the evidence in a case and to distinguish between evidence requiring evaluation by a jury and quantifiable evidence from the crime scene.","Such visual representations can be helpful for forensic practitioners, the police and lawyers who may need to assess the value that different pieces of evidence make to their arguments in a case.","In this paper we demonstrate for the first time how chain event graphs can be applied to a criminal case involving drug trafficking.","We show how different types of evidence (i.e. expert judgement and data collected from a crime scene) can be combined using a chain event graph and show how the hierarchical model deriving from the graph can be used to evaluate the degree of support for different activity-level propositions in the case.","We also develop a modification of the standard chain event graph to simplify their use in forensic applications."],"url":"http://arxiv.org/abs/2404.02778v1","category":"stat.AP"}
{"created":"2024-04-03 14:47:48","title":"Federated Computing -- Survey on Building Blocks, Extensions and Systems","abstract":"In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles. Federated Computing (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy. This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations. The motivation behind FC extends beyond technical considerations to encompass societal implications. As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty. FC comprises of Federated Learning (FL) and Federated Analytics (FA). FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces. Current surveys capture domain-specific FL use cases, describe individual components in an FC pipeline individually or decoupled from each other, or provide a quantitative overview of the number of published papers. This work surveys more than 150 papers to distill the underlying structure of FC systems with their basic building blocks, extensions, architecture, environment, and motivation. We capture FL and FA systems individually and point out unique difference between those two.","sentences":["In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles.","Federated Computing (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy.","This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations.","The motivation behind FC extends beyond technical considerations to encompass societal implications.","As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty.","FC comprises of Federated Learning (FL) and Federated Analytics (FA).","FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces.","Current surveys capture domain-specific FL use cases, describe individual components in an FC pipeline individually or decoupled from each other, or provide a quantitative overview of the number of published papers.","This work surveys more than 150 papers to distill the underlying structure of FC systems with their basic building blocks, extensions, architecture, environment, and motivation.","We capture FL and FA systems individually and point out unique difference between those two."],"url":"http://arxiv.org/abs/2404.02779v1","category":"cs.LG"}
{"created":"2024-04-03 14:05:39","title":"Unsupervised Occupancy Learning from Sparse Point Cloud","abstract":"Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while maximizing its entropy at the input point cloud. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve implicit shape inference with respect to baselines and the state-of-the-art using synthetic and real data.","sentences":["Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio.","Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry.","However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task.","In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs.","We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud.","We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while maximizing its entropy at the input point cloud.","Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve implicit shape inference with respect to baselines and the state-of-the-art using synthetic and real data."],"url":"http://arxiv.org/abs/2404.02759v1","category":"cs.CV"}
{"created":"2024-04-03 13:57:08","title":"DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement","abstract":"We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos. By leveraging the capabilities of diverse large language models (LLMs), we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence. Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training. Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components. By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets like YouCook2 and ActivityNet. We outperform the previous state-of-the-art Vid2Seq across a majority of metrics, achieving this with just 0.4% of the unlabeled video data used for pre-training by Vid2Seq.","sentences":["We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos.","By leveraging the capabilities of diverse large language models (LLMs), we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence.","Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training.","Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components.","By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets like YouCook2 and ActivityNet.","We outperform the previous state-of-the-art Vid2Seq across a majority of metrics, achieving this with just 0.4% of the unlabeled video data used for pre-training by Vid2Seq."],"url":"http://arxiv.org/abs/2404.02755v1","category":"cs.CV"}
{"created":"2024-04-03 13:56:33","title":"Continual Learning of Numerous Tasks from Long-tail Distributions","abstract":"Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance. We propose a method that reuses the optimizer states in Adam by maintaining a weighted average of the second moments from previous tasks. We demonstrate that our method, compatible with most existing continual learning algorithms, effectively reduces forgetting with only a small amount of additional computational or memory costs, and provides further improvements on existing continual learning algorithms, particularly in a long-tail task sequence.","sentences":["Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge.","Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios.","In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes.","We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting.","Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance.","We propose a method that reuses the optimizer states in Adam by maintaining a weighted average of the second moments from previous tasks.","We demonstrate that our method, compatible with most existing continual learning algorithms, effectively reduces forgetting with only a small amount of additional computational or memory costs, and provides further improvements on existing continual learning algorithms, particularly in a long-tail task sequence."],"url":"http://arxiv.org/abs/2404.02754v1","category":"cs.LG"}
{"created":"2024-04-03 13:41:27","title":"Statistical mechanics and pressure of composite multimoded weakly nonlinear optical systems","abstract":"Statistical mechanics can provide a versatile theoretical framework for investigating the collective dynamics of weakly nonlinear waves-settings that can be utterly complex to describe otherwise. In optics, composite systems arise due to interactions between different frequencies and/or polarizations. The purpose of this work is to develop a thermodynamic theory that takes into account the synergistic action of multiple components. We find that the type of the nonlinearity involved can have important implications in the thermalization process and, hence, can lead to different thermal equilibrium conditions. Importantly, we derive closed-form expressions for the actual optomechanical pressure that is exerted on the system. In particular, the total optomechanical pressure is the sum of the partial pressures due to each component. Our results can be applied to a variety of weakly nonlinear optical settings such as multimode fibers, bulk waveguides, photonic lattices, and coupled microresonators. We present two specific examples, where two colors interact in a waveguide array with either a cubic or quadratic nonlinearity.","sentences":["Statistical mechanics can provide a versatile theoretical framework for investigating the collective dynamics of weakly nonlinear waves-settings that can be utterly complex to describe otherwise.","In optics, composite systems arise due to interactions between different frequencies and/or polarizations.","The purpose of this work is to develop a thermodynamic theory that takes into account the synergistic action of multiple components.","We find that the type of the nonlinearity involved can have important implications in the thermalization process and, hence, can lead to different thermal equilibrium conditions.","Importantly, we derive closed-form expressions for the actual optomechanical pressure that is exerted on the system.","In particular, the total optomechanical pressure is the sum of the partial pressures due to each component.","Our results can be applied to a variety of weakly nonlinear optical settings such as multimode fibers, bulk waveguides, photonic lattices, and coupled microresonators.","We present two specific examples, where two colors interact in a waveguide array with either a cubic or quadratic nonlinearity."],"url":"http://arxiv.org/abs/2404.02745v1","category":"physics.optics"}
{"created":"2024-04-03 13:38:49","title":"Mixing Individual and Collective Behaviours to Predict Out-of-Routine Mobility","abstract":"Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics. While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours. Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy. Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods. Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility. During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models. By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges.","sentences":["Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics.","While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours.","Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy.","Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods.","Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility.","During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models.","By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges."],"url":"http://arxiv.org/abs/2404.02740v1","category":"cs.CY"}
{"created":"2024-04-03 13:30:56","title":"Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss","abstract":"Recent research has highlighted improvements in high-quality imaging guided by event cameras, with most of these efforts concentrating on the RGB domain. However, these advancements frequently neglect the unique challenges introduced by the inherent flaws in the sensor design of event cameras in the RAW domain. Specifically, this sensor design results in the partial loss of pixel values, posing new challenges for RAW domain processes like demosaicing. The challenge intensifies as most research in the RAW domain is based on the premise that each pixel contains a value, making the straightforward adaptation of these methods to event camera demosaicing problematic. To end this, we present a Swin-Transformer-based backbone and a pixel-focus loss function for demosaicing with missing pixel values in RAW domain processing. Our core motivation is to refine a general and widely applicable foundational model from the RGB domain for RAW domain processing, thereby broadening the model's applicability within the entire imaging process. Our method harnesses multi-scale processing and space-to-depth techniques to ensure efficiency and reduce computing complexity. We also proposed the Pixel-focus Loss function for network fine-tuning to improve network convergence based on our discovery of a long-tailed distribution in training loss. Our method has undergone validation on the MIPI Demosaic Challenge dataset, with subsequent analytical experimentation confirming its efficacy. All code and trained models are released here: https://github.com/yunfanLu/ev-demosaic","sentences":["Recent research has highlighted improvements in high-quality imaging guided by event cameras, with most of these efforts concentrating on the RGB domain.","However, these advancements frequently neglect the unique challenges introduced by the inherent flaws in the sensor design of event cameras in the RAW domain.","Specifically, this sensor design results in the partial loss of pixel values, posing new challenges for RAW domain processes like demosaicing.","The challenge intensifies as most research in the RAW domain is based on the premise that each pixel contains a value, making the straightforward adaptation of these methods to event camera demosaicing problematic.","To end this, we present a Swin-Transformer-based backbone and a pixel-focus loss function for demosaicing with missing pixel values in RAW domain processing.","Our core motivation is to refine a general and widely applicable foundational model from the RGB domain for RAW domain processing, thereby broadening the model's applicability within the entire imaging process.","Our method harnesses multi-scale processing and space-to-depth techniques to ensure efficiency and reduce computing complexity.","We also proposed the Pixel-focus Loss function for network fine-tuning to improve network convergence based on our discovery of a long-tailed distribution in training loss.","Our method has undergone validation on the MIPI Demosaic Challenge dataset, with subsequent analytical experimentation confirming its efficacy.","All code and trained models are released here: https://github.com/yunfanLu/ev-demosaic"],"url":"http://arxiv.org/abs/2404.02731v1","category":"eess.IV"}
{"created":"2024-04-03 13:29:12","title":"Learning Sequence Attractors in Recurrent Networks with Hidden Neurons","abstract":"The brain is targeted for processing temporal sequence information. It remains largely unclear how the brain learns to store and retrieve sequence memories. Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly. We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect. We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons. The algorithm is proven to converge and lead to sequence attractors. We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets. We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain.","sentences":["The brain is targeted for processing temporal sequence information.","It remains largely unclear how the brain learns to store and retrieve sequence memories.","Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly.","We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect.","We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons.","The algorithm is proven to converge and lead to sequence attractors.","We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets.","We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain."],"url":"http://arxiv.org/abs/2404.02729v1","category":"cs.NE"}
{"created":"2024-04-03 13:28:52","title":"Unsupervised Learning of Effective Actions in Robotics","abstract":"Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate \"action prototypes\", each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results show that our effect driven discretization outperforms uniformly and randomly sampled discretizations in convergence speed and maximum reward.","sentences":["Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics.","Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions.","Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data.","In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate \"action prototypes\", each producing different effects in the environment.","After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes.","We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results show that our effect driven discretization outperforms uniformly and randomly sampled discretizations in convergence speed and maximum reward."],"url":"http://arxiv.org/abs/2404.02728v1","category":"cs.RO"}
{"created":"2024-04-03 13:21:58","title":"Can We Understand Plasticity Through Neural Collapse?","abstract":"This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse. We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task. Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting.","sentences":["This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse.","We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task.","Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting."],"url":"http://arxiv.org/abs/2404.02719v1","category":"cs.LG"}
{"created":"2024-04-03 13:08:26","title":"ART: The Alternating Reading Task Corpus for Speech Entrainment and Imitation","abstract":"We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication. The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three sub-corpora encompassing French-, Italian-, and Slovak-accented English. This design allows systematic investigation of speech entrainment in a controlled and less-spontaneous setting. Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment. Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects.","sentences":["We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication.","The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three sub-corpora encompassing French-, Italian-, and Slovak-accented English.","This design allows systematic investigation of speech entrainment in a controlled and less-spontaneous setting.","Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment.","Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects."],"url":"http://arxiv.org/abs/2404.02710v1","category":"cs.CL"}
{"created":"2024-04-03 13:00:08","title":"PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders","abstract":"Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc. However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue. In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders. By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities. Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders. Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further improving the performance of PromptCodec. Experiments on LibriTTS demonstrate that our proposed PromptCodec consistently outperforms state-of-the-art neural speech codec models under all different bitrate conditions while achieving impressive performance with low bitrates.","sentences":["Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc.","However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue.","In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders.","By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities.","Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders.","Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further improving the performance of PromptCodec.","Experiments on LibriTTS demonstrate that our proposed PromptCodec consistently outperforms state-of-the-art neural speech codec models under all different bitrate conditions while achieving impressive performance with low bitrates."],"url":"http://arxiv.org/abs/2404.02702v1","category":"cs.SD"}
{"created":"2024-04-03 12:37:34","title":"Attention is Naturally Sparse with Gaussian Distributed Input","abstract":"The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of LLMs, paving the way for more scalable and efficient AI systems.","sentences":["The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures.","Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance.","This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs.","By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency.","Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness.","This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of LLMs, paving the way for more scalable and efficient AI systems."],"url":"http://arxiv.org/abs/2404.02690v1","category":"cs.LG"}
{"created":"2024-04-03 12:37:32","title":"Social clustering reinforces external influence on the majority opinion model","abstract":"Public opinion is subject to peer interaction via social networks and external pressures from the media, advertising, and other actors. In this paper, we study the interaction between external and peer influence on the stochastic opinion dynamics of a majority vote model. We introduce a model where agents update their opinions based on the combined influence from their local neighbourhood (peers) and from an external actor in the transition rates. In the first model, the external influence is only felt by agents non-aligned with the external actor ('push strategy'). In the second model, agents are affected by external influence, independently of their opinions ('nudging strategy'). In both cases, the external influence increases the possible macroscopic outcomes. These outcomes are determined by the chosen strategy. We also find that the social network structure affects the opinion dynamics, with high social clustering positively reinforcing the external influence while degree heterogeneity weakens it. These findings are relevant to businesses and policy making, helping to understand how groups of individuals collectively react to external actors.","sentences":["Public opinion is subject to peer interaction via social networks and external pressures from the media, advertising, and other actors.","In this paper, we study the interaction between external and peer influence on the stochastic opinion dynamics of a majority vote model.","We introduce a model where agents update their opinions based on the combined influence from their local neighbourhood (peers) and from an external actor in the transition rates.","In the first model, the external influence is only felt by agents non-aligned with the external actor ('push strategy').","In the second model, agents are affected by external influence, independently of their opinions ('nudging strategy').","In both cases, the external influence increases the possible macroscopic outcomes.","These outcomes are determined by the chosen strategy.","We also find that the social network structure affects the opinion dynamics, with high social clustering positively reinforcing the external influence while degree heterogeneity weakens it.","These findings are relevant to businesses and policy making, helping to understand how groups of individuals collectively react to external actors."],"url":"http://arxiv.org/abs/2404.02689v1","category":"physics.soc-ph"}
{"created":"2024-04-03 12:27:36","title":"Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers","abstract":"Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters. We experimented the efficacy of the method on varying sizes and alternative attention architectures and show that \\methodabbr significantly reduces the training time up to 2.5x times and converges to a better minimum with up to 2.6% stronger model on the LM benchmarks within the same compute budget.","sentences":["Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI).","A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers.","However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures.","In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework.","Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters.","We experimented the efficacy of the method on varying sizes and alternative attention architectures and show that \\methodabbr significantly reduces the training time up to 2.5x times and converges to a better minimum with up to 2.6% stronger model on the LM benchmarks within the same compute budget."],"url":"http://arxiv.org/abs/2404.02684v1","category":"cs.CL"}
{"created":"2024-04-03 12:24:48","title":"PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets","abstract":"Misogyny is often expressed through figurative language. Some neutral words can assume a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms. Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misogyny detection. Furthermore, we investigate LLMs' understanding of pejorative epithets by means of contextual word embeddings analysis and prompting.","sentences":["Misogyny is often expressed through figurative language.","Some neutral words can assume a negative connotation when functioning as pejorative epithets.","Disambiguating the meaning of such terms might help the detection of misogyny.","In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level.","We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection.","In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms.","Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misogyny detection.","Furthermore, we investigate LLMs' understanding of pejorative epithets by means of contextual word embeddings analysis and prompting."],"url":"http://arxiv.org/abs/2404.02681v1","category":"cs.CL"}
{"created":"2024-04-03 12:18:45","title":"Responsible Reporting for Frontier AI Development","abstract":"Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.","sentences":["Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems.","Organizations that develop and deploy frontier systems have significant access to such information.","By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems.","Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure.","We outline the key features of responsible reporting and propose mechanisms for implementing them in practice."],"url":"http://arxiv.org/abs/2404.02675v1","category":"cs.CY"}
{"created":"2024-04-03 11:40:17","title":"Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models","abstract":"Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.","sentences":["Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs).","Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs.","Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs.","However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs.","Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs.","Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL.","Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses."],"url":"http://arxiv.org/abs/2404.02657v1","category":"cs.CL"}
{"created":"2024-04-03 11:37:03","title":"Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging","abstract":"Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data. In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space. We introduce different types of non-negative matrix factorization (NMF) in few-shot learning, addressing the data scarcity issue in medical image classification. Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors. With 14 different datasets covering 11 distinct illness categories, thorough experimental results and comparison with related techniques demonstrate that NMF is a competitive alternative to PCA for few-shot learning in medical imaging, and the supervised NMF algorithms are more discriminative in the subspace with greater effectiveness. Furthermore, we show that the part-based representation of NMF, especially its supervised variants, is dramatically impactful in detecting lesion areas in medical imaging with limited samples.","sentences":["Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data.","In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space.","We introduce different types of non-negative matrix factorization (NMF) in few-shot learning, addressing the data scarcity issue in medical image classification.","Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors.","With 14 different datasets covering 11 distinct illness categories, thorough experimental results and comparison with related techniques demonstrate that NMF is a competitive alternative to PCA for few-shot learning in medical imaging, and the supervised NMF algorithms are more discriminative in the subspace with greater effectiveness.","Furthermore, we show that the part-based representation of NMF, especially its supervised variants, is dramatically impactful in detecting lesion areas in medical imaging with limited samples."],"url":"http://arxiv.org/abs/2404.02656v2","category":"cs.CV"}
{"created":"2024-04-03 11:25:20","title":"Towards detecting unanticipated bias in Large Language Models","abstract":"Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty of model decisions and to make the internal decision-making processes of LLMs more transparent, thereby identifying and understanding biases that are not immediately apparent. Through this research, we aim to contribute to the development of fairer and more transparent AI systems.","sentences":["Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems.","Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies.","This research largely targets well-known biases related to gender, race, ethnicity, and language.","However, it is clear that LLMs are also affected by other, less obvious implicit biases.","The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications.","In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods.","These approaches aim to assess the certainty of model decisions and to make the internal decision-making processes of LLMs more transparent, thereby identifying and understanding biases that are not immediately apparent.","Through this research, we aim to contribute to the development of fairer and more transparent AI systems."],"url":"http://arxiv.org/abs/2404.02650v1","category":"cs.LG"}
{"created":"2024-04-03 11:21:10","title":"A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems","abstract":"Recently, deep learning (DL) has been emerging as a promising approach for channel estimation and signal detection in wireless communications. The majority of the existing studies investigating the use of DL techniques in this domain focus on analysing channel impulse responses that are generated from only one channel distribution such as additive white Gaussian channel noise and Rayleigh channels. In practice, to cope with the dynamic nature of the wireless channel, DL methods must be re-trained on newly non-aged collected data which is costly, inefficient, and impractical. To tackle this challenge, this paper proposes a novel universal deep neural network (Uni-DNN) that can achieve high detection performance in various wireless environments without retraining the model. In particular, our proposed Uni-DNN model consists of a wireless channel classifier and a signal detector which are constructed by using DNNs. The wireless channel classifier enables the signal detector to generalise and perform optimally for multiple wireless channel distributions. In addition, to further improve the signal detection performance of the proposed model, convolutional neural network is employed. Extensive simulations using the orthogonal frequency division multiplexing scheme demonstrate that the bit error rate performance of our proposed solution can outperform conventional DL-based approaches as well as least square and minimum mean square error channel estimators in practical low pilot density scenarios.","sentences":["Recently, deep learning (DL) has been emerging as a promising approach for channel estimation and signal detection in wireless communications.","The majority of the existing studies investigating the use of DL techniques in this domain focus on analysing channel impulse responses that are generated from only one channel distribution such as additive white Gaussian channel noise and Rayleigh channels.","In practice, to cope with the dynamic nature of the wireless channel, DL methods must be re-trained on newly non-aged collected data which is costly, inefficient, and impractical.","To tackle this challenge, this paper proposes a novel universal deep neural network (Uni-DNN) that can achieve high detection performance in various wireless environments without retraining the model.","In particular, our proposed Uni-DNN model consists of a wireless channel classifier and a signal detector which are constructed by using DNNs.","The wireless channel classifier enables the signal detector to generalise and perform optimally for multiple wireless channel distributions.","In addition, to further improve the signal detection performance of the proposed model, convolutional neural network is employed.","Extensive simulations using the orthogonal frequency division multiplexing scheme demonstrate that the bit error rate performance of our proposed solution can outperform conventional DL-based approaches as well as least square and minimum mean square error channel estimators in practical low pilot density scenarios."],"url":"http://arxiv.org/abs/2404.02648v1","category":"cs.NI"}
{"created":"2024-04-03 11:14:37","title":"Leveraging Swarm Intelligence to Drive Autonomously: A Particle Swarm Optimization based Approach to Motion Planning","abstract":"Motion planning is an essential part of autonomous mobile platforms. A good pipeline should be modular enough to handle different vehicles, environments, and perception modules. The planning process has to cope with all the different modalities and has to have a modular and flexible design. But most importantly, it has to be safe and robust. In this paper, we want to present our motion planning pipeline with particle swarm optimization (PSO) at its core. This solution is independent of the vehicle type and has a clear and simple-to-implement interface for perception modules. Moreover, the approach stands out for being easily adaptable to new scenarios. Parallel calculation allows for fast planning cycles. Following the principles of PSO, the trajectory planer first generates a swarm of initial trajectories that are optimized afterward. We present the underlying control space and inner workings. Finally, the application to real-world automated driving is shown in the evaluation with a deeper look at the modeling of the cost function. The approach is used in our automated shuttles that have already driven more than 3.500 km safely and entirely autonomously in sub-urban everyday traffic.","sentences":["Motion planning is an essential part of autonomous mobile platforms.","A good pipeline should be modular enough to handle different vehicles, environments, and perception modules.","The planning process has to cope with all the different modalities and has to have a modular and flexible design.","But most importantly, it has to be safe and robust.","In this paper, we want to present our motion planning pipeline with particle swarm optimization (PSO) at its core.","This solution is independent of the vehicle type and has a clear and simple-to-implement interface for perception modules.","Moreover, the approach stands out for being easily adaptable to new scenarios.","Parallel calculation allows for fast planning cycles.","Following the principles of PSO, the trajectory planer first generates a swarm of initial trajectories that are optimized afterward.","We present the underlying control space and inner workings.","Finally, the application to real-world automated driving is shown in the evaluation with a deeper look at the modeling of the cost function.","The approach is used in our automated shuttles that have already driven more than 3.500 km safely and entirely autonomously in sub-urban everyday traffic."],"url":"http://arxiv.org/abs/2404.02644v1","category":"cs.RO"}
{"created":"2024-04-03 10:57:47","title":"SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation","abstract":"This paper aims at achieving fine-grained building attribute segmentation in a cross-view scenario, i.e., using satellite and street-view image pairs. The main challenge lies in overcoming the significant perspective differences between street views and satellite views. In this work, we introduce SG-BEV, a novel approach for satellite-guided BEV fusion for cross-view semantic segmentation. To overcome the limitations of existing cross-view projection methods in capturing the complete building facade features, we innovatively incorporate Bird's Eye View (BEV) method to establish a spatially explicit mapping of street-view features. Moreover, we fully leverage the advantages of multiple perspectives by introducing a novel satellite-guided reprojection module, optimizing the uneven feature distribution issues associated with traditional BEV methods. Our method demonstrates significant improvements on four cross-view datasets collected from multiple cities, including New York, San Francisco, and Boston. On average across these datasets, our method achieves an increase in mIOU by 10.13% and 5.21% compared with the state-of-the-art satellite-based and cross-view methods. The code and datasets of this work will be released at https://github.com/yejy53/SG-BEV.","sentences":["This paper aims at achieving fine-grained building attribute segmentation in a cross-view scenario, i.e., using satellite and street-view image pairs.","The main challenge lies in overcoming the significant perspective differences between street views and satellite views.","In this work, we introduce SG-BEV, a novel approach for satellite-guided BEV fusion for cross-view semantic segmentation.","To overcome the limitations of existing cross-view projection methods in capturing the complete building facade features, we innovatively incorporate Bird's Eye View (BEV) method to establish a spatially explicit mapping of street-view features.","Moreover, we fully leverage the advantages of multiple perspectives by introducing a novel satellite-guided reprojection module, optimizing the uneven feature distribution issues associated with traditional BEV methods.","Our method demonstrates significant improvements on four cross-view datasets collected from multiple cities, including New York, San Francisco, and Boston.","On average across these datasets, our method achieves an increase in mIOU by 10.13% and 5.21% compared with the state-of-the-art satellite-based and cross-view methods.","The code and datasets of this work will be released at https://github.com/yejy53/SG-BEV."],"url":"http://arxiv.org/abs/2404.02638v1","category":"cs.CV"}
{"created":"2024-04-03 10:54:07","title":"Vocabulary Attack to Hijack Large Language Model Applications","abstract":"The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single word insertion is sufficient. Second, we demonstrate that we can conduct our attack using a different model than the target model to conduct our attack with.","sentences":["The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications.","Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems.","They want the model to reveal confidential information, specific false information, or offensive behavior.","To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal.","Our approach is different.","It inserts words from the model vocabulary.","We find these words using an optimization procedure and embeddings from another LLM (attacker LLM).","We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively.","We present two main findings.","First, our approach creates inconspicuous instructions and therefore it is hard to detect.","For many attack cases, we find that even a single word insertion is sufficient.","Second, we demonstrate that we can conduct our attack using a different model than the target model to conduct our attack with."],"url":"http://arxiv.org/abs/2404.02637v1","category":"cs.CR"}
{"created":"2024-04-03 10:44:06","title":"3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization","abstract":"3D stylization, which entails the application of specific styles to three-dimensional objects, holds significant commercial potential as it enables the creation of diverse 3D objects with distinct moods and styles, tailored to specific demands of different scenes. With recent advancements in text-driven methods and artificial intelligence, the stylization process is increasingly intuitive and automated, thereby diminishing the reliance on manual labor and expertise. However, existing methods have predominantly focused on holistic stylization, thereby leaving the application of styles to individual components of a 3D object unexplored. In response, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization. Given a 3D mesh and a text prompt, 3DStyleGLIP leverages the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize the individual parts of the 3D mesh and modify their colors and local geometries to align them with the desired styles specified in the text prompt. 3DStyleGLIP is effectively trained for 3D stylization tasks through a part-level style loss working in GLIP's embedding space, supplemented by two complementary learning techniques. Extensive experimental validation confirms that our method achieves significant part-wise stylization capabilities, demonstrating promising potential in advancing the field of 3D stylization.","sentences":["3D stylization, which entails the application of specific styles to three-dimensional objects, holds significant commercial potential as it enables the creation of diverse 3D objects with distinct moods and styles, tailored to specific demands of different scenes.","With recent advancements in text-driven methods and artificial intelligence, the stylization process is increasingly intuitive and automated, thereby diminishing the reliance on manual labor and expertise.","However, existing methods have predominantly focused on holistic stylization, thereby leaving the application of styles to individual components of a 3D object unexplored.","In response, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization.","Given a 3D mesh and a text prompt, 3DStyleGLIP leverages the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize the individual parts of the 3D mesh and modify their colors and local geometries to align them with the desired styles specified in the text prompt.","3DStyleGLIP is effectively trained for 3D stylization tasks through a part-level style loss working in GLIP's embedding space, supplemented by two complementary learning techniques.","Extensive experimental validation confirms that our method achieves significant part-wise stylization capabilities, demonstrating promising potential in advancing the field of 3D stylization."],"url":"http://arxiv.org/abs/2404.02634v1","category":"cs.CV"}
{"created":"2024-04-03 10:29:06","title":"A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference","abstract":"Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders. Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent NLI in complex domains.","sentences":["Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI).","However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning.","In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS).","Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation.","Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders.","Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent NLI in complex domains."],"url":"http://arxiv.org/abs/2404.02625v1","category":"cs.CL"}
{"created":"2024-04-03 10:11:22","title":"Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models","abstract":"We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability. DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization methods, and 2) the automated identification of biases and spurious features.","sentences":["We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability.","DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions.","Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention.","The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text.","We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization methods, and 2) the automated identification of biases and spurious features."],"url":"http://arxiv.org/abs/2404.02618v1","category":"cs.CV"}
{"created":"2024-04-03 09:56:38","title":"SHIELD: A regularization technique for eXplainable Artificial Intelligence","abstract":"As Artificial Intelligence systems become integral across domains, the demand for explainability grows. While the effort by the scientific community is focused on obtaining a better explanation for the model, it is important not to ignore the potential of this explanation process to improve training as well. While existing efforts primarily focus on generating and evaluating explanations for black-box models, there remains a critical gap in directly enhancing models through these evaluations. This paper introduces SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a regularization technique for explainable artificial intelligence designed to improve model quality by concealing portions of input data and assessing the resulting discrepancy in predictions. In contrast to conventional approaches, SHIELD regularization seamlessly integrates into the objective function, enhancing model explainability while also improving performance. Experimental validation on benchmark datasets underscores SHIELD's effectiveness in improving Artificial Intelligence model explainability and overall performance. This establishes SHIELD regularization as a promising pathway for developing transparent and reliable Artificial Intelligence regularization techniques.","sentences":["As Artificial Intelligence systems become integral across domains, the demand for explainability grows.","While the effort by the scientific community is focused on obtaining a better explanation for the model, it is important not to ignore the potential of this explanation process to improve training as well.","While existing efforts primarily focus on generating and evaluating explanations for black-box models, there remains a critical gap in directly enhancing models through these evaluations.","This paper introduces SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a regularization technique for explainable artificial intelligence designed to improve model quality by concealing portions of input data and assessing the resulting discrepancy in predictions.","In contrast to conventional approaches, SHIELD regularization seamlessly integrates into the objective function, enhancing model explainability while also improving performance.","Experimental validation on benchmark datasets underscores SHIELD's effectiveness in improving Artificial Intelligence model explainability and overall performance.","This establishes SHIELD regularization as a promising pathway for developing transparent and reliable Artificial Intelligence regularization techniques."],"url":"http://arxiv.org/abs/2404.02611v1","category":"cs.AI"}
{"created":"2024-04-03 09:33:47","title":"Determining the Tactical Challenge of Scenarios to Efficiently Test Automated Driving Systems","abstract":"The selection of relevant test scenarios for the scenario-based testing and safety validation of automated driving systems (ADSs) remains challenging. An important aspect of the relevance of a scenario is the challenge it poses for an ADS. Existing methods for calculating the challenge of a scenario aim to express the challenge in terms of a metric value. Metric values are useful to select the least or most challenging scenario. However, they fail to provide human-interpretable information on the cause of the challenge which is critical information for the efficient selection of relevant test scenarios. Therefore, this paper presents the Challenge Description Method that mitigates this issue by analyzing scenarios and providing a description of their challenge in terms of the minimum required lane changes and their difficulty. Applying the method to different highway scenarios showed that it is capable of analyzing complex scenarios and providing easy-to-understand descriptions that can be used to select relevant test scenarios.","sentences":["The selection of relevant test scenarios for the scenario-based testing and safety validation of automated driving systems (ADSs) remains challenging.","An important aspect of the relevance of a scenario is the challenge it poses for an ADS.","Existing methods for calculating the challenge of a scenario aim to express the challenge in terms of a metric value.","Metric values are useful to select the least or most challenging scenario.","However, they fail to provide human-interpretable information on the cause of the challenge which is critical information for the efficient selection of relevant test scenarios.","Therefore, this paper presents the Challenge Description Method that mitigates this issue by analyzing scenarios and providing a description of their challenge in terms of the minimum required lane changes and their difficulty.","Applying the method to different highway scenarios showed that it is capable of analyzing complex scenarios and providing easy-to-understand descriptions that can be used to select relevant test scenarios."],"url":"http://arxiv.org/abs/2404.02599v1","category":"cs.SE"}
{"created":"2024-04-03 09:27:57","title":"Combining transition path sampling with data-driven collective variables through a reactivity-biased shooting algorithm","abstract":"Rare event sampling is a central problem in modern computational chemistry research. Among the existing methods, transition path sampling (TPS) can generate unbiased representations of reaction processes. However, its efficiency depends on the ability to generate reactive trial paths, which in turn depends on the quality of the shooting algorithm used. We propose a new algorithm based on the shooting success rate, i.e. reactivity, measured as a function of a reduced set of collective variables (CVs). These variables are extracted with a machine learning approach directly from TPS simulations, using a multi-task objective function. Iteratively, this workflow significantly improves shooting efficiency without any prior knowledge of the process. In addition, the optimized CVs can be used with biased enhanced sampling methodologies to accurately reconstruct the free energy profiles. We tested the method on three different systems: a two-dimensional toy model, conformational transitions of alanine dipeptide, and hydrolysis of acetyl chloride in bulk water. In the latter, we integrated our workflow with an active learning scheme to learn a reactive machine learning-based potential, which allowed us to study the mechanism and free energy profile with an ab initio-like accuracy.","sentences":["Rare event sampling is a central problem in modern computational chemistry research.","Among the existing methods, transition path sampling (TPS) can generate unbiased representations of reaction processes.","However, its efficiency depends on the ability to generate reactive trial paths, which in turn depends on the quality of the shooting algorithm used.","We propose a new algorithm based on the shooting success rate, i.e. reactivity, measured as a function of a reduced set of collective variables (CVs).","These variables are extracted with a machine learning approach directly from TPS simulations, using a multi-task objective function.","Iteratively, this workflow significantly improves shooting efficiency without any prior knowledge of the process.","In addition, the optimized CVs can be used with biased enhanced sampling methodologies to accurately reconstruct the free energy profiles.","We tested the method on three different systems: a two-dimensional toy model, conformational transitions of alanine dipeptide, and hydrolysis of acetyl chloride in bulk water.","In the latter, we integrated our workflow with an active learning scheme to learn a reactive machine learning-based potential, which allowed us to study the mechanism and free energy profile with an ab initio-like accuracy."],"url":"http://arxiv.org/abs/2404.02597v1","category":"physics.comp-ph"}
{"created":"2024-04-03 09:14:24","title":"Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation","abstract":"Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we fine-tuned a pre-trained language model specifically for emotion recognition in conversations, facilitating real-time affective annotations for utterances. For interpretability of recognition results, we formulate personality recognition as an NLI problem by determining whether the textual description of personality labels is entailed by the dialog content. Extensive experiments on two daily conversation datasets suggest that Affective-NLI significantly outperforms (by 6%-7%) state-of-the-art approaches. Additionally, our Flow experiment demonstrates that Affective-NLI can accurately recognize the speaker's personality in the early stages of conversations by surpassing state-of-the-art methods with 22%-34%.","sentences":["Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content.","It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly.","Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance.","First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored.","Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results.","In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC.","To utilize affectivity within dialog content for accurate personality recognition, we fine-tuned a pre-trained language model specifically for emotion recognition in conversations, facilitating real-time affective annotations for utterances.","For interpretability of recognition results, we formulate personality recognition as an NLI problem by determining whether the textual description of personality labels is entailed by the dialog content.","Extensive experiments on two daily conversation datasets suggest that Affective-NLI significantly outperforms (by 6%-7%) state-of-the-art approaches.","Additionally, our Flow experiment demonstrates that Affective-NLI can accurately recognize the speaker's personality in the early stages of conversations by surpassing state-of-the-art methods with 22%-34%."],"url":"http://arxiv.org/abs/2404.02589v1","category":"cs.CL"}
{"created":"2024-04-03 09:12:22","title":"The Surprising Effectiveness of Rankers Trained on Expanded Queries","abstract":"An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do LLM based query enrichment for training queries using relevant documents. Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a principled query performance based scoring method using base and specialized ranker offers a significant improvement of up to 25% on the passage ranking task and up to 48.4% on the document ranking task when compared to the baseline performance of using original queries, even outperforming SOTA model.","sentences":["An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution.","The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries.","In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries.","Firstly, we do LLM based query enrichment for training queries using relevant documents.","Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries.","We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query.","Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution.","In our extensive experiments on the DL-Hard dataset, we find that a principled query performance based scoring method using base and specialized ranker offers a significant improvement of up to 25% on the passage ranking task and up to 48.4% on the document ranking task when compared to the baseline performance of using original queries, even outperforming SOTA model."],"url":"http://arxiv.org/abs/2404.02587v1","category":"cs.IR"}
{"created":"2024-04-03 08:55:44","title":"Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation","abstract":"Optimizing deep learning models requires large amounts of annotated images, a process that is both time-intensive and costly. Especially for semantic segmentation models in which every pixel must be annotated. A potential strategy to mitigate annotation effort is active learning. Active learning facilitates the identification and selection of the most informative images from a large unlabelled pool. The underlying premise is that these selected images can improve the model's performance faster than random selection to reduce annotation effort. While active learning has demonstrated promising results on benchmark datasets like Cityscapes, its performance in the agricultural domain remains largely unexplored. This study addresses this research gap by conducting a comparative study of three active learning-based acquisition functions: Bayesian Active Learning by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random. The acquisition functions were tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing three semantic classes: background, crop and weed. Our results indicated that active learning, especially PowerBALD, yields a higher performance than Random sampling on both datasets. But due to the relatively large standard deviations, the differences observed were minimal; this was partly caused by high image redundancy and imbalanced classes. Specifically, more than 89\\% of the pixels belonged to the background class on both datasets. The absence of significant results on both datasets indicates that further research is required for applying active learning on agricultural datasets, especially if they contain a high-class imbalance and redundant images. Recommendations and insights are provided in this paper to potentially resolve such issues.","sentences":["Optimizing deep learning models requires large amounts of annotated images, a process that is both time-intensive and costly.","Especially for semantic segmentation models in which every pixel must be annotated.","A potential strategy to mitigate annotation effort is active learning.","Active learning facilitates the identification and selection of the most informative images from a large unlabelled pool.","The underlying premise is that these selected images can improve the model's performance faster than random selection to reduce annotation effort.","While active learning has demonstrated promising results on benchmark datasets like Cityscapes, its performance in the agricultural domain remains largely unexplored.","This study addresses this research gap by conducting a comparative study of three active learning-based acquisition functions: Bayesian Active Learning by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random.","The acquisition functions were tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing three semantic classes: background, crop and weed.","Our results indicated that active learning, especially PowerBALD, yields a higher performance than Random sampling on both datasets.","But due to the relatively large standard deviations, the differences observed were minimal; this was partly caused by high image redundancy and imbalanced classes.","Specifically, more than 89\\% of the pixels belonged to the background class on both datasets.","The absence of significant results on both datasets indicates that further research is required for applying active learning on agricultural datasets, especially if they contain a high-class imbalance and redundant images.","Recommendations and insights are provided in this paper to potentially resolve such issues."],"url":"http://arxiv.org/abs/2404.02580v1","category":"cs.CV"}
{"created":"2024-04-03 08:54:58","title":"Learning Alternative Ways of Performing a Task","abstract":"A common way of learning to perform a task is to observe how it is carried out by experts. However, it is well known that for most tasks there is no unique way to perform them. This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task. In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task). Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data. Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strategy of performing a task. By an iterative process based on generalisation and specialisation, we learn the underlying patterns that capture the different styles of performing a task exhibited by the examples. We illustrate our approach on two common activity recognition tasks: a surgical skills training task and a cooking domain. We evaluate the inferred models with respect to two metrics that measure how well the models represent the examples and capture the different forms of executing a task showed by the examples. We compare our results with the traditional process mining approach and show that a small set of meaningful examples is enough to obtain patterns that capture the different strategies that are followed to solve the tasks.","sentences":["A common way of learning to perform a task is to observe how it is carried out by experts.","However, it is well known that for most tasks there is no unique way to perform them.","This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task.","In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task).","Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data.","Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strategy of performing a task.","By an iterative process based on generalisation and specialisation, we learn the underlying patterns that capture the different styles of performing a task exhibited by the examples.","We illustrate our approach on two common activity recognition tasks: a surgical skills training task and a cooking domain.","We evaluate the inferred models with respect to two metrics that measure how well the models represent the examples and capture the different forms of executing a task showed by the examples.","We compare our results with the traditional process mining approach and show that a small set of meaningful examples is enough to obtain patterns that capture the different strategies that are followed to solve the tasks."],"url":"http://arxiv.org/abs/2404.02579v1","category":"cs.AI"}
{"created":"2024-04-03 08:47:32","title":"Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification","abstract":"Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time. Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks. Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making. This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time. The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings. Additionally, it incorporates a loss-based concept drift detection mechanism to recalculate graph prototypes when drift is detected.","sentences":["Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time.","Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks.","Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making.","This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time.","The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings.","Additionally, it incorporates a loss-based concept drift detection mechanism to recalculate graph prototypes when drift is detected."],"url":"http://arxiv.org/abs/2404.02572v1","category":"cs.LG"}
{"created":"2024-04-03 08:42:36","title":"SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing","abstract":"Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation. Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot.","sentences":["Cooking robots can enhance the home experience by reducing the burden of daily chores.","However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives.","This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks.","More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control.","Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board.","However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste.","Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation.","Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot."],"url":"http://arxiv.org/abs/2404.02569v1","category":"cs.RO"}
{"created":"2024-04-03 08:41:45","title":"On Future Power Systems Digital Twins: Towards a Standard Architecture","abstract":"The energy sector's digital transformation brings mutually dependent communication and energy infrastructure, tightening the relationship between the physical and the digital world. Digital twins (DT) are the key concept for this. This paper initially discusses the evolution of the DT concept across various engineering applications before narrowing its focus to the power systems domain. By reviewing different definitions and applications, we present a new definition of DTs specifically tailored to power systems. Based on the proposed definition and extensive deliberations and consultations with distribution system operators, energy traders, and municipalities, we introduce a standard DT ecosystem architecture that offers services beyond real-time updates and can seamlessly integrate with existing transmission and distribution system operators' processes, while reconciling with concepts such as microgrids and local energy communities based on a system-of-systems view. We also discuss the integration of power system DTs into various phases of the system's life cycle, such as long-term planning, emphasizing challenges that remain to be addressed, such as managing measurement and model errors, and uncertainty propagation. Finally, we present our vision of how artificial intelligence (AI) and machine learning (ML) can enhance several power system DT modules established in the proposed architecture.","sentences":["The energy sector's digital transformation brings mutually dependent communication and energy infrastructure, tightening the relationship between the physical and the digital world.","Digital twins (DT) are the key concept for this.","This paper initially discusses the evolution of the DT concept across various engineering applications before narrowing its focus to the power systems domain.","By reviewing different definitions and applications, we present a new definition of DTs specifically tailored to power systems.","Based on the proposed definition and extensive deliberations and consultations with distribution system operators, energy traders, and municipalities, we introduce a standard DT ecosystem architecture that offers services beyond real-time updates and can seamlessly integrate with existing transmission and distribution system operators' processes, while reconciling with concepts such as microgrids and local energy communities based on a system-of-systems view.","We also discuss the integration of power system DTs into various phases of the system's life cycle, such as long-term planning, emphasizing challenges that remain to be addressed, such as managing measurement and model errors, and uncertainty propagation.","Finally, we present our vision of how artificial intelligence (AI) and machine learning (ML) can enhance several power system DT modules established in the proposed architecture."],"url":"http://arxiv.org/abs/2404.02568v1","category":"eess.SY"}
{"created":"2024-04-03 08:29:44","title":"scenario.center: Methods from Real-world Data to a Scenario Database","abstract":"Scenario-based testing is a promising method to develop, verify and validate automated driving systems (ADS) since pure on-road testing seems inefficient for complex traffic environments. A major challenge for this approach is the provision and management of a sufficient number of scenarios to test a system. The provision, generation, and management of scenario at scale is investigated in current research. This paper presents the scenario database scenario.center ( https://scenario.center ) to process and manage scenario data covering the needs of scenario-based testing approaches comprehensively and automatically. Thereby, requirements for such databases are described. Based on those, a four-step approach is proposed. Firstly, a common input format with defined quality requirements is defined. This is utilized for detecting events and base scenarios automatically. Furthermore, methods for searchability, evaluation of data quality and different scenario generation methods are proposed to allow a broad applicability serving different needs. For evaluation, the methodology is compared to state-of-the-art scenario databases. Finally, the application and capabilities of the database are shown by applying the methodology to the inD dataset. A public demonstration of the database interface is provided at https://scenario.center .","sentences":["Scenario-based testing is a promising method to develop, verify and validate automated driving systems (ADS) since pure on-road testing seems inefficient for complex traffic environments.","A major challenge for this approach is the provision and management of a sufficient number of scenarios to test a system.","The provision, generation, and management of scenario at scale is investigated in current research.","This paper presents the scenario database scenario.center ( https://scenario.center ) to process and manage scenario data covering the needs of scenario-based testing approaches comprehensively and automatically.","Thereby, requirements for such databases are described.","Based on those, a four-step approach is proposed.","Firstly, a common input format with defined quality requirements is defined.","This is utilized for detecting events and base scenarios automatically.","Furthermore, methods for searchability, evaluation of data quality and different scenario generation methods are proposed to allow a broad applicability serving different needs.","For evaluation, the methodology is compared to state-of-the-art scenario databases.","Finally, the application and capabilities of the database are shown by applying the methodology to the inD dataset.","A public demonstration of the database interface is provided at https://scenario.center ."],"url":"http://arxiv.org/abs/2404.02561v1","category":"cs.SE"}
{"created":"2024-04-03 08:27:24","title":"Regional biases in image geolocation estimation: a case study with the SenseCity Africa dataset","abstract":"Advances in Artificial Intelligence are challenged by the biases rooted in the datasets used to train the models. In image geolocation estimation, models are mostly trained using data from specific geographic regions, notably the Western world, and as a result, they may struggle to comprehend the complexities of underrepresented regions. To assess this issue, we apply a state-of-the-art image geolocation estimation model (ISNs) to a crowd-sourced dataset of geolocated images from the African continent (SCA100), and then explore the regional and socioeconomic biases underlying the model's predictions. Our findings show that the ISNs model tends to over-predict image locations in high-income countries of the Western world, which is consistent with the geographic distribution of its training data, i.e., the IM2GPS3k dataset. Accordingly, when compared to the IM2GPS3k benchmark, the accuracy of the ISNs model notably decreases at all scales. Additionally, we cluster images of the SCA100 dataset based on how accurately they are predicted by the ISNs model and show the model's difficulties in correctly predicting the locations of images in low income regions, especially in Sub-Saharan Africa. Therefore, our results suggest that using IM2GPS3k as a training set and benchmark for image geolocation estimation and other computer vision models overlooks its potential application in the African context.","sentences":["Advances in Artificial Intelligence are challenged by the biases rooted in the datasets used to train the models.","In image geolocation estimation, models are mostly trained using data from specific geographic regions, notably the Western world, and as a result, they may struggle to comprehend the complexities of underrepresented regions.","To assess this issue, we apply a state-of-the-art image geolocation estimation model (ISNs) to a crowd-sourced dataset of geolocated images from the African continent (SCA100), and then explore the regional and socioeconomic biases underlying the model's predictions.","Our findings show that the ISNs model tends to over-predict image locations in high-income countries of the Western world, which is consistent with the geographic distribution of its training data, i.e., the IM2GPS3k dataset.","Accordingly, when compared to the IM2GPS3k benchmark, the accuracy of the ISNs model notably decreases at all scales.","Additionally, we cluster images of the SCA100 dataset based on how accurately they are predicted by the ISNs model and show the model's difficulties in correctly predicting the locations of images in low income regions, especially in Sub-Saharan Africa.","Therefore, our results suggest that using IM2GPS3k as a training set and benchmark for image geolocation estimation and other computer vision models overlooks its potential application in the African context."],"url":"http://arxiv.org/abs/2404.02558v1","category":"cs.CV"}
{"created":"2024-04-03 08:21:01","title":"Occurrence of the collective Ziman limit of heat transport in cubic semiconductors: scattering channels and size effects","abstract":"In this work, we discuss the possibility of reaching the Ziman conditions for collective heat transport in cubic bulk semiconductors, such as Si, Ge, AlAs and AlP. In natural and enriched silicon and germanium, the collective heat transport limit is impossible to reach due to strong isotopic scattering. However, we show that in hyperenriched silicon and germanium, as well as in materials with one single stable isotope like AlAs and AlP, at low temperatures, normal scattering plays an important role, making the observation of the collective heat transport possible. We further discuss the effects of sample sizes, and analyse our results for cubic materials by comparing them to bulk bismuth, in which second sound has been detected at cryogenic temperatures. We find that collective heat transport in cubic semiconductors studied in this work is expected to occur at temperatures between 10 and 20 K.","sentences":["In this work, we discuss the possibility of reaching the Ziman conditions for collective heat transport in cubic bulk semiconductors, such as Si, Ge, AlAs and AlP.","In natural and enriched silicon and germanium, the collective heat transport limit is impossible to reach due to strong isotopic scattering.","However, we show that in hyperenriched silicon and germanium, as well as in materials with one single stable isotope like AlAs and AlP, at low temperatures, normal scattering plays an important role, making the observation of the collective heat transport possible.","We further discuss the effects of sample sizes, and analyse our results for cubic materials by comparing them to bulk bismuth, in which second sound has been detected at cryogenic temperatures.","We find that collective heat transport in cubic semiconductors studied in this work is expected to occur at temperatures between 10 and 20 K."],"url":"http://arxiv.org/abs/2404.02553v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 08:18:45","title":"Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data","abstract":"Given the rarity of significant solar flares compared to smaller ones, training effective machine learning models for solar activity forecasting is challenging due to insufficient data. This study proposes using generative deep learning models, specifically a Denoising Diffusion Probabilistic Model (DDPM), to create synthetic images of solar phenomena, including flares of varying intensities. By employing a dataset from the AIA instrument aboard the SDO spacecraft, focusing on the 171 {\\AA} band that captures various solar activities, and classifying images with GOES X-ray measurements based on flare intensity, we aim to address the data scarcity issue. The DDPM's performance is evaluated using cluster metrics, Frechet Inception Distance (FID), and F1-score, showcasing promising results in generating realistic solar imagery. We conduct two experiments: one to train a supervised classifier for event identification and another for basic flare prediction, demonstrating the value of synthetic data in managing imbalanced datasets. This research underscores the potential of DDPMs in solar data analysis and forecasting, suggesting further exploration into their capabilities for solar flare prediction and application in other deep learning and physical tasks.","sentences":["Given the rarity of significant solar flares compared to smaller ones, training effective machine learning models for solar activity forecasting is challenging due to insufficient data.","This study proposes using generative deep learning models, specifically a Denoising Diffusion Probabilistic Model (DDPM), to create synthetic images of solar phenomena, including flares of varying intensities.","By employing a dataset from the AIA instrument aboard the SDO spacecraft, focusing on the 171 {\\AA} band that captures various solar activities, and classifying images with GOES X-ray measurements based on flare intensity, we aim to address the data scarcity issue.","The DDPM's performance is evaluated using cluster metrics, Frechet Inception Distance (FID), and F1-score, showcasing promising results in generating realistic solar imagery.","We conduct two experiments: one to train a supervised classifier for event identification and another for basic flare prediction, demonstrating the value of synthetic data in managing imbalanced datasets.","This research underscores the potential of DDPMs in solar data analysis and forecasting, suggesting further exploration into their capabilities for solar flare prediction and application in other deep learning and physical tasks."],"url":"http://arxiv.org/abs/2404.02552v1","category":"astro-ph.SR"}
{"created":"2024-04-03 08:16:12","title":"On the comparison between phenomenological and kinetic theories of gas mixtures with applications to flocking","abstract":"We study the compression between the phenomenological and kinetic models for a mixture of gases from the viewpoint of collective dynamics. In the case in which constituents are Eulerian gases, balance equations for mass, momentum, and energy are the same in the main differential part, but production terms due to the interchanges between constituents are different. They coincide only when the thermal and mechanical diffusion are sufficiently small. In this paper, we first verify that both models satisfy the universal requirements of conservation laws of total mass, momentum, and energy, Galilean invariance and entropy principle. Following the work of Ha and Ruggeri (ARMA 2017), we consider spatially homogeneous models which correspond to the generalizations of the Cucker Smale model with the thermal effect. In these circumstances, we provide analytical results for the comparison between two resulting models and also present several numerical simulations to complement analytical results.","sentences":["We study the compression between the phenomenological and kinetic models for a mixture of gases from the viewpoint of collective dynamics.","In the case in which constituents are Eulerian gases, balance equations for mass, momentum, and energy are the same in the main differential part, but production terms due to the interchanges between constituents are different.","They coincide only when the thermal and mechanical diffusion are sufficiently small.","In this paper, we first verify that both models satisfy the universal requirements of conservation laws of total mass, momentum, and energy, Galilean invariance and entropy principle.","Following the work of Ha and Ruggeri (ARMA 2017), we consider spatially homogeneous models which correspond to the generalizations of the Cucker Smale model with the thermal effect.","In these circumstances, we provide analytical results for the comparison between two resulting models and also present several numerical simulations to complement analytical results."],"url":"http://arxiv.org/abs/2404.02550v1","category":"math.DS"}
{"created":"2024-04-03 08:15:08","title":"AI-Tutoring in Software Engineering Education","abstract":"With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.","sentences":["With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation.","The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense.","However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored.","Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.","In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis.","Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor.","Additionally, the findings highlight advantages, such as timely feedback and scalability.","However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident.","This research adds to the discourse on AI's role in education."],"url":"http://arxiv.org/abs/2404.02548v1","category":"cs.SE"}
{"created":"2024-04-03 08:03:27","title":"Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning","abstract":"Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretically proved that only a few conditions are needed to obtain accurate uncertainty constraints in the proposed method. Moreover, we develop a Grid-Mapping Pseudo-Count Soft Actor-Critic(GPC-SAC) algorithm using GPC under the Soft Actor-Critic(SAC) framework to demonstrate the effectiveness of GPC. The experimental results on D4RL benchmark datasets show that GPC-SAC has better performance and less computational cost compared to other algorithms.","sentences":["Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application.","However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions.","To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter.","Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs.","In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost.","The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count.","It is theoretically proved that only a few conditions are needed to obtain accurate uncertainty constraints in the proposed method.","Moreover, we develop a Grid-Mapping Pseudo-Count Soft Actor-Critic(GPC-SAC) algorithm using GPC under the Soft Actor-Critic(SAC) framework to demonstrate the effectiveness of GPC.","The experimental results on D4RL benchmark datasets show that GPC-SAC has better performance and less computational cost compared to other algorithms."],"url":"http://arxiv.org/abs/2404.02545v1","category":"cs.LG"}
{"created":"2024-04-03 08:01:00","title":"Semi-Supervised Unconstrained Head Pose Estimation in the Wild","abstract":"Existing head pose estimation datasets are either composed of numerous samples by non-realistic synthesis or lab collection, or limited images by labor-intensive annotating. This makes deep supervised learning based solutions compromised due to the reliance on generous labeled data. To alleviate it, we propose the first semi-supervised unconstrained head pose estimation (SemiUHPE) method, which can leverage a large amount of unlabeled wild head images. Specifically, we follow the recent semi-supervised rotation regression, and focus on the diverse and complex head pose domain. Firstly, we claim that the aspect-ratio invariant cropping of heads is superior to the previous landmark-based affine alignment, which does not fit unlabeled natural heads or practical applications where landmarks are often unavailable. Then, instead of using an empirically fixed threshold to filter out pseudo labels, we propose the dynamic entropy-based filtering by updating thresholds for adaptively removing unlabeled outliers. Moreover, we revisit the design of weak-strong augmentations, and further exploit its superiority by devising two novel head-oriented strong augmentations named pose-irrelevant cut-occlusion and pose-altering rotation consistency. Extensive experiments show that SemiUHPE can surpass SOTAs with remarkable improvements on public benchmarks under both front-range and full-range. Our code is released in \\url{https://github.com/hnuzhy/SemiUHPE}.","sentences":["Existing head pose estimation datasets are either composed of numerous samples by non-realistic synthesis or lab collection, or limited images by labor-intensive annotating.","This makes deep supervised learning based solutions compromised due to the reliance on generous labeled data.","To alleviate it, we propose the first semi-supervised unconstrained head pose estimation (SemiUHPE) method, which can leverage a large amount of unlabeled wild head images.","Specifically, we follow the recent semi-supervised rotation regression, and focus on the diverse and complex head pose domain.","Firstly, we claim that the aspect-ratio invariant cropping of heads is superior to the previous landmark-based affine alignment, which does not fit unlabeled natural heads or practical applications where landmarks are often unavailable.","Then, instead of using an empirically fixed threshold to filter out pseudo labels, we propose the dynamic entropy-based filtering by updating thresholds for adaptively removing unlabeled outliers.","Moreover, we revisit the design of weak-strong augmentations, and further exploit its superiority by devising two novel head-oriented strong augmentations named pose-irrelevant cut-occlusion and pose-altering rotation consistency.","Extensive experiments show that SemiUHPE can surpass SOTAs with remarkable improvements on public benchmarks under both front-range and full-range.","Our code is released in \\url{https://github.com/hnuzhy/SemiUHPE}."],"url":"http://arxiv.org/abs/2404.02544v1","category":"cs.CV"}
{"created":"2024-04-03 08:00:46","title":"Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset","abstract":"Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments reveal that ULTR robustly improves click prediction. However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark.","sentences":["Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data.","While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines.","The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques.","Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques.","We revisit and extend the available experiments.","We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features.","Our experiments reveal that ULTR robustly improves click prediction.","However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark."],"url":"http://arxiv.org/abs/2404.02543v1","category":"cs.IR"}
{"created":"2024-04-03 07:44:38","title":"ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model","abstract":"In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.","sentences":["In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages.","However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape.","This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach.","In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks.","We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively."],"url":"http://arxiv.org/abs/2404.02534v1","category":"cs.CL"}
{"created":"2024-04-03 07:43:11","title":"Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game","abstract":"With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.","sentences":["With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise.","There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering.","As a result, large models counter malicious attackers' attacks using techniques such as safety alignment.","However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities.","In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent.","First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks.","After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents.","The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods.","Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations."],"url":"http://arxiv.org/abs/2404.02532v1","category":"cs.AI"}
{"created":"2024-04-03 07:33:30","title":"Severity Controlled Text-to-Image Generative Model Bias Manipulation","abstract":"Text-to-image (T2I) generative models are gaining wide popularity, especially in public domains. However, their intrinsic bias and potential malicious manipulations remain under-explored. Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models. By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias. As a by-product, this control also allows a form of precise prompt engineering to generate images which are generally implausible with regular text prompts. We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing. Our technique does not require training and is also framed as a backdoor attack with severity control using semantically-null text triggers in the prompts. With extensive analysis, we present interesting qualitative and quantitative results to expose potential manipulation possibilities for T2I models.   Key-words: Text-to-Image Models, Generative Models, Backdoor Attacks, Prompt Engineering, Bias","sentences":["Text-to-image (T2I) generative models are gaining wide popularity, especially in public domains.","However, their intrinsic bias and potential malicious manipulations remain under-explored.","Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models.","By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias.","As a by-product, this control also allows a form of precise prompt engineering to generate images which are generally implausible with regular text prompts.","We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing.","Our technique does not require training and is also framed as a backdoor attack with severity control using semantically-null text triggers in the prompts.","With extensive analysis, we present interesting qualitative and quantitative results to expose potential manipulation possibilities for T2I models.   ","Key-words: Text-to-Image Models, Generative Models, Backdoor Attacks, Prompt Engineering, Bias"],"url":"http://arxiv.org/abs/2404.02530v1","category":"cs.CV"}
{"created":"2024-04-03 07:23:03","title":"Text-driven Affordance Learning from Egocentric Vision","abstract":"Visual affordance learning is a key component for robots to understand how to interact with objects. Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios. The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects. This approach covers both hand-object and tool-object interactions. We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction. In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations. However, when we gather data for this task, manual annotations of these diverse interactions are costly. To this end, we propose a pseudo dataset creation pipeline and build a large pseudo-training dataset: TextAFF80K, consisting of over 80K instances of the contact points, trajectories, images, and text tuples. We extend existing referring expression comprehension models for our task, and experimental results show that our approach robustly handles multiple affordances, serving as a new standard for affordance learning in real-world scenarios.","sentences":["Visual affordance learning is a key component for robots to understand how to interact with objects.","Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios.","The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects.","This approach covers both hand-object and tool-object interactions.","We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction.","In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations.","However, when we gather data for this task, manual annotations of these diverse interactions are costly.","To this end, we propose a pseudo dataset creation pipeline and build a large pseudo-training dataset: TextAFF80K, consisting of over 80K instances of the contact points, trajectories, images, and text tuples.","We extend existing referring expression comprehension models for our task, and experimental results show that our approach robustly handles multiple affordances, serving as a new standard for affordance learning in real-world scenarios."],"url":"http://arxiv.org/abs/2404.02523v1","category":"cs.CV"}
{"created":"2024-04-03 07:12:18","title":"Differentially Private Verification of Survey-Weighted Estimates","abstract":"Several official statistics agencies release synthetic data as public use microdata files. In practice, synthetic data do not admit accurate results for every analysis. Thus, it is beneficial for agencies to provide users with feedback on the quality of their analyses of the synthetic data. One approach is to couple synthetic data with a verification server that provides users with measures of the similarity of estimates computed with the synthetic and underlying confidential data. However, such measures leak information about the confidential records, so that agencies may wish to apply disclosure control methods to the released verification measures. We present a verification measure that satisfies differential privacy and can be used when the underlying confidential are collected with a complex survey design. We illustrate the verification measure using repeated sampling simulations where the confidential data are sampled with a probability proportional to size design, and the analyst estimates a population total or mean with the synthetic data. The simulations suggest that the verification measures can provide useful information about the quality of synthetic data inferences.","sentences":["Several official statistics agencies release synthetic data as public use microdata files.","In practice, synthetic data do not admit accurate results for every analysis.","Thus, it is beneficial for agencies to provide users with feedback on the quality of their analyses of the synthetic data.","One approach is to couple synthetic data with a verification server that provides users with measures of the similarity of estimates computed with the synthetic and underlying confidential data.","However, such measures leak information about the confidential records, so that agencies may wish to apply disclosure control methods to the released verification measures.","We present a verification measure that satisfies differential privacy and can be used when the underlying confidential are collected with a complex survey design.","We illustrate the verification measure using repeated sampling simulations where the confidential data are sampled with a probability proportional to size design, and the analyst estimates a population total or mean with the synthetic data.","The simulations suggest that the verification measures can provide useful information about the quality of synthetic data inferences."],"url":"http://arxiv.org/abs/2404.02519v1","category":"cs.CR"}
{"created":"2024-04-03 07:11:19","title":"CPAISD: Core-penumbra acute ischemic stroke dataset","abstract":"We introduce the CPAISD: Core-Penumbra Acute Ischemic Stroke Dataset, aimed at enhancing the early detection and segmentation of ischemic stroke using Non-Contrast Computed Tomography (NCCT) scans. Addressing the challenges in diagnosing acute ischemic stroke during its early stages due to often non-revealing native CT findings, the dataset provides a collection of segmented NCCT images. These include annotations of ischemic core and penumbra regions, critical for developing machine learning models for rapid stroke identification and assessment. By offering a carefully collected and annotated dataset, we aim to facilitate the development of advanced diagnostic tools, contributing to improved patient care and outcomes in stroke management. Our dataset's uniqueness lies in its focus on the acute phase of ischemic stroke, with non-informative native CT scans, and includes a baseline model to demonstrate the dataset's application, encouraging further research and innovation in the field of medical imaging and stroke diagnosis.","sentences":["We introduce the CPAISD: Core-Penumbra Acute Ischemic Stroke Dataset, aimed at enhancing the early detection and segmentation of ischemic stroke using Non-Contrast Computed Tomography (NCCT) scans.","Addressing the challenges in diagnosing acute ischemic stroke during its early stages due to often non-revealing native CT findings, the dataset provides a collection of segmented NCCT images.","These include annotations of ischemic core and penumbra regions, critical for developing machine learning models for rapid stroke identification and assessment.","By offering a carefully collected and annotated dataset, we aim to facilitate the development of advanced diagnostic tools, contributing to improved patient care and outcomes in stroke management.","Our dataset's uniqueness lies in its focus on the acute phase of ischemic stroke, with non-informative native CT scans, and includes a baseline model to demonstrate the dataset's application, encouraging further research and innovation in the field of medical imaging and stroke diagnosis."],"url":"http://arxiv.org/abs/2404.02518v1","category":"eess.IV"}
{"created":"2024-04-04 17:59:40","title":"The More You See in 2D, the More You Perceive in 3D","abstract":"Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.","sentences":["Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images.","Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images.","Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning.","The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis.","We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods.","We demonstrate our system on real images as well as standard synthetic benchmarks.","Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding."],"url":"http://arxiv.org/abs/2404.03652v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:22","title":"Multipartite edge modes and tensor networks","abstract":"Holographic tensor networks model AdS/CFT, but so far they have been limited by involving only systems that are very different from gravity. Unfortunately, we cannot straightforwardly discretize gravity to incorporate it, because that would break diffeomorphism invariance. In this note, we explore a resolution. In low dimensions gravity can be written as a topological gauge theory, which can be discretized without breaking gauge-invariance. However, new problems arise. Foremost, we now need a qualitatively new kind of \"area operator,\" which has no relation to the number of links along the cut and is instead topological. Secondly, the inclusion of matter becomes trickier. We successfully construct a tensor network both including matter and with this new type of area. Notably, while this area is still related to the entanglement in \"edge mode\" degrees of freedom, the edge modes are no longer bipartite entangled pairs. Instead they are highly multipartite. Along the way, we calculate the entropy of novel subalgebras in a particular topological gauge theory. We also show that the multipartite nature of the edge modes gives rise to non-commuting area operators, a property that other tensor networks do not exhibit.","sentences":["Holographic tensor networks model AdS/CFT, but so far they have been limited by involving only systems that are very different from gravity.","Unfortunately, we cannot straightforwardly discretize gravity to incorporate it, because that would break diffeomorphism invariance.","In this note, we explore a resolution.","In low dimensions gravity can be written as a topological gauge theory, which can be discretized without breaking gauge-invariance.","However, new problems arise.","Foremost, we now need a qualitatively new kind of \"area operator,\" which has no relation to the number of links along the cut and is instead topological.","Secondly, the inclusion of matter becomes trickier.","We successfully construct a tensor network both including matter and with this new type of area.","Notably, while this area is still related to the entanglement in \"edge mode\" degrees of freedom, the edge modes are no longer bipartite entangled pairs.","Instead they are highly multipartite.","Along the way, we calculate the entropy of novel subalgebras in a particular topological gauge theory.","We also show that the multipartite nature of the edge modes gives rise to non-commuting area operators, a property that other tensor networks do not exhibit."],"url":"http://arxiv.org/abs/2404.03651v1","category":"hep-th"}
{"created":"2024-04-04 17:59:08","title":"OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views","abstract":"Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.","sentences":["Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner.","This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set.","More recently, first works on open-set segmentation in 3D scenes have appeared in the literature.","These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes.","However, these 3D scene representations do not align well with the image-based nature of the visual-language models.","Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features.","To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF.","This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization.","Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images.","For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU."],"url":"http://arxiv.org/abs/2404.03650v1","category":"cs.CV"}
{"created":"2024-04-04 17:55:38","title":"Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention","abstract":"In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication","sentences":["In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards.","Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation.","Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate.","In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention.","The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process.","To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks.","Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance.","Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks.","The source code is accessible online to facilitate replication"],"url":"http://arxiv.org/abs/2404.03637v1","category":"cs.IR"}
{"created":"2024-04-04 17:49:38","title":"ROBUST: 221 Bugs in the Robot Operating System","abstract":"As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS). We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes. Finally, we reflect on the implications of our findings and outline future research directions for the community.","sentences":["As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.","To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS).","We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes.","Finally, we reflect on the implications of our findings and outline future research directions for the community."],"url":"http://arxiv.org/abs/2404.03629v1","category":"cs.SE"}
{"created":"2024-04-04 17:39:41","title":"On the Efficiency of Convolutional Neural Networks","abstract":"Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models. Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago. Yet computer scientists make computational efficiency their primary objective. Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements. Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency. Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency. However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity. Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency. This insight enabled us to co-optimize the separate factors that determine latency. We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity. Therefore, kernels that implement these layers use significant memory resources. We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size. Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task. Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost.","sentences":["Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models.","Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago.","Yet computer scientists make computational efficiency their primary objective.","Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements.","Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency.","Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency.","However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity.","Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency.","This insight enabled us to co-optimize the separate factors that determine latency.","We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity.","Therefore, kernels that implement these layers use significant memory resources.","We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size.","Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task.","Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost."],"url":"http://arxiv.org/abs/2404.03617v1","category":"cs.LG"}
{"created":"2024-04-04 17:25:25","title":"A Unified Algorithmic Framework for Dynamic Assortment Optimization under MNL Choice","abstract":"We consider assortment and inventory planning problems with dynamic stockout-based substitution effects and no replenishment. We consider two settings: 1. Customers can see all available products when they arrive, which is commonly seen in physical stores. 2. The seller can choose to offer a subset of available products to each customer, which is typical on online platforms. Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different. We develop a unified algorithm framework under the MNL choice model for both settings. Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee, runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints. In the process, we establish various novel properties of dynamic assortment planning (under the MNL choice) that may be useful more broadly.","sentences":["We consider assortment and inventory planning problems with dynamic stockout-based substitution effects and no replenishment.","We consider two settings: 1. Customers can see all available products when they arrive, which is commonly seen in physical stores.","2.","The seller can choose to offer a subset of available products to each customer, which is typical on online platforms.","Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different.","We develop a unified algorithm framework under the MNL choice model for both settings.","Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee, runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints.","In the process, we establish various novel properties of dynamic assortment planning (under the MNL choice) that may be useful more broadly."],"url":"http://arxiv.org/abs/2404.03604v1","category":"math.OC"}
{"created":"2024-04-04 17:23:28","title":"Analysis of second-order temporal schemes for modeling flow-solute transport in unsaturated porous media","abstract":"In this study, second-order temporal discretizations are analyzed for solving the coupled system of infiltration and solute transport in unsaturated porous media. The Richards equation is used to describe unsaturated flow, while the advection-dispersion equation (ADE) is used for modeling solute transport. The standard finite element discretization in space is utilized and four time-stepping methods are studied. Three of these methods require an iterative resolution to solve the Richards equation in its mixed form. In the remaining method, a novel technique is proposed to linearize the system of equations in time, and the iterative processes are avoided. In this method, a free stabilized parameter is introduced. Numerical tests are conducted to analyze the accuracy and efficiency of methods. The developed linear scheme based on the optimal free parameter is accurate and performs better in terms of efficiency since it offers a considerable gain in computational time compared to the other methods. The reliability and effectiveness of the developed semi-implicit scheme are investigated using numerical experiments for modeling water flow and solute transport in unsaturated soils.","sentences":["In this study, second-order temporal discretizations are analyzed for solving the coupled system of infiltration and solute transport in unsaturated porous media.","The Richards equation is used to describe unsaturated flow, while the advection-dispersion equation (ADE) is used for modeling solute transport.","The standard finite element discretization in space is utilized and four time-stepping methods are studied.","Three of these methods require an iterative resolution to solve the Richards equation in its mixed form.","In the remaining method, a novel technique is proposed to linearize the system of equations in time, and the iterative processes are avoided.","In this method, a free stabilized parameter is introduced.","Numerical tests are conducted to analyze the accuracy and efficiency of methods.","The developed linear scheme based on the optimal free parameter is accurate and performs better in terms of efficiency since it offers a considerable gain in computational time compared to the other methods.","The reliability and effectiveness of the developed semi-implicit scheme are investigated using numerical experiments for modeling water flow and solute transport in unsaturated soils."],"url":"http://arxiv.org/abs/2404.03603v1","category":"math.NA"}
{"created":"2024-04-04 17:00:37","title":"Beyond the blur: using experimental point spread functions to help scanning Kelvin probe microscopy reach its full potential","abstract":"Scanning Kelvin probe microscopy (SKPM) is a powerful technique for investigating the electrostatic properties of material surfaces, enabling the imaging of variations in work function, topology, surface charge density, or combinations thereof. Regardless of the underlying signal source, SKPM results in a voltage image which is spatially distorted due to the finite size of the probe, long-range electrostatic interactions, mechanical and electrical noise, and the finite response time of the electronics. In order to recover the underlying signal, it is necessary to deconvolve the measurement with an appropriate point spread function (PSF) that accounts the aforementioned distortions, but determining this PSF is difficult. Here we describe how such PSFs can be determined experimentally, and show how they can be used to recover the underlying information of interest. We first consider the physical principles that enable SKPM, and discuss how these affect the system PSF. We then show how one can experimentally measure PSFs by looking at well defined features, and that these compare well to simulated PSFs, provided scans are performed extremely slowly and carefully. Next, we work at realistic scan speeds, and show that the idealised PSFs fail to capture temporal distortions in the scan direction. While simulating PSFs for these situations would be quite challenging, we show that measuring PSFs with similar scan parameters works well. Our approach clarifies the basic principles of and inherent challenges to SKPM measurements, and gives practical methods to improve results.","sentences":["Scanning Kelvin probe microscopy (SKPM) is a powerful technique for investigating the electrostatic properties of material surfaces, enabling the imaging of variations in work function, topology, surface charge density, or combinations thereof.","Regardless of the underlying signal source, SKPM results in a voltage image which is spatially distorted due to the finite size of the probe, long-range electrostatic interactions, mechanical and electrical noise, and the finite response time of the electronics.","In order to recover the underlying signal, it is necessary to deconvolve the measurement with an appropriate point spread function (PSF) that accounts the aforementioned distortions, but determining this PSF is difficult.","Here we describe how such PSFs can be determined experimentally, and show how they can be used to recover the underlying information of interest.","We first consider the physical principles that enable SKPM, and discuss how these affect the system PSF.","We then show how one can experimentally measure PSFs by looking at well defined features, and that these compare well to simulated PSFs, provided scans are performed extremely slowly and carefully.","Next, we work at realistic scan speeds, and show that the idealised PSFs fail to capture temporal distortions in the scan direction.","While simulating PSFs for these situations would be quite challenging, we show that measuring PSFs with similar scan parameters works well.","Our approach clarifies the basic principles of and inherent challenges to SKPM measurements, and gives practical methods to improve results."],"url":"http://arxiv.org/abs/2404.03593v1","category":"physics.app-ph"}
{"created":"2024-04-04 16:59:13","title":"Wilkins: HPC In Situ Workflows Made Easy","abstract":"In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at simulation time. Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks. In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks. Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism. Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications. We demonstrate the above features using both synthetic benchmarks and two science use cases in materials science and cosmology.","sentences":["In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at simulation time.","Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks.","In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks.","Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism.","Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications.","We demonstrate the above features using both synthetic benchmarks and two science use cases in materials science and cosmology."],"url":"http://arxiv.org/abs/2404.03591v1","category":"cs.DC"}
{"created":"2024-04-04 16:58:02","title":"Homotopy types of diagrams of chain complexes","abstract":"We study the homotopy theory of diagrams of chain complexes over a field indexed by a finite poset, and show that it can be completely described in terms of appropriate diagrams of graded vector spaces.","sentences":["We study the homotopy theory of diagrams of chain complexes over a field indexed by a finite poset, and show that it can be completely described in terms of appropriate diagrams of graded vector spaces."],"url":"http://arxiv.org/abs/2404.03589v1","category":"math.AT"}
{"created":"2024-04-04 16:42:41","title":"The CCube reconstruction algorithm for the SoLid experiment","abstract":"The SoLid experiment is a very-short-baseline experiment aimed at searching for nuclear reactor-produced active to sterile antineutrino oscillations. The detection principle is based on the pairing of two types of solid scintillators: polyvinyl toluene and $^6$LiF:ZnS(Ag), which is a new technology used in this field of Physics. In addition to good neutron-gamma discrimination, this setup allows the detector to be highly segmented (the basic detection unit is a 5~cm side cube). High segmentation provides numerous advantages, including the precise location of Inverse Beta Decay (IBD) products, the derivation of the considerate antineutrino energy estimator, and a powerful background reduction tool based on the topological signature of the signal. Finally, the system is read out by a network of wavelength-shifting fibres coupled to a photodetector (MPPC). This paper describes the design of the reconstruction algorithm that allows maximum use of the granularity of the detector. The goal of the algorithm is to convert the output of the optical-fibre readout to the list of the detection units from which it originated. This paper provides a performance comparison for three methods and concludes with a choice of the baseline approach for the experiment.","sentences":["The SoLid experiment is a very-short-baseline experiment aimed at searching for nuclear reactor-produced active to sterile antineutrino oscillations.","The detection principle is based on the pairing of two types of solid scintillators: polyvinyl toluene and $^6$LiF:ZnS(Ag), which is a new technology used in this field of Physics.","In addition to good neutron-gamma discrimination, this setup allows the detector to be highly segmented (the basic detection unit is a 5~cm side cube).","High segmentation provides numerous advantages, including the precise location of Inverse Beta Decay (IBD) products, the derivation of the considerate antineutrino energy estimator, and a powerful background reduction tool based on the topological signature of the signal.","Finally, the system is read out by a network of wavelength-shifting fibres coupled to a photodetector (MPPC).","This paper describes the design of the reconstruction algorithm that allows maximum use of the granularity of the detector.","The goal of the algorithm is to convert the output of the optical-fibre readout to the list of the detection units from which it originated.","This paper provides a performance comparison for three methods and concludes with a choice of the baseline approach for the experiment."],"url":"http://arxiv.org/abs/2404.03580v1","category":"physics.ins-det"}
{"created":"2024-04-04 16:40:11","title":"Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models","abstract":"Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning. To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering. KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions. (2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question. (3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions. We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT .","sentences":["Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters.","However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters.","This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory.","While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge.","Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning.","To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering.","KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions.","(2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question.","(3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions.","We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances.","Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT ."],"url":"http://arxiv.org/abs/2404.03577v1","category":"cs.CL"}
{"created":"2024-04-04 16:37:42","title":"Terrain Point Cloud Inpainting via Signal Decomposition","abstract":"The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains. However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data. Inpainting algorithms are widely used to patch these holes. However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined. On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling. Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds. This representation can help to repair the holes without clear boundaries. Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively. In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem. By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details. The experimental results also demonstrate the effectiveness of our method.","sentences":["The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains.","However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data.","Inpainting algorithms are widely used to patch these holes.","However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined.","On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling.","Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds.","This representation can help to repair the holes without clear boundaries.","Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively.","In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem.","By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details.","The experimental results also demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.03572v1","category":"cs.CV"}
{"created":"2024-04-04 16:30:20","title":"Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity","abstract":"We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.","sentences":["We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace.","Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping.","With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity.","We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks.","These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.","Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities.","One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies."],"url":"http://arxiv.org/abs/2404.03570v1","category":"cs.RO"}
{"created":"2024-04-04 16:15:27","title":"A characterization of zero entropy loosely Bernoulli flows via FK-pseudometric","abstract":"We introduce the Feldman-Katok pseudometric (FK-pseudometric for short) for flows. We then provide a characterization of zero entropy loosely Bernoulli measures for continuous flows via the FK-pseudometric extending the result known for discrete-time dynamical systems. We also provide a purely topological characterization of uniquely ergodic continuous flows whose unique invariant measure is zero entropy loosely Bernoulli.","sentences":["We introduce the Feldman-Katok pseudometric (FK-pseudometric for short) for flows.","We then provide a characterization of zero entropy loosely Bernoulli measures for continuous flows via the FK-pseudometric extending the result known for discrete-time dynamical systems.","We also provide a purely topological characterization of uniquely ergodic continuous flows whose unique invariant measure is zero entropy loosely Bernoulli."],"url":"http://arxiv.org/abs/2404.03559v1","category":"math.DS"}
{"created":"2024-04-04 16:10:45","title":"Signal-preserving CMB component separation with machine learning","abstract":"Analysis of microwave sky signals, such as the cosmic microwave background, often requires component separation with multi-frequency methods, where different signals are isolated by their frequency behaviors. Many so-called \"blind\" methods, such as the internal linear combination (ILC), make minimal assumptions about the spatial distribution of the signal or contaminants, and only assume knowledge of the frequency dependence of the signal. The ILC is a minimum-variance linear combination of the measured frequency maps. In the case of Gaussian, statistically isotropic fields, this is the optimal linear combination, as the variance is the only statistic of interest. However, in many cases the signal we wish to isolate, or the foregrounds we wish to remove, are non-Gaussian and/or statistically anisotropic (in particular for Galactic foregrounds). In such cases, it is possible that machine learning (ML) techniques can be used to exploit the non-Gaussian features of the foregrounds and thereby improve component separation. However, many ML techniques require the use of complex, difficult-to-interpret operations on the data. We propose a hybrid method whereby we train an ML model using only combinations of the data that $\\textit{do not contain the signal}$, and combine the resulting ML-predicted foreground estimate with the ILC solution to reduce the error from the ILC. We demonstrate our methods on simulations of extragalactic temperature and Galactic polarization foregrounds, and show that our ML model can exploit non-Gaussian features, such as point sources and spatially-varying spectral indices, to produce lower-variance maps than ILC - eg, reducing the variance of the B-mode residual by factors of up to 5 - while preserving the signal of interest in an unbiased manner. Moreover, we often find improved performance when applying our model to foreground models on which it was not trained.","sentences":["Analysis of microwave sky signals, such as the cosmic microwave background, often requires component separation with multi-frequency methods, where different signals are isolated by their frequency behaviors.","Many so-called \"blind\" methods, such as the internal linear combination (ILC), make minimal assumptions about the spatial distribution of the signal or contaminants, and only assume knowledge of the frequency dependence of the signal.","The ILC is a minimum-variance linear combination of the measured frequency maps.","In the case of Gaussian, statistically isotropic fields, this is the optimal linear combination, as the variance is the only statistic of interest.","However, in many cases the signal we wish to isolate, or the foregrounds we wish to remove, are non-Gaussian and/or statistically anisotropic (in particular for Galactic foregrounds).","In such cases, it is possible that machine learning (ML) techniques can be used to exploit the non-Gaussian features of the foregrounds and thereby improve component separation.","However, many ML techniques require the use of complex, difficult-to-interpret operations on the data.","We propose a hybrid method whereby we train an ML model using only combinations of the data that $\\textit{do not contain the signal}$, and combine the resulting ML-predicted foreground estimate with the ILC solution to reduce the error from the ILC.","We demonstrate our methods on simulations of extragalactic temperature and Galactic polarization foregrounds, and show that our ML model can exploit non-Gaussian features, such as point sources and spatially-varying spectral indices, to produce lower-variance maps than ILC - eg, reducing the variance of the B-mode residual by factors of up to 5 - while preserving the signal of interest in an unbiased manner.","Moreover, we often find improved performance when applying our model to foreground models on which it was not trained."],"url":"http://arxiv.org/abs/2404.03557v1","category":"astro-ph.CO"}
{"created":"2024-04-04 16:07:21","title":"Robot Safety Monitoring using Programmable Light Curtains","abstract":"As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical. Currently, laser-based light curtain sensors are widely used in factories for safety monitoring. While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space. Furthermore, they are typically expensive. Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces. Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope. Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots. Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors. We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution.","sentences":["As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical.","Currently, laser-based light curtain sensors are widely used in factories for safety monitoring.","While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space.","Furthermore, they are typically expensive.","Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces.","Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope.","Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots.","Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors.","We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution."],"url":"http://arxiv.org/abs/2404.03556v1","category":"cs.RO"}
{"created":"2024-04-04 15:51:43","title":"Quantum querying based on multicontrolled Toffoli gates for causal Feynman loop configurations and directed acyclic graphs","abstract":"Quantum algorithms are a promising framework for a proper treatment of Feynman loop integrals due to the existence of a manifestly causal representation scenario. Particularly, unfolding causal configurations of multiloop Feynman diagrams is understood as querying \\textit{directed acyclic graph} (DAG) configurations of undirected graphs in graph theory. In this paper we present a quantum algorithm for querying causality of multiloop Feynman diagrams using an ingenious change in the logic of the design of the oracle operator. The construction of the quantum oracle is surprisingly based exclusively on multicontrolled Toffoli gates and XNOT gates. The efficiency of the algorithm is evaluated performing a comparison with a quantum algorithm based on binary clauses. Additionally, we explicitly analise several three-, four- and five-eloop topologies, which have not been previously explored due to their higher complexity.","sentences":["Quantum algorithms are a promising framework for a proper treatment of Feynman loop integrals due to the existence of a manifestly causal representation scenario.","Particularly, unfolding causal configurations of multiloop Feynman diagrams is understood as querying \\textit{directed acyclic graph} (DAG) configurations of undirected graphs in graph theory.","In this paper we present a quantum algorithm for querying causality of multiloop Feynman diagrams using an ingenious change in the logic of the design of the oracle operator.","The construction of the quantum oracle is surprisingly based exclusively on multicontrolled Toffoli gates and XNOT gates.","The efficiency of the algorithm is evaluated performing a comparison with a quantum algorithm based on binary clauses.","Additionally, we explicitly analise several three-, four- and five-eloop topologies, which have not been previously explored due to their higher complexity."],"url":"http://arxiv.org/abs/2404.03544v1","category":"quant-ph"}
{"created":"2024-04-04 15:47:51","title":"A swimming bacterium in a two-fluid model of a polymer solution","abstract":"We analyse the motion of a flagellated bacterium in a two-fluid medium using slender body theory. The two-fluid model is useful for describing a body moving through a complex fluid with a microstructure whose length scale is comparable to the characteristic scale of the body. This is true for bacterial motion in biological fluids (entangled polymer solutions), where the entanglement results in a porous microstructure with typical pore diameters comparable to or larger than the flagellar bundle diameter but smaller than the diameter of the bacterial head. Thus the polymer and solvent satisfy different boundary conditions on the flagellar bundle and move with different velocities close to it. This gives rise to a screening length $L_B$ within which the fluids exchange momentum and the relative velocity between the two fluids decays. In this work, both the solvent and polymer of the two-fluid medium are modeled as Newtonian fluids with different viscosities $\\mu_s$ and $\\mu_p$ (viscosity ratio $\\lambda = \\mu_p/\\mu_s$), thereby capturing the effects solely introduced by the microstructure of the complex fluid. From our calculations, we observe an increased drag anisotropy for a rigid, slender flagellar bundle moving through this two-fluid medium, resulting in an enhanced swimming velocity of the organism. The results are sensitive to the interaction between the bundle and the polymer and we discuss two physical scenarios corresponding to two types of interaction. Our model provides an explanation for the experimentally observed enhancement of swimming velocity of bacteria in entangled polymer solutions and motivates further experimental investigations.","sentences":["We analyse the motion of a flagellated bacterium in a two-fluid medium using slender body theory.","The two-fluid model is useful for describing a body moving through a complex fluid with a microstructure whose length scale is comparable to the characteristic scale of the body.","This is true for bacterial motion in biological fluids (entangled polymer solutions), where the entanglement results in a porous microstructure with typical pore diameters comparable to or larger than the flagellar bundle diameter but smaller than the diameter of the bacterial head.","Thus the polymer and solvent satisfy different boundary conditions on the flagellar bundle and move with different velocities close to it.","This gives rise to a screening length $L_B$ within which the fluids exchange momentum and the relative velocity between the two fluids decays.","In this work, both the solvent and polymer of the two-fluid medium are modeled as Newtonian fluids with different viscosities $\\mu_s$ and $\\mu_p$ (viscosity ratio $\\lambda = \\mu_p/\\mu_s$), thereby capturing the effects solely introduced by the microstructure of the complex fluid.","From our calculations, we observe an increased drag anisotropy for a rigid, slender flagellar bundle moving through this two-fluid medium, resulting in an enhanced swimming velocity of the organism.","The results are sensitive to the interaction between the bundle and the polymer and we discuss two physical scenarios corresponding to two types of interaction.","Our model provides an explanation for the experimentally observed enhancement of swimming velocity of bacteria in entangled polymer solutions and motivates further experimental investigations."],"url":"http://arxiv.org/abs/2404.03540v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 15:35:43","title":"COMO: Compact Mapping and Odometry","abstract":"We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points. Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points. The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference. To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames. Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry.","sentences":["We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points.","Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points.","The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference.","To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames.","Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry."],"url":"http://arxiv.org/abs/2404.03531v1","category":"cs.CV"}
{"created":"2024-04-04 15:32:34","title":"Operator growth and spread complexity in open quantum systems","abstract":"Commonly, the notion of \"quantum chaos'' refers to the fast scrambling of information throughout complex quantum systems undergoing unitary evolution. Motivated by the Krylov complexity and the operator growth hypothesis, we demonstrate that the entropy of the population distribution for an operator in time is a useful way to capture the complexity of the internal information dynamics of a system when subject to an environment and is, in principle, agnostic to the specific choice of operator basis. We demonstrate its effectiveness for the Sachdev-Ye-Kitaev (SYK) model, examining the dynamics of the system in both its Krylov basis and the basis of operator strings. We prove that the former basis minimises spread complexity while the latter is an eigenbasis for high dissipation. In both cases, we probe the long-time dynamics of the model and the phenomenological effects of decoherence on the complexity of the dynamics.","sentences":["Commonly, the notion of \"quantum chaos'' refers to the fast scrambling of information throughout complex quantum systems undergoing unitary evolution.","Motivated by the Krylov complexity and the operator growth hypothesis, we demonstrate that the entropy of the population distribution for an operator in time is a useful way to capture the complexity of the internal information dynamics of a system when subject to an environment and is, in principle, agnostic to the specific choice of operator basis.","We demonstrate its effectiveness for the Sachdev-Ye-Kitaev (SYK) model, examining the dynamics of the system in both its Krylov basis and the basis of operator strings.","We prove that the former basis minimises spread complexity while the latter is an eigenbasis for high dissipation.","In both cases, we probe the long-time dynamics of the model and the phenomenological effects of decoherence on the complexity of the dynamics."],"url":"http://arxiv.org/abs/2404.03529v1","category":"quant-ph"}
{"created":"2024-04-04 15:21:40","title":"Model Checking Recursive Probabilistic Programs with Conditioning","abstract":"We address the problem of model checking temporal logic specifications for probabilistic programs with recursive procedures, nested queries, and conditioning expressed with observe statements. We introduce probabilistic Operator Precedence Automata (pOPA), a new class of probabilistic pushdown automata suitable to model constructs and behaviors of probabilistic programs. We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\\mathcal{X}$) on a pOPA in single EXPTIME. POTL$^f_\\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements. We provide the first probabilistic model checking implementation of context-free language properties for probabilistic pushdown systems.","sentences":["We address the problem of model checking temporal logic specifications for probabilistic programs with recursive procedures, nested queries, and conditioning expressed with observe statements.","We introduce probabilistic Operator Precedence Automata (pOPA), a new class of probabilistic pushdown automata suitable to model constructs and behaviors of probabilistic programs.","We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\\mathcal{X}$) on a pOPA in single EXPTIME.","POTL$^f_\\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements.","We provide the first probabilistic model checking implementation of context-free language properties for probabilistic pushdown systems."],"url":"http://arxiv.org/abs/2404.03515v1","category":"cs.LO"}
{"created":"2024-04-04 15:14:49","title":"Materials for High Temperature Digital Electronics","abstract":"Silicon microelectronics, consisting of complementary metal oxide semiconductor (CMOS) technology, have changed nearly all aspects of human life from communication to transportation, entertainment, and healthcare. Despite the widespread and mainstream use, current silicon-based devices suffer significant reliability issues at temperatures exceeding 125 {\\deg}C. The emergent technological frontiers of space exploration, geothermal energy harvesting, nuclear energy, unmanned avionic systems, and autonomous driving will rely on control systems, sensors, and communication devices which operate at temperatures as high as 500 {\\deg}C and beyond. At these extreme temperatures, active (heat exchanger, phase change cooling) or passive (fins and thermal interface materials) cooling strategies add significant mass and complication which is often infeasible. Thus, new material solutions beyond conventional silicon CMOS devices are necessary for high temperature, resilient electronic systems. Accomplishing this will require a united effort to explore development, integration, and ultimately manufacturing of non-silicon-based logic and memory technologies, non-traditional metals for interconnects, and ceramic packaging technology.","sentences":["Silicon microelectronics, consisting of complementary metal oxide semiconductor (CMOS) technology, have changed nearly all aspects of human life from communication to transportation, entertainment, and healthcare.","Despite the widespread and mainstream use, current silicon-based devices suffer significant reliability issues at temperatures exceeding 125 {\\deg}C.","The emergent technological frontiers of space exploration, geothermal energy harvesting, nuclear energy, unmanned avionic systems, and autonomous driving will rely on control systems, sensors, and communication devices which operate at temperatures as high as 500 {\\deg}C and beyond.","At these extreme temperatures, active (heat exchanger, phase change cooling) or passive (fins and thermal interface materials) cooling strategies add significant mass and complication which is often infeasible.","Thus, new material solutions beyond conventional silicon CMOS devices are necessary for high temperature, resilient electronic systems.","Accomplishing this will require a united effort to explore development, integration, and ultimately manufacturing of non-silicon-based logic and memory technologies, non-traditional metals for interconnects, and ceramic packaging technology."],"url":"http://arxiv.org/abs/2404.03510v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 15:09:25","title":"Entanglement Degradation in the Presence of Markovian Noise: a Statistical Analysis","abstract":"Adopting a statistical approach we study the degradation of entanglement of a quantum system under the action of an ensemble of randomly distributed Markovian noise. This enables us to address scenarios where only limited information is available on the mechanisms that rule the noisy evolution of the model. As an application, we characterize the statistic of entanglement deterioration for a quantum memory formed by $n$ qudits that undergo randomly distributed local, uniform, Markovian noise evolution.","sentences":["Adopting a statistical approach we study the degradation of entanglement of a quantum system under the action of an ensemble of randomly distributed Markovian noise.","This enables us to address scenarios where only limited information is available on the mechanisms that rule the noisy evolution of the model.","As an application, we characterize the statistic of entanglement deterioration for a quantum memory formed by $n$ qudits that undergo randomly distributed local, uniform, Markovian noise evolution."],"url":"http://arxiv.org/abs/2404.03505v1","category":"quant-ph"}
{"created":"2024-04-04 14:58:16","title":"Total positivity and two inequalities by Athanasiadis and Tzanaki","abstract":"Let $\\Delta$ be a (d-1)-dimensional simplicial complex and h^\\Delta = (h_0^ ,.. , h_d) its h-vector. For a face uniform subdivision operation \\F we write \\Delta_\\F for the subdivided complex and H_\\F for the matrix such that h^{\\Delta_\\F} = H_\\F h^\\Delta.   In connection with the real rootedness of symmetric decompositions Athanasiadis and Tzanaki studied for strictly positive h-vectors the inequalities   h_0 / h_1 \\leq h_1 / h_{d-1} \\leq .... \\leq h_d / h_0 and h_1 / h_{d-1} \\geq ... \\geq h_{d-2} / h_2 \\geq h_{d-1} / h_1.   In this paper we show that if the inequalities holds for a simplicial complex $\\Delta$ and H_\\F is TP_2 (all entries and two minors are non-negative) then the inequalities hold for \\Delta_\\F.   We prove that if \\F is the barycentric subdivision then H_\\F is TP_2. If \\F is the rth-edgewise subdivision then work of Diaconis and Fulman shows H_\\F is TP_2. Indeed in this case by work of Mao and Wang H_\\F is even TP.","sentences":["Let $\\Delta$ be a (d-1)-dimensional simplicial complex and h^\\Delta = (h_0^ ,.. , h_d) its h-vector.","For a face uniform subdivision operation \\F we write \\Delta_\\F for the subdivided complex and H_\\F for the matrix such that h^{\\Delta_\\F} = H_\\F h^\\Delta.   ","In connection with the real rootedness of symmetric decompositions Athanasiadis and Tzanaki studied for strictly positive h-vectors the inequalities   h_0 / h_1 \\leq h_1 / h_{d-1} \\leq ....","\\leq h_d / h_0 and h_1 / h_{d-1} \\geq ...","\\geq h_{d-2} / h_2 \\geq h_{d-1} / h_1.   ","In this paper we show that if the inequalities holds for a simplicial complex $\\Delta$ and H_\\F is TP_2","(all entries and two minors are non-negative) then the inequalities hold for \\Delta_\\F.   We prove that if \\F is the barycentric subdivision then H_\\F is TP_2.","If \\F is the rth-edgewise subdivision then work of Diaconis and Fulman shows H_\\F is TP_2.","Indeed in this case by work of Mao and Wang H_\\F is even TP."],"url":"http://arxiv.org/abs/2404.03500v1","category":"math.CO"}
{"created":"2024-04-04 14:56:41","title":"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work","abstract":"In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.","sentences":["In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement.","This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants.","This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots.","By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting.","Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs.","The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry."],"url":"http://arxiv.org/abs/2404.03498v1","category":"cs.RO"}
{"created":"2024-04-04 14:43:43","title":"Design of Stickbug: a Six-Armed Precision Pollination Robot","abstract":"This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses. Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability. Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity. Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination. Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate. Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files.","sentences":["This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses.","Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability.","Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity.","Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination.","Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate.","Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files."],"url":"http://arxiv.org/abs/2404.03489v1","category":"cs.RO"}
{"created":"2024-04-04 14:42:04","title":"Electronic transport, metal-insulator transition, and Wigner crystallization in transition metal dichalcogenide monolayers","abstract":"Two recent electronic transport experiments from Columbia University and Harvard University have reported record high mobility and low channel densities in transition metal dichalcogenide (TMD) WSe$_2$ monolayers [J. Pack, et al., arXiv:2310.19782; A. Y. Joe, et al., Phys. Rev. Lett. 132, 056303 (2024)]. A two-dimensional (2D) metal-insulator transition (MIT) is demonstrated in the Columbia sample at low densities, a regime where the formation of a Wigner crystal (WC) is theoretically anticipated in the absence of disorder. We employ the finite-temperature Boltzmann theory to understand the low-temperature transport properties of monolayer TMDs, taking into account realistic disorder scattering. We analyze the experimental results, focusing on the 2D MIT behavior and the influence of temperature and density on mobility and resistivity in the metallic phase. We provide a discussion of the nontrivial carrier density dependence of our transport results. Our analysis elucidates the linear-in-$T$ resistivity in the metallic phase, attributing it to Friedel oscillations associated with screened charged impurities. Furthermore, we explore whether Coulomb disorder could lead to the MIT through either a quantum Anderson localization transition or a classical percolation transition. Our theoretical estimates of the disorder-induced MIT critical densities, although smaller, are within a factor of ~2 of the experimental critical density. We examine the exceptionally high melting temperature ~10 K of WCs observed experimentally in the MoSe$_2$ systems at low density, an order of magnitude larger than the pristine melting temperature. This suggests that the observed 2D low-density MIT behavior is likely a result of the complex interplay between disorder effects and interaction-driven WC physics, offering a comprehensive understanding of the low-temperature transport phenomena in TMD monolayers.","sentences":["Two recent electronic transport experiments from Columbia University and Harvard University have reported record high mobility and low channel densities in transition metal dichalcogenide (TMD) WSe$_2$ monolayers [J. Pack, et al., arXiv:2310.19782; A. Y. Joe, et al., Phys.","Rev. Lett.","132, 056303 (2024)].","A two-dimensional (2D) metal-insulator transition (MIT) is demonstrated in the Columbia sample at low densities, a regime where the formation of a Wigner crystal (WC) is theoretically anticipated in the absence of disorder.","We employ the finite-temperature Boltzmann theory to understand the low-temperature transport properties of monolayer TMDs, taking into account realistic disorder scattering.","We analyze the experimental results, focusing on the 2D MIT behavior and the influence of temperature and density on mobility and resistivity in the metallic phase.","We provide a discussion of the nontrivial carrier density dependence of our transport results.","Our analysis elucidates the linear-in-$T$ resistivity in the metallic phase, attributing it to Friedel oscillations associated with screened charged impurities.","Furthermore, we explore whether Coulomb disorder could lead to the MIT through either a quantum Anderson localization transition or a classical percolation transition.","Our theoretical estimates of the disorder-induced MIT critical densities, although smaller, are within a factor of ~2 of the experimental critical density.","We examine the exceptionally high melting temperature ~10 K of WCs observed experimentally in the MoSe$_2$ systems at low density, an order of magnitude larger than the pristine melting temperature.","This suggests that the observed 2D low-density MIT behavior is likely a result of the complex interplay between disorder effects and interaction-driven WC physics, offering a comprehensive understanding of the low-temperature transport phenomena in TMD monolayers."],"url":"http://arxiv.org/abs/2404.03488v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 14:40:55","title":"Explicit Witt basis over the tensor product of Clifford algebras and octonions","abstract":"In this article, we investigate how the Witt basis serves as a link between real and complex variables in higher-dimensional spaces. Our focus is on the detailed construction of the Witt basis within the tensor product space combining Clifford algebra and multiple octonionic spaces. This construction effectively introduces complex coordinates. The technique is based on a specific subgroup of octonionic automorphisms, distinguished by binary codes. This method allows us to perform a Hermitian analysis of the complex structures within the tensor product space.","sentences":["In this article, we investigate how the Witt basis serves as a link between real and complex variables in higher-dimensional spaces.","Our focus is on the detailed construction of the Witt basis within the tensor product space combining Clifford algebra and multiple octonionic spaces.","This construction effectively introduces complex coordinates.","The technique is based on a specific subgroup of octonionic automorphisms, distinguished by binary codes.","This method allows us to perform a Hermitian analysis of the complex structures within the tensor product space."],"url":"http://arxiv.org/abs/2404.03487v1","category":"math.CV"}
{"created":"2024-04-04 14:37:06","title":"Coupled harmonics due to time-modulated point scatterers","abstract":"We consider the resonance and scattering properties of a composite medium containing scatterers whose properties are modulated in time. When excited with an incident wave of a single frequency, the scattered field consists of a family of coupled harmonics at frequencies differing by the frequency of temporal modulation. Similarly, the temporal modulation induces coupling between the resonance frequencies, leading to exceptional points at certain modulation amplitudes. Moreover, the lack of energy conservation causes scattering coefficients to blow up when (complex) resonances cross the real axis. We have developed an integral operator approach to characterize the scattering problem and, for high-contrast scatterers, we present small-volume asymptotic formulas analogous to the classical results for the static (unmodulated) case. We conclude the paper with a boundary integral formulation of the time-modulated problem, which gives an efficient numerical approach and corroborates the asymptotic formulas.","sentences":["We consider the resonance and scattering properties of a composite medium containing scatterers whose properties are modulated in time.","When excited with an incident wave of a single frequency, the scattered field consists of a family of coupled harmonics at frequencies differing by the frequency of temporal modulation.","Similarly, the temporal modulation induces coupling between the resonance frequencies, leading to exceptional points at certain modulation amplitudes.","Moreover, the lack of energy conservation causes scattering coefficients to blow up when (complex) resonances cross the real axis.","We have developed an integral operator approach to characterize the scattering problem and, for high-contrast scatterers, we present small-volume asymptotic formulas analogous to the classical results for the static (unmodulated) case.","We conclude the paper with a boundary integral formulation of the time-modulated problem, which gives an efficient numerical approach and corroborates the asymptotic formulas."],"url":"http://arxiv.org/abs/2404.03483v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 14:33:37","title":"Evolutionary theory of convective organization","abstract":"Observed patterns of convective cloud would be extremely improbable from random walks in an abstract space of configurations. Forcing is sometimes the driver, but complexity can also develop spontaneously. Here pattern evolution is considered as a natural selection process in a strategic game among configurations, akin to ecological succession. Information (entropy) quantifies improbability, interpreted as Darwinian fitness to the extent larger-scale forcings are properly accounted. Reconciling inferred or revealed fitness with energetics could make convection a showcase of evolutionary theory, simply as reinterpretation of spectral kinetic energy (KE) budgets for dry convection. For moist convection, the flux of a total energy E, the teleological reason for convection, is conjectured to be the central resource of an evolutionary game, with water playing a constraining role like nutrients in ecology. Shannon information H is reviewed in context of this evolutionary reasoning. Analysis of a new Cloud Botany shallow convection simulation set shows growth of H over tens of hours (perhaps more pertinently, it grows with KE throughput or cumulative buoyancy flux). Anisotropy and precipitation boost H in qualitatively distinct and contingent ways, like separate species or ecological guilds. Deep convection over lowland South America also shows many-hour evolution times, beyond simple convective adjustment notions. If evolving horizontal patterns contain or imply information about vertical density profiles, they could become a new resource in data assimilation to predict larger scale flow.","sentences":["Observed patterns of convective cloud would be extremely improbable from random walks in an abstract space of configurations.","Forcing is sometimes the driver, but complexity can also develop spontaneously.","Here pattern evolution is considered as a natural selection process in a strategic game among configurations, akin to ecological succession.","Information (entropy) quantifies improbability, interpreted as Darwinian fitness to the extent larger-scale forcings are properly accounted.","Reconciling inferred or revealed fitness with energetics could make convection a showcase of evolutionary theory, simply as reinterpretation of spectral kinetic energy (KE) budgets for dry convection.","For moist convection, the flux of a total energy E, the teleological reason for convection, is conjectured to be the central resource of an evolutionary game, with water playing a constraining role like nutrients in ecology.","Shannon information H is reviewed in context of this evolutionary reasoning.","Analysis of a new Cloud Botany shallow convection simulation set shows growth of H over tens of hours (perhaps more pertinently, it grows with KE throughput or cumulative buoyancy flux).","Anisotropy and precipitation boost H in qualitatively distinct and contingent ways, like separate species or ecological guilds.","Deep convection over lowland South America also shows many-hour evolution times, beyond simple convective adjustment notions.","If evolving horizontal patterns contain or imply information about vertical density profiles, they could become a new resource in data assimilation to predict larger scale flow."],"url":"http://arxiv.org/abs/2404.03480v1","category":"nlin.AO"}
{"created":"2024-04-04 14:23:42","title":"Measurable Structure Factors of Dense Dispersions Containing Polydisperse, Optically Inhomogeneous Particles","abstract":"We exemplarily investigate how optical properties of single scatterers in interacting multi-particle systems influence measurable structure factors. Both particles with linear gradients of their scattering length density and core-shell structures evoke characteristic deviations between the weighted sum $\\langle S(Q)\\rangle$ of partial structure factors in a multicomponent system and experimentally accessible, measurable structure factors $S_{\\mathrm{M}}(Q)$. While $\\langle S(Q)\\rangle$ contains only structural information of self-organising systems, $S_{\\mathrm{M}}(Q)$ additionally is influenced by optical properties of their constituents resulting in features such as changing amplitudes, additional peaks in the low wavevector region or splitting of higher-order maxima which are not related to structural reasons. Hence, a careful data analysis regarding size-distribution and optical properties of single scatters is mandatory to avoid a misinterpretation of measurable structure factors.","sentences":["We exemplarily investigate how optical properties of single scatterers in interacting multi-particle systems influence measurable structure factors.","Both particles with linear gradients of their scattering length density and core-shell structures evoke characteristic deviations between the weighted sum $\\langle S(Q)\\rangle$ of partial structure factors in a multicomponent system and experimentally accessible, measurable structure factors $S_{\\mathrm{M}}(Q)$. While $\\langle S(Q)\\rangle$ contains only structural information of self-organising systems, $S_{\\mathrm{M}}(Q)$ additionally is influenced by optical properties of their constituents resulting in features such as changing amplitudes, additional peaks in the low wavevector region or splitting of higher-order maxima which are not related to structural reasons.","Hence, a careful data analysis regarding size-distribution and optical properties of single scatters is mandatory to avoid a misinterpretation of measurable structure factors."],"url":"http://arxiv.org/abs/2404.03470v1","category":"cond-mat.soft"}
{"created":"2024-04-04 14:23:12","title":"Class-E, Active Electrically-Small Antenna for High-Power Wideband Transmission at the High-Frequency (HF) Band","abstract":"Antennas operating at the high-frequency (HF) band (3-30 MHz) are frequently electrically small due to the large wavelength of electromagnetic waves (10-100 m). However, the bandwidth-efficiency products of passively matched electrically small antennas (ESAs) are fundamentally limited. Wideband HF waveforms using bandwidths of 24 kHz or more have recently received significant attention in military communications applications. Efficiently radiating such signals from conventional passive ESAs is very challenging due to fundamental physical limits on bandwidth-efficiency products of ESAs. However, active antennas are not subject to the same constraints. In this work, we present the design and experimental characterization of a high-power, active ESA with enhanced bandwidth-efficiency product compared to {that of} passively matched ESAs. Specifically, the proposed active ESA can radiate wideband HF signals with banwidths of 24 kHz or more, with total efficiencies up to 80$\\%$, and radiated power levels approaching 100 W. Our approach uses a highly-efficient, integrated class-E switching circuit specifically designed to drive an electrically small, high-Q HF antenna over a bandwidth exceeding 24 kHz. Using a high-Q RLC antenna model, we have successfully demonstrated wideband binary ASK, PSK, and FSK modulations with the proposed class-E switching architecture. Experimental results indicate that the bandwidth-efficiency product of this class-E active antenna is 5.4-9.8 dB higher than that of an equivalent passive design with the same data rate, and bit-error-rate (BER).","sentences":["Antennas operating at the high-frequency (HF) band (3-30 MHz) are frequently electrically small due to the large wavelength of electromagnetic waves (10-100 m).","However, the bandwidth-efficiency products of passively matched electrically small antennas (ESAs) are fundamentally limited.","Wideband HF waveforms using bandwidths of 24 kHz or more have recently received significant attention in military communications applications.","Efficiently radiating such signals from conventional passive ESAs is very challenging due to fundamental physical limits on bandwidth-efficiency products of ESAs.","However, active antennas are not subject to the same constraints.","In this work, we present the design and experimental characterization of a high-power, active ESA with enhanced bandwidth-efficiency product compared to {that of} passively matched ESAs.","Specifically, the proposed active ESA can radiate wideband HF signals with banwidths of 24 kHz or more, with total efficiencies up to 80$\\%$, and radiated power levels approaching 100 W. Our approach uses a highly-efficient, integrated class-E switching circuit specifically designed to drive an electrically small, high-Q HF antenna over a bandwidth exceeding 24 kHz.","Using a high-Q RLC antenna model, we have successfully demonstrated wideband binary ASK, PSK, and FSK modulations with the proposed class-E switching architecture.","Experimental results indicate that the bandwidth-efficiency product of this class-E active antenna is 5.4-9.8 dB higher than that of an equivalent passive design with the same data rate, and bit-error-rate (BER)."],"url":"http://arxiv.org/abs/2404.03468v1","category":"physics.app-ph"}
{"created":"2024-04-04 14:18:23","title":"Patrick Moss 25/10/1947--17/3/2024","abstract":"Patrick Moss (1947--2024) had two distinct lives as a mathematician. The first was as a ring theorist in the late 1970s, in which he worked with Ginn and Lenagan as a student. After a long career as an inspirational mathematics teacher, Patrick completed a doctorate under my supervision in 2003. This led to a second mathematical life in arithmetic dynamics almost forty years after his first period of research. This is a short obituary of his remarkable contributions, several of which have stimulated further research.","sentences":["Patrick Moss (1947--2024) had two distinct lives as a mathematician.","The first was as a ring theorist in the late 1970s, in which he worked with Ginn and Lenagan as a student.","After a long career as an inspirational mathematics teacher, Patrick completed a doctorate under my supervision in 2003.","This led to a second mathematical life in arithmetic dynamics almost forty years after his first period of research.","This is a short obituary of his remarkable contributions, several of which have stimulated further research."],"url":"http://arxiv.org/abs/2404.03464v1","category":"math.HO"}
{"created":"2024-04-04 14:01:47","title":"Charting the Complex Structure Landscape of F-theory","abstract":"We explore the landscape of F-theory compactifications on Calabi--Yau fourfolds whose complex structure moduli space is the thrice-punctured sphere. As a first part, we enumerate all such Calabi--Yau fourfolds under the additional requirement that it has a large complex structure and conifold point at two of the punctures. We find 14 monodromy tuples by demanding the monodromy around infinity to be quasi-unipotent. As second part, we study the four different types of phases arising at infinity. For each we consider a working example where we determine the leading periods and other physical couplings. We also included a notebook that sets up the period vectors for any of these models.","sentences":["We explore the landscape of F-theory compactifications on Calabi--Yau fourfolds whose complex structure moduli space is the thrice-punctured sphere.","As a first part, we enumerate all such Calabi--Yau fourfolds under the additional requirement that it has a large complex structure and conifold point at two of the punctures.","We find 14 monodromy tuples by demanding the monodromy around infinity to be quasi-unipotent.","As second part, we study the four different types of phases arising at infinity.","For each we consider a working example where we determine the leading periods and other physical couplings.","We also included a notebook that sets up the period vectors for any of these models."],"url":"http://arxiv.org/abs/2404.03456v1","category":"hep-th"}
{"created":"2024-04-04 13:44:41","title":"Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering","abstract":"This paper proposes an algorithm for combined contact detection and state estimation for legged robots. The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground. The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state. The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa. This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot. The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts. The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts.","sentences":["This paper proposes an algorithm for combined contact detection and state estimation for legged robots.","The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground.","The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state.","The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa.","This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot.","The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts.","The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts."],"url":"http://arxiv.org/abs/2404.03444v1","category":"cs.RO"}
{"created":"2024-04-04 13:39:49","title":"Privacy Engineering From Principles to Practice: A Roadmap","abstract":"Privacy engineering is gaining momentum in industry and academia alike. So far, manifold low-level primitives and higher-level methods and strategies have successfully been established. Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice.","sentences":["Privacy engineering is gaining momentum in industry and academia alike.","So far, manifold low-level primitives and higher-level methods and strategies have successfully been established.","Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice."],"url":"http://arxiv.org/abs/2404.03442v1","category":"cs.CR"}
{"created":"2024-04-04 13:27:22","title":"Learning From Simplicial Data Based on Random Walks and 1D Convolutions","abstract":"Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks.","sentences":["Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes.","While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically.","To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships.","Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks.","We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks."],"url":"http://arxiv.org/abs/2404.03434v1","category":"cs.LG"}
{"created":"2024-04-04 13:24:33","title":"MEDIATE: Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange","abstract":"Recent advances in multi-agent systems (MAS) have shown that incorporating peer incentivization (PI) mechanisms vastly improves cooperation. Especially in social dilemmas, communication between the agents helps to overcome sub-optimal Nash equilibria. However, incentivization tokens need to be carefully selected. Furthermore, real-world applications might yield increased privacy requirements and limited exchange. Therefore, we extend the PI protocol for mutual acknowledgment token exchange (MATE) and provide additional analysis on the impact of the chosen tokens. Building upon those insights, we propose mutually endorsed distributed incentive acknowledgment token exchange (MEDIATE), an extended PI architecture employing automatic token derivation via decentralized consensus. Empirical results show the stable agreement on appropriate tokens yielding superior performance compared to static tokens and state-of-the-art approaches in different social dilemma environments with various reward distributions.","sentences":["Recent advances in multi-agent systems (MAS) have shown that incorporating peer incentivization (PI) mechanisms vastly improves cooperation.","Especially in social dilemmas, communication between the agents helps to overcome sub-optimal Nash equilibria.","However, incentivization tokens need to be carefully selected.","Furthermore, real-world applications might yield increased privacy requirements and limited exchange.","Therefore, we extend the PI protocol for mutual acknowledgment token exchange (MATE) and provide additional analysis on the impact of the chosen tokens.","Building upon those insights, we propose mutually endorsed distributed incentive acknowledgment token exchange (MEDIATE), an extended PI architecture employing automatic token derivation via decentralized consensus.","Empirical results show the stable agreement on appropriate tokens yielding superior performance compared to static tokens and state-of-the-art approaches in different social dilemma environments with various reward distributions."],"url":"http://arxiv.org/abs/2404.03431v1","category":"cs.MA"}
{"created":"2024-04-04 13:13:47","title":"GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration","abstract":"State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub.","sentences":["State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants.","These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization.","This often leads to misalignments in the calibration process.","Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations.","This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems.","Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms.","We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment.","We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results.","Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms.","The code is open source and available on GitHub."],"url":"http://arxiv.org/abs/2404.03427v1","category":"cs.RO"}
{"created":"2024-04-04 12:58:46","title":"Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View","abstract":"Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors. However, real-world scenarios are far more complex and exceed the capabilities of these methods. We therefore propose a hybrid method following a divide-and-conquer strategy. We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components. By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image. We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system. This enables the pipeline to naturally improve as future methods can replace the individual modules. We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works. Project page: https://andreeadogaru.github.io/Gen3DSR.","sentences":["Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors.","However, real-world scenarios are far more complex and exceed the capabilities of these methods.","We therefore propose a hybrid method following a divide-and-conquer strategy.","We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components.","By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image.","We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system.","This enables the pipeline to naturally improve as future methods can replace the individual modules.","We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works.","Project page: https://andreeadogaru.github.io/Gen3DSR."],"url":"http://arxiv.org/abs/2404.03421v1","category":"cs.CV"}
{"created":"2024-04-04 12:46:01","title":"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens","abstract":"This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/","sentences":["This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding.","The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos.","Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos.","MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components.","The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively.","Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/"],"url":"http://arxiv.org/abs/2404.03413v1","category":"cs.CV"}
{"created":"2024-04-04 12:45:49","title":"RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling","abstract":"Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems. For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures. Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered. We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair. For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods. We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control. Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques. In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement. Accompanying code and video can be found at https://mit-realm.github.io/radium/","sentences":["Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems.","For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures.","Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered.","We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair.","For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods.","We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control.","Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques.","In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement.","Accompanying code and video can be found at https://mit-realm.github.io/radium/"],"url":"http://arxiv.org/abs/2404.03412v1","category":"cs.RO"}
{"created":"2024-04-04 12:08:28","title":"Curves in the Fourier zeros of polytopal regions and the Pompeiu problem","abstract":"We prove that any finite union $P$ of interior-disjoint polytopes in ${\\mathbb R}^d$ has the Pompeiu property, a result first proved by Williams [Wil76]. This means that if a continuous function $f$ on $R^d$ integrates to 0 on any congruent copy of $P$ then $f$ is identically 0. By a fundamental result of Brown, Schreiber and Taylor [BST73] this is equivalent to showing that the Fourier-Laplace transform of the indicator function of $P$ does not vanish identically on any 0-centered complex sphere in ${\\mathbb C}^d$ . Our proof initially follows the recent one of Machado and Robins [MR23] who are using the Brion-Barvinok formula for the Fourier-Laplace transform of a polytope. But we simplify this method considerably by removing the use of properties of Bessel function zeros. Instead we use some elementary arguments on the growth of linear combinations of exponentials with rational functions as coefficients. Our approach allows us to prove the non-existence of complex spheres of any center in the zero-set of the Fourier-Laplace transform. The planar case is even simpler in that we do not even need the Brion-Barvinok formula. We then go further in the question of which sets can be contained in the null set of the Fourier-Laplace transform of a polytope by extending results of Engel [Eng23] who showed that rationally parametrized hypersurfaces, under some mild conditions, cannot be contained in this null-set. We show that a rationally parametrized curve which is not contained in an affine hyperplane in ${\\mathbb C}^d$ cannot be contained in this null-set. Results about curves parametrized by meromorphic functions are also given.","sentences":["We prove that any finite union $P$ of interior-disjoint polytopes in ${\\mathbb R}^d$ has the Pompeiu property, a result first proved by Williams [Wil76].","This means that if a continuous function $f$ on $R^d$ integrates to 0 on any congruent copy of $P$ then $f$ is identically 0.","By a fundamental result of Brown, Schreiber and Taylor [BST73] this is equivalent to showing that the Fourier-Laplace transform of the indicator function of $P$ does not vanish identically on any 0-centered complex sphere in ${\\mathbb C}^d$ .","Our proof initially follows the recent one of Machado and Robins","[MR23] who are using the Brion-Barvinok formula for the Fourier-Laplace transform of a polytope.","But we simplify this method considerably by removing the use of properties of Bessel function zeros.","Instead we use some elementary arguments on the growth of linear combinations of exponentials with rational functions as coefficients.","Our approach allows us to prove the non-existence of complex spheres of any center in the zero-set of the Fourier-Laplace transform.","The planar case is even simpler in that we do not even need the Brion-Barvinok formula.","We then go further in the question of which sets can be contained in the null set of the Fourier-Laplace transform of a polytope by extending results of Engel [Eng23] who showed that rationally parametrized hypersurfaces, under some mild conditions, cannot be contained in this null-set.","We show that a rationally parametrized curve which is not contained in an affine hyperplane in ${\\mathbb C}^d$ cannot be contained in this null-set.","Results about curves parametrized by meromorphic functions are also given."],"url":"http://arxiv.org/abs/2404.03405v1","category":"math.CA"}
{"created":"2024-04-04 12:03:15","title":"On steady solutions of the Hall-MHD system in Besov spaces","abstract":"In this paper, we investigate the well-posedness and ill-posedness issues for the incompressible stationary Hall-magnetohydrodynamic (Hall-MHD) system in $\\mathbb{R}^3.$ We first show the existence and uniqueness of solutions provided with the forces in $\\dot B^{3/p-3}_{p,r}(\\mathbb{R}^3)$ for $1\\leq p <3$ and $r=1$. Moreover, this result can be extended to any $1\\leq r\\leq \\infty$ whenever $p=2,$ without any additional assumption on the physical parameters. On the other hand, we establish some ill-posedness results for Hall-MHD system by using the discontinuity of the solution mapping of the three-dimensional stationary Navier-Stokes equations in \\emph{critical} function spaces $\\dot{B}^{3/p-1}_{p,r}(\\mathbb{R}^3)$ ($p\\geq 3$).","sentences":["In this paper, we investigate the well-posedness and ill-posedness issues for the incompressible stationary Hall-magnetohydrodynamic (Hall-MHD) system in $\\mathbb{R}^3.$ We first show the existence and uniqueness of solutions provided with the forces in $\\dot B^{3/p-3}_{p,r}(\\mathbb{R}^3)$ for $1\\leq p <3$ and $r=1$. Moreover, this result can be extended to any $1\\leq r\\leq \\infty$ whenever $p=2,$ without any additional assumption on the physical parameters.","On the other hand, we establish some ill-posedness results for Hall-MHD system by using the discontinuity of the solution mapping of the three-dimensional stationary Navier-Stokes equations in \\emph{critical} function spaces $\\dot{B}^{3/p-1}_{p,r}(\\mathbb{R}^3)$ ($p\\geq 3$)."],"url":"http://arxiv.org/abs/2404.03402v1","category":"math.AP"}
{"created":"2024-04-04 12:00:01","title":"An asynchronous discontinuous Galerkin method for massively parallel PDE solvers","abstract":"The discontinuous Galerkin (DG) method is widely being used to solve hyperbolic partial differential equations (PDEs) due to its ability to provide high-order accurate solutions in complex geometries, capture discontinuities, and exhibit high arithmetic intensity. However, the scalability of DG-based solvers is impeded by communication bottlenecks arising from the data movement and synchronization requirements at extreme scales. To address these challenges, recent studies have focused on the development of asynchronous computing approaches for PDE solvers. Herein, we introduce the asynchronous DG (ADG) method, which combines the benefits of the DG method with asynchronous computing to overcome communication bottlenecks. The ADG method relaxes the need for data communication and synchronization at a mathematical level, allowing processing elements to operate independently regardless of the communication status, thus potentially improving the scalability of solvers. The proposed ADG method ensures flux conservation and effectively addresses challenges arising from asynchrony. To assess its stability, Fourier-mode analysis is employed to examine the dissipation and dispersion behavior of fully-discrete equations that use the DG and ADG schemes along with the Runge-Kutta (RK) time integration scheme. Furthermore, an error analysis within a statistical framework is presented, which demonstrates that the ADG method with standard numerical fluxes achieves at most first-order accuracy. To recover accuracy, we introduce asynchrony-tolerant (AT) fluxes that utilize data from multiple time levels. Extensive numerical experiments were conducted to validate the performance of the ADG-AT scheme for both linear and nonlinear problems.","sentences":["The discontinuous Galerkin (DG) method is widely being used to solve hyperbolic partial differential equations (PDEs) due to its ability to provide high-order accurate solutions in complex geometries, capture discontinuities, and exhibit high arithmetic intensity.","However, the scalability of DG-based solvers is impeded by communication bottlenecks arising from the data movement and synchronization requirements at extreme scales.","To address these challenges, recent studies have focused on the development of asynchronous computing approaches for PDE solvers.","Herein, we introduce the asynchronous DG (ADG) method, which combines the benefits of the DG method with asynchronous computing to overcome communication bottlenecks.","The ADG method relaxes the need for data communication and synchronization at a mathematical level, allowing processing elements to operate independently regardless of the communication status, thus potentially improving the scalability of solvers.","The proposed ADG method ensures flux conservation and effectively addresses challenges arising from asynchrony.","To assess its stability, Fourier-mode analysis is employed to examine the dissipation and dispersion behavior of fully-discrete equations that use the DG and ADG schemes along with the Runge-Kutta (RK) time integration scheme.","Furthermore, an error analysis within a statistical framework is presented, which demonstrates that the ADG method with standard numerical fluxes achieves at most first-order accuracy.","To recover accuracy, we introduce asynchrony-tolerant (AT) fluxes that utilize data from multiple time levels.","Extensive numerical experiments were conducted to validate the performance of the ADG-AT scheme for both linear and nonlinear problems."],"url":"http://arxiv.org/abs/2404.03399v1","category":"physics.comp-ph"}
{"created":"2024-04-04 11:57:11","title":"Characterization of the Early Dynamics of Solar Coronal Bright Fronts","abstract":"We present a comprehensive characterization of 26 CME-driven compressive waves known as Coronal Bright Fronts (CBFs) observed in the low solar corona between 2010 and 2017. These CBFs have been found to be associated with SEP events near Earth, indicating their importance in understanding space weather phenomena. The aim of this study is to analyze and describe the early dynamics of CBFs using a physics-based heliospheric SEP forecasting system known as the SPREAdFAST framework. This framework utilizes a chain of data-driven analytic and numerical models to predict SEP fluxes at multiple locations in the inner heliosphere by considering their acceleration at CMEs near the Sun and subsequent interplanetary transport. To estimate the time-dependent plasma and compression parameters of the CBFs, we utilized sequences of base-difference images obtained from the AIA instrument on board the SDO satellite, and measurements of the height-time profiles of the CMEs obtained from the LASCO instrument on board the SOHO satellite. We employed kinematic measurements and plasma model results to derive these parameters. The SPREAdFAST framework facilitated the analysis and correlation of these observations with SEP events near Earth. Our analysis yielded statistical relations and distributions for both the shocks and plasma parameters associated with the 26 CBFs investigated. By combining the observations from the AIA and LASCO instruments, as well as the data products from the SPREAdFAST framework, we obtained a comprehensive understanding of the early dynamics of CBFs, including their temporal evolution, plasma properties, and compressional characteristics. These findings contribute to the growing body of knowledge in the field and have implications for space weather forecasting and the study of SEP events.","sentences":["We present a comprehensive characterization of 26 CME-driven compressive waves known as Coronal Bright Fronts (CBFs) observed in the low solar corona between 2010 and 2017.","These CBFs have been found to be associated with SEP events near Earth, indicating their importance in understanding space weather phenomena.","The aim of this study is to analyze and describe the early dynamics of CBFs using a physics-based heliospheric SEP forecasting system known as the SPREAdFAST framework.","This framework utilizes a chain of data-driven analytic and numerical models to predict SEP fluxes at multiple locations in the inner heliosphere by considering their acceleration at CMEs near the Sun and subsequent interplanetary transport.","To estimate the time-dependent plasma and compression parameters of the CBFs, we utilized sequences of base-difference images obtained from the AIA instrument on board the SDO satellite, and measurements of the height-time profiles of the CMEs obtained from the LASCO instrument on board the SOHO satellite.","We employed kinematic measurements and plasma model results to derive these parameters.","The SPREAdFAST framework facilitated the analysis and correlation of these observations with SEP events near Earth.","Our analysis yielded statistical relations and distributions for both the shocks and plasma parameters associated with the 26 CBFs investigated.","By combining the observations from the AIA and LASCO instruments, as well as the data products from the SPREAdFAST framework, we obtained a comprehensive understanding of the early dynamics of CBFs, including their temporal evolution, plasma properties, and compressional characteristics.","These findings contribute to the growing body of knowledge in the field and have implications for space weather forecasting and the study of SEP events."],"url":"http://arxiv.org/abs/2404.03396v1","category":"astro-ph.SR"}
{"created":"2024-04-04 11:56:51","title":"Movable Antennas-Assisted Secure Transmission Without Eavesdroppers' Instantaneous CSI","abstract":"Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions. In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers. Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel. Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation. Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice. However, the problem is highly non-convex because the objective includes the complex incomplete gamma function. To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model. Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm. Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas.","sentences":["Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions.","In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers.","Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel.","Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation.","Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice.","However, the problem is highly non-convex because the objective includes the complex incomplete gamma function.","To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model.","Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm.","Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.","As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas."],"url":"http://arxiv.org/abs/2404.03395v1","category":"cs.IT"}
{"created":"2024-04-04 11:32:17","title":"A unified Euler--Lagrange system for analyzing continuous-time accelerated gradient methods","abstract":"This paper presents an Euler--Lagrange system for a continuous-time model of the accelerated gradient methods in smooth convex optimization and proposes an associated Lyapunov-function-based convergence analysis framework. Recently, ordinary differential equations (ODEs) with dumping terms have been developed to intuitively interpret the accelerated gradient methods, and the design of unified model describing the various individual ODE models have been examined. In existing reports, the Lagrangian, which results in the Euler-Lagrange equation, and the Lyapunov function for the convergence analysis have been separately proposed for each ODE. This paper proposes a unified Euler--Lagrange system and its Lyapunov function to cover the existing various models. In the convergence analysis using the Lyapunov function, a condition that parameters in the Lagrangian and Lyapunov function must satisfy is derived, and a parameter design for improving the convergence rate naturally results in the mysterious dumping coefficients. Especially, a symmetric Bregman divergence can lead to a relaxed condition of the parameters and a resulting improved convergence rate. As an application of this study, a slight modification in the Lyapunov function establishes the similar convergence proof for ODEs with smooth approximation in nondifferentiable objective function minimization.","sentences":["This paper presents an Euler--Lagrange system for a continuous-time model of the accelerated gradient methods in smooth convex optimization and proposes an associated Lyapunov-function-based convergence analysis framework.","Recently, ordinary differential equations (ODEs) with dumping terms have been developed to intuitively interpret the accelerated gradient methods, and the design of unified model describing the various individual ODE models have been examined.","In existing reports, the Lagrangian, which results in the Euler-Lagrange equation, and the Lyapunov function for the convergence analysis have been separately proposed for each ODE.","This paper proposes a unified Euler--Lagrange system and its Lyapunov function to cover the existing various models.","In the convergence analysis using the Lyapunov function, a condition that parameters in the Lagrangian and Lyapunov function must satisfy is derived, and a parameter design for improving the convergence rate naturally results in the mysterious dumping coefficients.","Especially, a symmetric Bregman divergence can lead to a relaxed condition of the parameters and a resulting improved convergence rate.","As an application of this study, a slight modification in the Lyapunov function establishes the similar convergence proof for ODEs with smooth approximation in nondifferentiable objective function minimization."],"url":"http://arxiv.org/abs/2404.03383v1","category":"math.OC"}
{"created":"2024-04-04 11:14:01","title":"The Nearest Graph Laplacian in Frobenius Norm","abstract":"We address the problem of finding the nearest graph Laplacian to a given matrix, with the distance measured using the Frobenius norm. Specifically, for the directed graph Laplacian, we propose two novel algorithms by reformulating the problem as convex quadratic optimization problems with a special structure: one based on the active set method and the other on direct computation of Karush-Kuhn-Tucker (KKT) points. The proposed algorithms can be applied to system identification and model reduction problems involving Laplacian dynamics. We demonstrate that these algorithms possess lower time complexities and the finite termination property, unlike the interior point method and V-FISTA, the latter of which is an accelerated projected gradient method. Our numerical experiments confirm the effectiveness of the proposed algorithms.","sentences":["We address the problem of finding the nearest graph Laplacian to a given matrix, with the distance measured using the Frobenius norm.","Specifically, for the directed graph Laplacian, we propose two novel algorithms by reformulating the problem as convex quadratic optimization problems with a special structure: one based on the active set method and the other on direct computation of Karush-Kuhn-Tucker (KKT) points.","The proposed algorithms can be applied to system identification and model reduction problems involving Laplacian dynamics.","We demonstrate that these algorithms possess lower time complexities and the finite termination property, unlike the interior point method and V-FISTA, the latter of which is an accelerated projected gradient method.","Our numerical experiments confirm the effectiveness of the proposed algorithms."],"url":"http://arxiv.org/abs/2404.03371v1","category":"math.OC"}
{"created":"2024-04-04 11:09:49","title":"Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity","abstract":"Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states. Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions. This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors. PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as graphs. To take advantage of this relational inductive bias, graph neural networks (GNNs) have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies. In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected. Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy. This approach has not been explored in GNN literature yet because many available PSH graphs are limited to their respective subsystem boundaries, which makes the method unsuitable to be applied directly. In this work, we introduce the application of spectral-temporal graph neural networks, which leverage self-attention mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors. Our method effectively fuses data from the PSH's subsystems by operating on a unified, system-wide graph, learned directly from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability.","sentences":["Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states.","Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions.","This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors.","PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as graphs.","To take advantage of this relational inductive bias, graph neural networks (GNNs) have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies.","In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected.","Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy.","This approach has not been explored in GNN literature yet because many available PSH graphs are limited to their respective subsystem boundaries, which makes the method unsuitable to be applied directly.","In this work, we introduce the application of spectral-temporal graph neural networks, which leverage self-attention mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors.","Our method effectively fuses data from the PSH's subsystems by operating on a unified, system-wide graph, learned directly from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability."],"url":"http://arxiv.org/abs/2404.03368v1","category":"cs.LG"}
{"created":"2024-04-04 11:09:04","title":"Photonic Quantum Computing","abstract":"Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation. Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature. The field is largely divided between discrete- and continuous-variable photonic quantum computation. In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set. Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors. In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position). The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection. Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation. This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning.","sentences":["Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation.","Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature.","The field is largely divided between discrete- and continuous-variable photonic quantum computation.","In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set.","Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors.","In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position).","The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection.","Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation.","This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning."],"url":"http://arxiv.org/abs/2404.03367v1","category":"quant-ph"}
{"created":"2024-04-04 11:04:44","title":"Space Physiology and Technology: Musculoskeletal Adaptations, Countermeasures, and the Opportunity for Wearable Robotics","abstract":"Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth. While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors. A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space. This review focuses on the impact of the environment in space on the musculoskeletal system. It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health. Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative.","sentences":["Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth.","While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors.","A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space.","This review focuses on the impact of the environment in space on the musculoskeletal system.","It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health.","Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative."],"url":"http://arxiv.org/abs/2404.03363v1","category":"cs.RO"}
{"created":"2024-04-04 10:51:51","title":"Implementation of complex-valued sliding mode controllers in three-phase power converters","abstract":"This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters. The paper includes the description of the algorithms and a detailed analysis of the proposed implementations. The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system. The performance of the methods is compared in numerical simulations, and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform.","sentences":["This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters.","The paper includes the description of the algorithms and a detailed analysis of the proposed implementations.","The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system.","The performance of the methods is compared in numerical simulations, and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform."],"url":"http://arxiv.org/abs/2404.03358v1","category":"eess.SY"}
{"created":"2024-04-04 10:49:52","title":"Fast Computation of Robust Dynamic Operating Envelopes Based on Non-convex OPF for Unbalanced Distribution Networks","abstract":"Robust dynamic operating envelopes (RDOEs) solve the problem of secure allocation of latent network capacity to flexible distributed energy resources (DER) in unbalanced distribution networks. As the computational complexity of RDOEs is much higher than that of DOEs, which disregard uncertainties in network parameters and DER capacity utilisation, existing approaches to computing RDOEs have relied on linearised unbalanced three-phase optimal power flow (UTOPF) models to numerate the network feasible region approximately. The use of linearised models, however, risks producing RDOEs that undermine network integrity due to inherent errors in the approximation. This letter presents a practical sensitivity-filtering technique to simplify RDOE numerical computation based on non-convex UTOPF formulations. The accuracy and efficiency of the proposed approach are demonstrated on RDOE allocation with various fairness metrics by testing on representative Australian distribution networks.","sentences":["Robust dynamic operating envelopes (RDOEs) solve the problem of secure allocation of latent network capacity to flexible distributed energy resources (DER) in unbalanced distribution networks.","As the computational complexity of RDOEs is much higher than that of DOEs, which disregard uncertainties in network parameters and DER capacity utilisation, existing approaches to computing RDOEs have relied on linearised unbalanced three-phase optimal power flow (UTOPF) models to numerate the network feasible region approximately.","The use of linearised models, however, risks producing RDOEs that undermine network integrity due to inherent errors in the approximation.","This letter presents a practical sensitivity-filtering technique to simplify RDOE numerical computation based on non-convex UTOPF formulations.","The accuracy and efficiency of the proposed approach are demonstrated on RDOE allocation with various fairness metrics by testing on representative Australian distribution networks."],"url":"http://arxiv.org/abs/2404.03355v1","category":"math.OC"}
{"created":"2024-04-04 10:37:21","title":"Inverse scattering transform for the coupled Lakshmanan-Porsezian-Daniel equation with nonzero boundary conditions","abstract":"The challenge of solving the initial value problem for the coupled Lakshmanan Porsezian Daniel equation, while considering nonzero boundary conditions at infinity, is addressed through the development of a suitable inverse scattering transform. Analytical properties of the Jost eigenfunctions are examined, along with the analysis of scattering coefficient characteristics. This analysis leads to the derivation of additional auxiliary eigenfunctions necessary for the comprehensive investigation of the fundamental eigenfunctions. Two symmetry conditions are discussed to study the eigenfunctions and scattering coefficients. These symmetry results are utilized to rigorously define the discrete spectrum and ascertain the corresponding symmetries of scattering datas. The inverse scattering problem is formulated by the Riemann-Hilbert problem. Then we can derive the exact solutions by coupled Lakshmanan Porsezian Daniel equation, the novel soliton solutions are derived and examined in detail.","sentences":["The challenge of solving the initial value problem for the coupled Lakshmanan Porsezian Daniel equation, while considering nonzero boundary conditions at infinity, is addressed through the development of a suitable inverse scattering transform.","Analytical properties of the Jost eigenfunctions are examined, along with the analysis of scattering coefficient characteristics.","This analysis leads to the derivation of additional auxiliary eigenfunctions necessary for the comprehensive investigation of the fundamental eigenfunctions.","Two symmetry conditions are discussed to study the eigenfunctions and scattering coefficients.","These symmetry results are utilized to rigorously define the discrete spectrum and ascertain the corresponding symmetries of scattering datas.","The inverse scattering problem is formulated by the Riemann-Hilbert problem.","Then we can derive the exact solutions by coupled Lakshmanan Porsezian Daniel equation, the novel soliton solutions are derived and examined in detail."],"url":"http://arxiv.org/abs/2404.03351v1","category":"nlin.SI"}
{"created":"2024-04-04 10:23:26","title":"Out-of-equilibrium thermodynamics of autocatalytic networks","abstract":"The purpose of this work is to clarify how the stoichiometric trait of autocatalytic networks, namely their absence of a conservation law, shapes their non-equilibrium behavior. To do so, we consider an autocatalytic network coupled with external species acting as food/waste materials, necessary to fulfill mass conservation. Then, we show that the production of autocatalytic species requires a conservative influx of these external species. From this, we derive the thermodynamic potential of an autocatalytic sub-network. The latter can be obtained from the usual semigrand free-energy of an open system by taking into account the conservative work associated to the influx of external species fueling the production of autocatalytic species. In the end, we identify the cost dedicated to the production of autocatalytic species and its efficiency. It reveals that sustaining steady production of species in an autocatalytic network is possible only if they are coupled with the environment.","sentences":["The purpose of this work is to clarify how the stoichiometric trait of autocatalytic networks, namely their absence of a conservation law, shapes their non-equilibrium behavior.","To do so, we consider an autocatalytic network coupled with external species acting as food/waste materials, necessary to fulfill mass conservation.","Then, we show that the production of autocatalytic species requires a conservative influx of these external species.","From this, we derive the thermodynamic potential of an autocatalytic sub-network.","The latter can be obtained from the usual semigrand free-energy of an open system by taking into account the conservative work associated to the influx of external species fueling the production of autocatalytic species.","In the end, we identify the cost dedicated to the production of autocatalytic species and its efficiency.","It reveals that sustaining steady production of species in an autocatalytic network is possible only if they are coupled with the environment."],"url":"http://arxiv.org/abs/2404.03347v1","category":"physics.chem-ph"}
{"created":"2024-04-04 10:04:44","title":"Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation","abstract":"In recent years, deep reinforcement learning (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation. However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments. This work introduces a Population-Based Reinforcement Learning (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel. The PBRL framework is applied to three state-of-the-art RL algorithms -- PPO, SAC, and DDPG -- dynamically adjusting hyperparameters based on the performance of learning agents. The experiments are performed on four challenging tasks in Isaac Gym -- Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick -- by analyzing the effect of population size and mutation mechanisms for hyperparameters. The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents. The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer. Code and videos of the learned policies are available on our project website.","sentences":["In recent years, deep reinforcement learning (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation.","However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments.","This work introduces a Population-Based Reinforcement Learning (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel.","The PBRL framework is applied to three state-of-the-art RL algorithms -- PPO, SAC, and DDPG -- dynamically adjusting hyperparameters based on the performance of learning agents.","The experiments are performed on four challenging tasks in Isaac Gym -- Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick -- by analyzing the effect of population size and mutation mechanisms for hyperparameters.","The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents.","The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer.","Code and videos of the learned policies are available on our project website."],"url":"http://arxiv.org/abs/2404.03336v1","category":"cs.RO"}
{"created":"2024-04-04 10:03:42","title":"Control and homogenization of a system of coupled parabolic equations with an oscillating coefficient","abstract":"In this article, we study the uniform null controllability problem for a system of coupled parabolic equations with an oscillating coefficient. This is done in three steps -- first, we study the spectral properties of an elliptic operator; second, we allow the system to evolve freely and obtain the required decay; third, we use a Carleman estimate to prove a suitable observability result. This uniform null controllability property is then used to homogenize the associated coupled parabolic system.","sentences":["In this article, we study the uniform null controllability problem for a system of coupled parabolic equations with an oscillating coefficient.","This is done in three steps -- first, we study the spectral properties of an elliptic operator; second, we allow the system to evolve freely and obtain the required decay; third, we use a Carleman estimate to prove a suitable observability result.","This uniform null controllability property is then used to homogenize the associated coupled parabolic system."],"url":"http://arxiv.org/abs/2404.03335v1","category":"math.AP"}
{"created":"2024-04-04 10:02:18","title":"First detection in space of the high-energy isomer of cyanomethanimine: H2CNCN","abstract":"We report the first detection in the interstellar medium of $N$-cyanomethanimine (H$_2$CNCN), the stable dimer of HCN of highest energy, and the most complex organic molecule identified in space containing the prebiotically relevant NCN backbone. We have identified a plethora of $a$-type rotational transitions with 3 $\\leq J_\\text{up} \\leq$ 11 and $K_\\text{a} \\leq$ 2 that belong to this species towards the Galactic Center G+0.693-0.027 molecular cloud, the only interstellar source showing the three cyanomethanimine isomers (including the $Z$- and $E$- isomers of $C$-cyanomethanimine, HNCHCN). We have derived a total column density for H$_2$CNCN of (2.9$\\, \\pm \\,$0.1)$\\times$10$^{12}$ cm$^{-2}$, which translates into a total molecular abundance with respect to H$_2$ of (2.1$\\, \\pm \\,$0.3)$\\times$10$^{-11}$. We have also revisited the previous detection of $E$- and $Z$-HNCHCN, and found a total $C/N$-cyanomethanimine abundance ratio of 31.8$\\, \\pm \\,$1.8 and a $Z/E$-HNCHCN ratio of 4.5$\\, \\pm \\,$0.2. While the latter can be explained on the basis of thermodynamic equilibrium, chemical kinetics are more likely responsible for the observed $C/N$-cyanomethanimine abundance ratio, where the gas-phase reaction between methanimine (CH$_2$NH) and the cyanogen radical (CN) arises as the primary formation route.","sentences":["We report the first detection in the interstellar medium of $N$-cyanomethanimine (H$_2$CNCN), the stable dimer of HCN of highest energy, and the most complex organic molecule identified in space containing the prebiotically relevant NCN backbone.","We have identified a plethora of $a$-type rotational transitions with 3 $\\leq J_\\text{up} \\leq$ 11 and $K_\\text{a} \\leq$ 2 that belong to this species towards the Galactic Center G+0.693-0.027 molecular cloud, the only interstellar source showing the three cyanomethanimine isomers (including the $Z$- and $E$- isomers of $C$-cyanomethanimine, HNCHCN).","We have derived a total column density for H$_2$CNCN of (2.9$\\, \\pm \\,$0.1)$\\times$10$^{12}$ cm$^{-2}$, which translates into a total molecular abundance with respect to H$_2$ of (2.1$\\, \\pm \\,$0.3)$\\times$10$^{-11}$. We have also revisited the previous detection of $E$- and $Z$-HNCHCN, and found a total $C/N$-cyanomethanimine abundance ratio of 31.8$\\, \\pm \\,$1.8 and a $Z/E$-HNCHCN ratio of 4.5$\\, \\pm \\,$0.2.","While the latter can be explained on the basis of thermodynamic equilibrium, chemical kinetics are more likely responsible for the observed $C/N$-cyanomethanimine abundance ratio, where the gas-phase reaction between methanimine (CH$_2$NH) and the cyanogen radical (CN) arises as the primary formation route."],"url":"http://arxiv.org/abs/2404.03334v1","category":"astro-ph.GA"}
{"created":"2024-04-04 09:57:29","title":"LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace","abstract":"Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a~provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.","sentences":["Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure.","Gradient-based methods have emerged as a common approach to large-scale bilevel problems.","However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck.","To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process.","As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient.","Moreover, we propose a~provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system.","To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization.","This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks."],"url":"http://arxiv.org/abs/2404.03331v1","category":"math.OC"}
{"created":"2024-04-04 09:35:48","title":"Exploring Lightweight Federated Learning for Distributed Load Forecasting","abstract":"Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner. This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data. We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework. The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems. With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform.","sentences":["Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner.","This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data.","We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework.","The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems.","With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform."],"url":"http://arxiv.org/abs/2404.03320v1","category":"cs.LG"}
{"created":"2024-04-04 09:35:42","title":"Early warning systems for financial markets of emerging economies","abstract":"We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point. The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets. The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest. Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail. We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023.","sentences":["We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point.","The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets.","The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest.","Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail.","We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023."],"url":"http://arxiv.org/abs/2404.03319v1","category":"econ.EM"}
{"created":"2024-04-04 09:23:51","title":"The complexity of non-stationary ideals","abstract":"We present an overview of results on the question of whether the non-stationary ideal of an uncountable regular cardinal $\\kappa$ can be defined by a $\\Pi_1$-formula using parameters of hereditary cardinality at most $\\kappa$. These results show that this question is deeply connected to several central topics of current research in set theory.","sentences":["We present an overview of results on the question of whether the non-stationary ideal of an uncountable regular cardinal $\\kappa$ can be defined by a $\\Pi_1$-formula using parameters of hereditary cardinality at most $\\kappa$. These results show that this question is deeply connected to several central topics of current research in set theory."],"url":"http://arxiv.org/abs/2404.03315v1","category":"math.LO"}
{"created":"2024-04-04 09:17:22","title":"M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews","abstract":"Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions. Motivational interviews exhibit three important characteristics. First, there are two distinct roles, namely client and therapist. Second, they are often highly emotionally charged, which can be expressed both in text and in prosody. Finally, context is of central importance to classify any given utterance. Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues. In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification. Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour. Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context. With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification. In extensive ablation studies, we quantify the improvement resulting from each contribution.","sentences":["Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions.","Motivational interviews exhibit three important characteristics.","First, there are two distinct roles, namely client and therapist.","Second, they are often highly emotionally charged, which can be expressed both in text and in prosody.","Finally, context is of central importance to classify any given utterance.","Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues.","In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification.","Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour.","Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context.","With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification.","In extensive ablation studies, we quantify the improvement resulting from each contribution."],"url":"http://arxiv.org/abs/2404.03312v1","category":"cs.CL"}
{"created":"2024-04-04 09:13:06","title":"Non-wellfounded parsimonious proofs and non-uniform complexity","abstract":"In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality ! is interpreted as a constructor for streams over finite data. We present non-wellfounded parsimonious proof systems capturing the classes $\\mathbf{FPTIME}$ and $\\mathbf{FP}/\\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination. Completeness relies on an encoding of polynomial Turing machines with advice.   As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems.","sentences":["In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality !","is interpreted as a constructor for streams over finite data.","We present non-wellfounded parsimonious proof systems capturing the classes $\\mathbf{FPTIME}$ and $\\mathbf{FP}/\\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination.","Completeness relies on an encoding of polynomial Turing machines with advice.   ","As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems."],"url":"http://arxiv.org/abs/2404.03311v1","category":"cs.LO"}
{"created":"2024-04-04 09:08:04","title":"Optimistic Online Non-stochastic Control via FTRL","abstract":"This paper brings the concept of \"optimism\" to the new and promising framework of online Non-stochastic Control (NSC). Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs. The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family. This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds. These new bounds are commensurate with the oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to the order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail. By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers.","sentences":["This paper brings the concept of \"optimism\" to the new and promising framework of online Non-stochastic Control (NSC).","Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs.","The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family.","This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds.","These new bounds are commensurate with the oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to the order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail.","By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers."],"url":"http://arxiv.org/abs/2404.03309v1","category":"cs.LG"}
{"created":"2024-04-04 08:57:52","title":"Evolutionary game on any hypergraph","abstract":"Cooperation plays a fundamental role in societal and biological domains, and the population structure profoundly shapes the dynamics of evolution. Practically, individuals behave either altruistically or egoistically in multiple groups, such as relatives, friends and colleagues, and feedbacks from these groupwise interactions will contribute to one's cognition and behavior. Due to the intricacy within and between groups, exploration of evolutionary dynamics over hypergraphs is relatively limited to date. To uncover this conundrum, we develop a higher-order random walk framework for five distinct updating rules, thus establishing explicit conditions for cooperation emergence on hypergraphs, and finding the overlaps between groups tend to foster cooperative behaviors. Our systematic analysis quantifies how the order and hyperdegree govern evolutionary outcomes. We also discover that whenever following a group wisdom update protocol, choosing a high-fitness group to interact equally within its members, cooperators will significantly prevail throughout the community. These findings underscore a crucial role of higher-order interaction and interdisciplinary collaboration throughout a broad range of living systems, favoring social prosperity.","sentences":["Cooperation plays a fundamental role in societal and biological domains, and the population structure profoundly shapes the dynamics of evolution.","Practically, individuals behave either altruistically or egoistically in multiple groups, such as relatives, friends and colleagues, and feedbacks from these groupwise interactions will contribute to one's cognition and behavior.","Due to the intricacy within and between groups, exploration of evolutionary dynamics over hypergraphs is relatively limited to date.","To uncover this conundrum, we develop a higher-order random walk framework for five distinct updating rules, thus establishing explicit conditions for cooperation emergence on hypergraphs, and finding the overlaps between groups tend to foster cooperative behaviors.","Our systematic analysis quantifies how the order and hyperdegree govern evolutionary outcomes.","We also discover that whenever following a group wisdom update protocol, choosing a high-fitness group to interact equally within its members, cooperators will significantly prevail throughout the community.","These findings underscore a crucial role of higher-order interaction and interdisciplinary collaboration throughout a broad range of living systems, favoring social prosperity."],"url":"http://arxiv.org/abs/2404.03305v1","category":"nlin.AO"}
{"created":"2024-04-04 08:52:30","title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?","abstract":"By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.","sentences":["By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks.","However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages.","In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions.","We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions.","Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context.","Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents.","Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions.","Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information."],"url":"http://arxiv.org/abs/2404.03302v1","category":"cs.CL"}
{"created":"2024-04-04 08:52:25","title":"Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics","abstract":"Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.","sentences":["Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale).","Scalar implicatures arise from the consideration of alternative statements which could have been made.","They can be triggered by scalar adjectives and require listeners to reason pragmatically about them.","Some scalar adjectives are more likely to trigger scalar implicatures than others.","This phenomenon is referred to as scalar diversity.","In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity.","We find that they encode rich lexical-semantic information about scalar adjectives.","However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity.","We also compare current models of different sizes and complexities and find that larger models are not always better.","Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives."],"url":"http://arxiv.org/abs/2404.03301v1","category":"cs.CL"}
{"created":"2024-04-04 08:41:10","title":"Use Cases for High Performance Research Desktops","abstract":"High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems. These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure. By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work. Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment. In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD. This paper defines the concept of High Performance Research Desktops and summarizes use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years. Based on these use cases, possible future directions are presented.","sentences":["High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems.","These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure.","By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work.","Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment.","In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD.","This paper defines the concept of High Performance Research Desktops and summarizes use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years.","Based on these use cases, possible future directions are presented."],"url":"http://arxiv.org/abs/2404.03298v1","category":"cs.DC"}
{"created":"2024-04-04 08:38:40","title":"Gibbs measures for hardcore-SOS models on Cayley trees","abstract":"We investigate the finite-state $p$-solid-on-solid model, for $p=\\infty$, on Cayley trees of order $k\\geq 2$ and establish a system of functional equations where each solution corresponds to a (splitting) Gibbs measure of the model. Our main result is that, for three states, $k=2,3$ and increasing coupling strength, the number of translation-invariant Gibbs measures behaves as $1\\to3\\to5\\to6\\to7$. This phase diagram is qualitatively similar to the one observed for three-state $p$-SOS models with $p>0$ and, in the case of $k=2$, we demonstrate that, on the level of the functional equations, the transition $p\\to\\infty$ is continuous.","sentences":["We investigate the finite-state $p$-solid-on-solid model, for $p=\\infty$, on Cayley trees of order $k\\geq 2$ and establish a system of functional equations where each solution corresponds to a (splitting) Gibbs measure of the model.","Our main result is that, for three states, $k=2,3$ and increasing coupling strength, the number of translation-invariant Gibbs measures behaves as $1\\to3\\to5\\to6\\to7$. This phase diagram is qualitatively similar to the one observed for three-state $p$-SOS models with $p>0$ and, in the case of $k=2$, we demonstrate that, on the level of the functional equations, the transition $p\\to\\infty$ is continuous."],"url":"http://arxiv.org/abs/2404.03297v1","category":"math-ph"}
{"created":"2024-04-04 08:18:52","title":"A complex node of the cosmic web associated with the massive galaxy cluster MACS J0600.1-2008","abstract":"MACS J0600.1-2008 (MACS0600) is an X-ray luminous, massive galaxy cluster at $z_{\\mathrm{d}}=0.43$, studied previously as part of the REionization LensIng Cluster Survey (RELICS) and ALMA Lensing Cluster Survey (ALCS) projects which revealed a complex, bimodal mass distribution and an intriguing high-redshift object behind it. Here, we report on the results of an extended strong-lensing (SL) analysis of this system. Using new JWST and ground-based Gemini-N and Keck data, we obtain 13 new spectroscopic redshifts of multiply imaged galaxies and identify 12 new photometric multiple-image systems and candidates, including two multiply imaged $z\\sim7$ objects. Taking advantage of the larger areal coverage, our analysis reveals a new bimodal, massive SL structure adjacent to the cluster which we measure spectroscopically to lie at the same redshift and whose existence was implied by previous SL-modeling analyses. While based in part on photometric systems identified in ground-based imaging requiring further verification, our extended SL model suggests that the cluster may have the second-largest critical area and effective Einstein radius observed to date, $A_{\\mathrm{crit}}\\simeq2.16\\,\\mathrm{arcmin}^2$ and $\\theta_{\\mathrm{E}}=49.7''\\pm5.0''$ for a source at $z_{\\mathrm{s}}=2$, enclosing a total mass of $M(<\\theta_{\\mathrm{E}})=(4.7\\pm0.7)\\times10^{14}\\,\\mathrm{M}_{\\odot}$. Yet another, probably related massive cluster structure, discovered in X-rays $5'$ (1.7 Mpc) further north, suggests that MACS0600 is in fact part of an even larger filamentary structure. This discovery adds to several recent detections of massive structures around SL galaxy clusters and establishes MACS0600 as a prime target for future high-redshift surveys with JWST.","sentences":["MACS J0600.1-2008 (MACS0600) is an X-ray luminous, massive galaxy cluster at $z_{\\mathrm{d}}=0.43$, studied previously as part of the REionization LensIng Cluster Survey (RELICS) and ALMA Lensing Cluster Survey (ALCS) projects which revealed a complex, bimodal mass distribution and an intriguing high-redshift object behind it.","Here, we report on the results of an extended strong-lensing (SL) analysis of this system.","Using new JWST and ground-based Gemini-N and Keck data, we obtain 13 new spectroscopic redshifts of multiply imaged galaxies and identify 12 new photometric multiple-image systems and candidates, including two multiply imaged $z\\sim7$ objects.","Taking advantage of the larger areal coverage, our analysis reveals a new bimodal, massive SL structure adjacent to the cluster which we measure spectroscopically to lie at the same redshift and whose existence was implied by previous SL-modeling analyses.","While based in part on photometric systems identified in ground-based imaging requiring further verification, our extended SL model suggests that the cluster may have the second-largest critical area and effective Einstein radius observed to date, $A_{\\mathrm{crit}}\\simeq2.16\\,\\mathrm{arcmin}^2$ and $\\theta_{\\mathrm{E}}=49.7''\\pm5.0''$ for a source at $z_{\\mathrm{s}}=2$, enclosing a total mass of $M(<\\theta_{\\mathrm{E}})=(4.7\\pm0.7)\\times10^{14}\\,\\mathrm{M}_{\\odot}$. Yet another, probably related massive cluster structure, discovered in X-rays $5'$ (1.7 Mpc) further north, suggests that MACS0600 is in fact part of an even larger filamentary structure.","This discovery adds to several recent detections of massive structures around SL galaxy clusters and establishes MACS0600 as a prime target for future high-redshift surveys with JWST."],"url":"http://arxiv.org/abs/2404.03286v1","category":"astro-ph.GA"}
{"created":"2024-04-04 08:15:42","title":"Combined DL-UL Distributed Beamforming Design for Cell-Free Massive MIMO","abstract":"We consider a cell-free massive multiple-input multiple-output system with multi-antenna access points (APs) and user equipments (UEs), where the UEs can be served in both the downlink (DL) and uplink (UL) within a resource block. We tackle the combined optimization of the DL precoders and combiners at the APs and DL UEs, respectively, together with the UL combiners and precoders at the APs and UL UEs, respectively. To this end, we propose distributed beamforming designs enabled by iterative bi-directional training (IBT) and based on the minimum mean squared error criterion. To reduce the IBT overhead and thus enhance the effective DL and UL rates, we carry out the distributed beamforming design by assuming that all the UEs are served solely in the DL and then utilize the obtained beamformers for the DL and UL data transmissions after proper scaling. Numerical results show the superiority of the proposed combined DL-UL distributed beamforming design over separate DL and UL designs, especially with short resource blocks.","sentences":["We consider a cell-free massive multiple-input multiple-output system with multi-antenna access points (APs) and user equipments (UEs), where the UEs can be served in both the downlink (DL) and uplink (UL) within a resource block.","We tackle the combined optimization of the DL precoders and combiners at the APs and DL UEs, respectively, together with the UL combiners and precoders at the APs and UL UEs, respectively.","To this end, we propose distributed beamforming designs enabled by iterative bi-directional training (IBT) and based on the minimum mean squared error criterion.","To reduce the IBT overhead and thus enhance the effective DL and UL rates, we carry out the distributed beamforming design by assuming that all the UEs are served solely in the DL and then utilize the obtained beamformers for the DL and UL data transmissions after proper scaling.","Numerical results show the superiority of the proposed combined DL-UL distributed beamforming design over separate DL and UL designs, especially with short resource blocks."],"url":"http://arxiv.org/abs/2404.03285v1","category":"cs.IT"}
{"created":"2024-04-04 08:04:28","title":"MMSE Channel Estimation in Large-Scale MIMO: Improved Robustness with Reduced Complexity","abstract":"Large-scale MIMO systems with a massive number N of individually controlled antennas pose significant challenges for minimum mean square error (MMSE) channel estimation, based on uplink pilots. The major ones arise from the computational complexity, which scales with $N^3$, and from the need for accurate knowledge of the channel statistics. This paper aims to address both challenges by introducing reduced-complexity channel estimation methods that achieve the performance of MMSE in terms of estimation accuracy and uplink spectral efficiency while demonstrating improved robustness in practical scenarios where channel statistics must be estimated. This is achieved by exploiting the inherent structure of the spatial correlation matrix induced by the array geometry. Specifically, we use a Kronecker decomposition for uniform planar arrays and a well-suited circulant approximation for uniform linear arrays. By doing so, a significantly lower computational complexity is achieved, scaling as $N\\sqrt{N}$ and $N\\log N$ for squared planar arrays and linear arrays, respectively.","sentences":["Large-scale MIMO systems with a massive number N of individually controlled antennas pose significant challenges for minimum mean square error (MMSE) channel estimation, based on uplink pilots.","The major ones arise from the computational complexity, which scales with $N^3$, and from the need for accurate knowledge of the channel statistics.","This paper aims to address both challenges by introducing reduced-complexity channel estimation methods that achieve the performance of MMSE in terms of estimation accuracy and uplink spectral efficiency while demonstrating improved robustness in practical scenarios where channel statistics must be estimated.","This is achieved by exploiting the inherent structure of the spatial correlation matrix induced by the array geometry.","Specifically, we use a Kronecker decomposition for uniform planar arrays and a well-suited circulant approximation for uniform linear arrays.","By doing so, a significantly lower computational complexity is achieved, scaling as $N\\sqrt{N}$ and $N\\log N$ for squared planar arrays and linear arrays, respectively."],"url":"http://arxiv.org/abs/2404.03279v1","category":"cs.IT"}
{"created":"2024-04-04 07:59:18","title":"Traversability-aware Adaptive Optimization for Path Planning and Control in Mountainous Terrain","abstract":"Autonomous navigation in extreme mountainous terrains poses challenges due to the presence of mobility-stressing elements and undulating surfaces, making it particularly difficult compared to conventional off-road driving scenarios. In such environments, estimating traversability solely based on exteroceptive sensors often leads to the inability to reach the goal due to a high prevalence of non-traversable areas. In this paper, we consider traversability as a relative value that integrates the robot's internal state, such as speed and torque to exhibit resilient behavior to reach its goal successfully. We separate traversability into apparent traversability and relative traversability, then incorporate these distinctions in the optimization process of sampling-based planning and motion predictive control. Our method enables the robots to execute the desired behaviors more accurately while avoiding hazardous regions and getting stuck. Experiments conducted on simulation with 27 diverse types of mountainous terrain and real-world demonstrate the robustness of the proposed framework, with increasingly better performance observed in more complex environments.","sentences":["Autonomous navigation in extreme mountainous terrains poses challenges due to the presence of mobility-stressing elements and undulating surfaces, making it particularly difficult compared to conventional off-road driving scenarios.","In such environments, estimating traversability solely based on exteroceptive sensors often leads to the inability to reach the goal due to a high prevalence of non-traversable areas.","In this paper, we consider traversability as a relative value that integrates the robot's internal state, such as speed and torque to exhibit resilient behavior to reach its goal successfully.","We separate traversability into apparent traversability and relative traversability, then incorporate these distinctions in the optimization process of sampling-based planning and motion predictive control.","Our method enables the robots to execute the desired behaviors more accurately while avoiding hazardous regions and getting stuck.","Experiments conducted on simulation with 27 diverse types of mountainous terrain and real-world demonstrate the robustness of the proposed framework, with increasingly better performance observed in more complex environments."],"url":"http://arxiv.org/abs/2404.03274v1","category":"cs.RO"}
{"created":"2024-04-04 07:48:22","title":"Run your HPC jobs in Eco-Mode: revealing the potential of user-assisted power capping in supercomputing systems","abstract":"The energy consumption of an exascale High-Performance Computing (HPC) supercomputer rivals that of tens of thousands of people in terms of electricity demand. Given the substantial energy footprint of exascale HPC systems and the increasing strain on power grids due to climate-related events, electricity providers are starting to impose power caps during critical periods to their users. In this context, it becomes crucial to implement strategies that manage the power consumption of supercomputers while simultaneously ensuring their uninterrupted operation.This paper investigates the proposition that HPC users can willingly sacrifice some processing performance to contribute to a global energy-saving initiative. With the objective of offering an efficient energy-saving strategy by involving users, we introduce a user-assisted supercomputer power-capping methodology. In this approach, users have the option to voluntarily permit their applications to operate in a power-capped mode, denoted as 'Eco-Mode', as necessary. Leveraging HPC simulations, along with energy traces and application metadata derived from a recent Top500 HPC supercomputer, we conducted an experimental campaign to quantify the effects of Eco-Mode on energy conservation and on user experience. Specifically, our study aimed to demonstrate that, with a sufficient number of users choosing Eco-Mode, the supercomputer maintains good performances within the specified power cap. Furthermore, we sought to determine the optimal conditions regarding the number of users embracing Eco-Mode and the magnitude of power capping required for applications (i.e., the intensity of Eco-Mode). Our findings indicate that decreasing the speed of jobs can decrease significantly the number of jobs that must be killed. Moreover, as the adoption of Eco-Mode increases among users, the likelihood of every job to be killed also decreases.","sentences":["The energy consumption of an exascale High-Performance Computing (HPC) supercomputer rivals that of tens of thousands of people in terms of electricity demand.","Given the substantial energy footprint of exascale HPC systems and the increasing strain on power grids due to climate-related events, electricity providers are starting to impose power caps during critical periods to their users.","In this context, it becomes crucial to implement strategies that manage the power consumption of supercomputers while simultaneously ensuring their uninterrupted operation.","This paper investigates the proposition that HPC users can willingly sacrifice some processing performance to contribute to a global energy-saving initiative.","With the objective of offering an efficient energy-saving strategy by involving users, we introduce a user-assisted supercomputer power-capping methodology.","In this approach, users have the option to voluntarily permit their applications to operate in a power-capped mode, denoted as 'Eco-Mode', as necessary.","Leveraging HPC simulations, along with energy traces and application metadata derived from a recent Top500 HPC supercomputer, we conducted an experimental campaign to quantify the effects of Eco-Mode on energy conservation and on user experience.","Specifically, our study aimed to demonstrate that, with a sufficient number of users choosing Eco-Mode, the supercomputer maintains good performances within the specified power cap.","Furthermore, we sought to determine the optimal conditions regarding the number of users embracing Eco-Mode and the magnitude of power capping required for applications (i.e., the intensity of Eco-Mode).","Our findings indicate that decreasing the speed of jobs can decrease significantly the number of jobs that must be killed.","Moreover, as the adoption of Eco-Mode increases among users, the likelihood of every job to be killed also decreases."],"url":"http://arxiv.org/abs/2404.03271v1","category":"math.OC"}
{"created":"2024-04-04 07:35:39","title":"Quantum aggregation with temporal delay","abstract":"Advanced quantum networking systems rely on efficient quantum error correction codes for their optimal realization. The rate at which the encoded information is transmitted is a fundamental limit that affects the performance of such systems. Quantum aggregation allows one to increase the transmission rate by adding multiple paths connecting two distant users. Aggregating channels of different paths allows more users to simultaneously exchange the encoded information. Recent work has shown that quantum aggregation can also reduce the number of physical resources of an error correction code when it is combined with the quantum multiplexing technique. However, the different channel lengths across the various paths means some of the encoded quantum information will arrive earlier than others and it must be stored in quantum memories. The information stored will then deteriorate due to decoherence processes leading to detrimental effects for the fidelity of the final quantum state. Here, we explore the effects of a depolarization channel that occurs for the quantum Reed-Solomon code when quantum aggregation involving different channel lengths is used. We determine the best distribution of resources among the various channels connecting two remote users. Further we estimate the coherence time required to achieve a certain fidelity. Our results will have a significant impact on the ways physical resources are distributed across a quantum network.","sentences":["Advanced quantum networking systems rely on efficient quantum error correction codes for their optimal realization.","The rate at which the encoded information is transmitted is a fundamental limit that affects the performance of such systems.","Quantum aggregation allows one to increase the transmission rate by adding multiple paths connecting two distant users.","Aggregating channels of different paths allows more users to simultaneously exchange the encoded information.","Recent work has shown that quantum aggregation can also reduce the number of physical resources of an error correction code when it is combined with the quantum multiplexing technique.","However, the different channel lengths across the various paths means some of the encoded quantum information will arrive earlier than others and it must be stored in quantum memories.","The information stored will then deteriorate due to decoherence processes leading to detrimental effects for the fidelity of the final quantum state.","Here, we explore the effects of a depolarization channel that occurs for the quantum Reed-Solomon code when quantum aggregation involving different channel lengths is used.","We determine the best distribution of resources among the various channels connecting two remote users.","Further we estimate the coherence time required to achieve a certain fidelity.","Our results will have a significant impact on the ways physical resources are distributed across a quantum network."],"url":"http://arxiv.org/abs/2404.03262v1","category":"quant-ph"}
{"created":"2024-04-04 07:34:02","title":"Restricted Phase Space Thermodynamics of NED-AdS Black Holes","abstract":"We study the Restricted Phase Space Thermodynamics (RPST) of magnetically charged by nonlinear electrodynamics (NED)-AdS black holes. The first law and the corresponding Euler relation is examined using the scaling properties. The Euler relation is found to hold automatically. We observe that the first order homogeneity of mass and the zeroth order homogeneity of the intensive variables is intact. We use numerical and graphical techniques to find the critical points of the thermodynamic quantities $S, Q, T, F$. By utilizing the re-scaling properties of the equation of states, we study the thermodynamic processes using different pairs of variables. We also analyze the phase transition behavior of free energy and other thermodynamic variables. It is observed that the thermodynamics in the RPS formalism of our concerned black hole system is similar to RN-AdS, Kerr AdS and Kerr-Sen AdS black hole. This suggests that there should be some underlying universality in the RPST formalism. However, one particular $\\mu-C$ process is found to differ from the earlier studied black holes, calling for further studies on the $\\mu-C$ processes to comment on their universality.","sentences":["We study the Restricted Phase Space Thermodynamics (RPST) of magnetically charged by nonlinear electrodynamics (NED)-AdS black holes.","The first law and the corresponding Euler relation is examined using the scaling properties.","The Euler relation is found to hold automatically.","We observe that the first order homogeneity of mass and the zeroth order homogeneity of the intensive variables is intact.","We use numerical and graphical techniques to find the critical points of the thermodynamic quantities $S, Q, T, F$.","By utilizing the re-scaling properties of the equation of states, we study the thermodynamic processes using different pairs of variables.","We also analyze the phase transition behavior of free energy and other thermodynamic variables.","It is observed that the thermodynamics in the RPS formalism of our concerned black hole system is similar to RN-AdS, Kerr AdS and Kerr-Sen AdS black hole.","This suggests that there should be some underlying universality in the RPST formalism.","However, one particular $\\mu-C$ process is found to differ from the earlier studied black holes, calling for further studies on the $\\mu-C$ processes to comment on their universality."],"url":"http://arxiv.org/abs/2404.03261v1","category":"hep-th"}
{"created":"2024-04-04 07:25:25","title":"Geometry-induced friction at a soft interface","abstract":"Soft and biological matter come in a variety of shapes and geometries. When soft surfaces that do not fit into each other due to a mismatch in Gaussian curvatures form an interface, beautiful geometry-induced patterns emerge. In this paper, we study the effect of geometry on the dynamical response of soft surfaces moving relative to each other. Using a novel experimental scheme, we measure friction between a highly bendable thin polymer sheet and a hydrogel substrate. At this soft and low-friction interface, we find a strong dependence of friction on the relative geometry of the two surfaces - a flat sheet experiences significantly larger friction on a spherical substrate than on planar or cylindrical substrate. We show that the stress developed in the sheet due to its geometrically incompatible confinement is responsible for the enhanced friction. This mechanism also leads to a transition in the nature of friction as the sheet radius is increased beyond a critical value. Our finding reveals a hitherto unnoticed non-specific mechanism of purely geometrical origin that may influence friction significantly in soft, biological, and nano-scale systems. In particular, it provokes us to re-examine our understanding of phenomena such as the curvature dependence of biological cell mobility.","sentences":["Soft and biological matter come in a variety of shapes and geometries.","When soft surfaces that do not fit into each other due to a mismatch in Gaussian curvatures form an interface, beautiful geometry-induced patterns emerge.","In this paper, we study the effect of geometry on the dynamical response of soft surfaces moving relative to each other.","Using a novel experimental scheme, we measure friction between a highly bendable thin polymer sheet and a hydrogel substrate.","At this soft and low-friction interface, we find a strong dependence of friction on the relative geometry of the two surfaces - a flat sheet experiences significantly larger friction on a spherical substrate than on planar or cylindrical substrate.","We show that the stress developed in the sheet due to its geometrically incompatible confinement is responsible for the enhanced friction.","This mechanism also leads to a transition in the nature of friction as the sheet radius is increased beyond a critical value.","Our finding reveals a hitherto unnoticed non-specific mechanism of purely geometrical origin that may influence friction significantly in soft, biological, and nano-scale systems.","In particular, it provokes us to re-examine our understanding of phenomena such as the curvature dependence of biological cell mobility."],"url":"http://arxiv.org/abs/2404.03255v1","category":"cond-mat.soft"}
{"created":"2024-04-04 07:03:13","title":"On the Range of a class of Complex Monge-Amp\u00e8re operators on compact Hermitian manifolds","abstract":"Let $(X,\\omega)$ be a compact Hermitian manifold of complex dimension $n$. Let $\\beta$ be a smooth real closed $(1,1)$ form such that there exists a function $\\rho \\in \\mbox{PSH}(X,\\beta)\\cap L^{\\infty}(X)$. We study the range of the complex non-pluripolar Monge-Amp\\`ere operator $\\langle(\\beta+dd^c\\cdot)^n\\rangle$ on weighted Monge-Amp\\`ere energy classes on $X$. In particular, when $\\rho$ is assumed to be continuous, we give a complete characterization of the range of the complex Monge-Amp\\`ere operator on the class $\\mathcal E(X,\\beta)$, which is the class of all $\\varphi \\in \\mbox{PSH}(X,\\beta)$ with full Monge-Amp\\`ere mass, i.e. $\\int_X\\langle (\\beta+dd^c\\varphi)^n\\rangle=\\int_X\\beta^n$.","sentences":["Let $(X,\\omega)$ be a compact Hermitian manifold of complex dimension $n$. Let $\\beta$ be a smooth real closed $(1,1)$ form such that there exists a function $\\rho \\in \\mbox{PSH}(X,\\beta)\\cap L^{\\infty}(X)$. We study the range of the complex non-pluripolar Monge-Amp\\`ere operator $\\langle(\\beta+dd^c\\cdot)^n\\rangle$ on weighted Monge-Amp\\`ere energy classes on $X$. In particular, when $\\rho$ is assumed to be continuous, we give a complete characterization of the range of the complex Monge-Amp\\`ere operator on the class $\\mathcal E(X,\\beta)$, which is the class of all $\\varphi \\in \\mbox{PSH}(X,\\beta)$ with full Monge-Amp\\`ere mass, i.e. $\\int_X\\langle (\\beta+dd^c\\varphi)^n\\rangle=\\int_X\\beta^n$."],"url":"http://arxiv.org/abs/2404.03246v1","category":"math.CV"}
{"created":"2024-04-04 06:56:32","title":"Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows","abstract":"Physics-informed neural networks (PINNs) have gained significant prominence as a powerful tool in the field of scientific computing and simulations. Their ability to seamlessly integrate physical principles into deep learning architectures has revolutionized the approaches to solving complex problems in physics and engineering. However, a persistent challenge faced by mainstream PINNs lies in their handling of discontinuous input data, leading to inaccuracies in predictions. This study addresses these challenges by incorporating the discretized forms of the governing equations into the PINN framework. We propose to combine the power of neural networks with the dynamics imposed by the discretized differential equations. By discretizing the governing equations, the PINN learns to account for the discontinuities and accurately capture the underlying relationships between inputs and outputs, improving the accuracy compared to traditional interpolation techniques. Moreover, by leveraging the power of neural networks, the computational cost associated with numerical simulations is substantially reduced. We evaluate our model on a large-scale dataset for the prediction of pressure and saturation fields demonstrating high accuracies compared to non-physically aware models.","sentences":["Physics-informed neural networks (PINNs) have gained significant prominence as a powerful tool in the field of scientific computing and simulations.","Their ability to seamlessly integrate physical principles into deep learning architectures has revolutionized the approaches to solving complex problems in physics and engineering.","However, a persistent challenge faced by mainstream PINNs lies in their handling of discontinuous input data, leading to inaccuracies in predictions.","This study addresses these challenges by incorporating the discretized forms of the governing equations into the PINN framework.","We propose to combine the power of neural networks with the dynamics imposed by the discretized differential equations.","By discretizing the governing equations, the PINN learns to account for the discontinuities and accurately capture the underlying relationships between inputs and outputs, improving the accuracy compared to traditional interpolation techniques.","Moreover, by leveraging the power of neural networks, the computational cost associated with numerical simulations is substantially reduced.","We evaluate our model on a large-scale dataset for the prediction of pressure and saturation fields demonstrating high accuracies compared to non-physically aware models."],"url":"http://arxiv.org/abs/2404.03240v1","category":"cs.LG"}
{"created":"2024-04-04 06:24:11","title":"Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks","abstract":"We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.","sentences":["We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents.","Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology.","Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes).","We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information.","The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable.","We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture.","Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies.","Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning."],"url":"http://arxiv.org/abs/2404.03227v1","category":"eess.SP"}
{"created":"2024-04-04 06:23:21","title":"INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems","abstract":"As modern HPC computing platforms become increasingly heterogeneous, it is challenging for programmers to fully leverage the computation power of massive parallelism offered by such heterogeneity. Consequently, task-based runtime systems have been proposed as an intermediate layer to hide the complex heterogeneity from the application programmers. The core functionality of these systems is to realize efficient task-to-resource mapping in the form of Directed Acyclic Graph (DAG) scheduling. However, existing scheduling schemes face several drawbacks to determine task priorities due to the heavy reliance on domain knowledge or failure to efficiently exploit the interaction of application and hardware characteristics. In this paper, we propose INSPIRIT, an efficient and lightweight scheduling framework with adaptive priority designed for task-based runtime systems. INSPIRIT introduces two novel task attributes \\textit{inspiring ability} and \\textit{inspiring efficiency} for dictating scheduling, eliminating the need for application domain knowledge. In addition, INSPIRIT jointly considers runtime information such as ready tasks in worker queues to guide task scheduling. This approach exposes more performance opportunities in heterogeneous hardware at runtime while effectively reducing the overhead for adjusting task priorities. Our evaluation results demonstrate that INSPIRIT achieves superior performance compared to cutting edge scheduling schemes on both synthesized and real-world task DAGs.","sentences":["As modern HPC computing platforms become increasingly heterogeneous, it is challenging for programmers to fully leverage the computation power of massive parallelism offered by such heterogeneity.","Consequently, task-based runtime systems have been proposed as an intermediate layer to hide the complex heterogeneity from the application programmers.","The core functionality of these systems is to realize efficient task-to-resource mapping in the form of Directed Acyclic Graph (DAG) scheduling.","However, existing scheduling schemes face several drawbacks to determine task priorities due to the heavy reliance on domain knowledge or failure to efficiently exploit the interaction of application and hardware characteristics.","In this paper, we propose INSPIRIT, an efficient and lightweight scheduling framework with adaptive priority designed for task-based runtime systems.","INSPIRIT introduces two novel task attributes \\textit{inspiring ability} and \\textit{inspiring efficiency} for dictating scheduling, eliminating the need for application domain knowledge.","In addition, INSPIRIT jointly considers runtime information such as ready tasks in worker queues to guide task scheduling.","This approach exposes more performance opportunities in heterogeneous hardware at runtime while effectively reducing the overhead for adjusting task priorities.","Our evaluation results demonstrate that INSPIRIT achieves superior performance compared to cutting edge scheduling schemes on both synthesized and real-world task DAGs."],"url":"http://arxiv.org/abs/2404.03226v1","category":"cs.DC"}
{"created":"2024-04-04 06:19:29","title":"Diagrammatic Negative Information","abstract":"The flow of information through a complex system can be readily understood with category theory. However, negative information (e.g., what is not possible) does not have an immediately evident categorical representation. The formalization of nategories using unconventional composition addresses this issue, and lets imposed limitations on categories be considered. However, traditional nategories abandon core categorical constructs and rely on extensive mathematical development. This creates a divide between the consideration of positive and negative information composition. In this work, we show that negative information can be considered in a natural categorical manner. This is aided by functor string diagrams, a novel flexible diagrammatic approach that can intuitively show the operation of hom-functors and natural transformations in expressions. This insight reveals how to consider the composition of negative information with foundational categorical constructs without relying on enrichment. We present diagrammatic means to consider not only nategories, but preorders more broadly. This paper introduces diagrammatic methods for the consideration of triangle inequalities and co-designs $\\mathbf{DP/Feas_{Bool}}$, showing how important cases of negative information composition can be categorically and diagrammatically approached. In particular, we develop systematic tools to rigorously consider imposed limitations on systems, advancing our mathematical understanding, and present intuitive diagrams which motivate widespread adoption and usage for various applications.","sentences":["The flow of information through a complex system can be readily understood with category theory.","However, negative information (e.g., what is not possible) does not have an immediately evident categorical representation.","The formalization of nategories using unconventional composition addresses this issue, and lets imposed limitations on categories be considered.","However, traditional nategories abandon core categorical constructs and rely on extensive mathematical development.","This creates a divide between the consideration of positive and negative information composition.","In this work, we show that negative information can be considered in a natural categorical manner.","This is aided by functor string diagrams, a novel flexible diagrammatic approach that can intuitively show the operation of hom-functors and natural transformations in expressions.","This insight reveals how to consider the composition of negative information with foundational categorical constructs without relying on enrichment.","We present diagrammatic means to consider not only nategories, but preorders more broadly.","This paper introduces diagrammatic methods for the consideration of triangle inequalities and co-designs $\\mathbf{DP/Feas_{Bool}}$, showing how important cases of negative information composition can be categorically and diagrammatically approached.","In particular, we develop systematic tools to rigorously consider imposed limitations on systems, advancing our mathematical understanding, and present intuitive diagrams which motivate widespread adoption and usage for various applications."],"url":"http://arxiv.org/abs/2404.03224v1","category":"math.CT"}
{"created":"2024-04-04 06:10:57","title":"Enabling Clean Energy Resilience with Machine Learning-Empowered Underground Hydrogen Storage","abstract":"To address the urgent challenge of climate change, there is a critical need to transition away from fossil fuels towards sustainable energy systems, with renewable energy sources playing a pivotal role. However, the inherent variability of renewable energy, without effective storage solutions, often leads to imbalances between energy supply and demand. Underground Hydrogen Storage (UHS) emerges as a promising long-term storage solution to bridge this gap, yet its widespread implementation is impeded by the high computational costs associated with high fidelity UHS simulations. This paper introduces UHS from a data-driven perspective and outlines a roadmap for integrating machine learning into UHS, thereby facilitating the large-scale deployment of UHS.","sentences":["To address the urgent challenge of climate change, there is a critical need to transition away from fossil fuels towards sustainable energy systems, with renewable energy sources playing a pivotal role.","However, the inherent variability of renewable energy, without effective storage solutions, often leads to imbalances between energy supply and demand.","Underground Hydrogen Storage (UHS) emerges as a promising long-term storage solution to bridge this gap, yet its widespread implementation is impeded by the high computational costs associated with high fidelity UHS simulations.","This paper introduces UHS from a data-driven perspective and outlines a roadmap for integrating machine learning into UHS, thereby facilitating the large-scale deployment of UHS."],"url":"http://arxiv.org/abs/2404.03222v1","category":"cs.LG"}
{"created":"2024-04-04 05:54:19","title":"iSeg: Interactive 3D Segmentation via Interactive Attention","abstract":"We present iSeg, a new interactive technique for segmenting 3D shapes. Previous works have focused mainly on leveraging pre-trained 2D foundation models for 3D segmentation based on text. However, text may be insufficient for accurately describing fine-grained spatial segmentations. Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view. Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D. Our system accepts user clicks directly on the shape's surface, indicating the inclusion or exclusion of regions from the desired shape partition. To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model. We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user's specifications. Our project page is at https://threedle.github.io/iSeg/.","sentences":["We present iSeg, a new interactive technique for segmenting 3D shapes.","Previous works have focused mainly on leveraging pre-trained 2D foundation models for 3D segmentation based on text.","However, text may be insufficient for accurately describing fine-grained spatial segmentations.","Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view.","Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D.","Our system accepts user clicks directly on the shape's surface, indicating the inclusion or exclusion of regions from the desired shape partition.","To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model.","We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user's specifications.","Our project page is at https://threedle.github.io/iSeg/."],"url":"http://arxiv.org/abs/2404.03219v1","category":"cs.CV"}
{"created":"2024-04-04 05:38:16","title":"Holographic Global Vortices with Novel Boundary Conditions","abstract":"The AdS/CFT correspondence has significantly impacted the study of strongly coupled systems, providing insights into various condensed matter phenomena through its holographic duality. This paper introduces an alternative approach to the breaking of the global $U(1)$ symmetry in the bulk in two asymptotically AdS spacetimes: AdS plus hard wall and AdS Blackbrane. We explore a $(3+1)$-dimensional bulk $U(1)$ symmetry-breaking phase vacuum within a global $U(1)$ $\\phi^4$ field theory, without the gauge field present in prior models. We find the symmetry-breaking vacuum requires that the mass squared is proportional to the quartic coupling. We also investigate numerical solutions of topologically stable vortex strings extending into the bulk. We find evidence that the full UV expansion is dual to a point-like boundary excitation.","sentences":["The AdS/CFT correspondence has significantly impacted the study of strongly coupled systems, providing insights into various condensed matter phenomena through its holographic duality.","This paper introduces an alternative approach to the breaking of the global $U(1)$ symmetry in the bulk in two asymptotically AdS spacetimes: AdS plus hard wall and AdS Blackbrane.","We explore a $(3+1)$-dimensional bulk $U(1)$ symmetry-breaking phase vacuum within a global $U(1)$ $\\phi^4$ field theory, without the gauge field present in prior models.","We find the symmetry-breaking vacuum requires that the mass squared is proportional to the quartic coupling.","We also investigate numerical solutions of topologically stable vortex strings extending into the bulk.","We find evidence that the full UV expansion is dual to a point-like boundary excitation."],"url":"http://arxiv.org/abs/2404.03212v1","category":"hep-th"}
{"created":"2024-04-04 05:35:59","title":"Convergence Conditions of Online Regularized Statistical Learning in Reproducing Kernel Hilbert Space With Non-Stationary Data","abstract":"We study the convergence of recursive regularized learning algorithms in the reproducing kernel Hilbert space (RKHS) with dependent and non-stationary online data streams. Firstly, we study the mean square asymptotic stability of a class of random difference equations in RKHS, whose non-homogeneous terms are martingale difference sequences dependent on the homogeneous ones. Secondly, we introduce the concept of random Tikhonov regularization path, and show that if the regularization path is slowly time-varying in some sense, then the output of the algorithm is consistent with the regularization path in mean square. Furthermore, if the data streams also satisfy the RKHS persistence of excitation condition, i.e. there exists a fixed length of time period, such that each eigenvalue of the conditional expectation of the operators induced by the input data accumulated over every time period has a uniformly positive lower bound with respect to time, then the output of the algorithm is consistent with the unknown function in mean square. Finally, for the case with independent and non-identically distributed data streams, the algorithm achieves the mean square consistency provided the marginal probability measures induced by the input data are slowly time-varying and the average measure over each fixed-length time period has a uniformly strictly positive lower bound.","sentences":["We study the convergence of recursive regularized learning algorithms in the reproducing kernel Hilbert space (RKHS) with dependent and non-stationary online data streams.","Firstly, we study the mean square asymptotic stability of a class of random difference equations in RKHS, whose non-homogeneous terms are martingale difference sequences dependent on the homogeneous ones.","Secondly, we introduce the concept of random Tikhonov regularization path, and show that if the regularization path is slowly time-varying in some sense, then the output of the algorithm is consistent with the regularization path in mean square.","Furthermore, if the data streams also satisfy the RKHS persistence of excitation condition, i.e. there exists a fixed length of time period, such that each eigenvalue of the conditional expectation of the operators induced by the input data accumulated over every time period has a uniformly positive lower bound with respect to time, then the output of the algorithm is consistent with the unknown function in mean square.","Finally, for the case with independent and non-identically distributed data streams, the algorithm achieves the mean square consistency provided the marginal probability measures induced by the input data are slowly time-varying and the average measure over each fixed-length time period has a uniformly strictly positive lower bound."],"url":"http://arxiv.org/abs/2404.03211v1","category":"cs.LG"}
{"created":"2024-04-04 05:16:18","title":"Optimal Dynamical Gauge in the Quantum Rabi Model","abstract":"In this paper, we investigate the gauge dependence of various physical observables in the quantum Rabi model (QRM) under different potential fields, arising from the Hilbert-space truncation of the atomic degree of freedom. We discover that in both the square-well potential and oscillator potential,the optimal gauges for the ground-state energy of the QRM vary with respect to the cavity frequency, with the dipole gauge being optimal in the low-frequency limit and the Coulomb gauge in the high-frequency limit of the cavity frequency. Additionally, for higher energy levels, the optimal gauge asymptotically approaches the dipole gauge. However, for the dynamical quantity out-time-order correlator (OTOC), we find the necessity to introduce an optimal dynamical gauge. We determine the optimal dynamical gauge by minimizing the mean error between the two-level OTOC and the full Hamiltonian one. We expect that this study will contribute to a more profound understanding of the subtle relation between gauge choice and the dynamics of QED systems.","sentences":["In this paper, we investigate the gauge dependence of various physical observables in the quantum Rabi model (QRM) under different potential fields, arising from the Hilbert-space truncation of the atomic degree of freedom.","We discover that in both the square-well potential and oscillator potential,the optimal gauges for the ground-state energy of the QRM vary with respect to the cavity frequency, with the dipole gauge being optimal in the low-frequency limit and the Coulomb gauge in the high-frequency limit of the cavity frequency.","Additionally, for higher energy levels, the optimal gauge asymptotically approaches the dipole gauge.","However, for the dynamical quantity out-time-order correlator (OTOC), we find the necessity to introduce an optimal dynamical gauge.","We determine the optimal dynamical gauge by minimizing the mean error between the two-level OTOC and the full Hamiltonian one.","We expect that this study will contribute to a more profound understanding of the subtle relation between gauge choice and the dynamics of QED systems."],"url":"http://arxiv.org/abs/2404.03205v1","category":"quant-ph"}
{"created":"2024-04-04 05:10:26","title":"OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images","abstract":"Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published.","sentences":["Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics.","However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images.","In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction.","Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting.","According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering.","As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation.","Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images.","To benefit the research community, the code will be made publicly available once the paper is published."],"url":"http://arxiv.org/abs/2404.03202v1","category":"cs.CV"}
{"created":"2024-04-04 05:09:38","title":"Groundhog: Linearly-Scalable Smart Contracting via Commutative Transaction Semantics","abstract":"Groundhog is a novel design for a smart contract execution engine based around concurrent execution of blocks of transactions. Unlike prior work, transactions within a block in Groundhog are not ordered relative to one another. Instead, our key design insights are first, to design a set of commutative semantics that lets the Groundhog runtime deterministically resolve concurrent accesses to shared data. Second, some storage accesses (such as withdrawing money from an account) conflict irresolvably; Groundhog therefore enforces validity constraints on persistent storage accesses via a reserve-commit process. These two ideas give Groundhog a set of semantics that, while not as powerful as traditional sequential semantics, are flexible enough to implement a wide variety of important applications, and are strictly more powerful than the semantics used in some production blockchains today. Unlike prior smart contract systems, transactions throughput never suffers from contention between transactions. Using 96 CPU cores, Groundhog can process more than half a million payment transactions per second, whether between 10M accounts or just 2.","sentences":["Groundhog is a novel design for a smart contract execution engine based around concurrent execution of blocks of transactions.","Unlike prior work, transactions within a block in Groundhog are not ordered relative to one another.","Instead, our key design insights are first, to design a set of commutative semantics that lets the Groundhog runtime deterministically resolve concurrent accesses to shared data.","Second, some storage accesses (such as withdrawing money from an account) conflict irresolvably; Groundhog therefore enforces validity constraints on persistent storage accesses via a reserve-commit process.","These two ideas give Groundhog a set of semantics that, while not as powerful as traditional sequential semantics, are flexible enough to implement a wide variety of important applications, and are strictly more powerful than the semantics used in some production blockchains today.","Unlike prior smart contract systems, transactions throughput never suffers from contention between transactions.","Using 96 CPU cores, Groundhog can process more than half a million payment transactions per second, whether between 10M accounts or just 2."],"url":"http://arxiv.org/abs/2404.03201v1","category":"cs.DC"}
{"created":"2024-04-04 04:57:55","title":"A Rolling Horizon Restoration Framework for Post-disaster Restoration of Electrical Distribution Networks","abstract":"Severe weather events such as floods, hurricanes, earthquakes, and large wind or ice storms can cause extensive damage to electrical distribution networks, requiring a multi-day restoration effort. Complicating the recovery process is the lack of complete and accurate information regarding the extent and locations of damages, at least during the initial part of the recovery process. These factors make workforce planning challenging. In this paper, we adopt a rolling horizon restoration framework whereby repairs are planned for adjustable finite length restoration windows. Considering both repair times as well as travel times, we show that the optimal scheduling problem with multiple crews, each with their own time budget, can be recast in terms of a cost constrained reward maximizing mTSP (traveling salesman problem) on doubly weighted graphs, where the objective is to maximize the aggregate reward earned during the upcoming restoration window, provided no crew violates its time budget and certain electrical continuity constraints are met. We propose a mixed integer linear programming (MILP) model for solving the above problem which is validated on standard IEEE PES test feeder networks.","sentences":["Severe weather events such as floods, hurricanes, earthquakes, and large wind or ice storms can cause extensive damage to electrical distribution networks, requiring a multi-day restoration effort.","Complicating the recovery process is the lack of complete and accurate information regarding the extent and locations of damages, at least during the initial part of the recovery process.","These factors make workforce planning challenging.","In this paper, we adopt a rolling horizon restoration framework whereby repairs are planned for adjustable finite length restoration windows.","Considering both repair times as well as travel times, we show that the optimal scheduling problem with multiple crews, each with their own time budget, can be recast in terms of a cost constrained reward maximizing mTSP (traveling salesman problem) on doubly weighted graphs, where the objective is to maximize the aggregate reward earned during the upcoming restoration window, provided no crew violates its time budget and certain electrical continuity constraints are met.","We propose a mixed integer linear programming (MILP) model for solving the above problem which is validated on standard IEEE PES test feeder networks."],"url":"http://arxiv.org/abs/2404.03197v1","category":"eess.SY"}
{"created":"2024-04-04 04:30:38","title":"Reservoir Sampling over Joins","abstract":"Sampling over joins is a fundamental task in large-scale data analytics. Instead of computing the full join results, which could be massive, a uniform sample of the join results would suffice for many purposes, such as answering analytical queries or training machine learning models. In this paper, we study the problem of how to maintain a random sample over joins while the tuples are streaming in. Without the join, this problem can be solved by some simple and classical reservoir sampling algorithms. However, the join operator makes the problem significantly harder, as the join size can be polynomially larger than the input. We present a new algorithm for this problem that achieves a near-linear complexity. The key technical components are a generalized reservoir sampling algorithm that supports a predicate, and a dynamic index for sampling over joins. We also conduct extensive experiments on both graph and relational data over various join queries, and the experimental results demonstrate significant performance improvement over the state of the art.","sentences":["Sampling over joins is a fundamental task in large-scale data analytics.","Instead of computing the full join results, which could be massive, a uniform sample of the join results would suffice for many purposes, such as answering analytical queries or training machine learning models.","In this paper, we study the problem of how to maintain a random sample over joins while the tuples are streaming in.","Without the join, this problem can be solved by some simple and classical reservoir sampling algorithms.","However, the join operator makes the problem significantly harder, as the join size can be polynomially larger than the input.","We present a new algorithm for this problem that achieves a near-linear complexity.","The key technical components are a generalized reservoir sampling algorithm that supports a predicate, and a dynamic index for sampling over joins.","We also conduct extensive experiments on both graph and relational data over various join queries, and the experimental results demonstrate significant performance improvement over the state of the art."],"url":"http://arxiv.org/abs/2404.03194v1","category":"cs.DB"}
{"created":"2024-04-04 04:25:01","title":"Foundation of Floer homotopy theory I: Flow categories","abstract":"We construct a stable infinity category with objects flow categories and morphisms flow bimodules; our construction has many flavors, related to a choice of bordism theory, and we discuss in particular framed bordism and the bordism theory of complex oriented derived orbifolds. In this setup, the construction of homotopy types associated to Floer-theoretic data is immediate: the moduli spaces of solutions to Floer's equation assemble into a flow category with respect to the appropriate bordism theory, and the associated Floer homotopy types arise as suitable mapping spectra in this category. The definition of these mapping spectra is sufficiently explicit to allow a direct interpretation of the Floer homotopy groups as Floer bordism groups. In the setting of framed bordism, we show that the category we construct is a model for the category of spectra. We implement the construction of Floer homotopy types in this new formalism for the case of Hamiltonian Floer theory.","sentences":["We construct a stable infinity category with objects flow categories and morphisms flow bimodules; our construction has many flavors, related to a choice of bordism theory, and we discuss in particular framed bordism and the bordism theory of complex oriented derived orbifolds.","In this setup, the construction of homotopy types associated to Floer-theoretic data is immediate: the moduli spaces of solutions to Floer's equation assemble into a flow category with respect to the appropriate bordism theory, and the associated Floer homotopy types arise as suitable mapping spectra in this category.","The definition of these mapping spectra is sufficiently explicit to allow a direct interpretation of the Floer homotopy groups as Floer bordism groups.","In the setting of framed bordism, we show that the category we construct is a model for the category of spectra.","We implement the construction of Floer homotopy types in this new formalism for the case of Hamiltonian Floer theory."],"url":"http://arxiv.org/abs/2404.03193v1","category":"math.SG"}
{"created":"2024-04-04 04:02:05","title":"RAnGE: Reachability Analysis for Guaranteed Ergodicity","abstract":"This paper investigates performance guarantees on coverage-based ergodic exploration methods in environments containing disturbances. Ergodic exploration methods generate trajectories for autonomous robots such that time spent in an area is proportional to the utility of exploring in the area. However, providing formal performance guarantees for ergodic exploration methods is still an open challenge due to the complexities in the problem formulation. In this work, we propose to formulate ergodic search as a differential game, in which a controller and external disturbance force seek to minimize and maximize the ergodic metric, respectively. Through an extended-state Bolza-form transform of the ergodic problem, we demonstrate it is possible to use techniques from reachability analysis to solve for optimal controllers that guarantee coverage and are robust against disturbances. Our approach leverages neural-network based methods to obtain approximate value function solutions for reachability problems that mitigate the increased computational scaling due to the extended state. As a result, we are able to compute continuous value functions for the ergodic exploration problem and provide performance guarantees for coverage under disturbances. Simulated and experimental results demonstrate the efficacy of our approach to generate robust ergodic trajectories for search and exploration with external disturbance force.","sentences":["This paper investigates performance guarantees on coverage-based ergodic exploration methods in environments containing disturbances.","Ergodic exploration methods generate trajectories for autonomous robots such that time spent in an area is proportional to the utility of exploring in the area.","However, providing formal performance guarantees for ergodic exploration methods is still an open challenge due to the complexities in the problem formulation.","In this work, we propose to formulate ergodic search as a differential game, in which a controller and external disturbance force seek to minimize and maximize the ergodic metric, respectively.","Through an extended-state Bolza-form transform of the ergodic problem, we demonstrate it is possible to use techniques from reachability analysis to solve for optimal controllers that guarantee coverage and are robust against disturbances.","Our approach leverages neural-network based methods to obtain approximate value function solutions for reachability problems that mitigate the increased computational scaling due to the extended state.","As a result, we are able to compute continuous value functions for the ergodic exploration problem and provide performance guarantees for coverage under disturbances.","Simulated and experimental results demonstrate the efficacy of our approach to generate robust ergodic trajectories for search and exploration with external disturbance force."],"url":"http://arxiv.org/abs/2404.03186v1","category":"cs.RO"}
{"created":"2024-04-04 03:57:08","title":"$\\frac{5}{2}$ fractional quantum Hall state in GaAs with Landau level mixing","abstract":"The Landau level mixing is the key in understanding the mysterious 5/2 fractional quantum Hall effect in GaAs quantum well. Theoretical calculations with and without Landau level mixing show striking differences. However, the way to deal with the considerable strong Landau level mixing in GaAs is still unsatisfactory. We develop a method combining the screening and the perturbation theories to study the nature of the 5/2 fractional quantum Hall effect in GaAs efficiently. The screening which has been succeed in explaining ZnO systems integrates out the low-energy Landau levels close to the related Landau level, while the other high-energy Landau levels are integrated out by the perturbation theory. We find that the ground states still hold the quasi-triplet degeneracy which implies the Pfaffian nature of the system. Furthermore, the particle-hole symmetry is only weakly violated since the particle-hole parity is close to unity. We propose that the ground state can be described by the two-component trial state cluster constructed by the superposition of the Pfaffian and anti-Pfaffian states with varied weights depending on the external conditions. In the experimental environment the two weights are close, corresponding that its thermal conductance slightly biased from 2.5 quanta can be understood straightforwardly.","sentences":["The Landau level mixing is the key in understanding the mysterious 5/2 fractional quantum Hall effect in GaAs quantum well.","Theoretical calculations with and without Landau level mixing show striking differences.","However, the way to deal with the considerable strong Landau level mixing in GaAs is still unsatisfactory.","We develop a method combining the screening and the perturbation theories to study the nature of the 5/2 fractional quantum Hall effect in GaAs efficiently.","The screening which has been succeed in explaining ZnO systems integrates out the low-energy Landau levels close to the related Landau level, while the other high-energy Landau levels are integrated out by the perturbation theory.","We find that the ground states still hold the quasi-triplet degeneracy which implies the Pfaffian nature of the system.","Furthermore, the particle-hole symmetry is only weakly violated since the particle-hole parity is close to unity.","We propose that the ground state can be described by the two-component trial state cluster constructed by the superposition of the Pfaffian and anti-Pfaffian states with varied weights depending on the external conditions.","In the experimental environment the two weights are close, corresponding that its thermal conductance slightly biased from 2.5 quanta can be understood straightforwardly."],"url":"http://arxiv.org/abs/2404.03185v1","category":"cond-mat.str-el"}
{"created":"2024-04-04 03:28:44","title":"Berezinskii-Kosterlitz-Thouless transitions in a ferromagnetic superfluid: effects of axial magnetization","abstract":"An easy-plane ferromagnetic spin-1 Bose gas undergoes two Berezinskii-Kosterlitz-Thouless (BKT) transitions, associated with mass and spin superfluidity respectively. We study the effect of axial magnetization on the superfluid properties of this system. We find that nonzero axial magnetization couples mass and spin superflow, via a mechanism analogous to the Andreev-Bashkin effect present in two-component superfluids. With sufficiently large axial magnetization mass and spin superfluidity arise simultaneously. The cross-over to this phase provides a finite-temperature generalization of the zero-temperature broken-axisymmetric to easy-axis transition. We present analytic relations connecting mass and spin superfluidity with experimentally observable coherence of the three spinor components and local magnetization.","sentences":["An easy-plane ferromagnetic spin-1","Bose gas undergoes two Berezinskii-Kosterlitz-Thouless (BKT) transitions, associated with mass and spin superfluidity respectively.","We study the effect of axial magnetization on the superfluid properties of this system.","We find that nonzero axial magnetization couples mass and spin superflow, via a mechanism analogous to the Andreev-Bashkin effect present in two-component superfluids.","With sufficiently large axial magnetization mass and spin superfluidity arise simultaneously.","The cross-over to this phase provides a finite-temperature generalization of the zero-temperature broken-axisymmetric to easy-axis transition.","We present analytic relations connecting mass and spin superfluidity with experimentally observable coherence of the three spinor components and local magnetization."],"url":"http://arxiv.org/abs/2404.03178v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 03:14:52","title":"Centimeter-Scale Achromatic Hybrid Metalens Design: A New Paradigm Based on Differentiable Ray Tracing in the Visible Spectrum","abstract":"Single metalenses are limited by their physical constraints, precluding themselves from achieving high numerical aperture across a wide visible spectral band in large-aperture applications. A hybrid system that integrates a metalens with a refractive lens can address this issue, yet previous designs lacked sufficient flexibility. Here, by reanalyzing the generalized Snell's law, we introduce a new paradigm for the hybrid metalens design based on differentiable ray tracing. Through joint optimization of the phase distribution of the metalens and refractive lens parameters, our system achieves achromatic performance within the broad spectral range of 440-700 nm, with an aperture of 1 cm and an f-number of 1.4. Owing to the differentiable nature of the proposed system, it can be seamlessly integrated as the optical front-end into any differentiable computational imaging system. Our system offers unprecedented opportunities for the advancement of metalenses in innovative optical design and computational imaging domains.","sentences":["Single metalenses are limited by their physical constraints, precluding themselves from achieving high numerical aperture across a wide visible spectral band in large-aperture applications.","A hybrid system that integrates a metalens with a refractive lens can address this issue, yet previous designs lacked sufficient flexibility.","Here, by reanalyzing the generalized Snell's law, we introduce a new paradigm for the hybrid metalens design based on differentiable ray tracing.","Through joint optimization of the phase distribution of the metalens and refractive lens parameters, our system achieves achromatic performance within the broad spectral range of 440-700 nm, with an aperture of 1 cm and an f-number of 1.4.","Owing to the differentiable nature of the proposed system, it can be seamlessly integrated as the optical front-end into any differentiable computational imaging system.","Our system offers unprecedented opportunities for the advancement of metalenses in innovative optical design and computational imaging domains."],"url":"http://arxiv.org/abs/2404.03173v1","category":"physics.optics"}
{"created":"2024-04-04 03:11:24","title":"SEPE-SQED: Symbolic Quick Error Detection by Semantically Equivalent Program Execution","abstract":"Symbolic quick error detection (SQED) has greatly improved efficiency in formal chip verification. However, it has a limitation in detecting single-instruction bugs due to its reliance on the self-consistency property. To address this, we propose a new variant called symbolic quick error detection by semantically equivalent program execution (SEPE-SQED), which utilizes program synthesis techniques to find sequences with equivalent meanings to original instructions. SEPE-SQED effectively detects single-instruction bugs by differentiating their impact on the original instruction and its semantically equivalent program (instruction sequence). To manage the search space associated with program synthesis, we introduce the CEGIS based on the highest priority first algorithm. The experimental results show that our proposed CEGIS approach improves the speed of generating the desired set of equivalent programs by 50% in time compared to previous methods. Compared to SQED, SEPE-SQED offers a wider variety of instruction combinations and can provide a shorter trace for triggering bugs in certain scenarios.","sentences":["Symbolic quick error detection (SQED) has greatly improved efficiency in formal chip verification.","However, it has a limitation in detecting single-instruction bugs due to its reliance on the self-consistency property.","To address this, we propose a new variant called symbolic quick error detection by semantically equivalent program execution (SEPE-SQED), which utilizes program synthesis techniques to find sequences with equivalent meanings to original instructions.","SEPE-SQED effectively detects single-instruction bugs by differentiating their impact on the original instruction and its semantically equivalent program (instruction sequence).","To manage the search space associated with program synthesis, we introduce the CEGIS based on the highest priority first algorithm.","The experimental results show that our proposed CEGIS approach improves the speed of generating the desired set of equivalent programs by 50% in time compared to previous methods.","Compared to SQED, SEPE-SQED offers a wider variety of instruction combinations and can provide a shorter trace for triggering bugs in certain scenarios."],"url":"http://arxiv.org/abs/2404.03172v1","category":"cs.SE"}
{"created":"2024-04-04 02:55:31","title":"Maximal Entropy Measures for Non-Accessible Topological Skew Products","abstract":"In this paper we establish a dichotomy for the ergodic measures of maximal entropy for partially hyperbolic diffeomorphisms with one-dimensional compact center leaves which are virtually skew products over (transitive) Anosov homeomorphism. We prove that if the whole manifold is the unique minimal invariant set saturated by unstable foliation, then either there exists a unique measure of maximal entropy which is non-hyperbolic or there are exactly two hyperbolic ergodic measures of maximal entropy.","sentences":["In this paper we establish a dichotomy for the ergodic measures of maximal entropy for partially hyperbolic diffeomorphisms with one-dimensional compact center leaves which are virtually skew products over (transitive) Anosov homeomorphism.","We prove that if the whole manifold is the unique minimal invariant set saturated by unstable foliation, then either there exists a unique measure of maximal entropy which is non-hyperbolic or there are exactly two hyperbolic ergodic measures of maximal entropy."],"url":"http://arxiv.org/abs/2404.03169v1","category":"math.DS"}
{"created":"2024-04-04 02:30:51","title":"LTRDetector: Exploring Long-Term Relationship for Advanced Persistent Threats Detection","abstract":"Advanced Persistent Threat (APT) is challenging to detect due to prolonged duration, infrequent occurrence, and adept concealment techniques. Existing approaches primarily concentrate on the observable traits of attack behaviors, neglecting the intricate relationships formed throughout the persistent attack lifecycle. Thus, we present an innovative APT detection framework named LTRDetector, implementing an end-to-end holistic operation. LTRDetector employs an innovative graph embedding technique to retain comprehensive contextual information, then derives long-term features from these embedded provenance graphs. During the process, we compress the data of the system provenance graph for effective feature learning. Furthermore, in order to detect attacks conducted by using zero-day exploits, we captured the system's regular behavior and detects abnormal activities without relying on predefined attack signatures. We also conducted extensive evaluations using five prominent datasets, the efficacy evaluation of which underscores the superiority of LTRDetector compared to existing state-of-the-art techniques.","sentences":["Advanced Persistent Threat (APT) is challenging to detect due to prolonged duration, infrequent occurrence, and adept concealment techniques.","Existing approaches primarily concentrate on the observable traits of attack behaviors, neglecting the intricate relationships formed throughout the persistent attack lifecycle.","Thus, we present an innovative APT detection framework named LTRDetector, implementing an end-to-end holistic operation.","LTRDetector employs an innovative graph embedding technique to retain comprehensive contextual information, then derives long-term features from these embedded provenance graphs.","During the process, we compress the data of the system provenance graph for effective feature learning.","Furthermore, in order to detect attacks conducted by using zero-day exploits, we captured the system's regular behavior and detects abnormal activities without relying on predefined attack signatures.","We also conducted extensive evaluations using five prominent datasets, the efficacy evaluation of which underscores the superiority of LTRDetector compared to existing state-of-the-art techniques."],"url":"http://arxiv.org/abs/2404.03162v1","category":"cs.CR"}
{"created":"2024-04-04 02:15:16","title":"HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud","abstract":"Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff.","sentences":["Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications.","Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames.","Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality.","However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization.","Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds.","In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition.","Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets.","Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff."],"url":"http://arxiv.org/abs/2404.03159v1","category":"cs.CV"}
{"created":"2024-04-04 02:12:36","title":"Stabilization in two-species chemotaxis systems with singular sensitivity and Lotka-Volterra competitive kinetics","abstract":"The current paper is concerned with the stabilization in the following parabolic-parabolic-elliptic chemotaxis system with singular sensitivity and Lotka-Volterra competitive kinetics, \\begin{equation} \\begin{cases} u_t=\\Delta u-\\chi_1 \\nabla\\cdot (\\frac{u}{w} \\nabla w)+u(a_1-b_1u-c_1v) ,\\quad &x\\in \\Omega\\cr v_t=\\Delta v-\\chi_2 \\nabla\\cdot (\\frac{v}{w} \\nabla w)+v(a_2-b_2v-c_2u),\\quad &x\\in \\Omega\\cr 0=\\Delta w-\\mu w +\\nu u+ \\lambda v,\\quad &x\\in \\Omega \\cr \\frac{\\partial u}{\\partial n}=\\frac{\\partial v}{\\partial n}=\\frac{\\partial w}{\\partial n}=0,\\quad &x\\in\\partial\\Omega, \\end{cases} \\end{equation} where $\\Omega \\subset \\mathbb{R}^N$ is a bounded smooth domain, and $\\chi_i,a_i, b_i, c_i$ ($i=1,2$) and $\\mu,\\, \\nu, \\, \\lambda$ are positive constants. In [25], we proved that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0+v_0\\not \\equiv 0$, (0.1) has a unique globally defined classical solution provided that $\\min\\{a_1,a_2\\}$ is large relative to $\\chi_1,\\chi_2$, and $u_0+v_0$ is not small in the case that $(\\chi_1-\\chi_2)^2\\le \\max\\{4\\chi_1,4\\chi_2\\}$ and $u_0+v_0$ is neither small nor big in the case that $(\\chi_1-\\chi_2)^2>\\max\\{4\\chi_1,4\\chi_2\\}$. In this paper, we proved that (0.1) has a unique positive constant solution $(u^*,v^*,w^*)$, where $$ u^*=\\frac{a_1b_2-c_1a_2}{b_1b_2-c_1c_2},\\quad v^*=\\frac{b_1a_2-a_1c_2}{b_1b_2-c_1c_2}, \\quad w^*=\\frac{\\nu}{\\mu}u^*+\\frac{\\lambda}{\\mu} v^*. $$ We obtain some explicit conditions on $\\chi_1,\\chi_2$ which ensure that the positive constant solution $(u^*,v^*,w^*)$ is globally stable in the sense that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0\\not \\equiv 0$ and $v_0\\not \\equiv 0$, $$ \\lim_{t\\to\\infty}\\Big(\\|u(t,\\cdot;u_0,v_0)-u^*\\|_\\infty +\\|v(t,\\cdot;u_0,v_0)-v^*\\|_\\infty+\\|w(t,\\cdot;u_0,v_0)-w^*\\|_\\infty\\Big)=0. $$","sentences":["The current paper is concerned with the stabilization in the following parabolic-parabolic-elliptic chemotaxis system with singular sensitivity and Lotka-Volterra competitive kinetics, \\begin{equation} \\begin{cases} u_t=\\Delta u-\\chi_1 \\nabla\\cdot (\\frac{u}{w} \\nabla w)+u(a_1-b_1u-c_1v) ,\\quad &x\\in \\Omega\\cr v_t=\\Delta v-\\chi_2 \\nabla\\cdot (\\frac{v}{w} \\nabla w)+v(a_2-b_2v-c_2u),\\quad &x\\in \\Omega\\cr 0=\\Delta w-\\mu w +\\nu u+","\\lambda v,\\quad &x\\in \\Omega","\\cr \\frac{\\partial u}{\\partial n}=\\frac{\\partial v}{\\partial n}=\\frac{\\partial w}{\\partial n}=0,\\quad &x\\in\\partial\\Omega, \\end{cases} \\end{equation} where $\\Omega \\subset \\mathbb{R}^N$ is a bounded smooth domain, and $\\chi_i,a_i, b_i, c_i$ ($i=1,2$) and $\\mu,\\, \\nu, \\, \\lambda$ are positive constants.","In [25], we proved that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0+v_0\\not \\equiv 0$, (0.1) has a unique globally defined classical solution provided that $\\min\\{a_1,a_2\\}$ is large relative to $\\chi_1,\\chi_2$, and $u_0+v_0$ is not small in the case that $(\\chi_1-\\chi_2)^2\\le \\max\\{4\\chi_1,4\\chi_2\\}$ and $u_0+v_0$ is neither small nor big in the case that $(\\chi_1-\\chi_2)^2>\\max\\{4\\chi_1,4\\chi_2\\}$. In this paper, we proved that (0.1) has a unique positive constant solution $(u^*,v^*,w^*)$, where $$ u^*=\\frac{a_1b_2-c_1a_2}{b_1b_2-c_1c_2},\\quad v^*=\\frac{b_1a_2-a_1c_2}{b_1b_2-c_1c_2}, \\quad w^*=\\frac{\\nu}{\\mu}u^*+\\frac{\\lambda}{\\mu} v^*.","$$ We obtain some explicit conditions on $\\chi_1,\\chi_2$ which ensure that the positive constant solution $(u^*,v^*,w^*)$ is globally stable in the sense that for any given nonnegative initial data $u_0,v_0\\in C^0(\\bar\\Omega)$ with $u_0\\not \\equiv 0$ and $v_0\\not \\equiv 0$, $$ \\lim_{t\\to\\infty}\\Big(\\|u(t,\\cdot;u_0,v_0)-u^*\\|_\\infty +\\|v(t,\\cdot;u_0,v_0)-v^*\\|_\\infty+\\|w(t,\\cdot;u_0,v_0)-w^*\\|_\\infty\\Big)=0.","$$"],"url":"http://arxiv.org/abs/2404.03158v1","category":"math.AP"}
{"created":"2024-04-04 02:09:15","title":"Irreducible symplectic varieties via relative Prym varieties","abstract":"Generalizing work of Markushevich--Tikhomirov and Arbarello--Sacc\\`a--Ferretti, we use relative Prym varieties to construct Lagrangian fibered symplectic varieties in infinitely many dimensions. We then give criteria for when the construction yields primitive symplectic varieties, respectively, irreducible symplectic varieties. The starting point of the construction is a K3 surface endowed with an anti-symplectic involution and an effective linear system on the quotient surface. We give sufficient conditions on the linear system to ensure that the relative Prym varieties satisfy the criteria above. As a consequence, we produce infinite series of irreducible symplectic varieties.","sentences":["Generalizing work of Markushevich--Tikhomirov and Arbarello--Sacc\\`a--Ferretti, we use relative Prym varieties to construct Lagrangian fibered symplectic varieties in infinitely many dimensions.","We then give criteria for when the construction yields primitive symplectic varieties, respectively, irreducible symplectic varieties.","The starting point of the construction is a K3 surface endowed with an anti-symplectic involution and an effective linear system on the quotient surface.","We give sufficient conditions on the linear system to ensure that the relative Prym varieties satisfy the criteria above.","As a consequence, we produce infinite series of irreducible symplectic varieties."],"url":"http://arxiv.org/abs/2404.03157v1","category":"math.AG"}
{"created":"2024-04-04 02:09:12","title":"The Dynamics of Debris Disk Creation in Neutron Star Mergers","abstract":"The detection of GW170817/AT2017gfo inaugurated an era of multimessenger astrophysics, in which gravitational wave and multiwavelength photon observations complement one another to provide unique insight on astrophysical systems. A broad theoretical consensus exists in which the photon phenomenology of neutron star mergers largely rests upon the evolution of the small amount of matter left on bound orbits around the black hole or massive neutron star remaining after the merger. Because this accretion disk is far from inflow equilibrium, its subsequent evolution depends very strongly on its initial state, yet very little is known about how this state is determined. Using both snapshot and tracer particle data from a numerical relativity/MHD simulation of an equal-mass neutron star merger that collapses to a black hole, we show how gravitational forces arising in a non-axisymmetric, dynamical spacetime supplement hydrodynamical effects in shaping the initial structure of the bound debris disk. The work done by hydrodynamical forces is ${\\sim}10$ times greater than that due to time-dependent gravity. Although gravitational torques prior to remnant relaxation are an order of magnitude larger than hydrodynamical torques, their intrinsic sign symmetry leads to strong cancellation; as a result, hydrodynamical and gravitational torques have comparable effect. We also show that the debris disk's initial specific angular momentum distribution is sharply peaked at roughly the specific angular momentum of the merged neutron star's outer layers, a few $r_g c$, and identify the regulating mechanism.","sentences":["The detection of GW170817/AT2017gfo inaugurated an era of multimessenger astrophysics, in which gravitational wave and multiwavelength photon observations complement one another to provide unique insight on astrophysical systems.","A broad theoretical consensus exists in which the photon phenomenology of neutron star mergers largely rests upon the evolution of the small amount of matter left on bound orbits around the black hole or massive neutron star remaining after the merger.","Because this accretion disk is far from inflow equilibrium, its subsequent evolution depends very strongly on its initial state, yet very little is known about how this state is determined.","Using both snapshot and tracer particle data from a numerical relativity/MHD simulation of an equal-mass neutron star merger that collapses to a black hole, we show how gravitational forces arising in a non-axisymmetric, dynamical spacetime supplement hydrodynamical effects in shaping the initial structure of the bound debris disk.","The work done by hydrodynamical forces is ${\\sim}10$ times greater than that due to time-dependent gravity.","Although gravitational torques prior to remnant relaxation are an order of magnitude larger than hydrodynamical torques, their intrinsic sign symmetry leads to strong cancellation; as a result, hydrodynamical and gravitational torques have comparable effect.","We also show that the debris disk's initial specific angular momentum distribution is sharply peaked at roughly the specific angular momentum of the merged neutron star's outer layers, a few $r_g c$, and identify the regulating mechanism."],"url":"http://arxiv.org/abs/2404.03156v1","category":"astro-ph.HE"}
{"created":"2024-04-04 02:07:15","title":"TEGRA -- Scaling Up Terascale Graph Processing with Disaggregated Computing","abstract":"Graphs are essential for representing relationships in various domains, driving modern AI applications such as graph analytics and neural networks across science, engineering, cybersecurity, transportation, and economics. However, the size of modern graphs are rapidly expanding, posing challenges for traditional CPUs and GPUs in meeting real-time processing demands. As a result, hardware accelerators for graph processing have been proposed. However, the largest graphs that can be handled by these systems is still modest often targeting Twitter graph(1.4B edges approximately). This paper aims to address this limitation by developing a graph accelerator capable of terascale graph processing. Scale out architectures, architectures where nodes are replicated to expand to larger datasets, are natural for handling larger graphs. We argue that this approach is not appropriate for very large-scale graphs because it leads to under utilization of both memory resources and compute resources. Additionally, vertex and edge processing have different access patterns. Communication overheads also pose further challenges in designing scalable architectures. To overcome these issues, this paper proposes TEGRA, a scale-up architecture for terascale graph processing. TEGRA leverages a composable computing system with disaggregated resources and a communication architecture inspired by Active Messages. By employing direct communication between cores and optimizing memory interconnect utilization, TEGRA effectively reduces communication overhead and improves resource utilization, therefore enabling efficient processing of terascale graphs.","sentences":["Graphs are essential for representing relationships in various domains, driving modern AI applications such as graph analytics and neural networks across science, engineering, cybersecurity, transportation, and economics.","However, the size of modern graphs are rapidly expanding, posing challenges for traditional CPUs and GPUs in meeting real-time processing demands.","As a result, hardware accelerators for graph processing have been proposed.","However, the largest graphs that can be handled by these systems is still modest often targeting Twitter graph(1.4B edges approximately).","This paper aims to address this limitation by developing a graph accelerator capable of terascale graph processing.","Scale out architectures, architectures where nodes are replicated to expand to larger datasets, are natural for handling larger graphs.","We argue that this approach is not appropriate for very large-scale graphs because it leads to under utilization of both memory resources and compute resources.","Additionally, vertex and edge processing have different access patterns.","Communication overheads also pose further challenges in designing scalable architectures.","To overcome these issues, this paper proposes TEGRA, a scale-up architecture for terascale graph processing.","TEGRA leverages a composable computing system with disaggregated resources and a communication architecture inspired by Active Messages.","By employing direct communication between cores and optimizing memory interconnect utilization, TEGRA effectively reduces communication overhead and improves resource utilization, therefore enabling efficient processing of terascale graphs."],"url":"http://arxiv.org/abs/2404.03155v1","category":"cs.ET"}
{"created":"2024-04-04 01:46:31","title":"Design and Evaluation of a Compact 3D End-effector Assistive Robot for Adaptive Arm Support","abstract":"We developed a 3D end-effector type of upper limb assistive robot, named as Assistive Robotic Arm Extender (ARAE), that provides transparency movement and adaptive arm support control to achieve home-based therapy and training in the real environment. The proposed system composes five degrees of freedom, including three active motors and two passive joints at the end-effector module. The core structure of the system is based on a parallel mechanism. The kinematic and dynamic modeling are illustrated in detail. The proposed adaptive arm support control framework calculates the compensated force based on the estimated human arm posture in 3D space. It firstly estimates human arm joint angles using two proposed methods: fixed torso and sagittal plane models without using external sensors such as IMUs, magnetic sensors, or depth cameras. The experiments were carried out to evaluate the performance of the two proposed angle estimation methods. Then, the estimated human joint angles were input into the human upper limb dynamics model to derive the required support force generated by the robot. The muscular activities were measured to evaluate the effects of the proposed framework. The obvious reduction of muscular activities was exhibited when participants were tested with the ARAE under an adaptive arm gravity compensation control framework. The overall results suggest that the ARAE system, when combined with the proposed control framework, has the potential to offer adaptive arm support. This integration could enable effective training with Activities of Daily Living (ADLs) and interaction with real environments.","sentences":["We developed a 3D end-effector type of upper limb assistive robot, named as Assistive Robotic Arm Extender (ARAE), that provides transparency movement and adaptive arm support control to achieve home-based therapy and training in the real environment.","The proposed system composes five degrees of freedom, including three active motors and two passive joints at the end-effector module.","The core structure of the system is based on a parallel mechanism.","The kinematic and dynamic modeling are illustrated in detail.","The proposed adaptive arm support control framework calculates the compensated force based on the estimated human arm posture in 3D space.","It firstly estimates human arm joint angles using two proposed methods: fixed torso and sagittal plane models without using external sensors such as IMUs, magnetic sensors, or depth cameras.","The experiments were carried out to evaluate the performance of the two proposed angle estimation methods.","Then, the estimated human joint angles were input into the human upper limb dynamics model to derive the required support force generated by the robot.","The muscular activities were measured to evaluate the effects of the proposed framework.","The obvious reduction of muscular activities was exhibited when participants were tested with the ARAE under an adaptive arm gravity compensation control framework.","The overall results suggest that the ARAE system, when combined with the proposed control framework, has the potential to offer adaptive arm support.","This integration could enable effective training with Activities of Daily Living (ADLs) and interaction with real environments."],"url":"http://arxiv.org/abs/2404.03149v1","category":"cs.RO"}
{"created":"2024-04-04 01:18:11","title":"Robust Partitioning and Operation for Maximal Uncertain-Load Delivery in Distribution Grids","abstract":"To mitigate the vulnerability of distribution grids to severe weather events, some electric utilities use preemptive de-energization as the primary line of defense, causing significant power outages. In such instances, networked microgrids could improve resiliency and maximize load delivery, though the modeling of three-phase unbalanced network physics and computational complexity pose challenges. These challenges are further exacerbated by an increased penetration of uncertain loads. In this paper, we present a two-stage mixed-integer robust optimization problem that configures and operates networked microgrids, and is guaranteed to be robust and feasible to all realizations of loads within a specified uncertainty set, while maximizing load delivery. To solve this problem, we propose a cutting-plane algorithm, with convergence guarantees, which approximates a convex recourse function with sub-gradient cuts. Finally, we provide a detailed case study on the IEEE 37-bus test system to demonstrate the economic benefits of networking microgrids to maximize uncertain-load delivery.","sentences":["To mitigate the vulnerability of distribution grids to severe weather events, some electric utilities use preemptive de-energization as the primary line of defense, causing significant power outages.","In such instances, networked microgrids could improve resiliency and maximize load delivery, though the modeling of three-phase unbalanced network physics and computational complexity pose challenges.","These challenges are further exacerbated by an increased penetration of uncertain loads.","In this paper, we present a two-stage mixed-integer robust optimization problem that configures and operates networked microgrids, and is guaranteed to be robust and feasible to all realizations of loads within a specified uncertainty set, while maximizing load delivery.","To solve this problem, we propose a cutting-plane algorithm, with convergence guarantees, which approximates a convex recourse function with sub-gradient cuts.","Finally, we provide a detailed case study on the IEEE 37-bus test system to demonstrate the economic benefits of networking microgrids to maximize uncertain-load delivery."],"url":"http://arxiv.org/abs/2404.03137v1","category":"math.OC"}
{"created":"2024-04-04 01:16:49","title":"Promatch: Extending the Reach of Real-Time Quantum Error Correction with Adaptive Predecoding","abstract":"Fault-tolerant quantum computing relies on Quantum Error Correction, which encodes logical qubits into data and parity qubits. Error decoding is the process of translating the measured parity bits into types and locations of errors. To prevent a backlog of errors, error decoding must be performed in real-time. Minimum Weight Perfect Matching (MWPM) is an accurate decoding algorithm for surface code, and recent research has demonstrated real-time implementations of MWPM (RT-MWPM) for a distance of up to 9. Unfortunately, beyond d=9, the number of flipped parity bits in the syndrome, referred to as the Hamming weight of the syndrome, exceeds the capabilities of existing RT-MWPM decoders. In this work, our goal is to enable larger distance RT-MWPM decoders by using adaptive predecoding that converts high Hamming weight syndromes into low Hamming weight syndromes, which are accurately decoded by the RT-MWPM decoder. An effective predecoder must balance both accuracy and coverage. In this paper, we propose Promatch, a real-time adaptive predecoder that predecodes both simple and complex patterns using a locality-aware, greedy approach. Our approach ensures two crucial factors: 1) high accuracy in prematching flipped bits, ensuring that the decoding accuracy is not hampered by the predecoder, and 2) enough coverage adjusted based on the main decoder's capability given the time constraints. Promatch represents the first real-time decoding framework capable of decoding surface codes of distances 11 and 13, achieving an LER of $2.6\\times 10^{-14}$ for distance 13. Moreover, we demonstrate that running Promatch concurrently with the recently proposed Astrea-G achieves LER equivalent to MWPM LER, $3.4\\times10^{-15}$, for distance 13, representing the first real-time accurate decoder for up-to a distance of 13.","sentences":["Fault-tolerant quantum computing relies on Quantum Error Correction, which encodes logical qubits into data and parity qubits.","Error decoding is the process of translating the measured parity bits into types and locations of errors.","To prevent a backlog of errors, error decoding must be performed in real-time.","Minimum Weight Perfect Matching (MWPM) is an accurate decoding algorithm for surface code, and recent research has demonstrated real-time implementations of MWPM (RT-MWPM) for a distance of up to 9.","Unfortunately, beyond d=9, the number of flipped parity bits in the syndrome, referred to as the Hamming weight of the syndrome, exceeds the capabilities of existing RT-MWPM decoders.","In this work, our goal is to enable larger distance RT-MWPM decoders by using adaptive predecoding that converts high Hamming weight syndromes into low Hamming weight syndromes, which are accurately decoded by the RT-MWPM decoder.","An effective predecoder must balance both accuracy and coverage.","In this paper, we propose Promatch, a real-time adaptive predecoder that predecodes both simple and complex patterns using a locality-aware, greedy approach.","Our approach ensures two crucial factors: 1) high accuracy in prematching flipped bits, ensuring that the decoding accuracy is not hampered by the predecoder, and 2) enough coverage adjusted based on the main decoder's capability given the time constraints.","Promatch represents the first real-time decoding framework capable of decoding surface codes of distances 11 and 13, achieving an LER of $2.6\\times 10^{-14}$ for distance 13.","Moreover, we demonstrate that running Promatch concurrently with the recently proposed Astrea-G achieves LER equivalent to MWPM LER, $3.4\\times10^{-15}$, for distance 13, representing the first real-time accurate decoder for up-to a distance of 13."],"url":"http://arxiv.org/abs/2404.03136v1","category":"quant-ph"}
{"created":"2024-04-04 00:47:13","title":"Biodegradable Interactive Materials","abstract":"The sense of touch is fundamental to how we interact with the physical and digital world. Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects. In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties. Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data. We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties. We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods. Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes. We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors. We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces. We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach.","sentences":["The sense of touch is fundamental to how we interact with the physical and digital world.","Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects.","In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties.","Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data.","We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties.","We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods.","Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes.","We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors.","We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces.","We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach."],"url":"http://arxiv.org/abs/2404.03130v1","category":"cs.HC"}
{"created":"2024-04-04 00:44:58","title":"Performant Automatic Differentiation of Local Coupled Cluster Theories: Response Properties and Ab Initio Molecular Dynamics","abstract":"In this work, we introduce a differentiable implementation of the local natural orbital coupled cluster (LNOCC) method within the automatic differentiation framework of the PySCFAD package. The implementation is comprehensively tuned for enhanced performance, which enables the calculation of first-order static response properties on medium-sized molecular systems using coupled cluster theory with single, double, and perturbative triple excitations [CCSD(T)]. We evaluate the accuracy of our method by benchmarking it against the canonical CCSD(T) reference for nuclear gradients, dipole moments, and geometry optimizations. In addition, we demonstrate the possibility of property calculations for chemically interesting systems through the computation of bond orders and M\\\"ossbauer spectroscopy parameters for a [NiFe]-hydrogenase active site model, along with the simulation of infrared (IR) spectra via ab initio LNO-CC molecular dynamics for a protonated water hexamer.","sentences":["In this work, we introduce a differentiable implementation of the local natural orbital coupled cluster (LNOCC) method within the automatic differentiation framework of the PySCFAD package.","The implementation is comprehensively tuned for enhanced performance, which enables the calculation of first-order static response properties on medium-sized molecular systems using coupled cluster theory with single, double, and perturbative triple excitations [CCSD(T)].","We evaluate the accuracy of our method by benchmarking it against the canonical CCSD(T) reference for nuclear gradients, dipole moments, and geometry optimizations.","In addition, we demonstrate the possibility of property calculations for chemically interesting systems through the computation of bond orders and M\\\"ossbauer spectroscopy parameters for a [NiFe]-hydrogenase active site model, along with the simulation of infrared (IR) spectra via ab initio LNO-CC molecular dynamics for a protonated water hexamer."],"url":"http://arxiv.org/abs/2404.03129v1","category":"physics.chem-ph"}
{"created":"2024-04-03 23:59:59","title":"Utilizing Computer Vision for Continuous Monitoring of Vaccine Side Effects in Experimental Mice","abstract":"The demand for improved efficiency and accuracy in vaccine safety assessments is increasing. Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration. Traditional observation methods are labor-intensive and lack the capability for continuous monitoring. By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments. The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination. Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects. Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation.","sentences":["The demand for improved efficiency and accuracy in vaccine safety assessments is increasing.","Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration.","Traditional observation methods are labor-intensive and lack the capability for continuous monitoring.","By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments.","The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination.","Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects.","Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation."],"url":"http://arxiv.org/abs/2404.03121v1","category":"cs.CV"}
{"created":"2024-04-04 17:58:21","title":"Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation","abstract":"Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset. Code is available at https://github.com/heshuting555/DsHmp.","sentences":["Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues.","Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues.","However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception.","In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues.","In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension.","Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues.","Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales.","Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects.","These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset.","Code is available at https://github.com/heshuting555/DsHmp."],"url":"http://arxiv.org/abs/2404.03645v1","category":"cs.CV"}
{"created":"2024-04-04 17:55:41","title":"Cooperation between electron-phonon coupling and electronic interaction in bilayer nickelates La$_3$Ni$_2$O$_7$","abstract":"The recent observation of high-T$_c$ superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under pressure has garnered significant interests. While researches have predominantly focused on the role of electron-electron interactions in the superconducting mechanism, the impact of electron-phonon coupling (EPC) has remained elusive. In this work, we perform first-principles calculations to study the phonon spectrum and electron-phonon coupling within La$_3$Ni$_2$O$_7$ under pressure and explore of the interplay between EPC and electronic interactions on the superconductivity by employing functional renormalization group approach. Our calculations reveal that EPC alone is insufficient to trigger superconductivity in La$_3$Ni$_2$O$_7$ under pressure. We identify unique out-of-plane and in-plane breathing phonon modes which selectively couple with the Ni $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, showcasing an orbital-selective EPC. Within the bilayer two-orbital model, it is revealed that solely electronic interactions foster $s_{\\pm}$-wave pairing characterized by notable frustration in the band space, leading to a low transition temperature. Remarkably, we find that this out-of-plane EPC can act in concert with electronic interactions to promote the onsite and interlayer pairing in the $d_{z^2}$ orbital, partially releasing the pairing frustration and thus elevating T$_c$. In contrast, the inclusion of in-plane EPC only marginally affects the superconductivity, distinct from the cuprates. Potential experimental implications in La$_3$Ni$_2$O$_7$ are also discussed.","sentences":["The recent observation of high-T$_c$ superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under pressure has garnered significant interests.","While researches have predominantly focused on the role of electron-electron interactions in the superconducting mechanism, the impact of electron-phonon coupling (EPC) has remained elusive.","In this work, we perform first-principles calculations to study the phonon spectrum and electron-phonon coupling within La$_3$Ni$_2$O$_7$ under pressure and explore of the interplay between EPC and electronic interactions on the superconductivity by employing functional renormalization group approach.","Our calculations reveal that EPC alone is insufficient to trigger superconductivity in La$_3$Ni$_2$O$_7$ under pressure.","We identify unique out-of-plane and in-plane breathing phonon modes which selectively couple with the Ni $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, showcasing an orbital-selective EPC.","Within the bilayer two-orbital model, it is revealed that solely electronic interactions foster $s_{\\pm}$-wave pairing characterized by notable frustration in the band space, leading to a low transition temperature.","Remarkably, we find that this out-of-plane EPC can act in concert with electronic interactions to promote the onsite and interlayer pairing in the $d_{z^2}$ orbital, partially releasing the pairing frustration and thus elevating T$_c$.","In contrast, the inclusion of in-plane EPC only marginally affects the superconductivity, distinct from the cuprates.","Potential experimental implications in La$_3$Ni$_2$O$_7$ are also discussed."],"url":"http://arxiv.org/abs/2404.03638v1","category":"cond-mat.supr-con"}
{"created":"2024-04-04 17:32:37","title":"Fundamental inequalities for the iterated Fourier-cosine convolution with Gaussian weight and its application","abstract":"Derived from the results in (Math. Nachr., 283(12):1758-1770, 2010), in this paper, we devoted to studying the boundedness properties for Fourier-cosine convolution weighted by Gaussian functions via Young's type theorem and Saitoh's type inequality. New norm estimations in the weighted space are obtained and application of the corresponding class of convolutions in Fredholm's second kind of integral equation is discussed. The conditions for the solvability of this equation in $L_1$ space are also found, along with the analysis of an illustrative example, which exemplifies that the present object and method solve cases that are not under the conditions of previously known techniques.","sentences":["Derived from the results in (Math. Nachr.",", 283(12):1758-1770, 2010), in this paper, we devoted to studying the boundedness properties for Fourier-cosine convolution weighted by Gaussian functions via Young's type theorem and Saitoh's type inequality.","New norm estimations in the weighted space are obtained and application of the corresponding class of convolutions in Fredholm's second kind of integral equation is discussed.","The conditions for the solvability of this equation in $L_1$ space are also found, along with the analysis of an illustrative example, which exemplifies that the present object and method solve cases that are not under the conditions of previously known techniques."],"url":"http://arxiv.org/abs/2404.03609v1","category":"math.CA"}
{"created":"2024-04-04 17:25:30","title":"Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization","abstract":"We consider the problem of accurate quantization for language models, where both the weights and activations are uniformly quantized to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware. In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques. We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams. We then propose a simple strategy which regularizes a layer's inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization. We show that regularizing both the inputs and outputs is crucial for preventing a model's \"migrating\" the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult. When combined with weight PTQ, we show that our approach can obtain a W4A4 model that performs competitively to the standard-precision W16A16 baseline.","sentences":["We consider the problem of accurate quantization for language models, where both the weights and activations are uniformly quantized to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware.","In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques.","We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams.","We then propose a simple strategy which regularizes a layer's inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization.","We show that regularizing both the inputs and outputs is crucial for preventing a model's \"migrating\" the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult.","When combined with weight PTQ, we show that our approach can obtain a W4A4 model that performs competitively to the standard-precision W16A16 baseline."],"url":"http://arxiv.org/abs/2404.03605v1","category":"cs.LG"}
{"created":"2024-04-04 16:47:02","title":"Nuclear Matter Equation of State in the Brueckner-Hartree-Fock Approach and Standard Skyrme Energy-Density Functionals","abstract":"The equation of state of asymmetric nuclear matter as well as the neutron and proton effective masses and their partial-wave and spin-isospin decomposition are analyzed within the Brueckner--Hartree--Fock approach. Theoretical uncertainties for all these quantities are estimated by using several phase-shift-equivalent nucleon-nucleon forces together with two types of three-nucleon forces, phenomenological and microscopic. It is shown that the choice of the three-nucleon force plays an important role above saturation density, leading to different density dependencies of the energy per particle. These results are compared to the standard form of the Skyrme energy-density functional and we find that it is not possible to reproduce the BHF predictions in the $(S,T)$ channels in symmetric and neutron matter above saturation density, already at the level of the two-body interaction, and even more including the three-body interaction.","sentences":["The equation of state of asymmetric nuclear matter as well as the neutron and proton effective masses and their partial-wave and spin-isospin decomposition are analyzed within the Brueckner--Hartree--Fock approach.","Theoretical uncertainties for all these quantities are estimated by using several phase-shift-equivalent nucleon-nucleon forces together with two types of three-nucleon forces, phenomenological and microscopic.","It is shown that the choice of the three-nucleon force plays an important role above saturation density, leading to different density dependencies of the energy per particle.","These results are compared to the standard form of the Skyrme energy-density functional and we find that it is not possible to reproduce the BHF predictions in the $(S,T)$ channels in symmetric and neutron matter above saturation density, already at the level of the two-body interaction, and even more including the three-body interaction."],"url":"http://arxiv.org/abs/2404.03583v1","category":"nucl-th"}
{"created":"2024-04-04 16:38:40","title":"Enhanced mobility of quantum droplets in periodic lattices","abstract":"We predict that one- and two-dimensional self-bound quantum droplets, forming in Bose-Einstein condensates in the presence of Lee-Huang-Yang (LHY) quantum corrections to the mean-field energy, may demonstrate exceptional mobility in periodic optical lattices and that they may exhibit considerable displacements across the lattice, remaining dynamically stable, even under weak initial phase kicks imparted to them. Mobility properties of quantum droplets are determined by their internal structure and strongly depend on the number of particles in them. We find that due to the peculiar effect of the LHY quantum corrections, odd (i.e., on-site centered) and even (i.e., intersite-centered) one-dimensional quantum droplets feature alternating mobility and immobility bands closely corresponding to the regions, where translational perturbation mode is unstable and stable, respectively. This picture becomes even richer in two-dimensional case, where odd-odd, even-odd or even-even quantum droplets also feature alternating mobility and immobility domains, and where, surprisingly, the droplet may be mobile in one direction, but immobile in the orthogonal direction. We link changes in mobility properties with multiple intersections of energy $E(\\mu)$ and norm $N(\\mu)$ dependencies for droplets with different internal structure.","sentences":["We predict that one-","and two-dimensional self-bound quantum droplets, forming in Bose-Einstein condensates in the presence of Lee-Huang-Yang (LHY) quantum corrections to the mean-field energy, may demonstrate exceptional mobility in periodic optical lattices and that they may exhibit considerable displacements across the lattice, remaining dynamically stable, even under weak initial phase kicks imparted to them.","Mobility properties of quantum droplets are determined by their internal structure and strongly depend on the number of particles in them.","We find that due to the peculiar effect of the LHY quantum corrections, odd (i.e., on-site centered) and even (i.e., intersite-centered) one-dimensional quantum droplets feature alternating mobility and immobility bands closely corresponding to the regions, where translational perturbation mode is unstable and stable, respectively.","This picture becomes even richer in two-dimensional case, where odd-odd, even-odd or even-even quantum droplets also feature alternating mobility and immobility domains, and where, surprisingly, the droplet may be mobile in one direction, but immobile in the orthogonal direction.","We link changes in mobility properties with multiple intersections of energy $E(\\mu)$ and norm $N(\\mu)$ dependencies for droplets with different internal structure."],"url":"http://arxiv.org/abs/2404.03573v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 16:29:08","title":"The Cauchy problem for the nonlinear Schr\u00f6dinger equation with a convolution potential","abstract":"This paper investigates the nonlinear Schr\\\"{o}dinger equation with a singular convolution potential. It demonstrates the local well-posedness of this equation in a modified Sobolev space linked to the energy. Additionally, we derive conditions under which the solutions are uniformly bounded in the energy space. This finding is closely linked to the existence of standing waves for this equation.","sentences":["This paper investigates the nonlinear Schr\\\"{o}dinger equation with a singular convolution potential.","It demonstrates the local well-posedness of this equation in a modified Sobolev space linked to the energy.","Additionally, we derive conditions under which the solutions are uniformly bounded in the energy space.","This finding is closely linked to the existence of standing waves for this equation."],"url":"http://arxiv.org/abs/2404.03568v1","category":"math.AP"}
{"created":"2024-04-04 16:18:37","title":"EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German","abstract":"In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English. Compared to the original EASSE framework, EASSE-multi does not focus only on English. It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages. In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE. Further, we compare text simplification results when evaluating with different language or tokenization settings of the metrics. Based on this, we formulate recommendations on how to make the evaluation of (German) TS models more transparent and better comparable. The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at https://github.com/rstodden/easse-de.","sentences":["In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English.","Compared to the original EASSE framework, EASSE-multi does not focus only on English.","It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages.","In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE.","Further, we compare text simplification results when evaluating with different language or tokenization settings of the metrics.","Based on this, we formulate recommendations on how to make the evaluation of (German) TS models more transparent and better comparable.","The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at https://github.com/rstodden/easse-de."],"url":"http://arxiv.org/abs/2404.03563v1","category":"cs.CL"}
{"created":"2024-04-04 15:24:25","title":"Hub Network Design Problem with Capacity, Congestion and Heterogeneous Economies of Scale","abstract":"We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale. We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation. We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation. The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company.","sentences":["We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale.","We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation.","We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation.","The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company."],"url":"http://arxiv.org/abs/2404.03521v1","category":"math.OC"}
{"created":"2024-04-04 15:24:00","title":"Formal deformations of modular forms and multiple L-values","abstract":"We relate analytically defined deformations of modular curves and modular forms from the literature to motivic periods via cohomological descriptions of deformation theory. Leveraging cohomological vanishing results, we prove the existence and essential uniqueness of deformations, which we make constructive via established Lie algebraic arguments and a notion of formal logarithmic deformations. Further, we construct a canonical and a totally holomorphic canonical universal family of deformations of modular forms of all weights, which we obtain from the canonical cocycle associated with periods on the moduli space $\\mathcal{M}_{1,1}$. Our uniqueness statement shows that non-critical multiple $\\mathrm{L}$-values, which appear in our deformations but are a priori non-geometric, are genuinely linked to deformations. Our work thus suggests a new geometric perspective on them.","sentences":["We relate analytically defined deformations of modular curves and modular forms from the literature to motivic periods via cohomological descriptions of deformation theory.","Leveraging cohomological vanishing results, we prove the existence and essential uniqueness of deformations, which we make constructive via established Lie algebraic arguments and a notion of formal logarithmic deformations.","Further, we construct a canonical and a totally holomorphic canonical universal family of deformations of modular forms of all weights, which we obtain from the canonical cocycle associated with periods on the moduli space $\\mathcal{M}_{1,1}$. Our uniqueness statement shows that non-critical multiple $\\mathrm{L}$-values, which appear in our deformations but are a priori non-geometric, are genuinely linked to deformations.","Our work thus suggests a new geometric perspective on them."],"url":"http://arxiv.org/abs/2404.03519v1","category":"math.NT"}
{"created":"2024-04-04 15:23:14","title":"SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation","abstract":"Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. Our code is available at https://github.com/MartyrPenink/SDPose.","sentences":["Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE).","Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms.","Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts.","Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models.","To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters.","Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model.","Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs.","Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods.","Our code is available at https://github.com/MartyrPenink/SDPose."],"url":"http://arxiv.org/abs/2404.03518v1","category":"cs.CV"}
{"created":"2024-04-04 15:08:59","title":"Magnetic signals from oceanic tides: new satellite observations and applications","abstract":"Tidal flow of seawater across the Earth's magnetic field induces electric currents and magnetic fields within the ocean and solid Earth. The amplitude and phase of the induced fields depends on electrical properties of both the seawater and the solid Earth, thus can be used as a proxy to study seabed properties or potentially for monitoring long-term trends in the global ocean climatology. This paper presents new global oceanic tidal magnetic field models and their uncertainties for four tidal constituents, including $M_2, N_2, O_1$ and for the first time $Q_1$. Models are obtained through a robust least-squares analysis of magnetic field observations from the Swarm and CHAMP satellites using a specially designed data selection scheme. We compare the retrieved magnetic signals with several alternative models reported in the literature. Additionally, we validate them using a series of high-resolution global 3-D electromagnetic simulations and place constraints on the conductivity of sub-oceanic mantle for all tidal constituents, revealing an excellent agreement between all tidal constituents and the oceanic upper mantle structure.","sentences":["Tidal flow of seawater across the Earth's magnetic field induces electric currents and magnetic fields within the ocean and solid Earth.","The amplitude and phase of the induced fields depends on electrical properties of both the seawater and the solid Earth, thus can be used as a proxy to study seabed properties or potentially for monitoring long-term trends in the global ocean climatology.","This paper presents new global oceanic tidal magnetic field models and their uncertainties for four tidal constituents, including $M_2, N_2, O_1$ and for the first time $Q_1$. Models are obtained through a robust least-squares analysis of magnetic field observations from the Swarm and CHAMP satellites using a specially designed data selection scheme.","We compare the retrieved magnetic signals with several alternative models reported in the literature.","Additionally, we validate them using a series of high-resolution global 3-D electromagnetic simulations and place constraints on the conductivity of sub-oceanic mantle for all tidal constituents, revealing an excellent agreement between all tidal constituents and the oceanic upper mantle structure."],"url":"http://arxiv.org/abs/2404.03504v1","category":"physics.geo-ph"}
{"created":"2024-04-04 14:50:50","title":"About Test-time training for outlier detection","abstract":"In this paper, we introduce DOUST, our method applying test-time training for outlier detection, significantly improving the detection performance. After thoroughly evaluating our algorithm on common benchmark datasets, we discuss a common problem and show that it disappears with a large enough test set. Thus, we conclude that under reasonable conditions, our algorithm can reach almost supervised performance even when no labeled outliers are given.","sentences":["In this paper, we introduce DOUST, our method applying test-time training for outlier detection, significantly improving the detection performance.","After thoroughly evaluating our algorithm on common benchmark datasets, we discuss a common problem and show that it disappears with a large enough test set.","Thus, we conclude that under reasonable conditions, our algorithm can reach almost supervised performance even when no labeled outliers are given."],"url":"http://arxiv.org/abs/2404.03495v1","category":"cs.LG"}
{"created":"2024-04-04 14:39:07","title":"Combining exchangeable p-values","abstract":"Significant recent progress has been made on deriving combination rules that can take as input a set of arbitrarily dependent p-values, and produce as output a single valid p-value. Here, we show that under the assumption of exchangeability of the p-values, many of those rules can be improved (made more powerful). While this observation by itself has practical implications (for example, under repeated tests involving data splitting), it also has implications for combining arbitrarily dependent p-values, since the latter can be made exchangeable by applying a uniformly random permutation. In particular, we derive several simple randomized combination rules for arbitrarily dependent p-values that are more powerful than their deterministic counterparts. For example, we derive randomized and exchangeable improvements of well known p-value combination rules like \"twice the median\" and \"twice the average\", as well as geometric and harmonic means. The main technical advance is to show that all these combination rules can be obtained by calibrating the p-values to e-values (using an $\\alpha$-dependent calibrator), averaging those e-values, converting to a level $\\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests. The improvements are delivered via recent randomized and exchangeable variants of Markov's inequality.","sentences":["Significant recent progress has been made on deriving combination rules that can take as input a set of arbitrarily dependent p-values, and produce as output a single valid p-value.","Here, we show that under the assumption of exchangeability of the p-values, many of those rules can be improved (made more powerful).","While this observation by itself has practical implications (for example, under repeated tests involving data splitting), it also has implications for combining arbitrarily dependent p-values, since the latter can be made exchangeable by applying a uniformly random permutation.","In particular, we derive several simple randomized combination rules for arbitrarily dependent p-values that are more powerful than their deterministic counterparts.","For example, we derive randomized and exchangeable improvements of well known p-value combination rules like \"twice the median\" and \"twice the average\", as well as geometric and harmonic means.","The main technical advance is to show that all these combination rules can be obtained by calibrating the p-values to e-values (using an $\\alpha$-dependent calibrator), averaging those e-values, converting to a level $\\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests.","The improvements are delivered via recent randomized and exchangeable variants of Markov's inequality."],"url":"http://arxiv.org/abs/2404.03484v1","category":"math.ST"}
{"created":"2024-04-04 14:08:11","title":"On the sum of fifth powers in arithmetic progression","abstract":"In this paper we study equation $$(x-dr)^5+\\cdots+x^5+\\cdots+(x+dr)^5=y^p$$ under the condition $\\gcd(x,r)=1$. We present a recipe for proving the non-existence of non-trivial integer solutions of the above equation, and as an application we obtain explicit results for the cases $d=2,3$ (the case $d=1$ was already solved). We also prove an asymptotic result for $d\\equiv 1, 7\\pmod9$. Our main tools include the modular method, employing Frey curves and their associated modular forms, as well as the symplectic argument.","sentences":["In this paper we study equation $$(x-dr)^5+\\cdots+x^5+\\cdots+(x+dr)^5=y^p$$ under the condition $\\gcd(x,r)=1$. We present a recipe for proving the non-existence of non-trivial integer solutions of the above equation, and as an application we obtain explicit results for the cases $d=2,3$ (the case $d=1$ was already solved).","We also prove an asymptotic result for $d\\equiv 1, 7\\pmod9$.","Our main tools include the modular method, employing Frey curves and their associated modular forms, as well as the symplectic argument."],"url":"http://arxiv.org/abs/2404.03457v1","category":"math.NT"}
{"created":"2024-04-04 13:36:27","title":"Fractional-charge hadrons and leptons to tell the Standard Model group apart","abstract":"The gauge group of strong and electroweak interactions in Nature could be any of the four that share the same Lie algebra, $SU(3)_c\\times SU(2)_L\\times U(1)_Y/Z_p\\equiv G_p$ with $Z_p=\\left\\{Z_6,Z_3,Z_2,Z_1\\right\\}$. Each of these cases allows in its spectrum for the matter fields of the SM but also for new distinctive representations, e.g. under the assumption that $q_L$ possesses the minimum possible hypercharge in Nature, $G_p$ allows for particles with a multiple of $p\\,e/6$ for electric charge. This letter discusses how these new possibilities in the spectrum could be used to tell the SM group apart.","sentences":["The gauge group of strong and electroweak interactions in Nature could be any of the four that share the same Lie algebra, $SU(3)_c\\times SU(2)_L\\times U(1)_Y/Z_p\\equiv G_p$ with $Z_p=\\left\\{Z_6,Z_3,Z_2,Z_1\\right\\}$. Each of these cases allows in its spectrum for the matter fields of the SM but also for new distinctive representations, e.g. under the assumption that $q_L$ possesses the minimum possible hypercharge in Nature, $G_p$ allows for particles with a multiple of $p\\,e/6$ for electric charge.","This letter discusses how these new possibilities in the spectrum could be used to tell the SM group apart."],"url":"http://arxiv.org/abs/2404.03438v1","category":"hep-ph"}
{"created":"2024-04-04 13:09:26","title":"Accurate estimation of feature importance faithfulness for tree models","abstract":"In this paper, we consider a perturbation-based metric of predictive faithfulness of feature rankings (or attributions) that we call PGI squared. When applied to decision tree-based regression models, the metric can be computed accurately and efficiently for arbitrary independent feature perturbation distributions. In particular, the computation does not involve Monte Carlo sampling that has been typically used for computing similar metrics and which is inherently prone to inaccuracies. Moreover, we propose a method of ranking features by their importance for the tree model's predictions based on PGI squared. Our experiments indicate that in some respects, the method may identify the globally important features better than the state-of-the-art SHAP explainer","sentences":["In this paper, we consider a perturbation-based metric of predictive faithfulness of feature rankings (or attributions) that we call PGI squared.","When applied to decision tree-based regression models, the metric can be computed accurately and efficiently for arbitrary independent feature perturbation distributions.","In particular, the computation does not involve Monte Carlo sampling that has been typically used for computing similar metrics and which is inherently prone to inaccuracies.","Moreover, we propose a method of ranking features by their importance for the tree model's predictions based on PGI squared.","Our experiments indicate that in some respects, the method may identify the globally important features better than the state-of-the-art SHAP explainer"],"url":"http://arxiv.org/abs/2404.03426v1","category":"cs.LG"}
{"created":"2024-04-04 13:04:39","title":"Empirical Bayes for the Reluctant Frequentist","abstract":"Empirical Bayes methods offer valuable tools for a large class of compound decision problems. In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation. Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models. A more extensive introductory treatment will eventually be available in \\citet{kg24}. The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data.","sentences":["Empirical Bayes methods offer valuable tools for a large class of compound decision problems.","In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation.","Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models.","A more extensive introductory treatment will eventually be available in \\citet{kg24}.","The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data."],"url":"http://arxiv.org/abs/2404.03422v1","category":"stat.ME"}
{"created":"2024-04-04 12:02:35","title":"Riemannian Covariance Fitting for Direction-of-Arrival Estimation","abstract":"Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions. Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices. We assert that this oversight leads to inherent limitations. In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF. We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics. Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions. Comparing these features to classical beamformers shows significant advantage. In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels.","sentences":["Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions.","Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices.","We assert that this oversight leads to inherent limitations.","In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF.","We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics.","Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions.","Comparing these features to classical beamformers shows significant advantage.","In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels."],"url":"http://arxiv.org/abs/2404.03401v1","category":"eess.SP"}
{"created":"2024-04-04 11:44:17","title":"Material design optimization for large-m 11B4C-based Ni/Ti supermirror neutron optics","abstract":"State-of-the-art Ni/Ti supermirror neutron optics have limited reflected intensity and a restricted neutron energy range due to the interface width. Incorporating low-neutron-absorbing 11B4C enhances reflectivity and allows for thinner layers to be deposited, with which more efficient supermirrors with higher m-values can be realized. However, incorporating 11B4C reduces the optical contrast, limiting the attainable reflectivity at low scattering vectors, making this approach infeasible. This study explores various approaches to optimize the material design of 11B4C-containing Ni/Ti supermirrors to maintain high reflectivity at low scattering vectors and achieve low interface widths at large scattering vectors. The scattering length density contrast versus interface width is investigated for multilayer periods of 30 {\\AA}, 48 {\\AA}, and 84 {\\AA}, for designs involving pure Ni/Ti multilayers, multilayers with 11B4C co-deposited in Ni and Ti layers, multilayers with 11B4C co-deposited only in Ni layers, and multilayers with 11B4C as thin interlayers between Ni and Ti layers. Our results suggest that a depth-graded hybrid material design by incorporating 11B4C inside the Ni and Ti layers, below approximately 26 {\\AA}, and introducing 1.5 {\\AA} 11B4C interlayers between the thicker Ni and Ti layers can achieve a higher reflectivity than state-of-the-art Ni/Ti multilayers over the entire scattering vector range.","sentences":["State-of-the-art Ni/Ti supermirror neutron optics have limited reflected intensity and a restricted neutron energy range due to the interface width.","Incorporating low-neutron-absorbing 11B4C enhances reflectivity and allows for thinner layers to be deposited, with which more efficient supermirrors with higher m-values can be realized.","However, incorporating 11B4C reduces the optical contrast, limiting the attainable reflectivity at low scattering vectors, making this approach infeasible.","This study explores various approaches to optimize the material design of 11B4C-containing Ni/Ti supermirrors to maintain high reflectivity at low scattering vectors and achieve low interface widths at large scattering vectors.","The scattering length density contrast versus interface width is investigated for multilayer periods of 30 {\\AA}, 48 {\\AA}, and 84 {\\AA}, for designs involving pure Ni/Ti multilayers, multilayers with 11B4C co-deposited in Ni and Ti layers, multilayers with 11B4C co-deposited only in Ni layers, and multilayers with 11B4C as thin interlayers between Ni and Ti layers.","Our results suggest that a depth-graded hybrid material design by incorporating 11B4C inside the Ni and Ti layers, below approximately 26 {\\AA}, and introducing 1.5 {\\AA} 11B4C interlayers between the thicker Ni and Ti layers can achieve a higher reflectivity than state-of-the-art Ni/Ti multilayers over the entire scattering vector range."],"url":"http://arxiv.org/abs/2404.03390v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 11:24:33","title":"3D scaling laws and projection effects in The300-NIKA2 Sunyaev-Zeldovich Large Program Twin Samples","abstract":"The abundance of galaxy clusters with mass and redshift is a well-known cosmological probe. The cluster mass is a key parameter for studies that aim to constrain cosmological parameters using galaxy clusters, making it critical to understand and properly account for the errors in its estimates. Subsequently, it becomes important to correctly calibrate scaling relations between observables like the integrated Compton parameter and the mass of the cluster.   The NIKA2 Sunyaev-Zeldovich Large program (LPSZ) enables one to map the intracluster medium profiles in the mm-wavelength band with great details (resolution of $11 \\ \\mathrm{\\&}\\ 17^{\\prime \\prime}$ at $1.2 \\ \\mathrm{\\&}\\ 2 $ mm, respectively) and hence, to estimate the cluster hydrostatic mass more precisely than previous SZ observations. However, there are certain systematic effects which can only be accounted for with the use of simulations. For this purpose, we employ THE THREE HUNDRED simulations which have been modelled with a range of physics modules to simulate galaxy clusters. The so-called twin samples are constructed by picking synthetic clusters of galaxies with properties close to the observational targets of the LPSZ. In particular, we use the Compton parameter maps and projected total mass maps of these twin samples along 29 different lines of sight. We investigate the scatter that projection induces on the total masses. Eventually, we consider the statistical values along different lines of sight to construct a kind of 3D scaling law between the integrated Compton parameter, total mass, and overdensity of the galaxy clusters to determine the overdensity that is least impacted by the projection effect.","sentences":["The abundance of galaxy clusters with mass and redshift is a well-known cosmological probe.","The cluster mass is a key parameter for studies that aim to constrain cosmological parameters using galaxy clusters, making it critical to understand and properly account for the errors in its estimates.","Subsequently, it becomes important to correctly calibrate scaling relations between observables like the integrated Compton parameter and the mass of the cluster.   ","The NIKA2 Sunyaev-Zeldovich Large program (LPSZ) enables one to map the intracluster medium profiles in the mm-wavelength band with great details (resolution of $11 \\ \\mathrm{\\&}\\ 17^{\\prime \\prime}$ at $1.2 \\ \\mathrm{\\&}\\ 2 $ mm, respectively) and hence, to estimate the cluster hydrostatic mass more precisely than previous SZ observations.","However, there are certain systematic effects which can only be accounted for with the use of simulations.","For this purpose, we employ THE THREE HUNDRED simulations which have been modelled with a range of physics modules to simulate galaxy clusters.","The so-called twin samples are constructed by picking synthetic clusters of galaxies with properties close to the observational targets of the LPSZ.","In particular, we use the Compton parameter maps and projected total mass maps of these twin samples along 29 different lines of sight.","We investigate the scatter that projection induces on the total masses.","Eventually, we consider the statistical values along different lines of sight to construct a kind of 3D scaling law between the integrated Compton parameter, total mass, and overdensity of the galaxy clusters to determine the overdensity that is least impacted by the projection effect."],"url":"http://arxiv.org/abs/2404.03376v1","category":"astro-ph.CO"}
{"created":"2024-04-04 11:16:16","title":"Elementary Analysis of Policy Gradient Methods","abstract":"Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in reinforcement learning. There have been a flurry of recent activities in studying these algorithms from the theoretical aspect. Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations. In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods. Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy. New and elementary analysis techniques have been developed to establish these results.","sentences":["Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in reinforcement learning.","There have been a flurry of recent activities in studying these algorithms from the theoretical aspect.","Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations.","In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods.","Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy.","New and elementary analysis techniques have been developed to establish these results."],"url":"http://arxiv.org/abs/2404.03372v1","category":"math.OC"}
{"created":"2024-04-04 10:50:41","title":"Agora Elevator Bodily Sensation Study -- a report","abstract":"This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience. It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity? The following report documents the study, procedure, results and findings.","sentences":["This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience.","It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity?","The following report documents the study, procedure, results and findings."],"url":"http://arxiv.org/abs/2404.03356v1","category":"cs.HC"}
{"created":"2024-04-04 10:16:01","title":"The Classification of all weak solutions to $-\u0394u={u^{-\u03b3}}$ in the half-space","abstract":"We provide the classification of all the positive solutions to $-\\Delta u=\\frac{1}{u^\\gamma}$ in the half space, under minimal assumption.","sentences":["We provide the classification of all the positive solutions to $-\\Delta u=\\frac{1}{u^\\gamma}$ in the half space, under minimal assumption."],"url":"http://arxiv.org/abs/2404.03343v1","category":"math.AP"}
{"created":"2024-04-04 10:10:38","title":"Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks","abstract":"Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks. Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked. Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed. Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID). Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle. The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance. 2) The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration. Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks.","sentences":["Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks.","Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked.","Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed.","Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID).","Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle.","The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance.","2)","The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration.","Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks."],"url":"http://arxiv.org/abs/2404.03340v1","category":"cs.CV"}
{"created":"2024-04-04 09:57:08","title":"3D Growth and Remodeling Theory Supports the Hypothesis of Staphyloma Formation from Local Scleral Weakening under Normal Intraocular Pressure","abstract":"$\\bf{Purpose}$: To assess whether Growth & Remodeling (G&R) theory could explain staphyloma formation from a local scleral weakening.   $\\bf{Methods}$: A finite element model of a healthy eye was reconstructed, including the following connective tissues: the lamina cribrosa, the peripapillary sclera, and the peripheral sclera. The scleral shell was modelled as a constrained mixture, consisting of an isotropic ground matrix and two collagen fiber families (circumferential and meridional). The homogenized constrained mixture model was employed to simulate the adaptation of the sclera to alterations in its biomechanical environment over a duration of 13.7 years. G&R processes were triggered by reducing the shear stiffness of the ground matrix in the peripapillary sclera and lamina cribrosa by 85%. Three distinct G&R scenarios were investigated: (1) low mass turnover rate in combination with transmural volumetric growth; (2) high mass turnover rate in combination with transmural volumetric growth; and (3) high mass turnover rate in combination with mass density growth.   $\\bf{Results}$: In scenario 1, we observed a significant outpouching of the posterior pole, closely resembling the shape of a Type-III staphyloma. Additionally, we found a notable change in scleral curvature and a thinning of the peripapillary sclera by 84%. In contrast, scenarios 2 and 3 exhibited less drastic deformations, with stable posterior staphylomas after approximately 7 years.   $\\bf{Conclusions}$: Our framework suggests that local scleral weakening is sufficient to trigger staphyloma formation under normal intraocular pressure. With patient-specific scleral geometries (obtainable via wide-field optical coherence tomography), our framework could aid in identifying individuals at risk of developing posterior staphylomas.","sentences":["$\\bf{Purpose}$: To assess whether Growth & Remodeling (G&R) theory could explain staphyloma formation from a local scleral weakening.   ","$\\bf{Methods}$: A finite element model of a healthy eye was reconstructed, including the following connective tissues: the lamina cribrosa, the peripapillary sclera, and the peripheral sclera.","The scleral shell was modelled as a constrained mixture, consisting of an isotropic ground matrix and two collagen fiber families (circumferential and meridional).","The homogenized constrained mixture model was employed to simulate the adaptation of the sclera to alterations in its biomechanical environment over a duration of 13.7 years.","G&R processes were triggered by reducing the shear stiffness of the ground matrix in the peripapillary sclera and lamina cribrosa by 85%.","Three distinct G&R scenarios were investigated: (1) low mass turnover rate in combination with transmural volumetric growth; (2) high mass turnover rate in combination with transmural volumetric growth; and (3) high mass turnover rate in combination with mass density growth.   ","$\\bf{Results}$: In scenario 1, we observed a significant outpouching of the posterior pole, closely resembling the shape of a Type-III staphyloma.","Additionally, we found a notable change in scleral curvature and a thinning of the peripapillary sclera by 84%.","In contrast, scenarios 2 and 3 exhibited less drastic deformations, with stable posterior staphylomas after approximately 7 years.   ","$\\bf{Conclusions}$: Our framework suggests that local scleral weakening is sufficient to trigger staphyloma formation under normal intraocular pressure.","With patient-specific scleral geometries (obtainable via wide-field optical coherence tomography), our framework could aid in identifying individuals at risk of developing posterior staphylomas."],"url":"http://arxiv.org/abs/2404.03330v1","category":"cs.CE"}
{"created":"2024-04-04 08:50:32","title":"MusE GAs FLOw and Wind (MEGAFLOW) XI. Scaling relations between outflows and host galaxy properties","abstract":"Absorption line spectroscopy using background quasars can provide strong constraints on galactic outflows. In this paper, we investigate possible scaling relations between outflow properties, namely outflow velocity \\Vout, the mass ejection rate $\\dot M_{\\rm out}$, and the mass loading factor $\\eta$ and the host galaxy properties, such as star formation rate (SFR), SFR surface density, redshift, and stellar mass using galactic outflows probed by background quasars from MEGAFLOW and other surveys. We find that $V_{\\rm out}$ ($\\eta$) is (anti-)correlated with SFR and SFR surface density. We extend the formalism of momentum-driven outflows of Heckman et al. to show that it applies not only to down the barrel studies but also to winds probed by background quasars, suggesting a possible universal wind formalism. Under this formalism, we find a clear distinction between ``strong'' and ``weak'' outflows where ``strong'' outflows seem to have tighter correlations with galaxy properties (SFR or galaxy stellar mass) than ``weak'' outflows.","sentences":["Absorption line spectroscopy using background quasars can provide strong constraints on galactic outflows.","In this paper, we investigate possible scaling relations between outflow properties, namely outflow velocity \\Vout, the mass ejection rate $\\dot M_{\\rm out}$, and the mass loading factor $\\eta$ and the host galaxy properties, such as star formation rate (SFR), SFR surface density, redshift, and stellar mass using galactic outflows probed by background quasars from MEGAFLOW and other surveys.","We find that $V_{\\rm out}$ ($\\eta$) is (anti-)correlated with SFR and SFR surface density.","We extend the formalism of momentum-driven outflows of Heckman et al. to show that it applies not only to down the barrel studies but also to winds probed by background quasars, suggesting a possible universal wind formalism.","Under this formalism, we find a clear distinction between ``strong'' and ``weak'' outflows where ``strong'' outflows seem to have tighter correlations with galaxy properties (SFR or galaxy stellar mass) than ``weak'' outflows."],"url":"http://arxiv.org/abs/2404.03300v1","category":"astro-ph.GA"}
{"created":"2024-04-04 08:09:33","title":"Improving Patient Transport in Hospitals: A Literature Review of Operations Research Methods","abstract":"Most activities in hospitals require the presence of the patient. Delays in patient transport can therefore cause disruptions and costly downtime in many different areas and departments, which makes patient transport planning a central operational problem in hospitals. This paper provides the first literature review of Operations Research approaches for improving non-emergency patient transport in hospitals. We structure the different patient transport problems considered in the literature according to several main characteristics and introduce a four-field notation for patient transport problems that allows for a concise representation of different problem variants. We then analyze the relevant literature with respect to different aspects related to the considered problem variant, the employed modeling and solution techniques, as well as the data used and the level of practical implementation achieved. Based on our literature analysis and semi-structured interviews with hospital practitioners, we provide a comparison of current hospital practice and the existing literature on patient transport, and we identify research gaps and formulate an agenda for relevant future research in this area.","sentences":["Most activities in hospitals require the presence of the patient.","Delays in patient transport can therefore cause disruptions and costly downtime in many different areas and departments, which makes patient transport planning a central operational problem in hospitals.","This paper provides the first literature review of Operations Research approaches for improving non-emergency patient transport in hospitals.","We structure the different patient transport problems considered in the literature according to several main characteristics and introduce a four-field notation for patient transport problems that allows for a concise representation of different problem variants.","We then analyze the relevant literature with respect to different aspects related to the considered problem variant, the employed modeling and solution techniques, as well as the data used and the level of practical implementation achieved.","Based on our literature analysis and semi-structured interviews with hospital practitioners, we provide a comparison of current hospital practice and the existing literature on patient transport, and we identify research gaps and formulate an agenda for relevant future research in this area."],"url":"http://arxiv.org/abs/2404.03282v1","category":"math.OC"}
{"created":"2024-04-04 08:04:24","title":"Evaluating Document Simplification: On the Importance of Separately Assessing Simplicity and Meaning Preservation","abstract":"Text simplification intends to make a text easier to read while preserving its core meaning. Intuitively and as shown in previous works, these two dimensions (simplification and meaning preservation) are often-times inversely correlated. An overly conservative text will fail to simplify sufficiently, whereas extreme simplification will degrade meaning preservation. Yet, popular evaluation metrics either aggregate meaning preservation and simplification into a single score (SARI, LENS), or target meaning preservation alone (BERTScore, QuestEval). Moreover, these metrics usually require a set of references and most previous work has only focused on sentence-level simplification. In this paper, we focus on the evaluation of document-level text simplification and compare existing models using distinct metrics for meaning preservation and simplification. We leverage existing metrics from similar tasks and introduce a reference-less metric variant for simplicity, showing that models are mostly biased towards either simplification or meaning preservation, seldom performing well on both dimensions. Making use of the fact that the metrics we use are all reference-less, we also investigate the performance of existing models when applied to unseen data (where reference simplifications are unavailable).","sentences":["Text simplification intends to make a text easier to read while preserving its core meaning.","Intuitively and as shown in previous works, these two dimensions (simplification and meaning preservation) are often-times inversely correlated.","An overly conservative text will fail to simplify sufficiently, whereas extreme simplification will degrade meaning preservation.","Yet, popular evaluation metrics either aggregate meaning preservation and simplification into a single score (SARI, LENS), or target meaning preservation alone (BERTScore, QuestEval).","Moreover, these metrics usually require a set of references and most previous work has only focused on sentence-level simplification.","In this paper, we focus on the evaluation of document-level text simplification and compare existing models using distinct metrics for meaning preservation and simplification.","We leverage existing metrics from similar tasks and introduce a reference-less metric variant for simplicity, showing that models are mostly biased towards either simplification or meaning preservation, seldom performing well on both dimensions.","Making use of the fact that the metrics we use are all reference-less, we also investigate the performance of existing models when applied to unseen data (where reference simplifications are unavailable)."],"url":"http://arxiv.org/abs/2404.03278v1","category":"cs.CL"}
{"created":"2024-04-04 06:49:48","title":"Marginal Treatment Effects and Monotonicity","abstract":"How robust are analyses based on marginal treatment effects (MTE) to violations of Imbens and Angrist (1994) monotonicity? In this note, I present weaker forms of monotonicity under which popular MTE-based estimands still identify the parameters of interest.","sentences":["How robust are analyses based on marginal treatment effects (MTE) to violations of Imbens and Angrist (1994) monotonicity?","In this note, I present weaker forms of monotonicity under which popular MTE-based estimands still identify the parameters of interest."],"url":"http://arxiv.org/abs/2404.03235v1","category":"econ.EM"}
{"created":"2024-04-04 06:37:46","title":"Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning","abstract":"Machine unlearning has become a promising solution for fulfilling the \"right to be forgotten\", under which individuals can request the deletion of their data from machine learning models. However, existing studies of machine unlearning mainly focus on the efficacy and efficiency of unlearning methods, while neglecting the investigation of the privacy vulnerability during the unlearning process. With two versions of a model available to an adversary, that is, the original model and the unlearned model, machine unlearning opens up a new attack surface. In this paper, we conduct the first investigation to understand the extent to which machine unlearning can leak the confidential content of the unlearned data. Specifically, under the Machine Learning as a Service setting, we propose unlearning inversion attacks that can reveal the feature and label information of an unlearned sample by only accessing the original and unlearned model. The effectiveness of the proposed unlearning inversion attacks is evaluated through extensive experiments on benchmark datasets across various model architectures and on both exact and approximate representative unlearning approaches. The experimental results indicate that the proposed attack can reveal the sensitive information of the unlearned data. As such, we identify three possible defenses that help to mitigate the proposed attacks, while at the cost of reducing the utility of the unlearned model. The study in this paper uncovers an underexplored gap between machine unlearning and the privacy of unlearned data, highlighting the need for the careful design of mechanisms for implementing unlearning without leaking the information of the unlearned data.","sentences":["Machine unlearning has become a promising solution for fulfilling the \"right to be forgotten\", under which individuals can request the deletion of their data from machine learning models.","However, existing studies of machine unlearning mainly focus on the efficacy and efficiency of unlearning methods, while neglecting the investigation of the privacy vulnerability during the unlearning process.","With two versions of a model available to an adversary, that is, the original model and the unlearned model, machine unlearning opens up a new attack surface.","In this paper, we conduct the first investigation to understand the extent to which machine unlearning can leak the confidential content of the unlearned data.","Specifically, under the Machine Learning as a Service setting, we propose unlearning inversion attacks that can reveal the feature and label information of an unlearned sample by only accessing the original and unlearned model.","The effectiveness of the proposed unlearning inversion attacks is evaluated through extensive experiments on benchmark datasets across various model architectures and on both exact and approximate representative unlearning approaches.","The experimental results indicate that the proposed attack can reveal the sensitive information of the unlearned data.","As such, we identify three possible defenses that help to mitigate the proposed attacks, while at the cost of reducing the utility of the unlearned model.","The study in this paper uncovers an underexplored gap between machine unlearning and the privacy of unlearned data, highlighting the need for the careful design of mechanisms for implementing unlearning without leaking the information of the unlearned data."],"url":"http://arxiv.org/abs/2404.03233v1","category":"cs.CR"}
{"created":"2024-04-04 06:31:56","title":"Quantum Phases of a Dipolar Fermi Gas with Laser-assisted Interwire Tunneling","abstract":"We systematically investigate unconventional superfluid phases of fermionic dipolar particles lying in a double-wire setup with laser-assisted interwire tunneling. Our numerical simulations, based on the nonlocal Kohn-Sham Bogoliubov-de Gennes equation, reveal the existence of a large Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) region with a stripe phase under an imbalance of particle densities between two wires. When the laser-assisted interwire tunneling is present, it induces a transition from the FFLO phase to the topological superfluid phase and the associated Majorana zero modes exhibit an oscillation structure, which is significantly enhanced by the long-range nature of the interwire dipolar interaction. This distinguishes itself from the results obtained with usual contact interaction and offers new opportunities for manipulating and reshaping Majorana zero modes by adjusting the degree of the nonlocality and the interwire separation.","sentences":["We systematically investigate unconventional superfluid phases of fermionic dipolar particles lying in a double-wire setup with laser-assisted interwire tunneling.","Our numerical simulations, based on the nonlocal Kohn-Sham Bogoliubov-de Gennes equation, reveal the existence of a large Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) region with a stripe phase under an imbalance of particle densities between two wires.","When the laser-assisted interwire tunneling is present, it induces a transition from the FFLO phase to the topological superfluid phase and the associated Majorana zero modes exhibit an oscillation structure, which is significantly enhanced by the long-range nature of the interwire dipolar interaction.","This distinguishes itself from the results obtained with usual contact interaction and offers new opportunities for manipulating and reshaping Majorana zero modes by adjusting the degree of the nonlocality and the interwire separation."],"url":"http://arxiv.org/abs/2404.03230v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 05:45:52","title":"Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption","abstract":"As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies. Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model. However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF). We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) -- adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a time followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment. The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets. For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~ 13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works. Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy. Our code is available at https://github.com/TorchFHE/SmartPAF.","sentences":["As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies.","Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model.","However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF).","We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) -- adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a time followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment.","The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets.","For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~","13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works.","Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy.","Our code is available at https://github.com/TorchFHE/SmartPAF."],"url":"http://arxiv.org/abs/2404.03216v1","category":"cs.CR"}
{"created":"2024-04-04 05:39:09","title":"LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity","abstract":"Vision Transformers (ViTs), with their ability to model long-range dependencies through self-attention mechanisms, have become a standard architecture in computer vision. However, the interpretability of these models remains a challenge. To address this, we propose LeGrad, an explainability method specifically designed for ViTs. LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal. We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map. This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs. We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations. A demo and the code is available at https://github.com/WalBouss/LeGrad.","sentences":["Vision Transformers (ViTs), with their ability to model long-range dependencies through self-attention mechanisms, have become a standard architecture in computer vision.","However, the interpretability of these models remains a challenge.","To address this, we propose LeGrad, an explainability method specifically designed for ViTs.","LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal.","We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map.","This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs.","We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations.","A demo and the code is available at https://github.com/WalBouss/LeGrad."],"url":"http://arxiv.org/abs/2404.03214v1","category":"cs.CV"}
{"created":"2024-04-04 03:21:38","title":"Direct visualization of local magnetic domain dynamics in a 2D Van der Walls material/ferromagnet interface","abstract":"Exploring new strategies for controlling the magnetic domain propagation is the key to realize ultrafast, high-density domain wall-based memory and logic devices for next generation computing. These strategies include strain modulation in multiferroic devices, geometric confinement and area-selective pinning of domain wall. 2D Van der Waals materials introduce localized modifications to the interfacial magnetic order, enabling control over the propagation of magnetic domains. Here, using Lorentz-Transmission Electron Microscopy (L-TEM) along with the Modified Transport of Intensity equations (MTIE), we demonstrate controlled domain expansion with in-situ magnetic field in a ferromagnet (Permalloy, NiFe) interfacing with a 2D Van der Waals material Graphene (Gr). The Gr/NiFe interface exhibits distinctive domain expansion rate with magnetic field selectively near the interface which is further analyzed using micromagnetic simulations. Our findings are crucial for comprehending direct visualization of interface controlled magnetic domain expansion, offering insights for developing future domain wall-based technology.","sentences":["Exploring new strategies for controlling the magnetic domain propagation is the key to realize ultrafast, high-density domain wall-based memory and logic devices for next generation computing.","These strategies include strain modulation in multiferroic devices, geometric confinement and area-selective pinning of domain wall.","2D Van der Waals materials introduce localized modifications to the interfacial magnetic order, enabling control over the propagation of magnetic domains.","Here, using Lorentz-Transmission Electron Microscopy (L-TEM) along with the Modified Transport of Intensity equations (MTIE), we demonstrate controlled domain expansion with in-situ magnetic field in a ferromagnet (Permalloy, NiFe) interfacing with a 2D Van der Waals material Graphene (Gr).","The Gr/NiFe interface exhibits distinctive domain expansion rate with magnetic field selectively near the interface which is further analyzed using micromagnetic simulations.","Our findings are crucial for comprehending direct visualization of interface controlled magnetic domain expansion, offering insights for developing future domain wall-based technology."],"url":"http://arxiv.org/abs/2404.03177v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 03:20:35","title":"Information-Theoretic Generalization Bounds for Deep Neural Networks","abstract":"Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications. This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds. We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations. The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance. Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs. To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection. This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters. Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question.","sentences":["Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications.","This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds.","We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations.","The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance.","Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs.","To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection.","This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters.","Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question."],"url":"http://arxiv.org/abs/2404.03176v1","category":"cs.LG"}
{"created":"2024-04-04 03:17:27","title":"Matching-star size Ramsey numbers under connectivity constraint","abstract":"Recently, Caro, Patk\\'os, and Tuza (2022) introduced the concept of connected Tur\\'an number. We study a similar parameter in Ramsey theory. Given two graphs $G_1$ and $G_2$, the size Ramsey number $\\hat{r}(G_1,G_2)$ refers to the smallest number of edges in a graph $G$ such that for any red-blue edge-coloring of $G$, either a red subgraph $G_1$ or a blue subgraph $G_2$ is present in $G$. If we further restrict the host graph $G$ to be connected, we obtain the connected size Ramsey number, denoted as $\\hat{r}_c(G_1,G_2)$. Erd\\H{o}s and Faudree (1984) proved that $\\hat r(nK_2,K_{1,m})=mn$ for all positive integers $m,n$. In this paper, we concentrate on the connected analog of this result. Rahadjeng, Baskoro, and Assiyatun (2016) provided the exact values of $\\hat r_c(nK_2,K_{1,m})$ for $n=2,3$. We establish a more general result: for all positive integers $m$ and $n$ with $m\\ge (n^2+2pn+n-3)/2$, we have $\\hat r_c(nK_{1,p},K_{1,m})=n(m+p)-1$. As a corollary, $\\hat r_c(nK_2,K_{1,m})=nm+n-1$ for $m\\ge (n^2+3n-3)/2$. We also propose a conjecture for the interested reader.","sentences":["Recently, Caro, Patk\\'os, and Tuza (2022) introduced the concept of connected Tur\\'an number.","We study a similar parameter in Ramsey theory.","Given two graphs $G_1$ and $G_2$, the size Ramsey number $\\hat{r}(G_1,G_2)$ refers to the smallest number of edges in a graph $G$ such that for any red-blue edge-coloring of $G$, either a red subgraph $G_1$ or a blue subgraph $G_2$ is present in $G$. If we further restrict the host graph $G$ to be connected, we obtain the connected size Ramsey number, denoted as $\\hat{r}_c(G_1,G_2)$. Erd\\H{o}s and Faudree (1984) proved that $\\hat r(nK_2,K_{1,m})=mn$ for all positive integers $m,n$.","In this paper, we concentrate on the connected analog of this result.","Rahadjeng, Baskoro, and Assiyatun (2016) provided the exact values of $\\hat r_c(nK_2,K_{1,m})$ for $n=2,3$. We establish a more general result: for all positive integers $m$ and $n$ with $m\\ge (n^2+2pn+n-3)/2$, we have $\\hat r_c(nK_{1,p},K_{1,m})=n(m+p)-1$. As a corollary, $\\hat r_c(nK_2,K_{1,m})=nm+n-1$ for $m\\ge (n^2+3n-3)/2$. We also propose a conjecture for the interested reader."],"url":"http://arxiv.org/abs/2404.03175v1","category":"math.CO"}
{"created":"2024-04-04 02:55:55","title":"A Dynamic Droplet Breakup Model for Eulerian-Lagrangian Simulation of Liquid-fueled Detonation","abstract":"This study proposes a dynamic model to reflect the physical image of the droplet breakup process in two-phase detonation flows. This breakup model is implemented in a two-phase detonation solver developed based on an open-source computational fluid dynamic platform, OpenFOAM, and compared with three prevalent models (TAB, PilchErdman, and ReitzKH-RT model) under different droplet diameters in one- and two-dimensional detonation problems. The simulating results show that the present breakup model well predicts experimentally determined detonation parameters such as detonation velocities and post-wave temperature. In addition, the present model has the advantage of being free of the KH breakup time parameter, which is needed by the ReitzKH-RT model to fit the experimental data. The one-dimensional detonation simulations indicate that different breakup models have a slight impact on the detonation wave velocity because the droplet breakup process does not significantly affect the total heat release as long as it is sufficiently fast to sustain the detonation. However, the two-dimensional detonation simulations show that both the breakup model and the droplet initial diameter significantly affect the detonation cell size due to the different droplet distributions predicted by different models. The breakup length, which is the distance from the shock wave to the location at which sufficiently small child droplets appear, affects the chemical reaction zone thickness, which in turn affects the detonation cell size. A longer breakup length will result in a larger detonation cell size.","sentences":["This study proposes a dynamic model to reflect the physical image of the droplet breakup process in two-phase detonation flows.","This breakup model is implemented in a two-phase detonation solver developed based on an open-source computational fluid dynamic platform, OpenFOAM, and compared with three prevalent models (TAB, PilchErdman, and ReitzKH-RT model) under different droplet diameters in one-","and two-dimensional detonation problems.","The simulating results show that the present breakup model well predicts experimentally determined detonation parameters such as detonation velocities and post-wave temperature.","In addition, the present model has the advantage of being free of the KH breakup time parameter, which is needed by the ReitzKH-RT model to fit the experimental data.","The one-dimensional detonation simulations indicate that different breakup models have a slight impact on the detonation wave velocity because the droplet breakup process does not significantly affect the total heat release as long as it is sufficiently fast to sustain the detonation.","However, the two-dimensional detonation simulations show that both the breakup model and the droplet initial diameter significantly affect the detonation cell size due to the different droplet distributions predicted by different models.","The breakup length, which is the distance from the shock wave to the location at which sufficiently small child droplets appear, affects the chemical reaction zone thickness, which in turn affects the detonation cell size.","A longer breakup length will result in a larger detonation cell size."],"url":"http://arxiv.org/abs/2404.03170v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 02:34:46","title":"Towards Collaborative Family-Centered Design for Online Safety, Privacy and Security","abstract":"Traditional online safety technologies often overly restrict teens and invade their privacy, while parents often lack knowledge regarding their digital privacy. As such, prior researchers have called for more collaborative approaches on adolescent online safety and networked privacy. In this paper, we propose family-centered approaches to foster parent-teen collaboration in ensuring their mobile privacy and online safety while respecting individual privacy, to enhance open discussion and teens' self-regulation. However, challenges such as power imbalances and conflicts with family values arise when implementing such approaches, making parent-teen collaboration difficult. Therefore, attending the family-centered design workshop will provide an invaluable opportunity for us to discuss these challenges and identify best research practices for the future of collaborative online safety and privacy within families.","sentences":["Traditional online safety technologies often overly restrict teens and invade their privacy, while parents often lack knowledge regarding their digital privacy.","As such, prior researchers have called for more collaborative approaches on adolescent online safety and networked privacy.","In this paper, we propose family-centered approaches to foster parent-teen collaboration in ensuring their mobile privacy and online safety while respecting individual privacy, to enhance open discussion and teens' self-regulation.","However, challenges such as power imbalances and conflicts with family values arise when implementing such approaches, making parent-teen collaboration difficult.","Therefore, attending the family-centered design workshop will provide an invaluable opportunity for us to discuss these challenges and identify best research practices for the future of collaborative online safety and privacy within families."],"url":"http://arxiv.org/abs/2404.03165v1","category":"cs.HC"}
{"created":"2024-04-04 01:59:59","title":"Orthogonal calibration via posterior projections with applications to the Schwarzschild model","abstract":"The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics. Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures. This renders a statistical calibration problem with multivariate outcomes. In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability. Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes. We develop a functional projection approach using the theory of Hilbert spaces. A finite-dimensional analogue of the projection problem is also considered. We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration.","sentences":["The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics.","Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures.","This renders a statistical calibration problem with multivariate outcomes.","In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability.","Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes.","We develop a functional projection approach using the theory of Hilbert spaces.","A finite-dimensional analogue of the projection problem is also considered.","We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration."],"url":"http://arxiv.org/abs/2404.03152v1","category":"stat.ME"}
{"created":"2024-04-04 01:40:59","title":"Computational Study Based Prediction of New Photocatalysts for water splitting by systematic manipulation of MXene surfaces","abstract":"The compositional and structural flexibility of functionalised two-dimensional metal carbonitrides or MXenes has been exploited through a combinatorial search for new materials that can act as catalysts for photo-assisted water splitting by absorbing sunlight with energy in the infra-red region. Detailed calculations on 49 Janus MXenes where two surfaces are of asymmetric nature are carried out by first-principles Density Functional Theory. A screening procedure is adopted to arrive at potential candidates. Our calculations predict four new materials whose surfaces can activate both hydrogen and oxygen evolution reactions upon splitting water, two out of which are infra-red active, and the rest are visible light-active. We have performed a detailed microscopic analysis to find out the interrelations of the structural model of surface functionalisation, the chemistry of the surfaces, the electronic structure, and the alignment of bands with respect to the reaction potentials that explain our results. Apart from these four compounds, we find thirteen other compounds that are suitable for either hydrogen evolution or oxygen reduction reactions. This study lays out a guideline for the systematic discovery of potential new catalysts for water splitting under sunlight irradiation.","sentences":["The compositional and structural flexibility of functionalised two-dimensional metal carbonitrides or MXenes has been exploited through a combinatorial search for new materials that can act as catalysts for photo-assisted water splitting by absorbing sunlight with energy in the infra-red region.","Detailed calculations on 49 Janus MXenes where two surfaces are of asymmetric nature are carried out by first-principles Density Functional Theory.","A screening procedure is adopted to arrive at potential candidates.","Our calculations predict four new materials whose surfaces can activate both hydrogen and oxygen evolution reactions upon splitting water, two out of which are infra-red active, and the rest are visible light-active.","We have performed a detailed microscopic analysis to find out the interrelations of the structural model of surface functionalisation, the chemistry of the surfaces, the electronic structure, and the alignment of bands with respect to the reaction potentials that explain our results.","Apart from these four compounds, we find thirteen other compounds that are suitable for either hydrogen evolution or oxygen reduction reactions.","This study lays out a guideline for the systematic discovery of potential new catalysts for water splitting under sunlight irradiation."],"url":"http://arxiv.org/abs/2404.03146v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 00:38:51","title":"Results of existence and uniqueness for the Cauchy problem of semilinear heat equations on stratified Lie groups","abstract":"The aim of this paper is to give existence and uniqueness results for solutions of the Cauchy problem for semilinear heat equations on stratified Lie groups $\\mathbb{G}$ with the homogeneous dimension $N$. We consider the nonlinear function behaves like $|u|^{\\alpha}$ or $|u|^{\\alpha-1}u$ $(\\alpha>1)$ and the initial data $u_0$ belongs to the Sobolev spaces $L^p_s(\\mathbb{G})$ for $1<p<\\infty$ and $0<s<N/p$. Since stratified Lie groups $\\mathbb{G}$ include the Euclidean space ${\\mathbb R}^n$ as an example, our results are an extension of the existence and uniqueness results obtained by F. Ribaud on ${\\mathbb R}^n$ to $\\mathbb{G}$. It should be noted that our proof is very different from it given by Ribaud on ${\\mathbb R}^n$. We adopt the generalized fractional chain rule on $\\mathbb{G}$ to obtain the estimate for the nonlinear term, which is very different from the paracomposition technique adopted by Ribaud on ${\\mathbb R}^n$. By using the generalized fractional chain rule on $\\mathbb{G}$, we can avoid the discussion of Fourier analysis on $\\mathbb{G}$ and make the proof more simple.","sentences":["The aim of this paper is to give existence and uniqueness results for solutions of the Cauchy problem for semilinear heat equations on stratified Lie groups $\\mathbb{G}$ with the homogeneous dimension $N$. We consider the nonlinear function behaves like $|u|^{\\alpha}$ or $|u|^{\\alpha-1}u$ $(\\alpha>1)$ and the initial data $u_0$ belongs to the Sobolev spaces $L^p_s(\\mathbb{G})$ for $1<p<\\infty$ and $0<s<N/p$. Since stratified Lie groups $\\mathbb{G}$ include the Euclidean space ${\\mathbb R}^n$ as an example, our results are an extension of the existence and uniqueness results obtained by F. Ribaud on ${\\mathbb R}^n$ to $\\mathbb{G}$. It should be noted that our proof is very different from it given by Ribaud on ${\\mathbb R}^n$. We adopt the generalized fractional chain rule on $\\mathbb{G}$ to obtain the estimate for the nonlinear term, which is very different from the paracomposition technique adopted by Ribaud on ${\\mathbb R}^n$. By using the generalized fractional chain rule on $\\mathbb{G}$, we can avoid the discussion of Fourier analysis on $\\mathbb{G}$ and make the proof more simple."],"url":"http://arxiv.org/abs/2404.03128v1","category":"math.AP"}
{"created":"2024-04-04 00:28:50","title":"GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis","abstract":"We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.","sentences":["We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans.","We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies.","Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan.","We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss.","Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view.","We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies.","Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction.","Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations."],"url":"http://arxiv.org/abs/2404.03126v1","category":"eess.IV"}
{"created":"2024-04-04 00:21:57","title":"The Diffusive Ultrasound Modulated Bioluminescence Tomography with Partial Data and Uncertain Optical Parameters","abstract":"The paper studies an imaging problem in the diffusive ultrasound-modulated bioluminescence tomography with partial boundary measurement in an anisotropic medium. Assuming plane-wave modulation, we transform the imaging problem to an inverse problem with internal data, and derive a reconstruction procedure to recover the bioluminescent source. Subsequently, an uncertainty quantification estimate is established to assess the robustness of the reconstruction. To facilitate practical implementation, we discretize the diffusive model using the staggered grid scheme, resulting in a discrete formulation of the UMBLT inverse problem. A discrete reconstruction procedure is then presented along with a discrete uncertainty quantification estimate. Finally, the reconstruction procedure is quantitatively validated through numerical examples to demonstrate the efficacy and reliability of the proposed approach and estimates.","sentences":["The paper studies an imaging problem in the diffusive ultrasound-modulated bioluminescence tomography with partial boundary measurement in an anisotropic medium.","Assuming plane-wave modulation, we transform the imaging problem to an inverse problem with internal data, and derive a reconstruction procedure to recover the bioluminescent source.","Subsequently, an uncertainty quantification estimate is established to assess the robustness of the reconstruction.","To facilitate practical implementation, we discretize the diffusive model using the staggered grid scheme, resulting in a discrete formulation of the UMBLT inverse problem.","A discrete reconstruction procedure is then presented along with a discrete uncertainty quantification estimate.","Finally, the reconstruction procedure is quantitatively validated through numerical examples to demonstrate the efficacy and reliability of the proposed approach and estimates."],"url":"http://arxiv.org/abs/2404.03124v1","category":"math.AP"}
{"created":"2024-04-04 00:14:15","title":"Constraints on the spacetime variation of the fine-structure constant using DESI emission-line galaxies","abstract":"We present strong constraints on the spacetime variation of the fine-structure constant $\\alpha$ using the Dark Energy Spectroscopic Instrument (DESI). In this pilot work, we utilize $\\sim110,000$ galaxies with strong and narrow O III $\\lambda\\lambda$4959,5007 emission lines to measure the relative variation $\\Delta\\alpha/\\alpha$ in space and time. The O III doublet is arguably the best choice for this purpose owing to its wide wavelength separation between the two lines and its strong emission in many galaxies. Our galaxy sample spans a redshift range of $0<z<0.95$, covering half of all cosmic time. We divide the sample into subsamples in 10 redshift bins ($\\Delta z=0.1$), and calculate $\\Delta\\alpha/\\alpha$ for the individual subsamples. The uncertainties of the measured $\\Delta\\alpha/\\alpha$ are roughly between $2\\times10^{-6}$ and $2\\times10^{-5}$. We find an apparent $\\alpha$ variation with redshift at a level of $\\Delta\\alpha/\\alpha=(2\\sim3)\\times10^{-5}$. This is highly likely to be caused by systematics associated with wavelength calibration, since such small systematics can be caused by a wavelength distortion of $0.002-0.003$ \\AA, which is beyond the accuracy that the current DESI data can achieve. We refine the wavelength calibration using sky lines for a small fraction of the galaxies, but it does not change our main results. We further probe the spatial variation of $\\alpha$ in small redshift ranges, and do not find obvious, large-scale structures in the spatial distribution of $\\Delta\\alpha/\\alpha$. As DESI is ongoing, we will include more galaxies, and by improving the wavelength calibration, we expect to obtain a better constraint that is comparable to the strongest current constraint.","sentences":["We present strong constraints on the spacetime variation of the fine-structure constant $\\alpha$ using the Dark Energy Spectroscopic Instrument (DESI).","In this pilot work, we utilize $\\sim110,000$ galaxies with strong and narrow","O III $\\lambda\\lambda$4959,5007 emission lines to measure the relative variation $\\Delta\\alpha/\\alpha$ in space and time.","The O III doublet is arguably the best choice for this purpose owing to its wide wavelength separation between the two lines and its strong emission in many galaxies.","Our galaxy sample spans a redshift range of $0<z<0.95$, covering half of all cosmic time.","We divide the sample into subsamples in 10 redshift bins ($\\Delta z=0.1$), and calculate $\\Delta\\alpha/\\alpha$ for the individual subsamples.","The uncertainties of the measured $\\Delta\\alpha/\\alpha$ are roughly between $2\\times10^{-6}$ and $2\\times10^{-5}$. We find an apparent $\\alpha$ variation with redshift at a level of $\\Delta\\alpha/\\alpha=(2\\sim3)\\times10^{-5}$. This is highly likely to be caused by systematics associated with wavelength calibration, since such small systematics can be caused by a wavelength distortion of $0.002-0.003$ \\AA, which is beyond the accuracy that the current DESI data can achieve.","We refine the wavelength calibration using sky lines for a small fraction of the galaxies, but it does not change our main results.","We further probe the spatial variation of $\\alpha$ in small redshift ranges, and do not find obvious, large-scale structures in the spatial distribution of $\\Delta\\alpha/\\alpha$. As DESI is ongoing, we will include more galaxies, and by improving the wavelength calibration, we expect to obtain a better constraint that is comparable to the strongest current constraint."],"url":"http://arxiv.org/abs/2404.03123v1","category":"astro-ph.CO"}
{"created":"2024-04-03 23:59:35","title":"Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report","abstract":"Computer science (CS) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects. They are a core component in many CS undergraduate degrees and address the ACM curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing. However, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff. It demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders. In this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions. We outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students' written reflections. We share the lessons that we have learnt and provide recommendations to educators who are designing new capstone courses or looking to scale existing ones.","sentences":["Computer science (CS) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects.","They are a core component in many CS undergraduate degrees and address the ACM curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing.","However, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff.","It demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders.","In this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions.","We outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students' written reflections.","We share the lessons that we have learnt and provide recommendations to educators who are designing new capstone courses or looking to scale existing ones."],"url":"http://arxiv.org/abs/2404.03120v1","category":"cs.CY"}
{"created":"2024-04-03 23:51:01","title":"Suppressing the sample variance of DESI-like galaxy clustering with fast simulations","abstract":"Ongoing and upcoming galaxy redshift surveys, such as the Dark Energy Spectroscopic Instrument (DESI) survey, will observe vast regions of sky and a wide range of redshifts. In order to model the observations and address various systematic uncertainties, $N$-body simulations are routinely adopted, however, the number of large simulations with sufficiently high mass resolution is usually limited by available computing time. Therefore, achieving a simulation volume with the effective statistical errors significantly smaller than those of the observations becomes prohibitively expensive. In this study, we apply the Convergence Acceleration by Regression and Pooling (CARPool) method to mitigate the sample variance of the DESI-like galaxy clustering in the AbacusSummit simulations, with the assistance of the quasi-$N$-body simulations FastPM. Based on the halo occupation distribution (HOD) models, we construct different FastPM galaxy catalogs, including the luminous red galaxies (LRGs), emission line galaxies (ELGs), and quasars, with their number densities and two-point clustering statistics well matched to those of AbacusSummit. We also employ the same initial conditions between AbacusSummit and FastPM to achieve high cross-correlation, as it is useful in effectively suppressing the variance. Our method of reducing noise in clustering is equivalent to performing a simulation with volume larger by a factor of 5 and 4 for LRGs and ELGs, respectively. We also mitigate the standard deviation of the LRG bispectrum with the triangular configurations $k_2=2k_1=0.2\\hMpc$ by a factor of 1.6. With smaller sample variance on galaxy clustering, we are able to constrain the BAO scale parameters to higher precision. The CARPool method will be beneficial to better constrain the theoretical systematics of BAO, redshift space distortions (RSD) and primordial non-Gaussianity.","sentences":["Ongoing and upcoming galaxy redshift surveys, such as the Dark Energy Spectroscopic Instrument (DESI) survey, will observe vast regions of sky and a wide range of redshifts.","In order to model the observations and address various systematic uncertainties, $N$-body simulations are routinely adopted, however, the number of large simulations with sufficiently high mass resolution is usually limited by available computing time.","Therefore, achieving a simulation volume with the effective statistical errors significantly smaller than those of the observations becomes prohibitively expensive.","In this study, we apply the Convergence Acceleration by Regression and Pooling (CARPool) method to mitigate the sample variance of the DESI-like galaxy clustering in the AbacusSummit simulations, with the assistance of the quasi-$N$-body simulations FastPM.","Based on the halo occupation distribution (HOD) models, we construct different FastPM galaxy catalogs, including the luminous red galaxies (LRGs), emission line galaxies (ELGs), and quasars, with their number densities and two-point clustering statistics well matched to those of AbacusSummit.","We also employ the same initial conditions between AbacusSummit and FastPM to achieve high cross-correlation, as it is useful in effectively suppressing the variance.","Our method of reducing noise in clustering is equivalent to performing a simulation with volume larger by a factor of 5 and 4 for LRGs and ELGs, respectively.","We also mitigate the standard deviation of the LRG bispectrum with the triangular configurations $k_2=2k_1=0.2\\hMpc$ by a factor of 1.6.","With smaller sample variance on galaxy clustering, we are able to constrain the BAO scale parameters to higher precision.","The CARPool method will be beneficial to better constrain the theoretical systematics of BAO, redshift space distortions (RSD) and primordial non-Gaussianity."],"url":"http://arxiv.org/abs/2404.03117v1","category":"astro-ph.CO"}
{"created":"2024-04-03 23:32:53","title":"QED: Scalable Verification of Hardware Memory Consistency","abstract":"Memory consistency model (MCM) issues in out-of-order-issue microprocessor-based shared-memory systems are notoriously non-intuitive and a source of hardware design bugs. Prior hardware verification work is limited to in-order-issue processors, to proving the correctness only of some test cases, or to bounded verification that does not scale in practice beyond 7 instructions across all threads. Because cache coherence (i.e., write serialization and atomicity) and pipeline front-end verification and testing are well-studied, we focus on the memory ordering in an out-of-order-issue processor's load-store queue and the coherence interface between the core and global coherence. We propose QED based on the key notion of observability that any hardware reordering matters only if a forbidden value is produced. We argue that one needs to consider (1) only directly-ordered instruction pairs -- transitively non-redundant pairs connected by an edge in the MCM-imposed partial order -- and not all in-flight instructions, and (2) only the ordering of external events from other cores (e.g.,invalidations) but not the events' originating cores, achieving verification scalability in both the numbers of in-flight memory instructions and of cores. Exhaustively considering all pairs of instruction types and all types of external events intervening between each pair, QED attempts to restore any reordered instructions to an MCM-complaint order without changing the execution values, where failure indicates an MCM violation. Each instruction pair's exploration results in a decision tree of simple, narrowly-defined predicates to be evaluated against the RTL. In our experiments, we automatically generate the decision trees for SC, TSO, and RISC-V WMO, and illustrate automatable verification by evaluating a substantial predicate against BOOMv3 implementation of RISC-V WMO, leaving full automation to future work.","sentences":["Memory consistency model (MCM) issues in out-of-order-issue microprocessor-based shared-memory systems are notoriously non-intuitive and a source of hardware design bugs.","Prior hardware verification work is limited to in-order-issue processors, to proving the correctness only of some test cases, or to bounded verification that does not scale in practice beyond 7 instructions across all threads.","Because cache coherence (i.e., write serialization and atomicity) and pipeline front-end verification and testing are well-studied, we focus on the memory ordering in an out-of-order-issue processor's load-store queue and the coherence interface between the core and global coherence.","We propose QED based on the key notion of observability that any hardware reordering matters only if a forbidden value is produced.","We argue that one needs to consider (1) only directly-ordered instruction pairs -- transitively non-redundant pairs connected by an edge in the MCM-imposed partial order -- and not all in-flight instructions, and (2) only the ordering of external events from other cores (e.g.,invalidations) but not the events' originating cores, achieving verification scalability in both the numbers of in-flight memory instructions and of cores.","Exhaustively considering all pairs of instruction types and all types of external events intervening between each pair, QED attempts to restore any reordered instructions to an MCM-complaint order without changing the execution values, where failure indicates an MCM violation.","Each instruction pair's exploration results in a decision tree of simple, narrowly-defined predicates to be evaluated against the RTL.","In our experiments, we automatically generate the decision trees for SC, TSO, and RISC-V WMO, and illustrate automatable verification by evaluating a substantial predicate against BOOMv3 implementation of RISC-V WMO, leaving full automation to future work."],"url":"http://arxiv.org/abs/2404.03113v1","category":"cs.AR"}
{"created":"2024-04-03 23:24:52","title":"PDRs4All VIII: Mid-IR emission line inventory of the Orion Bar","abstract":"Mid-infrared emission features probe the properties of ionized gas, and hot or warm molecular gas. The Orion Bar is a frequently studied photodissociation region (PDR) containing large amounts of gas under these conditions, and was observed with the MIRI IFU aboard JWST as part of the \"PDRs4All\" program. The resulting IR spectroscopic images of high angular resolution (0.2\") reveal a rich observational inventory of mid-IR emission lines, and spatially resolve the substructure of the PDR, with a mosaic cutting perpendicularly across the ionization front and three dissociation fronts. We extracted five spectra that represent the ionized, atomic, and molecular gas layers, and measured the most prominent gas emission lines. An initial analysis summarizes the physical conditions of the gas and the potential of these data. We identified around 100 lines, report an additional 18 lines that remain unidentified, and measured the line intensities and central wavelengths. The H I recombination lines originating from the ionized gas layer bordering the PDR, have intensity ratios that are well matched by emissivity coefficients from H recombination theory, but deviate up to 10% due contamination by He I lines. We report the observed emission lines of various ionization stages of Ne, P, S, Cl, Ar, Fe, and Ni, and show how certain line ratios vary between the five regions. We observe the pure-rotational H$_2$ lines in the vibrational ground state from 0-0 S(1) to 0-0 S(8), and in the first vibrationally excited state from 1-1 S(5) to 1-1 S(9). We derive H$_2$ excitation diagrams, and approximate the excitation with one thermal (~700 K) component representative of an average gas temperature, and one non-thermal component (~2700 K) probing the effect of UV pumping. We compare these results to an existing model for the Orion Bar PDR and highlight the differences with the observations.","sentences":["Mid-infrared emission features probe the properties of ionized gas, and hot or warm molecular gas.","The Orion Bar is a frequently studied photodissociation region (PDR) containing large amounts of gas under these conditions, and was observed with the MIRI IFU aboard JWST as part of the \"PDRs4All\" program.","The resulting IR spectroscopic images of high angular resolution (0.2\") reveal a rich observational inventory of mid-IR emission lines, and spatially resolve the substructure of the PDR, with a mosaic cutting perpendicularly across the ionization front and three dissociation fronts.","We extracted five spectra that represent the ionized, atomic, and molecular gas layers, and measured the most prominent gas emission lines.","An initial analysis summarizes the physical conditions of the gas and the potential of these data.","We identified around 100 lines, report an additional 18 lines that remain unidentified, and measured the line intensities and central wavelengths.","The H I recombination lines originating from the ionized gas layer bordering the PDR, have intensity ratios that are well matched by emissivity coefficients from H recombination theory, but deviate up to 10% due contamination by He I lines.","We report the observed emission lines of various ionization stages of Ne, P, S, Cl, Ar, Fe, and Ni, and show how certain line ratios vary between the five regions.","We observe the pure-rotational H$_2$ lines in the vibrational ground state from 0-0 S(1) to 0-0 S(8), and in the first vibrationally excited state from 1-1 S(5) to 1-1 S(9).","We derive H$_2$ excitation diagrams, and approximate the excitation with one thermal (~700 K) component representative of an average gas temperature, and one non-thermal component (~2700 K) probing the effect of UV pumping.","We compare these results to an existing model for the Orion Bar PDR and highlight the differences with the observations."],"url":"http://arxiv.org/abs/2404.03111v1","category":"astro-ph.GA"}
{"created":"2024-04-03 23:15:26","title":"Reducing the Impact of I/O Contention in Numerical Weather Prediction Workflows at Scale Using DAOS","abstract":"Operational Numerical Weather Prediction (NWP) workflows are highly data-intensive. Data volumes have increased by many orders of magnitude over the last 40 years, and are expected to continue to do so, especially given the upcoming adoption of Machine Learning in forecast processes. Parallel POSIX-compliant file systems have been the dominant paradigm in data storage and exchange in HPC workflows for many years. This paper presents ECMWF's move beyond the POSIX paradigm, implementing a backend for their storage library to support DAOS -- a novel high-performance object store designed for massively distributed Non-Volatile Memory. This system is demonstrated to be able to outperform the highly mature and optimised POSIX backend when used under high load and contention, as per typical forecast workflow I/O patterns. This work constitutes a significant step forward, beyond the performance constraints imposed by POSIX semantics.","sentences":["Operational Numerical Weather Prediction (NWP) workflows are highly data-intensive.","Data volumes have increased by many orders of magnitude over the last 40 years, and are expected to continue to do so, especially given the upcoming adoption of Machine Learning in forecast processes.","Parallel POSIX-compliant file systems have been the dominant paradigm in data storage and exchange in HPC workflows for many years.","This paper presents ECMWF's move beyond the POSIX paradigm, implementing a backend for their storage library to support DAOS -- a novel high-performance object store designed for massively distributed Non-Volatile Memory.","This system is demonstrated to be able to outperform the highly mature and optimised POSIX backend when used under high load and contention, as per typical forecast workflow I/O patterns.","This work constitutes a significant step forward, beyond the performance constraints imposed by POSIX semantics."],"url":"http://arxiv.org/abs/2404.03107v1","category":"cs.DC"}
{"created":"2024-04-03 23:07:24","title":"Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation","abstract":"Mechanical ventilation is a critical life-support intervention that uses a machine to deliver controlled air and oxygen to a patient's lungs, assisting or replacing spontaneous breathing. While several data-driven approaches have been proposed to optimize ventilator control strategies, they often lack interpretability and agreement with general domain knowledge. This paper proposes a methodology for interpretable reinforcement learning (RL) using decision trees for mechanical ventilation control. Using a causal, nonparametric model-based off-policy evaluation, we evaluate the policies in their ability to gain increases in SpO2 while avoiding aggressive ventilator settings which are known to cause ventilator induced lung injuries and other complications. Numerical experiments using MIMIC-III data on the stays of real patients' intensive care unit stays demonstrate that the decision tree policy outperforms the behavior cloning policy and is comparable to state-of-the-art RL policy. Future work concerns better aligning the cost function with medical objectives to generate deeper clinical insights.","sentences":["Mechanical ventilation is a critical life-support intervention that uses a machine to deliver controlled air and oxygen to a patient's lungs, assisting or replacing spontaneous breathing.","While several data-driven approaches have been proposed to optimize ventilator control strategies, they often lack interpretability and agreement with general domain knowledge.","This paper proposes a methodology for interpretable reinforcement learning (RL) using decision trees for mechanical ventilation control.","Using a causal, nonparametric model-based off-policy evaluation, we evaluate the policies in their ability to gain increases in SpO2 while avoiding aggressive ventilator settings which are known to cause ventilator induced lung injuries and other complications.","Numerical experiments using MIMIC-III data on the stays of real patients' intensive care unit stays demonstrate that the decision tree policy outperforms the behavior cloning policy and is comparable to state-of-the-art RL policy.","Future work concerns better aligning the cost function with medical objectives to generate deeper clinical insights."],"url":"http://arxiv.org/abs/2404.03105v1","category":"cs.LG"}
{"created":"2024-04-03 22:06:14","title":"Direct detection of light dark matter charged under a $L_\u03bc - L_\u03c4$ symmetry","abstract":"A possible extension of the Standard Model able to explain the recent measurement of the anomalous magnetic moment of the muon consists in adding a gauged $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry. If the dark matter particle is charged under this symmetry, the kinetic mixing between the new gauge boson and the photon induces dark matter-electron interactions. We derive direct detection constraints on light dark matter charged under a $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry with electron recoil experiments, and explore prospects with XLZD and OSCURA to close in the parameter space able to explain simultaneously the recent measurement on the anomalous magnetic moment of the muon and the observed relic density of dark matter. We further discuss the spin-dependent scattering contribution arising in this model, which was ignored previously in the literature.","sentences":["A possible extension of the Standard Model able to explain the recent measurement of the anomalous magnetic moment of the muon consists in adding a gauged $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry.","If the dark matter particle is charged under this symmetry, the kinetic mixing between the new gauge boson and the photon induces dark matter-electron interactions.","We derive direct detection constraints on light dark matter charged under a $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetry with electron recoil experiments, and explore prospects with XLZD and OSCURA to close in the parameter space able to explain simultaneously the recent measurement on the anomalous magnetic moment of the muon and the observed relic density of dark matter.","We further discuss the spin-dependent scattering contribution arising in this model, which was ignored previously in the literature."],"url":"http://arxiv.org/abs/2404.03090v1","category":"hep-ph"}
{"created":"2024-04-03 22:01:26","title":"Auditing the Use of Language Models to Guide Hiring Decisions","abstract":"Regulatory efforts to protect against algorithmic bias have taken on increased urgency with rapid advances in large language models (LLMs), which are machine learning models that can achieve performance rivaling human experts on a wide array of tasks. A key theme of these initiatives is algorithmic \"auditing,\" but current regulations -- as well as the scientific literature -- provide little guidance on how to conduct these assessments. Here we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgements. In the employment context, correspondence experiments aim to measure the extent to which race and gender impact decisions by experimentally manipulating elements of submitted application materials that suggest an applicant's demographic traits, such as their listed name. We apply this method to audit candidate assessments produced by several state-of-the-art LLMs, using a novel corpus of applications to K-12 teaching positions in a large public school district. We find evidence of moderate race and gender disparities, a pattern largely robust to varying the types of application material input to the models, as well as the framing of the task to the LLMs. We conclude by discussing some important limitations of correspondence experiments for auditing algorithms.","sentences":["Regulatory efforts to protect against algorithmic bias have taken on increased urgency with rapid advances in large language models (LLMs), which are machine learning models that can achieve performance rivaling human experts on a wide array of tasks.","A key theme of these initiatives is algorithmic \"auditing,\" but current regulations -- as well as the scientific literature -- provide little guidance on how to conduct these assessments.","Here we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgements.","In the employment context, correspondence experiments aim to measure the extent to which race and gender impact decisions by experimentally manipulating elements of submitted application materials that suggest an applicant's demographic traits, such as their listed name.","We apply this method to audit candidate assessments produced by several state-of-the-art LLMs, using a novel corpus of applications to K-12 teaching positions in a large public school district.","We find evidence of moderate race and gender disparities, a pattern largely robust to varying the types of application material input to the models, as well as the framing of the task to the LLMs.","We conclude by discussing some important limitations of correspondence experiments for auditing algorithms."],"url":"http://arxiv.org/abs/2404.03086v1","category":"stat.AP"}
{"created":"2024-04-03 21:45:27","title":"vPALs: Towards Verified Performance-aware Learning System For Resource Management","abstract":"Accurately predicting task performance at runtime in a cluster is advantageous for a resource management system to determine whether a task should be migrated due to performance degradation caused by interference. This is beneficial for both cluster operators and service owners. However, deploying performance prediction systems with learning methods requires sophisticated safeguard mechanisms due to the inherent stochastic and black-box natures of these models, such as Deep Neural Networks (DNNs). Vanilla Neural Networks (NNs) can be vulnerable to out-of-distribution data samples that can lead to sub-optimal decisions. To take a step towards a safe learning system in performance prediction, We propose vPALs that leverage well-correlated system metrics, and verification to produce safe performance prediction at runtime, providing an extra layer of safety to integrate learning techniques to cluster resource management systems. Our experiments show that vPALs can outperform vanilla NNs across our benchmark workload.","sentences":["Accurately predicting task performance at runtime in a cluster is advantageous for a resource management system to determine whether a task should be migrated due to performance degradation caused by interference.","This is beneficial for both cluster operators and service owners.","However, deploying performance prediction systems with learning methods requires sophisticated safeguard mechanisms due to the inherent stochastic and black-box natures of these models, such as Deep Neural Networks (DNNs).","Vanilla Neural Networks (NNs) can be vulnerable to out-of-distribution data samples that can lead to sub-optimal decisions.","To take a step towards a safe learning system in performance prediction, We propose vPALs that leverage well-correlated system metrics, and verification to produce safe performance prediction at runtime, providing an extra layer of safety to integrate learning techniques to cluster resource management systems.","Our experiments show that vPALs can outperform vanilla NNs across our benchmark workload."],"url":"http://arxiv.org/abs/2404.03079v1","category":"cs.DC"}
{"created":"2024-04-03 21:37:49","title":"Quadrilateral Particle Arrangement within Shocks in a Two-Dimensional Dusty Plasma","abstract":"The microscopic structure within a two-dimensional shock was studied using data from a dusty plasma experiment. A single layer of charged microparticles, levitated in a glow-discharge plasma, was perturbed by an electrically floating wire that was moved at a steady supersonic speed to excite a compressional shock. A rearrangement of particles was observed, from a hexagonal lattice in the preshock into a quadrilateral microstructure within the shock. This quadrilateral structure would not be stable in a monolayer of identical repulsive particles, under equilibrium conditions. Glaser-Clark polygon analysis of the microstructure helped in identifying quadrilaterals. Voronoi analysis was used to characterize the defect fraction behind the shock, as an indication of shock-induced melting.","sentences":["The microscopic structure within a two-dimensional shock was studied using data from a dusty plasma experiment.","A single layer of charged microparticles, levitated in a glow-discharge plasma, was perturbed by an electrically floating wire that was moved at a steady supersonic speed to excite a compressional shock.","A rearrangement of particles was observed, from a hexagonal lattice in the preshock into a quadrilateral microstructure within the shock.","This quadrilateral structure would not be stable in a monolayer of identical repulsive particles, under equilibrium conditions.","Glaser-Clark polygon analysis of the microstructure helped in identifying quadrilaterals.","Voronoi analysis was used to characterize the defect fraction behind the shock, as an indication of shock-induced melting."],"url":"http://arxiv.org/abs/2404.03078v1","category":"physics.plasm-ph"}
{"created":"2024-04-03 21:30:46","title":"PowerSimulations.jl -- A Power Systems operations simulation Library","abstract":"PowerSimulations.jl is a Julia-based BSD-licensed power system operations simulation tool developed as a flexible and open source software for quasi-static power systems simulations including Production Cost Models. PowerSimulations.jl tackles the issues of developing a simulation model in a modular way providing tools for the formulation of decision models and emulation models that can be solved independently or in an interconnected fashion. This paper discusses the software implementation of PowerSimulations.jl as a template for the development and implementation of operation simulators, providing solutions to commonly encountered issues like time series read/write and results sharing between models. The paper includes a publicly-available validation of classical operations simulations as well as examples of the advanced features of the software.","sentences":["PowerSimulations.jl is a Julia-based BSD-licensed power system operations simulation tool developed as a flexible and open source software for quasi-static power systems simulations including Production Cost Models.","PowerSimulations.jl tackles the issues of developing a simulation model in a modular way providing tools for the formulation of decision models and emulation models that can be solved independently or in an interconnected fashion.","This paper discusses the software implementation of PowerSimulations.jl as a template for the development and implementation of operation simulators, providing solutions to commonly encountered issues like time series read/write and results sharing between models.","The paper includes a publicly-available validation of classical operations simulations as well as examples of the advanced features of the software."],"url":"http://arxiv.org/abs/2404.03074v1","category":"eess.SY"}
{"created":"2024-04-03 20:56:33","title":"Asymptotically-exact selective inference for quantile regression","abstract":"When analyzing large datasets, it is common to select a model prior to making inferences. For reliable inferences, it is important to make adjustments that account for the model selection process, resulting in selective inferences. Our paper introduces an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions. Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and ensures asymptotically-exact selective inferences without making strict distributional assumptions about the response variable. At the core of the pivot is the use of external randomization, which enables us to utilize the full sample for both selection and inference without the need to partition the data into independent data subsets or discard data at either step. On simulated data, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings. We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022.","sentences":["When analyzing large datasets, it is common to select a model prior to making inferences.","For reliable inferences, it is important to make adjustments that account for the model selection process, resulting in selective inferences.","Our paper introduces an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions.","Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and ensures asymptotically-exact selective inferences without making strict distributional assumptions about the response variable.","At the core of the pivot is the use of external randomization, which enables us to utilize the full sample for both selection and inference without the need to partition the data into independent data subsets or discard data at either step.","On simulated data, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings.","We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022."],"url":"http://arxiv.org/abs/2404.03059v1","category":"stat.ME"}
{"created":"2024-04-03 20:38:47","title":"Emission Line Predictions for Mock Galaxy Catalogues: a New Differentiable and Empirical Mapping from DESI","abstract":"We present a simple, differentiable method for predicting emission line strengths from rest-frame optical continua using an empirically-determined mapping. Extensive work has been done to develop mock galaxy catalogues that include robust predictions for galaxy photometry, but reliably predicting the strengths of emission lines has remained challenging. Our new mapping is a simple neural network implemented using the JAX Python automatic differentiation library. It is trained on Dark Energy Spectroscopic Instrument Early Release data to predict the equivalent widths (EWs) of the eight brightest optical emission lines (including H$\\alpha$, H$\\beta$, [O II], and [O III]) from a galaxy's rest-frame optical continuum. The predicted EW distributions are consistent with the observed ones when noise is accounted for, and we find Spearman's rank correlation coefficient $\\rho_s > 0.87$ between predictions and observations for most lines. Using a non-linear dimensionality reduction technique (UMAP), we show that this is true for galaxies across the full range of observed spectral energy distributions. In addition, we find that adding measurement uncertainties to the predicted line strengths is essential for reproducing the distribution of observed line-ratios in the BPT diagram. Our trained network can easily be incorporated into a differentiable stellar population synthesis pipeline without hindering differentiability or scalability with GPUs. A synthetic catalogue generated with such a pipeline can be used to characterise and account for biases in the spectroscopic training sets used for training and calibration of photo-$z$'s, improving the modelling of systematic incompleteness for the Rubin Observatory LSST and other surveys.","sentences":["We present a simple, differentiable method for predicting emission line strengths from rest-frame optical continua using an empirically-determined mapping.","Extensive work has been done to develop mock galaxy catalogues that include robust predictions for galaxy photometry, but reliably predicting the strengths of emission lines has remained challenging.","Our new mapping is a simple neural network implemented using the JAX Python automatic differentiation library.","It is trained on Dark Energy Spectroscopic Instrument Early Release data to predict the equivalent widths (EWs) of the eight brightest optical emission lines (including H$\\alpha$, H$\\beta$, [O II], and [O III]) from a galaxy's rest-frame optical continuum.","The predicted EW distributions are consistent with the observed ones when noise is accounted for, and we find Spearman's rank correlation coefficient $\\rho_s > 0.87$ between predictions and observations for most lines.","Using a non-linear dimensionality reduction technique (UMAP), we show that this is true for galaxies across the full range of observed spectral energy distributions.","In addition, we find that adding measurement uncertainties to the predicted line strengths is essential for reproducing the distribution of observed line-ratios in the BPT diagram.","Our trained network can easily be incorporated into a differentiable stellar population synthesis pipeline without hindering differentiability or scalability with GPUs.","A synthetic catalogue generated with such a pipeline can be used to characterise and account for biases in the spectroscopic training sets used for training and calibration of photo-$z$'s, improving the modelling of systematic incompleteness for the Rubin Observatory LSST and other surveys."],"url":"http://arxiv.org/abs/2404.03055v1","category":"astro-ph.GA"}
{"created":"2024-04-03 20:15:29","title":"Have any LISA verification binaries been found?","abstract":"Some electromagnetically observed ultra-compact binaries will be strong gravitational wave sources for space-based detectors like the Laser Interferometer Space Antenna (LISA). These sources have historically been referred to as \"verification binaries\" under the assumption that they will be exploited to assess mission performance. This paper quantitatively interrogates that scenario by considering targeted analyses of known galactic sources in the context of a full simulation of the galactic gravitational wave foreground. We find that the analysis of the best currently known LISA binaries, even making maximal use of the available information about the sources, is susceptible to ambiguity or biases when not simultaneously fitting to the rest of the galactic population. While galactic binaries discovered electromagnetically in advance of, or during, the LISA survey are highly valuable multimessenger systems, the need for a global treatment of the galactic gravitational wave foreground calls into question whether or not they are the best sources for data characterization.","sentences":["Some electromagnetically observed ultra-compact binaries will be strong gravitational wave sources for space-based detectors like the Laser Interferometer Space Antenna (LISA).","These sources have historically been referred to as \"verification binaries\" under the assumption that they will be exploited to assess mission performance.","This paper quantitatively interrogates that scenario by considering targeted analyses of known galactic sources in the context of a full simulation of the galactic gravitational wave foreground.","We find that the analysis of the best currently known LISA binaries, even making maximal use of the available information about the sources, is susceptible to ambiguity or biases when not simultaneously fitting to the rest of the galactic population.","While galactic binaries discovered electromagnetically in advance of, or during, the LISA survey are highly valuable multimessenger systems, the need for a global treatment of the galactic gravitational wave foreground calls into question whether or not they are the best sources for data characterization."],"url":"http://arxiv.org/abs/2404.03046v1","category":"astro-ph.HE"}
{"created":"2024-04-03 20:15:16","title":"Analysis of a VEM-fully discrete polytopal scheme with bubble stabilisation for contact mechanics with Tresca friction","abstract":"This work performs the convergence analysis of the polytopal nodal discretisation of contact-mechanics (with Tresca friction) recently introduced in [18] in the framework of poro-elastic models in fractured porous media. The scheme is based on a mixed formulation, using face-wise constant approximations of the Lagrange multipliers along the fracture network and a fully discrete first order nodal approximation of the displacement field. The displacement field is enriched with additional bubble degrees of freedom along the fractures to ensure the inf-sup stability with the Lagrange multiplier space. It is presented in a fully discrete formulation, which makes its study more straightforward, but also has a Virtual Element interpretation. The analysis establishes an abstract error estimate accounting for the fully discrete framework and the non-conformity of the discretisation. A first order error estimate is deduced for sufficiently smooth solutions both for the gradient of the displacement field and the Lagrange multiplier. A key difficulty of the numerical analysis is the proof of a discrete inf-sup condition, which is based on a non-standard $H^{-1/2}$-norm (to deal with fracture networks) and involves the jump of the displacements, not their traces. The analysis also requires the proof of a discrete Korn inequality for the discrete displacement field which takes into account fracture networks. Numerical experiments based on analytical solutions confirm our theoretical findings","sentences":["This work performs the convergence analysis of the polytopal nodal discretisation of contact-mechanics (with Tresca friction) recently introduced in [18] in the framework of poro-elastic models in fractured porous media.","The scheme is based on a mixed formulation, using face-wise constant approximations of the Lagrange multipliers along the fracture network and a fully discrete first order nodal approximation of the displacement field.","The displacement field is enriched with additional bubble degrees of freedom along the fractures to ensure the inf-sup stability with the Lagrange multiplier space.","It is presented in a fully discrete formulation, which makes its study more straightforward, but also has a Virtual Element interpretation.","The analysis establishes an abstract error estimate accounting for the fully discrete framework and the non-conformity of the discretisation.","A first order error estimate is deduced for sufficiently smooth solutions both for the gradient of the displacement field and the Lagrange multiplier.","A key difficulty of the numerical analysis is the proof of a discrete inf-sup condition, which is based on a non-standard $H^{-1/2}$-norm (to deal with fracture networks) and involves the jump of the displacements, not their traces.","The analysis also requires the proof of a discrete Korn inequality for the discrete displacement field which takes into account fracture networks.","Numerical experiments based on analytical solutions confirm our theoretical findings"],"url":"http://arxiv.org/abs/2404.03045v1","category":"math.NA"}
{"created":"2024-04-03 20:05:00","title":"Linear Anchored Gaussian Mixture Model for Location and Width Computation of Objects in Thick Line Shape","abstract":"An accurate detection of the centerlines of linear objects is a challenging topic in many sensitive real-world applications such X-ray imaging, remote sensing and lane marking detection in road traffic. Model-based approaches using Hough and Radon transforms are often used but, are not recommended for thick line detection, whereas approaches based on image derivatives need further step-by-step processing, making their efficiency dependent on each step outcomes. In this paper, we aim to detect linear structures found in images by considering the 3D representation of the image gray levels as a finite mixture model of statistical distribution. The latter, which we named linear anchored Gaussian distribution could be parametrized by a scale value {\\sigma} describing the linear structure thickness and a line equation, parametrized, in turn, by a radius \\r{ho} and an orientation angle {\\theta}, describing the linear structure centerline location. Expectation-Maximization (EM) algorithm is used for the mixture model parameter estimation, where a new paradigm, using the background subtraction for the likelihood function computation, is proposed. For the EM algorithm, two {\\theta} parameter initialization schemes are used: the first one is based on a random choice of the first component of {\\theta} vector, whereas the second is based on the image Hessian with a simultaneous computation of the mixture model components number. Experiments on real world images and synthetic images corrupted by blur and additive noise show the good performance of the proposed methods, where the algorithm using background subtraction and Hessian-based {\\theta} initialization provides an outstanding accuracy of the linear structure detection despite irregular image background and presence of blur and noise.","sentences":["An accurate detection of the centerlines of linear objects is a challenging topic in many sensitive real-world applications such X-ray imaging, remote sensing and lane marking detection in road traffic.","Model-based approaches using Hough and Radon transforms are often used but, are not recommended for thick line detection, whereas approaches based on image derivatives need further step-by-step processing, making their efficiency dependent on each step outcomes.","In this paper, we aim to detect linear structures found in images by considering the 3D representation of the image gray levels as a finite mixture model of statistical distribution.","The latter, which we named linear anchored Gaussian distribution could be parametrized by a scale value {\\sigma} describing the linear structure thickness and a line equation, parametrized, in turn, by a radius \\r{ho} and an orientation angle {\\theta}, describing the linear structure centerline location.","Expectation-Maximization (EM) algorithm is used for the mixture model parameter estimation, where a new paradigm, using the background subtraction for the likelihood function computation, is proposed.","For the EM algorithm, two {\\theta} parameter initialization schemes are used: the first one is based on a random choice of the first component of {\\theta} vector, whereas the second is based on the image Hessian with a simultaneous computation of the mixture model components number.","Experiments on real world images and synthetic images corrupted by blur and additive noise show the good performance of the proposed methods, where the algorithm using background subtraction and Hessian-based {\\theta} initialization provides an outstanding accuracy of the linear structure detection despite irregular image background and presence of blur and noise."],"url":"http://arxiv.org/abs/2404.03043v1","category":"cs.CV"}
{"created":"2024-04-03 19:34:06","title":"The Impact of Extended H$_{2}$O Cross-Sections on Temperate Anoxic Planet Atmospheres: Implications for Spectral Characterization of Habitable Worlds","abstract":"JWST has created a new era of terrestrial exoplanet atmospheric characterization, and with it the possibility to detect potential biosignature gases like CH$_{4}$. Our interpretation of exoplanet atmospheric spectra, and the veracity of these interpretations, will be limited by our understanding of atmospheric processes and the accuracy of input modeling data. Molecular cross-sections are essential inputs to these models. The photochemistry of temperate planets depends on photolysis reactions whose rates are governed by the dissociation cross-sections of key molecules. H$_{2}$O is one such molecule; the photolysis of H$_{2}$O produces OH, a highly reactive and efficient sink for atmospheric trace gases. We investigate the photochemical effects of improved H$_{2}$O cross-sections on anoxic terrestrial planets as a function of host star spectral type (FGKM) and CH$_{4}$ surface flux. Our results show that updated H$_{2}$O cross-sections, extended to wavelengths $>$200 nm, substantially impact the predicted abundances of trace gases destroyed by OH. The differences for anoxic terrestrial planets orbiting Sun-like host stars are greatest, showing changes of up to three orders of magnitude in surface CO levels, and over an order of magnitude in surface CH$_{4}$ levels. These differences lead to observable changes in simulated planetary spectra, especially important in the context of future direct-imaging missions. In contrast, the atmospheres of planets orbiting M-dwarf stars are substantially less affected. Our results demonstrate a pressing need for refined dissociation cross-section data for H$_{2}$O, where uncertainties remain, and other key molecules, especially at mid-UV wavelengths $>$200 nm.","sentences":["JWST has created a new era of terrestrial exoplanet atmospheric characterization, and with it the possibility to detect potential biosignature gases like CH$_{4}$. Our interpretation of exoplanet atmospheric spectra, and the veracity of these interpretations, will be limited by our understanding of atmospheric processes and the accuracy of input modeling data.","Molecular cross-sections are essential inputs to these models.","The photochemistry of temperate planets depends on photolysis reactions whose rates are governed by the dissociation cross-sections of key molecules.","H$_{2}$O is one such molecule; the photolysis of H$_{2}$O produces OH, a highly reactive and efficient sink for atmospheric trace gases.","We investigate the photochemical effects of improved H$_{2}$O cross-sections on anoxic terrestrial planets as a function of host star spectral type (FGKM) and CH$_{4}$ surface flux.","Our results show that updated H$_{2}$O cross-sections, extended to wavelengths $>$200 nm, substantially impact the predicted abundances of trace gases destroyed by OH.","The differences for anoxic terrestrial planets orbiting Sun-like host stars are greatest, showing changes of up to three orders of magnitude in surface CO levels, and over an order of magnitude in surface CH$_{4}$ levels.","These differences lead to observable changes in simulated planetary spectra, especially important in the context of future direct-imaging missions.","In contrast, the atmospheres of planets orbiting M-dwarf stars are substantially less affected.","Our results demonstrate a pressing need for refined dissociation cross-section data for H$_{2}$O, where uncertainties remain, and other key molecules, especially at mid-UV wavelengths $>$200 nm."],"url":"http://arxiv.org/abs/2404.03031v1","category":"astro-ph.EP"}
{"created":"2024-04-03 19:32:57","title":"Leveraging Apache Arrow for Zero-copy, Zero-serialization Cluster Shared Memory","abstract":"This paper describes a distributed implementation of Apache Arrow that can leverage cluster-shared load-store addressable memory that is hardware-coherent only within each node. The implementation is built on the ThymesisFlow prototype that leverages the OpenCAPI interface to create a shared address space across a cluster. While Apache Arrow structures are immutable, simplifying their use in a cluster shared memory, this paper creates distributed Apache Arrow tables and makes them accessible in each node.","sentences":["This paper describes a distributed implementation of Apache Arrow that can leverage cluster-shared load-store addressable memory that is hardware-coherent only within each node.","The implementation is built on the ThymesisFlow prototype that leverages the OpenCAPI interface to create a shared address space across a cluster.","While Apache Arrow structures are immutable, simplifying their use in a cluster shared memory, this paper creates distributed Apache Arrow tables and makes them accessible in each node."],"url":"http://arxiv.org/abs/2404.03030v1","category":"cs.ET"}
{"created":"2024-04-03 19:31:56","title":"An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models","abstract":"Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.","sentences":["Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions.","Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning.","How do these different capabilities relate?","Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task.","Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures."],"url":"http://arxiv.org/abs/2404.03028v1","category":"cs.CL"}
{"created":"2024-04-03 19:07:39","title":"Probing electron quadrupling order through ultrasound","abstract":"Recent experiments have pointed to the formation of a new state of matter, the electron quadrupling condensate in Ba$_{1-x}$K$_x$Fe$_2$As$_2$ . The state spontaneously breaks time-reversal symmetry and is sandwiched between two critical points, separating it from the superconducting and normal metal states. The adjacent two critical points make acoustic effects a promising tool to study such states because of their sensitivity to symmetry-breaking transitions. We report a theory of the acoustic effects of systems with an electron quadrupling phase and new ultrasound velocity measurements of Ba$_{1-x}$K$_x$Fe$_2$As$_2$ single crystals. The presented theory for the electron quadrupling state gives the same type of singularities that are observed in experiment.","sentences":["Recent experiments have pointed to the formation of a new state of matter, the electron quadrupling condensate in Ba$_{1-x}$K$_x$Fe$_2$As$_2$ .","The state spontaneously breaks time-reversal symmetry and is sandwiched between two critical points, separating it from the superconducting and normal metal states.","The adjacent two critical points make acoustic effects a promising tool to study such states because of their sensitivity to symmetry-breaking transitions.","We report a theory of the acoustic effects of systems with an electron quadrupling phase and new ultrasound velocity measurements of Ba$_{1-x}$K$_x$Fe$_2$As$_2$ single crystals.","The presented theory for the electron quadrupling state gives the same type of singularities that are observed in experiment."],"url":"http://arxiv.org/abs/2404.03020v1","category":"cond-mat.supr-con"}
{"created":"2024-04-03 19:00:19","title":"Incorporating non-linear effects in fast semi-analytical thermal modelling of powder bed fusion","abstract":"The usefulness of semi-analytical thermal models for predicting the connection between process, microstructure and properties in powder bed fusion has been well illustrated in recent years. Such an approach provides the promise of accuracy comparable to tools that are orders of magnitude more computationally expensive. The opportunity to make predictions that span several orders of magnitude in space and time comes at the cost of significant simplifications, limiting fully quantitative predictions without empirical calibration. This approach relies on solving a linear problem meaning that first order non-linear effects induced by e.g. the temperature dependence of material properties and surface boundary conditions, are not incorporated. Here, we revisit these limitations and highlight ways that temperature varying material properties and radiative heat loss from the melt pool can be systematically accounted for. These corrections, made with an eye to minimizing additional computational overhead, bring the technique's predictive capability much closer to that of high fidelity thermal simulations. Quantitative comparisons to experiments are used to illustrate the important impact of including such corrections.","sentences":["The usefulness of semi-analytical thermal models for predicting the connection between process, microstructure and properties in powder bed fusion has been well illustrated in recent years.","Such an approach provides the promise of accuracy comparable to tools that are orders of magnitude more computationally expensive.","The opportunity to make predictions that span several orders of magnitude in space and time comes at the cost of significant simplifications, limiting fully quantitative predictions without empirical calibration.","This approach relies on solving a linear problem meaning that first order non-linear effects induced by e.g. the temperature dependence of material properties and surface boundary conditions, are not incorporated.","Here, we revisit these limitations and highlight ways that temperature varying material properties and radiative heat loss from the melt pool can be systematically accounted for.","These corrections, made with an eye to minimizing additional computational overhead, bring the technique's predictive capability much closer to that of high fidelity thermal simulations.","Quantitative comparisons to experiments are used to illustrate the important impact of including such corrections."],"url":"http://arxiv.org/abs/2404.03018v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 18:57:54","title":"Distributionally Robust Policy and Lyapunov-Certificate Learning","abstract":"This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty. A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment. We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied. We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the global asymptotic stability of its equilibrium can be certified with high confidence, even with Out-of-Distribution (OoD) model uncertainties. To demonstrate the efficacy and efficiency of the proposed methodology, we compare it with an uncertainty-agnostic baseline approach and several reinforcement learning approaches in two control problems in simulation.","sentences":["This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty.","A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment.","We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate.","To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied.","We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the global asymptotic stability of its equilibrium can be certified with high confidence, even with Out-of-Distribution (OoD) model uncertainties.","To demonstrate the efficacy and efficiency of the proposed methodology, we compare it with an uncertainty-agnostic baseline approach and several reinforcement learning approaches in two control problems in simulation."],"url":"http://arxiv.org/abs/2404.03017v1","category":"eess.SY"}
{"created":"2024-04-03 18:54:27","title":"DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection","abstract":"The perception of autonomous vehicles has to be efficient, robust, and cost-effective. However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others. Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information. We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations. Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data. As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time. The code is made available as open-source software under https://github.com/TUMFTM/DPFT.","sentences":["The perception of autonomous vehicles has to be efficient, robust, and cost-effective.","However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others.","Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information.","We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations.","Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data.","As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time.","The code is made available as open-source software under https://github.com/TUMFTM/DPFT."],"url":"http://arxiv.org/abs/2404.03015v1","category":"cs.CV"}
{"created":"2024-04-03 18:41:49","title":"DESI 2024 III: Baryon Acoustic Oscillations from Galaxies and Quasars","abstract":"We present the DESI 2024 galaxy and quasar baryon acoustic oscillations (BAO) measurements using over 5.7 million unique galaxy and quasar redshifts in the range 0.1<z<2.1. Divided by tracer type, we utilize 300,017 galaxies from the magnitude-limited Bright Galaxy Survey with 0.1<z<0.4, 2,138,600 Luminous Red Galaxies with 0.4<z<1.1, 2,432,022 Emission Line Galaxies with 0.8<z<1.6, and 856,652 quasars with 0.8<z<2.1, over a ~7,500 square degree footprint. The analysis was blinded at the catalog-level to avoid confirmation bias. All fiducial choices of the BAO fitting and reconstruction methodology, as well as the size of the systematic errors, were determined on the basis of the tests with mock catalogs and the blinded data catalogs. We present several improvements to the BAO analysis pipeline, including enhancing the BAO fitting and reconstruction methods in a more physically-motivated direction, and also present results using combinations of tracers. We present a re-analysis of SDSS BOSS and eBOSS results applying the improved DESI methodology and find scatter consistent with the level of the quoted SDSS theoretical systematic uncertainties. With the total effective survey volume of ~ 18 Gpc$^3$, the combined precision of the BAO measurements across the six different redshift bins is ~0.52%, marking a 1.2-fold improvement over the previous state-of-the-art results using only first-year data. We detect the BAO in all of these six redshift bins. The highest significance of BAO detection is $9.1\\sigma$ at the effective redshift of 0.93, with a constraint of 0.86% placed on the BAO scale. We find our measurements are systematically larger than the prediction of Planck-2018 LCDM model at z<0.8. We translate the results into transverse comoving distance and radial Hubble distance measurements, which are used to constrain cosmological models in our companion paper [abridged].","sentences":["We present the DESI 2024 galaxy and quasar baryon acoustic oscillations (BAO) measurements using over 5.7 million unique galaxy and quasar redshifts in the range 0.1<z<2.1.","Divided by tracer type, we utilize 300,017 galaxies from the magnitude-limited Bright Galaxy Survey with 0.1<z<0.4, 2,138,600 Luminous Red Galaxies with 0.4<z<1.1, 2,432,022 Emission Line Galaxies with 0.8<z<1.6, and 856,652 quasars with 0.8<z<2.1, over a ~7,500 square degree footprint.","The analysis was blinded at the catalog-level to avoid confirmation bias.","All fiducial choices of the BAO fitting and reconstruction methodology, as well as the size of the systematic errors, were determined on the basis of the tests with mock catalogs and the blinded data catalogs.","We present several improvements to the BAO analysis pipeline, including enhancing the BAO fitting and reconstruction methods in a more physically-motivated direction, and also present results using combinations of tracers.","We present a re-analysis of SDSS BOSS and eBOSS results applying the improved DESI methodology and find scatter consistent with the level of the quoted SDSS theoretical systematic uncertainties.","With the total effective survey volume of ~ 18","Gpc$^3$, the combined precision of the BAO measurements across the six different redshift bins is ~0.52%, marking a 1.2-fold improvement over the previous state-of-the-art results using only first-year data.","We detect the BAO in all of these six redshift bins.","The highest significance of BAO detection is $9.1\\sigma$ at the effective redshift of 0.93, with a constraint of 0.86% placed on the BAO scale.","We find our measurements are systematically larger than the prediction of Planck-2018 LCDM model at z<0.8.","We translate the results into transverse comoving distance and radial Hubble distance measurements, which are used to constrain cosmological models in our companion paper [abridged]."],"url":"http://arxiv.org/abs/2404.03000v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:38:39","title":"The off-shell expansion relation of the Yang-Mills scalar theory","abstract":"In this work, we investigate the off-shell expansion relation of the Yang-Mills scalar theory. We have showed explicitly that the single-trace Berends-Giele current in YMS theory can be decomposed into a term expressed by a linear combination of bi-adjoint scalar Berends-Giele currents and one that vanishes under the on-shell limit. We proved that the bi-adjoint scalar currents, as well as the corresponding coefficients, can be characterized by a graphic approach that was originally studied in Einstein-Yang-Mills expansion. Furthermore, we generalized the decomposition to the multi-trace case through unifying relations and established the connection both in single-trace and multi-trace graphic descriptions. Finally, we established the relations between the Yang-Mills currents and the single-trace Yang-Mills scalar currents choosing special reference orders of the Yang-Mills graphs.","sentences":["In this work, we investigate the off-shell expansion relation of the Yang-Mills scalar theory.","We have showed explicitly that the single-trace Berends-Giele current in YMS theory can be decomposed into a term expressed by a linear combination of bi-adjoint scalar Berends-Giele currents and one that vanishes under the on-shell limit.","We proved that the bi-adjoint scalar currents, as well as the corresponding coefficients, can be characterized by a graphic approach that was originally studied in Einstein-Yang-Mills expansion.","Furthermore, we generalized the decomposition to the multi-trace case through unifying relations and established the connection both in single-trace and multi-trace graphic descriptions.","Finally, we established the relations between the Yang-Mills currents and the single-trace Yang-Mills scalar currents choosing special reference orders of the Yang-Mills graphs."],"url":"http://arxiv.org/abs/2404.02997v1","category":"hep-th"}
{"created":"2024-04-03 18:37:51","title":"Tricks from the Trade for Large-Scale Markdown Pricing: Heuristic Cut Generation for Lagrangian Decomposition","abstract":"In automated decision making processes in the online fashion industry, the 'predict-then-optimize' paradigm is frequently applied, particularly for markdown pricing strategies. This typically involves a mixed-integer optimization step, which is crucial for maximizing profit and merchandise volume. In practice, the size and complexity of the optimization problem is prohibitive for using off-the-shelf solvers for mixed integer programs and specifically tailored approaches are a necessity. Our paper introduces specific heuristics designed to work alongside decomposition methods, leading to almost-optimal solutions. These heuristics, which include both primal heuristic methods and a cutting plane generation technique within a Lagrangian decomposition framework, are the core focus of the present paper. We provide empirical evidence for their effectiveness, drawing on real-world applications at Zalando SE, one of Europe's leading online fashion retailers, highlighting the practical value of our work. The contributions of this paper are deeply ingrained into Zalando's production environment to its large-scale catalog ranging in the millions of products and improving weekly profits by millions of Euros.","sentences":["In automated decision making processes in the online fashion industry, the 'predict-then-optimize' paradigm is frequently applied, particularly for markdown pricing strategies.","This typically involves a mixed-integer optimization step, which is crucial for maximizing profit and merchandise volume.","In practice, the size and complexity of the optimization problem is prohibitive for using off-the-shelf solvers for mixed integer programs and specifically tailored approaches are a necessity.","Our paper introduces specific heuristics designed to work alongside decomposition methods, leading to almost-optimal solutions.","These heuristics, which include both primal heuristic methods and a cutting plane generation technique within a Lagrangian decomposition framework, are the core focus of the present paper.","We provide empirical evidence for their effectiveness, drawing on real-world applications at Zalando SE, one of Europe's leading online fashion retailers, highlighting the practical value of our work.","The contributions of this paper are deeply ingrained into Zalando's production environment to its large-scale catalog ranging in the millions of products and improving weekly profits by millions of Euros."],"url":"http://arxiv.org/abs/2404.02996v1","category":"math.OC"}
{"created":"2024-04-03 18:23:41","title":"Topological Data Analysis for Particulate Gels","abstract":"Soft gels, formed via the self-assembly of particulate organic materials, exhibit intricate multi-scale structures that provides them with flexibility and resilience when subjected to external stresses. This work combines molecular simulations and topological data analysis (TDA) to characterize the complex multi-scale structure of soft gels. Our TDA analysis focuses on the use of the Euler characteristic, which is an interpretable and computationally-scalable topological descriptor that is combined with filtration operations to obtain information on the geometric (local) and topological (global) structure of soft gels. We reduce the topological information obtained with TDA using principal component analysis (PCA) and show that this provides an informative low-dimensional representation of gel structure. We use the proposed computational framework to investigate the influence of gel preparation (e.g., quench rate, volume fraction) on soft gel structure and to explore dynamic deformations that emerge under oscillatory shear in various response regimes (linear, nonlinear, and flow). Our analysis identifies specific scales and extents at which hierarchical structures in soft gels are affected; moreover, correlations between structural deformations and mechanical phenomena (such as shear stiffening) are explored. In summary, we show that TDA facilitates the mathematical representation, quantification, and analysis of soft gel structures, extending traditional network analysis methods to capture both local and global organization.","sentences":["Soft gels, formed via the self-assembly of particulate organic materials, exhibit intricate multi-scale structures that provides them with flexibility and resilience when subjected to external stresses.","This work combines molecular simulations and topological data analysis (TDA) to characterize the complex multi-scale structure of soft gels.","Our TDA analysis focuses on the use of the Euler characteristic, which is an interpretable and computationally-scalable topological descriptor that is combined with filtration operations to obtain information on the geometric (local) and topological (global) structure of soft gels.","We reduce the topological information obtained with TDA using principal component analysis (PCA) and show that this provides an informative low-dimensional representation of gel structure.","We use the proposed computational framework to investigate the influence of gel preparation (e.g., quench rate, volume fraction) on soft gel structure and to explore dynamic deformations that emerge under oscillatory shear in various response regimes (linear, nonlinear, and flow).","Our analysis identifies specific scales and extents at which hierarchical structures in soft gels are affected; moreover, correlations between structural deformations and mechanical phenomena (such as shear stiffening) are explored.","In summary, we show that TDA facilitates the mathematical representation, quantification, and analysis of soft gel structures, extending traditional network analysis methods to capture both local and global organization."],"url":"http://arxiv.org/abs/2404.02991v1","category":"cond-mat.soft"}
{"created":"2024-04-03 18:16:47","title":"Risk-averse Learning with Non-Stationary Distributions","abstract":"Considering non-stationary environments in online optimization enables decision-maker to effectively adapt to changes and improve its performance over time. In such cases, it is favorable to adopt a strategy that minimizes the negative impact of change to avoid potentially risky situations. In this paper, we investigate risk-averse online optimization where the distribution of the random cost changes over time. We minimize risk-averse objective function using the Conditional Value at Risk (CVaR) as risk measure. Due to the difficulty in obtaining the exact CVaR gradient, we employ a zeroth-order optimization approach that queries the cost function values multiple times at each iteration and estimates the CVaR gradient using the sampled values. To facilitate the regret analysis, we use a variation metric based on Wasserstein distance to capture time-varying distributions. Given that the distribution variation is sub-linear in the total number of episodes, we show that our designed learning algorithm achieves sub-linear dynamic regret with high probability for both convex and strongly convex functions. Moreover, theoretical results suggest that increasing the number of samples leads to a reduction in the dynamic regret bounds until the sampling number reaches a specific limit. Finally, we provide numerical experiments of dynamic pricing in a parking lot to illustrate the efficacy of the designed algorithm.","sentences":["Considering non-stationary environments in online optimization enables decision-maker to effectively adapt to changes and improve its performance over time.","In such cases, it is favorable to adopt a strategy that minimizes the negative impact of change to avoid potentially risky situations.","In this paper, we investigate risk-averse online optimization where the distribution of the random cost changes over time.","We minimize risk-averse objective function using the Conditional Value at Risk (CVaR) as risk measure.","Due to the difficulty in obtaining the exact CVaR gradient, we employ a zeroth-order optimization approach that queries the cost function values multiple times at each iteration and estimates the CVaR gradient using the sampled values.","To facilitate the regret analysis, we use a variation metric based on Wasserstein distance to capture time-varying distributions.","Given that the distribution variation is sub-linear in the total number of episodes, we show that our designed learning algorithm achieves sub-linear dynamic regret with high probability for both convex and strongly convex functions.","Moreover, theoretical results suggest that increasing the number of samples leads to a reduction in the dynamic regret bounds until the sampling number reaches a specific limit.","Finally, we provide numerical experiments of dynamic pricing in a parking lot to illustrate the efficacy of the designed algorithm."],"url":"http://arxiv.org/abs/2404.02988v1","category":"eess.SY"}
{"created":"2024-04-03 18:14:23","title":"Universal Functional Regression with Neural Operator Flows","abstract":"Regression on function spaces is typically limited to models with Gaussian process priors. We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression. To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows. OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations. OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space. We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution.","sentences":["Regression on function spaces is typically limited to models with Gaussian process priors.","We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression.","To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows.","OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations.","OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space.","We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution."],"url":"http://arxiv.org/abs/2404.02986v1","category":"cs.LG"}
{"created":"2024-04-03 18:14:11","title":"R-matrix with time-dependence calculations for three-sideband RABBITT in helium","abstract":"Following up on a recent paper [Bharti et al., Phys. Rev. A 109 (2024) 023110], we compare the predictions from severalR-matrix with time-dependence calculations for a modified three-sideband version of the \"reconstruction of attosecond beating by interference of two-photon transitions\" (RABBITT) configuration applied to helium. Except for the special case of the threshold sideband, which appears to be very sensitive to the details of coupling to the bound Rydberg states, increasing the number of coupled states in the close-coupling expansion used to describe the ejected-electron--residual-ion interaction hardly changes the results. Consequently, the remaining discrepancies between the experimental data and the theoretical predictions are likely due to uncertainties in the experimental parameters, particularly the detailed knowledge of the laser pulse.","sentences":["Following up on a recent paper [Bharti et al., Phys.","Rev.","A 109 (2024) 023110], we compare the predictions from severalR-matrix with time-dependence calculations for a modified three-sideband version of the \"reconstruction of attosecond beating by interference of two-photon transitions\" (RABBITT) configuration applied to helium.","Except for the special case of the threshold sideband, which appears to be very sensitive to the details of coupling to the bound Rydberg states, increasing the number of coupled states in the close-coupling expansion used to describe the ejected-electron--residual-ion interaction hardly changes the results.","Consequently, the remaining discrepancies between the experimental data and the theoretical predictions are likely due to uncertainties in the experimental parameters, particularly the detailed knowledge of the laser pulse."],"url":"http://arxiv.org/abs/2404.02985v1","category":"physics.atom-ph"}
{"created":"2024-04-03 18:07:02","title":"Spatio-temporal Modeling of Count Data","abstract":"We introduce parsimonious parameterisations for multivariate autoregressive count time series models for spatio-temporal data, including possible regressions on covariates. The number of parameters is reduced by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies. Consistency and asymptotic normality of the parameter estimators are obtained under mild assumptions by employing quasi-maximum likelihood methodology. This is used to obtain an asymptotic Wald test for testing the significance of individual or group effects. Several simulations and two data examples support and illustrate the methods proposed in this paper.","sentences":["We introduce parsimonious parameterisations for multivariate autoregressive count time series models for spatio-temporal data, including possible regressions on covariates.","The number of parameters is reduced by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies.","Consistency and asymptotic normality of the parameter estimators are obtained under mild assumptions by employing quasi-maximum likelihood methodology.","This is used to obtain an asymptotic Wald test for testing the significance of individual or group effects.","Several simulations and two data examples support and illustrate the methods proposed in this paper."],"url":"http://arxiv.org/abs/2404.02982v1","category":"stat.ME"}
{"created":"2024-04-03 18:03:59","title":"Quasi-Random Frequency Sampling for Optical Turbulence Simulations","abstract":"Optical turbulence modelling and simulation are crucial for developing astronomical ground-based instruments, laser communication, laser metrology, or any application where light propagates through a turbulent medium. In the context of spectrum-based optical turbulence Monte-Carlo simulations, we present an alternative approach to the methods based on the Fast Fourier Transform (FFT) using a quasi-random frequency sampling heuristic. This approach provides complete control over the spectral information expressed in the simulated measurable, without the drawbacks encountered with FFT-based methods such as high-frequency aliasing, low-frequency under-sampling, and static sampling statistics. The method's heuristics, implementation, and an application example from the study of differential piston fluctuations are discussed.","sentences":["Optical turbulence modelling and simulation are crucial for developing astronomical ground-based instruments, laser communication, laser metrology, or any application where light propagates through a turbulent medium.","In the context of spectrum-based optical turbulence Monte-Carlo simulations, we present an alternative approach to the methods based on the Fast Fourier Transform (FFT) using a quasi-random frequency sampling heuristic.","This approach provides complete control over the spectral information expressed in the simulated measurable, without the drawbacks encountered with FFT-based methods such as high-frequency aliasing, low-frequency under-sampling, and static sampling statistics.","The method's heuristics, implementation, and an application example from the study of differential piston fluctuations are discussed."],"url":"http://arxiv.org/abs/2404.02978v1","category":"astro-ph.IM"}
{"created":"2024-04-03 18:00:48","title":"NGTS-30 b/TOI-4862 b: An 1 Gyr old 98-day transiting warm Jupiter","abstract":"Long-period transiting exoplanets bridge the gap between the bulk of transit- and Doppler-based exoplanet discoveries, providing key insights into the formation and evolution of planetary systems. The wider separation between these planets and their host stars results in the exoplanets typically experiencing less radiation from their host stars; hence, they should maintain more of their original atmospheres, which can be probed during transit via transmission spectroscopy. Although the known population of long-period transiting exoplanets is relatively sparse, surveys performed by the Transiting Exoplanet Survey Satellite (TESS) and the Next Generation Transit Survey (NGTS) are now discovering new exoplanets to fill in this crucial region of the exoplanetary parameter space. This study presents the detection and characterisation of NGTS-30 b/TOI-4862 b, a new long-period transiting exoplanet detected by following up on a single-transit candidate found in the TESS mission. Through monitoring using a combination of photometric instruments (TESS, NGTS, and EulerCam) and spectroscopic instruments (CORALIE, FEROS, HARPS, and PFS), NGTS-30 b/TOI-4862 b was found to be a long-period (P = 98.29838 day) Jupiter-sized (0.928 RJ; 0.960 MJ) planet transiting a 1.1 Gyr old G-type star. With a moderate eccentricity of 0.294, its equilibrium temperature could be expected to vary from 274 K to 500 K over the course of its orbit. Through interior modelling, NGTS-30 b/TOI-4862 b was found to have a heavy element mass fraction of 0.23 and a heavy element enrichment (Zp/Z_star) of 20, making it metal-enriched compared to its host star. NGTS-30 b/TOI-4862 b is one of the youngest well-characterised long-period exoplanets found to date and will therefore be important in the quest to understanding the formation and evolution of exoplanets across the full range of orbital separations and ages.","sentences":["Long-period transiting exoplanets bridge the gap between the bulk of transit- and Doppler-based exoplanet discoveries, providing key insights into the formation and evolution of planetary systems.","The wider separation between these planets and their host stars results in the exoplanets typically experiencing less radiation from their host stars; hence, they should maintain more of their original atmospheres, which can be probed during transit via transmission spectroscopy.","Although the known population of long-period transiting exoplanets is relatively sparse, surveys performed by the Transiting Exoplanet Survey Satellite (TESS) and the Next Generation Transit Survey (NGTS) are now discovering new exoplanets to fill in this crucial region of the exoplanetary parameter space.","This study presents the detection and characterisation of NGTS-30 b/TOI-4862 b, a new long-period transiting exoplanet detected by following up on a single-transit candidate found in the TESS mission.","Through monitoring using a combination of photometric instruments (TESS, NGTS, and EulerCam) and spectroscopic instruments (CORALIE, FEROS, HARPS, and PFS), NGTS-30 b/TOI-4862 b was found to be a long-period (P = 98.29838 day)","Jupiter-sized (0.928 RJ; 0.960 MJ) planet transiting a 1.1 Gyr old G-type star.","With a moderate eccentricity of 0.294, its equilibrium temperature could be expected to vary from 274 K to 500 K over the course of its orbit.","Through interior modelling, NGTS-30 b/TOI-4862 b was found to have a heavy element mass fraction of 0.23 and a heavy element enrichment (Zp/Z_star) of 20, making it metal-enriched compared to its host star.","NGTS-30 b/TOI-4862 b is one of the youngest well-characterised long-period exoplanets found to date and will therefore be important in the quest to understanding the formation and evolution of exoplanets across the full range of orbital separations and ages."],"url":"http://arxiv.org/abs/2404.02974v1","category":"astro-ph.EP"}
{"created":"2024-04-03 18:00:09","title":"A novel strategy to prove chiral symmetry breaking in QCD-like theories","abstract":"We demonstrate that chiral symmetry breaking occurs in the confining phase of QCD-like theories with $N_c$ colors and $N_f$ flavors. Our proof is based on a novel strategy, called `downlifting', by which solutions of the 't Hooft anomaly matching and persistent mass conditions for a theory with $N_f-1$ flavors are constructed from those of a theory with $N_f$ flavors, while $N_c$ is fixed. By induction, chiral symmetry breaking is proven for any $N_f\\geq p_{min}$, where $p_{min}$ is the smallest prime factor of $N_c$. The proof can be extended to $N_f <p_{min}$ under the additional assumption on the absence of phase transitions when quark masses are sent to infinity. Our results do not rely on ad-hoc assumptions on the spectrum of massless bound states.","sentences":["We demonstrate that chiral symmetry breaking occurs in the confining phase of QCD-like theories with $N_c$ colors and $N_f$ flavors.","Our proof is based on a novel strategy, called `downlifting', by which solutions of the 't Hooft anomaly matching and persistent mass conditions for a theory with $N_f-1$ flavors are constructed from those of a theory with $N_f$ flavors, while $N_c$ is fixed.","By induction, chiral symmetry breaking is proven for any $N_f\\geq p_{min}$, where $p_{min}$ is the smallest prime factor of $N_c$. The proof can be extended to $N_f <p_{min}$ under the additional assumption on the absence of phase transitions when quark masses are sent to infinity.","Our results do not rely on ad-hoc assumptions on the spectrum of massless bound states."],"url":"http://arxiv.org/abs/2404.02967v1","category":"hep-th"}
{"created":"2024-04-03 18:00:04","title":"Hamiltonian Simulation in the Interaction Picture Using the Magnus Expansion","abstract":"We propose an algorithm for simulating the dynamics of a geometrically local Hamiltonian $A$ under a small geometrically local perturbation $\\alpha B$. In certain regimes, the algorithm achieves the optimal scaling and outperforms the state-of-the-art algorithms. By moving into the interaction frame of $A$ and classically computing the Magnus expansion of the interaction-picture Hamiltonian, our algorithm bypasses the need for ancillary qubits. In analyzing its performance, we develop a framework to capture the quasi-locality of the Magnus operators, leading to a tightened bound for the error of the Magnus truncation. The Lieb-Robinson bound also guarantees the efficiency of computing the Magnus operators and of their subsequent decomposition into elementary quantum gates. These features make our algorithm appealing for near-term and early-fault-tolerant simulations.","sentences":["We propose an algorithm for simulating the dynamics of a geometrically local Hamiltonian $A$ under a small geometrically local perturbation $\\alpha B$. In certain regimes, the algorithm achieves the optimal scaling and outperforms the state-of-the-art algorithms.","By moving into the interaction frame of $A$ and classically computing the Magnus expansion of the interaction-picture Hamiltonian, our algorithm bypasses the need for ancillary qubits.","In analyzing its performance, we develop a framework to capture the quasi-locality of the Magnus operators, leading to a tightened bound for the error of the Magnus truncation.","The Lieb-Robinson bound also guarantees the efficiency of computing the Magnus operators and of their subsequent decomposition into elementary quantum gates.","These features make our algorithm appealing for near-term and early-fault-tolerant simulations."],"url":"http://arxiv.org/abs/2404.02966v1","category":"quant-ph"}
{"created":"2024-04-03 18:00:02","title":"A first determination of the strong coupling $\u03b1_S$ at approximate N$^{3}$LO order in a global PDF fit","abstract":"We present the first determination of the value of the strong coupling via a simultaneous global fit of the proton parton distribution functions (PDFs) at approximate N$^{3}$LO (aN$^{3}$LO) order in QCD. This makes use of the MSHT global PDF fitting framework, and in particular the recent theoretical advances that allow a PDF fit to now be performed at this order. The value of the strong coupling is found to be $\\alpha_S(M_Z^2)$(aN$^{3}$LO)$ = 0.1170 \\pm 0.0016$. This is in excellent agreement with the NNLO value of $\\alpha_S(M_Z^2)$(NNLO) $= 0.1171 \\pm 0.0014$, indicating that good perturbative convergence has been found. The resulting uncertainties, calculated using the MSHT dynamic tolerance procedure, are somewhat larger, but more accurate, at aN$^{3}$LO, due to the missing higher order theoretical uncertainties that are included at this order, but not at NNLO. We in addition present a detailed breakdown of the individual dataset sensitivity to the value of the strong coupling, with special focus on the impact of fitting dijet rather than inclusive jet data. This choice is found to have a non-negligible impact, but with overall good consistency found, especially at aN$^{3}$LO.","sentences":["We present the first determination of the value of the strong coupling via a simultaneous global fit of the proton parton distribution functions (PDFs) at approximate N$^{3}$LO (aN$^{3}$LO) order in QCD.","This makes use of the MSHT global PDF fitting framework, and in particular the recent theoretical advances that allow a PDF fit to now be performed at this order.","The value of the strong coupling is found to be $\\alpha_S(M_Z^2)$(aN$^{3}$LO)$ = 0.1170 \\pm 0.0016$.","This is in excellent agreement with the NNLO value of $\\alpha_S(M_Z^2)$(NNLO) $= 0.1171 \\pm 0.0014$, indicating that good perturbative convergence has been found.","The resulting uncertainties, calculated using the MSHT dynamic tolerance procedure, are somewhat larger, but more accurate, at aN$^{3}$LO, due to the missing higher order theoretical uncertainties that are included at this order, but not at NNLO.","We in addition present a detailed breakdown of the individual dataset sensitivity to the value of the strong coupling, with special focus on the impact of fitting dijet rather than inclusive jet data.","This choice is found to have a non-negligible impact, but with overall good consistency found, especially at aN$^{3}$LO."],"url":"http://arxiv.org/abs/2404.02964v1","category":"hep-ph"}
{"created":"2024-04-03 18:00:01","title":"Probing the eccentricity in protostellar discs -- Modeling kinematics and morphologies","abstract":"Protostellar discs are mostly modelled as circular structures of gas and dust orbiting a protostar. However, a number of physical mechanisms, e.g. the presence of a (sub)stellar companion or initial axial asymmetry, can cause the gas and dust orbital motion to become eccentric. Theoretical studies have revealed that, when present, disc eccentricity is expected to occur with predictable profiles that can be long-lasting and potentially observable in protostellar systems. We construct an analytical model predicting the typical features of the kinematics and morphology of eccentric protostellar discs, with the final goal of characterising the observational appearance of eccentricity in discs. We validate the model using a numerical simulation of a circumbinary disc (where the binary makes the disc eccentric). We finally post-process the simulation with Monte Carlo Radiative Transfer to study how eccentric features would appear through the \"eyes\" of ALMA. Besides the motion of the material on eccentric Keplerian orbits in the disc orbital plane, the most characteristic eccentric feature emerging from the analytical model is strong vertical motion with a typical anti-symmetric pattern (with respect to the disc line of pericentres). A circumbinary disc with a $\\approx 40$ au eccentric cavity ($e_{\\rm cav}=0.2$), carved by an $a_{\\rm bin}=15$ au binary, placed at a distance $d=130$ pc, is expected to host in its upper emission surface vertical oscillations up to $v_{z}\\sim 400\\, {\\rm ms}^{-1}$ close to the cavity edge, i.e. well within ALMA spectral and spatial resolution capabilities. A residual spiral pattern in the vertical velocity $\\Delta v_{z}\\sim 150\\, {\\rm ms}^{-1}$ of the simulation cannot be captured by the theoretical model, we speculate it to be possibly linked to the presence of a companion in the system.","sentences":["Protostellar discs are mostly modelled as circular structures of gas and dust orbiting a protostar.","However, a number of physical mechanisms, e.g. the presence of a (sub)stellar companion or initial axial asymmetry, can cause the gas and dust orbital motion to become eccentric.","Theoretical studies have revealed that, when present, disc eccentricity is expected to occur with predictable profiles that can be long-lasting and potentially observable in protostellar systems.","We construct an analytical model predicting the typical features of the kinematics and morphology of eccentric protostellar discs, with the final goal of characterising the observational appearance of eccentricity in discs.","We validate the model using a numerical simulation of a circumbinary disc (where the binary makes the disc eccentric).","We finally post-process the simulation with Monte Carlo Radiative Transfer to study how eccentric features would appear through the \"eyes\" of ALMA.","Besides the motion of the material on eccentric Keplerian orbits in the disc orbital plane, the most characteristic eccentric feature emerging from the analytical model is strong vertical motion with a typical anti-symmetric pattern (with respect to the disc line of pericentres).","A circumbinary disc with a $\\approx 40$ au eccentric cavity ($e_{\\rm cav}=0.2$), carved by an $a_{\\rm bin}=15$ au binary, placed at a distance $d=130$ pc, is expected to host in its upper emission surface vertical oscillations up to $v_{z}\\sim 400\\, {\\rm ms}^{-1}$ close to the cavity edge, i.e. well within ALMA spectral and spatial resolution capabilities.","A residual spiral pattern in the vertical velocity $\\Delta v_{z}\\sim 150\\, {\\rm ms}^{-1}$ of the simulation cannot be captured by the theoretical model, we speculate it to be possibly linked to the presence of a companion in the system."],"url":"http://arxiv.org/abs/2404.02958v1","category":"astro-ph.EP"}
{"created":"2024-04-03 18:00:00","title":"Sledgehamr: Simulating Scalar Fields with Adaptive Mesh Refinement","abstract":"Understanding the non-linear dynamics of coupled scalar fields often necessitates simulations on a three-dimensional mesh. These simulations can be computationally expensive if a large scale separation is involved. A common solution is adaptive mesh refinement which, however, greatly increases a simulation's complexity. In this work, we present sledgehamr, an AMReX-based code package to make the simulation of coupled scalar fields on an adaptive mesh more accessible. Compatible with both GPU and CPU clusters, sledgehamr offers a flexible and customizable framework. While the code had been primarily developed to evolve axion string networks, this framework enables various other applications, such as the study of gravitational waves sourced by the dynamics of scalar fields.","sentences":["Understanding the non-linear dynamics of coupled scalar fields often necessitates simulations on a three-dimensional mesh.","These simulations can be computationally expensive if a large scale separation is involved.","A common solution is adaptive mesh refinement which, however, greatly increases a simulation's complexity.","In this work, we present sledgehamr, an AMReX-based code package to make the simulation of coupled scalar fields on an adaptive mesh more accessible.","Compatible with both GPU and CPU clusters, sledgehamr offers a flexible and customizable framework.","While the code had been primarily developed to evolve axion string networks, this framework enables various other applications, such as the study of gravitational waves sourced by the dynamics of scalar fields."],"url":"http://arxiv.org/abs/2404.02950v1","category":"hep-ph"}
{"created":"2024-04-03 18:00:00","title":"Inferring dark matter subhalo properties from simulated subhalo-stream encounters","abstract":"In the cold dark matter paradigm, our Galaxy is predicted to contain >10000 dark matter subhaloes in the $10^5-10^8M_\\odot$ range which should be completely devoid of stars. Stellar streams are sensitive to the presence of these subhaloes, which can create small-scale features in streams if they pass closely enough. Modelling these encounters can therefore, potentially recover the subhalo's properties. In this work, we demonstrate this for streams generated in numerical simulations, modelled on eccentric orbits in a realistic Milky Way potential, which includes the Large Magellanic Cloud and the subhalo itself. We focus on a mock model of the ATLAS-Aliqa Uma stream and inject a $10^7 M_\\odot$ subhalo, creating a similar discontinuous morphology to current observations. We then explore how well subhalo properties are recovered using mock stream observations, consisting of no observational errors, as well as assuming realistic observational setups. These setups include present day style observations, and what will be possible with 4MOST and Gaia DR5 in the future. We show that we can recover all parameters describing the impact even with uncertainties matching existing data, including subhalo positions, velocities, mass and scale radius. Modelling the subhalo on an orbit instead of assuming an impulse approximation, we greatly reduce the degeneracy between subhalo mass and velocity seen in previous works. However, we find a slight bias in the subhalo mass (~0.1 dex). This demonstrates that we should be able to reliably extract the properties of subhaloes with stellar streams in the near future.","sentences":["In the cold dark matter paradigm, our Galaxy is predicted to contain >10000 dark matter subhaloes in the $10^5-10^8M_\\odot$ range which should be completely devoid of stars.","Stellar streams are sensitive to the presence of these subhaloes, which can create small-scale features in streams if they pass closely enough.","Modelling these encounters can therefore, potentially recover the subhalo's properties.","In this work, we demonstrate this for streams generated in numerical simulations, modelled on eccentric orbits in a realistic Milky Way potential, which includes the Large Magellanic Cloud and the subhalo itself.","We focus on a mock model of the ATLAS-Aliqa Uma stream and inject a $10^7 M_\\odot$ subhalo, creating a similar discontinuous morphology to current observations.","We then explore how well subhalo properties are recovered using mock stream observations, consisting of no observational errors, as well as assuming realistic observational setups.","These setups include present day style observations, and what will be possible with 4MOST and Gaia DR5 in the future.","We show that we can recover all parameters describing the impact even with uncertainties matching existing data, including subhalo positions, velocities, mass and scale radius.","Modelling the subhalo on an orbit instead of assuming an impulse approximation, we greatly reduce the degeneracy between subhalo mass and velocity seen in previous works.","However, we find a slight bias in the subhalo mass (~0.1 dex).","This demonstrates that we should be able to reliably extract the properties of subhaloes with stellar streams in the near future."],"url":"http://arxiv.org/abs/2404.02953v1","category":"astro-ph.GA"}
{"created":"2024-04-03 17:58:29","title":"The Lavrentiev phenomenon","abstract":"The basic problem of the calculus of variations consists of finding a function that minimizes an energy, like finding the fastest trajectory between two points for a point mass in a gravity field moving without friction under the influence of gravity or finding the best shape of a wing. The existence of a solution may be established in quite abstract spaces, much larger than the space of smooth functions. An important practical problem is that of being able to approach the value of the infimum of the energy. However, numerical methods work with very concrete functions and sometimes they are unable to approximate the infimum: this is the surprising Lavrentiev phenomenon. The papers that ensure the non-occurrence of the phenomenon form a recent saga, and the most general result formulated in the early '90s was actually fully proved just recently, more than 30 years later. Our aim here is to introduce the reader to the calculus of variations, to illustrate the Lavrentiev phenomenon with the simplest known example, and to give an elementary proof of the non-occurrence of the phenomenon.","sentences":["The basic problem of the calculus of variations consists of finding a function that minimizes an energy, like finding the fastest trajectory between two points for a point mass in a gravity field moving without friction under the influence of gravity or finding the best shape of a wing.","The existence of a solution may be established in quite abstract spaces, much larger than the space of smooth functions.","An important practical problem is that of being able to approach the value of the infimum of the energy.","However, numerical methods work with very concrete functions and sometimes they are unable to approximate the infimum: this is the surprising Lavrentiev phenomenon.","The papers that ensure the non-occurrence of the phenomenon form a recent saga, and the most general result formulated in the early '90s was actually fully proved just recently, more than 30 years later.","Our aim here is to introduce the reader to the calculus of variations, to illustrate the Lavrentiev phenomenon with the simplest known example, and to give an elementary proof of the non-occurrence of the phenomenon."],"url":"http://arxiv.org/abs/2404.02901v1","category":"math.OC"}
{"created":"2024-04-03 17:51:20","title":"Automated Transparency: A Legal and Empirical Analysis of the Digital Services Act Transparency Database","abstract":"The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.","sentences":["The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency.","Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study.","SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023.","The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance.","This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises.","We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices.","In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices."],"url":"http://arxiv.org/abs/2404.02894v1","category":"cs.CY"}
{"created":"2024-04-03 17:33:21","title":"Linear Attention Sequence Parallelism","abstract":"Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.","sentences":["Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU.","However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models.","In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models.","Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP.","We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters.","Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches.","We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes.","LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster.","The code is available at https://github.com/OpenNLPLab/LASP."],"url":"http://arxiv.org/abs/2404.02882v1","category":"cs.LG"}
{"created":"2024-04-03 17:32:52","title":"On computing approximate Lewis weights","abstract":"In this note we provide and analyze a simple method that given an $n \\times d$ matrix, outputs approximate $\\ell_p$-Lewis weights, a natural measure of the importance of the rows with respect to the $\\ell_p$ norm, for $p \\geq 2$. More precisely, we provide a simple post-processing procedure that turns natural one-sided approximate $\\ell_p$-Lewis weights into two-sided approximations. When combined with a simple one-sided approximation algorithm presented by Lee (PhD thesis, `16) this yields an algorithm for computing two-sided approximations of the $\\ell_p$-Lewis weights of an $n \\times d$-matrix using $\\mathrm{poly}(d,p)$ approximate leverage score computations. While efficient high-accuracy algorithms for approximating $\\ell_p$-Lewis had been established previously by Fazel, Lee, Padmanabhan and Sidford (SODA `22), the simple structure and approximation tolerance of our algorithm may make it of use for different applications.","sentences":["In this note we provide and analyze a simple method that given an $n \\times d$ matrix, outputs approximate $\\ell_p$-Lewis weights, a natural measure of the importance of the rows with respect to the $\\ell_p$ norm, for $p \\geq 2$.","More precisely, we provide a simple post-processing procedure that turns natural one-sided approximate $\\ell_p$-Lewis weights into two-sided approximations.","When combined with a simple one-sided approximation algorithm presented by Lee (PhD thesis, `16) this yields an algorithm for computing two-sided approximations of the $\\ell_p$-Lewis weights of an $n \\times d$-matrix using $\\mathrm{poly}(d,p)$ approximate leverage score computations.","While efficient high-accuracy algorithms for approximating $\\ell_p$-Lewis had been established previously by Fazel, Lee, Padmanabhan and Sidford (SODA `22), the simple structure and approximation tolerance of our algorithm may make it of use for different applications."],"url":"http://arxiv.org/abs/2404.02881v1","category":"cs.DS"}
{"created":"2024-04-03 17:32:50","title":"Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?","abstract":"Everyone spends some time waiting every day. HCI research has developed tools for boosting productivity while waiting. However, little is known about how people naturally spend their waiting time. We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks. The aim of this study is to understand the activities people do while waiting and the effect of situational factors. We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities. These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day. Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance. Our findings shed light on future empirical research and system design for time management.","sentences":["Everyone spends some time waiting every day.","HCI research has developed tools for boosting productivity while waiting.","However, little is known about how people naturally spend their waiting time.","We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks.","The aim of this study is to understand the activities people do while waiting and the effect of situational factors.","We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities.","These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day.","Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance.","Our findings shed light on future empirical research and system design for time management."],"url":"http://arxiv.org/abs/2404.02880v1","category":"cs.HC"}
{"created":"2024-04-03 17:21:59","title":"Sensing Resource Allocation Against Data-Poisoning Attacks in Traffic Routing","abstract":"Data-poisoning attacks can disrupt the efficient operations of transportation systems by misdirecting traffic flows via falsified data. One challenge in countering these attacks is to reduce the uncertainties on the types of attacks, such as the distribution of their targets and intensities. We introduce a resource allocation method in transportation networks to detect and distinguish different types of attacks and facilitate efficient traffic routing. The idea is to first cluster different types of attacks based on the corresponding optimal routing strategies, then allocate sensing resources to a subset of network links to distinguish attacks from different clusters via lexicographical mixed-integer programming. We illustrate the application of the proposed method using the Anaheim network, a benchmark model in traffic routing that contains more than 400 nodes and 900 links.","sentences":["Data-poisoning attacks can disrupt the efficient operations of transportation systems by misdirecting traffic flows via falsified data.","One challenge in countering these attacks is to reduce the uncertainties on the types of attacks, such as the distribution of their targets and intensities.","We introduce a resource allocation method in transportation networks to detect and distinguish different types of attacks and facilitate efficient traffic routing.","The idea is to first cluster different types of attacks based on the corresponding optimal routing strategies, then allocate sensing resources to a subset of network links to distinguish attacks from different clusters via lexicographical mixed-integer programming.","We illustrate the application of the proposed method using the Anaheim network, a benchmark model in traffic routing that contains more than 400 nodes and 900 links."],"url":"http://arxiv.org/abs/2404.02876v1","category":"math.OC"}
{"created":"2024-04-03 17:08:23","title":"A mean-field model of optimal investment","abstract":"We establish the existence and uniqueness of the equilibrium for a stochastic mean-field game of optimal investment. The analysis covers both finite and infinite time horizons, and the mean-field interaction of the representative company with a mass of identical and indistinguishable firms is modeled through the time-dependent price at which the produced good is sold. At equilibrium, this price is given in terms of a nonlinear function of the expected (optimally controlled) production capacity of the representative company at each time. The proof of the existence and uniqueness of the mean-field equilibrium relies on a priori estimates and the study of nonlinear integral equations, but employs different techniques for the finite and infinite horizon cases. Additionally, we investigate the deterministic counterpart of the mean-field game under study.","sentences":["We establish the existence and uniqueness of the equilibrium for a stochastic mean-field game of optimal investment.","The analysis covers both finite and infinite time horizons, and the mean-field interaction of the representative company with a mass of identical and indistinguishable firms is modeled through the time-dependent price at which the produced good is sold.","At equilibrium, this price is given in terms of a nonlinear function of the expected (optimally controlled) production capacity of the representative company at each time.","The proof of the existence and uniqueness of the mean-field equilibrium relies on a priori estimates and the study of nonlinear integral equations, but employs different techniques for the finite and infinite horizon cases.","Additionally, we investigate the deterministic counterpart of the mean-field game under study."],"url":"http://arxiv.org/abs/2404.02871v1","category":"math.OC"}
{"created":"2024-04-03 17:03:18","title":"UDON: A case for offloading to general purpose compute on CXL memory","abstract":"Upcoming CXL-based disaggregated memory devices feature special purpose units to offload compute to near-memory. In this paper, we explore opportunities for offloading compute to general purpose cores on CXL memory devices, thereby enabling a greater utility and diversity of offload.   We study two classes of popular memory intensive applications: ML inference and vector database as candidates for computational offload. The study uses Arm AArch64-based dual-socket NUMA systems to emulate CXL type-2 devices.   Our study shows promising results. With our ML inference model partitioning strategy for compute offload, we can place up to 90% data in remote memory with just 20% performance trade-off. Offloading Hierarchical Navigable Small World (HNSW) kernels in vector databases can provide upto 6.87$\\times$ performance improvement with under 10% offload overhead.","sentences":["Upcoming CXL-based disaggregated memory devices feature special purpose units to offload compute to near-memory.","In this paper, we explore opportunities for offloading compute to general purpose cores on CXL memory devices, thereby enabling a greater utility and diversity of offload.   ","We study two classes of popular memory intensive applications: ML inference and vector database as candidates for computational offload.","The study uses Arm AArch64-based dual-socket NUMA systems to emulate CXL type-2 devices.   ","Our study shows promising results.","With our ML inference model partitioning strategy for compute offload, we can place up to 90% data in remote memory with just 20% performance trade-off.","Offloading Hierarchical Navigable Small World (HNSW) kernels in vector databases can provide upto 6.87$\\times$ performance improvement with under 10% offload overhead."],"url":"http://arxiv.org/abs/2404.02868v1","category":"cs.ET"}
{"created":"2024-04-03 17:01:21","title":"Dark energy as a geometrical effect in geodetic brane gravity","abstract":"Within the framework of the modified geodetic brane gravity, conformed by the Regge-Teitelboim model and enhanced with a linear term in the extrinsic curvature of the brane, the possibility that under an FRW geometry this theory emulates the so-called dark energy is discussed. The cosmological behavior of this model displays a self-(non-self)-accelerated expansion of this universe which is caused by a combination of usual matter and gravitational geometric effects controlled by a $\\beta$ parameter that accompanies the correction $K$ term. Indeed, the self-accelerated branch, provided by the trace $K$ model raises the question of whether the extrinsic curvature correction terms might be suitable for dark energy candidates. We discuss the analytical expression obtained for $\\rd$ in addition to the main cosmological parameters such as the state parameter $\\omega_{\\text{\\tiny eff}}$ and the deceleration parameter $q$. Moreover, when we call for the contribution of dark radiation-like energy to be switched off, $\\Odr \\to 0$, we find the same acceleration behavior, as well as the same dark energy content provided by the DGP theory.","sentences":["Within the framework of the modified geodetic brane gravity, conformed by the Regge-Teitelboim model and enhanced with a linear term in the extrinsic curvature of the brane, the possibility that under an FRW geometry this theory emulates the so-called dark energy is discussed.","The cosmological behavior of this model displays a self-(non-self)-accelerated expansion of this universe which is caused by a combination of usual matter and gravitational geometric effects controlled by a $\\beta$ parameter that accompanies the correction $K$ term.","Indeed, the self-accelerated branch, provided by the trace $K$ model raises the question of whether the extrinsic curvature correction terms might be suitable for dark energy candidates.","We discuss the analytical expression obtained for $\\rd$ in addition to the main cosmological parameters such as the state parameter $\\omega_{\\text{\\tiny eff}}$ and the deceleration parameter $q$. Moreover, when we call for the contribution of dark radiation-like energy to be switched off, $\\Odr \\to 0$, we find the same acceleration behavior, as well as the same dark energy content provided by the DGP theory."],"url":"http://arxiv.org/abs/2404.02867v1","category":"gr-qc"}
{"created":"2024-04-03 16:57:16","title":"Discovery of universal phonon thermal Hall effect in crystals","abstract":"Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field. While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggest that phonons play a significant role. However, the mechanism behind phonon THE is still unknown. Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductor: SrTiO$_3$, SiO$_2$ (quartz), MgO and Si. While the presence of antiferrodistortive domains in SrTiO$_3$ and chiral phonons in SiO$_2$ may complicate the interpretation of THE, the striking observations of THE in the trivial insulator MgO and high-purity intrinsic semiconductor Si demonstrate that phonon THE is a universal property of crystals. Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |$\\kappa_{xy}$| $\\sim$ $\\kappa_{xx}^2$. Our results experimentally discover a fundamental physics of phonons in magnetic field, which must come from the direct coupling between atom vibrations and the field. Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered.","sentences":["Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field.","While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggest that phonons play a significant role.","However, the mechanism behind phonon THE is still unknown.","Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductor:","SrTiO$_3$, SiO$_2$ (quartz), MgO and Si.","While the presence of antiferrodistortive domains in SrTiO$_3$ and chiral phonons in SiO$_2$ may complicate the interpretation of THE, the striking observations of THE in the trivial insulator MgO and high-purity intrinsic semiconductor Si demonstrate that phonon THE is a universal property of crystals.","Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |$\\kappa_{xy}$| $\\sim$ $\\kappa_{xx}^2$. Our results experimentally discover a fundamental physics of phonons in magnetic field, which must come from the direct coupling between atom vibrations and the field.","Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered."],"url":"http://arxiv.org/abs/2404.02863v1","category":"cond-mat.str-el"}
{"created":"2024-04-03 16:53:32","title":"Pre-equilibrium Photon and Dilepton Production","abstract":"We use QCD kinetic theory to compute photon and dilepton production in the chemically equilibrating out-of-equilibrium quark-gluon plasma created in the early stages of high-energy heavy-ion collisions. We derive universal scaling functions for the pre-equilibrium spectra of photons and dileptons. These scaling functions can be used to make realistic predictions for the pre-equilibrium emission and consequently establish the significance of the pre-equilibrium phase for the production of electromagnetic probes in heavy-ion collisions.","sentences":["We use QCD kinetic theory to compute photon and dilepton production in the chemically equilibrating out-of-equilibrium quark-gluon plasma created in the early stages of high-energy heavy-ion collisions.","We derive universal scaling functions for the pre-equilibrium spectra of photons and dileptons.","These scaling functions can be used to make realistic predictions for the pre-equilibrium emission and consequently establish the significance of the pre-equilibrium phase for the production of electromagnetic probes in heavy-ion collisions."],"url":"http://arxiv.org/abs/2404.02861v1","category":"hep-ph"}
{"created":"2024-04-03 16:53:24","title":"Spin alignment of $K^\\ast$ induced by strange-baryon density inhomogeneity","abstract":"The difference between the spin alignments of $K^\\ast$ and those of $\\phi$ at the low collision energies is a puzzle raised by the recent experiments. Unlike $\\phi$ meson, $K^\\ast$, carrying a unit strange charge, should react to strange chemical potential $\\mu_S$. In this paper, we shall first convince you that $\\mu_S$ is not small in a brayon-rich medium for keeping strange neutrality, and then derive the spin alignment induced by the gradient of $\\mu_S$, and hence of baryon chemical potential $\\mu_B$, using linear response theory, with the transport coefficients expressed, without any approximation, in terms of the $K^\\ast$'s in-medium spectral properties by employing Ward-Takahashi identity. It turns out that such an effect applies mainly to the particles whose longitudinal and transverse modes diverge, and induces only the local spin alignment in a static medium. The magnitudes of these coefficients will be further estimated under the quasi-particle approximation.","sentences":["The difference between the spin alignments of $K^\\ast$ and those of $\\phi$ at the low collision energies is a puzzle raised by the recent experiments.","Unlike $\\phi$ meson, $K^\\ast$, carrying a unit strange charge, should react to strange chemical potential $\\mu_S$. In this paper, we shall first convince you that $\\mu_S$ is not small in a brayon-rich medium for keeping strange neutrality, and then derive the spin alignment induced by the gradient of $\\mu_S$, and hence of baryon chemical potential $\\mu_B$, using linear response theory, with the transport coefficients expressed, without any approximation, in terms of the $K^\\ast$'s in-medium spectral properties by employing Ward-Takahashi identity.","It turns out that such an effect applies mainly to the particles whose longitudinal and transverse modes diverge, and induces only the local spin alignment in a static medium.","The magnitudes of these coefficients will be further estimated under the quasi-particle approximation."],"url":"http://arxiv.org/abs/2404.02860v1","category":"nucl-th"}
{"created":"2024-04-03 16:37:16","title":"Tight stability bounds for entropic Brenier maps","abstract":"Entropic Brenier maps are regularized analogues of Brenier maps (optimal transport maps) which converge to Brenier maps as the regularization parameter shrinks. In this work, we prove quantitative stability bounds between entropic Brenier maps under variations of the target measure. In particular, when all measures have bounded support, we establish the optimal Lipschitz constant for the mapping from probability measures to entropic Brenier maps. This provides an exponential improvement to a result of Carlier, Chizat, and Laborde (2024). As an application, we prove near-optimal bounds for the stability of semi-discrete \\emph{unregularized} Brenier maps for a family of discrete target measures.","sentences":["Entropic Brenier maps are regularized analogues of Brenier maps (optimal transport maps) which converge to Brenier maps as the regularization parameter shrinks.","In this work, we prove quantitative stability bounds between entropic Brenier maps under variations of the target measure.","In particular, when all measures have bounded support, we establish the optimal Lipschitz constant for the mapping from probability measures to entropic Brenier maps.","This provides an exponential improvement to a result of Carlier, Chizat, and Laborde (2024).","As an application, we prove near-optimal bounds for the stability of semi-discrete \\emph{unregularized} Brenier maps for a family of discrete target measures."],"url":"http://arxiv.org/abs/2404.02855v1","category":"math.PR"}
{"created":"2024-04-03 16:36:21","title":"Axisymmetric steady Navier-Stokes flows under suction","abstract":"We prove the existence of solutions for the axisymmetric steady Navier-Stokes system around an infinite cylinder under external forces. The solutions are constructed to be decaying at the horizontal infinity, despite an analogue of the Stokes paradox for the linearized system, and having neither periodicity nor decay in the vertical direction. The proof is based on perturbation of the nonlinear system around a suction flow. The class of functions in this paper, which is a subspace of the space of Fourier transformed vector finite Radon measures, is inspired by Giga-Saal (2013) treating rotating boundary layers.","sentences":["We prove the existence of solutions for the axisymmetric steady Navier-Stokes system around an infinite cylinder under external forces.","The solutions are constructed to be decaying at the horizontal infinity, despite an analogue of the Stokes paradox for the linearized system, and having neither periodicity nor decay in the vertical direction.","The proof is based on perturbation of the nonlinear system around a suction flow.","The class of functions in this paper, which is a subspace of the space of Fourier transformed vector finite Radon measures, is inspired by Giga-Saal (2013) treating rotating boundary layers."],"url":"http://arxiv.org/abs/2404.02854v1","category":"math.AP"}
{"created":"2024-04-03 16:33:48","title":"Domination number of modular product graphs","abstract":"The modular product $G\\diamond H$ of graphs $G$ and $H$ is a graph on vertex set $V(G)\\times V(H)$. Two vertices $(g,h)$ and $(g^{\\prime},h^{\\prime})$ of $G\\diamond H$ are adjacent if $g=g^{\\prime}$ and $hh^{\\prime}\\in E(H)$, or $gg^{\\prime}\\in E(G)$ and $h=h^{\\prime}$, or $gg^{\\prime}\\in E(G)$ and $hh^{\\prime}\\in E(H)$, or (for $g\\neq g^{\\prime}$ and $h\\neq h^{\\prime}$) $gg^{\\prime}\\notin E(G)$ and $hh^{\\prime}\\notin E(H)$. A set $D\\subseteq V(G)$ is a dominating set of $G$ if every vertex outside of $D$ contains a neighbor in $D$. A set $D\\subseteq V(G)$ is a total dominating set of $G$ if every vertex of $G$ contains a neighbor in $D$. The domination number $\\gamma(G)$ (resp. total domination number $\\gamma_{t}(G)$) of $G$ is the minimum cardinality of a dominating set (resp. total dominating set) of $G$. In this work we give several upper and lower bounds for $\\gamma(G\\diamond H)$ in terms of $\\gamma(G),$ $\\gamma(H)$, $\\gamma_{t}(\\overline{G})$ and $\\gamma _{t}(\\overline{H})$, where $\\overline{G}$ is the complement graph of $G$. Further, we fully describe graphs where $\\gamma(G\\diamond H)=k$ for $k\\in\\{1,2,3\\}$. Several conditions on $G$ and $H$ under which $\\gamma (G\\diamond H)$ is at most $4$ and $5$ are also given. A new type of simultaneous domination $\\bar{\\gamma}(G)$, defined as the smallest number of vertices that dominates $G$ and totally dominates the complement of $G,$ emerged as useful and we believe it could be of independent interest. We conclude the paper by proposing few directions for possible further research.","sentences":["The modular product $G\\diamond H$ of graphs $G$ and $H$ is a graph on vertex set $V(G)\\times V(H)$. Two vertices $(g,h)$ and $(g^{\\prime},h^{\\prime})$ of $G\\diamond H$ are adjacent if $g=g^{\\prime}$ and $hh^{\\prime}\\in E(H)$, or $gg^{\\prime}\\in E(G)$ and $h=h^{\\prime}$, or $gg^{\\prime}\\in E(G)$ and $hh^{\\prime}\\in E(H)$, or (for $g\\neq g^{\\prime}$ and $h\\neq h^{\\prime}$) $gg^{\\prime}\\notin E(G)$ and $hh^{\\prime}\\notin E(H)$. A set $D\\subseteq V(G)$ is a dominating set of $G$ if every vertex outside of $D$ contains a neighbor in $D$. A set $D\\subseteq V(G)$ is a total dominating set of $G$ if every vertex of $G$ contains a neighbor in $D$. The domination number $\\gamma(G)$ (resp.","total domination number $\\gamma_{t}(G)$) of $G$ is the minimum cardinality of a dominating set (resp.","total dominating set) of $G$.","In this work we give several upper and lower bounds for $\\gamma(G\\diamond H)$ in terms of $\\gamma(G),$ $\\gamma(H)$, $\\gamma_{t}(\\overline{G})$ and $\\gamma _{t}(\\overline{H})$, where $\\overline{G}$ is the complement graph of $G$. Further, we fully describe graphs where $\\gamma(G\\diamond H)=k$ for $k\\in\\{1,2,3\\}$. Several conditions on $G$ and $H$ under which $\\gamma (G\\diamond H)$ is at most $4$ and $5$ are also given.","A new type of simultaneous domination $\\bar{\\gamma}(G)$, defined as the smallest number of vertices that dominates $G$ and totally dominates the complement of $G,$ emerged as useful and we believe it could be of independent interest.","We conclude the paper by proposing few directions for possible further research."],"url":"http://arxiv.org/abs/2404.02853v1","category":"math.CO"}
{"created":"2024-04-03 16:33:42","title":"Toward Inference-optimal Mixture-of-Expert Large Language Models","abstract":"Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.","sentences":["Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers.","Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens?","We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree.","Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time.","We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss.","We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training.","On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget."],"url":"http://arxiv.org/abs/2404.02852v1","category":"cs.LG"}
{"created":"2024-04-03 16:30:48","title":"Efficient Structure-Informed Featurization and Property Prediction of Ordered, Dilute, and Random Atomic Structures","abstract":"Structure-informed materials informatics is a rapidly evolving discipline of materials science relying on the featurization of atomic structures or configurations to construct vector, voxel, graph, graphlet, and other representations useful for machine learning prediction of properties, fingerprinting, and generative design. This work discusses how current featurizers typically perform redundant calculations and how their efficiency could be improved by considering (1) fundamentals of crystallographic (orbits) equivalency to optimize ordered cases and (2) representation-dependent equivalency to optimize cases of dilute, doped, and defect structures with broken symmetry. It also discusses and contrasts ways of (3) approximating random solid solutions occupying arbitrary lattices under such representations.   Efficiency improvements discussed in this work were implemented within pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks developed by authors since 2019 and shown to increase performance from 2 to 10 times for typical inputs. Throughout this work, the authors explicitly discuss how these advances can be applied to different kinds of similar tools in the community.","sentences":["Structure-informed materials informatics is a rapidly evolving discipline of materials science relying on the featurization of atomic structures or configurations to construct vector, voxel, graph, graphlet, and other representations useful for machine learning prediction of properties, fingerprinting, and generative design.","This work discusses how current featurizers typically perform redundant calculations and how their efficiency could be improved by considering (1) fundamentals of crystallographic (orbits) equivalency to optimize ordered cases and (2) representation-dependent equivalency to optimize cases of dilute, doped, and defect structures with broken symmetry.","It also discusses and contrasts ways of (3) approximating random solid solutions occupying arbitrary lattices under such representations.   ","Efficiency improvements discussed in this work were implemented within pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks developed by authors since 2019 and shown to increase performance from 2 to 10 times for typical inputs.","Throughout this work, the authors explicitly discuss how these advances can be applied to different kinds of similar tools in the community."],"url":"http://arxiv.org/abs/2404.02849v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 16:21:01","title":"Mixed volumes of zonoids and the absolute value of the Grassmannian (Extended Abstract)","abstract":"Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments. We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map. We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3). Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve.","sentences":["Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments.","We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map.","We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3).","Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve."],"url":"http://arxiv.org/abs/2404.02842v1","category":"math.CO"}
{"created":"2024-04-03 16:03:59","title":"Legendre Transformation under Micro Canonical Ensemble","abstract":"The Legendre transformation is a crucial tool in theoretical physics, known for its symmetry, especially when applied to multivariate functions. In statistical mechanics, ensembles represent the central focus. Leveraging the dimensionless aspect of Legendre transformation, this paper explores the transformation process from the entropy characteristic function of microcanonical ensembles to the analogous definition of partition function transformation. Additionally, it derives characteristic functions, partition functions, and establishes their interrelations, along with deriving corresponding thermodynamic formulas for various ensembles. This streamlined approach sheds light on the fundamental principles of statistical mechanics and underscores the symmetry inherent in Legendre transformation.","sentences":["The Legendre transformation is a crucial tool in theoretical physics, known for its symmetry, especially when applied to multivariate functions.","In statistical mechanics, ensembles represent the central focus.","Leveraging the dimensionless aspect of Legendre transformation, this paper explores the transformation process from the entropy characteristic function of microcanonical ensembles to the analogous definition of partition function transformation.","Additionally, it derives characteristic functions, partition functions, and establishes their interrelations, along with deriving corresponding thermodynamic formulas for various ensembles.","This streamlined approach sheds light on the fundamental principles of statistical mechanics and underscores the symmetry inherent in Legendre transformation."],"url":"http://arxiv.org/abs/2404.02829v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 15:59:31","title":"An Error-Bounded Lossy Compression Method with Bit-Adaptive Quantization for Particle Data","abstract":"This paper presents error-bounded lossy compression tailored for particle datasets from diverse scientific applications in cosmology, fluid dynamics, and fusion energy sciences. As today's high-performance computing capabilities advance, these datasets often reach trillions of points, posing significant visualization, analysis, and storage challenges. While error-bounded lossy compression makes it possible to represent floating-point values with strict pointwise accuracy guarantees, the lack of correlations in particle data's storage ordering often limits the compression ratio. Inspired by quantization-encoding schemes in SZ lossy compressors, we dynamically determine the number of bits to encode particles of the dataset to increase the compression ratio. Specifically, we utilize a k-d tree to partition particles into subregions and generate ``bit boxes'' centered at particles for each subregion to encode their positions. These bit boxes ensure error control while reducing the bit count used for compression. We comprehensively evaluate our method against state-of-the-art compressors on cosmology, fluid dynamics, and fusion plasma datasets.","sentences":["This paper presents error-bounded lossy compression tailored for particle datasets from diverse scientific applications in cosmology, fluid dynamics, and fusion energy sciences.","As today's high-performance computing capabilities advance, these datasets often reach trillions of points, posing significant visualization, analysis, and storage challenges.","While error-bounded lossy compression makes it possible to represent floating-point values with strict pointwise accuracy guarantees, the lack of correlations in particle data's storage ordering often limits the compression ratio.","Inspired by quantization-encoding schemes in SZ lossy compressors, we dynamically determine the number of bits to encode particles of the dataset to increase the compression ratio.","Specifically, we utilize a k-d tree to partition particles into subregions and generate ``bit boxes'' centered at particles for each subregion to encode their positions.","These bit boxes ensure error control while reducing the bit count used for compression.","We comprehensively evaluate our method against state-of-the-art compressors on cosmology, fluid dynamics, and fusion plasma datasets."],"url":"http://arxiv.org/abs/2404.02826v2","category":"cs.IT"}
{"created":"2024-04-04 17:58:31","title":"Locating and Editing Factual Associations in Mamba","abstract":"We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers. Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models. Third, we examine the linearity of Mamba's representations of factual relations. Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall. We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities.","sentences":["We investigate the mechanisms of factual recall in the Mamba state space model.","Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized.","To investigate this, we conduct four lines of experiments on Mamba.","First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers.","Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models.","Third, we examine the linearity of Mamba's representations of factual relations.","Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall.","We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities."],"url":"http://arxiv.org/abs/2404.03646v1","category":"cs.CL"}
{"created":"2024-04-04 17:54:12","title":"PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments","abstract":"Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes. Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments. This work offers a robust and adaptable solution for manipulating ungraspable objects.","sentences":["Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes.","Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen.","Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments.","This work offers a robust and adaptable solution for manipulating ungraspable objects."],"url":"http://arxiv.org/abs/2404.03634v1","category":"cs.RO"}
{"created":"2024-04-04 15:49:25","title":"Physics Informed Neural Networks for Free Shear Flows","abstract":"The transformative impact of machine learning, particularly Deep Learning (DL), on scientific and engineering domains is evident. In the context of computational fluid dynamics (CFD), Physics-Informed Neural Networks (PINNs) represent a significant innovation, enabling data-driven fluid simulations while incorporating physics-based laws described by partial differential equations (PDEs). While PINNs have demonstrated efficacy in various fluid flow scenarios, a noticeable gap exists in their application to simulate jet flows - an essential category in engineering. Jets, crucial for downburst outflow, ventilation, and heat transfer, lack comprehensive exploration through PINNs in existing literature. This study addresses this gap by focusing on the application of PINNs to simulate steady jet flows, specifically 2D planar turbulent jet flow scenarios. The novelty lies not only in adapting PINNs for simulating jet flows but also in overcoming challenges such as poor convergence during training, attributed to imbalances in loss terms. We propose a novel PINN architecture for Reynolds-Averaged Navier-Stokes (RANS) simulation of steady turbulent jet flows, without the need for turbulence models and simulation data. Additionally, an extended dynamic weighting strategy is introduced to enhance the balance of loss term contributions in the PINN, resulting in improved convergence and more accurate predictions.","sentences":["The transformative impact of machine learning, particularly Deep Learning (DL), on scientific and engineering domains is evident.","In the context of computational fluid dynamics (CFD), Physics-Informed Neural Networks (PINNs) represent a significant innovation, enabling data-driven fluid simulations while incorporating physics-based laws described by partial differential equations (PDEs).","While PINNs have demonstrated efficacy in various fluid flow scenarios, a noticeable gap exists in their application to simulate jet flows - an essential category in engineering.","Jets, crucial for downburst outflow, ventilation, and heat transfer, lack comprehensive exploration through PINNs in existing literature.","This study addresses this gap by focusing on the application of PINNs to simulate steady jet flows, specifically 2D planar turbulent jet flow scenarios.","The novelty lies not only in adapting PINNs for simulating jet flows but also in overcoming challenges such as poor convergence during training, attributed to imbalances in loss terms.","We propose a novel PINN architecture for Reynolds-Averaged Navier-Stokes (RANS) simulation of steady turbulent jet flows, without the need for turbulence models and simulation data.","Additionally, an extended dynamic weighting strategy is introduced to enhance the balance of loss term contributions in the PINN, resulting in improved convergence and more accurate predictions."],"url":"http://arxiv.org/abs/2404.03542v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 15:29:50","title":"Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data","abstract":"This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning. We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy. Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning. Our solution combines both offline data sharing and approximate gradient coding techniques. Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data.","sentences":["This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning.","We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy.","Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning.","Our solution combines both offline data sharing and approximate gradient coding techniques.","Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data."],"url":"http://arxiv.org/abs/2404.03524v1","category":"cs.LG"}
{"created":"2024-04-04 14:25:21","title":"Lower bounds for graph reconstruction with maximal independent set queries","abstract":"We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden graph. We show that randomised adaptive algorithms need at least $\\Omega(\\Delta^2 \\log(n / \\Delta) / \\log \\Delta)$ queries to reconstruct $n$-vertex graphs of maximum degree $\\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\\Omega(\\Delta^2 \\log(n / \\Delta))$ for randomised non-adaptive algorithms. We also prove that deterministic non-adaptive algorithms require at least $\\Omega(\\Delta^3 \\log n / \\log \\Delta)$ queries.   This improves bounds of Konrad, O'Sullivan, and Traistaru, and answers one of their questions. The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds.","sentences":["We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden graph.","We show that randomised adaptive algorithms need at least $\\Omega(\\Delta^2 \\log(n / \\Delta) / \\log \\Delta)$ queries to reconstruct $n$-vertex graphs of maximum degree $\\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\\Omega(\\Delta^2 \\log(n / \\Delta))$ for randomised non-adaptive algorithms.","We also prove that deterministic non-adaptive algorithms require at least $\\Omega(\\Delta^3 \\log n / \\log \\Delta)$ queries.   ","This improves bounds of Konrad, O'Sullivan, and Traistaru, and answers one of their questions.","The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds."],"url":"http://arxiv.org/abs/2404.03472v1","category":"cs.DS"}
{"created":"2024-04-04 11:13:54","title":"Weighted Energy-Dissipation approach to semilinear gradient flows with state-dependent dissipation","abstract":"We investigate the Weighted Energy-Dissipation variational approach to semilinear gradient flows with state-dependent dissipation. A family of parameter-dependent functionals defined over entire trajectories is introduced and proved to admit global minimizers. These global minimizers correspond to solutions of elliptic-in-time regularizations of the limiting causal problem. By passing to the limit in the parameter we prove that such global minimizers converge, up to subsequences, to a solution of the gradient flow.","sentences":["We investigate the Weighted Energy-Dissipation variational approach to semilinear gradient flows with state-dependent dissipation.","A family of parameter-dependent functionals defined over entire trajectories is introduced and proved to admit global minimizers.","These global minimizers correspond to solutions of elliptic-in-time regularizations of the limiting causal problem.","By passing to the limit in the parameter we prove that such global minimizers converge, up to subsequences, to a solution of the gradient flow."],"url":"http://arxiv.org/abs/2404.03370v1","category":"math.AP"}
{"created":"2024-04-04 09:53:00","title":"DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement","abstract":"Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow. In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging. Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function. In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space. We also design a variance suppression loss for regulating the additional offset term. Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed. Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods.","sentences":["Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow.","In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging.","Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function.","In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space.","We also design a variance suppression loss for regulating the additional offset term.","Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed.","Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods."],"url":"http://arxiv.org/abs/2404.03327v1","category":"cs.CV"}
{"created":"2024-04-04 09:48:14","title":"A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off","abstract":"The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy. Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\\textit{epsilon ($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field.","sentences":["The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets.","In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations.","To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy.","Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other.","In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\\textit{epsilon ($\\varepsilon$)}$ parameter, or privacy budget.","In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction.","As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field."],"url":"http://arxiv.org/abs/2404.03324v1","category":"cs.CL"}
{"created":"2024-04-04 08:37:27","title":"AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution","abstract":"Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors. These bit mappings are calibrated and fine-tuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000. Codes are available at https://github.com/Cheeun/AdaBM.","sentences":["Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs.","Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks.","Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy.","However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage.","To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds.","We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors.","These bit mappings are calibrated and fine-tuned using only a small number of calibration images.","We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000.","Codes are available at https://github.com/Cheeun/AdaBM."],"url":"http://arxiv.org/abs/2404.03296v1","category":"cs.CV"}
{"created":"2024-04-04 05:53:33","title":"An adaptive heavy ball method for ill-posed inverse problems","abstract":"In this paper we consider ill-posed inverse problems, both linear and nonlinear, by a heavy ball method in which a strongly convex regularization function is incorporated to detect the feature of the sought solution. We develop ideas on how to adaptively choose the step-sizes and the momentum coefficients to achieve acceleration over the Landweber-type method. We then analyze the method and establish its regularization property when it is terminated by the discrepancy principle. Various numerical results are reported which demonstrate the superior performance of our method over the Landweber-type method by reducing substantially the required number of iterations and the computational time.","sentences":["In this paper we consider ill-posed inverse problems, both linear and nonlinear, by a heavy ball method in which a strongly convex regularization function is incorporated to detect the feature of the sought solution.","We develop ideas on how to adaptively choose the step-sizes and the momentum coefficients to achieve acceleration over the Landweber-type method.","We then analyze the method and establish its regularization property when it is terminated by the discrepancy principle.","Various numerical results are reported which demonstrate the superior performance of our method over the Landweber-type method by reducing substantially the required number of iterations and the computational time."],"url":"http://arxiv.org/abs/2404.03218v1","category":"math.NA"}
{"created":"2024-04-04 03:29:41","title":"Goldfish: An Efficient Federated Unlearning Framework","abstract":"With recent legislation on the right to be forgotten, machine unlearning has emerged as a crucial research area. It facilitates the removal of a user's data from federated trained machine learning models without the necessity for retraining from scratch. However, current machine unlearning algorithms are confronted with challenges of efficiency and validity.To address the above issues, we propose a new framework, named Goldfish. It comprises four modules: basic model, loss function, optimization, and extension. To address the challenge of low validity in existing machine unlearning algorithms, we propose a novel loss function. It takes into account the loss arising from the discrepancy between predictions and actual labels in the remaining dataset. Simultaneously, it takes into consideration the bias of predicted results on the removed dataset. Moreover, it accounts for the confidence level of predicted results. Additionally, to enhance efficiency, we adopt knowledge distillation technique in basic model and introduce an optimization module that encompasses the early termination mechanism guided by empirical risk and the data partition mechanism. Furthermore, to bolster the robustness of the aggregated model, we propose an extension module that incorporates a mechanism using adaptive distillation temperature to address the heterogeneity of user local data and a mechanism using adaptive weight to handle the variety in the quality of uploaded models. Finally, we conduct comprehensive experiments to illustrate the effectiveness of proposed approach.","sentences":["With recent legislation on the right to be forgotten, machine unlearning has emerged as a crucial research area.","It facilitates the removal of a user's data from federated trained machine learning models without the necessity for retraining from scratch.","However, current machine unlearning algorithms are confronted with challenges of efficiency and validity.","To address the above issues, we propose a new framework, named Goldfish.","It comprises four modules: basic model, loss function, optimization, and extension.","To address the challenge of low validity in existing machine unlearning algorithms, we propose a novel loss function.","It takes into account the loss arising from the discrepancy between predictions and actual labels in the remaining dataset.","Simultaneously, it takes into consideration the bias of predicted results on the removed dataset.","Moreover, it accounts for the confidence level of predicted results.","Additionally, to enhance efficiency, we adopt knowledge distillation technique in basic model and introduce an optimization module that encompasses the early termination mechanism guided by empirical risk and the data partition mechanism.","Furthermore, to bolster the robustness of the aggregated model, we propose an extension module that incorporates a mechanism using adaptive distillation temperature to address the heterogeneity of user local data and a mechanism using adaptive weight to handle the variety in the quality of uploaded models.","Finally, we conduct comprehensive experiments to illustrate the effectiveness of proposed approach."],"url":"http://arxiv.org/abs/2404.03180v1","category":"cs.LG"}
{"created":"2024-04-04 01:08:45","title":"Spin caloric resistance of Dirac plasma in graphene Corbino device","abstract":"The thermal resistance of a spin-polarized hydrodynamic Dirac plasma in graphene is considered. A mechanism for the coupling of heat and spin flows is discussed, demonstrating that spin diffusion and spin thermocurrent modify viscous dissipation, leading to a significant enhancement of thermal resistance. Practical calculations are then presented for graphene devices in the Corbino geometry.","sentences":["The thermal resistance of a spin-polarized hydrodynamic Dirac plasma in graphene is considered.","A mechanism for the coupling of heat and spin flows is discussed, demonstrating that spin diffusion and spin thermocurrent modify viscous dissipation, leading to a significant enhancement of thermal resistance.","Practical calculations are then presented for graphene devices in the Corbino geometry."],"url":"http://arxiv.org/abs/2404.03135v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 00:24:49","title":"A primal-dual adaptive finite element method for total variation based motion estimation","abstract":"Based on previous work we extend a primal-dual semi-smooth Newton method for minimizing a general $L^1$-$L^2$-$TV$ functional over the space of functions of bounded variations by adaptivity in a finite element setting. For automatically generating an adaptive grid we introduce indicators based on a-posteriori error estimates. Further we discuss data interpolation methods on unstructured grids in the context of image processing and present a pixel-based interpolation method. For the computation of optical flow we derive an adaptive finite element coarse-to-fine scheme for resolving large displacements. The coarse-to-fine scheme speeds-up the computing time tremendously.","sentences":["Based on previous work we extend a primal-dual semi-smooth Newton method for minimizing a general $L^1$-$L^2$-$TV$ functional over the space of functions of bounded variations by adaptivity in a finite element setting.","For automatically generating an adaptive grid we introduce indicators based on a-posteriori error estimates.","Further we discuss data interpolation methods on unstructured grids in the context of image processing and present a pixel-based interpolation method.","For the computation of optical flow we derive an adaptive finite element coarse-to-fine scheme for resolving large displacements.","The coarse-to-fine scheme speeds-up the computing time tremendously."],"url":"http://arxiv.org/abs/2404.03125v1","category":"math.NA"}
{"created":"2024-04-03 23:59:20","title":"Krylov-based Adaptive-Rank Implicit Time Integrators for Stiff Problems with Application to Nonlinear Fokker-Planck Kinetic Models","abstract":"We propose a high order adaptive-rank implicit integrators for stiff time-dependent PDEs, leveraging extended Krylov subspaces to efficiently and adaptively populate low-rank solution bases. This allows for the accurate representation of solutions with significantly reduced computational costs. We further introduce an efficient mechanism for residual evaluation and an adaptive rank-seeking strategy that optimizes low-rank settings based on a comparison between the residual size and the local truncation errors of the time-stepping discretization. We demonstrate our approach with the challenging Lenard-Bernstein Fokker-Planck (LBFP) nonlinear equation, which describes collisional processes in a fully ionized plasma. The preservation of {the equilibrium state} is achieved through the Chang-Cooper discretization, and strict conservation of mass, momentum and energy via a Locally Macroscopic Conservative (LoMaC) procedure. The development of implicit adaptive-rank integrators, demonstrated here up to third-order temporal accuracy via diagonally implicit Runge-Kutta schemes, showcases superior performance in terms of accuracy, computational efficiency, equilibrium preservation, and conservation of macroscopic moments. This study offers a starting point for developing scalable, efficient, and accurate methods for high-dimensional time-dependent problems.","sentences":["We propose a high order adaptive-rank implicit integrators for stiff time-dependent PDEs, leveraging extended Krylov subspaces to efficiently and adaptively populate low-rank solution bases.","This allows for the accurate representation of solutions with significantly reduced computational costs.","We further introduce an efficient mechanism for residual evaluation and an adaptive rank-seeking strategy that optimizes low-rank settings based on a comparison between the residual size and the local truncation errors of the time-stepping discretization.","We demonstrate our approach with the challenging Lenard-Bernstein Fokker-Planck (LBFP) nonlinear equation, which describes collisional processes in a fully ionized plasma.","The preservation of {the equilibrium state} is achieved through the Chang-Cooper discretization, and strict conservation of mass, momentum and energy via a Locally Macroscopic Conservative (LoMaC) procedure.","The development of implicit adaptive-rank integrators, demonstrated here up to third-order temporal accuracy via diagonally implicit Runge-Kutta schemes, showcases superior performance in terms of accuracy, computational efficiency, equilibrium preservation, and conservation of macroscopic moments.","This study offers a starting point for developing scalable, efficient, and accurate methods for high-dimensional time-dependent problems."],"url":"http://arxiv.org/abs/2404.03119v1","category":"math.NA"}
{"created":"2024-04-03 23:20:40","title":"Many-to-many Image Generation with Auto-regressive Diffusion Models","abstract":"Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context. This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms. This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios. To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images. Utilizing Stable Diffusion with varied latent noises, our method produces a set of interconnected images from a single caption. Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a diffusion framework. Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns. Furthermore, through task-specific fine-tuning, our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation.","sentences":["Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context.","This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms.","This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios.","To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images.","Utilizing Stable Diffusion with varied latent noises, our method produces a set of interconnected images from a single caption.","Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a diffusion framework.","Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns.","Furthermore, through task-specific fine-tuning, our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation."],"url":"http://arxiv.org/abs/2404.03109v1","category":"cs.CV"}
{"created":"2024-04-03 23:10:09","title":"Ab initio leading order effective potential for elastic proton scattering based on the symmetry-adapted no-core shell model","abstract":"Based on the Watson expansion of the multiple scattering series, we employ a nonlocal translationally invariant nuclear density derived within the symmetry-adapted no-core shell model (SA-NCSM) framework from a chiral next-to-next-to-leading order (NNLO) nucleon-nucleon interaction and the very same interaction for a consistent full-folding calculation of the effective (optical) potential for nucleon-nucleus scattering for medium-heavy nuclei. The leading order effective (optical) folding potential is computed by integrating over a translationally invariant SA-NCSM one-body scalar density, spin-projected momentum distribution, and the Wolfenstein amplitudes $A$, $C$, and $M$. The resulting nonlocal potentials serve as input for a momentum space Lippmann-Schwinger equation, whose solutions are summed up to obtain nucleon-nucleus scattering observables. In the SA-NCSM, the model space is systematically up-selected using $\\SpR{3}$ symmetry considerations. For the light nucleus of $^6$He, we establish a systematic selection scheme in the SA-NCSM for scattering observables. Then, we apply this scheme to calculations of scattering observables, such as differential cross sections, analyzing powers, and spin rotation functions for elastic proton scattering from $^{20}$Ne and $^{40}$Ca in the energy regime between 65 and 200 MeV, and compare to available data. Our calculations show that the leading order effective nucleon-nucleus potential in the Watson expansion of multiple scattering theory obtained from an up-selected SA-NCSM model space describes $^{40}$Ca elastic scattering observables reasonably well to about 60 degrees in the center-of-mass frame, which coincides roughly with the validity of the NNLO chiral interaction used to calculate both the nucleon-nucleon amplitudes and the one-body scalar and spin nuclear densities.","sentences":["Based on the Watson expansion of the multiple scattering series, we employ a nonlocal translationally invariant nuclear density derived within the symmetry-adapted no-core shell model (SA-NCSM) framework from a chiral next-to-next-to-leading order (NNLO) nucleon-nucleon interaction and the very same interaction for a consistent full-folding calculation of the effective (optical) potential for nucleon-nucleus scattering for medium-heavy nuclei.","The leading order effective (optical) folding potential is computed by integrating over a translationally invariant SA-NCSM one-body scalar density, spin-projected momentum distribution, and the Wolfenstein amplitudes $A$, $C$, and $M$. The resulting nonlocal potentials serve as input for a momentum space Lippmann-Schwinger equation, whose solutions are summed up to obtain nucleon-nucleus scattering observables.","In the SA-NCSM, the model space is systematically up-selected using $\\SpR{3}$ symmetry considerations.","For the light nucleus of $^6$He, we establish a systematic selection scheme in the SA-NCSM for scattering observables.","Then, we apply this scheme to calculations of scattering observables, such as differential cross sections, analyzing powers, and spin rotation functions for elastic proton scattering from $^{20}$Ne and $^{40}$Ca in the energy regime between 65 and 200 MeV, and compare to available data.","Our calculations show that the leading order effective nucleon-nucleus potential in the Watson expansion of multiple scattering theory obtained from an up-selected SA-NCSM model space describes $^{40}$Ca elastic scattering observables reasonably well to about 60 degrees in the center-of-mass frame, which coincides roughly with the validity of the NNLO chiral interaction used to calculate both the nucleon-nucleon amplitudes and the one-body scalar and spin nuclear densities."],"url":"http://arxiv.org/abs/2404.03106v1","category":"nucl-th"}
{"created":"2024-04-03 23:03:53","title":"Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density","abstract":"Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning. A single camera may observe multiple actors at once, and the robot team may observe individual actors from multiple views. As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations. Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly. We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view. Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation. This gives rise to a multi-robot perception planning problem which we solve via a combination of value iteration and greedy submodular maximization. %using a combination of value iteration to optimize views for individual robots and sequential submodular maximization methods to coordinate the team. We evaluate our approach on challenging scenarios modeled after various kinds of social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly based on the movements of groups of actors. Simulation results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner's approximation of pixel densities our approach also performs comparably for evaluation based on rendered views. Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds the formation and assignment baselines in all scenarios we consider.","sentences":["Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning.","A single camera may observe multiple actors at once, and the robot team may observe individual actors from multiple views.","As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations.","Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly.","We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view.","Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation.","This gives rise to a multi-robot perception planning problem which we solve via a combination of value iteration and greedy submodular maximization.","%using a combination of value iteration to optimize views for individual robots and sequential submodular maximization methods to coordinate the team.","We evaluate our approach on challenging scenarios modeled after various kinds of social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly based on the movements of groups of actors.","Simulation results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner's approximation of pixel densities our approach also performs comparably for evaluation based on rendered views.","Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds the formation and assignment baselines in all scenarios we consider."],"url":"http://arxiv.org/abs/2404.03103v1","category":"cs.RO"}
{"created":"2024-04-03 22:51:54","title":"MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search","abstract":"Cooperative multi-agent reinforcement learning (MARL) has been an increasingly important research topic in the last half-decade because of its great potential for real-world applications. Because of the curse of dimensionality, the popular \"centralized training decentralized execution\" framework requires a long time in training, yet still cannot converge efficiently. In this paper, we propose a general training framework, MARL-LNS, to algorithmically address these issues by training on alternating subsets of agents using existing deep MARL algorithms as low-level trainers, while not involving any additional parameters to be trained. Based on this framework, we provide three algorithm variants based on the framework: random large neighborhood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood search (ALNS), which alternate the subsets of agents differently. We test our algorithms on both the StarCraft Multi-Agent Challenge and Google Research Football, showing that our algorithms can automatically reduce at least 10% of training time while reaching the same final skill level as the original algorithm.","sentences":["Cooperative multi-agent reinforcement learning (MARL) has been an increasingly important research topic in the last half-decade because of its great potential for real-world applications.","Because of the curse of dimensionality, the popular \"centralized training decentralized execution\" framework requires a long time in training, yet still cannot converge efficiently.","In this paper, we propose a general training framework, MARL-LNS, to algorithmically address these issues by training on alternating subsets of agents using existing deep MARL algorithms as low-level trainers, while not involving any additional parameters to be trained.","Based on this framework, we provide three algorithm variants based on the framework: random large neighborhood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood search (ALNS), which alternate the subsets of agents differently.","We test our algorithms on both the StarCraft Multi-Agent Challenge and Google Research Football, showing that our algorithms can automatically reduce at least 10% of training time while reaching the same final skill level as the original algorithm."],"url":"http://arxiv.org/abs/2404.03101v1","category":"cs.MA"}
{"created":"2024-04-03 22:38:54","title":"SalFoM: Dynamic Saliency Prediction with Video Foundation Models","abstract":"Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP. However, current state-of-the-art models employ spatio-temporal transformers trained on limited amounts of data, hindering generalizability adaptation to downstream tasks. The benefits of vision foundation models present a potential solution to improve the VSP process. However, adapting image foundation models to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information. To address these challenges, and as the first initiative to design a VSP model based on video foundation models, we introduce SalFoM, a novel encoder-decoder video transformer architecture. Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal transformer and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map. Our qualitative and quantitative experiments on the challenging VSP benchmark datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods.","sentences":["Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP.","However, current state-of-the-art models employ spatio-temporal transformers trained on limited amounts of data, hindering generalizability adaptation to downstream tasks.","The benefits of vision foundation models present a potential solution to improve the VSP process.","However, adapting image foundation models to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information.","To address these challenges, and as the first initiative to design a VSP model based on video foundation models, we introduce SalFoM, a novel encoder-decoder video transformer architecture.","Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal transformer and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map.","Our qualitative and quantitative experiments on the challenging VSP benchmark datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.03097v1","category":"cs.CV"}
{"created":"2024-04-03 21:47:02","title":"First-order PDES for Graph Neural Networks: Advection And Burgers Equation Models","abstract":"Graph Neural Networks (GNNs) have established themselves as the preferred methodology in a multitude of domains, ranging from computer vision to computational biology, especially in contexts where data inherently conform to graph structures. While many existing methods have endeavored to model GNNs using various techniques, a prevalent challenge they grapple with is the issue of over-smoothing. This paper presents new Graph Neural Network models that incorporate two first-order Partial Differential Equations (PDEs). These models do not increase complexity but effectively mitigate the over-smoothing problem. Our experimental findings highlight the capacity of our new PDE model to achieve comparable results with higher-order PDE models and fix the over-smoothing problem up to 64 layers. These results underscore the adaptability and versatility of GNNs, indicating that unconventional approaches can yield outcomes on par with established techniques.","sentences":["Graph Neural Networks (GNNs) have established themselves as the preferred methodology in a multitude of domains, ranging from computer vision to computational biology, especially in contexts where data inherently conform to graph structures.","While many existing methods have endeavored to model GNNs using various techniques, a prevalent challenge they grapple with is the issue of over-smoothing.","This paper presents new Graph Neural Network models that incorporate two first-order Partial Differential Equations (PDEs).","These models do not increase complexity but effectively mitigate the over-smoothing problem.","Our experimental findings highlight the capacity of our new PDE model to achieve comparable results with higher-order PDE models and fix the over-smoothing problem up to 64 layers.","These results underscore the adaptability and versatility of GNNs, indicating that unconventional approaches can yield outcomes on par with established techniques."],"url":"http://arxiv.org/abs/2404.03081v1","category":"cs.LG"}
{"created":"2024-04-03 20:02:49","title":"Unveiling Energy Pathways in AGN Accretion Flows with the Warm Corona Model for the Soft Excess","abstract":"The soft excess in active galactic nuclei (AGNs) may arise through a combination of relativistic reflection and the effects of a warm corona at the surface of the accretion disc. Detailed examination of the soft excess can therefore constrain models of the transport and dissipation of accretion energy. Here, we analyze 34 XMM-Newton observations from 14 Type I AGNs with the reXcor spectral model which self-consistently combines emission from a warm corona with relativistic reflection assuming a lamppost corona. The model divides accretion energy between the disc, the warm corona, and the lamppost. The XMM-Newton observations span a factor of 188 in Eddington ratio ($\\lambda_{\\mathrm{obs}}$) and 350 in black hole mass, and we find that a warm corona is a significant contributor to the soft excess for 13 of the 14 AGNs with a mean warm corona heating fraction of $0.51$. The reXcor fits reveal that the fraction of accretion energy dissipated in the lamppost is anti-correlated with $\\lambda_{\\mathrm{obs}}$. In contrast, the relationship between $\\lambda_{\\mathrm{obs}}$ and both the optical depth and heating fraction of the warm corona appears to transition from an anti-correlation to a correlation at $\\lambda_{\\mathrm{obs,t}} \\approx 0.15$. Therefore, at least one other physical process in addition to the accretion rate is needed to explain the evolution of the warm corona. Overall, we find that a warm corona appears to be a crucial depository of accretion energy in AGNs across a broad range of $\\lambda_{\\mathrm{obs}}$ and black hole mass.","sentences":["The soft excess in active galactic nuclei (AGNs) may arise through a combination of relativistic reflection and the effects of a warm corona at the surface of the accretion disc.","Detailed examination of the soft excess can therefore constrain models of the transport and dissipation of accretion energy.","Here, we analyze 34 XMM-Newton observations from 14 Type I AGNs with the reXcor spectral model which self-consistently combines emission from a warm corona with relativistic reflection assuming a lamppost corona.","The model divides accretion energy between the disc, the warm corona, and the lamppost.","The XMM-Newton observations span a factor of 188 in Eddington ratio ($\\lambda_{\\mathrm{obs}}$) and 350 in black hole mass, and we find that a warm corona is a significant contributor to the soft excess for 13 of the 14 AGNs with a mean warm corona heating fraction of $0.51$. The reXcor fits reveal that the fraction of accretion energy dissipated in the lamppost is anti-correlated with $\\lambda_{\\mathrm{obs}}$. In contrast, the relationship between $\\lambda_{\\mathrm{obs}}$ and both the optical depth and heating fraction of the warm corona appears to transition from an anti-correlation to a correlation at $\\lambda_{\\mathrm{obs,t}} \\approx 0.15$.","Therefore, at least one other physical process in addition to the accretion rate is needed to explain the evolution of the warm corona.","Overall, we find that a warm corona appears to be a crucial depository of accretion energy in AGNs across a broad range of $\\lambda_{\\mathrm{obs}}$ and black hole mass."],"url":"http://arxiv.org/abs/2404.03040v1","category":"astro-ph.HE"}
{"created":"2024-04-03 19:43:17","title":"Global Convergence of High-Order Regularization Methods with Sums-of-Squares Taylor Models","abstract":"High-order tensor methods that employ Taylor-based local models (of degree $p\\ge 3$) within adaptive regularization frameworks have been recently proposed for both convex and nonconvex optimization problems. They have been shown to have superior, and even optimal, worst-case global convergence rates and local rates compared to Newton's method. Finding rigorous and efficient techniques for minimizing the Taylor polynomial sub-problems remains a challenging aspect for these algorithms. Ahmadi et al. recently introduced a tensor method based on sum-of-squares (SoS) reformulations, so that each Taylor polynomial sub-problem in their approach can be tractably minimized using semidefinite programming (SDP); however, the global convergence and complexity of their method have not been addressed for general nonconvex problems. This paper introduces an algorithmic framework that combines the Sum of Squares (SoS) Taylor model with adaptive regularization techniques for nonconvex smooth optimization problems. Each iteration minimizes an SoS Taylor model, offering a polynomial cost per iteration. For general nonconvex functions, the worst-case evaluation complexity bound is $\\mathcal{O}(\\epsilon^{-2})$, while for strongly convex functions, an improved evaluation complexity bound of $\\mathcal{O}(\\epsilon^{-\\frac{1}{p}})$ is established. To the best of our knowledge, this is the first global rate analysis for an adaptive regularization algorithm with a tractable high-order sub-problem in nonconvex smooth optimization, opening the way for further improvements.","sentences":["High-order tensor methods that employ Taylor-based local models (of degree $p\\ge 3$) within adaptive regularization frameworks have been recently proposed for both convex and nonconvex optimization problems.","They have been shown to have superior, and even optimal, worst-case global convergence rates and local rates compared to Newton's method.","Finding rigorous and efficient techniques for minimizing the Taylor polynomial sub-problems remains a challenging aspect for these algorithms.","Ahmadi et al. recently introduced a tensor method based on sum-of-squares (SoS) reformulations, so that each Taylor polynomial sub-problem in their approach can be tractably minimized using semidefinite programming (SDP); however, the global convergence and complexity of their method have not been addressed for general nonconvex problems.","This paper introduces an algorithmic framework that combines the Sum of Squares (SoS) Taylor model with adaptive regularization techniques for nonconvex smooth optimization problems.","Each iteration minimizes an SoS Taylor model, offering a polynomial cost per iteration.","For general nonconvex functions, the worst-case evaluation complexity bound is $\\mathcal{O}(\\epsilon^{-2})$, while for strongly convex functions, an improved evaluation complexity bound of $\\mathcal{O}(\\epsilon^{-\\frac{1}{p}})$ is established.","To the best of our knowledge, this is the first global rate analysis for an adaptive regularization algorithm with a tractable high-order sub-problem in nonconvex smooth optimization, opening the way for further improvements."],"url":"http://arxiv.org/abs/2404.03035v1","category":"math.OC"}
{"created":"2024-04-03 18:04:17","title":"Damping Reveals Hidden Dimensions in Elastic Metastructures Through Induced Transparency","abstract":"Damping typically results in attenuation of vibrations and elastic wave propagation in mechanical systems. Contrary to this conventional understanding, we demonstrate experimentally and explain theoretically the revival of an elastic wave transmitted through a periodic metastructure when a weak non-Hermitian defect (damping mechanism) induces violation of time-reversal symmetry. Damping alters the nature of the system's resonant modes, instigating interference in the scattering field. This leads to transmission revival, revealing the presence of hidden modes which are otherwise masked by the symmetry. Our findings offer an innovative approach for designing dissipation-driven switches and controllers and non-destructive structural health monitoring systems.","sentences":["Damping typically results in attenuation of vibrations and elastic wave propagation in mechanical systems.","Contrary to this conventional understanding, we demonstrate experimentally and explain theoretically the revival of an elastic wave transmitted through a periodic metastructure when a weak non-Hermitian defect (damping mechanism) induces violation of time-reversal symmetry.","Damping alters the nature of the system's resonant modes, instigating interference in the scattering field.","This leads to transmission revival, revealing the presence of hidden modes which are otherwise masked by the symmetry.","Our findings offer an innovative approach for designing dissipation-driven switches and controllers and non-destructive structural health monitoring systems."],"url":"http://arxiv.org/abs/2404.02979v1","category":"physics.app-ph"}
{"created":"2024-04-03 18:00:36","title":"Scaling Laws for Galaxy Images","abstract":"We present the first systematic investigation of supervised scaling laws outside of an ImageNet-like context - on images of galaxies. We use 840k galaxy images and over 100M annotations by Galaxy Zoo volunteers, comparable in scale to Imagenet-1K. We find that adding annotated galaxy images provides a power law improvement in performance across all architectures and all tasks, while adding trainable parameters is effective only for some (typically more subjectively challenging) tasks. We then compare the downstream performance of finetuned models pretrained on either ImageNet-12k alone vs. additionally pretrained on our galaxy images. We achieve an average relative error rate reduction of 31% across 5 downstream tasks of scientific interest. Our finetuned models are more label-efficient and, unlike their ImageNet-12k-pretrained equivalents, often achieve linear transfer performance equal to that of end-to-end finetuning. We find relatively modest additional downstream benefits from scaling model size, implying that scaling alone is not sufficient to address our domain gap, and suggest that practitioners with qualitatively different images might benefit more from in-domain adaption followed by targeted downstream labelling.","sentences":["We present the first systematic investigation of supervised scaling laws outside of an ImageNet-like context - on images of galaxies.","We use 840k galaxy images and over 100M annotations by Galaxy Zoo volunteers, comparable in scale to Imagenet-1K.","We find that adding annotated galaxy images provides a power law improvement in performance across all architectures and all tasks, while adding trainable parameters is effective only for some (typically more subjectively challenging) tasks.","We then compare the downstream performance of finetuned models pretrained on either ImageNet-12k alone vs. additionally pretrained on our galaxy images.","We achieve an average relative error rate reduction of 31% across 5 downstream tasks of scientific interest.","Our finetuned models are more label-efficient and, unlike their ImageNet-12k-pretrained equivalents, often achieve linear transfer performance equal to that of end-to-end finetuning.","We find relatively modest additional downstream benefits from scaling model size, implying that scaling alone is not sufficient to address our domain gap, and suggest that practitioners with qualitatively different images might benefit more from in-domain adaption followed by targeted downstream labelling."],"url":"http://arxiv.org/abs/2404.02973v1","category":"cs.CV"}
{"created":"2024-04-03 16:23:37","title":"Cross-Modal Conditioned Reconstruction for Language-guided Medical Image Segmentation","abstract":"Recent developments underscore the potential of textual information in enhancing learning models for a deeper understanding of medical visual semantics. However, language-guided medical image segmentation still faces a challenging issue. Previous works employ implicit and ambiguous architectures to embed textual information. This leads to segmentation results that are inconsistent with the semantics represented by the language, sometimes even diverging significantly. To this end, we propose a novel cross-modal conditioned Reconstruction for Language-guided Medical Image Segmentation (RecLMIS) to explicitly capture cross-modal interactions, which assumes that well-aligned medical visual features and medical notes can effectively reconstruct each other. We introduce conditioned interaction to adaptively predict patches and words of interest. Subsequently, they are utilized as conditioning factors for mutual reconstruction to align with regions described in the medical notes. Extensive experiments demonstrate the superiority of our RecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+ dataset and achieving an average increase of 1.89% mIoU for cross-domain tests on our QATA-CoV19 dataset. Simultaneously, we achieve a relative reduction of 20.2% in parameter count and a 55.5% decrease in computational load. The code will be available at https://github.com/ShashankHuang/RecLMIS.","sentences":["Recent developments underscore the potential of textual information in enhancing learning models for a deeper understanding of medical visual semantics.","However, language-guided medical image segmentation still faces a challenging issue.","Previous works employ implicit and ambiguous architectures to embed textual information.","This leads to segmentation results that are inconsistent with the semantics represented by the language, sometimes even diverging significantly.","To this end, we propose a novel cross-modal conditioned Reconstruction for Language-guided Medical Image Segmentation (RecLMIS) to explicitly capture cross-modal interactions, which assumes that well-aligned medical visual features and medical notes can effectively reconstruct each other.","We introduce conditioned interaction to adaptively predict patches and words of interest.","Subsequently, they are utilized as conditioning factors for mutual reconstruction to align with regions described in the medical notes.","Extensive experiments demonstrate the superiority of our RecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+ dataset and achieving an average increase of 1.89% mIoU for cross-domain tests on our QATA-CoV19 dataset.","Simultaneously, we achieve a relative reduction of 20.2% in parameter count and a 55.5% decrease in computational load.","The code will be available at https://github.com/ShashankHuang/RecLMIS."],"url":"http://arxiv.org/abs/2404.02845v1","category":"cs.CV"}
{"created":"2024-04-03 15:23:22","title":"Dancing above the abyss: Environmental effects and dark matter signatures in inspirals into massive black holes","abstract":"In this dissertation, we look at environmental effects in extreme and intermediate mass ratio inspirals into massive black holes. In these systems, stellar mass compact objects orbit massive black holes and lose orbital energy due to gravitational wave emission and other dissipative forces. We explore environmental interactions with dark matter spikes, stellar distributions, accretion disks, and combine and compare them. We discuss the existence and properties of dark matter spikes in the presence of these environmental effects. The signatures of the environmental effects, such as the phase space flow, dephasing, deshifting of the periapse, and alignment with accretion disks, are examined. These signatures are quantified in isolated spike systems, in dry, and in wet inspirals. We generally find dark matter effects to be subdominant to the other environmental effects, but their impact on the waveform is still observable and identifiable. Lastly, the rates of inspirals and the impact of spikes are estimated. All of these results are obtained with the help of a code imripy that is published alongside. If dark matter spikes exist, they should be observable with space-based gravitational wave observatories","sentences":["In this dissertation, we look at environmental effects in extreme and intermediate mass ratio inspirals into massive black holes.","In these systems, stellar mass compact objects orbit massive black holes and lose orbital energy due to gravitational wave emission and other dissipative forces.","We explore environmental interactions with dark matter spikes, stellar distributions, accretion disks, and combine and compare them.","We discuss the existence and properties of dark matter spikes in the presence of these environmental effects.","The signatures of the environmental effects, such as the phase space flow, dephasing, deshifting of the periapse, and alignment with accretion disks, are examined.","These signatures are quantified in isolated spike systems, in dry, and in wet inspirals.","We generally find dark matter effects to be subdominant to the other environmental effects, but their impact on the waveform is still observable and identifiable.","Lastly, the rates of inspirals and the impact of spikes are estimated.","All of these results are obtained with the help of a code imripy that is published alongside.","If dark matter spikes exist, they should be observable with space-based gravitational wave observatories"],"url":"http://arxiv.org/abs/2404.02808v1","category":"gr-qc"}
{"created":"2024-04-03 15:19:40","title":"Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations","abstract":"In this note, we extend the analysis for the residual-based a posteriori error estimators in the energy norm defined for the algebraic flux correction (AFC) schemes [Jha20.CAMWA] to the newly proposed algebraic stabilization schemes [JK21.NM, Kn23.NA]. Numerical simulations on adaptively refined grids are performed in two dimensions showing the higher efficiency of an algebraic stabilization with similar accuracy compared with an AFC scheme.","sentences":["In this note, we extend the analysis for the residual-based a posteriori error estimators in the energy norm defined for the algebraic flux correction (AFC) schemes [Jha20.CAMWA] to the newly proposed algebraic stabilization schemes [JK21.NM, Kn23.NA].","Numerical simulations on adaptively refined grids are performed in two dimensions showing the higher efficiency of an algebraic stabilization with similar accuracy compared with an AFC scheme."],"url":"http://arxiv.org/abs/2404.02804v1","category":"math.NA"}
{"created":"2024-04-03 14:58:30","title":"Information propagation in far-from-equilibrium molecular templating networks is optimised by pseudo-equilibrium systems with negligible dissipation","abstract":"Far-from equilibrium molecular templating networks, like those that maintain the populations of RNA and protein molecules in the cell, are key biological motifs. These networks share the general property that assembled products are produced and degraded via complex pathways controlled by catalysts, including molecular templates. Although it has been suggested that the information propagated from templates to products sets a lower bound on the thermodynamic cost of these networks, this bound has not been explored rigorously to date. We show that, for an arbitrarily catalytic reaction network in steady state, the specificity with which a single product can dominate the ensemble is upper bounded, and the entropy of the product ensemble lower bounded, by a function of $\\Delta G$, the difference between the maximal and minimal free-energy changes along pathways to assembly. These simple bounds are particularly restrictive for systems with a smaller number of possible products $M$. Remarkably, however, although $\\Delta G$ constrains the information propagated to the product distribution, the systems that saturate the bound operate in a pseudo-equilibrium fashion, and there is no minimal entropy production rate for maintaining this non-equilibrium distribution. Moreover, for large systems, a vanishingly small subset of the possible products can dominate the product ensemble even for small values of $\\Delta G/\\ln M$.","sentences":["Far-from equilibrium molecular templating networks, like those that maintain the populations of RNA and protein molecules in the cell, are key biological motifs.","These networks share the general property that assembled products are produced and degraded via complex pathways controlled by catalysts, including molecular templates.","Although it has been suggested that the information propagated from templates to products sets a lower bound on the thermodynamic cost of these networks, this bound has not been explored rigorously to date.","We show that, for an arbitrarily catalytic reaction network in steady state, the specificity with which a single product can dominate the ensemble is upper bounded, and the entropy of the product ensemble lower bounded, by a function of $\\Delta G$, the difference between the maximal and minimal free-energy changes along pathways to assembly.","These simple bounds are particularly restrictive for systems with a smaller number of possible products $M$. Remarkably, however, although $\\Delta G$ constrains the information propagated to the product distribution, the systems that saturate the bound operate in a pseudo-equilibrium fashion, and there is no minimal entropy production rate for maintaining this non-equilibrium distribution.","Moreover, for large systems, a vanishingly small subset of the possible products can dominate the product ensemble even for small values of $\\Delta G/\\ln M$."],"url":"http://arxiv.org/abs/2404.02791v1","category":"physics.bio-ph"}
{"created":"2024-04-03 14:39:47","title":"FPT: Feature Prompt Tuning for Few-shot Readability Assessment","abstract":"Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features. Also, our proposed model significantly outperforms the large language model gpt-3.5-turbo-16k in most cases. Our proposed method establishes a new architecture for prompt tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks.","sentences":["Prompt-based methods have achieved promising results in most few-shot text classification tasks.","However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential.","Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.","To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT).","Specifically, we extract linguistic features from the text and embed them into trainable soft prompts.","Further, we devise a new loss function to calibrate the similarity ranking order between categories.","Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features.","Also, our proposed model significantly outperforms the large language model gpt-3.5-turbo-16k in most cases.","Our proposed method establishes a new architecture for prompt tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks."],"url":"http://arxiv.org/abs/2404.02772v1","category":"cs.CL"}
{"created":"2024-04-03 14:33:49","title":"Locking-free hybrid high-order method for linear elasticity","abstract":"The hybrid-high order (HHO) scheme has many successful applications including linear elasticity as the first step towards computational solid mechanics. The striking advantage is the simplicity among other higher-order nonconforming schemes and its geometric flexibility as a polytopal method on the expanse of a parameter-free refined stabilization. The classical suggestion of a locking-free HHO discretization requires a split of the the reconstruction terms with an additional reconstruction of the divergence operator that might be motivated by the Stokes equations for the robust approximation in the incompressible limit, when one Lam\\'e parameter $\\lambda\\to\\infty$ becomes very large. This paper utilizes just one reconstruction operator for the linear Green strain and therefore does not rely on a split in deviatoric and spherical behavior. The a priori error analysis provides quasi-best approximation with $\\lambda$-independent equivalence constants. The reliable and (up to data oscillations) efficient a posteriori error estimates are stabilization-free and $\\lambda$-robust. The error analysis is carried out on simplicial meshes to allow conforming piecewise polynomials finite elements in the kernel of the stabilization terms. Numerical benchmarks provide empirical evidence for optimal convergence rates of the a posteriori error estimator in some associated adaptive mesh-refining algorithm also in the incompressible limit.","sentences":["The hybrid-high order (HHO) scheme has many successful applications including linear elasticity as the first step towards computational solid mechanics.","The striking advantage is the simplicity among other higher-order nonconforming schemes and its geometric flexibility as a polytopal method on the expanse of a parameter-free refined stabilization.","The classical suggestion of a locking-free HHO discretization requires a split of the the reconstruction terms with an additional reconstruction of the divergence operator that might be motivated by the Stokes equations for the robust approximation in the incompressible limit, when one Lam\\'e parameter $\\lambda\\to\\infty$ becomes very large.","This paper utilizes just one reconstruction operator for the linear Green strain and therefore does not rely on a split in deviatoric and spherical behavior.","The a priori error analysis provides quasi-best approximation with $\\lambda$-independent equivalence constants.","The reliable and (up to data oscillations) efficient a posteriori error estimates are stabilization-free and $\\lambda$-robust.","The error analysis is carried out on simplicial meshes to allow conforming piecewise polynomials finite elements in the kernel of the stabilization terms.","Numerical benchmarks provide empirical evidence for optimal convergence rates of the a posteriori error estimator in some associated adaptive mesh-refining algorithm also in the incompressible limit."],"url":"http://arxiv.org/abs/2404.02768v1","category":"math.NA"}
{"created":"2024-04-03 14:29:42","title":"Fast Diffusion Model For Seismic Data Noise Attenuation","abstract":"Noise is one of the primary sources of interference in seismic exploration. Many authors have proposed various methods to remove noise from seismic data; however, in the face of strong noise conditions, satisfactory results are often not achievable. In recent years, methods based on diffusion models have been applied to the task of strong noise processing in seismic data. However, due to iterative computations, the computational efficiency of diffusion-based methods is much lower than conventional methods. To address this issue, we propose using an improved Bayesian equation for iterations, removing the stochastic terms from the computation. Additionally, we proposed a new normalization method adapted to the diffusion model. Through various improvements, on synthetic datasets and field datasets, our proposed method achieves significantly better noise attenuation effects compared to the benchmark methods, while also achieving a several-fold increase in computational speed. We employ transfer learning to demonstrate the robustness of our proposed method on open-source synthetic seismic data and validate on open-source field data sets. Finally, we open-sourced the code to promote the development of high-precision and efficient seismic exploration work.","sentences":["Noise is one of the primary sources of interference in seismic exploration.","Many authors have proposed various methods to remove noise from seismic data; however, in the face of strong noise conditions, satisfactory results are often not achievable.","In recent years, methods based on diffusion models have been applied to the task of strong noise processing in seismic data.","However, due to iterative computations, the computational efficiency of diffusion-based methods is much lower than conventional methods.","To address this issue, we propose using an improved Bayesian equation for iterations, removing the stochastic terms from the computation.","Additionally, we proposed a new normalization method adapted to the diffusion model.","Through various improvements, on synthetic datasets and field datasets, our proposed method achieves significantly better noise attenuation effects compared to the benchmark methods, while also achieving a several-fold increase in computational speed.","We employ transfer learning to demonstrate the robustness of our proposed method on open-source synthetic seismic data and validate on open-source field data sets.","Finally, we open-sourced the code to promote the development of high-precision and efficient seismic exploration work."],"url":"http://arxiv.org/abs/2404.02767v1","category":"physics.geo-ph"}
{"created":"2024-04-03 14:19:35","title":"Closing the Implementation Gap in MC: Fully Chemical Synchronization and Detection for Cellular Receivers","abstract":"In the context of the Internet of Bio-Nano Things (IoBNT), nano-devices are envisioned to perform complex tasks collaboratively, i.e., by communicating with each other. One candidate for the implementation of such devices are engineered cells due to their inherent biocompatibility. However, because each engineered cell has only little computational capabilities, transmitter and receiver (RX) functionalities can afford only limited complexity. In this paper, we propose a simple, yet modular, architecture for a cellular RX that is capable of processing a stream of observed symbols using chemical reaction networks. Furthermore, we propose two specific detector implementations for the RX. The first detector is based on a machine learning model that is trained offline, i.e., before the cellular RX is deployed. The second detector utilizes pilot symbol-based training and is therefore able to continuously adapt to changing channel conditions online, i.e., after deployment. To coordinate the different chemical processing steps involved in symbol detection, the proposed cellular RX leverages an internal chemical timer. Furthermore, the RX is synchronized with the transmitter via external, i.e., extracellular, signals. Finally, the proposed architecture is validated using theoretical analysis and stochastic simulations. The presented results confirm the feasibility of both proposed implementations and reveal that the proposed online learning-based RX is able to perform reliable detection even in initially unknown or slowly changing channels. By its modular design and exclusively chemical implementation, the proposed RX contributes towards the realization of versatile and biocompatible nano-scale communication networks for IoBNT applications narrowing the existing implementation gap in cellular molecular communication (MC).","sentences":["In the context of the Internet of Bio-Nano Things (IoBNT), nano-devices are envisioned to perform complex tasks collaboratively, i.e., by communicating with each other.","One candidate for the implementation of such devices are engineered cells due to their inherent biocompatibility.","However, because each engineered cell has only little computational capabilities, transmitter and receiver (RX) functionalities can afford only limited complexity.","In this paper, we propose a simple, yet modular, architecture for a cellular RX that is capable of processing a stream of observed symbols using chemical reaction networks.","Furthermore, we propose two specific detector implementations for the RX.","The first detector is based on a machine learning model that is trained offline, i.e., before the cellular RX is deployed.","The second detector utilizes pilot symbol-based training and is therefore able to continuously adapt to changing channel conditions online, i.e., after deployment.","To coordinate the different chemical processing steps involved in symbol detection, the proposed cellular RX leverages an internal chemical timer.","Furthermore, the RX is synchronized with the transmitter via external, i.e., extracellular, signals.","Finally, the proposed architecture is validated using theoretical analysis and stochastic simulations.","The presented results confirm the feasibility of both proposed implementations and reveal that the proposed online learning-based RX is able to perform reliable detection even in initially unknown or slowly changing channels.","By its modular design and exclusively chemical implementation, the proposed RX contributes towards the realization of versatile and biocompatible nano-scale communication networks for IoBNT applications narrowing the existing implementation gap in cellular molecular communication (MC)."],"url":"http://arxiv.org/abs/2404.02765v1","category":"cs.ET"}
{"created":"2024-04-03 14:16:15","title":"Impact and Integration of Mini Photovoltaic Systems on Electric Power Distribution Grids","abstract":"This work analyzes the impact of varying concentrations mini-photovoltaic (MPV) systems, often referred to as balcony power plants, on the stability and control of the low-voltage (LV) grid. By local energy use and potentially reversing meter operation, we focus on how these MPV systems transform grid dynamics and elucidate consumer participation in the energy transition. We scrutinize the effects of these systems on power quality, power loss, transformer loading, and the functioning of other inverter-based voltage-regulating distributed energy resources (DER). Owing to the rise in renewable output from MPVs, the emerging bidirectional energy flow poses challenges for distribution grids abundant with DERs. Our case studies, featuring sensitivity analysis and comparison of distributed and decentralized DER control strategies, highlight that autonomous inverters are essential for providing ancillary services. With the growing use of battery energy storage (BES) systems in LV grids for these services, the need for adaptable DER control strategies becomes increasingly evident.","sentences":["This work analyzes the impact of varying concentrations mini-photovoltaic (MPV) systems, often referred to as balcony power plants, on the stability and control of the low-voltage (LV) grid.","By local energy use and potentially reversing meter operation, we focus on how these MPV systems transform grid dynamics and elucidate consumer participation in the energy transition.","We scrutinize the effects of these systems on power quality, power loss, transformer loading, and the functioning of other inverter-based voltage-regulating distributed energy resources (DER).","Owing to the rise in renewable output from MPVs, the emerging bidirectional energy flow poses challenges for distribution grids abundant with DERs.","Our case studies, featuring sensitivity analysis and comparison of distributed and decentralized DER control strategies, highlight that autonomous inverters are essential for providing ancillary services.","With the growing use of battery energy storage (BES) systems in LV grids for these services, the need for adaptable DER control strategies becomes increasingly evident."],"url":"http://arxiv.org/abs/2404.02763v1","category":"eess.SY"}
{"created":"2024-04-03 14:03:39","title":"Coarse spaces for non-symmetric two-level preconditioners based on local generalized eigenproblems","abstract":"Domain decomposition (DD) methods are a natural way to take advantage of parallel computers when solving large scale linear systems. Their scalability depends on the design of the coarse space used in the two-level method. The analysis of adaptive coarse spaces we present here is quite general since it applies to symmetric and non symmetric problems, to symmetric preconditioners such the additive Schwarz method (ASM) and to the non-symmetric preconditioner restricted additive Schwarz (RAS), as well as to exact or inexact subdomain solves. The coarse space is built by solving generalized eigenvalues in the subdomains and applying a well-chosen operator to the selected eigenvectors.","sentences":["Domain decomposition (DD) methods are a natural way to take advantage of parallel computers when solving large scale linear systems.","Their scalability depends on the design of the coarse space used in the two-level method.","The analysis of adaptive coarse spaces we present here is quite general since it applies to symmetric and non symmetric problems, to symmetric preconditioners such the additive Schwarz method (ASM) and to the non-symmetric preconditioner restricted additive Schwarz (RAS), as well as to exact or inexact subdomain solves.","The coarse space is built by solving generalized eigenvalues in the subdomains and applying a well-chosen operator to the selected eigenvectors."],"url":"http://arxiv.org/abs/2404.02758v1","category":"math.NA"}
{"created":"2024-04-03 13:50:57","title":"Direct in-situ observations of wave-induced floe collisions in the deeper Marginal Ice Zone","abstract":"Ocean waves propagating through the Marginal Ice Zone (MIZ) and the pack ice are strongly attenuated. This attenuation is critical for protecting sea ice from energetic wave events that could otherwise lead to sea ice break-up and dislocation over large areas. Despite the importance of waves-in-ice attenuation, the exact physical mechanisms involved, and their relative importance, are still uncertain. Here we present for the first time direct in situ measurements of floe-floe interactions under the influence of waves, including collisions between adjacent floes. The collision events we report are aligned with the incoming wave direction, and phase-locked to the wave signal, which indicates that the individual collisions we detect are wave-induced. The observations demonstrate that wave attenuation by wave-induced floe-floe collisions, which have been studied in idealized laboratory and field experiments in the outer MIZ, are indeed a likely source of wave energy dissipation deep inside the MIZ as well.","sentences":["Ocean waves propagating through the Marginal Ice Zone (MIZ) and the pack ice are strongly attenuated.","This attenuation is critical for protecting sea ice from energetic wave events that could otherwise lead to sea ice break-up and dislocation over large areas.","Despite the importance of waves-in-ice attenuation, the exact physical mechanisms involved, and their relative importance, are still uncertain.","Here we present for the first time direct in situ measurements of floe-floe interactions under the influence of waves, including collisions between adjacent floes.","The collision events we report are aligned with the incoming wave direction, and phase-locked to the wave signal, which indicates that the individual collisions we detect are wave-induced.","The observations demonstrate that wave attenuation by wave-induced floe-floe collisions, which have been studied in idealized laboratory and field experiments in the outer MIZ, are indeed a likely source of wave energy dissipation deep inside the MIZ as well."],"url":"http://arxiv.org/abs/2404.02750v1","category":"physics.ao-ph"}
{"created":"2024-04-03 13:35:51","title":"Adaptive Affinity-Based Generalization For MRI Imaging Segmentation Across Resource-Limited Settings","abstract":"The joint utilization of diverse data sources for medical imaging segmentation has emerged as a crucial area of research, aiming to address challenges such as data heterogeneity, domain shift, and data quality discrepancies. Integrating information from multiple data domains has shown promise in improving model generalizability and adaptability. However, this approach often demands substantial computational resources, hindering its practicality. In response, knowledge distillation (KD) has garnered attention as a solution. KD involves training light-weight models to emulate the behavior of more resource-intensive models, thereby mitigating the computational burden while maintaining performance. This paper addresses the pressing need to develop a lightweight and generalizable model for medical imaging segmentation that can effectively handle data integration challenges. Our proposed approach introduces a novel relation-based knowledge framework by seamlessly combining adaptive affinity-based and kernel-based distillation through a gram matrix that can capture the style representation across features. This methodology empowers the student model to accurately replicate the feature representations of the teacher model, facilitating robust performance even in the face of domain shift and data heterogeneity. To validate our innovative approach, we conducted experiments on publicly available multi-source prostate MRI data. The results demonstrate a significant enhancement in segmentation performance using lightweight networks. Notably, our method achieves this improvement while reducing both inference time and storage usage, rendering it a practical and efficient solution for real-time medical imaging segmentation.","sentences":["The joint utilization of diverse data sources for medical imaging segmentation has emerged as a crucial area of research, aiming to address challenges such as data heterogeneity, domain shift, and data quality discrepancies.","Integrating information from multiple data domains has shown promise in improving model generalizability and adaptability.","However, this approach often demands substantial computational resources, hindering its practicality.","In response, knowledge distillation (KD) has garnered attention as a solution.","KD involves training light-weight models to emulate the behavior of more resource-intensive models, thereby mitigating the computational burden while maintaining performance.","This paper addresses the pressing need to develop a lightweight and generalizable model for medical imaging segmentation that can effectively handle data integration challenges.","Our proposed approach introduces a novel relation-based knowledge framework by seamlessly combining adaptive affinity-based and kernel-based distillation through a gram matrix that can capture the style representation across features.","This methodology empowers the student model to accurately replicate the feature representations of the teacher model, facilitating robust performance even in the face of domain shift and data heterogeneity.","To validate our innovative approach, we conducted experiments on publicly available multi-source prostate MRI data.","The results demonstrate a significant enhancement in segmentation performance using lightweight networks.","Notably, our method achieves this improvement while reducing both inference time and storage usage, rendering it a practical and efficient solution for real-time medical imaging segmentation."],"url":"http://arxiv.org/abs/2404.02738v1","category":"cs.CV"}
{"created":"2024-04-03 13:34:09","title":"InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation","abstract":"Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements. Our codes will be available at https://github.com/InstantStyle/InstantStyle.","sentences":["Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization.","However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation.","Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others.","Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details.","Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability.","In this paper, we commence by examining several compelling yet frequently overlooked observations.","We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another.","2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.","Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements.","Our codes will be available at https://github.com/InstantStyle/InstantStyle."],"url":"http://arxiv.org/abs/2404.02733v1","category":"cs.CV"}
{"created":"2024-04-03 12:50:45","title":"Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition","abstract":"In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of face recognition, developing a novel method for privacy-preserving representation learning within an end-to-end training framework. Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss. This research provides a foundational exploration into the integration of information-theoretic privacy principles with representation learning, focusing specifically on the face recognition systems. We particularly highlight the adaptability of our framework with recent advancements in face recognition networks, such as AdaFace and ArcFace. In addition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This $\\mathsf{GenPF}$ model brings new perspectives on data generation methods with estimation-theoretic and information-theoretic privacy guarantees. Complementing these developments, we also present the deep variational PF (DVPF) model. This model proposes a tractable variational bound for measuring information leakage, enhancing the understanding of privacy preservation challenges in deep representation learning. The DVPF model, associated with both $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections with various generative models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion models. Complementing our theoretical contributions, we release a reproducible PyTorch package, facilitating further exploration and application of these privacy-preserving methodologies in face recognition systems.","sentences":["In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of face recognition, developing a novel method for privacy-preserving representation learning within an end-to-end training framework.","Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss.","This research provides a foundational exploration into the integration of information-theoretic privacy principles with representation learning, focusing specifically on the face recognition systems.","We particularly highlight the adaptability of our framework with recent advancements in face recognition networks, such as AdaFace and ArcFace.","In addition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$).","This $\\mathsf{GenPF}$ model brings new perspectives on data generation methods with estimation-theoretic and information-theoretic privacy guarantees.","Complementing these developments, we also present the deep variational PF (DVPF) model.","This model proposes a tractable variational bound for measuring information leakage, enhancing the understanding of privacy preservation challenges in deep representation learning.","The DVPF model, associated with both $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections with various generative models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion models.","Complementing our theoretical contributions, we release a reproducible PyTorch package, facilitating further exploration and application of these privacy-preserving methodologies in face recognition systems."],"url":"http://arxiv.org/abs/2404.02696v1","category":"cs.LG"}
{"created":"2024-04-03 11:15:56","title":"One Stack to Rule them All: To Drive Automated Vehicles, and Reach for the 4th level","abstract":"Most automated driving functions are designed for a specific task or vehicle. Most often, the underlying architecture is fixed to specific algorithms to increase performance. Therefore, it is not possible to deploy new modules and algorithms easily. In this paper, we present our automated driving stack which combines both scalability and adaptability. Due to the modular design, our stack allows for a fast integration and testing of novel and state-of-the-art research approaches. Furthermore, it is flexible to be used for our different testing vehicles, including modified EasyMile EZ10 shuttles and different passenger cars. These vehicles differ in multiple ways, e.g. sensor setups, control systems, maximum speed, or steering angle limitations. Finally, our stack is deployed in real world environments, including passenger transport in urban areas. Our stack includes all components needed for operating an autonomous vehicle, including localization, perception, planning, controller, and additional safety modules. Our stack is developed, tested, and evaluated in real world traffic in multiple test sites, including the Test Area Autonomous Driving Baden-W\\\"urttemberg.","sentences":["Most automated driving functions are designed for a specific task or vehicle.","Most often, the underlying architecture is fixed to specific algorithms to increase performance.","Therefore, it is not possible to deploy new modules and algorithms easily.","In this paper, we present our automated driving stack which combines both scalability and adaptability.","Due to the modular design, our stack allows for a fast integration and testing of novel and state-of-the-art research approaches.","Furthermore, it is flexible to be used for our different testing vehicles, including modified EasyMile EZ10 shuttles and different passenger cars.","These vehicles differ in multiple ways, e.g. sensor setups, control systems, maximum speed, or steering angle limitations.","Finally, our stack is deployed in real world environments, including passenger transport in urban areas.","Our stack includes all components needed for operating an autonomous vehicle, including localization, perception, planning, controller, and additional safety modules.","Our stack is developed, tested, and evaluated in real world traffic in multiple test sites, including the Test Area Autonomous Driving Baden-W\\\"urttemberg."],"url":"http://arxiv.org/abs/2404.02645v1","category":"cs.RO"}
{"created":"2024-04-03 11:07:05","title":"Goal-oriented time adaptivity for port-Hamiltonian systems","abstract":"Port-Hamiltonian systems provide an energy-based modeling paradigm for dynamical input-state-output systems. At their core, they fulfill an energy balance relating stored, dissipated and supplied energy. To accurately resolve this energy balance in time discretizations, we propose an adaptive grid refinement technique based on a posteriori error estimation. The evaluation of the error estimator includes the computation of adjoint sensitivities. To interpret this adjoint equation as a backwards-in-time equation, we show piecewise weak differentiability of the dual variable. Then, leveraging dissipativity of the port-Hamiltonian dynamics, we present a parallelizable approximation of the underlying adjoint system in the spirit of a block-Jacobi method to efficiently compute error indicators. We illustrate the performance of the proposed scheme by means of numerical experiments showing that it yields a smaller violation of the energy balance when compared to uniform refinements and traditional step-size controlled time stepping.","sentences":["Port-Hamiltonian systems provide an energy-based modeling paradigm for dynamical input-state-output systems.","At their core, they fulfill an energy balance relating stored, dissipated and supplied energy.","To accurately resolve this energy balance in time discretizations, we propose an adaptive grid refinement technique based on a posteriori error estimation.","The evaluation of the error estimator includes the computation of adjoint sensitivities.","To interpret this adjoint equation as a backwards-in-time equation, we show piecewise weak differentiability of the dual variable.","Then, leveraging dissipativity of the port-Hamiltonian dynamics, we present a parallelizable approximation of the underlying adjoint system in the spirit of a block-Jacobi method to efficiently compute error indicators.","We illustrate the performance of the proposed scheme by means of numerical experiments showing that it yields a smaller violation of the energy balance when compared to uniform refinements and traditional step-size controlled time stepping."],"url":"http://arxiv.org/abs/2404.02641v1","category":"math.NA"}
{"created":"2024-04-03 10:25:45","title":"Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition","abstract":"Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN). In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions. In this paper, we propose self-attention GCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets. We utilize spatial self-attention module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal self-attention module to examine correlations between frames of a node. These two are followed by multi-scale convolution network with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors. They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier.","sentences":["Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN).","In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions.","In this paper, we propose self-attention GCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets.","We utilize spatial self-attention module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal self-attention module to examine correlations between frames of a node.","These two are followed by multi-scale convolution network with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors.","They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier."],"url":"http://arxiv.org/abs/2404.02624v1","category":"cs.CV"}
{"created":"2024-04-03 09:15:38","title":"Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect","abstract":"The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning. The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation. Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B). We formally demonstrate that the negativity bias remains in this set-up. We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative.","sentences":["The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning.","The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation.","Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B).","We formally demonstrate that the negativity bias remains in this set-up.","We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative."],"url":"http://arxiv.org/abs/2404.02591v1","category":"cs.LG"}
{"created":"2024-04-03 08:53:42","title":"Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering","abstract":"We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.","sentences":["We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility.","Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage.","A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities.","This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy.","This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task.","Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism.","This iterative and adaptable process enables the agent to learn a desired optimal policy.","Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency."],"url":"http://arxiv.org/abs/2404.02577v1","category":"cs.LG"}
{"created":"2024-04-03 08:25:39","title":"Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks","abstract":"High-dimensional interpolation problems appear in various applications of uncertainty quantification, stochastic optimization and machine learning. Such problems are computationally expensive and request the use of adaptive grid generation strategies like anisotropic sparse grids to mitigate the curse of dimensionality. However, it is well known that the standard dimension-adaptive sparse grid method converges very slowly or even fails in the case of non-smooth functions. For piecewise smooth functions with kinks, we construct two novel hp-adaptive sparse grid collocation algorithms that combine low-order basis functions with local support in parts of the domain with less regularity and variable-order basis functions elsewhere. Spatial refinement is realized by means of a hierarchical multivariate knot tree which allows the construction of localised hierarchical basis functions with varying order. Hierarchical surplus is used as an error indicator to automatically detect the non-smooth region and adaptively refine the collocation points there. The local polynomial degrees are optionally selected by a greedy approach or a kink detection procedure. Three numerical benchmark examples with different dimensions are discussed and comparison with locally linear and highest degree basis functions are given to show the efficiency and accuracy of the proposed methods.","sentences":["High-dimensional interpolation problems appear in various applications of uncertainty quantification, stochastic optimization and machine learning.","Such problems are computationally expensive and request the use of adaptive grid generation strategies like anisotropic sparse grids to mitigate the curse of dimensionality.","However, it is well known that the standard dimension-adaptive sparse grid method converges very slowly or even fails in the case of non-smooth functions.","For piecewise smooth functions with kinks, we construct two novel hp-adaptive sparse grid collocation algorithms that combine low-order basis functions with local support in parts of the domain with less regularity and variable-order basis functions elsewhere.","Spatial refinement is realized by means of a hierarchical multivariate knot tree which allows the construction of localised hierarchical basis functions with varying order.","Hierarchical surplus is used as an error indicator to automatically detect the non-smooth region and adaptively refine the collocation points there.","The local polynomial degrees are optionally selected by a greedy approach or a kink detection procedure.","Three numerical benchmark examples with different dimensions are discussed and comparison with locally linear and highest degree basis functions are given to show the efficiency and accuracy of the proposed methods."],"url":"http://arxiv.org/abs/2404.02556v1","category":"math.NA"}
{"created":"2024-04-03 07:57:22","title":"A Coupled Neural Field Model for the Standard Consolidation Theory","abstract":"The standard consolidation theory states that short-term memories located in the hippocampus enable the consolidation of long-term memories in the neocortex. In other words, the neocortex slowly learns long-term memories with a transient support of the hippocampus that quickly learns unstable memories. However, it is not clear yet what could be the neurobiological mechanisms underlying these differences in learning rates and memory time-scales. Here, we propose a novel modelling approach of the standard consolidation theory, that focuses on its potential neurobiological mechanisms. In addition to synaptic plasticity and spike frequency adaptation, our model incorporates adult neurogenesis in the dentate gyrus as well as the difference in size between the neocortex and the hippocampus, that we associate with distance-dependent synaptic plasticity. We also take into account the interconnected spatial structure of the involved brain areas, by incorporating the above neurobiological mechanisms in a coupled neural field framework, where each area is represented by a separate neural field with intra- and inter-area connections. To our knowledge, this is the first attempt to apply neural fields to this process. Using numerical simulations and mathematical analysis, we explore the short-term and long-term dynamics of the model upon alternance of phases of hippocampal replay and retrieval cue of an external input. This external input is encodable as a memory pattern in the form of a multiple bump attractor pattern in the individual neural fields. In the model, hippocampal memory patterns become encoded first, before neocortical ones, because of the smaller distances between the bumps of the hippocampal memory patterns. As a result, retrieval of the input pattern in the neocortex at short time-scales necessitates the additional input delivered by the memory pattern of the hippocampus. Neocortical memory patterns progressively consolidate at longer times, up to a point where their retrieval does not need the support of the hippocampus anymore. At longer times, perturbation of the hippocampal neural fields by neurogenesis erases the hippocampus pattern, leading to a final state where the memory pattern is exclusively evoked in the neocortex. Therefore, the dynamics of our model successfully reproduces the main features of the standard consolidation theory. This suggests that neurogenesis in the hippocampus and distance-dependent synaptic plasticity coupled to synaptic depression and spike frequency adaptation, are indeed critical neurobiological processes in memory consolidation.","sentences":["The standard consolidation theory states that short-term memories located in the hippocampus enable the consolidation of long-term memories in the neocortex.","In other words, the neocortex slowly learns long-term memories with a transient support of the hippocampus that quickly learns unstable memories.","However, it is not clear yet what could be the neurobiological mechanisms underlying these differences in learning rates and memory time-scales.","Here, we propose a novel modelling approach of the standard consolidation theory, that focuses on its potential neurobiological mechanisms.","In addition to synaptic plasticity and spike frequency adaptation, our model incorporates adult neurogenesis in the dentate gyrus as well as the difference in size between the neocortex and the hippocampus, that we associate with distance-dependent synaptic plasticity.","We also take into account the interconnected spatial structure of the involved brain areas, by incorporating the above neurobiological mechanisms in a coupled neural field framework, where each area is represented by a separate neural field with intra- and inter-area connections.","To our knowledge, this is the first attempt to apply neural fields to this process.","Using numerical simulations and mathematical analysis, we explore the short-term and long-term dynamics of the model upon alternance of phases of hippocampal replay and retrieval cue of an external input.","This external input is encodable as a memory pattern in the form of a multiple bump attractor pattern in the individual neural fields.","In the model, hippocampal memory patterns become encoded first, before neocortical ones, because of the smaller distances between the bumps of the hippocampal memory patterns.","As a result, retrieval of the input pattern in the neocortex at short time-scales necessitates the additional input delivered by the memory pattern of the hippocampus.","Neocortical memory patterns progressively consolidate at longer times, up to a point where their retrieval does not need the support of the hippocampus anymore.","At longer times, perturbation of the hippocampal neural fields by neurogenesis erases the hippocampus pattern, leading to a final state where the memory pattern is exclusively evoked in the neocortex.","Therefore, the dynamics of our model successfully reproduces the main features of the standard consolidation theory.","This suggests that neurogenesis in the hippocampus and distance-dependent synaptic plasticity coupled to synaptic depression and spike frequency adaptation, are indeed critical neurobiological processes in memory consolidation."],"url":"http://arxiv.org/abs/2404.02938v1","category":"q-bio.NC"}
{"created":"2024-04-03 07:27:33","title":"Large Language Model for Vulnerability Detection and Repair: Literature Review and Roadmap","abstract":"The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous recent studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, covering 36 papers published at 21 distinct venues. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of challenges that still need to be tackled considering existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.","sentences":["The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair.","Numerous recent studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks.","Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair.","In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs.","The review encompasses research work from leading SE, AI, and Security conferences and journals, covering 36 papers published at 21 distinct venues.","By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair.","Based on our findings, we have identified a series of challenges that still need to be tackled considering existing studies.","Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors."],"url":"http://arxiv.org/abs/2404.02525v1","category":"cs.SE"}
{"created":"2024-04-03 07:26:15","title":"Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion","abstract":"Generating realistic and controllable agent behaviors in traffic simulation is crucial for the development of autonomous vehicles. This problem is often formulated as imitation learning (IL) from real-world driving data by either directly predicting future trajectories or inferring cost functions with inverse optimal control. In this paper, we draw a conceptual connection between IL and diffusion-based generative modeling and introduce a novel framework Versatile Behavior Diffusion (VBD) to simulate interactive scenarios with multiple traffic participants. Our model not only generates scene-consistent multi-agent interactions but also enables scenario editing through multi-step guidance and refinement. Experimental evaluations show that VBD achieves state-of-the-art performance on the Waymo Sim Agents benchmark. In addition, we illustrate the versatility of our model by adapting it to various applications. VBD is capable of producing scenarios conditioning on priors, integrating with model-based optimization, sampling multi-modal scene-consistent scenarios by fusing marginal predictions, and generating safety-critical scenarios when combined with a game-theoretic solver.","sentences":["Generating realistic and controllable agent behaviors in traffic simulation is crucial for the development of autonomous vehicles.","This problem is often formulated as imitation learning (IL) from real-world driving data by either directly predicting future trajectories or inferring cost functions with inverse optimal control.","In this paper, we draw a conceptual connection between IL and diffusion-based generative modeling and introduce a novel framework Versatile Behavior Diffusion (VBD) to simulate interactive scenarios with multiple traffic participants.","Our model not only generates scene-consistent multi-agent interactions but also enables scenario editing through multi-step guidance and refinement.","Experimental evaluations show that VBD achieves state-of-the-art performance on the Waymo Sim Agents benchmark.","In addition, we illustrate the versatility of our model by adapting it to various applications.","VBD is capable of producing scenarios conditioning on priors, integrating with model-based optimization, sampling multi-modal scene-consistent scenarios by fusing marginal predictions, and generating safety-critical scenarios when combined with a game-theoretic solver."],"url":"http://arxiv.org/abs/2404.02524v1","category":"cs.RO"}
{"created":"2024-04-03 06:53:48","title":"Utilizing Quantum Processor for the Analysis of Strongly Correlated Materials","abstract":"This study introduces a systematic approach for analyzing strongly correlated systems by adapting the conventional quantum cluster method to a quantum circuit model. We have developed a more concise formula for calculating the cluster's Green's function, requiring only real-number computations on the quantum circuit instead of complex ones. This approach is inherently more suited to quantum circuits, which primarily yield statistical probabilities. As an illustrative example, we explored the Hubbard model on a 2D lattice. The ground state is determined utilizing Xiaohong, a superconducting quantum processor equipped with 66 qubits, supplied by QuantumCTek Co., Ltd. Subsequently, we employed the circuit model to compute the real-time retarded Green's function for the cluster, which is then used to determine the lattice Green's function. We conducted an examination of the band structure in the insulator phase of the lattice system. This preliminary investigation lays the groundwork for exploring a wealth of innovative physics within the field of condensed matter physics.","sentences":["This study introduces a systematic approach for analyzing strongly correlated systems by adapting the conventional quantum cluster method to a quantum circuit model.","We have developed a more concise formula for calculating the cluster's Green's function, requiring only real-number computations on the quantum circuit instead of complex ones.","This approach is inherently more suited to quantum circuits, which primarily yield statistical probabilities.","As an illustrative example, we explored the Hubbard model on a 2D lattice.","The ground state is determined utilizing Xiaohong, a superconducting quantum processor equipped with 66 qubits, supplied by QuantumCTek Co., Ltd.","Subsequently, we employed the circuit model to compute the real-time retarded Green's function for the cluster, which is then used to determine the lattice Green's function.","We conducted an examination of the band structure in the insulator phase of the lattice system.","This preliminary investigation lays the groundwork for exploring a wealth of innovative physics within the field of condensed matter physics."],"url":"http://arxiv.org/abs/2404.02509v1","category":"quant-ph"}
{"created":"2024-04-03 06:53:27","title":"VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments","abstract":"Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on the images. Our results show that VIAssist provides +0.21 and +0.31 higher BERTScore and ROUGE scores than the baseline, respectively.","sentences":["Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people.","An estimated 2.2 billion individuals worldwide are affected by visual impairments.","Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains.","It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning.","However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests.","For example, the target object is not fully or partially placed in the image.","This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers.","VIAssist can identify undesired images and provide detailed actions.","Finally, VIAssist can provide reliable answers to users' queries based on the images.","Our results show that VIAssist provides +0.21 and +0.31 higher BERTScore and ROUGE scores than the baseline, respectively."],"url":"http://arxiv.org/abs/2404.02508v1","category":"cs.CV"}
{"created":"2024-04-03 06:47:55","title":"Adaptation of the Phase Distance Correlation Periodogram to Account for Measurement Uncertainties","abstract":"We present an improvement of the phase distance correlation (PDC) periodogram to account for uncertainties in the time-series data. The PDC periodogram, which we introduced in previous papers, is based on the statistical concept of distance correlation. By viewing each measurement and its accompanying error estimate as a probability distribution, we use the concept of energy distance to design a distance function (metric) between measurement-uncertainty pairs. We use this metric as the basis for the PDC periodogram, instead of the simple absolute difference. We demonstrate the periodogram performance using both simulated and real-life data. This adaptation makes the PDC periodogram much more useful, and it can be helpful for the exploration of large time-resolved astronomical databases, from Gaia radial velocity and photometry data releases to those of smaller surveys, such as APOGEE and LAMOST. We have made a public GitHub repository with a Python implementation of the new tools available to the community.","sentences":["We present an improvement of the phase distance correlation (PDC) periodogram to account for uncertainties in the time-series data.","The PDC periodogram, which we introduced in previous papers, is based on the statistical concept of distance correlation.","By viewing each measurement and its accompanying error estimate as a probability distribution, we use the concept of energy distance to design a distance function (metric) between measurement-uncertainty pairs.","We use this metric as the basis for the PDC periodogram, instead of the simple absolute difference.","We demonstrate the periodogram performance using both simulated and real-life data.","This adaptation makes the PDC periodogram much more useful, and it can be helpful for the exploration of large time-resolved astronomical databases, from Gaia radial velocity and photometry data releases to those of smaller surveys, such as APOGEE and LAMOST.","We have made a public GitHub repository with a Python implementation of the new tools available to the community."],"url":"http://arxiv.org/abs/2404.02506v1","category":"astro-ph.IM"}
{"created":"2024-04-03 06:45:48","title":"Nonlinear Corner States in Topologically Nontrivial Kagome Lattice","abstract":"We investigate a higher-order topological insulator (HOTI) under strong nonlinearity, focusing on the existence and stability of high-amplitude corner states, which can find applications in optics, acoustics, elastodynamics, and other wave-based systems. Our study centers on a breathing Kagome lattice composed of point masses and springs known to exhibit edge and corner states in its linear regime. By introducing onsite cubic nonlinearity, we analyze its impact on both edge and corner states. The nonlinear continuation of the corner state unveils stable high-amplitude corner states within the lattice, featuring non-zero displacements at even sites from the corner -- a characteristic absent in the linear limit. Interestingly, the nonlinear continuation of the edge state reveals its transformation into distinct families of high-amplitude corner states via two pitchfork bifurcations. While some states maintain stability, others become unstable through real instability and Neimark-Sacker bifurcation. These unstable corner states dissipate their energy into the edges and the bulk over an extended period, as corroborated by long-time dynamical simulations. Consequently, our study provides insights into achieving significant energy localization at the corners of HOTIs through various classes of nonlinear states.","sentences":["We investigate a higher-order topological insulator (HOTI) under strong nonlinearity, focusing on the existence and stability of high-amplitude corner states, which can find applications in optics, acoustics, elastodynamics, and other wave-based systems.","Our study centers on a breathing Kagome lattice composed of point masses and springs known to exhibit edge and corner states in its linear regime.","By introducing onsite cubic nonlinearity, we analyze its impact on both edge and corner states.","The nonlinear continuation of the corner state unveils stable high-amplitude corner states within the lattice, featuring non-zero displacements at even sites from the corner -- a characteristic absent in the linear limit.","Interestingly, the nonlinear continuation of the edge state reveals its transformation into distinct families of high-amplitude corner states via two pitchfork bifurcations.","While some states maintain stability, others become unstable through real instability and Neimark-Sacker bifurcation.","These unstable corner states dissipate their energy into the edges and the bulk over an extended period, as corroborated by long-time dynamical simulations.","Consequently, our study provides insights into achieving significant energy localization at the corners of HOTIs through various classes of nonlinear states."],"url":"http://arxiv.org/abs/2404.02504v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 06:45:19","title":"Valley-controlled photoswitching of metal-insulator nanotextures","abstract":"Spatial heterogeneity and phase competition are hallmarks of strongly-correlated materials, promising tunable functionality on the nanoscale. Light-induced switching of a correlated insulator to a metallic state is well established. However, optical excitation generally lacks the specificity to select sub-wavelength domains and control final textures. Here, we employ valley-selective photodoping to drive the domain-specific quench of a textured Peierls insulator. Polarized excitation leverages the anisotropy of quasi-one-dimensional states at the correlated gap to initiate an insulator-to-metal transition with minimal electronic heating. We find that averting dissipation facilitates domain-specific carrier confinement, control over nanotextured phases, and a prolonged lifetime of the metastable metallic state. Complementing existing manipulation schemes, valley-selective photoexcitation will enable the activation of electronic phase separation beyond thermodynamic limitations, facilitating optically-controlled hidden states, engineered heterostructures, and polarization-sensitive percolation networks.","sentences":["Spatial heterogeneity and phase competition are hallmarks of strongly-correlated materials, promising tunable functionality on the nanoscale.","Light-induced switching of a correlated insulator to a metallic state is well established.","However, optical excitation generally lacks the specificity to select sub-wavelength domains and control final textures.","Here, we employ valley-selective photodoping to drive the domain-specific quench of a textured Peierls insulator.","Polarized excitation leverages the anisotropy of quasi-one-dimensional states at the correlated gap to initiate an insulator-to-metal transition with minimal electronic heating.","We find that averting dissipation facilitates domain-specific carrier confinement, control over nanotextured phases, and a prolonged lifetime of the metastable metallic state.","Complementing existing manipulation schemes, valley-selective photoexcitation will enable the activation of electronic phase separation beyond thermodynamic limitations, facilitating optically-controlled hidden states, engineered heterostructures, and polarization-sensitive percolation networks."],"url":"http://arxiv.org/abs/2404.02503v1","category":"cond-mat.str-el"}
{"created":"2024-04-03 05:50:42","title":"DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation","abstract":"State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method's performance and to identify promising areas for further improvements.","sentences":["State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot.","However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information.","Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming.","To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain.","Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters.","Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets.","We complement our results with a thorough analysis for more in-depth understanding of the proposed method's performance and to identify promising areas for further improvements."],"url":"http://arxiv.org/abs/2404.02489v1","category":"cs.IR"}
{"created":"2024-04-03 05:13:23","title":"SSwsrNet: A Semi-Supervised Few-Shot Learning Framework for Wireless Signal Recognition","abstract":"Wireless signal recognition (WSR) is crucial in modern and future wireless communication networks since it aims to identify properties of the received signal. Although many deep learning-based WSR models have been developed, they still rely on a large amount of labeled training data. Thus, they cannot tackle the few-sample problem in the practically and dynamically changing wireless communication environment. To overcome this challenge, a novel SSwsrNet framework is proposed by using the deep residual shrinkage network (DRSN) and semi-supervised learning. The DRSN can learn discriminative features from noisy signals. Moreover, a modular semi-supervised learning method that combines labeled and unlabeled data using MixMatch is exploited to further improve the classification performance under few-sample conditions. Extensive simulation results on automatic modulation classification (AMC) and wireless technology classification (WTC) demonstrate that our proposed WSR scheme can achieve better performance than the benchmark schemes in terms of classification accuracy. This novel method enables more robust and adaptive signal recognition for next-generation wireless networks.","sentences":["Wireless signal recognition (WSR) is crucial in modern and future wireless communication networks since it aims to identify properties of the received signal.","Although many deep learning-based WSR models have been developed, they still rely on a large amount of labeled training data.","Thus, they cannot tackle the few-sample problem in the practically and dynamically changing wireless communication environment.","To overcome this challenge, a novel SSwsrNet framework is proposed by using the deep residual shrinkage network (DRSN) and semi-supervised learning.","The DRSN can learn discriminative features from noisy signals.","Moreover, a modular semi-supervised learning method that combines labeled and unlabeled data using MixMatch is exploited to further improve the classification performance under few-sample conditions.","Extensive simulation results on automatic modulation classification (AMC) and wireless technology classification (WTC) demonstrate that our proposed WSR scheme can achieve better performance than the benchmark schemes in terms of classification accuracy.","This novel method enables more robust and adaptive signal recognition for next-generation wireless networks."],"url":"http://arxiv.org/abs/2404.02467v1","category":"eess.SP"}
{"created":"2024-04-03 05:04:06","title":"On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study","abstract":"This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training. One such domain is IoT applications. Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions. More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional supervised deep neural networks (DNNs). We also demonstrate its superior convergence over supervised solutions. Our findings highlight the advantages of vibration-based FMs (and FM-inspired selfsupervised models in general) in terms of inference robustness, runtime efficiency, and model adaptation (via fine-tuning) in resource-limited IoT settings.","sentences":["This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications.","A case study is presented featuring a vehicle classification application using acoustic and seismic sensing.","The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training.","One such domain is IoT applications.","Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data.","The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions.","More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional supervised deep neural networks (DNNs).","We also demonstrate its superior convergence over supervised solutions.","Our findings highlight the advantages of vibration-based FMs (and FM-inspired selfsupervised models in general) in terms of inference robustness, runtime efficiency, and model adaptation (via fine-tuning) in resource-limited IoT settings."],"url":"http://arxiv.org/abs/2404.02461v1","category":"cs.LG"}
{"created":"2024-04-03 05:02:46","title":"A coarse-grained description of anharmonic lattice environments affecting the quantum dynamics of charge carriers","abstract":"Lattice softness has a significant impact on charge carrier dynamics in condensed matter systems, contributing to the emergence of various properties and functions. Examples include the remarkable carrier lifetimes and defect tolerances of hybrid organic-inorganic perovskites. Recent studies suggest the contribution of quartic anharmonicity of the lattice vibrations. The quartic anharmonicity can be discussed with a double-well potential, and the transition between the two minima can be coarse-grained as a two-state jump stochastic process. Such a stochastic approach is typically employed to describe dynamic fluctuations introduced into a system by two-state transitions in the surroundings. To investigate charge transport in materials, however, it is crucial to describe not only the fluctuations but also the dynamic lattice distortion associated with charge transport. Therefore, there is a need for a theory to describe the charge carrier dynamics proceeding alongside the lattice distortion dynamics. In this study, we present a theory that describes quantum dynamics under the influence of an environment with two stable states, termed a bistable environment. The theory describes the effects of fluctuations and dissipation induced from the bistable environment in a reasonable manner, and the effects exhibit a different temperature dependence than the widely employed Gaussian environment. The physical implication of this temperature dependence is provided in terms of the environmental dynamics. The results of this study are expected to provide a step forward in describing charge carrier dynamics in materials with lattice softness and pronounced lattice anharmonicity, e.g., hybrid organic-inorganic perovskites. Moreover, these findings represent an advancement in our understanding of and capacity to predict and control the physical properties and functions of these materials.","sentences":["Lattice softness has a significant impact on charge carrier dynamics in condensed matter systems, contributing to the emergence of various properties and functions.","Examples include the remarkable carrier lifetimes and defect tolerances of hybrid organic-inorganic perovskites.","Recent studies suggest the contribution of quartic anharmonicity of the lattice vibrations.","The quartic anharmonicity can be discussed with a double-well potential, and the transition between the two minima can be coarse-grained as a two-state jump stochastic process.","Such a stochastic approach is typically employed to describe dynamic fluctuations introduced into a system by two-state transitions in the surroundings.","To investigate charge transport in materials, however, it is crucial to describe not only the fluctuations but also the dynamic lattice distortion associated with charge transport.","Therefore, there is a need for a theory to describe the charge carrier dynamics proceeding alongside the lattice distortion dynamics.","In this study, we present a theory that describes quantum dynamics under the influence of an environment with two stable states, termed a bistable environment.","The theory describes the effects of fluctuations and dissipation induced from the bistable environment in a reasonable manner, and the effects exhibit a different temperature dependence than the widely employed Gaussian environment.","The physical implication of this temperature dependence is provided in terms of the environmental dynamics.","The results of this study are expected to provide a step forward in describing charge carrier dynamics in materials with lattice softness and pronounced lattice anharmonicity, e.g., hybrid organic-inorganic perovskites.","Moreover, these findings represent an advancement in our understanding of and capacity to predict and control the physical properties and functions of these materials."],"url":"http://arxiv.org/abs/2404.02459v1","category":"physics.chem-ph"}
{"created":"2024-04-03 05:02:46","title":"TSNet:A Two-stage Network for Image Dehazing with Multi-scale Fusion and Adaptive Learning","abstract":"Image dehazing has been a popular topic of research for a long time. Previous deep learning-based image dehazing methods have failed to achieve satisfactory dehazing effects on both synthetic datasets and real-world datasets, exhibiting poor generalization. Moreover, single-stage networks often result in many regions with artifacts and color distortion in output images. To address these issues, this paper proposes a two-stage image dehazing network called TSNet, mainly consisting of the multi-scale fusion module (MSFM) and the adaptive learning module (ALM). Specifically, MSFM and ALM enhance the generalization of TSNet. The MSFM can obtain large receptive fields at multiple scales and integrate features at different frequencies to reduce the differences between inputs and learning objectives. The ALM can actively learn of regions of interest in images and restore texture details more effectively. Additionally, TSNet is designed as a two-stage network, where the first-stage network performs image dehazing, and the second-stage network is employed to improve issues such as artifacts and color distortion present in the results of the first-stage network. We also change the learning objective from ground truth images to opposite fog maps, which improves the learning efficiency of TSNet. Extensive experiments demonstrate that TSNet exhibits superior dehazing performance on both synthetic and real-world datasets compared to previous state-of-the-art methods.","sentences":["Image dehazing has been a popular topic of research for a long time.","Previous deep learning-based image dehazing methods have failed to achieve satisfactory dehazing effects on both synthetic datasets and real-world datasets, exhibiting poor generalization.","Moreover, single-stage networks often result in many regions with artifacts and color distortion in output images.","To address these issues, this paper proposes a two-stage image dehazing network called TSNet, mainly consisting of the multi-scale fusion module (MSFM) and the adaptive learning module (ALM).","Specifically, MSFM and ALM enhance the generalization of TSNet.","The MSFM can obtain large receptive fields at multiple scales and integrate features at different frequencies to reduce the differences between inputs and learning objectives.","The ALM can actively learn of regions of interest in images and restore texture details more effectively.","Additionally, TSNet is designed as a two-stage network, where the first-stage network performs image dehazing, and the second-stage network is employed to improve issues such as artifacts and color distortion present in the results of the first-stage network.","We also change the learning objective from ground truth images to opposite fog maps, which improves the learning efficiency of TSNet.","Extensive experiments demonstrate that TSNet exhibits superior dehazing performance on both synthetic and real-world datasets compared to previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.02460v1","category":"cs.CV"}
{"created":"2024-04-03 04:40:57","title":"Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations","abstract":"Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language.","sentences":["Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss.","To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language.","In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT).","The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language.","Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning.","Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language."],"url":"http://arxiv.org/abs/2404.02452v1","category":"cs.CL"}
{"created":"2024-04-03 03:27:01","title":"RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation","abstract":"Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair. Moreover, while parameter-efficient LoRA finetuning has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models. To tackle these challenges, we propose to Repair Sparse Vision-Language Models via Sparse Cross-modality Adaptation (RESSA). RESSA utilizes cross-modality finetuning to enhance task-specific performance and facilitate knowledge distillation from original dense models. Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models. Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3\\% improvement under 2:4 sparsity and a remarkable 47.6\\% enhancement under unstructured 70\\% sparsity.","sentences":["Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks.","However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios.","While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs.","To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance.","For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair.","Moreover, while parameter-efficient LoRA finetuning has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models.","To tackle these challenges, we propose to Repair Sparse Vision-Language Models via Sparse Cross-modality Adaptation (RESSA).","RESSA utilizes cross-modality finetuning to enhance task-specific performance and facilitate knowledge distillation from original dense models.","Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models.","Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3\\% improvement under 2:4 sparsity and a remarkable 47.6\\% enhancement under unstructured 70\\% sparsity."],"url":"http://arxiv.org/abs/2404.02424v1","category":"cs.LG"}
{"created":"2024-04-03 02:48:22","title":"Drug-target interaction prediction by integrating heterogeneous information with mutual attention network","abstract":"Identification of drug-target interactions is an indispensable part of drug discovery. While conventional shallow machine learning and recent deep learning methods based on chemogenomic properties of drugs and target proteins have pushed this prediction performance improvement to a new level, these methods are still difficult to adapt to novel structures. Alternatively, large-scale biological and pharmacological data provide new ways to accelerate drug-target interaction prediction. Here, we propose DrugMAN, a deep learning model for predicting drug-target interaction by integrating multiplex heterogeneous functional networks with a mutual attention network (MAN). DrugMAN uses a graph attention network-based integration algorithm to learn network-specific low-dimensional features for drugs and target proteins by integrating four drug networks and seven gene/protein networks, respectively. DrugMAN then captures interaction information between drug and target representations by a mutual attention network to improve drug-target prediction. DrugMAN achieves the best prediction performance under four different scenarios, especially in real-world scenarios. DrugMAN spotlights heterogeneous information to mine drug-target interactions and can be a powerful tool for drug discovery and drug repurposing.","sentences":["Identification of drug-target interactions is an indispensable part of drug discovery.","While conventional shallow machine learning and recent deep learning methods based on chemogenomic properties of drugs and target proteins have pushed this prediction performance improvement to a new level, these methods are still difficult to adapt to novel structures.","Alternatively, large-scale biological and pharmacological data provide new ways to accelerate drug-target interaction prediction.","Here, we propose DrugMAN, a deep learning model for predicting drug-target interaction by integrating multiplex heterogeneous functional networks with a mutual attention network (MAN).","DrugMAN uses a graph attention network-based integration algorithm to learn network-specific low-dimensional features for drugs and target proteins by integrating four drug networks and seven gene/protein networks, respectively.","DrugMAN then captures interaction information between drug and target representations by a mutual attention network to improve drug-target prediction.","DrugMAN achieves the best prediction performance under four different scenarios, especially in real-world scenarios.","DrugMAN spotlights heterogeneous information to mine drug-target interactions and can be a powerful tool for drug discovery and drug repurposing."],"url":"http://arxiv.org/abs/2404.03516v1","category":"q-bio.QM"}
{"created":"2024-04-03 02:27:03","title":"Quantitative Hydrodynamic Stability for Couette Flow on Unbounded Domains with Navier Boundary Conditions","abstract":"We prove a stability threshold theorem for 2D Navier-Stokes on three unbounded domains: the whole plane $\\mathbb{R} \\times \\mathbb{R}$, the half plane $\\mathbb{R} \\times [0,\\infty)$ with Navier boundary conditions, and the infinite channel $\\mathbb{R} \\times [-1, 1]$ with Navier boundary conditions. Starting with the Couette shear flow, we consider initial perturbations $\\omega_{in}$ which are of size $\\nu^{1/2}(1+\\ln(1/\\nu)^{1/2})^{-1}$ in an anisotropic Sobolev space with an additional low frequency control condition for the planar cases. We then demonstrate that such perturbations exhibit inviscid damping of the velocity, as well as enhanced dissipation at $x$-frequencies $|k| \\gg \\nu$ with decay time-scale $O(\\nu^{-1/3}|k|^{-2/3})$. On the plane and half-plane, we show Taylor dispersion for $x$-frequencies $|k| \\ll \\nu$ with decay time-scale $O(\\nu |k|^{-2})$, while on the channel we show low frequency dispersion for $|k| \\ll \\nu$ with decay time-scale $O(\\nu^{-1})$. Generalizing the work of arXiv:2311.00141 done on $\\mathbb{T} \\times [-1,1]$, the key contribution of this paper is to perform new nonlinear computations at low frequencies with wave number $|k| \\lesssim \\nu$ and at intermediate frequencies with wave number $\\nu \\lesssim |k| \\leq 1$, and to provide the first enhanced dissipation result for a fully-nonlinear shear flow on an unbounded $x$-domain.","sentences":["We prove a stability threshold theorem for 2D Navier-Stokes on three unbounded domains: the whole plane $\\mathbb{R} \\times \\mathbb{R}$, the half plane $\\mathbb{R} \\times","[0,\\infty)$ with Navier boundary conditions, and the infinite channel $\\mathbb{R} \\times","[-1, 1]$ with Navier boundary conditions.","Starting with the Couette shear flow, we consider initial perturbations $\\omega_{in}$ which are of size $\\nu^{1/2}(1+\\ln(1/\\nu)^{1/2})^{-1}$ in an anisotropic Sobolev space with an additional low frequency control condition for the planar cases.","We then demonstrate that such perturbations exhibit inviscid damping of the velocity, as well as enhanced dissipation at $x$-frequencies $|k| \\gg \\nu$ with decay time-scale $O(\\nu^{-1/3}|k|^{-2/3})$. On the plane and half-plane, we show Taylor dispersion for $x$-frequencies $|k| \\ll \\nu$ with decay time-scale $O(\\nu |k|^{-2})$, while on the channel we show low frequency dispersion for $|k| \\ll \\nu$ with decay time-scale $O(\\nu^{-1})$. Generalizing the work of arXiv:2311.00141 done on $\\mathbb{T} \\times","[-1,1]$, the key contribution of this paper is to perform new nonlinear computations at low frequencies with wave number $|k| \\lesssim \\nu$ and at intermediate frequencies with wave number $\\nu \\lesssim |k| \\leq 1$, and to provide the first enhanced dissipation result for a fully-nonlinear shear flow on an unbounded $x$-domain."],"url":"http://arxiv.org/abs/2404.02412v1","category":"math.AP"}
{"created":"2024-04-03 02:21:46","title":"CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models","abstract":"Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev","sentences":["Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models.","This could present a significant obstacle for language community members and linguists to use NLP tools.","This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models.","CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data.","We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework.","Code is available at https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev"],"url":"http://arxiv.org/abs/2404.02408v1","category":"cs.CL"}
{"created":"2024-04-03 02:17:34","title":"Decision Transformer as a Foundation Model for Partially Observable Continuous Control","abstract":"Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT's capability to capture the parameter-agnostic structures intrinsic to control tasks. DT exhibits remarkable zero-shot generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data. These findings highlight the potential of DT as a foundational controller for general control applications.","sentences":["Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools.","Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior.","To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture.","Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design.","Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA).","Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT's capability to capture the parameter-agnostic structures intrinsic to control tasks.","DT exhibits remarkable zero-shot generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data.","These findings highlight the potential of DT as a foundational controller for general control applications."],"url":"http://arxiv.org/abs/2404.02407v1","category":"eess.SY"}
{"created":"2024-04-03 02:16:30","title":"TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression","abstract":"In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on hand-crafted components in query-based detectors for temporal action detection (TAD). Despite significant advancements towards an end-to-end framework in object detection, query-based detectors have been limited in achieving full end-to-end modeling in TAD. To address this issue, we propose \\modelname{}, a full end-to-end temporal action detection transformer that integrates time-aligned coordinate expression. We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment. Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set. Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors. Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular benchmark datasets. Code is available at: https://github.com/Dotori-HJ/TE-TAD","sentences":["In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on hand-crafted components in query-based detectors for temporal action detection (TAD).","Despite significant advancements towards an end-to-end framework in object detection, query-based detectors have been limited in achieving full end-to-end modeling in TAD.","To address this issue, we propose \\modelname{}, a full end-to-end temporal action detection transformer that integrates time-aligned coordinate expression.","We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment.","Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set.","Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors.","Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular benchmark datasets.","Code is available at: https://github.com/Dotori-HJ/TE-TAD"],"url":"http://arxiv.org/abs/2404.02405v2","category":"cs.CV"}
{"created":"2024-04-03 01:19:15","title":"Locally homogeneous RCD spaces","abstract":"The goal of this note is to demonstrate how existing results can be adapted to establish the following result: A locally metric measure homogeneous $\\mathrm{RCD}(K,N)$ space is isometric to, after multiplying a positive constant to the reference measure, a smooth Riemannian manifold with the Riemannian volume measure.","sentences":["The goal of this note is to demonstrate how existing results can be adapted to establish the following result: A locally metric measure homogeneous $\\mathrm{RCD}(K,N)$ space is isometric to, after multiplying a positive constant to the reference measure, a smooth Riemannian manifold with the Riemannian volume measure."],"url":"http://arxiv.org/abs/2404.02390v1","category":"math.DG"}
{"created":"2024-04-04 17:57:53","title":"Exploring scalar contributions with $K^+ \\to \u03c0^+ \\ell^+ \\ell^-$","abstract":"The rare kaon decay $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ is an interesting process that offers insights into both Standard Model physics and potential New Physics contributions. While primarily driven by vector interactions in the Standard Model, it also provides a window to explore non-standard contributions. In this letter, we analyse the potential of $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ decays to test the limits on scalar contributions. A simple yet effective analysis using differential decay width and the Forward-Backward Asymmetry is proposed to achieve the state-of-the-art limits on the extent of these scalar contributions. The bounds are obtained for the first time since the E865 experiment through a reinterpretation of the NA48/2 and NA62 experimental results.","sentences":["The rare kaon decay $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ is an interesting process that offers insights into both Standard Model physics and potential New Physics contributions.","While primarily driven by vector interactions in the Standard Model, it also provides a window to explore non-standard contributions.","In this letter, we analyse the potential of $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ decays to test the limits on scalar contributions.","A simple yet effective analysis using differential decay width and the Forward-Backward Asymmetry is proposed to achieve the state-of-the-art limits on the extent of these scalar contributions.","The bounds are obtained for the first time since the E865 experiment through a reinterpretation of the NA48/2 and NA62 experimental results."],"url":"http://arxiv.org/abs/2404.03643v1","category":"hep-ph"}
{"created":"2024-04-04 17:55:51","title":"Representation theory of the reflection equation algebra I: A quantization of Sylvester's law of inertia","abstract":"We prove a version of Sylvester's law of inertia for the Reflection Equation Algebra (=REA). We will only be concerned with the REA constructed from the $R$-matrix associated to the standard $q$-deformation of $GL(N,\\mathbb{C})$. For $q$ positive, this particular REA comes equipped with a natural $*$-structure, by which it can be viewed as a $q$-deformation of the $*$-algebra of polynomial functions on the space of self-adjoint $N$-by-$N$-matrices. We will show that this REA satisfies a type $I$-condition, so that its irreducible representations can in principle be classified. Moreover, we will show that, up to the adjoint action of quantum $GL(N,\\mathbb{C})$, any irreducible representation of the REA is determined by its \\emph{extended signature}, which is a classical signature vector extended by a parameter in $\\mathbb{R}/\\mathbb{Z}$. It is this latter result that we see as a quantized version of Sylvester's law of inertia.","sentences":["We prove a version of Sylvester's law of inertia for the Reflection Equation Algebra (=REA).","We will only be concerned with the REA constructed from the $R$-matrix associated to the standard $q$-deformation of $GL(N,\\mathbb{C})$. For $q$ positive, this particular REA comes equipped with a natural $*$-structure, by which it can be viewed as a $q$-deformation of the $*$-algebra of polynomial functions on the space of self-adjoint $N$-by-$N$-matrices.","We will show that this REA satisfies a type $I$-condition, so that its irreducible representations can in principle be classified.","Moreover, we will show that, up to the adjoint action of quantum $GL(N,\\mathbb{C})$, any irreducible representation of the REA is determined by its \\emph{extended signature}, which is a classical signature vector extended by a parameter in $\\mathbb{R}/\\mathbb{Z}$. It is this latter result that we see as a quantized version of Sylvester's law of inertia."],"url":"http://arxiv.org/abs/2404.03640v1","category":"math.RT"}
{"created":"2024-04-04 17:53:56","title":"Existence and asymptotic behaviour of solutions for a multi-dimensional fractional thin-film equation","abstract":"In this paper, we discuss existence and finite speed of propagation for the solutions to an initial-boundary value problem for a family of fractional thin-film equations in a bounded domain in $\\mathbb{R}^d$. The nonlocal operator we consider is the spectral fractional Laplacian with Neumann boundary conditions. In the case of a \"strong slippage\" regime with \"complete wetting\" interfacial conditions, we prove local entropy estimates that entail finite speed of propagation of the support and a lower bound for the waiting time phenomenon.","sentences":["In this paper, we discuss existence and finite speed of propagation for the solutions to an initial-boundary value problem for a family of fractional thin-film equations in a bounded domain in $\\mathbb{R}^d$. The nonlocal operator we consider is the spectral fractional Laplacian with Neumann boundary conditions.","In the case of a \"strong slippage\" regime with \"complete wetting\" interfacial conditions, we prove local entropy estimates that entail finite speed of propagation of the support and a lower bound for the waiting time phenomenon."],"url":"http://arxiv.org/abs/2404.03633v1","category":"math.AP"}
{"created":"2024-04-04 16:18:26","title":"Multi-mode masers of thermally polarized nuclear spins in solution NMR","abstract":"We present experimental single and multimode sustained 1H NMR masers in solution on thermally polarized spins at room temperature and 9.4 T achieved through the electronic control of radiation feedback (radiation damping). Our observations illustrate the breakdown of the usual three-dimensional Maxwell-Bloch equations for radiation feedback and a simple toy model of few coupled classical moments is used to interpret these experiments.","sentences":["We present experimental single and multimode sustained 1H NMR masers in solution on thermally polarized spins at room temperature and 9.4 T achieved through the electronic control of radiation feedback (radiation damping).","Our observations illustrate the breakdown of the usual three-dimensional Maxwell-Bloch equations for radiation feedback and a simple toy model of few coupled classical moments is used to interpret these experiments."],"url":"http://arxiv.org/abs/2404.03562v1","category":"physics.chem-ph"}
{"created":"2024-04-04 15:31:21","title":"BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering","abstract":"Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text.","sentences":["Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner.","Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language.","Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text.","We utilize multilingual LLMs to understand various languages and correlate entities and relations universally.","By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG.","To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters.","Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG.","Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text."],"url":"http://arxiv.org/abs/2404.03528v1","category":"cs.CL"}
{"created":"2024-04-04 13:56:38","title":"Schr\u00f6dinger equation in dimension two with competing logarithmic self-interaction","abstract":"In this paper we study the equation \\[ -\\Delta u +(\\log |\\cdot|*|u|^2)u=(\\log|\\cdot|*|u|^q)|u|^{q-2}u, \\qquad \\hbox{ in }\\mathbb{R}^2, \\] where $8/3 < q < 4$. By means of variational arguments, we find infinitely many radially symmetric classical solutions. The main difficulties rely on the competition between the two nonlocal terms and on the presence of logarithmic kernels, which have not a prescribed sign. In addition, in order to find finite energy solutions, a suitable functional setting analysis is required.","sentences":["In this paper we study the equation \\[ -\\Delta u +(\\log |\\cdot|*|u|^2)u=(\\log|\\cdot|*|u|^q)|u|^{q-2}u, \\qquad \\hbox{ in }\\mathbb{R}^2, \\] where $8/3 < q < 4$. By means of variational arguments, we find infinitely many radially symmetric classical solutions.","The main difficulties rely on the competition between the two nonlocal terms and on the presence of logarithmic kernels, which have not a prescribed sign.","In addition, in order to find finite energy solutions, a suitable functional setting analysis is required."],"url":"http://arxiv.org/abs/2404.03452v1","category":"math.AP"}
{"created":"2024-04-04 13:06:19","title":"Information entropy for central $^{197}$Au+$^{197}$Au collisions in the UrQMD model","abstract":"Multiplicity information entropy in central $^{197}$Au + $^{197}$Au collisions at impact parameters of 0$-$3 fm are calculated at various center of mass energies ($\\sqrt{s_{\\rm NN}}$) of 5.0, 7.7, 11.5, 14.5, 19.6, 27.0, 32.0, 35.0, 39.0, and 54.4 GeV using the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD). The simulations in UrQMD model are compared with hydro modes with three different equations of state (EoS) and also with a default mode without hydrodynamics. The study reveals that the information entropies of baryons, net-baryons and net-protons with different equations of state in the hydro modes exhibit first decreases and then slowly increases with the increase of $\\sqrt{s_{\\rm NN}}$, while those of hadrons and anti-hadrons, antibaryons show a monotonous increase with $\\sqrt{s_{\\rm NN}}$. An enhancement is found around $\\sqrt{s_{\\rm NN}}$ $\\sim$ 30 GeV potentially corresponding to the critical endpoint with chiral hadron gas EoS and Bag model EoS.","sentences":["Multiplicity information entropy in central $^{197}$Au + $^{197}$Au collisions at impact parameters of 0$-$3 fm are calculated at various center of mass energies ($\\sqrt{s_{\\rm NN}}$) of 5.0, 7.7, 11.5, 14.5, 19.6, 27.0, 32.0, 35.0, 39.0, and 54.4 GeV using the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD).","The simulations in UrQMD model are compared with hydro modes with three different equations of state (EoS) and also with a default mode without hydrodynamics.","The study reveals that the information entropies of baryons, net-baryons and net-protons with different equations of state in the hydro modes exhibit first decreases and then slowly increases with the increase of $\\sqrt{s_{\\rm NN}}$, while those of hadrons and anti-hadrons, antibaryons show a monotonous increase with $\\sqrt{s_{\\rm NN}}$. An enhancement is found around $\\sqrt{s_{\\rm NN}}$ $\\sim$ 30 GeV potentially corresponding to the critical endpoint with chiral hadron gas EoS and Bag model EoS."],"url":"http://arxiv.org/abs/2404.03424v1","category":"nucl-th"}
{"created":"2024-04-04 11:04:21","title":"Exploring Universe acceleration through observational constraints via Hubble parameter reconstruction","abstract":"In this article, we introduce an innovative parametric representation of the Hubble parameter, providing a model-independent means to explore the dynamics of an accelerating cosmos. The model's parameters are rigorously constrained through a Markov Chain Monte Carlo (MCMC) approach, leveraging a comprehensive dataset consisting of 31 data points from cosmic chronometers (CC), 1701 updated observations of Pantheon supernovae type Ia (SNeIa), and 6 data points from baryonic acoustic oscillations (BAO). Our analysis delves into the behavior of various cosmological parameters within the model, including the transition from a decelerating phase to an accelerating one, as well as the density parameters and the equation of state (EoS) parameter. The outcomes of our investigation reveal that the equation of state parameter aligns with characteristics reminiscent of the phantom model, supporting the prevailing understanding of our universe's current state of acceleration. This research contributes valuable insights into the ongoing cosmic expansion and underscores the utility of our novel parametric approach.","sentences":["In this article, we introduce an innovative parametric representation of the Hubble parameter, providing a model-independent means to explore the dynamics of an accelerating cosmos.","The model's parameters are rigorously constrained through a Markov Chain Monte Carlo (MCMC) approach, leveraging a comprehensive dataset consisting of 31 data points from cosmic chronometers (CC), 1701 updated observations of Pantheon supernovae type Ia (SNeIa), and 6 data points from baryonic acoustic oscillations (BAO).","Our analysis delves into the behavior of various cosmological parameters within the model, including the transition from a decelerating phase to an accelerating one, as well as the density parameters and the equation of state (EoS) parameter.","The outcomes of our investigation reveal that the equation of state parameter aligns with characteristics reminiscent of the phantom model, supporting the prevailing understanding of our universe's current state of acceleration.","This research contributes valuable insights into the ongoing cosmic expansion and underscores the utility of our novel parametric approach."],"url":"http://arxiv.org/abs/2404.03362v1","category":"astro-ph.CO"}
{"created":"2024-04-04 10:57:42","title":"Geometrisation of Fermions in Flat Spacetimes","abstract":"Requiring physical consistency in a classical flat spacetime geometrisation of fermions is shown to suggest the introduction of torsion. A resulting simple model for that torsion produces a localised quantum-like particle as a solution of a spinor identity that closely resembles the Dirac equation of quantum electrodynamics. All relevant integrals involving solutions of the spinor identity converge and require that the spin, which is no longer intrinsic but arises from circulating currents, be 1/2 whilst the magnetic moment takes the quantum value of 1 magneton, characteristic of an isolated Dirac particle. The underlying torsion associates the otherwise strictly localised particle with an extended spacetime structure that may be relevant to wider quantum phenomena.","sentences":["Requiring physical consistency in a classical flat spacetime geometrisation of fermions is shown to suggest the introduction of torsion.","A resulting simple model for that torsion produces a localised quantum-like particle as a solution of a spinor identity that closely resembles the Dirac equation of quantum electrodynamics.","All relevant integrals involving solutions of the spinor identity converge and require that the spin, which is no longer intrinsic but arises from circulating currents, be 1/2 whilst the magnetic moment takes the quantum value of 1 magneton, characteristic of an isolated Dirac particle.","The underlying torsion associates the otherwise strictly localised particle with an extended spacetime structure that may be relevant to wider quantum phenomena."],"url":"http://arxiv.org/abs/2404.03360v1","category":"hep-th"}
{"created":"2024-04-04 10:30:28","title":"VF-NeRF: Viewshed Fields for Rigid NeRF Registration","abstract":"3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese.","sentences":["3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes.","This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF).","In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given.","Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras.","We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese."],"url":"http://arxiv.org/abs/2404.03349v1","category":"cs.CV"}
{"created":"2024-04-04 10:12:34","title":"Defect of irreducible plane curves with simple singularities","abstract":"In this note we focus on the defect of singular plane curve that was recently introduced by Dimca. Roughly speaking, the defect of a reduced plane curve measures the discrepancy from the property of being a free curve. We find some lower-bound on the defect for certain classes of irreducible plane curves admitting nodes, ordinary cusps and ordinary triple points. The main result of the note tells us that reduced simply singular plane curves with sufficiently high Arnold exponents are never free.","sentences":["In this note we focus on the defect of singular plane curve that was recently introduced by Dimca.","Roughly speaking, the defect of a reduced plane curve measures the discrepancy from the property of being a free curve.","We find some lower-bound on the defect for certain classes of irreducible plane curves admitting nodes, ordinary cusps and ordinary triple points.","The main result of the note tells us that reduced simply singular plane curves with sufficiently high Arnold exponents are never free."],"url":"http://arxiv.org/abs/2404.03341v1","category":"math.AG"}
{"created":"2024-04-04 10:08:52","title":"Approximation of Some Nonlinear Fractional Order BVPs by Weighted Residual Methods","abstract":"To extract the approximate solutions in the case of nonlinear fractional order differential equations with the homogeneous and nonhomogeneous boundary conditions, the weighted residual method is embedded here. We exploit three methods such as Galerkin, Least Square, and Collocation for the efficient numerical solution of nonlinear two-point boundary value problems. Some nonlinear cases are examined for observing the maximum absolute errors by the considered methods, demonstrating the accuracy and reliability of the present technique using the modified Legendre and modified Bernoulli polynomials as weight functions. The mathematical formulations and computational algorithms are more straightforward and uncomplicated to understand. Absolute errors and the graphical representation reflect that our method is more accurate and reliable.","sentences":["To extract the approximate solutions in the case of nonlinear fractional order differential equations with the homogeneous and nonhomogeneous boundary conditions, the weighted residual method is embedded here.","We exploit three methods such as Galerkin, Least Square, and Collocation for the efficient numerical solution of nonlinear two-point boundary value problems.","Some nonlinear cases are examined for observing the maximum absolute errors by the considered methods, demonstrating the accuracy and reliability of the present technique using the modified Legendre and modified Bernoulli polynomials as weight functions.","The mathematical formulations and computational algorithms are more straightforward and uncomplicated to understand.","Absolute errors and the graphical representation reflect that our method is more accurate and reliable."],"url":"http://arxiv.org/abs/2404.03338v1","category":"math.NA"}
{"created":"2024-04-04 09:34:41","title":"Cartan Flat Non-degenerate CR Lie Groups","abstract":"In this paper we determine all the simply connected non-degenerate CR Lie groups, which are flat with respect to the Cartan connection: in terms of associated Lie algebras, we assert that the only Cartan flat non-degenerate CR Lie algebras are $\\mathfrak{su}(2)$, $\\mathfrak{sl}(2,\\mathbb{R})$, $\\mathfrak{aff}(\\mathbb{R}) \\oplus \\mathbb{R}$, and $\\mathfrak{h}_{2m+1}$ with its modifications, where $\\mathfrak{aff}(\\mathbb{R})$ is the affine Lie algebra of dimension 2 and $\\mathfrak{h}_{2m+1}$ is the Heisenberg Lie algebra of dimension $2m+1$. Furthermore, we determine all the (flat and non-flat) non-degenerate CR structures on each of these Lie groups.","sentences":["In this paper we determine all the simply connected non-degenerate CR Lie groups, which are flat with respect to the Cartan connection: in terms of associated Lie algebras, we assert that the only Cartan flat non-degenerate CR Lie algebras are $\\mathfrak{su}(2)$, $\\mathfrak{sl}(2,\\mathbb{R})$, $\\mathfrak{aff}(\\mathbb{R})","\\oplus \\mathbb{R}$, and $\\mathfrak{h}_{2m+1}$ with its modifications, where $\\mathfrak{aff}(\\mathbb{R})$ is the affine Lie algebra of dimension 2 and $\\mathfrak{h}_{2m+1}$ is the Heisenberg Lie algebra of dimension $2m+1$. Furthermore, we determine all the (flat and non-flat) non-degenerate CR structures on each of these Lie groups."],"url":"http://arxiv.org/abs/2404.03318v1","category":"math.DG"}
{"created":"2024-04-04 07:45:18","title":"Riemannian L-systems: Modeling growing forms in curved spaces","abstract":"In the past 50 years, the formalism of L-systems has been successfully used and developed to model the growth of filamentous and branching biological forms. These simulations take place in classical 2-D or 3-D Euclidean spaces. However, various biological forms actually grow in curved, non-Euclidean, spaces. This is for example the case of vein networks growing within curved leaf blades, of unicellular filaments, such as pollen tubes, growing on curved surfaces to fertilize distant ovules, of teeth patterns growing on folded epithelia of animals, of diffusion of chemical or mechanical signals at the surface of plant or animal tissues, etc. To model these forms growing in curved spaces, we thus extended the formalism of L-systems to non-Euclidean spaces. In a first step we show that this extension can be carried out by integrating concepts of differential geometry in the notion of turtle geometry. We then illustrate how this extension can be applied to model and program the development of both mathematical and biological forms on curved surfaces embedded in our Euclidean space. We provide various examples applied to plant development. We finally show that this approach can be extended to more abstract spaces, called abstract Riemannian spaces, that are not embedded into any higher space, while being intrinsically curved. We suggest that this abstract extension can be used to provide a new approach for effective modeling of tropism phenomena and illustrate this idea on a few conceptual examples.","sentences":["In the past 50 years, the formalism of L-systems has been successfully used and developed to model the growth of filamentous and branching biological forms.","These simulations take place in classical 2-D or 3-D Euclidean spaces.","However, various biological forms actually grow in curved, non-Euclidean, spaces.","This is for example the case of vein networks growing within curved leaf blades, of unicellular filaments, such as pollen tubes, growing on curved surfaces to fertilize distant ovules, of teeth patterns growing on folded epithelia of animals, of diffusion of chemical or mechanical signals at the surface of plant or animal tissues, etc.","To model these forms growing in curved spaces, we thus extended the formalism of L-systems to non-Euclidean spaces.","In a first step we show that this extension can be carried out by integrating concepts of differential geometry in the notion of turtle geometry.","We then illustrate how this extension can be applied to model and program the development of both mathematical and biological forms on curved surfaces embedded in our Euclidean space.","We provide various examples applied to plant development.","We finally show that this approach can be extended to more abstract spaces, called abstract Riemannian spaces, that are not embedded into any higher space, while being intrinsically curved.","We suggest that this abstract extension can be used to provide a new approach for effective modeling of tropism phenomena and illustrate this idea on a few conceptual examples."],"url":"http://arxiv.org/abs/2404.03270v1","category":"q-bio.QM"}
{"created":"2024-04-04 07:42:47","title":"To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process","abstract":"Incorporating Search and Recommendation (S&R) services within a singular application is prevalent in online platforms, leading to a new task termed open-app motivation prediction, which aims to predict whether users initiate the application with the specific intent of information searching, or to explore recommended content for entertainment. Studies have shown that predicting users' motivation to open an app can help to improve user engagement and enhance performance in various downstream tasks. However, accurately predicting open-app motivation is not trivial, as it is influenced by user-specific factors, search queries, clicked items, as well as their temporal occurrences. Furthermore, these activities occur sequentially and exhibit intricate temporal dependencies. Inspired by the success of the Neural Hawkes Process (NHP) in modeling temporal dependencies in sequences, this paper proposes a novel neural Hawkes process model to capture the temporal dependencies between historical user browsing and querying actions. The model, referred to as Neural Hawkes Process-based Open-App Motivation prediction model (NHP-OAM), employs a hierarchical transformer and a novel intensity function to encode multiple factors, and open-app motivation prediction layer to integrate time and user-specific information for predicting users' open-app motivations. To demonstrate the superiority of our NHP-OAM model and construct a benchmark for the Open-App Motivation Prediction task, we not only extend the public S&R dataset ZhihuRec but also construct a new real-world Open-App Motivation Dataset (OAMD). Experiments on these two datasets validate NHP-OAM's superiority over baseline models. Further downstream application experiments demonstrate NHP-OAM's effectiveness in predicting users' Open-App Motivation, highlighting the immense application value of NHP-OAM.","sentences":["Incorporating Search and Recommendation (S&R) services within a singular application is prevalent in online platforms, leading to a new task termed open-app motivation prediction, which aims to predict whether users initiate the application with the specific intent of information searching, or to explore recommended content for entertainment.","Studies have shown that predicting users' motivation to open an app can help to improve user engagement and enhance performance in various downstream tasks.","However, accurately predicting open-app motivation is not trivial, as it is influenced by user-specific factors, search queries, clicked items, as well as their temporal occurrences.","Furthermore, these activities occur sequentially and exhibit intricate temporal dependencies.","Inspired by the success of the Neural Hawkes Process (NHP) in modeling temporal dependencies in sequences, this paper proposes a novel neural Hawkes process model to capture the temporal dependencies between historical user browsing and querying actions.","The model, referred to as Neural Hawkes Process-based Open-App Motivation prediction model (NHP-OAM), employs a hierarchical transformer and a novel intensity function to encode multiple factors, and open-app motivation prediction layer to integrate time and user-specific information for predicting users' open-app motivations.","To demonstrate the superiority of our NHP-OAM model and construct a benchmark for the Open-App Motivation Prediction task, we not only extend the public S&R dataset ZhihuRec but also construct a new real-world Open-App Motivation Dataset (OAMD).","Experiments on these two datasets validate NHP-OAM's superiority over baseline models.","Further downstream application experiments demonstrate NHP-OAM's effectiveness in predicting users' Open-App Motivation, highlighting the immense application value of NHP-OAM."],"url":"http://arxiv.org/abs/2404.03267v1","category":"cs.IR"}
{"created":"2024-04-04 07:42:02","title":"On the derivation of the linear Boltzmann equation from the nonideal Rayleigh gas","abstract":"This paper's objective is to improve the existing proof of the derivation of the Rayleigh--Boltzmann equation from the nonideal Rayleigh gas [6], yielding a far faster convergence rate. This equation is a linear version of the Boltzmann equation, describing the behavior of a small fraction of tagged particles having been perturbed from thermodynamic equilibrium. This linear equation, derived from the microscopic Newton laws as suggested by the Hilbert's sixth problem, is much better understood than the quadratic Boltzmann equation, and even enable results on long time scales for the kinetic description of gas dynamics.The present paper improves the physically poor convergence rate that had been previously proved, into a much more satisfactory rate which is more than exponentially better.","sentences":["This paper's objective is to improve the existing proof of the derivation of the Rayleigh--Boltzmann equation from the nonideal Rayleigh gas [6], yielding a far faster convergence rate.","This equation is a linear version of the Boltzmann equation, describing the behavior of a small fraction of tagged particles having been perturbed from thermodynamic equilibrium.","This linear equation, derived from the microscopic Newton laws as suggested by the Hilbert's sixth problem, is much better understood than the quadratic Boltzmann equation, and even enable results on long time scales for the kinetic description of gas dynamics.","The present paper improves the physically poor convergence rate that had been previously proved, into a much more satisfactory rate which is more than exponentially better."],"url":"http://arxiv.org/abs/2404.03266v1","category":"math.AP"}
{"created":"2024-04-04 07:32:22","title":"Interaction-induced nonlinear magnon transport in noncentrosymmetric ferromagnets","abstract":"We study the effect of the magnon-magnon interaction on the nonlinear magnon transport. The magnon-magnon interaction induces nonreciprocal magnon decay when the time-reversal symmetry is broken, and leads to nonlinear thermal responses of magnons. We construct a theoretical framework to study the nonlinear thermal responses due to the nonreciprocal magnon decay by using the imaginary Dyson equation and quantum kinetic theory, which is then applied to a model of honeycomb ferromagnets with Dzyaloshinskii-Moriya interactions. An order estimate shows that the nonlinear thermal response from the present mechanism is feasible for experimental measurement.","sentences":["We study the effect of the magnon-magnon interaction on the nonlinear magnon transport.","The magnon-magnon interaction induces nonreciprocal magnon decay when the time-reversal symmetry is broken, and leads to nonlinear thermal responses of magnons.","We construct a theoretical framework to study the nonlinear thermal responses due to the nonreciprocal magnon decay by using the imaginary Dyson equation and quantum kinetic theory, which is then applied to a model of honeycomb ferromagnets with Dzyaloshinskii-Moriya interactions.","An order estimate shows that the nonlinear thermal response from the present mechanism is feasible for experimental measurement."],"url":"http://arxiv.org/abs/2404.03260v1","category":"cond-mat.str-el"}
{"created":"2024-04-04 06:17:03","title":"Blow up analysis for a parabolic MEMS problem, I: H\u00f6lder estimate","abstract":"This is the first in a series of papers devoted to the blow up analysis for the quenching phenomena in a parabolic MEMS equation. In this paper, we first give an optimal H\\\"{o}lder estimate for solutions to this equation by using the blow up method and some Liouville theorems on stationary two-valued caloric functions, and then establish a convergence theory for sequences of uniformly H\\\"{o}lder continuous solutions. These results are also used to prove a stratification theorem on the rupture set $\\{u=0\\}$.","sentences":["This is the first in a series of papers devoted to the blow up analysis for the quenching phenomena in a parabolic MEMS equation.","In this paper, we first give an optimal","H\\\"{o}lder estimate for solutions to this equation by using the blow up method and some Liouville theorems on stationary two-valued caloric functions, and then establish a convergence theory for sequences of uniformly H\\\"{o}lder continuous solutions.","These results are also used to prove a stratification theorem on the rupture set $\\{u=0\\}$."],"url":"http://arxiv.org/abs/2404.03223v1","category":"math.AP"}
{"created":"2024-04-04 05:38:30","title":"Neutrino mean free path in neutron stars in the presence of hyperons","abstract":"We investigate the neutrino elastic differential cross-section (NDCS) and corresponding mean free path for neutral current scattering in the dense matter of a neutron star. A wide range of observed neutron star (NS) masses is considered, including the presence of $\\Lambda$, $\\Xi^{-}$, and $\\Xi^{0}$ hyperons in the heaviest stars. Their presence significantly decreases the total neutrino mean free path in the heavier stars.","sentences":["We investigate the neutrino elastic differential cross-section (NDCS) and corresponding mean free path for neutral current scattering in the dense matter of a neutron star.","A wide range of observed neutron star (NS) masses is considered, including the presence of $\\Lambda$, $\\Xi^{-}$, and $\\Xi^{0}$ hyperons in the heaviest stars.","Their presence significantly decreases the total neutrino mean free path in the heavier stars."],"url":"http://arxiv.org/abs/2404.03213v1","category":"nucl-th"}
{"created":"2024-04-04 04:12:30","title":"AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales","abstract":"We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and dataset will be made publicly available.","sentences":["We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps.","AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view.","To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design.","The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching.","The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data.","Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps.","This significantly improves real-world applicability in scenarios with unknown map scales.","To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment.","The code and dataset will be made publicly available."],"url":"http://arxiv.org/abs/2404.03187v1","category":"cs.CV"}
{"created":"2024-04-04 03:45:17","title":"BodyMAP -- Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed","abstract":"Accurately predicting the 3D human posture and the pressure exerted on the body for people resting in bed, visualized as a body mesh (3D pose & shape) with a 3D pressure map, holds significant promise for healthcare applications, particularly, in the prevention of pressure ulcers. Current methods focus on singular facets of the problem -- predicting only 2D/3D poses, generating 2D pressure images, predicting pressure only for certain body regions instead of the full body, or forming indirect approximations to the 3D pressure map. In contrast, we introduce BodyMAP, which jointly predicts the human body mesh and 3D applied pressure map across the entire human body. Our network leverages multiple visual modalities, incorporating both a depth image of a person in bed and its corresponding 2D pressure image acquired from a pressure-sensing mattress. The 3D pressure map is represented as a pressure value at each mesh vertex and thus allows for precise localization of high-pressure regions on the body. Additionally, we present BodyMAP-WS, a new formulation of pressure prediction in which we implicitly learn pressure in 3D by aligning sensed 2D pressure images with a differentiable 2D projection of the predicted 3D pressure maps. In evaluations with real-world human data, our method outperforms the current state-of-the-art technique by 25% on both body mesh and 3D applied pressure map prediction tasks for people in bed.","sentences":["Accurately predicting the 3D human posture and the pressure exerted on the body for people resting in bed, visualized as a body mesh (3D pose & shape) with a 3D pressure map, holds significant promise for healthcare applications, particularly, in the prevention of pressure ulcers.","Current methods focus on singular facets of the problem -- predicting only 2D/3D poses, generating 2D pressure images, predicting pressure only for certain body regions instead of the full body, or forming indirect approximations to the 3D pressure map.","In contrast, we introduce BodyMAP, which jointly predicts the human body mesh and 3D applied pressure map across the entire human body.","Our network leverages multiple visual modalities, incorporating both a depth image of a person in bed and its corresponding 2D pressure image acquired from a pressure-sensing mattress.","The 3D pressure map is represented as a pressure value at each mesh vertex and thus allows for precise localization of high-pressure regions on the body.","Additionally, we present BodyMAP-WS, a new formulation of pressure prediction in which we implicitly learn pressure in 3D by aligning sensed 2D pressure images with a differentiable 2D projection of the predicted 3D pressure maps.","In evaluations with real-world human data, our method outperforms the current state-of-the-art technique by 25% on both body mesh and 3D applied pressure map prediction tasks for people in bed."],"url":"http://arxiv.org/abs/2404.03183v1","category":"cs.CV"}
{"created":"2024-04-04 03:17:01","title":"A comparison between the deflection angles of massive and massless particles in the Shchwarzschild space-time and their consequences on black hole shadows","abstract":"We present comparisons of the deflection angles of massless and massive particles in the Schwarzschild space-time. For the case of photons in a general static space-time, we construct a spatial 3D equation of motion for their path that leads to an implicit formula for the deflection angle. We then compare our results with well known results of the literature in the Schwarzschild space-time. For the case of massive particles we calculate the deflection angle only in the Schwarzschild space-time. The end result is that as the velocity of any massive particle diminishes, the deflection angle increases. To show the relevance of these comparisons, we constructed different black hole shadows for massive particles in order to be compared with a shadow made by of photons.","sentences":["We present comparisons of the deflection angles of massless and massive particles in the Schwarzschild space-time.","For the case of photons in a general static space-time, we construct a spatial 3D equation of motion for their path that leads to an implicit formula for the deflection angle.","We then compare our results with well known results of the literature in the Schwarzschild space-time.","For the case of massive particles we calculate the deflection angle only in the Schwarzschild space-time.","The end result is that as the velocity of any massive particle diminishes, the deflection angle increases.","To show the relevance of these comparisons, we constructed different black hole shadows for massive particles in order to be compared with a shadow made by of photons."],"url":"http://arxiv.org/abs/2404.03174v1","category":"gr-qc"}
{"created":"2024-04-04 01:39:01","title":"DreamWalk: Style Space Exploration using Diffusion Guidance","abstract":"Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform \"prompt engineering,\" constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \\emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model's neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: https://mshu1.github.io/dreamwalk.github.io/","sentences":["Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control.","Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform \"prompt engineering,\" constructing special text sentences to control the style or amount of a particular subject present in the output image.","Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1).","Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process.","We introduce guidance scale functions to control when in the diffusion process and \\emph{where} in the image to intervene.","Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model's neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2).","Project page: https://mshu1.github.io/dreamwalk.github.io/"],"url":"http://arxiv.org/abs/2404.03145v1","category":"cs.CV"}
{"created":"2024-04-04 01:24:27","title":"Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., sidelining authors of lowly-cited papers when predicting paper topics in citation networks. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias.","sentences":["Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks.","This degree bias can reinforce social marginalization by, e.g., sidelining authors of lowly-cited papers when predicting paper topics in citation networks.","While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory.","Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters.","We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained.","Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors).","Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power.","Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others.","We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias."],"url":"http://arxiv.org/abs/2404.03139v1","category":"cs.LG"}
{"created":"2024-04-04 00:50:39","title":"Preventing mass loss in the standard level set method: New insights from variational analyses","abstract":"For decades, the computational multiphase flow community has grappled with mass loss in the level set method. Numerous solutions have been proposed, from fixing the reinitialization step to combining the level set method with other conservative schemes. However, our work reveals a more fundamental culprit: the smooth Heaviside and delta functions inherent to the standard formulation. Even if reinitialization is done exactly, i.e., the zero contour interface remains stationary, the use of smooth functions lead to violation of mass conservation. We propose a novel approach using variational analysis to incorporate a mass conservation constraint. This introduces a Lagrange multiplier that enforces overall mass balance. Notably, as the delta function sharpens, i.e., approaches the Dirac delta limit, the Lagrange multiplier approaches zero. However, the exact Lagrange multiplier method disrupts the signed distance property of the level set function. This motivates us to develop an approximate version of the Lagrange multiplier that preserves both overall mass and signed distance property of the level set function. Our framework even recovers existing mass-conserving level set methods, revealing some inconsistencies in prior analyses. We extend this approach to three-phase flows for fluid-structure interaction (FSI) simulations. We present variational equations in both immersed and non-immersed forms, demonstrating the convergence of the former formulation to the latter when the body delta function sharpens. Rigorous test problems confirm that the FSI dynamics produced by our simple, easy-to-implement immersed formulation with the approximate Lagrange multiplier method are accurate and match state-of-the-art solvers.","sentences":["For decades, the computational multiphase flow community has grappled with mass loss in the level set method.","Numerous solutions have been proposed, from fixing the reinitialization step to combining the level set method with other conservative schemes.","However, our work reveals a more fundamental culprit: the smooth Heaviside and delta functions inherent to the standard formulation.","Even if reinitialization is done exactly, i.e., the zero contour interface remains stationary, the use of smooth functions lead to violation of mass conservation.","We propose a novel approach using variational analysis to incorporate a mass conservation constraint.","This introduces a Lagrange multiplier that enforces overall mass balance.","Notably, as the delta function sharpens, i.e., approaches the Dirac delta limit, the Lagrange multiplier approaches zero.","However, the exact Lagrange multiplier method disrupts the signed distance property of the level set function.","This motivates us to develop an approximate version of the Lagrange multiplier that preserves both overall mass and signed distance property of the level set function.","Our framework even recovers existing mass-conserving level set methods, revealing some inconsistencies in prior analyses.","We extend this approach to three-phase flows for fluid-structure interaction (FSI) simulations.","We present variational equations in both immersed and non-immersed forms, demonstrating the convergence of the former formulation to the latter when the body delta function sharpens.","Rigorous test problems confirm that the FSI dynamics produced by our simple, easy-to-implement immersed formulation with the approximate Lagrange multiplier method are accurate and match state-of-the-art solvers."],"url":"http://arxiv.org/abs/2404.03132v1","category":"physics.flu-dyn"}
{"created":"2024-04-03 22:18:49","title":"Analyzing Warp Drive Spacetimes with Warp Factory","abstract":"The field of warp research has been dominated by analytical methods to investigate potential solutions. However, these approaches often favor simple metric forms that facilitate analysis but ultimately limit the range of exploration of novel solutions. So far the proposed solutions have been unphysical, requiring energy condition violations and large energy requirements. To overcome the analytical limitations in warp research, we introduce Warp Factory: a numerical toolkit designed for modeling warp drive spacetimes. By leveraging numerical analysis, Warp Factory enables the examination of general warp drive geometries by evaluating the Einstein field equations and computing energy conditions. Furthermore, this comprehensive toolkit provides the determination of metric scalars and insightful visualizations in both 2D and 3D, offering a deeper understanding of metrics and their corresponding stress-energy tensors. The paper delves into the methodology employed by Warp Factory in evaluating the physicality of warp drive spacetimes and highlights its application in assessing commonly modeled warp drive metrics. By leveraging the capabilities of Warp Factory, we aim to further warp drive research and hopefully bring us closer to realizing physically achievable warp drives.","sentences":["The field of warp research has been dominated by analytical methods to investigate potential solutions.","However, these approaches often favor simple metric forms that facilitate analysis but ultimately limit the range of exploration of novel solutions.","So far the proposed solutions have been unphysical, requiring energy condition violations and large energy requirements.","To overcome the analytical limitations in warp research, we introduce Warp Factory: a numerical toolkit designed for modeling warp drive spacetimes.","By leveraging numerical analysis, Warp Factory enables the examination of general warp drive geometries by evaluating the Einstein field equations and computing energy conditions.","Furthermore, this comprehensive toolkit provides the determination of metric scalars and insightful visualizations in both 2D and 3D, offering a deeper understanding of metrics and their corresponding stress-energy tensors.","The paper delves into the methodology employed by Warp Factory in evaluating the physicality of warp drive spacetimes and highlights its application in assessing commonly modeled warp drive metrics.","By leveraging the capabilities of Warp Factory, we aim to further warp drive research and hopefully bring us closer to realizing physically achievable warp drives."],"url":"http://arxiv.org/abs/2404.03095v1","category":"gr-qc"}
{"created":"2024-04-03 21:51:33","title":"Implantable silicon neural probes with nanophotonic phased arrays for single-lobe beam steering","abstract":"In brain activity mapping experiments using optogenetics, patterned illumination is crucial for deterministic and localized stimulation of neurons. However, due to optical scattering in brain tissue, light-emitting implantable devices are needed to bring precise patterned illumination to deep brain regions. A promising solution is silicon neural probes with integrated nanophotonic circuits that form tailored beam emission patterns without lenses. Here, we demonstrate neural probes with grating-based light emitters that generate a single steerable light beam across $> 60\\%$ of the steering range with $\\ge 4$ dB of background suppression for optogenetic photostimulation. The light emitters, optimized for blue or amber light, combine end-fire optical phased arrays with slab gratings to suppress higher-order sidelobes.","sentences":["In brain activity mapping experiments using optogenetics, patterned illumination is crucial for deterministic and localized stimulation of neurons.","However, due to optical scattering in brain tissue, light-emitting implantable devices are needed to bring precise patterned illumination to deep brain regions.","A promising solution is silicon neural probes with integrated nanophotonic circuits that form tailored beam emission patterns without lenses.","Here, we demonstrate neural probes with grating-based light emitters that generate a single steerable light beam across $> 60\\%$ of the steering range with $\\ge 4$ dB of background suppression for optogenetic photostimulation.","The light emitters, optimized for blue or amber light, combine end-fire optical phased arrays with slab gratings to suppress higher-order sidelobes."],"url":"http://arxiv.org/abs/2404.03083v1","category":"physics.optics"}
{"created":"2024-04-03 21:17:25","title":"From equations in coordinate space to Picard-Fuchs and back","abstract":"We continue the development of a position space approach to equations for Feynman multi-loop integrals. The key idea of the approach is that unintegrated products of Greens functions in position space are still loop integral in momentum space. The natural place to start are the famous banana diagrams, which we explore in this paper. In position space, these are just products of $n$ propagators. Firstly, we explain that these functions satisfy an equation of order $2^n$. These should be compared with Picard-Fuchs equations derived for the momentum space integral. We find that the Fourier transform of the position space operator contains the Picard-Fuchs one as a rightmost factor. The order of these operators is a special issue, especially since the order in momentum space is governed by degree in $x$ in position space. For the generic mass case this factorization pattern is complicated and it seems like the order of the Fourier transformed position space operators is much bigger than that of the Picard-Fuchs. Furthermore, one may ask what happens if after factorization we take the Picard-Fuchs operators back into position space. We discover that the result is again factorized, with the rightmost factor being the original position space equation. We demonstrate how this works in examples and discuss implications for more sophisticated Feynman integrals.","sentences":["We continue the development of a position space approach to equations for Feynman multi-loop integrals.","The key idea of the approach is that unintegrated products of Greens functions in position space are still loop integral in momentum space.","The natural place to start are the famous banana diagrams, which we explore in this paper.","In position space, these are just products of $n$ propagators.","Firstly, we explain that these functions satisfy an equation of order $2^n$. These should be compared with Picard-Fuchs equations derived for the momentum space integral.","We find that the Fourier transform of the position space operator contains the Picard-Fuchs one as a rightmost factor.","The order of these operators is a special issue, especially since the order in momentum space is governed by degree in $x$ in position space.","For the generic mass case this factorization pattern is complicated and it seems like the order of the Fourier transformed position space operators is much bigger than that of the Picard-Fuchs.","Furthermore, one may ask what happens if after factorization we take the Picard-Fuchs operators back into position space.","We discover that the result is again factorized, with the rightmost factor being the original position space equation.","We demonstrate how this works in examples and discuss implications for more sophisticated Feynman integrals."],"url":"http://arxiv.org/abs/2404.03069v1","category":"hep-th"}
{"created":"2024-04-03 21:06:08","title":"Consistency of the bootstrap for asymptotically linear estimators based on machine learning","abstract":"The bootstrap is a popular method of constructing confidence intervals due to its ease of use and broad applicability. Theoretical properties of bootstrap procedures have been established in a variety of settings. However, there is limited theoretical research on the use of the bootstrap in the context of estimation of a differentiable functional in a nonparametric or semiparametric model when nuisance functions are estimated using machine learning. In this article, we provide general conditions for consistency of the bootstrap in such scenarios. Our results cover a range of estimator constructions, nuisance estimation methods, bootstrap sampling distributions, and bootstrap confidence interval types. We provide refined results for the empirical bootstrap and smoothed bootstraps, and for one-step estimators, plug-in estimators, empirical mean plug-in estimators, and estimating equations-based estimators. We illustrate the use of our general results by demonstrating the asymptotic validity of bootstrap confidence intervals for the average density value and G-computed conditional mean parameters, and compare their performance in finite samples using numerical studies. Throughout, we emphasize whether and how the bootstrap can produce asymptotically valid confidence intervals when standard methods fail to do so.","sentences":["The bootstrap is a popular method of constructing confidence intervals due to its ease of use and broad applicability.","Theoretical properties of bootstrap procedures have been established in a variety of settings.","However, there is limited theoretical research on the use of the bootstrap in the context of estimation of a differentiable functional in a nonparametric or semiparametric model when nuisance functions are estimated using machine learning.","In this article, we provide general conditions for consistency of the bootstrap in such scenarios.","Our results cover a range of estimator constructions, nuisance estimation methods, bootstrap sampling distributions, and bootstrap confidence interval types.","We provide refined results for the empirical bootstrap and smoothed bootstraps, and for one-step estimators, plug-in estimators, empirical mean plug-in estimators, and estimating equations-based estimators.","We illustrate the use of our general results by demonstrating the asymptotic validity of bootstrap confidence intervals for the average density value and G-computed conditional mean parameters, and compare their performance in finite samples using numerical studies.","Throughout, we emphasize whether and how the bootstrap can produce asymptotically valid confidence intervals when standard methods fail to do so."],"url":"http://arxiv.org/abs/2404.03064v1","category":"math.ST"}
{"created":"2024-04-03 20:45:15","title":"Functionality Optimization for Singlet Fission Rate Screening in the Full-Dimensional Molecular and Intermolecular Coordinate Space","abstract":"In computational chemistry, accurately predicting molecular configurations that exhibit specific properties remains a critical challenge. Its intricacies become especially evident in the study of molecular aggregates, where the light-induced functionality is tied to highly structure-dependent electronic couplings between molecules. Here, we present an efficient strategy for the targeted screening of the structural space employing a \"functionality optimization\" technique, in which a chosen descriptor, constrained by the ground state energy expression, is optimized. The chosen algorithmic differentiation (AD) framework allows to automatically obtain gradients without its tedious implementation. We demonstrate the effectiveness of the approach by identifying Perylene Bisiimide (PBI) dimer motifs with enhanced SF rate. Our findings reveal that certain structural modifications of the PBI monomer, such as helical twisting and bending, as well as slipped-rotated packing arrangements, can significantly increase the SF rate.","sentences":["In computational chemistry, accurately predicting molecular configurations that exhibit specific properties remains a critical challenge.","Its intricacies become especially evident in the study of molecular aggregates, where the light-induced functionality is tied to highly structure-dependent electronic couplings between molecules.","Here, we present an efficient strategy for the targeted screening of the structural space employing a \"functionality optimization\" technique, in which a chosen descriptor, constrained by the ground state energy expression, is optimized.","The chosen algorithmic differentiation (AD) framework allows to automatically obtain gradients without its tedious implementation.","We demonstrate the effectiveness of the approach by identifying Perylene Bisiimide (PBI) dimer motifs with enhanced SF rate.","Our findings reveal that certain structural modifications of the PBI monomer, such as helical twisting and bending, as well as slipped-rotated packing arrangements, can significantly increase the SF rate."],"url":"http://arxiv.org/abs/2404.03056v1","category":"physics.comp-ph"}
{"created":"2024-04-03 20:30:38","title":"Language, Environment, and Robotic Navigation","abstract":"This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.","sentences":["This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition.","It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field.","By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences.","Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems."],"url":"http://arxiv.org/abs/2404.03049v1","category":"cs.CL"}
{"created":"2024-04-03 19:49:24","title":"On a counterexample to Mordell's Pellian equation conjecture: a non-computer based approach","abstract":"In this note, we investigate a recently discovered counterexample to Mordell's Pellian equation conjecture. We provide a verification of this counterexample that can be checked without computer assistance.","sentences":["In this note, we investigate a recently discovered counterexample to Mordell's Pellian equation conjecture.","We provide a verification of this counterexample that can be checked without computer assistance."],"url":"http://arxiv.org/abs/2404.03038v1","category":"math.NT"}
{"created":"2024-04-03 19:41:12","title":"General Effect Modelling (GEM) -- Part 3. GEM applied on proteome data of cerebrospinal fluid of multiple sclerosis and clinically isolated syndrome","abstract":"The novel data analytical platform General Effect Modelling (GEM), is an umbrella platform covering different data analytical methods that handle data with multiple design variables (or pseudo design variables) and multivariate responses. GEM is here demonstrated in an analysis of proteome data from cerebrospinal fluid (CSF) from two independent previously published datasets, one data set comprised of persons with relapsing-remitting multiple sclerosis, persons with other neurological disorders and persons without neurological disorders, and one data set had persons with clinically isolated syndrome (CIS), which is the first clinical symptom of MS, and controls. The primary aim of the present publication is to use these data to demonstrate how patient stratification can be utilised by GEM for multivariate analysis. We also emphasize how the findings shed light on important aspects of the molecular mechanism of MS that may otherwise be lost. We identified proteins involved in neural development as significantly lower for MS/CIS than for their respective controls. This information was only seen after stratification of the persons into two groups, which were found to have different inflammatory patterns and the utilisation of this by GEM. Our conclusion from the study of these data is that disrupted neural development may be an early event in CIS and MS.","sentences":["The novel data analytical platform General Effect Modelling (GEM), is an umbrella platform covering different data analytical methods that handle data with multiple design variables (or pseudo design variables) and multivariate responses.","GEM is here demonstrated in an analysis of proteome data from cerebrospinal fluid (CSF) from two independent previously published datasets, one data set comprised of persons with relapsing-remitting multiple sclerosis, persons with other neurological disorders and persons without neurological disorders, and one data set had persons with clinically isolated syndrome (CIS), which is the first clinical symptom of MS, and controls.","The primary aim of the present publication is to use these data to demonstrate how patient stratification can be utilised by GEM for multivariate analysis.","We also emphasize how the findings shed light on important aspects of the molecular mechanism of MS that may otherwise be lost.","We identified proteins involved in neural development as significantly lower for MS/CIS than for their respective controls.","This information was only seen after stratification of the persons into two groups, which were found to have different inflammatory patterns and the utilisation of this by GEM.","Our conclusion from the study of these data is that disrupted neural development may be an early event in CIS and MS."],"url":"http://arxiv.org/abs/2404.03034v1","category":"stat.ME"}
{"created":"2024-04-03 19:03:15","title":"GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU","abstract":"In recent years, Graph Neural Networks (GNNs) have ignited a surge of innovation, significantly enhancing the processing of geometric data structures such as graphs, point clouds, and meshes. As the domain continues to evolve, a series of frameworks and libraries are being developed to push GNN efficiency to new heights. While graph-centric libraries have achieved success in the past, the advent of efficient tensor compilers has highlighted the urgent need for tensor-centric libraries. Yet, efficient tensor-centric frameworks for GNNs remain scarce due to unique challenges and limitations encountered when implementing segment reduction in GNN contexts.   We introduce GeoT, a cutting-edge tensor-centric library designed specifically for GNNs via efficient segment reduction. GeoT debuts innovative parallel algorithms that not only introduce new design principles but also expand the available design space. Importantly, GeoT is engineered for straightforward fusion within a computation graph, ensuring compatibility with contemporary tensor-centric machine learning frameworks and compilers. Setting a new performance benchmark, GeoT marks a considerable advancement by showcasing an average operator speedup of 1.80x and an end-to-end speedup of 1.68x.","sentences":["In recent years, Graph Neural Networks (GNNs) have ignited a surge of innovation, significantly enhancing the processing of geometric data structures such as graphs, point clouds, and meshes.","As the domain continues to evolve, a series of frameworks and libraries are being developed to push GNN efficiency to new heights.","While graph-centric libraries have achieved success in the past, the advent of efficient tensor compilers has highlighted the urgent need for tensor-centric libraries.","Yet, efficient tensor-centric frameworks for GNNs remain scarce due to unique challenges and limitations encountered when implementing segment reduction in GNN contexts.   ","We introduce GeoT, a cutting-edge tensor-centric library designed specifically for GNNs via efficient segment reduction.","GeoT debuts innovative parallel algorithms that not only introduce new design principles but also expand the available design space.","Importantly, GeoT is engineered for straightforward fusion within a computation graph, ensuring compatibility with contemporary tensor-centric machine learning frameworks and compilers.","Setting a new performance benchmark, GeoT marks a considerable advancement by showcasing an average operator speedup of 1.80x and an end-to-end speedup of 1.68x."],"url":"http://arxiv.org/abs/2404.03019v1","category":"cs.DC"}
{"created":"2024-04-03 18:41:51","title":"DESI 2024 VI: Cosmological Constraints from the Measurements of Baryon Acoustic Oscillations","abstract":"We present cosmological results from the measurement of baryon acoustic oscillations (BAO) in galaxy, quasar and Lyman-$\\alpha$ forest tracers from the first year of observations from the Dark Energy Spectroscopic Instrument (DESI), to be released in the DESI Data Release 1. DESI BAO provide robust measurements of the transverse comoving distance and Hubble rate, or their combination, relative to the sound horizon, in seven redshift bins from over 6 million extragalactic objects in the redshift range $0.1<z<4.2$. DESI BAO data alone are consistent with the standard flat $\\Lambda$CDM cosmological model with a matter density $\\Omega_\\mathrm{m}=0.295\\pm 0.015$. Paired with a BBN prior and the robustly measured acoustic angular scale from the CMB, DESI requires $H_0=(68.52\\pm0.62)$ km/s/Mpc. In conjunction with CMB anisotropies from Planck and CMB lensing data from Planck and ACT, we find $\\Omega_\\mathrm{m}=0.307\\pm 0.005$ and $H_0=(67.97\\pm0.38)$ km/s/Mpc. Extending the baseline model with a constant dark energy equation of state parameter $w$, DESI BAO alone require $w=-0.99^{+0.15}_{-0.13}$. In models with a time-varying dark energy equation of state parametrized by $w_0$ and $w_a$, combinations of DESI with CMB or with SN~Ia individually prefer $w_0>-1$ and $w_a<0$. This preference is 2.6$\\sigma$ for the DESI+CMB combination, and persists or grows when SN~Ia are added in, giving results discrepant with the $\\Lambda$CDM model at the $2.5\\sigma$, $3.5\\sigma$ or $3.9\\sigma$ levels for the addition of Pantheon+, Union3, or DES-SN5YR datasets respectively. For the flat $\\Lambda$CDM model with the sum of neutrino mass $\\sum m_\\nu$ free, combining the DESI and CMB data yields an upper limit $\\sum m_\\nu < 0.072$ $(0.113)$ eV at 95% confidence for a $\\sum m_\\nu>0$ $(\\sum m_\\nu>0.059)$ eV prior. These neutrino-mass constraints are substantially relaxed in models beyond $\\Lambda$CDM. [Abridged.]","sentences":["We present cosmological results from the measurement of baryon acoustic oscillations (BAO) in galaxy, quasar and Lyman-$\\alpha$ forest tracers from the first year of observations from the Dark Energy Spectroscopic Instrument (DESI), to be released in the DESI Data Release 1.","DESI BAO provide robust measurements of the transverse comoving distance and Hubble rate, or their combination, relative to the sound horizon, in seven redshift bins from over 6 million extragalactic objects in the redshift range $0.1<z<4.2$. DESI BAO data alone are consistent with the standard flat $\\Lambda$CDM cosmological model with a matter density $\\Omega_\\mathrm{m}=0.295\\pm 0.015$. Paired with a BBN prior and the robustly measured acoustic angular scale from the CMB, DESI requires $H_0=(68.52\\pm0.62)$ km/s/Mpc.","In conjunction with CMB anisotropies from Planck and CMB lensing data from Planck and ACT, we find $\\Omega_\\mathrm{m}=0.307\\pm 0.005$ and $H_0=(67.97\\pm0.38)$ km/s/Mpc.","Extending the baseline model with a constant dark energy equation of state parameter $w$, DESI BAO alone require $w=-0.99^{+0.15}_{-0.13}$. In models with a time-varying dark energy equation of state parametrized by $w_0$ and $w_a$, combinations of DESI with CMB or with SN~Ia individually prefer $w_0>-1$ and $w_a<0$. This preference is 2.6$\\sigma$ for the DESI+CMB combination, and persists or grows when SN~Ia are added in, giving results discrepant with the $\\Lambda$CDM model at the $2.5\\sigma$, $3.5\\sigma$ or $3.9\\sigma$ levels for the addition of Pantheon+, Union3, or DES-SN5YR datasets respectively.","For the flat $\\Lambda$CDM model with the sum of neutrino mass $\\sum m_\\nu$ free, combining the DESI and CMB data yields an upper limit $\\sum m_\\nu < 0.072$ $(0.113)$ eV at 95% confidence for a $\\sum m_\\nu>0$ $(\\sum m_\\nu>0.059)$ eV prior.","These neutrino-mass constraints are substantially relaxed in models beyond $\\Lambda$CDM.","[Abridged.]"],"url":"http://arxiv.org/abs/2404.03002v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:40:48","title":"MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy","abstract":"Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering realistic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate realistic simulations as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN perform well, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures.","sentences":["Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy.","Rendering realistic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate realistic simulations as well as ground truth camera poses and depth maps.","Although image-to-image (I2I) translation models such as CycleGAN perform well, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames.","We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering.","MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods.","With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs.","We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training and preoperative planning.","Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures."],"url":"http://arxiv.org/abs/2404.02999v1","category":"eess.IV"}
{"created":"2024-04-03 18:33:40","title":"Poisson structures on wrinkled fibrations","abstract":"We provide local formul{\\ae} for Poisson bivectors and symplectic forms on the leaves of Poisson structures associated to wrinkled fibrations on smooth $4$--manifolds.","sentences":["We provide local formul{\\ae} for Poisson bivectors and symplectic forms on the leaves of Poisson structures associated to wrinkled fibrations on smooth $4$--manifolds."],"url":"http://arxiv.org/abs/2404.02995v1","category":"math.SG"}
{"created":"2024-04-03 18:16:12","title":"Singular solutions for complex second order elliptic equations and their application to time-harmonic diffuse optical tomography","abstract":"We construct singular solutions of a complex elliptic equation of second order, having an isolated singularity of any order. In particular, we extend results obtained for the real partial differential equation in divergence form by Alessandrini in 1990. Our solutions can be applied to the determination of the optical properties of an anisotropic medium in time-harmonic Diffuse Optical Tomography (DOT).","sentences":["We construct singular solutions of a complex elliptic equation of second order, having an isolated singularity of any order.","In particular, we extend results obtained for the real partial differential equation in divergence form by Alessandrini in 1990.","Our solutions can be applied to the determination of the optical properties of an anisotropic medium in time-harmonic Diffuse Optical Tomography (DOT)."],"url":"http://arxiv.org/abs/2404.02987v1","category":"math.AP"}
{"created":"2024-04-03 18:06:54","title":"On the pseudo-Riemann metrizability of SO(3)-symmetric Berwald-Finsler spaces","abstract":"We clarify, for SO(3)-symmetric 4-dimensional Berwald-Finsler spaces of indefinite signature, the question of the existence of an affinely equivalent pseudo-Riemannian metric. It turns out that the answer depends on the holonomy distribution of the canonical affine connection and on the symmetry of its Ricci tensor. In particular, we find all classes of 4-dimensional SO(3)-invariant, symmetric affine connections which do not arise as Levi-Civita connections of any pseudo-Riemannian metric, but can still be metrized by (SO(3)-symmetric) Finsler functions; in Lorentzian signature, these will provide Berwald spacetime structures whose geodesic structure cannot be ascribed to any Lorentzian metric. Some concrete examples are also presented.","sentences":["We clarify, for SO(3)-symmetric 4-dimensional Berwald-Finsler spaces of indefinite signature, the question of the existence of an affinely equivalent pseudo-Riemannian metric.","It turns out that the answer depends on the holonomy distribution of the canonical affine connection and on the symmetry of its Ricci tensor.","In particular, we find all classes of 4-dimensional SO(3)-invariant, symmetric affine connections which do not arise as Levi-Civita connections of any pseudo-Riemannian metric, but can still be metrized by (SO(3)-symmetric) Finsler functions; in Lorentzian signature, these will provide Berwald spacetime structures whose geodesic structure cannot be ascribed to any Lorentzian metric.","Some concrete examples are also presented."],"url":"http://arxiv.org/abs/2404.02980v1","category":"math.DG"}
{"created":"2024-04-03 18:00:12","title":"On the Proof of Chiral Symmetry Breaking through Anomaly Matching in QCD-like Theories: An Exemplification","abstract":"Our recent works revisit the proof of chiral symmetry breaking in the confining phase of four-dimensional QCD-like theories, i.e. $SU(N_c)$ gauge theories with $N_f$ flavors of vectorlike quarks in the fundamental representation. The analysis relies on the structure of 't Hooft anomaly matching and persistent mass conditions for theories with same $N_c$ and different $N_f$. In this paper, we work out concrete examples with $N_c=3$ and $N_c=5$ to support and elucidate the results in the companion papers. Within the same examples, we also test some claims made in earlier works.","sentences":["Our recent works revisit the proof of chiral symmetry breaking in the confining phase of four-dimensional QCD-like theories, i.e. $SU(N_c)$ gauge theories with $N_f$ flavors of vectorlike quarks in the fundamental representation.","The analysis relies on the structure of 't Hooft anomaly matching and persistent mass conditions for theories with same $N_c$ and different $N_f$. In this paper, we work out concrete examples with $N_c=3$ and $N_c=5$ to support and elucidate the results in the companion papers.","Within the same examples, we also test some claims made in earlier works."],"url":"http://arxiv.org/abs/2404.02971v1","category":"hep-th"}
{"created":"2024-04-03 18:00:09","title":"Critical Properties of Weak Measurement Induced Phase Transitions in Random Quantum Circuits","abstract":"The effects of different forms of weak measurements on the nature of the measurement induced phase transition are theoretically studied in hybrid random quantum circuits of qubits. We use a combination of entanglement measures, ancilla purification dynamics, and a transfer matrix approach to compute the critical exponents, the effective central charge, and the multifractal spectrum of the measurement induced transitions. We compare weak measurements with an infinite number of discrete outcomes to a protocol with only a pair of outcomes and find that to within our numerical accuracy the universal critical properties are unaffected by the weak measurement protocols and are consistent with the universality class found for strong projective measurements.","sentences":["The effects of different forms of weak measurements on the nature of the measurement induced phase transition are theoretically studied in hybrid random quantum circuits of qubits.","We use a combination of entanglement measures, ancilla purification dynamics, and a transfer matrix approach to compute the critical exponents, the effective central charge, and the multifractal spectrum of the measurement induced transitions.","We compare weak measurements with an infinite number of discrete outcomes to a protocol with only a pair of outcomes and find that to within our numerical accuracy the universal critical properties are unaffected by the weak measurement protocols and are consistent with the universality class found for strong projective measurements."],"url":"http://arxiv.org/abs/2404.02968v1","category":"quant-ph"}
{"created":"2024-04-03 18:00:00","title":"Chirality-Driven Orbital Angular Momentum and Circular Dichroism in CoSi","abstract":"Chiral crystals and molecules were recently predicted to form an intriguing platform for unconventional orbital physics. Here, we report the observation of chirality-driven orbital textures in the bulk electronic structure of CoSi, a prototype member of the cubic B20 family of chiral crystals. Using circular dichroism in soft X-ray angle-resolved photoemission, we demonstrate the formation of a bulk orbital-angular-momentum texture and monopole-like orbital-momentum locking that depends on crystal handedness. We introduce the intrinsic chiral circular dichroism, icCD, as a differential photoemission observable and a natural probe of chiral electron states. Our findings render chiral crystals promising for spin-orbitronics applications.","sentences":["Chiral crystals and molecules were recently predicted to form an intriguing platform for unconventional orbital physics.","Here, we report the observation of chirality-driven orbital textures in the bulk electronic structure of CoSi, a prototype member of the cubic B20 family of chiral crystals.","Using circular dichroism in soft X-ray angle-resolved photoemission, we demonstrate the formation of a bulk orbital-angular-momentum texture and monopole-like orbital-momentum locking that depends on crystal handedness.","We introduce the intrinsic chiral circular dichroism, icCD, as a differential photoemission observable and a natural probe of chiral electron states.","Our findings render chiral crystals promising for spin-orbitronics applications."],"url":"http://arxiv.org/abs/2404.02952v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 17:53:32","title":"Comment on \"Machine learning conservation laws from differential equations\"","abstract":"In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the author. However, their derivation contained six serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed.","sentences":["In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark","[2, 3], without citing the author.","However, their derivation contained six serious errors, causing both their method and result to be incorrect.","In this Comment, those errors are reviewed."],"url":"http://arxiv.org/abs/2404.02896v1","category":"cs.LG"}
{"created":"2024-04-03 17:52:28","title":"Renormalized energy of proper maps and conformal geodesics","abstract":"We introduce a certain renormalized energy of proper maps between conformally compact Einstein manifolds, which is then used to give a holographic description of conformal geodesics on the boundary at infinity, in a way deeply inspired by a work of Fine and Herfray on renormalized area minimization and conformal geodesics.","sentences":["We introduce a certain renormalized energy of proper maps between conformally compact Einstein manifolds, which is then used to give a holographic description of conformal geodesics on the boundary at infinity, in a way deeply inspired by a work of Fine and Herfray on renormalized area minimization and conformal geodesics."],"url":"http://arxiv.org/abs/2404.02895v1","category":"math.DG"}
{"created":"2024-04-03 17:49:41","title":"MODNO: Multi Operator Learning With Distributed Neural Operators","abstract":"The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset. Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method. Our results demonstrate enhanced efficiency and satisfactory accuracy. Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning. This highlights another MOL's potential to bolster operator learning.","sentences":["The study of operator learning involves the utilization of neural networks to approximate operators.","Traditionally, the focus has been on single-operator learning (SOL).","However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL).","In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs.","Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).","The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset.","Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method.","Our results demonstrate enhanced efficiency and satisfactory accuracy.","Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning.","This highlights another MOL's potential to bolster operator learning."],"url":"http://arxiv.org/abs/2404.02892v1","category":"cs.LG"}
{"created":"2024-04-03 17:42:22","title":"Learning Quadrupedal Locomotion via Differentiable Simulation","abstract":"The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional Reinforcement Learning (RL) methods. While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce. This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients. The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact. Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact simulation. We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients.","sentences":["The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional Reinforcement Learning (RL) methods.","While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce.","This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients.","The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact.","Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact simulation.","We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients."],"url":"http://arxiv.org/abs/2404.02887v1","category":"cs.RO"}
{"created":"2024-04-03 17:38:04","title":"Stability of multiphase mean curvature flow beyond circular topology changes","abstract":"We prove a weak-strong uniqueness principle for varifold-BV solutions to planar multiphase mean curvature flow beyond a circular topology change: Assuming that there exists a classical solution with an interface that becomes increasingly circular and shrinks to a point, any varifold-BV solution with the same initial interface must coincide with it, and any varifold-BV solution with similar initial data must undergo the same type of topology change. Our result illustrates the robustness of the relative energy method for establishing weak-strong uniqueness principles for interface evolution equations, showing that it may also be applied beyond certain topological changes.","sentences":["We prove a weak-strong uniqueness principle for varifold-BV solutions to planar multiphase mean curvature flow beyond a circular topology change: Assuming that there exists a classical solution with an interface that becomes increasingly circular and shrinks to a point, any varifold-BV solution with the same initial interface must coincide with it, and any varifold-BV solution with similar initial data must undergo the same type of topology change.","Our result illustrates the robustness of the relative energy method for establishing weak-strong uniqueness principles for interface evolution equations, showing that it may also be applied beyond certain topological changes."],"url":"http://arxiv.org/abs/2404.02884v1","category":"math.AP"}
{"created":"2024-04-03 17:25:18","title":"Pair production in rotating electric fields via quantum kinetic equations: Resolving helicity states","abstract":"We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a strong electric field of circular polarization. By means of a nonperturbative approach based on the quantum kinetic equations (QKEs), we numerically calculate helicity-resolved momentum distributions of the particles produced and analyze the corresponding helicity asymmetry. It is demonstrated that the external rotating field tends to generate left-handed and right-handed particles traveling in opposite directions. Generic symmetry properties of the momentum spectra are examined analytically by means of the QKEs and also confirmed and illustrated by direct numerical computations. The helicity signatures revealed in our study are expected to provide a firmer basis for possible experimental investigations of the fundamental phenomenon of vacuum pair production in strong fields.","sentences":["We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a strong electric field of circular polarization.","By means of a nonperturbative approach based on the quantum kinetic equations (QKEs), we numerically calculate helicity-resolved momentum distributions of the particles produced and analyze the corresponding helicity asymmetry.","It is demonstrated that the external rotating field tends to generate left-handed and right-handed particles traveling in opposite directions.","Generic symmetry properties of the momentum spectra are examined analytically by means of the QKEs and also confirmed and illustrated by direct numerical computations.","The helicity signatures revealed in our study are expected to provide a firmer basis for possible experimental investigations of the fundamental phenomenon of vacuum pair production in strong fields."],"url":"http://arxiv.org/abs/2404.02878v1","category":"hep-ph"}
{"created":"2024-04-03 17:16:37","title":"Subconductance states in a semimicroscopic model for a tetrameric pore","abstract":"A physical model for a structured tetrameric pore is studied. The pore is modeled as a device composed of four subunits, each one exhibiting two possible states (open and closed). The pore is located within a membrane that separates two reservoirs with ionic solutions. All variables of the model follow physical dynamical equations accounting for the internal structure of the pore, derived from a single energy functional and supplemented with thermal noises. An extensive study of the resulting ionic intensity is performed for different values of the control parameters, mainly membrane potential and reservoir ion concentrations. Two possible physical devices are studied: voltage-gated (including a voltage sensor in each subunit) and non-voltage-gated pores. The ionic flux through the pore exhibits several distinct dynamical configurations, in particular subconductance states, which indicate very different dynamical internal states of the subunits. Such subconductance states become much easier to observe in sensorless pores. These results are compared with available experimental data on tetrameric K channels and analytical predictions.","sentences":["A physical model for a structured tetrameric pore is studied.","The pore is modeled as a device composed of four subunits, each one exhibiting two possible states (open and closed).","The pore is located within a membrane that separates two reservoirs with ionic solutions.","All variables of the model follow physical dynamical equations accounting for the internal structure of the pore, derived from a single energy functional and supplemented with thermal noises.","An extensive study of the resulting ionic intensity is performed for different values of the control parameters, mainly membrane potential and reservoir ion concentrations.","Two possible physical devices are studied: voltage-gated (including a voltage sensor in each subunit) and non-voltage-gated pores.","The ionic flux through the pore exhibits several distinct dynamical configurations, in particular subconductance states, which indicate very different dynamical internal states of the subunits.","Such subconductance states become much easier to observe in sensorless pores.","These results are compared with available experimental data on tetrameric K channels and analytical predictions."],"url":"http://arxiv.org/abs/2404.02875v1","category":"physics.bio-ph"}
{"created":"2024-04-03 16:58:03","title":"Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds","abstract":"Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.","sentences":["Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers.","The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\").","The added noise helps prevent reconstruction of the inputs from the noisy features.","Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise.","Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds.","Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification.","The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes.","Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet.","In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features.","Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification."],"url":"http://arxiv.org/abs/2404.02866v1","category":"cs.LG"}
{"created":"2024-04-03 16:57:26","title":"End-To-End Self-tuning Self-supervised Time Series Anomaly Detection","abstract":"Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type. Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters. In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types.","sentences":["Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc.","A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data.","Modern neural networks have outstanding ability in modeling complex time series.","Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training.","However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels.","Our work aims to fill this gap.","We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end.","It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type.","Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters.","In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types."],"url":"http://arxiv.org/abs/2404.02865v1","category":"cs.LG"}
{"created":"2024-04-03 16:13:29","title":"Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison","abstract":"Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes. We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.","sentences":["Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process.","While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored.","In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes.","We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning.","Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures.","We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board."],"url":"http://arxiv.org/abs/2404.02835v1","category":"cs.CL"}
{"created":"2024-04-03 15:43:47","title":"Optimal distributed control with stability guarantees by training a network of neural closed-loop maps","abstract":"This paper proposes a novel approach to improve the performance of distributed nonlinear control systems while preserving stability by leveraging Deep Neural Networks (DNNs). We build upon the Neural System Level Synthesis (Neur-SLS) framework and introduce a method to parameterize stabilizing control policies that are distributed across a network topology. A distinctive feature is that we iteratively minimize an arbitrary control cost function through an unconstrained optimization algorithm, all while preserving the stability of the overall network architecture by design. This is achieved through two key steps. First, we establish a method to parameterize interconnected Recurrent Equilibrium Networks (RENs) that guarantees a bounded $\\mathcal{L}_2$ gain at the network level. This ensures stability. Second, we demonstrate how the information flow within the network is preserved, enabling a fully distributed implementation where each subsystem only communicates with its neighbors. To showcase the effectiveness of our approach, we present a simulation of a distributed formation control problem for a fleet of vehicles. The simulation demonstrates how the proposed neural controller enables the vehicles to maintain a desired formation while navigating obstacles and avoiding collisions, all while guaranteeing network stability.","sentences":["This paper proposes a novel approach to improve the performance of distributed nonlinear control systems while preserving stability by leveraging Deep Neural Networks (DNNs).","We build upon the Neural System Level Synthesis (Neur-SLS) framework and introduce a method to parameterize stabilizing control policies that are distributed across a network topology.","A distinctive feature is that we iteratively minimize an arbitrary control cost function through an unconstrained optimization algorithm, all while preserving the stability of the overall network architecture by design.","This is achieved through two key steps.","First, we establish a method to parameterize interconnected Recurrent Equilibrium Networks (RENs) that guarantees a bounded $\\mathcal{L}_2$ gain at the network level.","This ensures stability.","Second, we demonstrate how the information flow within the network is preserved, enabling a fully distributed implementation where each subsystem only communicates with its neighbors.","To showcase the effectiveness of our approach, we present a simulation of a distributed formation control problem for a fleet of vehicles.","The simulation demonstrates how the proposed neural controller enables the vehicles to maintain a desired formation while navigating obstacles and avoiding collisions, all while guaranteeing network stability."],"url":"http://arxiv.org/abs/2404.02820v1","category":"math.OC"}
{"created":"2024-04-03 15:42:25","title":"Efficient Quantum Circuits for Non-Unitary and Unitary Diagonal Operators with Space-Time-Accuracy trade-offs","abstract":"Unitary and non-unitary diagonal operators are fundamental building blocks in quantum algorithms with applications in the resolution of partial differential equations, Hamiltonian simulations, the loading of classical data on quantum computers (quantum state preparation) and many others. In this paper, we introduce a general approach to implement unitary and non-unitary diagonal operators with efficient-adjustable-depth quantum circuits. The depth, {\\sl i.e.}, the number of layers of quantum gates of the quantum circuit, is reducible with respect either to the width, {\\sl i.e.}, the number of ancilla qubits, or to the accuracy between the implemented operator and the target one. While exact methods have an optimal exponential scaling either in terms of size, {\\sl i.e.}, the total number of primitive quantum gates, or width, approximate methods prove to be efficient for the class of diagonal operators depending on smooth, at least differentiable, functions. Our approach is general enough to allow any method for diagonal operators to become adjustable-depth or approximate, decreasing the depth of the circuit by increasing its width or its approximation level. This feature offers flexibility and can match with the hardware limitations in coherence time or cumulative gate error. We illustrate these methods by performing quantum state preparation and non-unitary-real-space simulation of the diffusion equation: an initial Gaussian function is prepared on a set of qubits before being evolved through the non-unitary evolution operator of the diffusion process.","sentences":["Unitary and non-unitary diagonal operators are fundamental building blocks in quantum algorithms with applications in the resolution of partial differential equations, Hamiltonian simulations, the loading of classical data on quantum computers (quantum state preparation) and many others.","In this paper, we introduce a general approach to implement unitary and non-unitary diagonal operators with efficient-adjustable-depth quantum circuits.","The depth, {\\sl i.e.}, the number of layers of quantum gates of the quantum circuit, is reducible with respect either to the width, {\\sl i.e.}, the number of ancilla qubits, or to the accuracy between the implemented operator and the target one.","While exact methods have an optimal exponential scaling either in terms of size, {\\sl i.e.}, the total number of primitive quantum gates, or width, approximate methods prove to be efficient for the class of diagonal operators depending on smooth, at least differentiable, functions.","Our approach is general enough to allow any method for diagonal operators to become adjustable-depth or approximate, decreasing the depth of the circuit by increasing its width or its approximation level.","This feature offers flexibility and can match with the hardware limitations in coherence time or cumulative gate error.","We illustrate these methods by performing quantum state preparation and non-unitary-real-space simulation of the diffusion equation: an initial Gaussian function is prepared on a set of qubits before being evolved through the non-unitary evolution operator of the diffusion process."],"url":"http://arxiv.org/abs/2404.02819v1","category":"quant-ph"}
{"created":"2024-04-03 15:38:13","title":"A Dual Geometric Test for Forward-Flatness","abstract":"Forward-flatness is a generalization of static feedback linearizability and a special case of a more general flatness concept for discrete-time systems. Recently, it has been shown that this practically quite relevant property can be checked by computing a unique sequence of involutive distributions which generalizes the well-known static feedback linearization test. In this paper, a dual test for forward-flatness based on a unique sequence of integrable codistributions is derived. Since the main mathematical operations for determining this sequence are the intersection of codistributions and the calculation of Lie derivatives of 1-forms, it is computationally quite efficient. Furthermore, the formulation with codistributions also facilitates a comparison with the existing discrete-time literature regarding the closely related topic of dynamic feedback linearization, which is mostly formulated in terms of 1-forms rather than vector fields. The presented results are illustrated by two examples.","sentences":["Forward-flatness is a generalization of static feedback linearizability and a special case of a more general flatness concept for discrete-time systems.","Recently, it has been shown that this practically quite relevant property can be checked by computing a unique sequence of involutive distributions which generalizes the well-known static feedback linearization test.","In this paper, a dual test for forward-flatness based on a unique sequence of integrable codistributions is derived.","Since the main mathematical operations for determining this sequence are the intersection of codistributions and the calculation of Lie derivatives of 1-forms, it is computationally quite efficient.","Furthermore, the formulation with codistributions also facilitates a comparison with the existing discrete-time literature regarding the closely related topic of dynamic feedback linearization, which is mostly formulated in terms of 1-forms rather than vector fields.","The presented results are illustrated by two examples."],"url":"http://arxiv.org/abs/2404.02816v1","category":"math.OC"}
{"created":"2024-04-03 15:37:02","title":"GPU-Accelerated RSF Level Set Evolution for Large-Scale Microvascular Segmentation","abstract":"Microvascular networks are challenging to model because these structures are currently near the diffraction limit for most advanced three-dimensional imaging modalities, including confocal and light sheet microscopy. This makes semantic segmentation difficult, because individual components of these networks fluctuate within the confines of individual pixels. Level set methods are ideally suited to solve this problem by providing surface and topological constraints on the resulting model, however these active contour techniques are extremely time intensive and impractical for terabyte-scale images. We propose a reformulation and implementation of the region-scalable fitting (RSF) level set model that makes it amenable to three-dimensional evaluation using both single-instruction multiple data (SIMD) and single-program multiple-data (SPMD) parallel processing. This enables evaluation of the level set equation on independent regions of the data set using graphics processing units (GPUs), making large-scale segmentation of high-resolution networks practical and inexpensive.   We tested this 3D parallel RSF approach on multiple data sets acquired using state-of-the-art imaging techniques to acquire microvascular data, including micro-CT, light sheet fluorescence microscopy (LSFM) and milling microscopy. To assess the performance and accuracy of the RSF model, we conducted a Monte-Carlo-based validation technique to compare results to other segmentation methods. We also provide a rigorous profiling to show the gains in processing speed leveraging parallel hardware. This study showcases the practical application of the RSF model, emphasizing its utility in the challenging domain of segmenting large-scale high-topology network structures with a particular focus on building microvascular models.","sentences":["Microvascular networks are challenging to model because these structures are currently near the diffraction limit for most advanced three-dimensional imaging modalities, including confocal and light sheet microscopy.","This makes semantic segmentation difficult, because individual components of these networks fluctuate within the confines of individual pixels.","Level set methods are ideally suited to solve this problem by providing surface and topological constraints on the resulting model, however these active contour techniques are extremely time intensive and impractical for terabyte-scale images.","We propose a reformulation and implementation of the region-scalable fitting (RSF) level set model that makes it amenable to three-dimensional evaluation using both single-instruction multiple data (SIMD) and single-program multiple-data (SPMD) parallel processing.","This enables evaluation of the level set equation on independent regions of the data set using graphics processing units (GPUs), making large-scale segmentation of high-resolution networks practical and inexpensive.   ","We tested this 3D parallel RSF approach on multiple data sets acquired using state-of-the-art imaging techniques to acquire microvascular data, including micro-CT, light sheet fluorescence microscopy (LSFM) and milling microscopy.","To assess the performance and accuracy of the RSF model, we conducted a Monte-Carlo-based validation technique to compare results to other segmentation methods.","We also provide a rigorous profiling to show the gains in processing speed leveraging parallel hardware.","This study showcases the practical application of the RSF model, emphasizing its utility in the challenging domain of segmenting large-scale high-topology network structures with a particular focus on building microvascular models."],"url":"http://arxiv.org/abs/2404.02813v1","category":"eess.IV"}
{"created":"2024-04-03 15:34:47","title":"Auxiliary Monge-Amp\u00e8re Equations in Orbifold Setting -- a Mean-Value Inequality","abstract":"In this note, we generalize a mean-value inequality of Guo-Phong-Sturm to the setting of a compact K\\\"ahler orbifold. This shows that their reasoning is insensitive to quotient singularities. As we aim for a self-contained exposition, we generalize some fundamental results, namely the $\\alpha$-invariant estimate by H\\\"ormander and Tian, an approximation result for the psh-envelope of a $(1,1)$-form in a K\\\"ahler class by Berman and an $L^\\infty$ estimate for this envelope by Guo-Phong-Tong-Wang.","sentences":["In this note, we generalize a mean-value inequality of Guo-Phong-Sturm to the setting of a compact K\\\"ahler orbifold.","This shows that their reasoning is insensitive to quotient singularities.","As we aim for a self-contained exposition, we generalize some fundamental results, namely the $\\alpha$-invariant estimate by H\\\"ormander and Tian, an approximation result for the psh-envelope of a $(1,1)$-form in a K\\\"ahler class by Berman and an $L^\\infty$ estimate for this envelope by Guo-Phong-Tong-Wang."],"url":"http://arxiv.org/abs/2404.02812v1","category":"math.DG"}
{"created":"2024-04-03 15:31:18","title":"Generative-Contrastive Heterogeneous Graph Neural Network","abstract":"Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \\textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest contrastive and generative baselines on node classification and link prediction tasks. To reproduce our work, we have open-sourced our code at https://github.com/xxx.","sentences":["Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges.","In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks.","However, data augmentation is still limited due to the discrete and abstract nature of graphs.","To tackle the above limitations, we propose a novel \\textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}.","Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm.","This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder.","2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples.","3) A hierarchical contrastive learning strategy for capturing local and global information.","Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced discriminator under the generative-contrastive perspective.","Finally, we compare our model with seventeen baselines on eight real-world datasets.","Our model outperforms the latest contrastive and generative baselines on node classification and link prediction tasks.","To reproduce our work, we have open-sourced our code at https://github.com/xxx."],"url":"http://arxiv.org/abs/2404.02810v1","category":"cs.LG"}
{"created":"2024-04-03 15:18:00","title":"Quantum Big-Bounce as a phenomenology of RQM in the Mini-superspace","abstract":"We investigate the emergence of a quantum Big-Bounce in the context of an isotropic Universe, filled by a self-interacting scalar field, which plays the role of a physical clock. The bouncing cosmology is the result of a scattering process, driven by the scalar field potential, which presence breaks down the frequency separation of the Wheeler-DeWitt equation, treated in strict analogy to a relativistic quantum system. Differently from previous analyses, we consider a really perturbative self-interaction potential, affecting the dynamics in a finite range of the time labeled by the scalar clock (and in particular we remove the divergent character previously allowed). The main result of the present analysis is that, when the Relativistic Quantum Mechanics formalism is properly implemented in the Mini-superspace analogy, the probability amplitude for the bounce is, both in the standard and polymerized case, characterized by a maximum in correspondence of the quasi-classical condition of a Universe minimum volume.","sentences":["We investigate the emergence of a quantum Big-Bounce in the context of an isotropic Universe, filled by a self-interacting scalar field, which plays the role of a physical clock.","The bouncing cosmology is the result of a scattering process, driven by the scalar field potential, which presence breaks down the frequency separation of the Wheeler-DeWitt equation, treated in strict analogy to a relativistic quantum system.","Differently from previous analyses, we consider a really perturbative self-interaction potential, affecting the dynamics in a finite range of the time labeled by the scalar clock (and in particular we remove the divergent character previously allowed).","The main result of the present analysis is that, when the Relativistic Quantum Mechanics formalism is properly implemented in the Mini-superspace analogy, the probability amplitude for the bounce is, both in the standard and polymerized case, characterized by a maximum in correspondence of the quasi-classical condition of a Universe minimum volume."],"url":"http://arxiv.org/abs/2404.02802v2","category":"gr-qc"}
{"created":"2024-04-03 15:08:57","title":"Direct, simple, and efficient computation of all components of the virtual-casing magnetic field in axisymmetric geometries with Kapur-Rokhlin quadrature","abstract":"In a recent publication (Toler et al. 2023), we demonstrated that for axisymmetric geometries, the Kapur-Rokhlin quadrature rule provided an efficient and high-order accurate method for computing the normal component, on the plasma surface, of the magnetic field due to the toroidal current flowing in the plasma, via the virtual-casing principle. The calculation was indirect, as it required the prior computation of the magnetic vector potential from the virtual-casing principle, followed by the computation of its tangential derivative by Fourier differentiation, in order to obtain the normal component of the magnetic field. Our approach did not provide the other components of the virtual-casing magnetic field.   In this letter, we show that a more direct and more general approach is available for the computation of the virtual-casing magnetic field. The Kapur-Rokhlin quadrature rule accurately calculates the principal value integrals in the expression for all the components of the magnetic field on the plasma boundary, and the numerical error converges at a rate nearly as high as the indirect method we presented previously.","sentences":["In a recent publication (Toler et al. 2023), we demonstrated that for axisymmetric geometries, the Kapur-Rokhlin quadrature rule provided an efficient and high-order accurate method for computing the normal component, on the plasma surface, of the magnetic field due to the toroidal current flowing in the plasma, via the virtual-casing principle.","The calculation was indirect, as it required the prior computation of the magnetic vector potential from the virtual-casing principle, followed by the computation of its tangential derivative by Fourier differentiation, in order to obtain the normal component of the magnetic field.","Our approach did not provide the other components of the virtual-casing magnetic field.   ","In this letter, we show that a more direct and more general approach is available for the computation of the virtual-casing magnetic field.","The Kapur-Rokhlin quadrature rule accurately calculates the principal value integrals in the expression for all the components of the magnetic field on the plasma boundary, and the numerical error converges at a rate nearly as high as the indirect method we presented previously."],"url":"http://arxiv.org/abs/2404.02799v1","category":"physics.plasm-ph"}
{"created":"2024-04-03 14:40:50","title":"Profile Likelihood via Optimisation and Differential Equations","abstract":"Profile likelihood provides a general framework to infer on a scalar parameter of a statistical model. A confidence interval is obtained by numerically finding the two abscissas where the profile log-likelihood curve intersects an horizontal line. An alternative derivation for this interval can be obtained by solving a constrained optimisation problem which can broadly be described as: maximise or minimise the parameter of interest under the constraint that the log-likelihood is high enough. This formulation allows nice geometrical interpretations; It can be used to infer on a parameter or on a known scalar function of the parameter, such as a quantile. Widely available routines for constrained optimisation can be used for this task, as well as Markov Chain Monte Carlo samplers. When the interest is on a smooth function depending on an extra continuous variable, the constrained optimisation framework can be used to derive Ordinary Differential Equation (ODE) for the confidence limits. This is illustrated with the return levels of Extreme Value models based on the Generalised Extreme Value distribution. Moreover the same ODE-based technique applies as well to the derivation of profile likelihood contours for couples of parameters. The initial value of the ODE used in the determination of the interval or the contour can itself be obtained by another auxiliary ODE with known initial value obtained by using the confidence level as the extra continuous variable.","sentences":["Profile likelihood provides a general framework to infer on a scalar parameter of a statistical model.","A confidence interval is obtained by numerically finding the two abscissas where the profile log-likelihood curve intersects an horizontal line.","An alternative derivation for this interval can be obtained by solving a constrained optimisation problem which can broadly be described as: maximise or minimise the parameter of interest under the constraint that the log-likelihood is high enough.","This formulation allows nice geometrical interpretations; It can be used to infer on a parameter or on a known scalar function of the parameter, such as a quantile.","Widely available routines for constrained optimisation can be used for this task, as well as Markov Chain Monte Carlo samplers.","When the interest is on a smooth function depending on an extra continuous variable, the constrained optimisation framework can be used to derive Ordinary Differential Equation (ODE) for the confidence limits.","This is illustrated with the return levels of Extreme Value models based on the Generalised Extreme Value distribution.","Moreover the same ODE-based technique applies as well to the derivation of profile likelihood contours for couples of parameters.","The initial value of the ODE used in the determination of the interval or the contour can itself be obtained by another auxiliary ODE with known initial value obtained by using the confidence level as the extra continuous variable."],"url":"http://arxiv.org/abs/2404.02774v1","category":"stat.CO"}
{"created":"2024-04-03 14:36:29","title":"Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators","abstract":"This paper considers the implicit Euler discretization of Levant's arbitrary order robust exact differentiator in presence of sampled measurements. Existing implicit discretizations of that differentiator are shown to exhibit either unbounded bias errors or, surprisingly, discretization chattering despite the use of the implicit discretization. A new, proper implicit discretization that exhibits neither of these two detrimental effects is proposed by computing the differentiator's outputs as appropriately designed linear combinations of its state variables. A numerical differentiator implementation is discussed and closed-form stability conditions for arbitrary differentiation orders are given. The influence of bounded measurement noise and numerical approximation errors is formally analyzed. Numerical simulations confirm the obtained results.","sentences":["This paper considers the implicit Euler discretization of Levant's arbitrary order robust exact differentiator in presence of sampled measurements.","Existing implicit discretizations of that differentiator are shown to exhibit either unbounded bias errors or, surprisingly, discretization chattering despite the use of the implicit discretization.","A new, proper implicit discretization that exhibits neither of these two detrimental effects is proposed by computing the differentiator's outputs as appropriately designed linear combinations of its state variables.","A numerical differentiator implementation is discussed and closed-form stability conditions for arbitrary differentiation orders are given.","The influence of bounded measurement noise and numerical approximation errors is formally analyzed.","Numerical simulations confirm the obtained results."],"url":"http://arxiv.org/abs/2404.02770v1","category":"math.NA"}
{"created":"2024-04-03 13:39:29","title":"LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis","abstract":"Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at https://github.com/ispc-lab/LiDAR4D.","sentences":["Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored.","Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds.","In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis.","In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner.","Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency.","For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns.","Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction.","Codes are available at https://github.com/ispc-lab/LiDAR4D."],"url":"http://arxiv.org/abs/2404.02742v1","category":"cs.CV"}
{"created":"2024-04-03 13:36:53","title":"The Blaschke rolling theorem in Riemannian manifolds of bounded curvature","abstract":"We generalize the classical Blaschke Rolling Theorem to convex domains in Riemannian manifolds of bounded sectional curvature and arbitrary dimension. Our results are sharp and, in this sharp form, are new even in the model spaces of constant curvature.","sentences":["We generalize the classical Blaschke Rolling Theorem to convex domains in Riemannian manifolds of bounded sectional curvature and arbitrary dimension.","Our results are sharp and, in this sharp form, are new even in the model spaces of constant curvature."],"url":"http://arxiv.org/abs/2404.02739v1","category":"math.DG"}
{"created":"2024-04-03 13:35:13","title":"Entanglement structures from modified IR geometry","abstract":"We investigate a new proposal connecting the geometry at various radial scales in asymptotic AdS spacetime with entanglement structure at corresponding real-space length scales of the boundary theory. With this proposal, the bulk IR geometry encodes the long-scale entanglement structure of the dual quantum system. We consider two distinct types of IR geometries, namely the spherical case and the hyperbolic case, which are intimately related to the physics of differential entropy and brane-world holography separately. We explore the corresponding change in the dual long-scale entanglement structures, utilizing the tools of the Ryu-Takayanagi formula, conditional mutual information, and partial entanglement entropy. The results indicate that modifying the IR geometry leads to a redistribution of entanglement at scales longer than a critical length determined by the location of the IR region, with the two modified IR geometries corresponding to two opposite ways of redistribution. Furthermore, we establish the maximum amount of entanglement that can be modified, which is proportional to the area of the IR region.","sentences":["We investigate a new proposal connecting the geometry at various radial scales in asymptotic AdS spacetime with entanglement structure at corresponding real-space length scales of the boundary theory.","With this proposal, the bulk IR geometry encodes the long-scale entanglement structure of the dual quantum system.","We consider two distinct types of IR geometries, namely the spherical case and the hyperbolic case, which are intimately related to the physics of differential entropy and brane-world holography separately.","We explore the corresponding change in the dual long-scale entanglement structures, utilizing the tools of the Ryu-Takayanagi formula, conditional mutual information, and partial entanglement entropy.","The results indicate that modifying the IR geometry leads to a redistribution of entanglement at scales longer than a critical length determined by the location of the IR region, with the two modified IR geometries corresponding to two opposite ways of redistribution.","Furthermore, we establish the maximum amount of entanglement that can be modified, which is proportional to the area of the IR region."],"url":"http://arxiv.org/abs/2404.02737v1","category":"hep-th"}
{"created":"2024-04-03 13:22:47","title":"On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices","abstract":"Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.","sentences":["Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation.","Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks.","Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon.","In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure.","Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores."],"url":"http://arxiv.org/abs/2404.02722v1","category":"cs.LG"}
{"created":"2024-04-03 13:22:25","title":"Light-quark mass dependence of the $\u039b(1405)$ resonance","abstract":"We present the light-quark mass dependence of the $\\Lambda(1405)$ resonance at leading order in a renormalizable framework of covariant chiral effective field theory. The meson-baryon scattering amplitudes, which are obtained by solving the scattering equation within time-ordered perturbation theory, follow the quark mass trajectory of the Coordinated Lattice Simulations consortium. At $M_\\pi\\approx 200$ MeV and $M_K\\approx 487$ MeV, our parameter-free prediction of $\\Lambda(1405)$ poles is consistent with the recent lattice results of BaSc Collaboration [Phys. Rev. Lett. 132, 051901 (2024)]. Varying the pion mass from $135$ MeV to $400$ MeV, we present the evolution of double-pole positions of $\\Lambda(1405)$: the higher pole remains a resonance around the $\\bar{K}N$ threshold; whereas the lower pole undergoes a transition from resonance to a virtual state, and ultimately to a bound state of the $\\pi\\Sigma$ system, which could serve as a prediction of the forthcoming lattice QCD simulations.","sentences":["We present the light-quark mass dependence of the $\\Lambda(1405)$ resonance at leading order in a renormalizable framework of covariant chiral effective field theory.","The meson-baryon scattering amplitudes, which are obtained by solving the scattering equation within time-ordered perturbation theory, follow the quark mass trajectory of the Coordinated Lattice Simulations consortium.","At $M_\\pi\\approx 200$ MeV and $M_K\\approx 487$ MeV, our parameter-free prediction of $\\Lambda(1405)$ poles is consistent with the recent lattice results of BaSc Collaboration [Phys.","Rev. Lett.","132, 051901 (2024)].","Varying the pion mass from $135$ MeV to $400$ MeV, we present the evolution of double-pole positions of $\\Lambda(1405)$: the higher pole remains a resonance around the $\\bar{K}N$ threshold; whereas the lower pole undergoes a transition from resonance to a virtual state, and ultimately to a bound state of the $\\pi\\Sigma$ system, which could serve as a prediction of the forthcoming lattice QCD simulations."],"url":"http://arxiv.org/abs/2404.02720v1","category":"hep-ph"}
{"created":"2024-04-03 13:13:55","title":"Quantum conjugate gradient method using the positive-side quantum eigenvalue transformation","abstract":"Quantum algorithms are still challenging to solve linear systems of equations on real devices. This challenge arises from the need for deep circuits and numerous ancilla qubits. We introduce the quantum conjugate gradient (QCG) method using the quantum eigenvalue transformation (QET). The circuit depth of this algorithm depends on the square root of the coefficient matrix's condition number $\\kappa$, representing a square root improvement compared to the previous quantum algorithms. The number of ancilla qubits is constant, similar to other QET-based algorithms. Additionally, to implement the QCG method efficiently, we devise a QET-based technique that uses only the positive side of the polynomial (denoted by $P(x)$ for $x\\in[0,1]$). We conduct numerical experiments by applying our algorithm to the one-dimensional Poisson equation and successfully solve it. Based on the numerical results, our algorithm significantly improves circuit depth, outperforming another QET-based algorithm by three to four orders of magnitude.","sentences":["Quantum algorithms are still challenging to solve linear systems of equations on real devices.","This challenge arises from the need for deep circuits and numerous ancilla qubits.","We introduce the quantum conjugate gradient (QCG) method using the quantum eigenvalue transformation (QET).","The circuit depth of this algorithm depends on the square root of the coefficient matrix's condition number $\\kappa$, representing a square root improvement compared to the previous quantum algorithms.","The number of ancilla qubits is constant, similar to other QET-based algorithms.","Additionally, to implement the QCG method efficiently, we devise a QET-based technique that uses only the positive side of the polynomial (denoted by $P(x)$ for $x\\in[0,1]$).","We conduct numerical experiments by applying our algorithm to the one-dimensional Poisson equation and successfully solve it.","Based on the numerical results, our algorithm significantly improves circuit depth, outperforming another QET-based algorithm by three to four orders of magnitude."],"url":"http://arxiv.org/abs/2404.02713v1","category":"quant-ph"}
{"created":"2024-04-03 13:06:21","title":"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM","abstract":"Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.","sentences":["Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users.","Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated.","Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in.","Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text.","These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals.","To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text.","To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text.","The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness.","HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components.","HintDroid demo video: https://youtu.be/FWgfcctRbfI."],"url":"http://arxiv.org/abs/2404.02706v1","category":"cs.HC"}
{"created":"2024-04-03 12:40:23","title":"Stable patterns in the Lugiato-Lefever equation with a confined vortex pump","abstract":"We introduce a model of a passive optical cavity based on a novel variety of the two-dimensional Lugiato-Lefever equation, with a localized pump carrying intrinsic vorticity S, and the cubic or cubic-quintic nonlinearity. Up to S = 5, stable confined vortex-ring states (vortex pixels) are produced by means of a variational approximation and in a numerical form. Surprisingly, vast stability areas of the vortex states are found, for both the self-focusing and defocusing signs of the nonlinearity, in the plane of the pump and loss parameters. When the vortex-rings are unstable, they are destroyed by azimuthal perturbations which break the axial symmetry. The results suggest new possibilities for mode manipulations in passive nonlinear photonic media by means of appropriately designed pump beams.","sentences":["We introduce a model of a passive optical cavity based on a novel variety of the two-dimensional Lugiato-Lefever equation, with a localized pump carrying intrinsic vorticity S, and the cubic or cubic-quintic nonlinearity.","Up to S = 5, stable confined vortex-ring states (vortex pixels) are produced by means of a variational approximation and in a numerical form.","Surprisingly, vast stability areas of the vortex states are found, for both the self-focusing and defocusing signs of the nonlinearity, in the plane of the pump and loss parameters.","When the vortex-rings are unstable, they are destroyed by azimuthal perturbations which break the axial symmetry.","The results suggest new possibilities for mode manipulations in passive nonlinear photonic media by means of appropriately designed pump beams."],"url":"http://arxiv.org/abs/2404.02693v1","category":"physics.optics"}
{"created":"2024-04-03 12:37:39","title":"Elementary methods provide more replicable results in microbial differential abundance analysis","abstract":"Differential abundance analysis is a key component of microbiome studies. It focuses on the task of assessing the magnitude and statistical significance of differences in microbial abundances between conditions. While dozens of methods for differential abundance analysis exist, they have been reported to produce remarkably discordant results. Currently, there is no consensus on the preferred methods. While correctness of results in differential abundance analysis is an ambiguous concept that cannot be evaluated without employing simulated data, we argue that consistency of results across datasets should be considered as an essential quality of a well-performing method. We compared the performance of 13 differential abundance analysis methods employing datasets from multiple (N = 54) taxonomic profiling studies based on 16S rRNA gene or shotgun sequencing. For each method, we examined how the results replicated between random partitions of each dataset and between datasets from independent studies. While certain methods showed good consistency, some widely used methods were observed to make a substantial number of conflicting findings. Overall, the highest consistency without unnecessary reduction in sensitivity was attained by analyzing total sum scaling (TSS) normalized counts with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression (MaAsLin2). Comparable performance was also attained by analyzing presence/absence of taxa with logistic regression. In conclusion, while numerous sophisticated methods for differential abundance analysis have been developed, elementary methods seem to provide more consistent results without unnecessarily compromising sensitivity. We therefore suggest that the elementary methods should be preferred in microbial differential abundance analysis when replicability needs to be emphasized.","sentences":["Differential abundance analysis is a key component of microbiome studies.","It focuses on the task of assessing the magnitude and statistical significance of differences in microbial abundances between conditions.","While dozens of methods for differential abundance analysis exist, they have been reported to produce remarkably discordant results.","Currently, there is no consensus on the preferred methods.","While correctness of results in differential abundance analysis is an ambiguous concept that cannot be evaluated without employing simulated data, we argue that consistency of results across datasets should be considered as an essential quality of a well-performing method.","We compared the performance of 13 differential abundance analysis methods employing datasets from multiple (N = 54) taxonomic profiling studies based on 16S rRNA gene or shotgun sequencing.","For each method, we examined how the results replicated between random partitions of each dataset and between datasets from independent studies.","While certain methods showed good consistency, some widely used methods were observed to make a substantial number of conflicting findings.","Overall, the highest consistency without unnecessary reduction in sensitivity was attained by analyzing total sum scaling (TSS) normalized counts with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression (MaAsLin2).","Comparable performance was also attained by analyzing presence/absence of taxa with logistic regression.","In conclusion, while numerous sophisticated methods for differential abundance analysis have been developed, elementary methods seem to provide more consistent results without unnecessarily compromising sensitivity.","We therefore suggest that the elementary methods should be preferred in microbial differential abundance analysis when replicability needs to be emphasized."],"url":"http://arxiv.org/abs/2404.02691v1","category":"stat.AP"}
{"created":"2024-04-03 12:25:00","title":"A weak-strong uniqueness principle for the Mullins-Sekerka equation","abstract":"We establish a weak-strong uniqueness principle for the two-phase Mullins-Sekerka equation in the plane: As long as a classical solution to the evolution problem exists, any weak De Giorgi type varifold solution (see for this notion the recent work of Stinson and the second author, Arch. Ration. Mech. Anal. 248, 8, 2024) must coincide with it. In particular, in the absence of geometric singularities such weak solutions do not introduce a mechanism for (unphysical) non-uniqueness. We also derive a stability estimate with respect to changes in the data. Our method is based on the notion of relative entropies for interface evolution problems, a reduction argument to a perturbative graph setting (which is the only step in our argument exploiting in an essential way the planar setting), and a stability analysis in this perturbative regime relying crucially on the gradient flow structure of the Mullins-Sekerka equation.","sentences":["We establish a weak-strong uniqueness principle for the two-phase Mullins-Sekerka equation in the plane: As long as a classical solution to the evolution problem exists, any weak De Giorgi type varifold solution (see for this notion the recent work of Stinson and the second author, Arch.","Ration.","Mech.","Anal.","248, 8, 2024) must coincide with it.","In particular, in the absence of geometric singularities such weak solutions do not introduce a mechanism for (unphysical) non-uniqueness.","We also derive a stability estimate with respect to changes in the data.","Our method is based on the notion of relative entropies for interface evolution problems, a reduction argument to a perturbative graph setting (which is the only step in our argument exploiting in an essential way the planar setting), and a stability analysis in this perturbative regime relying crucially on the gradient flow structure of the Mullins-Sekerka equation."],"url":"http://arxiv.org/abs/2404.02682v1","category":"math.AP"}
{"created":"2024-04-03 12:06:01","title":"RS-Mamba for Large Remote Sensing Image Dense Prediction","abstract":"The spatial resolution of remote sensing images is becoming increasingly higher, posing challenges in handling large very-high-resolution (VHR) remote sensing images for dense prediction tasks. Models based on convolutional neural networks are limited in their ability to model global features of remote sensing images due to local convolution operations. Transformer based models, despite their global modeling capabilities, face computational challenges with large VHR images due to their quadratic complexity. The common practice of cropping large images into smaller patches leads to a significant loss of contextual information. To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in VHR remote sensing. RSM is designed to model global features of remote sensing images with linear complexity, enabling it to process large VHR images effectively. It employs an omnidirectional selective scan module to globally model the images in multiple directions, capturing large spatial features from various directions. Experiments on semantic segmentation and change detection tasks across various objects demonstrate the effectiveness of RSM. With simple model architecture and training approach, RSM achieves state-of-the-art performance on the dense prediction tasks of VHR remote sensing. The code for this work will be available at https://github.com/walking-shadow/Official_Remote_Sensing_Mamba.","sentences":["The spatial resolution of remote sensing images is becoming increasingly higher, posing challenges in handling large very-high-resolution (VHR)","remote sensing images for dense prediction tasks.","Models based on convolutional neural networks are limited in their ability to model global features of remote sensing images due to local convolution operations.","Transformer based models, despite their global modeling capabilities, face computational challenges with large VHR images due to their quadratic complexity.","The common practice of cropping large images into smaller patches leads to a significant loss of contextual information.","To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in VHR remote sensing.","RSM is designed to model global features of remote sensing images with linear complexity, enabling it to process large VHR images effectively.","It employs an omnidirectional selective scan module to globally model the images in multiple directions, capturing large spatial features from various directions.","Experiments on semantic segmentation and change detection tasks across various objects demonstrate the effectiveness of RSM.","With simple model architecture and training approach, RSM achieves state-of-the-art performance on the dense prediction tasks of VHR remote sensing.","The code for this work will be available at https://github.com/walking-shadow/Official_Remote_Sensing_Mamba."],"url":"http://arxiv.org/abs/2404.02668v1","category":"cs.CV"}
{"created":"2024-04-03 11:55:18","title":"Oberwolfach Report: Scalar Curvature Stability","abstract":"Although scalar curvature is the simplest curvature invariant, our understanding of scalar curvature has not matured to the same level as Ricci or sectional curvature. Despite this fact, many rigidity phenomenon have been established which give some of the strongest insights into scalar curvature. Important examples include Geroch's conjecture, the positive mass theorem, and Llarull's theorem. In order to further understand scalar curvature we ask corresponding geometric stability questions, where the hypotheses of the rigidity phenomenon are relaxed, and one would like to show that Riemannian manifolds which satisfy the relaxed conditions are close to the rigid objects in some topology. In this note we will survey what is known for scalar curvature stability, discuss what the questions are in this area, and introduce important tools which have been useful so far.","sentences":["Although scalar curvature is the simplest curvature invariant, our understanding of scalar curvature has not matured to the same level as Ricci or sectional curvature.","Despite this fact, many rigidity phenomenon have been established which give some of the strongest insights into scalar curvature.","Important examples include Geroch's conjecture, the positive mass theorem, and Llarull's theorem.","In order to further understand scalar curvature we ask corresponding geometric stability questions, where the hypotheses of the rigidity phenomenon are relaxed, and one would like to show that Riemannian manifolds which satisfy the relaxed conditions are close to the rigid objects in some topology.","In this note we will survey what is known for scalar curvature stability, discuss what the questions are in this area, and introduce important tools which have been useful so far."],"url":"http://arxiv.org/abs/2404.02662v1","category":"math.DG"}
{"created":"2024-04-03 11:49:43","title":"Adversarial Attacks and Dimensionality in Text Classifiers","abstract":"Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, adversarial attacks have been first identified and studied in the domain of image processing. In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks. We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effectiveness on models tuned with input samples with same embedding dimension. We utilize this sensitivity to design an adversarial defense mechanism. We use ensemble models of varying inherent dimensionality to thwart the attacks. This is tested on multiple datasets for its efficacy in providing robustness. We also study the problem of measuring adversarial perturbation using different distance metrics. For all of the aforementioned studies, we have run tests on multiple models with varying dimensionality and used a word-vector level adversarial attack to substantiate the findings.","sentences":["Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases.","They significantly undermine the ability of high-performance neural networks by forcing misclassifications.","These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it.","Historically, adversarial attacks have been first identified and studied in the domain of image processing.","In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks.","We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model.","Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effectiveness on models tuned with input samples with same embedding dimension.","We utilize this sensitivity to design an adversarial defense mechanism.","We use ensemble models of varying inherent dimensionality to thwart the attacks.","This is tested on multiple datasets for its efficacy in providing robustness.","We also study the problem of measuring adversarial perturbation using different distance metrics.","For all of the aforementioned studies, we have run tests on multiple models with varying dimensionality and used a word-vector level adversarial attack to substantiate the findings."],"url":"http://arxiv.org/abs/2404.02660v1","category":"cs.LG"}
{"created":"2024-04-03 11:47:20","title":"A Satellite Band Selection Framework for Amazon Forest Deforestation Detection Task","abstract":"The conservation of tropical forests is a topic of significant social and ecological relevance due to their crucial role in the global ecosystem. Unfortunately, deforestation and degradation impact millions of hectares annually, necessitating government or private initiatives for effective forest monitoring. This study introduces a novel framework that employs the Univariate Marginal Distribution Algorithm (UMDA) to select spectral bands from Landsat-8 satellite, optimizing the representation of deforested areas. This selection guides a semantic segmentation architecture, DeepLabv3+, enhancing its performance. Experimental results revealed several band compositions that achieved superior balanced accuracy compared to commonly adopted combinations for deforestation detection, utilizing segment classification via a Support Vector Machine (SVM). Moreover, the optimal band compositions identified by the UMDA-based approach improved the performance of the DeepLabv3+ architecture, surpassing state-of-the-art approaches compared in this study. The observation that a few selected bands outperform the total contradicts the data-driven paradigm prevalent in the deep learning field. Therefore, this suggests an exception to the conventional wisdom that 'more is always better'.","sentences":["The conservation of tropical forests is a topic of significant social and ecological relevance due to their crucial role in the global ecosystem.","Unfortunately, deforestation and degradation impact millions of hectares annually, necessitating government or private initiatives for effective forest monitoring.","This study introduces a novel framework that employs the Univariate Marginal Distribution Algorithm (UMDA) to select spectral bands from Landsat-8 satellite, optimizing the representation of deforested areas.","This selection guides a semantic segmentation architecture, DeepLabv3+, enhancing its performance.","Experimental results revealed several band compositions that achieved superior balanced accuracy compared to commonly adopted combinations for deforestation detection, utilizing segment classification via a Support Vector Machine (SVM).","Moreover, the optimal band compositions identified by the UMDA-based approach improved the performance of the DeepLabv3+ architecture, surpassing state-of-the-art approaches compared in this study.","The observation that a few selected bands outperform the total contradicts the data-driven paradigm prevalent in the deep learning field.","Therefore, this suggests an exception to the conventional wisdom that 'more is always better'."],"url":"http://arxiv.org/abs/2404.02659v1","category":"cs.CV"}
{"created":"2024-04-03 11:04:27","title":"Vacuum instability in QED with an asymmetric x-step. New example of exactly solvable case","abstract":"We present a new exactly solvable case in strong-field QED with one-dimensional step potential (x-step). The corresponding x-step is given by an analytic asymmetric with respect to the axis x reflection function. The step can be considered as a certain analytic \"deformation\" of the symmetric Sauter field. Moreover, it can be treated as a new regularization of the Klein step field. We study the vacuum instability caused by this x-step in the framework of a nonperturbative approach to strong-field QED. Exact solutions of the Dirac equation used in the corresponding nonperturbative calculations, are represented in the form of stationary plane waves with special left and right asymptotics and identified as components of initial and final wave packets of particles. We show that in spite of the fact that the symmetry with respect to positive and negative bands of energies is broken, distribution of created pairs and other physical quantities can be expressed via elementary functions. We consider the processes of transmission and reflection in the ranges of the stable vacuum and study physical quantities specifying the vacuum instability. We find the differential mean numbers of electron-positron pairs created from the vacuum, the components of current density and energy-momentum tensor of the created electrons and positrons leaving the area of the strong field under consideration. Besides, we study the particular case of the particle creation due to a weakly inhomogeneous electric field and obtain explicitly the total number, the current density and energy-momentum tensor of created particles. Unlike the symmetric case of the Sauter field the asymmetric form of the field under consideration causes the energy density and longitudinal pressure of created electrons to be not equal to the energy density and longitudinal pressure of created positrons.","sentences":["We present a new exactly solvable case in strong-field QED with one-dimensional step potential (x-step).","The corresponding x-step is given by an analytic asymmetric with respect to the axis x reflection function.","The step can be considered as a certain analytic \"deformation\" of the symmetric Sauter field.","Moreover, it can be treated as a new regularization of the Klein step field.","We study the vacuum instability caused by this x-step in the framework of a nonperturbative approach to strong-field QED.","Exact solutions of the Dirac equation used in the corresponding nonperturbative calculations, are represented in the form of stationary plane waves with special left and right asymptotics and identified as components of initial and final wave packets of particles.","We show that in spite of the fact that the symmetry with respect to positive and negative bands of energies is broken, distribution of created pairs and other physical quantities can be expressed via elementary functions.","We consider the processes of transmission and reflection in the ranges of the stable vacuum and study physical quantities specifying the vacuum instability.","We find the differential mean numbers of electron-positron pairs created from the vacuum, the components of current density and energy-momentum tensor of the created electrons and positrons leaving the area of the strong field under consideration.","Besides, we study the particular case of the particle creation due to a weakly inhomogeneous electric field and obtain explicitly the total number, the current density and energy-momentum tensor of created particles.","Unlike the symmetric case of the Sauter field the asymmetric form of the field under consideration causes the energy density and longitudinal pressure of created electrons to be not equal to the energy density and longitudinal pressure of created positrons."],"url":"http://arxiv.org/abs/2404.02640v1","category":"hep-th"}
{"created":"2024-04-03 10:52:23","title":"Sibyll$^{\\bigstar}$","abstract":"In the last decade, an increasing number of datasets have revealed a consistent discrepancy between the number of muons measured in ultra-high-energy extensive air showers (EAS) and the numbers predicted by simulations. This gap persists despite incorporating Large Hadron Collider (LHC) data into the tuning of current hadronic interaction models, leading to the phenomenon often termed the ''muon puzzle''. To gain a deeper understanding of the potential origins of this muon puzzle, we have developed Sibyll$^{\\bigstar}$, a series of phenomenologically modified versions of Sibyll 2.3d. In these models, we have increased muon production by altering $\\rho^0$, baryon-antibaryon pair, or kaon production in hadronic multiparticle production processes. These variants remain within bounds from provided by accelerator measurements, including those from the LHC and fixed-target experiments, notably NA49 and NA61, showing a level of consistency comparable to Sibyll 2.3d. Our findings show that these modifications can increase the muon count in EAS by up to 35%, while minimally affecting the depth of shower maximum ($X_{\\rm max}$) and other shower variables. Additionally, we assess the impact of these modifications on various observables, including inclusive muon and neutrino fluxes and the multiplicities of muon bundles in deep underground and water/ice Cherenkov detectors. We aim for at least one of these model variants to offer a more accurate representation of EAS data at the highest energies, thereby enhancing the quality of Monte Carlo predictions used in training neural networks. This improvement is crucial for achieving more reliable data analyses and interpretations.","sentences":["In the last decade, an increasing number of datasets have revealed a consistent discrepancy between the number of muons measured in ultra-high-energy extensive air showers (EAS) and the numbers predicted by simulations.","This gap persists despite incorporating Large Hadron Collider (LHC) data into the tuning of current hadronic interaction models, leading to the phenomenon often termed the ''muon puzzle''.","To gain a deeper understanding of the potential origins of this muon puzzle, we have developed Sibyll$^{\\bigstar}$, a series of phenomenologically modified versions of Sibyll 2.3d.","In these models, we have increased muon production by altering $\\rho^0$, baryon-antibaryon pair, or kaon production in hadronic multiparticle production processes.","These variants remain within bounds from provided by accelerator measurements, including those from the LHC and fixed-target experiments, notably NA49 and NA61, showing a level of consistency comparable to Sibyll 2.3d.","Our findings show that these modifications can increase the muon count in EAS by up to 35%, while minimally affecting the depth of shower maximum ($X_{\\rm max}$) and other shower variables.","Additionally, we assess the impact of these modifications on various observables, including inclusive muon and neutrino fluxes and the multiplicities of muon bundles in deep underground and water/ice Cherenkov detectors.","We aim for at least one of these model variants to offer a more accurate representation of EAS data at the highest energies, thereby enhancing the quality of Monte Carlo predictions used in training neural networks.","This improvement is crucial for achieving more reliable data analyses and interpretations."],"url":"http://arxiv.org/abs/2404.02636v1","category":"hep-ph"}
{"created":"2024-04-03 10:46:10","title":"An Inexact Regularized Proximal Newton Method without Line Search","abstract":"In this paper, we introduce an inexact regularized proximal Newton method (IRPNM) that does not require any line search. The method is designed to minimize the sum of a twice continuously differentiable function $f$ and a convex (possibly non-smooth and extended-valued) function $\\varphi$. Instead of controlling a step size by a line search procedure, we update the regularization parameter in a suitable way, based on the success of the previous iteration. The global convergence of the sequence of iterations and its superlinear convergence rate under a local H\\\"olderian error bound assumption are shown. Notably, these convergence results are obtained without requiring a global Lipschitz property for $ \\nabla f $, which, to the best of the authors' knowledge, is a novel contribution for proximal Newton methods. To highlight the efficiency of our approach, we provide numerical comparisons with an IRPNM using a line search globalization and a modern FISTA-type method.","sentences":["In this paper, we introduce an inexact regularized proximal Newton method (IRPNM) that does not require any line search.","The method is designed to minimize the sum of a twice continuously differentiable function $f$ and a convex (possibly non-smooth and extended-valued) function $\\varphi$. Instead of controlling a step size by a line search procedure, we update the regularization parameter in a suitable way, based on the success of the previous iteration.","The global convergence of the sequence of iterations and its superlinear convergence rate under a local H\\\"olderian error bound assumption are shown.","Notably, these convergence results are obtained without requiring a global Lipschitz property for $ \\nabla f $, which, to the best of the authors' knowledge, is a novel contribution for proximal Newton methods.","To highlight the efficiency of our approach, we provide numerical comparisons with an IRPNM using a line search globalization and a modern FISTA-type method."],"url":"http://arxiv.org/abs/2404.02635v1","category":"math.OC"}
{"created":"2024-04-03 10:37:56","title":"Two-Stage Super-Resolution Simulation Method for Three-Dimensional Flow Fields Around Buildings for Real-Time Prediction of Urban Micrometeorology","abstract":"A two-stage super-resolution simulation method is proposed for building-resolving micrometeorology simulations, which considerably reduces the computation time while maintaining accuracy. The first stage employs a convolutional neural network (CNN) to correct large-scale flows above buildings in the input of low-resolution (LR) simulation results. The second stage uses another CNN to reconstruct small-scale flows between buildings from the output of the first stage, resulting in high-resolution (HR) inferences. The CNNs are trained using HR simulation data for the second stage and their coarse-grained version for the first stage. This learning approach separates the flow scales to be inferred in each stage. The effectiveness of the proposed method was evaluated using micrometeorological simulations in an actual urban area around Tokyo Station in Japan. The super-resolution simulation successfully inferred HR atmospheric flows, reducing errors by about 50% for air temperature and 60\\% for wind velocity compared to the LR simulations. Furthermore, the two-stage approach allowed for localized HR inferences, reducing GPU memory usage to 12% during the training phase. The total wall-clock time for a 60-min prediction was reduced to about 9.92 min, which was approximately 3.2% (i.e., a 31-fold speedup) of the HR simulation time (309 min). The proposed method demonstrates the feasibility of real-time micrometeorology predictions in urban areas with a combination of physics-based and data-driven models.","sentences":["A two-stage super-resolution simulation method is proposed for building-resolving micrometeorology simulations, which considerably reduces the computation time while maintaining accuracy.","The first stage employs a convolutional neural network (CNN) to correct large-scale flows above buildings in the input of low-resolution (LR) simulation results.","The second stage uses another CNN to reconstruct small-scale flows between buildings from the output of the first stage, resulting in high-resolution (HR) inferences.","The CNNs are trained using HR simulation data for the second stage and their coarse-grained version for the first stage.","This learning approach separates the flow scales to be inferred in each stage.","The effectiveness of the proposed method was evaluated using micrometeorological simulations in an actual urban area around Tokyo Station in Japan.","The super-resolution simulation successfully inferred HR atmospheric flows, reducing errors by about 50% for air temperature and 60\\% for wind velocity compared to the LR simulations.","Furthermore, the two-stage approach allowed for localized HR inferences, reducing GPU memory usage to 12% during the training phase.","The total wall-clock time for a 60-min prediction was reduced to about 9.92 min, which was approximately 3.2% (i.e., a 31-fold speedup) of the HR simulation time (309 min).","The proposed method demonstrates the feasibility of real-time micrometeorology predictions in urban areas with a combination of physics-based and data-driven models."],"url":"http://arxiv.org/abs/2404.02631v1","category":"physics.ao-ph"}
{"created":"2024-04-03 10:08:55","title":"Neural Radiance Fields with Torch Units","abstract":"Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction methods widely used in industrial applications. Although prevalent methods achieve considerable improvements in small-scale scenes, accomplishing reconstruction in complex and large-scale scenes is still challenging. First, the background in complex scenes shows a large variance among different views. Second, the current inference pattern, $i.e.$, a pixel only relies on an individual camera ray, fails to capture contextual information. To solve these problems, we propose to enlarge the ray perception field and build up the sample points interactions. In this paper, we design a novel inference pattern that encourages a single camera ray possessing more contextual information, and models the relationship among sample points on each camera ray. To hold contextual information,a camera ray in our proposed method can render a patch of pixels simultaneously. Moreover, we replace the MLP in neural radiance field models with distance-aware convolutions to enhance the feature propagation among sample points from the same camera ray. To summarize, as a torchlight, a ray in our proposed method achieves rendering a patch of image. Thus, we call the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF show that the Torch-NeRF exhibits excellent performance.","sentences":["Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction methods widely used in industrial applications.","Although prevalent methods achieve considerable improvements in small-scale scenes, accomplishing reconstruction in complex and large-scale scenes is still challenging.","First, the background in complex scenes shows a large variance among different views.","Second, the current inference pattern, $i.e.$, a pixel only relies on an individual camera ray, fails to capture contextual information.","To solve these problems, we propose to enlarge the ray perception field and build up the sample points interactions.","In this paper, we design a novel inference pattern that encourages a single camera ray possessing more contextual information, and models the relationship among sample points on each camera ray.","To hold contextual information,a camera ray in our proposed method can render a patch of pixels simultaneously.","Moreover, we replace the MLP in neural radiance field models with distance-aware convolutions to enhance the feature propagation among sample points from the same camera ray.","To summarize, as a torchlight, a ray in our proposed method achieves rendering a patch of image.","Thus, we call the proposed method, Torch-NeRF.","Extensive experiments on KITTI-360 and LLFF show that the Torch-NeRF exhibits excellent performance."],"url":"http://arxiv.org/abs/2404.02617v1","category":"cs.CV"}
{"created":"2024-04-03 10:01:37","title":"Structures associated with the Borromean rings complement in the Poincar\u00e9 ball","abstract":"Guided by physical needs, we deal with the rotationally isotropic Poincar\\'e ball, when considering the complement of Borromean rings embedded in it. We consistently describe the geometry of the complement and realize the fundamental group as isometry subgroup in three dimensions. Applying this realization, we reveal normal stochastization and multifractal behavior within the examined model of directed random walks on the rooted Cayley tree, whose graphs are associated with dendritic polymers. According to Penner, we construct the Teichm\\\"uller space of the decorated ideal octahedral surface related to the quotient space of the fundamental group action. Using the conformality of decoration, we define six moduli and the mapping class group generated by cyclic permutations of the ideal vertices. Intending to quantize the geometric area, we state the connection between the induced geometry and the sine-Gordon model. Due to such a correspondence we obtain the differential two-form in the cotangent bundle.","sentences":["Guided by physical needs, we deal with the rotationally isotropic Poincar\\'e ball, when considering the complement of Borromean rings embedded in it.","We consistently describe the geometry of the complement and realize the fundamental group as isometry subgroup in three dimensions.","Applying this realization, we reveal normal stochastization and multifractal behavior within the examined model of directed random walks on the rooted Cayley tree, whose graphs are associated with dendritic polymers.","According to Penner, we construct the Teichm\\\"uller space of the decorated ideal octahedral surface related to the quotient space of the fundamental group action.","Using the conformality of decoration, we define six moduli and the mapping class group generated by cyclic permutations of the ideal vertices.","Intending to quantize the geometric area, we state the connection between the induced geometry and the sine-Gordon model.","Due to such a correspondence we obtain the differential two-form in the cotangent bundle."],"url":"http://arxiv.org/abs/2404.02615v1","category":"math-ph"}
{"created":"2024-04-03 10:01:23","title":"Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields","abstract":"Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination. To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable. In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and recurrent neural networks for prospective tumor growth prediction. In the proposed method, each tumor is represented as a signed distance function (SDF) conditioned on a low-dimensional latent code. Unlike previous studies that perform tumor shape prediction directly in the image space, we predict the latent codes instead and then reconstruct future shapes from it. To deal with irregular time intervals, we introduce a time-conditioned recurrent module based on a ConvLSTM and a novel temporal encoding strategy, which enables the proposed model to output varying tumor shapes over time. The experiments on an in-house longitudinal VS dataset showed that the proposed model significantly improved the performance ($\\ge 1.6\\%$ Dice score and $\\ge0.20$ mm 95\\% Hausdorff distance), in particular for top 20\\% tumors that grow or shrink the most ($\\ge 4.6\\%$ Dice score and $\\ge 0.73$ mm 95\\% Hausdorff distance). Our code is available at ~\\burl{https://github.com/cyjdswx/DeepGrowth}","sentences":["Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination.","To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable.","In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and recurrent neural networks for prospective tumor growth prediction.","In the proposed method, each tumor is represented as a signed distance function (SDF) conditioned on a low-dimensional latent code.","Unlike previous studies that perform tumor shape prediction directly in the image space, we predict the latent codes instead and then reconstruct future shapes from it.","To deal with irregular time intervals, we introduce a time-conditioned recurrent module based on a ConvLSTM and a novel temporal encoding strategy, which enables the proposed model to output varying tumor shapes over time.","The experiments on an in-house longitudinal VS dataset showed that the proposed model significantly improved the performance ($\\ge 1.6\\%$ Dice score and $\\ge0.20$ mm 95\\% Hausdorff distance), in particular for top 20\\% tumors that grow or shrink the most ($\\ge 4.6\\%$ Dice score and $\\ge 0.73$ mm 95\\% Hausdorff distance).","Our code is available at ~\\burl{https://github.com/cyjdswx/DeepGrowth}"],"url":"http://arxiv.org/abs/2404.02614v2","category":"eess.IV"}
{"created":"2024-04-03 09:55:51","title":"Some properties of a modified Hilbert transform","abstract":"Recently, Steinbach et al. introduced a novel operator $\\mathcal{H}_T: L^2(0,T) \\to L^2(0,T)$, known as the modified Hilbert transform. This operator has shown its significance in space-time formulations related to the heat and wave equations. In this paper, we establish a direct connection between the modified Hilbert transform $\\mathcal{H}_T$ and the canonical Hilbert transform $\\mathcal{H}$. Specifically, we prove the relationship $\\mathcal{H}_T \\varphi = -\\mathcal{H} \\tilde{\\varphi}$, where $\\varphi \\in L^2(0,T)$ and $\\tilde{\\varphi}$ is a suitable extension of $\\varphi$ over the entire $\\mathbb{R}$. By leveraging this crucial result, we derive some properties of $\\mathcal{H}_T$, including a new inversion formula, that emerge as immediate consequences of well-established findings on $\\mathcal{H}$.","sentences":["Recently, Steinbach et al. introduced a novel operator $\\mathcal{H}_T: L^2(0,T) \\to L^2(0,T)$, known as the modified Hilbert transform.","This operator has shown its significance in space-time formulations related to the heat and wave equations.","In this paper, we establish a direct connection between the modified Hilbert transform $\\mathcal{H}_T$ and the canonical Hilbert transform $\\mathcal{H}$. Specifically, we prove the relationship $\\mathcal{H}_T \\varphi = -\\mathcal{H} \\tilde{\\varphi}$, where $\\varphi \\in L^2(0,T)$ and $\\tilde{\\varphi}$ is a suitable extension of $\\varphi$ over the entire $\\mathbb{R}$. By leveraging this crucial result, we derive some properties of $\\mathcal{H}_T$, including a new inversion formula, that emerge as immediate consequences of well-established findings on $\\mathcal{H}$."],"url":"http://arxiv.org/abs/2404.02609v1","category":"math.CA"}
{"created":"2024-04-04 17:34:35","title":"Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior","abstract":"Much of the research in online moderation focuses on punitive actions. However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms. We extend this research by studying the \"creator heart\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given. We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users. We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time. We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users. We discuss avenues for extending our study to understanding positive signals from moderators on other platforms.","sentences":["Much of the research in online moderation focuses on punitive actions.","However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms.","We extend this research by studying the \"creator heart\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given.","We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users.","We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time.","We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users.","We discuss avenues for extending our study to understanding positive signals from moderators on other platforms."],"url":"http://arxiv.org/abs/2404.03612v1","category":"cs.HC"}
{"created":"2024-04-04 16:50:10","title":"PAC-learning of free-fermionic states is NP-hard","abstract":"Free-fermionic states, also known as matchgates or Gaussian states, are a fundamental class of quantum states due to their efficient classical simulability and their crucial role across various domains of Physics. With the advent of quantum devices, experiments now yield data from quantum states, including estimates of expectation values. We establish that deciding whether a given dataset, formed by a few Majorana correlation functions estimates, can be consistent with a free-fermionic state is an NP-complete problem. Our result also extends to datasets formed by estimates of Pauli expectation values. This is in stark contrast to the case of stabilizer states, where the analogous problem can be efficiently solved. Moreover, our results directly imply that free-fermionic states are computationally hard to properly PAC-learn, where PAC-learning of quantum states is a learning framework introduced by Aaronson. Remarkably, this is the first class of classically simulable quantum states shown to have this property.","sentences":["Free-fermionic states, also known as matchgates or Gaussian states, are a fundamental class of quantum states due to their efficient classical simulability and their crucial role across various domains of Physics.","With the advent of quantum devices, experiments now yield data from quantum states, including estimates of expectation values.","We establish that deciding whether a given dataset, formed by a few Majorana correlation functions estimates, can be consistent with a free-fermionic state is an NP-complete problem.","Our result also extends to datasets formed by estimates of Pauli expectation values.","This is in stark contrast to the case of stabilizer states, where the analogous problem can be efficiently solved.","Moreover, our results directly imply that free-fermionic states are computationally hard to properly PAC-learn, where PAC-learning of quantum states is a learning framework introduced by Aaronson.","Remarkably, this is the first class of classically simulable quantum states shown to have this property."],"url":"http://arxiv.org/abs/2404.03585v1","category":"quant-ph"}
{"created":"2024-04-04 16:48:40","title":"Towards more realistic human motion prediction with attention to motion coordination","abstract":"Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.","sentences":["Joint relation modeling is a curial component in human motion prediction.","Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned.","However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.","Thus, the final predicted motions usually appear unrealistic.","To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations.","Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned.","Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner.","Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction.","Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW."],"url":"http://arxiv.org/abs/2404.03584v1","category":"cs.CV"}
{"created":"2024-04-04 13:36:51","title":"Science, Technology, Engineering, and Mathematics Undergraduates' Knowledge and Interest in Quantum Careers: Barriers and Opportunities to Building a Diverse Quantum Workforce","abstract":"Efforts to build the workforce in support of the second quantum revolution are growing, including the creation of education programs that will prepare students for jobs in this area. We surveyed 186 undergraduate students with majors across the STEM disciplines and followed up with group interviews to understand their perspectives. The project was designed to understand what these STEM students know about quantum and quantum career opportunities and their level of interest in pursuing a career related to quantum. We found that most of the students know very little about quantum. Nevertheless, except for students in the life sciences, there was an interest in quantum careers. Across STEM majors, women were less likely to express interest in quantum careers than men, but this difference disappeared when we examined only physical and computer science majors. Of the few students who had knowledge of quantum concepts, most learned about this topic from online media, especially online videos. Some students reported learning about quantum in high school classes, where it was taught as an extension beyond the usual topics of the course. The undergraduate STEM students in our study identified multiple ways they would like to learn more about quantum, including short videos, seminars, courses, certificates, and degree programs.","sentences":["Efforts to build the workforce in support of the second quantum revolution are growing, including the creation of education programs that will prepare students for jobs in this area.","We surveyed 186 undergraduate students with majors across the STEM disciplines and followed up with group interviews to understand their perspectives.","The project was designed to understand what these STEM students know about quantum and quantum career opportunities and their level of interest in pursuing a career related to quantum.","We found that most of the students know very little about quantum.","Nevertheless, except for students in the life sciences, there was an interest in quantum careers.","Across STEM majors, women were less likely to express interest in quantum careers than men, but this difference disappeared when we examined only physical and computer science majors.","Of the few students who had knowledge of quantum concepts, most learned about this topic from online media, especially online videos.","Some students reported learning about quantum in high school classes, where it was taught as an extension beyond the usual topics of the course.","The undergraduate STEM students in our study identified multiple ways they would like to learn more about quantum, including short videos, seminars, courses, certificates, and degree programs."],"url":"http://arxiv.org/abs/2404.03439v1","category":"physics.ed-ph"}
{"created":"2024-04-04 12:38:14","title":"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?","abstract":"Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md .","sentences":["Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs.","Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input.","However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison.","Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies.","Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models.","We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs.","(2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models.","(3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods.","The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md ."],"url":"http://arxiv.org/abs/2404.03411v1","category":"cs.LG"}
{"created":"2024-04-04 11:53:37","title":"Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation","abstract":"In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.","sentences":["In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects.","On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination.","This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM.","The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels.","Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance."],"url":"http://arxiv.org/abs/2404.03394v1","category":"cs.CV"}
{"created":"2024-04-04 11:49:56","title":"Two Tricks to Improve Unsupervised Segmentation Learning","abstract":"We present two practical improvement techniques for unsupervised segmentation learning. These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods. Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs. Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme. This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other. Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques.","sentences":["We present two practical improvement techniques for unsupervised segmentation learning.","These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods.","Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs.","Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme.","This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other.","Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques."],"url":"http://arxiv.org/abs/2404.03392v1","category":"cs.CV"}
{"created":"2024-04-04 11:12:33","title":"Data harvesting vs data farming: A study of the importance of variation vs sample size in deep learning-based auto-segmentation for breast cancer patients","abstract":"The aim of this study was to investigate the difference in output, when training a model in three different scenarios: a large clinical delineated data set (with 700/78 patients for training/testing, from the Danish Breast Cancer Group (DBCG) RT Nation Study), a clinical but curated dataset (with 328/36 patients for training/testing, from the DBCG RT Nation Study) and a smaller, but dedicated data set created by delineation experts (with 123/14 patients for training/testing, consensus delineations created by delineation experts). The model performance was estimated based on the performance metrics dice similarity coefficient (DSC), Hausdorff 95th percentile (HD95) and mean surface distance (MSD). Models were tested in test sets from their own cohort, and afterwards also compared in the dedicated data test set. The difference between model output was finally estimated by measuring the mean width and cranial caudal length of the model output for the models. When testing the model output between the clinical models and the dedicated models in their own test set, the two clinical models had a poorer performance, than the dedicated models, but not all metrics showed statistically significance. When testing the models in the dedicated data, the dedicated model showed a slightly better performance, along with fewer segmentation outliers. As a way of taking advantage of the strength from both types of data set, it could be an option to use a large clinical data set as a baseline model, and then finetune with smaller sized cohorts with dedicated delineations.","sentences":["The aim of this study was to investigate the difference in output, when training a model in three different scenarios: a large clinical delineated data set (with 700/78 patients for training/testing, from the Danish Breast Cancer Group (DBCG) RT Nation Study), a clinical but curated dataset (with 328/36 patients for training/testing, from the DBCG RT Nation Study) and a smaller, but dedicated data set created by delineation experts (with 123/14 patients for training/testing, consensus delineations created by delineation experts).","The model performance was estimated based on the performance metrics dice similarity coefficient (DSC), Hausdorff 95th percentile (HD95) and mean surface distance (MSD).","Models were tested in test sets from their own cohort, and afterwards also compared in the dedicated data test set.","The difference between model output was finally estimated by measuring the mean width and cranial caudal length of the model output for the models.","When testing the model output between the clinical models and the dedicated models in their own test set, the two clinical models had a poorer performance, than the dedicated models, but not all metrics showed statistically significance.","When testing the models in the dedicated data, the dedicated model showed a slightly better performance, along with fewer segmentation outliers.","As a way of taking advantage of the strength from both types of data set, it could be an option to use a large clinical data set as a baseline model, and then finetune with smaller sized cohorts with dedicated delineations."],"url":"http://arxiv.org/abs/2404.03369v1","category":"physics.med-ph"}
{"created":"2024-04-04 07:09:43","title":"Multi-task learning via robust regularized clustering with non-convex group penalties","abstract":"Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection. The connection between the extended robust clustering and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data.","sentences":["Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks.","One natural assumption in MTL is that tasks are classified into clusters based on their characteristics.","However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks.","To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC).","MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties.","The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection.","The connection between the extended robust clustering and the multivariate M-estimator is also established.","This provides an interpretation of the robustness of MTLRRC against outlier tasks.","An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters.","The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data."],"url":"http://arxiv.org/abs/2404.03250v1","category":"stat.ME"}
{"created":"2024-04-04 07:07:34","title":"Learning Transferable Negative Prompts for Out-of-Distribution Detection","abstract":"Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate. To address this issue, we introduce a novel OOD detection method, named 'NegPrompt', to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images. It learns such negative prompts with ID data only, without any reliance on external outlier data. Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast, our learned negative prompts are transferable to novel class labels. Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at https://github.com/mala-lab/negprompt.","sentences":["Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate.","To address this issue, we introduce a novel OOD detection method, named 'NegPrompt', to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images.","It learns such negative prompts with ID data only, without any reliance on external outlier data.","Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training.","In contrast, our learned negative prompts are transferable to novel class labels.","Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios.","Code is available at https://github.com/mala-lab/negprompt."],"url":"http://arxiv.org/abs/2404.03248v1","category":"cs.CV"}
{"created":"2024-04-04 06:20:22","title":"FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification","abstract":"Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to adversarial attacks. Existing works improve robustness by training models on adversarial samples. However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks. In this paper, we propose FACTUAL, a novel Contrastive Learning framework for Adversarial Training and robust SAR classification. FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical adversarial attacks (such as OTSA) to build a supervised adversarial pre-training network. This network utilizes class labels for clustering clean and perturbed images together into a more informative feature space. (2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels. By pre-training and fine-tuning our model on both clean and adversarial samples, we show that our model achieves high prediction accuracy on both cases. Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods.","sentences":["Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to adversarial attacks.","Existing works improve robustness by training models on adversarial samples.","However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks.","In this paper, we propose FACTUAL, a novel Contrastive Learning framework for Adversarial Training and robust SAR classification.","FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical adversarial attacks (such as OTSA) to build a supervised adversarial pre-training network.","This network utilizes class labels for clustering clean and perturbed images together into a more informative feature space.","(2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels.","By pre-training and fine-tuning our model on both clean and adversarial samples, we show that our model achieves high prediction accuracy on both cases.","Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.03225v1","category":"cs.CV"}
{"created":"2024-04-04 04:16:31","title":"Classification of Nasopharyngeal Cases using DenseNet Deep Learning Architecture","abstract":"Nasopharyngeal carcinoma (NPC) is one of the understudied yet deadliest cancers in South East Asia. In Malaysia, the prevalence is identified mainly in Sarawak, among the ethnic of Bidayuh. NPC is often late-diagnosed because it is asymptomatic at the early stage. There are several tissue representations from the nasopharynx biopsy, such as nasopharyngeal inflammation (NPI), lymphoid hyperplasia (LHP), nasopharyngeal carcinoma (NPC) and normal tissue. This paper is our first initiative to identify the difference between NPC, NPI and normal cases. Seven whole slide images (WSIs) with gigapixel resolutions from seven different patients and two hospitals were experimented with using two test setups, consisting of a different set of images. The tissue regions are patched into smaller blocks and classified using DenseNet architecture with 21 dense layers. Two tests are carried out, each for proof of concept (Test 1) and real-test scenario (Test 2). The accuracy achieved for NPC class is 94.8% for Test 1 and 67.0% for Test 2.","sentences":["Nasopharyngeal carcinoma (NPC) is one of the understudied yet deadliest cancers in South East Asia.","In Malaysia, the prevalence is identified mainly in Sarawak, among the ethnic of Bidayuh.","NPC is often late-diagnosed because it is asymptomatic at the early stage.","There are several tissue representations from the nasopharynx biopsy, such as nasopharyngeal inflammation (NPI), lymphoid hyperplasia (LHP), nasopharyngeal carcinoma (NPC) and normal tissue.","This paper is our first initiative to identify the difference between NPC, NPI and normal cases.","Seven whole slide images (WSIs) with gigapixel resolutions from seven different patients and two hospitals were experimented with using two test setups, consisting of a different set of images.","The tissue regions are patched into smaller blocks and classified using DenseNet architecture with 21 dense layers.","Two tests are carried out, each for proof of concept (Test 1) and real-test scenario (Test 2).","The accuracy achieved for NPC class is 94.8% for Test 1 and 67.0% for Test 2."],"url":"http://arxiv.org/abs/2404.03188v1","category":"eess.IV"}
{"created":"2024-04-04 03:28:57","title":"UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization","abstract":"Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities. To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task. Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference. UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks.","sentences":["Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL).","Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content.","In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time.","UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities.","To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task.","Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference.","UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks."],"url":"http://arxiv.org/abs/2404.03179v1","category":"cs.CV"}
{"created":"2024-04-04 03:03:38","title":"Multi-modal Learning for WebAssembly Reverse Engineering","abstract":"The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering. Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools. Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data. Moreover, previous works overlook the high-level semantics present in source code and its documentation. Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering.   In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering. WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data. WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships. WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency. We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization. Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering.","sentences":["The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering.","Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools.","Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data.","Moreover, previous works overlook the high-level semantics present in source code and its documentation.","Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering.   ","In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering.","WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data.","WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships.","WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency.","We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization.","Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering."],"url":"http://arxiv.org/abs/2404.03171v1","category":"cs.SE"}
{"created":"2024-04-03 23:38:31","title":"Deep Learning-Based Weather-Related Power Outage Prediction with Socio-Economic and Power Infrastructure Data","abstract":"This paper presents a deep learning-based approach for hourly power outage probability prediction within census tracts encompassing a utility company's service territory. Two distinct deep learning models, conditional Multi-Layer Perceptron (MLP) and unconditional MLP, were developed to forecast power outage probabilities, leveraging a rich array of input features gathered from publicly available sources including weather data, weather station locations, power infrastructure maps, socio-economic and demographic statistics, and power outage records. Given a one-hour-ahead weather forecast, the models predict the power outage probability for each census tract, taking into account both the weather prediction and the location's characteristics. The deep learning models employed different loss functions to optimize prediction performance. Our experimental results underscore the significance of socio-economic factors in enhancing the accuracy of power outage predictions at the census tract level.","sentences":["This paper presents a deep learning-based approach for hourly power outage probability prediction within census tracts encompassing a utility company's service territory.","Two distinct deep learning models, conditional Multi-Layer Perceptron (MLP) and unconditional MLP, were developed to forecast power outage probabilities, leveraging a rich array of input features gathered from publicly available sources including weather data, weather station locations, power infrastructure maps, socio-economic and demographic statistics, and power outage records.","Given a one-hour-ahead weather forecast, the models predict the power outage probability for each census tract, taking into account both the weather prediction and the location's characteristics.","The deep learning models employed different loss functions to optimize prediction performance.","Our experimental results underscore the significance of socio-economic factors in enhancing the accuracy of power outage predictions at the census tract level."],"url":"http://arxiv.org/abs/2404.03115v1","category":"cs.LG"}
{"created":"2024-04-03 22:13:04","title":"Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot","abstract":"Towards addressing the Symbol Grounding Problem and motivated by early childhood language development, we leverage a robot which has been equipped with an approximate model of curiosity with particular focus on bottom-up building of unsupervised categories grounded in the physical world. That is, rather than starting with a top-down symbol (e.g., a word referring to an object) and providing meaning through the application of predetermined samples, the robot autonomously and gradually breaks up its exploration space into a series of increasingly specific unlabeled categories at which point an external expert may optionally provide a symbol association. We extend prior work by using a robot that can observe the visual world, introducing a higher dimensional sensory space, and using a more generalizable method of category building. Our experiments show that the robot learns categories based on actions and what it visually observes, and that those categories can be symbolically grounded into.https://info.arxiv.org/help/prep#comments","sentences":["Towards addressing the Symbol Grounding Problem and motivated by early childhood language development, we leverage a robot which has been equipped with an approximate model of curiosity with particular focus on bottom-up building of unsupervised categories grounded in the physical world.","That is, rather than starting with a top-down symbol (e.g., a word referring to an object) and providing meaning through the application of predetermined samples, the robot autonomously and gradually breaks up its exploration space into a series of increasingly specific unlabeled categories at which point an external expert may optionally provide a symbol association.","We extend prior work by using a robot that can observe the visual world, introducing a higher dimensional sensory space, and using a more generalizable method of category building.","Our experiments show that the robot learns categories based on actions and what it visually observes, and that those categories can be symbolically grounded into.https://info.arxiv.org/help/prep#comments"],"url":"http://arxiv.org/abs/2404.03092v1","category":"cs.CL"}
{"created":"2024-04-03 21:47:44","title":"Machine Learning and Data Analysis Using Posets: A Survey","abstract":"Posets are discrete mathematical structures which are ubiquitous in a broad range of data analysis and machine learning applications. Research connecting posets to the data science domain has been ongoing for many years. In this paper, a comprehensive review of a wide range of studies on data analysis amd machine learning using posets are examined in terms of their theory, algorithms and applications. In addition, the applied lattice theory domain of formal concept analysis will also be highlighted in terms of its machine learning applications.","sentences":["Posets are discrete mathematical structures which are ubiquitous in a broad range of data analysis and machine learning applications.","Research connecting posets to the data science domain has been ongoing for many years.","In this paper, a comprehensive review of a wide range of studies on data analysis amd machine learning using posets are examined in terms of their theory, algorithms and applications.","In addition, the applied lattice theory domain of formal concept analysis will also be highlighted in terms of its machine learning applications."],"url":"http://arxiv.org/abs/2404.03082v1","category":"cs.LG"}
{"created":"2024-04-03 21:29:40","title":"Mai Ho'om\u0101una i ka 'Ai: Language Models Improve Automatic Speech Recognition in Hawaiian","abstract":"In this paper we address the challenge of improving Automatic Speech Recognition (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper. To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text. We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data. As a baseline, we use Whisper without an external LM. Experimental results reveal a small but significant improvement in WER when ASR outputs are rescored with a Hawaiian LM. The results support leveraging all available data in the development of ASR systems for underrepresented languages.","sentences":["In this paper we address the challenge of improving Automatic Speech Recognition (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper.","To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text.","We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data.","As a baseline, we use Whisper without an external LM.","Experimental results reveal a small but significant improvement in WER when ASR outputs are rescored with a Hawaiian LM.","The results support leveraging all available data in the development of ASR systems for underrepresented languages."],"url":"http://arxiv.org/abs/2404.03073v1","category":"cs.CL"}
{"created":"2024-04-03 21:18:27","title":"Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion","abstract":"In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings. Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor. Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh. The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements. We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene geometry. The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces. The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces. As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes. We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction. 3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage.","sentences":["In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings.","Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor.","Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh.","The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements.","We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene geometry.","The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces.","The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces.","As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes.","We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction.","3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage."],"url":"http://arxiv.org/abs/2404.03070v1","category":"cs.CV"}
{"created":"2024-04-03 20:35:36","title":"GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification","abstract":"Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. Supervised and unsupervised learning are common approaches for designing text detoxification solutions. However, these methods necessitate fine-tuning, leading to computational overhead. In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences. To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA as benchmark detoxification datasets. Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets.","sentences":["Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms.","Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content.","Supervised and unsupervised learning are common approaches for designing text detoxification solutions.","However, these methods necessitate fine-tuning, leading to computational overhead.","In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo.","We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences.","To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES).","We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings.","We use ParaDetox and APPDIA as benchmark detoxification datasets.","Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA.","Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets."],"url":"http://arxiv.org/abs/2404.03052v1","category":"cs.CL"}
{"created":"2024-04-03 20:34:18","title":"ANOVA-boosting for Random Fourier Features","abstract":"We propose two algorithms for boosting random Fourier feature models for approximating high-dimensional functions. These methods utilize the classical and generalized analysis of variance (ANOVA) decomposition to learn low-order functions, where there are few interactions between the variables. Our algorithms are able to find an index set of important input variables and variable interactions reliably. Furthermore, we generalize already existing random Fourier feature models to an ANOVA setting, where terms of different order can be used. Our algorithms have the advantage of interpretability, meaning that the influence of every input variable is known in the learned model, even for dependent input variables. We give theoretical as well as numerical results that our algorithms perform well for sensitivity analysis. The ANOVA-boosting step reduces the approximation error of existing methods significantly.","sentences":["We propose two algorithms for boosting random Fourier feature models for approximating high-dimensional functions.","These methods utilize the classical and generalized analysis of variance (ANOVA) decomposition to learn low-order functions, where there are few interactions between the variables.","Our algorithms are able to find an index set of important input variables and variable interactions reliably.","Furthermore, we generalize already existing random Fourier feature models to an ANOVA setting, where terms of different order can be used.","Our algorithms have the advantage of interpretability, meaning that the influence of every input variable is known in the learned model, even for dependent input variables.","We give theoretical as well as numerical results that our algorithms perform well for sensitivity analysis.","The ANOVA-boosting step reduces the approximation error of existing methods significantly."],"url":"http://arxiv.org/abs/2404.03050v1","category":"cs.LG"}
{"created":"2024-04-03 20:29:40","title":"Decentralised Moderation for Interoperable Social Networks: A Conversation-based Approach for Pleroma and the Fediverse","abstract":"The recent development of decentralised and interoperable social networks (such as the \"fediverse\") creates new challenges for content moderators. This is because millions of posts generated on one server can easily \"spread\" to another, even if the recipient server has very different moderation policies. An obvious solution would be to leverage moderation tools to automatically tag (and filter) posts that contravene moderation policies, e.g. related to toxic speech. Recent work has exploited the conversational context of a post to improve this automatic tagging, e.g. using the replies to a post to help classify if it contains toxic speech. This has shown particular potential in environments with large training sets that contain complete conversations. This, however, creates challenges in a decentralised context, as a single conversation may be fragmented across multiple servers. Thus, each server only has a partial view of an entire conversation because conversations are often federated across servers in a non-synchronized fashion. To address this, we propose a decentralised conversation-aware content moderation approach suitable for the fediverse. Our approach employs a graph deep learning model (GraphNLI) trained locally on each server. The model exploits local data to train a model that combines post and conversational information captured through random walks to detect toxicity. We evaluate our approach with data from Pleroma, a major decentralised and interoperable micro-blogging network containing 2 million conversations. Our model effectively detects toxicity on larger instances, exclusively trained using their local post information (0.8837 macro-F1). Our approach has considerable scope to improve moderation in decentralised and interoperable social networks such as Pleroma or Mastodon.","sentences":["The recent development of decentralised and interoperable social networks (such as the \"fediverse\") creates new challenges for content moderators.","This is because millions of posts generated on one server can easily \"spread\" to another, even if the recipient server has very different moderation policies.","An obvious solution would be to leverage moderation tools to automatically tag (and filter) posts that contravene moderation policies, e.g. related to toxic speech.","Recent work has exploited the conversational context of a post to improve this automatic tagging, e.g. using the replies to a post to help classify if it contains toxic speech.","This has shown particular potential in environments with large training sets that contain complete conversations.","This, however, creates challenges in a decentralised context, as a single conversation may be fragmented across multiple servers.","Thus, each server only has a partial view of an entire conversation because conversations are often federated across servers in a non-synchronized fashion.","To address this, we propose a decentralised conversation-aware content moderation approach suitable for the fediverse.","Our approach employs a graph deep learning model (GraphNLI) trained locally on each server.","The model exploits local data to train a model that combines post and conversational information captured through random walks to detect toxicity.","We evaluate our approach with data from Pleroma, a major decentralised and interoperable micro-blogging network containing 2 million conversations.","Our model effectively detects toxicity on larger instances, exclusively trained using their local post information (0.8837 macro-F1).","Our approach has considerable scope to improve moderation in decentralised and interoperable social networks such as Pleroma or Mastodon."],"url":"http://arxiv.org/abs/2404.03048v1","category":"cs.CY"}
{"created":"2024-04-03 20:04:44","title":"AWOL: Analysis WithOut synthesis using Language","abstract":"Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters. For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model. Our key idea is to leverage language to control such existing models to produce novel shapes. This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which we do using a small set of shape and text pairs. Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training. If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes. We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees). We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training. In particular, we demonstrate state-of-the-art shape estimation of 3D dogs. This work also constitutes the first language-driven method for generating 3D trees. Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images.","sentences":["Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters.","For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model.","Our key idea is to leverage language to control such existing models to produce novel shapes.","This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which we do using a small set of shape and text pairs.","Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training.","If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes.","We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees).","We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training.","In particular, we demonstrate state-of-the-art shape estimation of 3D dogs.","This work also constitutes the first language-driven method for generating 3D trees.","Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images."],"url":"http://arxiv.org/abs/2404.03042v1","category":"cs.CV"}
{"created":"2024-04-03 19:20:57","title":"General Effect Modelling (GEM) -- Part 1. Method description","abstract":"We present a flexible tool, called General Effect Modelling (GEM), for the analysis of any multivariate data influenced by one or more qualitative (categorical) or quantitative (continuous) input variables. The variables can be design factors or observed values, e.g., age, sex, or income, or they may represent subgroups of the samples found by data exploration. The first step in GEM separates the variation in the multivariate data into effect matrices associated with each of the influencing variables (and possibly interactions between these) by applying a general linear model. The residuals of the model are added to each of the effect matrices and the results are called Effect plus Residual (ER) values. The tables of ER values have the same dimensions as the original multivariate data. The second step of GEM is a multi- or univariate exploration of the ER tables to learn more about the multivariate data in relation to each input variables. The exploration is simplified as it addresses one input variable at the time, or if preferred, a combination of input variables. One example is a study to identify molecular fingerprints associated with a disease that is not influenced by age where individuals at different ages with and without the disease are included in the experiment. This situation can be described as an experiment with two input variables: the targeted disease and the individual age. Through GEM, the effect of age can be removed, thus focusing further analysis on the targeted disease without the influence of the confounding effect of age. ER values can also be the combined effect of several input variables. This publication has three parts: the first part describes the GEM methodology, Part 2 is a consideration of multivariate data and the benefits of treating such data by multivariate analysis, with a focus on omics data, and Part 3 is a case study in Multiple Sclerosis (MS).","sentences":["We present a flexible tool, called General Effect Modelling (GEM), for the analysis of any multivariate data influenced by one or more qualitative (categorical) or quantitative (continuous) input variables.","The variables can be design factors or observed values, e.g., age, sex, or income, or they may represent subgroups of the samples found by data exploration.","The first step in GEM separates the variation in the multivariate data into effect matrices associated with each of the influencing variables (and possibly interactions between these) by applying a general linear model.","The residuals of the model are added to each of the effect matrices and the results are called Effect plus Residual (ER) values.","The tables of ER values have the same dimensions as the original multivariate data.","The second step of GEM is a multi- or univariate exploration of the ER tables to learn more about the multivariate data in relation to each input variables.","The exploration is simplified as it addresses one input variable at the time, or if preferred, a combination of input variables.","One example is a study to identify molecular fingerprints associated with a disease that is not influenced by age where individuals at different ages with and without the disease are included in the experiment.","This situation can be described as an experiment with two input variables: the targeted disease and the individual age.","Through GEM, the effect of age can be removed, thus focusing further analysis on the targeted disease without the influence of the confounding effect of age.","ER values can also be the combined effect of several input variables.","This publication has three parts: the first part describes the GEM methodology, Part 2 is a consideration of multivariate data and the benefits of treating such data by multivariate analysis, with a focus on omics data, and Part 3 is a case study in Multiple Sclerosis (MS)."],"url":"http://arxiv.org/abs/2404.03024v1","category":"stat.ME"}
{"created":"2024-04-03 19:17:43","title":"BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes","abstract":"Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion. Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes. To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result. Our best model utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as the text encoder and CLIP as the image encoder. It outperforms the baseline by a large margin in all 12 subtasks. In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance. The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders. This highlights the potential for improving abstract visual semantics encoding.","sentences":["Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion.","Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes.","To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result.","Our best model utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as the text encoder and CLIP as the image encoder.","It outperforms the baseline by a large margin in all 12 subtasks.","In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance.","The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders.","This highlights the potential for improving abstract visual semantics encoding."],"url":"http://arxiv.org/abs/2404.03022v1","category":"cs.CL"}
{"created":"2024-04-03 18:50:14","title":"Spectral Clustering in Convex and Constrained Settings","abstract":"Spectral clustering methods have gained widespread recognition for their effectiveness in clustering high-dimensional data. Among these techniques, constrained spectral clustering has emerged as a prominent approach, demonstrating enhanced performance by integrating pairwise constraints. However, the application of such constraints to semidefinite spectral clustering, a variant that leverages semidefinite programming to optimize clustering objectives, remains largely unexplored. In this paper, we introduce a novel framework for seamlessly integrating pairwise constraints into semidefinite spectral clustering. Our methodology systematically extends the capabilities of semidefinite spectral clustering to capture complex data structures, thereby addressing real-world clustering challenges more effectively. Additionally, we extend this framework to encompass both active and self-taught learning scenarios, further enhancing its versatility and applicability. Empirical studies conducted on well-known datasets demonstrate the superiority of our proposed framework over existing spectral clustering methods, showcasing its robustness and scalability across diverse datasets and learning settings. By bridging the gap between constrained learning and semidefinite spectral clustering, our work contributes to the advancement of spectral clustering techniques, offering researchers and practitioners a versatile tool for addressing complex clustering challenges in various real-world applications. Access to the data, code, and experimental results is provided for further exploration (https://github.com/swarupbehera/SCCCS).","sentences":["Spectral clustering methods have gained widespread recognition for their effectiveness in clustering high-dimensional data.","Among these techniques, constrained spectral clustering has emerged as a prominent approach, demonstrating enhanced performance by integrating pairwise constraints.","However, the application of such constraints to semidefinite spectral clustering, a variant that leverages semidefinite programming to optimize clustering objectives, remains largely unexplored.","In this paper, we introduce a novel framework for seamlessly integrating pairwise constraints into semidefinite spectral clustering.","Our methodology systematically extends the capabilities of semidefinite spectral clustering to capture complex data structures, thereby addressing real-world clustering challenges more effectively.","Additionally, we extend this framework to encompass both active and self-taught learning scenarios, further enhancing its versatility and applicability.","Empirical studies conducted on well-known datasets demonstrate the superiority of our proposed framework over existing spectral clustering methods, showcasing its robustness and scalability across diverse datasets and learning settings.","By bridging the gap between constrained learning and semidefinite spectral clustering, our work contributes to the advancement of spectral clustering techniques, offering researchers and practitioners a versatile tool for addressing complex clustering challenges in various real-world applications.","Access to the data, code, and experimental results is provided for further exploration (https://github.com/swarupbehera/SCCCS)."],"url":"http://arxiv.org/abs/2404.03012v1","category":"cs.LG"}
{"created":"2024-04-03 18:42:19","title":"Skeleton Recall Loss for Connectivity Conserving and Resource Efficient Segmentation of Thin Tubular Structures","abstract":"Accurately segmenting thin tubular structures, such as vessels, nerves, roads or concrete cracks, is a crucial task in computer vision. Standard deep learning-based segmentation loss functions, such as Dice or Cross-Entropy, focus on volumetric overlap, often at the expense of preserving structural connectivity or topology. This can lead to segmentation errors that adversely affect downstream tasks, including flow calculation, navigation, and structural inspection. Although current topology-focused losses mark an improvement, they introduce significant computational and memory overheads. This is particularly relevant for 3D data, rendering these losses infeasible for larger volumes as well as increasingly important multi-class segmentation problems. To mitigate this, we propose a novel Skeleton Recall Loss, which effectively addresses these challenges by circumventing intensive GPU-based calculations with inexpensive CPU operations. It demonstrates overall superior performance to current state-of-the-art approaches on five public datasets for topology-preserving segmentation, while substantially reducing computational overheads by more than 90%. In doing so, we introduce the first multi-class capable loss function for thin structure segmentation, excelling in both efficiency and efficacy for topology-preservation.","sentences":["Accurately segmenting thin tubular structures, such as vessels, nerves, roads or concrete cracks, is a crucial task in computer vision.","Standard deep learning-based segmentation loss functions, such as Dice or Cross-Entropy, focus on volumetric overlap, often at the expense of preserving structural connectivity or topology.","This can lead to segmentation errors that adversely affect downstream tasks, including flow calculation, navigation, and structural inspection.","Although current topology-focused losses mark an improvement, they introduce significant computational and memory overheads.","This is particularly relevant for 3D data, rendering these losses infeasible for larger volumes as well as increasingly important multi-class segmentation problems.","To mitigate this, we propose a novel Skeleton Recall Loss, which effectively addresses these challenges by circumventing intensive GPU-based calculations with inexpensive CPU operations.","It demonstrates overall superior performance to current state-of-the-art approaches on five public datasets for topology-preserving segmentation, while substantially reducing computational overheads by more than 90%.","In doing so, we introduce the first multi-class capable loss function for thin structure segmentation, excelling in both efficiency and efficacy for topology-preservation."],"url":"http://arxiv.org/abs/2404.03010v1","category":"eess.IV"}
{"created":"2024-04-03 18:09:33","title":"Towards a Fully Interpretable and More Scalable RSA Model for Metaphor Understanding","abstract":"The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic reasoning in computational terms. However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability. Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods. The model was tested against 24 metaphors, not limited to the conventional $\\textit{John-is-a-shark}$ type. Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept. Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing Large Language Models interpretability. Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines.","sentences":["The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic reasoning in computational terms.","However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability.","Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods.","The model was tested against 24 metaphors, not limited to the conventional $\\textit{John-is-a-shark}$ type.","Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept.","Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing Large Language Models interpretability.","Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines."],"url":"http://arxiv.org/abs/2404.02983v1","category":"cs.CL"}
{"created":"2024-04-03 17:54:37","title":"Deep Image Composition Meets Image Forgery","abstract":"Image forgery is a topic that has been studied for many years. Before the breakthrough of deep learning, forged images were detected using handcrafted features that did not require training. These traditional methods failed to perform satisfactorily even on datasets much worse in quality than real-life image manipulations. Advances in deep learning have impacted image forgery detection as much as they have impacted other areas of computer vision and have improved the state of the art. Deep learning models require large amounts of labeled data for training. In the case of image forgery, labeled data at the pixel level is a very important factor for the models to learn. None of the existing datasets have sufficient size, realism and pixel-level labeling at the same time. This is due to the high cost of producing and labeling quality images. It can take hours for an image editing expert to manipulate just one image. To bridge this gap, we automate data generation using image composition techniques that are very related to image forgery. Unlike other automated data generation frameworks, we use state of the art image composition deep learning models to generate spliced images close to the quality of real-life manipulations. Finally, we test the generated dataset on the SOTA image manipulation detection model and show that its prediction performance is lower compared to existing datasets, i.e. we produce realistic images that are more difficult to detect. Dataset will be available at https://github.com/99eren99/DIS25k .","sentences":["Image forgery is a topic that has been studied for many years.","Before the breakthrough of deep learning, forged images were detected using handcrafted features that did not require training.","These traditional methods failed to perform satisfactorily even on datasets much worse in quality than real-life image manipulations.","Advances in deep learning have impacted image forgery detection as much as they have impacted other areas of computer vision and have improved the state of the art.","Deep learning models require large amounts of labeled data for training.","In the case of image forgery, labeled data at the pixel level is a very important factor for the models to learn.","None of the existing datasets have sufficient size, realism and pixel-level labeling at the same time.","This is due to the high cost of producing and labeling quality images.","It can take hours for an image editing expert to manipulate just one image.","To bridge this gap, we automate data generation using image composition techniques that are very related to image forgery.","Unlike other automated data generation frameworks, we use state of the art image composition deep learning models to generate spliced images close to the quality of real-life manipulations.","Finally, we test the generated dataset on the SOTA image manipulation detection model and show that its prediction performance is lower compared to existing datasets, i.e. we produce realistic images that are more difficult to detect.","Dataset will be available at https://github.com/99eren99/DIS25k ."],"url":"http://arxiv.org/abs/2404.02897v1","category":"cs.CV"}
{"created":"2024-04-03 17:38:15","title":"PoCo: Point Context Cluster for RGBD Indoor Place Recognition","abstract":"We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database. The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors. We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning. Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation. We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%). In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment.","sentences":["We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database.","The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors.","We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning.","Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation.","We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively.","PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%).","In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment."],"url":"http://arxiv.org/abs/2404.02885v1","category":"cs.CV"}
{"created":"2024-04-03 17:09:25","title":"Gaussian Process Regression with Soft Inequality and Monotonicity Constraints","abstract":"Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems.","sentences":["Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models.","Standard GP regression can lead to an unbounded model in which some points can take infeasible values.","We introduce a new GP method that enforces the physical constraints in a probabilistic manner.","This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC).","QHMC is an efficient way to sample from a broad class of distributions.","Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution.","Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model.","According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems."],"url":"http://arxiv.org/abs/2404.02873v1","category":"stat.ML"}
{"created":"2024-04-03 16:37:38","title":"An Information Bottleneck Approach for Markov Model Construction","abstract":"Markov state models (MSMs) are valuable for studying dynamics of protein conformational changes via statistical analysis of molecular dynamics (MD) simulations. In MSMs, the complex configuration space is coarse-grained into conformational states, with the dynamics modeled by a series of Markovian transitions among these states at discrete lag times. Constructing the Markovian model at a specific lag time requires state defined without significant internal energy barriers, enabling internal dynamics relaxation within the lag time. This process coarse grains time and space, integrating out rapid motions within metastable states. This work introduces a continuous embedding approach for molecular conformations using the state predictive information bottleneck (SPIB), which unifies dimensionality reduction and state space partitioning via a continuous, machine learned basis set. Without explicit optimization of VAMP-based scores, SPIB demonstrates state-of-the-art performance in identifying slow dynamical processes and constructing predictive multi-resolution Markovian models. When applied to mini-proteins trajectories, SPIB showcases unique advantages compared to competing methods. It automatically adjusts the number of metastable states based on a specified minimal time resolution, eliminating the need for manual tuning. While maintaining efficacy in dynamical properties, SPIB excels in accurately distinguishing metastable states and capturing numerous well-populated macrostates. Furthermore, SPIB's ability to learn a low-dimensional continuous embedding of the underlying MSMs enhances the interpretation of dynamic pathways. Accordingly, we propose SPIB as an easy-to-implement methodology for end-to-end MSM construction.","sentences":["Markov state models (MSMs) are valuable for studying dynamics of protein conformational changes via statistical analysis of molecular dynamics (MD) simulations.","In MSMs, the complex configuration space is coarse-grained into conformational states, with the dynamics modeled by a series of Markovian transitions among these states at discrete lag times.","Constructing the Markovian model at a specific lag time requires state defined without significant internal energy barriers, enabling internal dynamics relaxation within the lag time.","This process coarse grains time and space, integrating out rapid motions within metastable states.","This work introduces a continuous embedding approach for molecular conformations using the state predictive information bottleneck (SPIB), which unifies dimensionality reduction and state space partitioning via a continuous, machine learned basis set.","Without explicit optimization of VAMP-based scores, SPIB demonstrates state-of-the-art performance in identifying slow dynamical processes and constructing predictive multi-resolution Markovian models.","When applied to mini-proteins trajectories, SPIB showcases unique advantages compared to competing methods.","It automatically adjusts the number of metastable states based on a specified minimal time resolution, eliminating the need for manual tuning.","While maintaining efficacy in dynamical properties, SPIB excels in accurately distinguishing metastable states and capturing numerous well-populated macrostates.","Furthermore, SPIB's ability to learn a low-dimensional continuous embedding of the underlying MSMs enhances the interpretation of dynamic pathways.","Accordingly, we propose SPIB as an easy-to-implement methodology for end-to-end MSM construction."],"url":"http://arxiv.org/abs/2404.02856v1","category":"physics.bio-ph"}
{"created":"2024-04-03 16:10:17","title":"\"Are Adversarial Phishing Webpages a Threat in Reality?\" Understanding the Users' Perception of Adversarial Webpages","abstract":"Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation. Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages. However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing -- the end users. In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD. Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t. unperturbed ones. However, not all adversarial perturbations are equally effective. For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background). We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence. We release our resources.","sentences":["Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation.","Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages.","However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing -- the end users.","In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD.","Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t.","unperturbed ones.","However, not all adversarial perturbations are equally effective.","For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background).","We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence.","We release our resources."],"url":"http://arxiv.org/abs/2404.02832v1","category":"cs.CR"}
{"created":"2024-04-03 15:59:42","title":"BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models","abstract":"This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is available at https://github.com/Ledzy/BAdam.","sentences":["This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver.","BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property.","Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU.","The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO.","Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO.","Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark.","The results demonstrate that BAdam is capable of narrowing the performance gap with Adam.","Our code is available at https://github.com/Ledzy/BAdam."],"url":"http://arxiv.org/abs/2404.02827v1","category":"cs.LG"}
{"created":"2024-04-03 14:56:06","title":"GenN2N: Generative NeRF2NeRF Translation","abstract":"We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc. Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space. Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs. To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs. The latent space is trained to align with a Gaussian distribution and the NeRFs are supervised through an adversarial loss on its renderings. To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a contrastive learning scheme. Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power. More results on our project page: https://xiangyueliu.github.io/GenN2N/","sentences":["We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc.","Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space.","Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs.","To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs.","The latent space is trained to align with a Gaussian distribution and the NeRFs are supervised through an adversarial loss on its renderings.","To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a contrastive learning scheme.","Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power.","More results on our project page: https://xiangyueliu.github.io/GenN2N/"],"url":"http://arxiv.org/abs/2404.02788v1","category":"cs.CV"}
{"created":"2024-04-04 15:44:52","title":"On the penalization by the perimeter in shape optimization applied to Dirichlet inverse obstacle problem","abstract":"This paper is devoted to the understanding of regularisation process in the shape optimization approach to the so-called Dirichlet inverse obstacle problem for elliptic operators. More precisely, we study two different regularisations of the very classical shape optimization approach consisting in minimizing a mismatched functional. The first one is an implicit regularisation when working in the class of inclusion having a uniform $\\varepsilon$-cone property, a natural class in shape optimization. As this regularity is not trivial to guarantee numerically, we discuss the regularisation by perimeter penalization. We show that this second regularisation provides a stability gain in the minimization process.","sentences":["This paper is devoted to the understanding of regularisation process in the shape optimization approach to the so-called Dirichlet inverse obstacle problem for elliptic operators.","More precisely, we study two different regularisations of the very classical shape optimization approach consisting in minimizing a mismatched functional.","The first one is an implicit regularisation when working in the class of inclusion having a uniform $\\varepsilon$-cone property, a natural class in shape optimization.","As this regularity is not trivial to guarantee numerically, we discuss the regularisation by perimeter penalization.","We show that this second regularisation provides a stability gain in the minimization process."],"url":"http://arxiv.org/abs/2404.03536v1","category":"math.OC"}
{"created":"2024-04-04 15:05:50","title":"A QAOA approach with fake devices: A case study for the maximum cut in ring graphs","abstract":"The quantum approximate optimization algorithm (QAOA) can require considerable processing time for developers to test and debug their codes on expensive quantum devices. One avenue to circumvent this difficulty is to use the error maps of quantum devices, where a local simulator can be automatically configured to mimic an actual device backend. In our work, we evaluated some error maps of quantum devices, known as fake devices, that are freely available in the cloud. The QAOA and the problem of maximum cut in 2-regular connected graphs, known as ring of disagrees, were used as tools for the noise analysis. The approximation ratio, the expectation energy and the probability of success for this problem have been evaluated in two scenarios. First, the quantities were studied through noisy simulations using fake devices. Second, error mitigation methods such as optimization levels and translation (connectivity mapping) of the original problem were applied. These results were then compared with the analytical solution of the ring graph. The study shows that error mitigation methods were crucial in obtaining better results for the expectation value of the energy, the approximation ratio, and the probability of success for the ring graphs.","sentences":["The quantum approximate optimization algorithm (QAOA) can require considerable processing time for developers to test and debug their codes on expensive quantum devices.","One avenue to circumvent this difficulty is to use the error maps of quantum devices, where a local simulator can be automatically configured to mimic an actual device backend.","In our work, we evaluated some error maps of quantum devices, known as fake devices, that are freely available in the cloud.","The QAOA and the problem of maximum cut in 2-regular connected graphs, known as ring of disagrees, were used as tools for the noise analysis.","The approximation ratio, the expectation energy and the probability of success for this problem have been evaluated in two scenarios.","First, the quantities were studied through noisy simulations using fake devices.","Second, error mitigation methods such as optimization levels and translation (connectivity mapping) of the original problem were applied.","These results were then compared with the analytical solution of the ring graph.","The study shows that error mitigation methods were crucial in obtaining better results for the expectation value of the energy, the approximation ratio, and the probability of success for the ring graphs."],"url":"http://arxiv.org/abs/2404.03501v1","category":"quant-ph"}
{"created":"2024-04-04 13:46:49","title":"Radiative corrections to the spin asymmetry in elastic polarized electron - nucleus collisions at high energy","abstract":"Improving the numerical precision, dispersion corrections to the beam-normal spin asymmetry which arise from the dominant nuclear excitations, are estimated up to a collision energy of 1 GeV. A nonperturbative calculation of vacuum polarization and the vertex plus self-energy correction, using optimized potentials, indicates that for small scattering angles both these quantum electrodynamical (QED) effects on the spin asymmetry decrease with energy above 200 MeV and can mostly be neglected at high energies. Examples are given for the 12C and 208Pb nuclei. While our results disagree with the measured high-energy spin asymmetry for 12C, they are able to explain the data on 208Pb near 1 GeV.","sentences":["Improving the numerical precision, dispersion corrections to the beam-normal spin asymmetry which arise from the dominant nuclear excitations, are estimated up to a collision energy of 1 GeV. A nonperturbative calculation of vacuum polarization and the vertex plus self-energy correction, using optimized potentials, indicates that for small scattering angles both these quantum electrodynamical (QED) effects on the spin asymmetry decrease with energy above 200 MeV and can mostly be neglected at high energies.","Examples are given for the 12C and 208Pb nuclei.","While our results disagree with the measured high-energy spin asymmetry for 12C, they are able to explain the data on 208Pb near 1 GeV."],"url":"http://arxiv.org/abs/2404.03445v1","category":"nucl-th"}
{"created":"2024-04-04 13:38:42","title":"Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity","abstract":"This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks. The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers. The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link. To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC. Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target. Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC. To efficiently quantize the signal samples, a Karhunen-Lo\\`eve Transform coding scheme is employed. Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization. Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC.","sentences":["This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks.","The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers.","The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link.","To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC.","Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target.","Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC.","To efficiently quantize the signal samples, a Karhunen-Lo\\`eve Transform coding scheme is employed.","Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization.","Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC."],"url":"http://arxiv.org/abs/2404.03440v1","category":"cs.IT"}
{"created":"2024-04-04 12:49:42","title":"Future Predictive Success-or-Failure Classification for Long-Horizon Robotic Tasks","abstract":"Automating long-horizon tasks with a robotic arm has been a central research topic in robotics. Optimization-based action planning is an efficient approach for creating an action plan to complete a given task. Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects. The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually. To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically. The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions. The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan. This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution. The regularization term improves future prediction and classification performance. The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments.","sentences":["Automating long-horizon tasks with a robotic arm has been a central research topic in robotics.","Optimization-based action planning is an efficient approach for creating an action plan to complete a given task.","Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects.","The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually.","To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically.","The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions.","The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan.","This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution.","The regularization term improves future prediction and classification performance.","The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments."],"url":"http://arxiv.org/abs/2404.03415v1","category":"cs.RO"}
{"created":"2024-04-04 10:45:07","title":"Towards Pareto Optimal Throughput in Small Language Model Serving","abstract":"Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.","sentences":["Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks.","Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance.","In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels.","Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator.","In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs."],"url":"http://arxiv.org/abs/2404.03353v1","category":"cs.CL"}
{"created":"2024-04-04 09:22:18","title":"Spatio-Spectral Structure Tensor Total Variation for Hyperspectral Image Denoising and Destriping","abstract":"This paper proposes a novel regularization method, named Spatio-Spectral Structure Tensor Total Variation (S3TTV), for denoising and destriping of hyperspectral (HS) images. HS images are inevitably contaminated by various types of noise, during acquisition process, due to the measurement equipment and the environment. For HS image denoising and destriping tasks, Spatio-Spectral Total Variation (SSTV), defined using second-order spatio-spectral differences, is widely known as a powerful regularization approach that models the underlying spatio-spectral properties. However, since SSTV refers only to adjacent pixels/bands, semi-local spatial structures are not preserved during denoising process. To address this problem, we newly design S3TTV, defined by the sum of the nuclear norms of matrices consisting of second-order spatio-spectral differences in small spectral blocks (we call these matrices as spatio-spectral structure tensors). The proposed regularization method simultaneously models the spatial piecewise-smoothness, the spatial similarity between adjacent bands, and the spectral correlation across all bands in small spectral blocks, leading to effective noise removal while preserving the semi-local spatial structures. Furthermore, we formulate the HS image denoising and destriping problem as a convex optimization problem involving S3TTV and develop an algorithm based on a preconditioned primal-dual splitting method to solve this problem efficiently. Finally, we demonstrate the effectiveness of S3TTV by comparing it with existing methods, including state-of-the-art ones through denoising and destriping experiments.","sentences":["This paper proposes a novel regularization method, named Spatio-Spectral Structure Tensor Total Variation (S3TTV), for denoising and destriping of hyperspectral (HS) images.","HS images are inevitably contaminated by various types of noise, during acquisition process, due to the measurement equipment and the environment.","For HS image denoising and destriping tasks, Spatio-Spectral Total Variation (SSTV), defined using second-order spatio-spectral differences, is widely known as a powerful regularization approach that models the underlying spatio-spectral properties.","However, since SSTV refers only to adjacent pixels/bands, semi-local spatial structures are not preserved during denoising process.","To address this problem, we newly design S3TTV, defined by the sum of the nuclear norms of matrices consisting of second-order spatio-spectral differences in small spectral blocks (we call these matrices as spatio-spectral structure tensors).","The proposed regularization method simultaneously models the spatial piecewise-smoothness, the spatial similarity between adjacent bands, and the spectral correlation across all bands in small spectral blocks, leading to effective noise removal while preserving the semi-local spatial structures.","Furthermore, we formulate the HS image denoising and destriping problem as a convex optimization problem involving S3TTV and develop an algorithm based on a preconditioned primal-dual splitting method to solve this problem efficiently.","Finally, we demonstrate the effectiveness of S3TTV by comparing it with existing methods, including state-of-the-art ones through denoising and destriping experiments."],"url":"http://arxiv.org/abs/2404.03313v1","category":"eess.SP"}
{"created":"2024-04-04 08:21:32","title":"Adsorption of Gases on Ti3C2Tx MXene: Implications from X-ray Photoelectron Spectroscopy","abstract":"One of the most explored MXenes is the Ti3C2Tx, where Tx is designated to inherently formed termination species. Among many applications, Ti3C2Tx is an excellent material for energy storage, energy converting, and CO2-capturing devices. However, active sites for adsorption and surface reactions on the Ti3C2Tx-surface are still open questions to explore, which have implications for preparation methods when to obtain correct and optimized surface requirements. Here we use X-ray photoelectron spectroscopy to study adsorption of common gas molecules such as H2, CO2, and H2O, which all might be present in energy storage, energy converting, and CO2-capturing devices based on 2D flakes of Ti3C2Tx. The study shows that H2O, with a strong bonding to the Ti-Ti bridge-sites, can be considered as a termination species. An H2O terminated Ti3C2Tx-surface restricts the CO2 adsorption to the Ti on-top sites and reduces the ability to store positive ions, such as Li+ and Na+. On the other hand, an H2O terminated Ti3C2Tx-surface shows the capability to split water. The study further shows that H2 has the ability to remove F at moderate temperatures. The results from this study have implications for correct selection of MXene preparations and the environment around the MXene in different implementations.","sentences":["One of the most explored MXenes is the Ti3C2Tx, where Tx is designated to inherently formed termination species.","Among many applications, Ti3C2Tx is an excellent material for energy storage, energy converting, and CO2-capturing devices.","However, active sites for adsorption and surface reactions on the Ti3C2Tx-surface are still open questions to explore, which have implications for preparation methods when to obtain correct and optimized surface requirements.","Here we use X-ray photoelectron spectroscopy to study adsorption of common gas molecules such as H2, CO2, and H2O, which all might be present in energy storage, energy converting, and CO2-capturing devices based on 2D flakes of Ti3C2Tx.","The study shows that H2O, with a strong bonding to the Ti-Ti bridge-sites, can be considered as a termination species.","An H2O terminated Ti3C2Tx-surface restricts the CO2 adsorption to the Ti on-top sites and reduces the ability to store positive ions, such as Li+ and Na+.","On the other hand, an H2O terminated Ti3C2Tx-surface shows the capability to split water.","The study further shows that H2 has the ability to remove F at moderate temperatures.","The results from this study have implications for correct selection of MXene preparations and the environment around the MXene in different implementations."],"url":"http://arxiv.org/abs/2404.03287v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 03:42:17","title":"Direct interpolative construction of the discrete Fourier transform as a matrix product operator","abstract":"The quantum Fourier transform (QFT), which can be viewed as a reindexing of the discrete Fourier transform (DFT), has been shown to be compressible as a low-rank matrix product operator (MPO) or quantized tensor train (QTT) operator. However, the original proof of this fact does not furnish a construction of the MPO with a guaranteed error bound. Meanwhile, the existing practical construction of this MPO, based on the compression of a quantum circuit, is not as efficient as possible. We present a simple closed-form construction of the QFT MPO using the interpolative decomposition, with guaranteed near-optimal compression error for a given rank. This construction can speed up the application of the QFT and the DFT, respectively, in quantum circuit simulations and QTT applications. We also connect our interpolative construction to the approximate quantum Fourier transform (AQFT) by demonstrating that the AQFT can be viewed as an MPO constructed using a different interpolation scheme.","sentences":["The quantum Fourier transform (QFT), which can be viewed as a reindexing of the discrete Fourier transform (DFT), has been shown to be compressible as a low-rank matrix product operator (MPO) or quantized tensor train (QTT) operator.","However, the original proof of this fact does not furnish a construction of the MPO with a guaranteed error bound.","Meanwhile, the existing practical construction of this MPO, based on the compression of a quantum circuit, is not as efficient as possible.","We present a simple closed-form construction of the QFT MPO using the interpolative decomposition, with guaranteed near-optimal compression error for a given rank.","This construction can speed up the application of the QFT and the DFT, respectively, in quantum circuit simulations and QTT applications.","We also connect our interpolative construction to the approximate quantum Fourier transform (AQFT) by demonstrating that the AQFT can be viewed as an MPO constructed using a different interpolation scheme."],"url":"http://arxiv.org/abs/2404.03182v1","category":"quant-ph"}
{"created":"2024-04-04 02:45:28","title":"Asymptotic Purification of Disordered Quantum Trajectories","abstract":"The theory of quantum trajectories in a time-dependent disordered environment is studied. We develop a general framework in which one can study the behavior of quantum trajectories that are obtained by repeated not-necessarily-independent but stationary measurements. Using this framework, we are able to generalize the asymptotic purification results of K\\\"ummerer and Maassen [KM] to the current setting. Most notably, the concept of a dark subspace from [KM] is generalized to the current setting, which enables us to lift the main theorem of [KM] to the disordered case.","sentences":["The theory of quantum trajectories in a time-dependent disordered environment is studied.","We develop a general framework in which one can study the behavior of quantum trajectories that are obtained by repeated not-necessarily-independent but stationary measurements.","Using this framework, we are able to generalize the asymptotic purification results of K\\\"ummerer and Maassen [KM] to the current setting.","Most notably, the concept of a dark subspace from [KM] is generalized to the current setting, which enables us to lift the main theorem of [KM] to the disordered case."],"url":"http://arxiv.org/abs/2404.03168v1","category":"quant-ph"}
{"created":"2024-04-04 02:36:24","title":"Non-variational Quantum Combinatorial Optimisation","abstract":"This paper introduces a non-variational quantum algorithm designed to solve a wide range of combinatorial optimisation problems. The algorithm leverages an engineered interference process achieved through repeated application of two unitaries; one inducing phase-shifts dependent on objective function values, and the other mixing phase-shifted probability amplitudes via a continuous-time quantum walk (CTQW) on a problem-specific graph. The algorithm's versatility is demonstrated through its application to various problems, namely those for which solutions are characterised by either a vector of binary variables, a vector of non-binary integer variables, or permutations (a vector of integer variables without repetition). An efficient quantum circuit implementation of the CTQW for each of these problem types is also discussed. A penalty function approach for constrained problems is also introduced, including a method for optimising the penalty function. The algorithm's performance is demonstrated through numerical simulation for randomly generated instances of the following problems (and problem sizes): weighted maxcut (18 vertices), maximum independent set (18 vertices), k-means clustering (12 datapoints, 3 clusters), capacitated facility location (12 customers, 3 facility locations), and the quadratic assignment problem (9 locations). For each problem instance, the algorithm finds a globally optimal solution with a small number of iterations.","sentences":["This paper introduces a non-variational quantum algorithm designed to solve a wide range of combinatorial optimisation problems.","The algorithm leverages an engineered interference process achieved through repeated application of two unitaries; one inducing phase-shifts dependent on objective function values, and the other mixing phase-shifted probability amplitudes via a continuous-time quantum walk (CTQW) on a problem-specific graph.","The algorithm's versatility is demonstrated through its application to various problems, namely those for which solutions are characterised by either a vector of binary variables, a vector of non-binary integer variables, or permutations (a vector of integer variables without repetition).","An efficient quantum circuit implementation of the CTQW for each of these problem types is also discussed.","A penalty function approach for constrained problems is also introduced, including a method for optimising the penalty function.","The algorithm's performance is demonstrated through numerical simulation for randomly generated instances of the following problems (and problem sizes): weighted maxcut (18 vertices), maximum independent set (18 vertices), k-means clustering (12 datapoints, 3 clusters), capacitated facility location (12 customers, 3 facility locations), and the quadratic assignment problem (9 locations).","For each problem instance, the algorithm finds a globally optimal solution with a small number of iterations."],"url":"http://arxiv.org/abs/2404.03167v1","category":"quant-ph"}
{"created":"2024-04-04 01:22:23","title":"Discontinuity-preserving Normal Integration with Auxiliary Edges","abstract":"Many surface reconstruction methods incorporate normal integration, which is a process to obtain a depth map from surface gradients. In this process, the input may represent a surface with discontinuities, e.g., due to self-occlusion. To reconstruct an accurate depth map from the input normal map, hidden surface gradients occurring from the jumps must be handled. To model these jumps correctly, we design a novel discretization scheme for the domain of normal integration. Our key idea is to introduce auxiliary edges, which bridge between piecewise-smooth patches in the domain so that the magnitude of hidden jumps can be explicitly expressed. Using the auxiliary edges, we design a novel algorithm to optimize the discontinuity and the depth map from the input normal map. Our method optimizes discontinuities by using a combination of iterative re-weighted least squares and iterative filtering of the jump magnitudes on auxiliary edges to provide strong sparsity regularization. Compared to previous discontinuity-preserving normal integration methods, which model the magnitudes of jumps only implicitly, our method reconstructs subtle discontinuities accurately thanks to our explicit representation of jumps allowing for strong sparsity regularization.","sentences":["Many surface reconstruction methods incorporate normal integration, which is a process to obtain a depth map from surface gradients.","In this process, the input may represent a surface with discontinuities, e.g., due to self-occlusion.","To reconstruct an accurate depth map from the input normal map, hidden surface gradients occurring from the jumps must be handled.","To model these jumps correctly, we design a novel discretization scheme for the domain of normal integration.","Our key idea is to introduce auxiliary edges, which bridge between piecewise-smooth patches in the domain so that the magnitude of hidden jumps can be explicitly expressed.","Using the auxiliary edges, we design a novel algorithm to optimize the discontinuity and the depth map from the input normal map.","Our method optimizes discontinuities by using a combination of iterative re-weighted least squares and iterative filtering of the jump magnitudes on auxiliary edges to provide strong sparsity regularization.","Compared to previous discontinuity-preserving normal integration methods, which model the magnitudes of jumps only implicitly, our method reconstructs subtle discontinuities accurately thanks to our explicit representation of jumps allowing for strong sparsity regularization."],"url":"http://arxiv.org/abs/2404.03138v1","category":"cs.CV"}
{"created":"2024-04-03 23:24:25","title":"Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking","abstract":"Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.","sentences":["Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories.","Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target.","Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model.","These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections.","Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories.","In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models.","Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter.","This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model.","We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT.","In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively.","At the same time, it elevates other performance metrics such as HOTA by more than 5%.","Our source code is available at https://github.com/noyzzz/EMAP."],"url":"http://arxiv.org/abs/2404.03110v1","category":"cs.CV"}
{"created":"2024-04-03 22:16:49","title":"Low Frequency Sampling in Model Predictive Path Integral Control","abstract":"Sampling-based model-predictive controllers have become a powerful optimization tool for planning and control problems in various challenging environments. In this paper, we show how the default choice of uncorrelated Gaussian distributions can be improved upon with the use of a colored noise distribution. Our choice of distribution allows for the emphasis on low frequency control signals, which can result in smoother and more exploratory samples. We use this frequency-based sampling distribution with Model Predictive Path Integral (MPPI) in both hardware and simulation experiments to show better or equal performance on systems with various speeds of input response.","sentences":["Sampling-based model-predictive controllers have become a powerful optimization tool for planning and control problems in various challenging environments.","In this paper, we show how the default choice of uncorrelated Gaussian distributions can be improved upon with the use of a colored noise distribution.","Our choice of distribution allows for the emphasis on low frequency control signals, which can result in smoother and more exploratory samples.","We use this frequency-based sampling distribution with Model Predictive Path Integral (MPPI) in both hardware and simulation experiments to show better or equal performance on systems with various speeds of input response."],"url":"http://arxiv.org/abs/2404.03094v1","category":"cs.RO"}
{"created":"2024-04-03 22:07:30","title":"The circle packing problem: a theoretical comparison of various convexification techniques","abstract":"We consider the problem of packing congruent circles with the maximum radius in a unit square as a mathematical optimization problem. Due to the presence of non-overlapping constraints, this problem is a notoriously difficult nonconvex quadratically constrained optimization problem, which possesses many local optima. We consider several popular convexification techniques, giving rise to linear programming relaxations and semidefinite programming relaxations for the circle packing problem. We compare the strength of these relaxations theoretically, thereby proving the conjectures by Anstreicher (JOGO, 2009). Our results serve as a theoretical justification for the ineffectiveness of existing machinery for convexification of non-overlapping constraints.","sentences":["We consider the problem of packing congruent circles with the maximum radius in a unit square as a mathematical optimization problem.","Due to the presence of non-overlapping constraints, this problem is a notoriously difficult nonconvex quadratically constrained optimization problem, which possesses many local optima.","We consider several popular convexification techniques, giving rise to linear programming relaxations and semidefinite programming relaxations for the circle packing problem.","We compare the strength of these relaxations theoretically, thereby proving the conjectures by Anstreicher (JOGO, 2009).","Our results serve as a theoretical justification for the ineffectiveness of existing machinery for convexification of non-overlapping constraints."],"url":"http://arxiv.org/abs/2404.03091v1","category":"math.OC"}
{"created":"2024-04-03 21:27:50","title":"A Hybrid BLE/UWB Localization Technique with Automatic Radio Map Creation","abstract":"Localization systems intended for home use by people with mild cognitive impairment should comply with specific requirements. They should provide the users with sub-meter accuracy allowing for analyzing patient's movement trajectory and be energy effective, so the devices do not need frequent charging. Such requirements could be satisfied by employing a hybrid positioning system combining accurate UWB with energy efficient Bluetooth Low Energy (BLE) technology. In the paper, such a solution is presented and experimentally verified. In the proposed system, user's location is derived using BLE based fingerprinting. A radio map utilized by the algorithm is created automatically during system operation with the support of UWB subsystem. Such an approach allows the users to repeat system calibration as often as possible, which raises systems resistance to environmental changes.","sentences":["Localization systems intended for home use by people with mild cognitive impairment should comply with specific requirements.","They should provide the users with sub-meter accuracy allowing for analyzing patient's movement trajectory and be energy effective, so the devices do not need frequent charging.","Such requirements could be satisfied by employing a hybrid positioning system combining accurate UWB with energy efficient Bluetooth Low Energy (BLE) technology.","In the paper, such a solution is presented and experimentally verified.","In the proposed system, user's location is derived using BLE based fingerprinting.","A radio map utilized by the algorithm is created automatically during system operation with the support of UWB subsystem.","Such an approach allows the users to repeat system calibration as often as possible, which raises systems resistance to environmental changes."],"url":"http://arxiv.org/abs/2404.03072v1","category":"eess.SP"}
{"created":"2024-04-03 21:17:02","title":"Multiple UAV-Assisted Cooperative DF Relaying in Multi-User Massive MIMO IoT Systems","abstract":"This work considers a multi-user massive multiple-input multiple-output (MU-mMIMO) Internet-of-Things (IoT) system, where multiple unmanned aerial vehicles (UAVs) operating as decode-and-forward (DF) relays connect the base station (BS) to a large number of IoT devices. To maximize the total achievable rate, we propose a novel joint optimization problem of hybrid beamforming (HBF), multiple UAV relay positioning, and power allocation (PA) to multiple IoT users. The study adopts a geometry-based millimeter-wave (mmWave) channel model for both links and utilizes sequential optimization based on K-means UAV-user association. The radio frequency (RF) stages are designed based on the slow time-varying angular information, while the baseband (BB) stages are designed utilizing the reduced-dimension effective channel matrices. The illustrative results show that multiple UAV-assisted cooperative relaying systems outperform a single UAV system in practical user distributions. Moreover, compared to fixed positions and equal PA of UAVs and BS, the joint optimization of UAV location and PA substantially enhances the total achievable rate.","sentences":["This work considers a multi-user massive multiple-input multiple-output (MU-mMIMO) Internet-of-Things (IoT) system, where multiple unmanned aerial vehicles (UAVs) operating as decode-and-forward (DF) relays connect the base station (BS) to a large number of IoT devices.","To maximize the total achievable rate, we propose a novel joint optimization problem of hybrid beamforming (HBF), multiple UAV relay positioning, and power allocation (PA) to multiple IoT users.","The study adopts a geometry-based millimeter-wave (mmWave) channel model for both links and utilizes sequential optimization based on K-means UAV-user association.","The radio frequency (RF) stages are designed based on the slow time-varying angular information, while the baseband (BB) stages are designed utilizing the reduced-dimension effective channel matrices.","The illustrative results show that multiple UAV-assisted cooperative relaying systems outperform a single UAV system in practical user distributions.","Moreover, compared to fixed positions and equal PA of UAVs and BS, the joint optimization of UAV location and PA substantially enhances the total achievable rate."],"url":"http://arxiv.org/abs/2404.03068v1","category":"cs.IT"}
{"created":"2024-04-03 21:13:15","title":"Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks","abstract":"Traffic dynamics is universally crucial in analyzing and designing almost any network. This article introduces a novel theoretical approach to analyzing network traffic dynamics. This theory's machinery is based on the notion of traffic divergence, which captures the flow (im)balance of network nodes and links. It features various analytical probes to investigate both spatial and temporal traffic dynamics. In particular, the maximal traffic distribution in a network can be characterized by spatial traffic divergence rate, which reveals the relative difference among node traffic divergence. To illustrate the usefulness, we apply the theory to two network-driven problems: throughput estimation of data center networks and power-optimized communication planning for robot networks, and show the merits of the proposed theory through simulations.","sentences":["Traffic dynamics is universally crucial in analyzing and designing almost any network.","This article introduces a novel theoretical approach to analyzing network traffic dynamics.","This theory's machinery is based on the notion of traffic divergence, which captures the flow (im)balance of network nodes and links.","It features various analytical probes to investigate both spatial and temporal traffic dynamics.","In particular, the maximal traffic distribution in a network can be characterized by spatial traffic divergence rate, which reveals the relative difference among node traffic divergence.","To illustrate the usefulness, we apply the theory to two network-driven problems: throughput estimation of data center networks and power-optimized communication planning for robot networks, and show the merits of the proposed theory through simulations."],"url":"http://arxiv.org/abs/2404.03066v1","category":"cs.MA"}
{"created":"2024-04-03 18:41:56","title":"Semi-analytical covariance matrices for two-point correlation function for DESI 2024 data","abstract":"We present an optimized way of producing the fast semi-analytical covariance matrices for the Legendre moments of the two-point correlation function, taking into account survey geometry and mimicking the non-Gaussian effects. We validate the approach on simulated (mock) catalogs for different galaxy types, representative of the Dark Energy Spectroscopic Instrument (DESI) Data Release 1, used in 2024 analyses. We find only a few percent differences between the mock sample covariance matrix and our results, which can be expected given the approximate nature of the mocks, although we do identify discrepancies between the shot-noise properties of the DESI fiber assignment algorithm and the faster approximation used in the mocks. Importantly, we find a close agreement (<~ 5% relative differences) in the projected errorbars for distance scale parameters for the baryon acoustic oscillation measurements. This confirms our method as an attractive alternative to simulation-based covariance matrices, especially for non-standard models or galaxy sample selections, in particular, relevant to the broad current and future analyses of DESI data.","sentences":["We present an optimized way of producing the fast semi-analytical covariance matrices for the Legendre moments of the two-point correlation function, taking into account survey geometry and mimicking the non-Gaussian effects.","We validate the approach on simulated (mock) catalogs for different galaxy types, representative of the Dark Energy Spectroscopic Instrument (DESI) Data Release 1, used in 2024 analyses.","We find only a few percent differences between the mock sample covariance matrix and our results, which can be expected given the approximate nature of the mocks, although we do identify discrepancies between the shot-noise properties of the DESI fiber assignment algorithm and the faster approximation used in the mocks.","Importantly, we find a close agreement (<~ 5% relative differences) in the projected errorbars for distance scale parameters for the baryon acoustic oscillation measurements.","This confirms our method as an attractive alternative to simulation-based covariance matrices, especially for non-standard models or galaxy sample selections, in particular, relevant to the broad current and future analyses of DESI data."],"url":"http://arxiv.org/abs/2404.03007v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:41:54","title":"Optimal Reconstruction of Baryon Acoustic Oscillations for DESI 2024","abstract":"Baryon acoustic oscillations (BAO) provide a robust standard ruler to measure the expansion history of the Universe through galaxy clustering. Density-field reconstruction is now a widely adopted procedure for increasing the precision and accuracy of the BAO detection. With the goal of finding the optimal reconstruction settings to be used in the DESI 2024 galaxy BAO analysis, we assess the sensitivity of the post-reconstruction BAO constraints to different choices in our analysis configuration, performing tests on blinded data from the first year of DESI observations (DR1), as well as on mocks that mimic the expected clustering and selection properties of the DESI DR1 target samples. Overall, we find that BAO constraints remain robust against multiple aspects in the reconstruction process, including the choice of smoothing scale, treatment of redshift-space distortions, fiber assignment incompleteness, and parameterizations of the BAO model. We also present a series of tests that DESI followed in order to assess the maturity of the end-to-end galaxy BAO pipeline before the unblinding of the large-scale structure catalogs.","sentences":["Baryon acoustic oscillations (BAO) provide a robust standard ruler to measure the expansion history of the Universe through galaxy clustering.","Density-field reconstruction is now a widely adopted procedure for increasing the precision and accuracy of the BAO detection.","With the goal of finding the optimal reconstruction settings to be used in the DESI 2024 galaxy BAO analysis, we assess the sensitivity of the post-reconstruction BAO constraints to different choices in our analysis configuration, performing tests on blinded data from the first year of DESI observations (DR1), as well as on mocks that mimic the expected clustering and selection properties of the DESI DR1 target samples.","Overall, we find that BAO constraints remain robust against multiple aspects in the reconstruction process, including the choice of smoothing scale, treatment of redshift-space distortions, fiber assignment incompleteness, and parameterizations of the BAO model.","We also present a series of tests that DESI followed in order to assess the maturity of the end-to-end galaxy BAO pipeline before the unblinding of the large-scale structure catalogs."],"url":"http://arxiv.org/abs/2404.03005v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:00:01","title":"Spatiotemporal Quenches for Efficient Critical Ground State Preparation in Two-Dimensional Quantum Systems","abstract":"Quantum simulators have the potential to shed light on the study of quantum many-body systems and materials, offering unique insights into various quantum phenomena. While adiabatic evolution has been conventionally employed for state preparation, it faces challenges when the system evolves too quickly or the coherence time is limited. In such cases, shortcuts to adiabaticity, such as spatiotemporal quenches, provide a promising alternative. This paper numerically investigates the application of spatiotemporal quenches in the two-dimensional transverse field Ising model with ferromagnetic interactions, focusing on the emergence of the ground state and its correlation properties at criticality when the gap vanishes. We demonstrate the effectiveness of these quenches in rapidly preparing ground states in critical systems. Our simulations reveal the existence of an optimal quench front velocity at the emergent speed of light, leading to minimal excitation energy density and correlation lengths of the order of finite system sizes we can simulate. These findings emphasize the potential of spatiotemporal quenches for efficient ground state preparation in quantum systems, with implications for the exploration of strongly correlated phases and programmable quantum computing.","sentences":["Quantum simulators have the potential to shed light on the study of quantum many-body systems and materials, offering unique insights into various quantum phenomena.","While adiabatic evolution has been conventionally employed for state preparation, it faces challenges when the system evolves too quickly or the coherence time is limited.","In such cases, shortcuts to adiabaticity, such as spatiotemporal quenches, provide a promising alternative.","This paper numerically investigates the application of spatiotemporal quenches in the two-dimensional transverse field Ising model with ferromagnetic interactions, focusing on the emergence of the ground state and its correlation properties at criticality when the gap vanishes.","We demonstrate the effectiveness of these quenches in rapidly preparing ground states in critical systems.","Our simulations reveal the existence of an optimal quench front velocity at the emergent speed of light, leading to minimal excitation energy density and correlation lengths of the order of finite system sizes we can simulate.","These findings emphasize the potential of spatiotemporal quenches for efficient ground state preparation in quantum systems, with implications for the exploration of strongly correlated phases and programmable quantum computing."],"url":"http://arxiv.org/abs/2404.02957v1","category":"quant-ph"}
{"created":"2024-04-03 18:00:00","title":"Surrogate optimization of variational quantum circuits","abstract":"Variational quantum eigensolvers are touted as a near-term algorithm capable of impacting many applications. However, the potential has not yet been realized, with few claims of quantum advantage and high resource estimates, especially due to the need for optimization in the presence of noise. Finding algorithms and methods to improve convergence is important to accelerate the capabilities of near-term hardware for VQE or more broad applications of hybrid methods in which optimization is required. To this goal, we look to use modern approaches developed in circuit simulations and stochastic classical optimization, which can be combined to form a surrogate optimization approach to quantum circuits. Using an approximate (classical CPU/GPU) state vector simulator as a surrogate model, we efficiently calculate an approximate Hessian, passed as an input for a quantum processing unit or exact circuit simulator. This method will lend itself well to parallelization across quantum processing units. We demonstrate the capabilities of such an approach with and without sampling noise and a proof-of-principle demonstration on a quantum processing unit utilizing 40 qubits.","sentences":["Variational quantum eigensolvers are touted as a near-term algorithm capable of impacting many applications.","However, the potential has not yet been realized, with few claims of quantum advantage and high resource estimates, especially due to the need for optimization in the presence of noise.","Finding algorithms and methods to improve convergence is important to accelerate the capabilities of near-term hardware for VQE or more broad applications of hybrid methods in which optimization is required.","To this goal, we look to use modern approaches developed in circuit simulations and stochastic classical optimization, which can be combined to form a surrogate optimization approach to quantum circuits.","Using an approximate (classical CPU/GPU) state vector simulator as a surrogate model, we efficiently calculate an approximate Hessian, passed as an input for a quantum processing unit or exact circuit simulator.","This method will lend itself well to parallelization across quantum processing units.","We demonstrate the capabilities of such an approach with and without sampling noise and a proof-of-principle demonstration on a quantum processing unit utilizing 40 qubits."],"url":"http://arxiv.org/abs/2404.02951v1","category":"quant-ph"}
{"created":"2024-04-03 17:55:20","title":"A Mean Field Game Model for Timely Computation in Edge Computing Systems","abstract":"We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES. Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two. We model the problem as a large population non-cooperative game among the $N$ devices. Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user's multi-objective optimization problem, with a coupled consistency condition. By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption. In numerical simulations, we validate that a higher load at the ES may lead devices to upload their task to the ES less often.","sentences":["We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES.","Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two.","We model the problem as a large population non-cooperative game among the $N$ devices.","Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user's multi-objective optimization problem, with a coupled consistency condition.","By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption.","In numerical simulations, we validate that a higher load at the ES may lead devices to upload their task to the ES less often."],"url":"http://arxiv.org/abs/2404.02898v1","category":"cs.IT"}
{"created":"2024-04-03 16:50:05","title":"The Life Care Annuity: enhancing product features and refining pricing methods","abstract":"In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods. In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option. Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process. As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method. We use such a method to estimate the fair price of the product. Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder. Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach.","sentences":["In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods.","In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option.","Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process.","As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method.","We use such a method to estimate the fair price of the product.","Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder.","Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach."],"url":"http://arxiv.org/abs/2404.02858v1","category":"q-fin.CP"}
{"created":"2024-04-03 16:16:31","title":"Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models","abstract":"This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. These findings highlight the potential of CherryQ for enabling efficient deployment of LLMs by taking advantage of parameter heterogeneity.","sentences":["This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs).","We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact.","This heterogeneity is found to be prevalent across different model families, scales, and types.","Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters.","CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision.","Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance.","Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts.","These findings highlight the potential of CherryQ for enabling efficient deployment of LLMs by taking advantage of parameter heterogeneity."],"url":"http://arxiv.org/abs/2404.02837v1","category":"cs.CL"}
{"created":"2024-04-03 16:01:25","title":"A camel with a less strict diet","abstract":"A camel can carry $B$ bananas on its back. It can have $2$ bananas at a time in its stomach. For each mile the camel walks, the amount of bananas in its stomach decreases $1$. As soon as the amount of bananas in the camel's stomach is at most $1$, it can eat a new banana. When the camel's stomach is empty, the camel must eat a new banana (in order to be able to continue its itinerary).   Let there be a stock of $N$ bananas at the border of the desert. How far can the camel penetrate into the desert, starting at this point? (Of course it can form new stocks with transported bananas.)   The case $B=1$ is solved completely. The round trip variant is solved for $B=1$ as well. For $B=2$, the round trip variant is solved for $N$ which are a power of $2$ and $N \\le 8$, and estimated up to $1/(N-1)$ miles for general $N$.","sentences":["A camel can carry $B$ bananas on its back.","It can have $2$ bananas at a time in its stomach.","For each mile the camel walks, the amount of bananas in its stomach decreases $1$. As soon as the amount of bananas in the camel's stomach is at most $1$, it can eat a new banana.","When the camel's stomach is empty, the camel must eat a new banana (in order to be able to continue its itinerary).   ","Let there be a stock of $N$ bananas at the border of the desert.","How far can the camel penetrate into the desert, starting at this point?","(Of course it can form new stocks with transported bananas.)   ","The case $B=1$ is solved completely.","The round trip variant is solved for $B=1$ as well.","For $B=2$, the round trip variant is solved for $N$ which are a power of $2$ and $N \\le 8$, and estimated up to $1/(N-1)$ miles for general $N$."],"url":"http://arxiv.org/abs/2404.02828v1","category":"math.HO"}
{"created":"2024-04-03 15:31:49","title":"Wideband Beamforming for Near-Field Communications with Circular Arrays","abstract":"The beamforming performance of the uniform circular array (UCA) in near-field wideband communication systems is investigated. Compared to uniform linear array (ULA), UCA exhibits uniform effective array aperture in all directions, thus enabling more users to benefit from near-field communications. In this paper, the unique beam squint effect in near-field wideband UCA systems is comprehensively analyzed in both the distance and angular domains. It is rigorously demonstrated that the beam focal point only exists at a specific frequency in wideband UCA systems, resulting in significant beamforming loss. To alleviate this unique beam squint effect, the true-time delay (TTD)-based beamforming architecture is exploited. In particular, two wideband beamforming optimization approaches leveraging TTD units are proposed. 1) Analytical approach: In this approach, the phase shifters (PSs) and the time delay of TTD units are designed based on the analytical formula for beamforming gain. Following this design, the minimum number of TTD units required to achieve a predetermined beamforming gain is quantified. 2) Joint-optimization approach: In this method, the PSs and the TTD units are jointly optimized under practical maximum delay constraints to approximate the optimal unconstrained analog beamformer. Specifically, an efficient alternating optimization algorithm is proposed, where the PSs and the TTD units are alternately updated using either the closed-form solution or the low-complexity linear search approach. Extensive numerical results demonstrate that 1) the proposed beamforming schemes effectively mitigate the beam squint effect, and 2) the joint-optimization approach outperforms the analytical approach in terms of array gain and achievable spectral efficiency.","sentences":["The beamforming performance of the uniform circular array (UCA) in near-field wideband communication systems is investigated.","Compared to uniform linear array (ULA), UCA exhibits uniform effective array aperture in all directions, thus enabling more users to benefit from near-field communications.","In this paper, the unique beam squint effect in near-field wideband UCA systems is comprehensively analyzed in both the distance and angular domains.","It is rigorously demonstrated that the beam focal point only exists at a specific frequency in wideband UCA systems, resulting in significant beamforming loss.","To alleviate this unique beam squint effect, the true-time delay (TTD)-based beamforming architecture is exploited.","In particular, two wideband beamforming optimization approaches leveraging TTD units are proposed.","1) Analytical approach: In this approach, the phase shifters (PSs) and the time delay of TTD units are designed based on the analytical formula for beamforming gain.","Following this design, the minimum number of TTD units required to achieve a predetermined beamforming gain is quantified.","2) Joint-optimization approach: In this method, the PSs and the TTD units are jointly optimized under practical maximum delay constraints to approximate the optimal unconstrained analog beamformer.","Specifically, an efficient alternating optimization algorithm is proposed, where the PSs and the TTD units are alternately updated using either the closed-form solution or the low-complexity linear search approach.","Extensive numerical results demonstrate that 1) the proposed beamforming schemes effectively mitigate the beam squint effect, and 2) the joint-optimization approach outperforms the analytical approach in terms of array gain and achievable spectral efficiency."],"url":"http://arxiv.org/abs/2404.02811v1","category":"cs.IT"}
{"created":"2024-04-03 15:20:24","title":"Efficient Multi-Vector Dense Retrieval Using Bit Vectors","abstract":"Dense retrieval techniques employ pre-trained large language models to build a high-dimensional representation of queries and passages. These representations compute the relevance of a passage w.r.t. to a query using efficient similarity measures. In this line, multi-vector representations show improved effectiveness at the expense of a one-order-of-magnitude increase in memory footprint and query latency by encoding queries and documents on a per-token level. Recently, PLAID has tackled these problems by introducing a centroid-based term representation to reduce the memory impact of multi-vector systems. By exploiting a centroid interaction mechanism, PLAID filters out non-relevant documents, thus reducing the cost of the successive ranking stages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit vectors'' (EMVB), a novel framework for efficient query processing in multi-vector dense retrieval. First, EMVB employs a highly efficient pre-filtering step of passages using optimized bit vectors. Second, the computation of the centroid interaction happens column-wise, exploiting SIMD instructions, thus reducing its latency. Third, EMVB leverages Product Quantization (PQ) to reduce the memory footprint of storing vector representations while jointly allowing for fast late interaction. Fourth, we introduce a per-document term filtering method that further improves the efficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB is up to 2.8x faster while reducing the memory footprint by 1.8x with no loss in retrieval accuracy compared to PLAID.","sentences":["Dense retrieval techniques employ pre-trained large language models to build a high-dimensional representation of queries and passages.","These representations compute the relevance of a passage w.r.t.","to a query using efficient similarity measures.","In this line, multi-vector representations show improved effectiveness at the expense of a one-order-of-magnitude increase in memory footprint and query latency by encoding queries and documents on a per-token level.","Recently, PLAID has tackled these problems by introducing a centroid-based term representation to reduce the memory impact of multi-vector systems.","By exploiting a centroid interaction mechanism, PLAID filters out non-relevant documents, thus reducing the cost of the successive ranking stages.","This paper proposes ``Efficient Multi-Vector dense retrieval with Bit vectors'' (EMVB), a novel framework for efficient query processing in multi-vector dense retrieval.","First, EMVB employs a highly efficient pre-filtering step of passages using optimized bit vectors.","Second, the computation of the centroid interaction happens column-wise, exploiting SIMD instructions, thus reducing its latency.","Third, EMVB leverages Product Quantization (PQ) to reduce the memory footprint of storing vector representations while jointly allowing for fast late interaction.","Fourth, we introduce a per-document term filtering method that further improves the efficiency of the last step.","Experiments on MS MARCO and LoTTE show that EMVB is up to 2.8x faster while reducing the memory footprint by 1.8x with no loss in retrieval accuracy compared to PLAID."],"url":"http://arxiv.org/abs/2404.02805v1","category":"cs.IR"}
{"created":"2024-04-03 15:02:03","title":"Planning for Robust Open-loop Pushing: Exploiting Quasi-static Belief Dynamics and Contact-informed Optimization","abstract":"Non-prehensile manipulation such as pushing is typically subject to uncertain, non-smooth dynamics. However, modeling the uncertainty of the dynamics typically results in intractable belief dynamics, making data-efficient planning under uncertainty difficult. This article focuses on the problem of efficiently generating robust open-loop pushing plans. First, we investigate how the belief over object configurations propagates through quasi-static contact dynamics. We exploit the simplified dynamics to predict the variance of the object configuration without sampling from a perturbation distribution. In a sampling-based trajectory optimization algorithm, the gain of the variance is constrained in order to enforce robustness of the plan. Second, we propose an informed trajectory sampling mechanism for drawing robot trajectories that are likely to make contact with the object. This sampling mechanism is shown to significantly improve chances of finding robust solutions, especially when making-and-breaking contacts is required. We demonstrate that the proposed approach is able to synthesize bi-manual pushing trajectories, resulting in successful long-horizon pushing maneuvers without exteroceptive feedback such as vision or tactile feedback.","sentences":["Non-prehensile manipulation such as pushing is typically subject to uncertain, non-smooth dynamics.","However, modeling the uncertainty of the dynamics typically results in intractable belief dynamics, making data-efficient planning under uncertainty difficult.","This article focuses on the problem of efficiently generating robust open-loop pushing plans.","First, we investigate how the belief over object configurations propagates through quasi-static contact dynamics.","We exploit the simplified dynamics to predict the variance of the object configuration without sampling from a perturbation distribution.","In a sampling-based trajectory optimization algorithm, the gain of the variance is constrained in order to enforce robustness of the plan.","Second, we propose an informed trajectory sampling mechanism for drawing robot trajectories that are likely to make contact with the object.","This sampling mechanism is shown to significantly improve chances of finding robust solutions, especially when making-and-breaking contacts is required.","We demonstrate that the proposed approach is able to synthesize bi-manual pushing trajectories, resulting in successful long-horizon pushing maneuvers without exteroceptive feedback such as vision or tactile feedback."],"url":"http://arxiv.org/abs/2404.02795v1","category":"cs.RO"}
{"created":"2024-04-03 14:40:42","title":"Simultaneously Cloaking Electric and Hydrodynamic Fields via Electro-osmosis","abstract":"In this paper, we develop a general mathematical framework for the electro-osmosis problem to design simultaneous microscale electric and hydrodynamic cloaking in a Hele-Shaw configuration. A novel approach to achieving simultaneously cloaking both the electric and flow fields through a combination of scattering-cancellation technology and an electro-osmosis effect is proposed. In the design, the electric field is manipulated with scattering-cancellation technology while the pressure with electro-osmosis effect. As proof of this concept, the perfect electric and hydrodynamic cloaking conditions are derived for the cloaks with the cross-sectional shape being annulus or confocal ellipses using the layer potential techniques. Furthermore, we also propose an optimization scheme for the design of approximate cloaks within general geometries and prove the well-posedness of the optimization problem. In particular, the conditions that can ensure the simultaneous occurrence of approximate cloaks for general geometries are also established. Our theoretical findings are validated by a variety of numerical results and guide efficiently designing electric-related multiphysics cloaking.","sentences":["In this paper, we develop a general mathematical framework for the electro-osmosis problem to design simultaneous microscale electric and hydrodynamic cloaking in a Hele-Shaw configuration.","A novel approach to achieving simultaneously cloaking both the electric and flow fields through a combination of scattering-cancellation technology and an electro-osmosis effect is proposed.","In the design, the electric field is manipulated with scattering-cancellation technology while the pressure with electro-osmosis effect.","As proof of this concept, the perfect electric and hydrodynamic cloaking conditions are derived for the cloaks with the cross-sectional shape being annulus or confocal ellipses using the layer potential techniques.","Furthermore, we also propose an optimization scheme for the design of approximate cloaks within general geometries and prove the well-posedness of the optimization problem.","In particular, the conditions that can ensure the simultaneous occurrence of approximate cloaks for general geometries are also established.","Our theoretical findings are validated by a variety of numerical results and guide efficiently designing electric-related multiphysics cloaking."],"url":"http://arxiv.org/abs/2404.02773v1","category":"math.AP"}
{"created":"2024-04-03 14:34:22","title":"Precise Omega baryons from lattice QCD","abstract":"In this paper we determine the masses of $I(J^P)=0\\left(3/2^+\\right)$ and $0\\left(3/2^-\\right)$ $\\Omega$-baryon ground states using lattice QCD. We utilise Wilson-clover ensembles with $2+1$ dynamical quark flavours generated by the CLS consortium along a trajectory with a constant trace of the quark-mass matrix. We show that N$^3$LO $\\text{SU}(3)_f$ chiral perturbation theory expressions describe the ground-state masses with positive-parity well, and we use them to set the lattice scale. Methodologically, our combination of gauge-fixed wall sources and the generalized Pencil of Functions allows for high-precision determinations of the lattice spacing at a relative error of around $0.3\\%$ with controlled excited-state contamination. The fit we perform allows for the continuum value of $t_0$ to vary, thereby determining this quantity with a comparable level of precision to that of the lattice scale. Using the resulting scales our measurement of the negative-parity $\\Omega^{3/2^{-}}$ state is found to be consistent with the recently-discovered $\\Omega(2012)^-$, which can therefore be assigned the quantum numbers $I(J^P)=0\\left(3/2^-\\right)$.","sentences":["In this paper we determine the masses of $I(J^P)=0\\left(3/2^+\\right)$ and $0\\left(3/2^-\\right)$","$\\Omega$-baryon ground states using lattice QCD.","We utilise Wilson-clover ensembles with $2+1$ dynamical quark flavours generated by the CLS consortium along a trajectory with a constant trace of the quark-mass matrix.","We show that N$^3$LO $\\text{SU}(3)_f$ chiral perturbation theory expressions describe the ground-state masses with positive-parity well, and we use them to set the lattice scale.","Methodologically, our combination of gauge-fixed wall sources and the generalized Pencil of Functions allows for high-precision determinations of the lattice spacing at a relative error of around $0.3\\%$ with controlled excited-state contamination.","The fit we perform allows for the continuum value of $t_0$ to vary, thereby determining this quantity with a comparable level of precision to that of the lattice scale.","Using the resulting scales our measurement of the negative-parity $\\Omega^{3/2^{-}}$ state is found to be consistent with the recently-discovered $\\Omega(2012)^-$, which can therefore be assigned the quantum numbers $I(J^P)=0\\left(3/2^-\\right)$."],"url":"http://arxiv.org/abs/2404.02769v1","category":"hep-lat"}
{"created":"2024-04-03 13:53:03","title":"Investigating the Relation Between Problem Hardness and QUBO Properties","abstract":"Combinatorial optimization problems, integral to various scientific and industrial applications, often vary significantly in their complexity and computational difficulty. Transforming such problems into Quadratic Unconstrained Binary Optimization (QUBO) has regained considerable research attention in recent decades due to the central role of QUBO in Quantum Annealing. This work aims to shed some light on the relationship between the problems' properties. In particular, we examine how the spectral gap of the QUBO formulation correlates with the original problem, since it has an impact on how efficiently it can be solved on quantum computers. We analyze two well-known problems from Machine Learning, namely Clustering and Support Vector Machine (SVM) training, regarding the spectral gaps of their respective QUBO counterparts. An empirical evaluation provides interesting insights, showing that the spectral gap of Clustering QUBO instances positively correlates with data separability, while for SVM QUBO the opposite is true.","sentences":["Combinatorial optimization problems, integral to various scientific and industrial applications, often vary significantly in their complexity and computational difficulty.","Transforming such problems into Quadratic Unconstrained Binary Optimization (QUBO) has regained considerable research attention in recent decades due to the central role of QUBO in Quantum Annealing.","This work aims to shed some light on the relationship between the problems' properties.","In particular, we examine how the spectral gap of the QUBO formulation correlates with the original problem, since it has an impact on how efficiently it can be solved on quantum computers.","We analyze two well-known problems from Machine Learning, namely Clustering and Support Vector Machine (SVM) training, regarding the spectral gaps of their respective QUBO counterparts.","An empirical evaluation provides interesting insights, showing that the spectral gap of Clustering QUBO instances positively correlates with data separability, while for SVM QUBO the opposite is true."],"url":"http://arxiv.org/abs/2404.02751v1","category":"quant-ph"}
{"created":"2024-04-03 13:41:22","title":"Terraced Compression Method with Automated Threshold Selection for Multidimensional Image Clustering of Heterogeneous Bodies","abstract":"Multispectral transmission imaging provides strong benefits for early breast cancer screening. The frame accumulation method addresses the challenge of low grayscale and signal-to-noise ratio resulting from the strong absorption and scattering of light by breast tissue. This method introduces redundancy in data while improving the grayscale and signal-to-noise ratio of the image. Existing terraced compression algorithms effectively eliminate the data redundancy introduced by frame accumulation but necessitate significant time for manual debugging of threshold values. Hence, this paper proposes an improved terrace compression algorithm. The algorithm necessitates solely the input of the desired heterogeneous body size and autonomously calculates the optimal area threshold and gradient threshold by counting the grayscale and combining its distribution. Experimental acquisition involved multi-wavelength images of heterogeneous bodies exhibiting diverse textures, depths, and thicknesses. Subsequently, the method was applied after pre-processing to determine the thresholds for terraced compression at each wavelength, coupled with a window function for multi-dimensional image clustering. The results illustrate the method's efficacy in detecting and identifying various heterogeneous body types, depths, and thicknesses. This approach is expected to accurately identify the locations and types of breast tumors in the future, thus providing a more dependable tool for early breast cancer screening.","sentences":["Multispectral transmission imaging provides strong benefits for early breast cancer screening.","The frame accumulation method addresses the challenge of low grayscale and signal-to-noise ratio resulting from the strong absorption and scattering of light by breast tissue.","This method introduces redundancy in data while improving the grayscale and signal-to-noise ratio of the image.","Existing terraced compression algorithms effectively eliminate the data redundancy introduced by frame accumulation but necessitate significant time for manual debugging of threshold values.","Hence, this paper proposes an improved terrace compression algorithm.","The algorithm necessitates solely the input of the desired heterogeneous body size and autonomously calculates the optimal area threshold and gradient threshold by counting the grayscale and combining its distribution.","Experimental acquisition involved multi-wavelength images of heterogeneous bodies exhibiting diverse textures, depths, and thicknesses.","Subsequently, the method was applied after pre-processing to determine the thresholds for terraced compression at each wavelength, coupled with a window function for multi-dimensional image clustering.","The results illustrate the method's efficacy in detecting and identifying various heterogeneous body types, depths, and thicknesses.","This approach is expected to accurately identify the locations and types of breast tumors in the future, thus providing a more dependable tool for early breast cancer screening."],"url":"http://arxiv.org/abs/2404.02744v1","category":"eess.IV"}
{"created":"2024-04-03 13:28:28","title":"Extending direct data-driven predictive control towards systems with finite control sets","abstract":"Although classical model predictive control with finite control sets (FCS-MPC) is quite a popular control method, particularly in the realm of power electronics systems, its direct data-driven predictive control (FCS-DPC) counterpart has received relatively limited attention. In this paper, we introduce a novel reformulation of a commonly used DPC scheme that allows for the application of a modified sphere decoding algorithm, known for its efficiency and prominence in FCS-MPC applications. We test the reformulation on a popular electrical drive example and compare the computation times of sphere decoding FCS-DPC with an enumeration-based and a MIQP method.","sentences":["Although classical model predictive control with finite control sets (FCS-MPC) is quite a popular control method, particularly in the realm of power electronics systems, its direct data-driven predictive control (FCS-DPC) counterpart has received relatively limited attention.","In this paper, we introduce a novel reformulation of a commonly used DPC scheme that allows for the application of a modified sphere decoding algorithm, known for its efficiency and prominence in FCS-MPC applications.","We test the reformulation on a popular electrical drive example and compare the computation times of sphere decoding FCS-DPC with an enumeration-based and a MIQP method."],"url":"http://arxiv.org/abs/2404.02727v1","category":"eess.SY"}
{"created":"2024-04-03 13:22:46","title":"Towards a unifying framework for data-driven predictive control with quadratic regularization","abstract":"Data-driven predictive control (DPC) has recently gained popularity as an alternative to model predictive control (MPC). Amidst the surge in proposed DPC frameworks, upon closer inspection, many of these frameworks are more closely related (or perhaps even equivalent) to each other than it may first appear. We argue for a more formal characterization of these relationships so that results can be freely transferred from one framework to another, rather than being uniquely attributed to a particular framework. We demonstrate this idea by examining the connection between $\\gamma$-DDPC and the original DeePC formulation.","sentences":["Data-driven predictive control (DPC) has recently gained popularity as an alternative to model predictive control (MPC).","Amidst the surge in proposed DPC frameworks, upon closer inspection, many of these frameworks are more closely related (or perhaps even equivalent) to each other than it may first appear.","We argue for a more formal characterization of these relationships so that results can be freely transferred from one framework to another, rather than being uniquely attributed to a particular framework.","We demonstrate this idea by examining the connection between $\\gamma$-DDPC and the original DeePC formulation."],"url":"http://arxiv.org/abs/2404.02721v1","category":"eess.SY"}
{"created":"2024-04-03 13:20:24","title":"Automatic Prompt Selection for Large Language Models","abstract":"Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training and inference. It demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA.","sentences":["Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts.","However, designing effective prompts manually is challenging and time-consuming.","Existing methods for automatic prompt optimization either lack flexibility or efficiency.","In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts.","Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time.","Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training and inference.","It demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA."],"url":"http://arxiv.org/abs/2404.02717v1","category":"cs.CL"}
{"created":"2024-04-03 13:10:05","title":"QDsim: An user-friendly toolbox for simulating large-scale quantum dot device","abstract":"We introduce QDsim, a python package tailored for the rapid generation of charge stability diagrams in large-scale quantum dot devices, extending beyond traditional double or triple dots. QDsim is founded on the constant interaction model from which we rephrase the task of finding the lowest energy charge configuration as a convex optimization problem. Therefore, we can leverage the existing package CVXPY, in combination with an appropriate powerful solver, for the convex optimization which streamlines the creation of stability diagrams and polytopes. Through multiple examples, we demonstrate how QDsim enables the generation of large-scale dataset that can serve a basis for the training of machine-learning models for automated tuning algorithms. While the package currently does not support quantum effects beyond the constant interaction model, QDsim is a tool that directly addresses the critical need for cost-effective and expeditious data acquisition for better tuning algorithms in order to accelerate the development of semiconductor quantum devices.","sentences":["We introduce QDsim, a python package tailored for the rapid generation of charge stability diagrams in large-scale quantum dot devices, extending beyond traditional double or triple dots.","QDsim is founded on the constant interaction model from which we rephrase the task of finding the lowest energy charge configuration as a convex optimization problem.","Therefore, we can leverage the existing package CVXPY, in combination with an appropriate powerful solver, for the convex optimization which streamlines the creation of stability diagrams and polytopes.","Through multiple examples, we demonstrate how QDsim enables the generation of large-scale dataset that can serve a basis for the training of machine-learning models for automated tuning algorithms.","While the package currently does not support quantum effects beyond the constant interaction model, QDsim is a tool that directly addresses the critical need for cost-effective and expeditious data acquisition for better tuning algorithms in order to accelerate the development of semiconductor quantum devices."],"url":"http://arxiv.org/abs/2404.02712v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 12:58:02","title":"Optimizing Peak Age of Information in MEC Systems: Computing Preemption and Non-preemption","abstract":"The freshness of information in real-time monitoring systems has received increasing attention, with Age of Information (AoI) emerging as a novel metric for measuring information freshness. In many applications, update packets need to be computed before being delivered to a destination. Mobile edge computing (MEC) is a promising approach for efficiently accomplishing the computing process, where the transmission process and computation process are coupled, jointly affecting freshness. In this paper, we aim to minimize the average peak AoI (PAoI) in an MEC system. We consider the generate-at-will source model and study when to generate a new update in two edge server setups: 1) computing preemption, where the packet in the computing process will be preempted by the newly arrived one, and 2) non-preemption, where the newly arrived packet will wait in the queue until the current one completes computing. We prove that the fixed threshold policy is optimal in a non-preemptive system for arbitrary transmission time and computation time distributions. In a preemptive system, we show that the transmission-aware threshold policy is optimal when the computing time follows an exponential distribution. Our numerical simulation results not only validate the theoretical findings but also demonstrate that: 1) in our problem, preemptive systems are not always superior to non-preemptive systems, even with exponential distribution, and 2) as the ratio of the mean transmission time to the mean computation time increases, the optimal threshold increases in preemptive systems but decreases in non-preemptive systems.","sentences":["The freshness of information in real-time monitoring systems has received increasing attention, with Age of Information (AoI) emerging as a novel metric for measuring information freshness.","In many applications, update packets need to be computed before being delivered to a destination.","Mobile edge computing (MEC) is a promising approach for efficiently accomplishing the computing process, where the transmission process and computation process are coupled, jointly affecting freshness.","In this paper, we aim to minimize the average peak AoI (PAoI) in an MEC system.","We consider the generate-at-will source model and study when to generate a new update in two edge server setups: 1) computing preemption, where the packet in the computing process will be preempted by the newly arrived one, and 2) non-preemption, where the newly arrived packet will wait in the queue until the current one completes computing.","We prove that the fixed threshold policy is optimal in a non-preemptive system for arbitrary transmission time and computation time distributions.","In a preemptive system, we show that the transmission-aware threshold policy is optimal when the computing time follows an exponential distribution.","Our numerical simulation results not only validate the theoretical findings but also demonstrate that: 1) in our problem, preemptive systems are not always superior to non-preemptive systems, even with exponential distribution, and 2) as the ratio of the mean transmission time to the mean computation time increases, the optimal threshold increases in preemptive systems but decreases in non-preemptive systems."],"url":"http://arxiv.org/abs/2404.02700v1","category":"cs.IT"}
{"created":"2024-04-03 12:39:37","title":"Automated Inference of Graph Transformation Rules","abstract":"The explosion of data available in life sciences is fueling an increasing demand for expressive models and computational methods. Graph transformation is a model for dynamic systems with a large variety of applications. We introduce a novel method of the graph transformation model construction, combining generative and dynamical viewpoints to give a fully automated data-driven model inference method.   The method takes the input dynamical properties, given as a \"snapshot\" of the dynamics encoded by explicit transitions, and constructs a compatible model. The obtained model is guaranteed to be minimal, thus framing the approach as model compression (from a set of transitions into a set of rules). The compression is permissive to a lossy case, where the constructed model is allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics.   The task of graph transformation model inference is naturally highly challenging due to the combinatorics involved. We tackle the exponential explosion by proposing a heuristically minimal translation of the task into a well-established problem, set cover, for which highly optimized solutions exist. We further showcase how our results relate to Kolmogorov complexity expressed in terms of graph transformation.","sentences":["The explosion of data available in life sciences is fueling an increasing demand for expressive models and computational methods.","Graph transformation is a model for dynamic systems with a large variety of applications.","We introduce a novel method of the graph transformation model construction, combining generative and dynamical viewpoints to give a fully automated data-driven model inference method.   ","The method takes the input dynamical properties, given as a \"snapshot\" of the dynamics encoded by explicit transitions, and constructs a compatible model.","The obtained model is guaranteed to be minimal, thus framing the approach as model compression (from a set of transitions into a set of rules).","The compression is permissive to a lossy case, where the constructed model is allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics.   ","The task of graph transformation model inference is naturally highly challenging due to the combinatorics involved.","We tackle the exponential explosion by proposing a heuristically minimal translation of the task into a well-established problem, set cover, for which highly optimized solutions exist.","We further showcase how our results relate to Kolmogorov complexity expressed in terms of graph transformation."],"url":"http://arxiv.org/abs/2404.02692v1","category":"cs.DM"}
{"created":"2024-04-03 12:17:49","title":"History Trees and Their Applications","abstract":"In the theoretical study of distributed communication networks, \"history trees\" are a discrete structure that naturally models the concept that anonymous agents become distinguishable upon receiving different sets of messages from neighboring agents. By conveniently organizing temporal information in a systematic manner, history trees have been instrumental in the development of optimal deterministic algorithms for networks that are both anonymous and dynamically evolving.   This note provides an accessible introduction to history trees, drawing comparisons with more traditional structures found in existing literature and reviewing the latest advancements in the applications of history trees, especially within dynamic networks. Furthermore, it expands the theoretical framework of history trees in new directions, also highlighting several open problems for further investigation.","sentences":["In the theoretical study of distributed communication networks, \"history trees\" are a discrete structure that naturally models the concept that anonymous agents become distinguishable upon receiving different sets of messages from neighboring agents.","By conveniently organizing temporal information in a systematic manner, history trees have been instrumental in the development of optimal deterministic algorithms for networks that are both anonymous and dynamically evolving.   ","This note provides an accessible introduction to history trees, drawing comparisons with more traditional structures found in existing literature and reviewing the latest advancements in the applications of history trees, especially within dynamic networks.","Furthermore, it expands the theoretical framework of history trees in new directions, also highlighting several open problems for further investigation."],"url":"http://arxiv.org/abs/2404.02673v2","category":"cs.DC"}
{"created":"2024-04-03 12:12:18","title":"Bayesian Bi-level Sparse Group Regressions for Macroeconomic Forecasting","abstract":"We propose a Machine Learning approach for optimal macroeconomic forecasting in a high-dimensional setting with covariates presenting a known group structure. Our model encompasses forecasting settings with many series, mixed frequencies, and unknown nonlinearities. We introduce in time-series econometrics the concept of bi-level sparsity, i.e. sparsity holds at both the group level and within groups, and we assume the true model satisfies this assumption. We propose a prior that induces bi-level sparsity, and the corresponding posterior distribution is demonstrated to contract at the minimax-optimal rate, recover the model parameters, and have a support that includes the support of the model asymptotically. Our theory allows for correlation between groups, while predictors in the same group can be characterized by strong covariation as well as common characteristics and patterns. Finite sample performance is illustrated through comprehensive Monte Carlo experiments and a real-data nowcasting exercise of the US GDP growth rate.","sentences":["We propose a Machine Learning approach for optimal macroeconomic forecasting in a high-dimensional setting with covariates presenting a known group structure.","Our model encompasses forecasting settings with many series, mixed frequencies, and unknown nonlinearities.","We introduce in time-series econometrics the concept of bi-level sparsity, i.e. sparsity holds at both the group level and within groups, and we assume the true model satisfies this assumption.","We propose a prior that induces bi-level sparsity, and the corresponding posterior distribution is demonstrated to contract at the minimax-optimal rate, recover the model parameters, and have a support that includes the support of the model asymptotically.","Our theory allows for correlation between groups, while predictors in the same group can be characterized by strong covariation as well as common characteristics and patterns.","Finite sample performance is illustrated through comprehensive Monte Carlo experiments and a real-data nowcasting exercise of the US GDP growth rate."],"url":"http://arxiv.org/abs/2404.02671v1","category":"econ.EM"}
{"created":"2024-04-03 11:42:38","title":"Demonstration of weighted graph optimization on a Rydberg atom array using local light-shifts","abstract":"Neutral atom arrays have emerged as a versatile platform towards scalable quantum computation and optimization. In this paper we present first demonstrations of weighted graph optimization on a Rydberg atom array using annealing with local light-shifts. We verify the ability to prepare weighted graphs in 1D and 2D arrays, including embedding a five vertex non-unit disk graph using nine physical qubits. We find common annealing ramps leading to preparation of the target ground state robustly over a substantial range of different graph weightings. This work provides a route to exploring large-scale optimization of non-planar weighted graphs relevant for solving relevant real-world problems.","sentences":["Neutral atom arrays have emerged as a versatile platform towards scalable quantum computation and optimization.","In this paper we present first demonstrations of weighted graph optimization on a Rydberg atom array using annealing with local light-shifts.","We verify the ability to prepare weighted graphs in 1D and 2D arrays, including embedding a five vertex non-unit disk graph using nine physical qubits.","We find common annealing ramps leading to preparation of the target ground state robustly over a substantial range of different graph weightings.","This work provides a route to exploring large-scale optimization of non-planar weighted graphs relevant for solving relevant real-world problems."],"url":"http://arxiv.org/abs/2404.02658v1","category":"quant-ph"}
{"created":"2024-04-03 11:36:12","title":"Calibrating the Confidence of Large Language Models by Eliciting Fidelity","abstract":"Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.","sentences":["Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless.","However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate.","In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models.","Then, we propose a plug-and-play method to estimate the confidence of language models.","Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets.","Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}.","Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration."],"url":"http://arxiv.org/abs/2404.02655v1","category":"cs.CL"}
{"created":"2024-04-03 10:25:20","title":"Self-similar intermediate asymptotics for first-order mean field games","abstract":"We study the intermediate asymptotic behavior of solutions to the first-order mean field games system with a local coupling, when the initial density is a compactly supported function on the real line, and the coupling is of power type. Addressing a question that was left open in arXiv:2308.00314, we prove that the solutions converge to the self-similar profile. We proceed by analyzing a continuous rescaling of the solution, and identifying an appropriate Lyapunov functional. We identify a critical value for the parameter of the coupling, which determines the qualitative behavior of the functional, and the well-posedness of the infinite horizon system. Accordingly, we also establish, in the subcritical and critical cases, a second convergence result which characterizes the behavior of the full solution as the time horizon approaches infinity. We also prove the corresponding results for the mean field planning problem. A large part of our analysis and methodology apply just as well to arbitrary dimensions. As such, this work is a major step towards settling these questions in the higher-dimensional setting.","sentences":["We study the intermediate asymptotic behavior of solutions to the first-order mean field games system with a local coupling, when the initial density is a compactly supported function on the real line, and the coupling is of power type.","Addressing a question that was left open in arXiv:2308.00314, we prove that the solutions converge to the self-similar profile.","We proceed by analyzing a continuous rescaling of the solution, and identifying an appropriate Lyapunov functional.","We identify a critical value for the parameter of the coupling, which determines the qualitative behavior of the functional, and the well-posedness of the infinite horizon system.","Accordingly, we also establish, in the subcritical and critical cases, a second convergence result which characterizes the behavior of the full solution as the time horizon approaches infinity.","We also prove the corresponding results for the mean field planning problem.","A large part of our analysis and methodology apply just as well to arbitrary dimensions.","As such, this work is a major step towards settling these questions in the higher-dimensional setting."],"url":"http://arxiv.org/abs/2404.02623v1","category":"math.AP"}
{"created":"2024-04-03 10:19:53","title":"Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals","abstract":"This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning graph structures from nodal signals. Our key contribution lies in modeling the signals as Gaussian and stationary on the graph, enabling the development of a graph-learning formulation that combines the strengths of graphical lasso with a more encompassing model. Specifically, we assume that the precision matrix can take any polynomial form of the sought graph, allowing for increased flexibility in modeling nodal relationships. Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the graph and precision matrices, and (ii) characterize its convergence. We evaluate the performance of PGL through comprehensive numerical simulations using both synthetic and real data, demonstrating its superiority over several alternatives. Overall, this approach presents a significant advancement in graph learning and holds promise for various applications in graph-aware signal analysis and beyond.","sentences":["This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning graph structures from nodal signals.","Our key contribution lies in modeling the signals as Gaussian and stationary on the graph, enabling the development of a graph-learning formulation that combines the strengths of graphical lasso with a more encompassing model.","Specifically, we assume that the precision matrix can take any polynomial form of the sought graph, allowing for increased flexibility in modeling nodal relationships.","Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the graph and precision matrices, and (ii) characterize its convergence.","We evaluate the performance of PGL through comprehensive numerical simulations using both synthetic and real data, demonstrating its superiority over several alternatives.","Overall, this approach presents a significant advancement in graph learning and holds promise for various applications in graph-aware signal analysis and beyond."],"url":"http://arxiv.org/abs/2404.02621v1","category":"eess.SP"}
{"created":"2024-04-03 09:51:39","title":"A mixed-integer-programming-based Gauss-Seidel method for multi-leader-multi-follower games","abstract":"We design a computational approach to find equilibria in a class of Nash games possessing a hierarchical structure. By using tools from mixed-integer programming and the characterization of variational equilibria in terms of the Karush--Kuhn--Tucker conditions, we propose a mixed-integer game formulation for solving such a challenging instance. Besides providing an equivalent reformulation, we build upon our previous work and design a proximal Gauss--Seidel method with global convergence guarantees for the case in which the game enjoys a potential structure. In addition to proving the convergence of the resulting scheme, we show its performance on a numerical instance of a ride-hail market problem.","sentences":["We design a computational approach to find equilibria in a class of Nash games possessing a hierarchical structure.","By using tools from mixed-integer programming and the characterization of variational equilibria in terms of the Karush--Kuhn--Tucker conditions, we propose a mixed-integer game formulation for solving such a challenging instance.","Besides providing an equivalent reformulation, we build upon our previous work and design a proximal Gauss--Seidel method with global convergence guarantees for the case in which the game enjoys a potential structure.","In addition to proving the convergence of the resulting scheme, we show its performance on a numerical instance of a ride-hail market problem."],"url":"http://arxiv.org/abs/2404.02605v1","category":"math.OC"}
{"created":"2024-04-03 09:17:38","title":"Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation","abstract":"Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech. Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English. Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness. In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns. Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips. Architectural design choices are validated through comparisons with baseline models and ablation studies using subjective and objective metrics, thus confirming model performance.","sentences":["Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech.","Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English.","Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness.","In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns.","Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips.","Architectural design choices are validated through comparisons with baseline models and ablation studies using subjective and objective metrics, thus confirming model performance."],"url":"http://arxiv.org/abs/2404.02592v1","category":"cs.CL"}
{"created":"2024-04-03 09:09:42","title":"Unsegment Anything by Simulating Deformation","abstract":"Foundation segmentation models, while powerful, pose a significant risk: they enable users to effortlessly extract any objects from any digital content with a single click, potentially leading to copyright infringement or malicious misuse. To mitigate this risk, we introduce a new task \"Anything Unsegmentable\" to grant any image \"the right to be unsegmented\". The ambitious pursuit of the task is to achieve highly transferable adversarial attacks against all prompt-based segmentation models, regardless of model parameterizations and prompts. We highlight the non-transferable and heterogeneous nature of prompt-specific adversarial noises. Our approach focuses on disrupting image encoder features to achieve prompt-agnostic attacks. Intriguingly, targeted feature attacks exhibit better transferability compared to untargeted ones, suggesting the optimal update direction aligns with the image manifold. Based on the observations, we design a novel attack named Unsegment Anything by Simulating Deformation (UAD). Our attack optimizes a differentiable deformation function to create a target deformed image, which alters structural information while preserving achievable feature distance by adversarial example. Extensive experiments verify the effectiveness of our approach, compromising a variety of promptable segmentation models with different architectures and prompt interfaces. We release the code at https://github.com/jiahaolu97/anything-unsegmentable.","sentences":["Foundation segmentation models, while powerful, pose a significant risk: they enable users to effortlessly extract any objects from any digital content with a single click, potentially leading to copyright infringement or malicious misuse.","To mitigate this risk, we introduce a new task \"Anything Unsegmentable\" to grant any image \"the right to be unsegmented\".","The ambitious pursuit of the task is to achieve highly transferable adversarial attacks against all prompt-based segmentation models, regardless of model parameterizations and prompts.","We highlight the non-transferable and heterogeneous nature of prompt-specific adversarial noises.","Our approach focuses on disrupting image encoder features to achieve prompt-agnostic attacks.","Intriguingly, targeted feature attacks exhibit better transferability compared to untargeted ones, suggesting the optimal update direction aligns with the image manifold.","Based on the observations, we design a novel attack named Unsegment Anything by Simulating Deformation (UAD).","Our attack optimizes a differentiable deformation function to create a target deformed image, which alters structural information while preserving achievable feature distance by adversarial example.","Extensive experiments verify the effectiveness of our approach, compromising a variety of promptable segmentation models with different architectures and prompt interfaces.","We release the code at https://github.com/jiahaolu97/anything-unsegmentable."],"url":"http://arxiv.org/abs/2404.02585v1","category":"cs.CV"}
{"created":"2024-04-03 09:08:15","title":"Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization","abstract":"Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It efficiently generates a piecewise linear approximation for the value function, significantly reducing computation time while preserving solution quality, thus marking a promising progression in the treatment of large-scale multistage stochastic programming problems.","sentences":["Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase.","Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems.","Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm.","This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function.","Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems.","It efficiently generates a piecewise linear approximation for the value function, significantly reducing computation time while preserving solution quality, thus marking a promising progression in the treatment of large-scale multistage stochastic programming problems."],"url":"http://arxiv.org/abs/2404.02583v1","category":"cs.LG"}
{"created":"2024-04-03 08:56:23","title":"Quantum computing approach to realistic ESG-friendly stock portfolios","abstract":"Finding an optimal balance between risk and returns in investment portfolios is a central challenge in quantitative finance, often addressed through Markowitz portfolio theory (MPT). While traditional portfolio optimization is carried out in a continuous fashion, as if stocks could be bought in fractional increments, practical implementations often resort to approximations, as fractional stocks are typically not tradeable. While these approximations are effective for large investment budgets, they deteriorate as budgets decrease. To alleviate this issue, a discrete Markowitz portfolio theory (DMPT) with finite budgets and integer stock weights can be formulated, but results in a non-polynomial (NP)-hard problem. Recent progress in quantum processing units (QPUs), including quantum annealers, makes solving DMPT problems feasible. Our study explores portfolio optimization on quantum annealers, establishing a mapping between continuous and discrete Markowitz portfolio theories. We find that correctly normalized discrete portfolios converge to continuous solutions as budgets increase. Our DMPT implementation provides efficient frontier solutions, outperforming traditional rounding methods, even for moderate budgets. Responding to the demand for environmentally and socially responsible investments, we enhance our discrete portfolio optimization with ESG (environmental, social, governance) ratings for EURO STOXX 50 index stocks. We introduce a utility function incorporating ESG ratings to balance risk, return, and ESG-friendliness, and discuss implications for ESG-aware investors.","sentences":["Finding an optimal balance between risk and returns in investment portfolios is a central challenge in quantitative finance, often addressed through Markowitz portfolio theory (MPT).","While traditional portfolio optimization is carried out in a continuous fashion, as if stocks could be bought in fractional increments, practical implementations often resort to approximations, as fractional stocks are typically not tradeable.","While these approximations are effective for large investment budgets, they deteriorate as budgets decrease.","To alleviate this issue, a discrete Markowitz portfolio theory (DMPT) with finite budgets and integer stock weights can be formulated, but results in a non-polynomial (NP)-hard problem.","Recent progress in quantum processing units (QPUs), including quantum annealers, makes solving DMPT problems feasible.","Our study explores portfolio optimization on quantum annealers, establishing a mapping between continuous and discrete Markowitz portfolio theories.","We find that correctly normalized discrete portfolios converge to continuous solutions as budgets increase.","Our DMPT implementation provides efficient frontier solutions, outperforming traditional rounding methods, even for moderate budgets.","Responding to the demand for environmentally and socially responsible investments, we enhance our discrete portfolio optimization with ESG (environmental, social, governance) ratings for EURO STOXX 50 index stocks.","We introduce a utility function incorporating ESG ratings to balance risk, return, and ESG-friendliness, and discuss implications for ESG-aware investors."],"url":"http://arxiv.org/abs/2404.02582v1","category":"q-fin.PM"}
{"created":"2024-04-03 08:54:42","title":"SpectroTranslator: a deep-neural network algorithm to homogenize spectroscopic parameters","abstract":"The emergence of large spectroscopic surveys requires homogenising on the same scale the quantities they measure in order to increase their scientific legacy. We developed the SpectroTranslator, a data-driven deep neural network algorithm that can convert spectroscopic parameters from the base of one survey to another. The algorithm also includes a method to estimate the importance that the various parameters play in the conversion from base A to B. As a showcase, we apply the algorithm to transform effective temperature, surface gravity, metallicity, [Mg/Fe] and los velocity from the base of GALAH into the APOGEE base. We demonstrate the efficiency of the SpectroTranslator algorithm to translate the spectroscopic parameters from one base to another using parameters directly by the survey teams, and are able to achieve a similar performance than previous works that have performed a similar type of conversion but using the full spectrum rather than the spectroscopic parameters, allowing to reduce the computational time, and to use the output of pipelines optimized for each survey. By combining the transformed GALAH catalogue with the APOGEE catalogue, we study the distribution of [Fe/H] and [Mg/Fe] across the Galaxy, and we find that the median distribution of both quantities present a vertical asymmetry at large radii. We attribute it to the recent perturbations generated by the passage of a dwarf galaxy across the disc or by the infall of the Large Magellanic Cloud. Although several aspects still need to be refined, in particular how to deal in an optimal manner with regions of the parameter space meagrely populated by stars in the training sample, the SpectroTranslator already shows its capability and promises to play a crucial role in standardizing various spectroscopic surveys onto a unified basis.","sentences":["The emergence of large spectroscopic surveys requires homogenising on the same scale the quantities they measure in order to increase their scientific legacy.","We developed the SpectroTranslator, a data-driven deep neural network algorithm that can convert spectroscopic parameters from the base of one survey to another.","The algorithm also includes a method to estimate the importance that the various parameters play in the conversion from base A to B. As a showcase, we apply the algorithm to transform effective temperature, surface gravity, metallicity, [Mg/Fe] and los velocity from the base of GALAH into the APOGEE base.","We demonstrate the efficiency of the SpectroTranslator algorithm to translate the spectroscopic parameters from one base to another using parameters directly by the survey teams, and are able to achieve a similar performance than previous works that have performed a similar type of conversion but using the full spectrum rather than the spectroscopic parameters, allowing to reduce the computational time, and to use the output of pipelines optimized for each survey.","By combining the transformed GALAH catalogue with the APOGEE catalogue, we study the distribution of [Fe/H] and [Mg/Fe] across the Galaxy, and we find that the median distribution of both quantities present a vertical asymmetry at large radii.","We attribute it to the recent perturbations generated by the passage of a dwarf galaxy across the disc or by the infall of the Large Magellanic Cloud.","Although several aspects still need to be refined, in particular how to deal in an optimal manner with regions of the parameter space meagrely populated by stars in the training sample, the SpectroTranslator already shows its capability and promises to play a crucial role in standardizing various spectroscopic surveys onto a unified basis."],"url":"http://arxiv.org/abs/2404.02578v1","category":"astro-ph.GA"}
{"created":"2024-04-03 08:35:26","title":"Effect of constraint relaxation on dynamic critical phenomena in minimum vertex cover problem","abstract":"The effects of constraint relaxation on dynamic critical phenomena in the Minimum Vertex Cover (MVC) problem on Erd\\H{o}s-R\\'enyi random graphs are investigated using Markov chain Monte Carlo simulations. Following our previous work that revealed the reduction of the critical temperature by constraint relaxation based on the penalty function method, this study focuses on investigating the critical properties of the relaxation time along its phase boundary. It is found that the dynamical correlation function of MVC with respect to the problem size and the constraint strength follows a universal scaling function. The analysis shows that the relaxation time decreases as the constraints are relaxed. This decrease is more pronounced for the critical amplitude than for the critical exponent, and this result is interpreted in terms of the system's microscopic energy barriers due to the constraint relaxation.","sentences":["The effects of constraint relaxation on dynamic critical phenomena in the Minimum Vertex Cover (MVC) problem on Erd\\H{o}s-R\\'enyi random graphs are investigated using Markov chain Monte Carlo simulations.","Following our previous work that revealed the reduction of the critical temperature by constraint relaxation based on the penalty function method, this study focuses on investigating the critical properties of the relaxation time along its phase boundary.","It is found that the dynamical correlation function of MVC with respect to the problem size and the constraint strength follows a universal scaling function.","The analysis shows that the relaxation time decreases as the constraints are relaxed.","This decrease is more pronounced for the critical amplitude than for the critical exponent, and this result is interpreted in terms of the system's microscopic energy barriers due to the constraint relaxation."],"url":"http://arxiv.org/abs/2404.02564v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 08:21:03","title":"Optimal Riemannian metric for Poincar{\u00e9} inequalities and how to ideally precondition Langevin dymanics","abstract":"Poincar{\\'e} inequality is a fundamental property that rises naturally in different branches of mathematics. The associated Poincar{\\'e} constant plays a central role in many applications, since it governs the convergence of various practical algorithms. For instance, the convergence rate of the Langevin dynamics is exactly given by the Poincar{\\'e} constant. This paper investigates a Riemannian version of Poincar{\\'e} inequality where a positive definite weighting matrix field (i.e. a Riemannian metric) is introduced to improve the Poincar{\\'e} constant, and therefore the performances of the associated algorithm. Assuming the underlying measure is a \\emph{moment measure}, we show that an optimal metric exists and the resulting Poincar{\\'e} constant is 1. We demonstrate that such optimal metric is necessarily a \\emph{Stein kernel}, offering a novel perspective on these complex but central mathematical objects that are hard to obtain in practice. We further discuss how to numerically obtain the optimal metric by deriving an implementable optimization algorithm. The resulting method is illustrated on a few simple but nontrivial examples, where solutions are revealed to be rather sophisticated. We also demonstrate how to design efficient Langevin-based sampling schemes by utilizing the precomputed optimal metric as a preconditioner.","sentences":["Poincar{\\'e} inequality is a fundamental property that rises naturally in different branches of mathematics.","The associated Poincar{\\'e} constant plays a central role in many applications, since it governs the convergence of various practical algorithms.","For instance, the convergence rate of the Langevin dynamics is exactly given by the Poincar{\\'e} constant.","This paper investigates a Riemannian version of Poincar{\\'e} inequality where a positive definite weighting matrix field (i.e. a Riemannian metric) is introduced to improve the Poincar{\\'e} constant, and therefore the performances of the associated algorithm.","Assuming the underlying measure is a \\emph{moment measure}, we show that an optimal metric exists and the resulting Poincar{\\'e} constant is 1.","We demonstrate that such optimal metric is necessarily a \\emph{Stein kernel}, offering a novel perspective on these complex but central mathematical objects that are hard to obtain in practice.","We further discuss how to numerically obtain the optimal metric by deriving an implementable optimization algorithm.","The resulting method is illustrated on a few simple but nontrivial examples, where solutions are revealed to be rather sophisticated.","We also demonstrate how to design efficient Langevin-based sampling schemes by utilizing the precomputed optimal metric as a preconditioner."],"url":"http://arxiv.org/abs/2404.02554v1","category":"math.PR"}
{"created":"2024-04-03 08:18:30","title":"Degree Sequence Optimization and Extremal Degree Enumerators","abstract":"The degree sequence optimization problem is to find a subgraph of a given graph which maximizes the sum of given functions evaluated at the subgraph degrees. Here we study this problem by replacing degree sequences, via suitable nonlinear transformations, by suitable degree enumerators, and we introduce suitable degree enumerator polytopes.   We characterize their vertices, that is, the extremal degree enumerators, for complete graphs and some complete bipartite graphs, and use these characterizations to obtain simpler and faster algorithms for optimization over degree sequences for such graphs.","sentences":["The degree sequence optimization problem is to find a subgraph of a given graph which maximizes the sum of given functions evaluated at the subgraph degrees.","Here we study this problem by replacing degree sequences, via suitable nonlinear transformations, by suitable degree enumerators, and we introduce suitable degree enumerator polytopes.   ","We characterize their vertices, that is, the extremal degree enumerators, for complete graphs and some complete bipartite graphs, and use these characterizations to obtain simpler and faster algorithms for optimization over degree sequences for such graphs."],"url":"http://arxiv.org/abs/2404.02551v1","category":"math.CO"}
{"created":"2024-04-03 08:05:09","title":"Analysis and approximation to parabolic optimal control problems with measure-valued controls in time","abstract":"In this paper, we investigate an optimal control problem governed by parabolic equations with measure-valued controls over time. We establish the well-posedness of the optimal control problem and derive the first-order optimality condition using Clarke's subgradients, revealing a sparsity structure in time for the optimal control. Consequently, these optimal control problems represent a generalization of impulse control for evolution equations. To discretize the optimal control problem, we employ the space-time finite element method. Here, the state equation is approximated using piecewise linear and continuous finite elements in space, alongside a Petrov-Galerkin method utilizing piecewise constant trial functions and piecewise linear and continuous test functions in time. The control variable is discretized using the variational discretization concept. For error estimation, we initially derive a priori error estimates and stabilities for the finite element discretizations of the state and adjoint equations. Subsequently, we establish weak-* convergence for the control under the norm $\\mathcal{M}(\\bar I_c;L^2(\\omega))$, with a convergence order of $O(h^\\frac{1}{2}+\\tau^\\frac{1}{4})$ for the state.","sentences":["In this paper, we investigate an optimal control problem governed by parabolic equations with measure-valued controls over time.","We establish the well-posedness of the optimal control problem and derive the first-order optimality condition using Clarke's subgradients, revealing a sparsity structure in time for the optimal control.","Consequently, these optimal control problems represent a generalization of impulse control for evolution equations.","To discretize the optimal control problem, we employ the space-time finite element method.","Here, the state equation is approximated using piecewise linear and continuous finite elements in space, alongside a Petrov-Galerkin method utilizing piecewise constant trial functions and piecewise linear and continuous test functions in time.","The control variable is discretized using the variational discretization concept.","For error estimation, we initially derive a priori error estimates and stabilities for the finite element discretizations of the state and adjoint equations.","Subsequently, we establish weak-* convergence for the control under the norm $\\mathcal{M}(\\bar I_c;L^2(\\omega))$, with a convergence order of $O(h^\\frac{1}{2}+\\tau^\\frac{1}{4})$ for the state."],"url":"http://arxiv.org/abs/2404.02546v1","category":"math.OC"}
{"created":"2024-04-03 07:33:57","title":"Computationally Efficient Unsupervised Deep Learning for Robust Joint AP Clustering and Beamforming Design in Cell-Free Systems","abstract":"In this paper, we consider robust joint access point (AP) clustering and beamforming design with imperfect channel state information (CSI) in cell-free systems. Specifically, we jointly optimize AP clustering and beamforming with imperfect CSI to simultaneously maximize the worst-case sum rate and minimize the number of AP clustering under power constraint and the sparsity constraint of AP clustering. By transformations, the semi-infinite constraints caused by the imperfect CSI are converted into more tractable forms for facilitating a computationally efficient unsupervised deep learning algorithm. In addition, to further reduce the computational complexity, a computationally effective unsupervised deep learning algorithm is proposed to implement robust joint AP clustering and beamforming design with imperfect CSI in cell-free systems. Numerical results demonstrate that the proposed unsupervised deep learning algorithm achieves a higher worst-case sum rate under a smaller number of AP clustering with computational efficiency.","sentences":["In this paper, we consider robust joint access point (AP) clustering and beamforming design with imperfect channel state information (CSI) in cell-free systems.","Specifically, we jointly optimize AP clustering and beamforming with imperfect CSI to simultaneously maximize the worst-case sum rate and minimize the number of AP clustering under power constraint and the sparsity constraint of AP clustering.","By transformations, the semi-infinite constraints caused by the imperfect CSI are converted into more tractable forms for facilitating a computationally efficient unsupervised deep learning algorithm.","In addition, to further reduce the computational complexity, a computationally effective unsupervised deep learning algorithm is proposed to implement robust joint AP clustering and beamforming design with imperfect CSI in cell-free systems.","Numerical results demonstrate that the proposed unsupervised deep learning algorithm achieves a higher worst-case sum rate under a smaller number of AP clustering with computational efficiency."],"url":"http://arxiv.org/abs/2404.02531v1","category":"cs.IT"}
{"created":"2024-04-03 07:10:18","title":"HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras","abstract":"Three-dimensional perception from multi-view cameras is a crucial component in autonomous driving systems, which involves multiple tasks like 3D object detection and bird's-eye-view (BEV) semantic segmentation. To improve perception precision, large image encoders, high-resolution images, and long-term temporal inputs have been adopted in recent 3D perception models, bringing remarkable performance gains. However, these techniques are often incompatible in training and inference scenarios due to computational resource constraints. Besides, modern autonomous driving systems prefer to adopt an end-to-end framework for multi-task 3D perception, which can simplify the overall system architecture and reduce the implementation complexity. However, conflict between tasks often arises when optimizing multiple tasks jointly within an end-to-end 3D perception model. To alleviate these issues, we present an end-to-end framework named HENet for multi-task 3D perception in this paper. Specifically, we propose a hybrid image encoding network, using a large image encoder for short-term frames and a small image encoder for long-term temporal frames. Then, we introduce a temporal feature integration module based on the attention mechanism to fuse the features of different frames extracted by the two aforementioned hybrid image encoders. Finally, according to the characteristics of each perception task, we utilize BEV features of different grid sizes, independent BEV encoders, and task decoders for different tasks. Experimental results show that HENet achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, including 3D object detection and BEV semantic segmentation. The source code and models will be released at https://github.com/VDIGPKU/HENet.","sentences":["Three-dimensional perception from multi-view cameras is a crucial component in autonomous driving systems, which involves multiple tasks like 3D object detection and bird's-eye-view (BEV) semantic segmentation.","To improve perception precision, large image encoders, high-resolution images, and long-term temporal inputs have been adopted in recent 3D perception models, bringing remarkable performance gains.","However, these techniques are often incompatible in training and inference scenarios due to computational resource constraints.","Besides, modern autonomous driving systems prefer to adopt an end-to-end framework for multi-task 3D perception, which can simplify the overall system architecture and reduce the implementation complexity.","However, conflict between tasks often arises when optimizing multiple tasks jointly within an end-to-end 3D perception model.","To alleviate these issues, we present an end-to-end framework named HENet for multi-task 3D perception in this paper.","Specifically, we propose a hybrid image encoding network, using a large image encoder for short-term frames and a small image encoder for long-term temporal frames.","Then, we introduce a temporal feature integration module based on the attention mechanism to fuse the features of different frames extracted by the two aforementioned hybrid image encoders.","Finally, according to the characteristics of each perception task, we utilize BEV features of different grid sizes, independent BEV encoders, and task decoders for different tasks.","Experimental results show that HENet achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, including 3D object detection and BEV semantic segmentation.","The source code and models will be released at https://github.com/VDIGPKU/HENet."],"url":"http://arxiv.org/abs/2404.02517v1","category":"cs.CV"}
{"created":"2024-04-03 07:07:29","title":"Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots","abstract":"Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments. To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots. We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots. Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration. Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates. Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel odometry online for creating a reasonable constraint. The proposed method is validated through three experiments. The first indoor experiment shows that the proposed method is robust in severe degeneracy cases (long corridors) and changes in the wheel radii. The second outdoor experiment demonstrates that our method accurately estimates the sensor trajectory despite being in rough outdoor terrain owing to online uncertainty estimation of wheel odometry. The third experiment shows the proposed online calibration enables robust odometry estimation in changing terrains.","sentences":["Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments.","To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots.","We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots.","Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration.","Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates.","Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel odometry online for creating a reasonable constraint.","The proposed method is validated through three experiments.","The first indoor experiment shows that the proposed method is robust in severe degeneracy cases (long corridors) and changes in the wheel radii.","The second outdoor experiment demonstrates that our method accurately estimates the sensor trajectory despite being in rough outdoor terrain owing to online uncertainty estimation of wheel odometry.","The third experiment shows the proposed online calibration enables robust odometry estimation in changing terrains."],"url":"http://arxiv.org/abs/2404.02515v1","category":"cs.RO"}
{"created":"2024-04-03 06:55:59","title":"Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach","abstract":"In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges. In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost. This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations. To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ and $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\\varepsilon^2)$ for both settings given a stochastic gradient oracle with variance $\\sigma^2$. Compared with the prior work \\cite{wai-fw-2017}, our framework relaxes the assumption of the optimal solution being a strict interior point of the feasible set and enjoys wider applicability for large-scale training using a stochastic gradient oracle. We also demonstrate the efficiency of our algorithms with various numerical experiments.","sentences":["In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges.","In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost.","This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations.","To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ and $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\\varepsilon^2)$ for both settings given a stochastic gradient oracle with variance $\\sigma^2$. Compared with the prior work \\cite{wai-fw-2017}, our framework relaxes the assumption of the optimal solution being a strict interior point of the feasible set and enjoys wider applicability for large-scale training using a stochastic gradient oracle.","We also demonstrate the efficiency of our algorithms with various numerical experiments."],"url":"http://arxiv.org/abs/2404.02511v1","category":"math.OC"}
