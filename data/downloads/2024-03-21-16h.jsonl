{"created":"2024-03-19 17:59:09","title":"TexTile: A Differentiable Metric for Texture Tileability","abstract":"We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy. We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality. Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field.","sentences":["We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability).","Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture.","In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures.","Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.","Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy.","We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality.","Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field."],"url":"http://arxiv.org/abs/2403.12961v1","category":"cs.CV"}
{"created":"2024-03-19 17:58:02","title":"WHAC: World-grounded Humans and Cameras","abstract":"Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available.","sentences":["Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem.","In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera.","Our approach is founded on two key observations.","Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth.","Secondly, human motions inherently provide absolute spatial cues.","By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques.","Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories.","Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework.","We will make the code and dataset publicly available."],"url":"http://arxiv.org/abs/2403.12959v1","category":"cs.CV"}
{"created":"2024-03-19 17:54:34","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models","abstract":"Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements.","sentences":["Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting.","Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments.","To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs.","Our method is based on the notion of modulating per-class prototypes in the shared embedding space.","By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering.","At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy.","A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods.","Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements."],"url":"http://arxiv.org/abs/2403.12952v1","category":"cs.CV"}
{"created":"2024-03-19 17:48:38","title":"DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset","abstract":"The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.","sentences":["The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies.","However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour.","As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity.","In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months.","We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability.","We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup."],"url":"http://arxiv.org/abs/2403.12945v1","category":"cs.RO"}
{"created":"2024-03-19 17:47:37","title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers","abstract":"While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations. We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos. Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications. Project website: vid2robot.github.io","sentences":["While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans?","This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment.","We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots.","Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions.","This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory.","The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task.","To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations.","We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos.","Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications.","Project website: vid2robot.github.io"],"url":"http://arxiv.org/abs/2403.12943v1","category":"cs.RO"}
{"created":"2024-03-19 17:43:08","title":"Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models","abstract":"Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases. We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision. The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes. Through our analysis, we demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information extraction, highlighting the potential of LLMs in revolutionising the way legal information is processed and utilised, offering significant implications for legal research and practice.","sentences":["Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions.","The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public.","With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient.","This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases.","We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data.","Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision.","The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes.","Through our analysis, we demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information extraction, highlighting the potential of LLMs in revolutionising the way legal information is processed and utilised, offering significant implications for legal research and practice."],"url":"http://arxiv.org/abs/2403.12936v1","category":"cs.CL"}
{"created":"2024-03-19 17:32:01","title":"Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models","abstract":"Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content. This lends to it a quality of creativity which can be empowering in the early stages of design. In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas. Participants typically prompted in a straightforward manner with concise instructions. We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users. Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM.","sentences":["Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.","This lends to it a quality of creativity which can be empowering in the early stages of design.","In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them.","We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.","Participants typically prompted in a straightforward manner with concise instructions.","We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.","Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM."],"url":"http://arxiv.org/abs/2403.12928v1","category":"cs.HC"}
{"created":"2024-03-19 17:21:29","title":"Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts","abstract":"Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets.","sentences":["Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting.","Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights.","However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions.","To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs.","Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection.","Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting.","We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets."],"url":"http://arxiv.org/abs/2403.12918v1","category":"cs.CL"}
{"created":"2024-03-19 17:11:25","title":"Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers","abstract":"In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS). MIMS support both small peer-to-peer networks and large groups. Misinformation in the former may spread due to recipients' trust in the sender while in the latter, misinformation can directly reach a wide audience. The encryption of MIMS makes it difficult to address misinformation directly. As such, chatbots have become an alternative solution where users can disclose their chat content directly to fact checking services. To understand how effective fact checking chatbots are as an intervention and how trust in three different fact checkers (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents. We found mixed results for the fact checkers but support for the chatbot intervention overall. We also found a striking contradiction between participants' trust in the fact checkers and their behaviour towards them. Specifically, those who reported a high level of trust in the government performed worse and tended to follow the fact checking tool less when it was endorsed by the government.","sentences":["In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS).","MIMS support both small peer-to-peer networks and large groups.","Misinformation in the former may spread due to recipients' trust in the sender while in the latter, misinformation can directly reach a wide audience.","The encryption of MIMS makes it difficult to address misinformation directly.","As such, chatbots have become an alternative solution where users can disclose their chat content directly to fact checking services.","To understand how effective fact checking chatbots are as an intervention and how trust in three different fact checkers (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents.","We found mixed results for the fact checkers but support for the chatbot intervention overall.","We also found a striking contradiction between participants' trust in the fact checkers and their behaviour towards them.","Specifically, those who reported a high level of trust in the government performed worse and tended to follow the fact checking tool less when it was endorsed by the government."],"url":"http://arxiv.org/abs/2403.12913v1","category":"cs.HC"}
{"created":"2024-03-19 17:08:24","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","abstract":"Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.","sentences":["Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations.","However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.","Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback?","In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections.","We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions.","This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback.","Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.","Videos and code are available at https://yay-robot.github.io/."],"url":"http://arxiv.org/abs/2403.12910v1","category":"cs.RO"}
{"created":"2024-03-19 16:56:47","title":"Measurement of vector boson production cross sections and their ratios using $pp$ collisions at $\\sqrt{s}=13.6$ TeV with the ATLAS detector","abstract":"Fiducial and total $W^\\pm$ and $Z$ boson cross sections, their ratios and the ratio of top-antitop-quark pair and $W$-boson fiducial cross sections are measured in proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}=13.6$ TeV, corresponding to an integrated luminosity of 29 fb$^{-1}$ of data collected in 2022 by the ATLAS experiment at the Large Hadron Collider. The measured fiducial cross-section values for $W^+\\to \\ell^+\\nu$, $W^-\\to \\ell^-\\bar{\\nu}$, and $Z\\to \\ell^+\\ell^-$ ($\\ell=e$ or $\\mu$) boson productions are $4250\\pm 150$ pb, $3310\\pm 120$ pb, and $744\\pm 20$ pb, respectively, where the uncertainty is the total uncertainty, including that arising from the luminosity of about 2.2%. The measurements are in agreement with Standard-Model predictions calculated at next-to-next-to-leading-order in $\\alpha_s$, next-to-next-to-leading logarithmic accuracy and next-to-leading-order electroweak accuracy.","sentences":["Fiducial and total $W^\\pm$ and $Z$ boson cross sections, their ratios and the ratio of top-antitop-quark pair and $W$-boson fiducial cross sections are measured in proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}=13.6$ TeV, corresponding to an integrated luminosity of 29 fb$^{-1}$ of data collected in 2022 by the ATLAS experiment at the Large Hadron Collider.","The measured fiducial cross-section values for $W^+\\to \\ell^+\\nu$, $W^-\\to \\ell^-\\bar{\\nu}$, and $Z\\to \\ell^+\\ell^-$ ($\\ell=e$ or $\\mu$) boson productions are $4250\\pm 150$ pb, $3310\\pm 120$ pb, and $744\\pm 20$ pb, respectively, where the uncertainty is the total uncertainty, including that arising from the luminosity of about 2.2%.","The measurements are in agreement with Standard-Model predictions calculated at next-to-next-to-leading-order in $\\alpha_s$, next-to-next-to-leading logarithmic accuracy and next-to-leading-order electroweak accuracy."],"url":"http://arxiv.org/abs/2403.12902v1","category":"hep-ex"}
{"created":"2024-03-19 16:53:53","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference","abstract":"The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data. This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence.","sentences":["The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure.","This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services.","Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency.","Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes.","Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data.","This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence."],"url":"http://arxiv.org/abs/2403.12900v1","category":"cs.DC"}
{"created":"2024-03-19 16:45:45","title":"Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain. Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations. We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies. With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers. Finally, we provide simulation results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS.","sentences":["This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain.","Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations.","We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies.","With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers.","Finally, we provide simulation results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS."],"url":"http://arxiv.org/abs/2403.12893v1","category":"cs.IT"}
{"created":"2024-03-19 16:40:57","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types","abstract":"In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric. Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food.","sentences":["In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF).","The goal is to acquire (i.e., scoop) food items from a bowl.","However, achieving robust and adaptive food manipulation is particularly challenging.","To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping.","Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors.","We validate the effectiveness of our approach by conducting experiments on a real robot.","We also compare its performance with a baseline.","The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric.","Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food."],"url":"http://arxiv.org/abs/2403.12891v1","category":"cs.RO"}
{"created":"2024-03-19 16:33:26","title":"EmoVOCA: Speech-Driven Emotional 3D Talking Heads","abstract":"The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available.","sentences":["The domain of 3D talking head generation has witnessed significant progress in recent years.","A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions.","Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions.","In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences.","To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face.","Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature.","Our code and pre-trained model will be made available."],"url":"http://arxiv.org/abs/2403.12886v1","category":"cs.CV"}
{"created":"2024-03-19 16:25:30","title":"Clustered Mallows Model","abstract":"Rankings are a type of preference elicitation that arise in experiments where assessors arrange items, for example, in decreasing order of utility. Orderings of n items labelled {1,...,n} denoted are permutations that reflect strict preferences. For a number of reasons, strict preferences can be unrealistic assumptions for real data. For example, when items share common traits it may be reasonable to attribute them equal ranks. Also, there can be different importance attributions to decisions that form the ranking. In a situation with, for example, a large number of items, an assessor may wish to rank at top a certain number items; to rank other items at the bottom and to express indifference to all others. In addition, when aggregating opinions, a judging body might be decisive about some parts of the rank but ambiguous for others. In this paper we extend the well-known Mallows (Mallows, 1957) model (MM) to accommodate item indifference, a phenomenon that can be in place for a variety of reasons, such as those above mentioned.The underlying grouping of similar items motivates the proposed Clustered Mallows Model (CMM). The CMM can be interpreted as a Mallows distribution for tied ranks where ties are learned from the data. The CMM provides the flexibility to combine strict and indifferent relations, achieving a simpler and robust representation of rank collections in the form of ordered clusters. Bayesian inference for the CMM is in the class of doubly-intractable problems since the model's normalisation constant is not available in closed form. We overcome this challenge by sampling from the posterior with a version of the exchange algorithm \\citep{murray2006}. Real data analysis of food preferences and results of Formula 1 races are presented, illustrating the CMM in practical situations.","sentences":["Rankings are a type of preference elicitation that arise in experiments where assessors arrange items, for example, in decreasing order of utility.","Orderings of n items labelled {1,...,n} denoted are permutations that reflect strict preferences.","For a number of reasons, strict preferences can be unrealistic assumptions for real data.","For example, when items share common traits it may be reasonable to attribute them equal ranks.","Also, there can be different importance attributions to decisions that form the ranking.","In a situation with, for example, a large number of items, an assessor may wish to rank at top a certain number items; to rank other items at the bottom and to express indifference to all others.","In addition, when aggregating opinions, a judging body might be decisive about some parts of the rank but ambiguous for others.","In this paper we extend the well-known Mallows (Mallows, 1957) model (MM) to accommodate item indifference, a phenomenon that can be in place for a variety of reasons, such as those above mentioned.","The underlying grouping of similar items motivates the proposed Clustered Mallows Model (CMM).","The CMM can be interpreted as a Mallows distribution for tied ranks where ties are learned from the data.","The CMM provides the flexibility to combine strict and indifferent relations, achieving a simpler and robust representation of rank collections in the form of ordered clusters.","Bayesian inference for the CMM is in the class of doubly-intractable problems since the model's normalisation constant is not available in closed form.","We overcome this challenge by sampling from the posterior with a version of the exchange algorithm \\citep{murray2006}.","Real data analysis of food preferences and results of Formula 1 races are presented, illustrating the CMM in practical situations."],"url":"http://arxiv.org/abs/2403.12880v1","category":"stat.ME"}
{"created":"2024-03-19 16:21:42","title":"Boundary layers in thermal convection are fluctuation-dominated","abstract":"We study the dynamics of thermal and momentum boundary layers in three-dimensional direct numerical simulations of Rayleigh-B\\'enard convection for the Rayleigh number range $10^5 \\le Ra \\le 10^{11}$ and $Pr=0.7$. Using a Cartesian slab with horizontal periodic boundary conditions and an aspect ratio of 4, we obtain statistical homogeneity in the horizontal $x$- and $y$-directions, thus approximating an infinitely extended system. We observe that upon canonical use of long-time and area averages, a coherent mean flow is practically absent. Instead, the velocity field close to the wall is a collection of differently oriented local shear patches interspersed by shear-free, incoherent flow regions. These shear patches occupy an area fraction of approximately $40\\%$ for all $Ra$. Rather than resulting in a pronounced mean with small fluctuations about it, the velocity field is dominated by strong fluctuations of all three components around a non-existent or weak mean. This feature is particularly pronounced for $Ra \\ge 10^9$. We discuss the consequences of these observations for convection layers with larger aspect ratios, including boundary layer instabilities and the resulting turbulent heat transport.","sentences":["We study the dynamics of thermal and momentum boundary layers in three-dimensional direct numerical simulations of Rayleigh-B\\'enard convection for the Rayleigh number range $10^5 \\le Ra \\le 10^{11}$ and $Pr=0.7$. Using a Cartesian slab with horizontal periodic boundary conditions and an aspect ratio of 4, we obtain statistical homogeneity in the horizontal $x$- and $y$-directions, thus approximating an infinitely extended system.","We observe that upon canonical use of long-time and area averages, a coherent mean flow is practically absent.","Instead, the velocity field close to the wall is a collection of differently oriented local shear patches interspersed by shear-free, incoherent flow regions.","These shear patches occupy an area fraction of approximately $40\\%$ for all $Ra$. Rather than resulting in a pronounced mean with small fluctuations about it, the velocity field is dominated by strong fluctuations of all three components around a non-existent or weak mean.","This feature is particularly pronounced for $Ra \\ge 10^9$.","We discuss the consequences of these observations for convection layers with larger aspect ratios, including boundary layer instabilities and the resulting turbulent heat transport."],"url":"http://arxiv.org/abs/2403.12877v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 16:12:25","title":"Regularization in Spider-Style Strategy Discovery and Schedule Construction","abstract":"To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem. In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider. We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property.","sentences":["To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem.","In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider.","We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property."],"url":"http://arxiv.org/abs/2403.12869v1","category":"cs.AI"}
{"created":"2024-03-19 15:57:32","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems","abstract":"Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings.","sentences":["Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise.","Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks.","RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform.","Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP.","We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings."],"url":"http://arxiv.org/abs/2403.12853v1","category":"cs.RO"}
{"created":"2024-03-19 15:57:04","title":"Generative Enhancement for 3D Medical Images","abstract":"The limited availability of 3D medical image datasets, due to privacy concerns and high collection or annotation costs, poses significant challenges in the field of medical imaging. While a promising alternative is the use of synthesized medical data, there are few solutions for realistic 3D medical image synthesis due to difficulties in backbone design and fewer 3D training samples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel generative approach to the synthesis of 3D medical images and the enhancement of existing datasets using conditional diffusion models. Our method begins with a 2D slice, noted as the informed slice to serve the patient prior, and propagates the generation process using a 3D segmentation mask. By decomposing the 3D medical images into masks and patient prior information, GEM-3D offers a flexible yet effective solution for generating versatile 3D images from existing datasets. GEM-3D can enable dataset enhancement by combining informed slice selection and generation at random positions, along with editable mask volumes to introduce large variations in diffusion sampling. Moreover, as the informed slice contains patient-wise information, GEM-3D can also facilitate counterfactual image synthesis and dataset-level de-enhancement with desired control. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D is capable of synthesizing high-quality 3D medical images with volumetric consistency, offering a straightforward solution for dataset enhancement during inference. The code is available at https://github.com/HKU-MedAI/GEM-3D.","sentences":["The limited availability of 3D medical image datasets, due to privacy concerns and high collection or annotation costs, poses significant challenges in the field of medical imaging.","While a promising alternative is the use of synthesized medical data, there are few solutions for realistic 3D medical image synthesis due to difficulties in backbone design and fewer 3D training samples compared to 2D counterparts.","In this paper, we propose GEM-3D, a novel generative approach to the synthesis of 3D medical images and the enhancement of existing datasets using conditional diffusion models.","Our method begins with a 2D slice, noted as the informed slice to serve the patient prior, and propagates the generation process using a 3D segmentation mask.","By decomposing the 3D medical images into masks and patient prior information, GEM-3D offers a flexible yet effective solution for generating versatile 3D images from existing datasets.","GEM-3D can enable dataset enhancement by combining informed slice selection and generation at random positions, along with editable mask volumes to introduce large variations in diffusion sampling.","Moreover, as the informed slice contains patient-wise information, GEM-3D can also facilitate counterfactual image synthesis and dataset-level de-enhancement with desired control.","Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D is capable of synthesizing high-quality 3D medical images with volumetric consistency, offering a straightforward solution for dataset enhancement during inference.","The code is available at https://github.com/HKU-MedAI/GEM-3D."],"url":"http://arxiv.org/abs/2403.12852v1","category":"eess.IV"}
{"created":"2024-03-19 15:51:21","title":"MELTing point: Mobile Evaluation of Language Transformers","abstract":"Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound. Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost. Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments.","sentences":["Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''.","However, their runtime requirements have prevented them from being broadly deployed on mobile.","As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs).","To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices.","We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   ","Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models.","Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound.","Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost.","Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience.","Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost.","We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments."],"url":"http://arxiv.org/abs/2403.12844v2","category":"cs.LG"}
{"created":"2024-03-19 15:43:16","title":"How Spammers and Scammers Leverage AI-Generated Images on Facebook for Audience Growth","abstract":"Much of the research and discourse on risks from artificial intelligence (AI) image generators, such as DALL-E and Midjourney, has centered around whether they could be used to inject false information into political discourse. We show that spammers and scammers - seemingly motivated by profit or clout, not ideology - are already using AI-generated images to gain significant traction on Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated images to users who neither follow the Pages posting the images nor realize that the images are AI-generated, highlighting the need for improved transparency and provenance standards as AI models proliferate.","sentences":["Much of the research and discourse on risks from artificial intelligence (AI) image generators, such as DALL-E and Midjourney, has centered around whether they could be used to inject false information into political discourse.","We show that spammers and scammers - seemingly motivated by profit or clout, not ideology - are already using AI-generated images to gain significant traction on Facebook.","At times, the Facebook Feed is recommending unlabeled AI-generated images to users who neither follow the Pages posting the images nor realize that the images are AI-generated, highlighting the need for improved transparency and provenance standards as AI models proliferate."],"url":"http://arxiv.org/abs/2403.12838v1","category":"cs.CY"}
{"created":"2024-03-19 15:42:46","title":"Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments","abstract":"Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater. This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene. The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization. Probabilistic data association is used to determine observation to landmark correspondences. Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates. Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available. Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene.","sentences":["Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater.","This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene.","The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization.","Probabilistic data association is used to determine observation to landmark correspondences.","Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates.","Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available.","Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene."],"url":"http://arxiv.org/abs/2403.12837v1","category":"cs.RO"}
{"created":"2024-03-19 15:24:49","title":"Answer Set Programming for Flexible Payroll Management","abstract":"Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries. Moreover, the rules are often complex and change regularly. Therefore, payroll management systems must be flexible in design. In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard. It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios. We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances.","sentences":["Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries.","Moreover, the rules are often complex and change regularly.","Therefore, payroll management systems must be flexible in design.","In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard.","It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios.","We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances."],"url":"http://arxiv.org/abs/2403.12823v1","category":"cs.LO"}
{"created":"2024-03-19 15:21:10","title":"FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer","abstract":"The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.","sentences":["The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution.","Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets.","Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance.","For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture.","FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking.","Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models.","Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer."],"url":"http://arxiv.org/abs/2403.12821v1","category":"cs.LG"}
{"created":"2024-03-19 15:18:03","title":"Two-dimensional inference of divertor plasma characteristics: advancements to a multi-instrument Bayesian analysis system","abstract":"An integrated data analysis system based on Bayesian inference has been developed for application to data from multiple diagnostics over the two-dimensional cross-section of tokamak divertors. Tests of the divertor multi-instrument Bayesian analysis system (D-MIBAS) on a synthetic data set (including realistic experimental uncertainties) generated from SOLPS-ITER predictions of the MAST-U divertor have been performed. The resulting inference was within 6\\%, 5\\% and 30\\% median absolute percentage error of the SOLPS-predicted electron temperature, electron density and neutral atomic hydrogen density, respectively, across a two-dimensional poloidal cross-section of the MAST-U Super-X outer divertor.   To accommodate molecular contributions to Balmer emission, an advanced emission model has been developed which is shown to be crucial for inference accuracy. Our D-MIBAS system utilises a mesh aligned to poloidal magnetic flux-surfaces, throughout the divertor, with plasma parameters assigned to each mesh vertex and collectively considered in the inference. This allowed comprehensive forward models to multiple diagnostics and the inclusion of expected physics. This is shown to be important for inference precision when including molecular contributions to Balmer emission. These developments pave the way for accurate two-dimensional electron temperature, electron density and neutral atomic hydrogen density inferences for MAST-U divertor experimental data for the first time.","sentences":["An integrated data analysis system based on Bayesian inference has been developed for application to data from multiple diagnostics over the two-dimensional cross-section of tokamak divertors.","Tests of the divertor multi-instrument Bayesian analysis system (D-MIBAS) on a synthetic data set (including realistic experimental uncertainties) generated from SOLPS-ITER predictions of the MAST-U divertor have been performed.","The resulting inference was within 6\\%, 5\\% and 30\\% median absolute percentage error of the SOLPS-predicted electron temperature, electron density and neutral atomic hydrogen density, respectively, across a two-dimensional poloidal cross-section of the MAST-U Super-X outer divertor.   ","To accommodate molecular contributions to Balmer emission, an advanced emission model has been developed which is shown to be crucial for inference accuracy.","Our D-MIBAS system utilises a mesh aligned to poloidal magnetic flux-surfaces, throughout the divertor, with plasma parameters assigned to each mesh vertex and collectively considered in the inference.","This allowed comprehensive forward models to multiple diagnostics and the inclusion of expected physics.","This is shown to be important for inference precision when including molecular contributions to Balmer emission.","These developments pave the way for accurate two-dimensional electron temperature, electron density and neutral atomic hydrogen density inferences for MAST-U divertor experimental data for the first time."],"url":"http://arxiv.org/abs/2403.12819v1","category":"physics.plasm-ph"}
{"created":"2024-03-19 15:17:16","title":"Searching for Lepton Flavor Violation with the CMS Experiment","abstract":"Searches for lepton flavor violation (LFV) stand at the forefront of experimental particle physics research, offering a sensitive probe to many scenarios of physics beyond the Standard Model. The high proton-proton collision energy and luminosity provided by the CERN Large Hadron Collider (LHC) and the excellent CMS detector performance allow for an extensive program of LFV searches. This article reviews a broad range of LFV searches conducted at the CMS experiment using data collected in LHC Run 2, including $\\tau\\to3\\mu$ decays, Higgs boson decays, and top quark production and decays. In each analysis, the online and offline event selections, signal modeling, background suppression and estimation, and statistical interpretation are elucidated. These searches involve various final state particles in a large transverse momentum range, showcasing the capability of the CMS experiment in exploring fundamental questions in particle physics.","sentences":["Searches for lepton flavor violation (LFV) stand at the forefront of experimental particle physics research, offering a sensitive probe to many scenarios of physics beyond the Standard Model.","The high proton-proton collision energy and luminosity provided by the CERN Large Hadron Collider (LHC) and the excellent CMS detector performance allow for an extensive program of LFV searches.","This article reviews a broad range of LFV searches conducted at the CMS experiment using data collected in LHC Run 2, including $\\tau\\to3\\mu$ decays, Higgs boson decays, and top quark production and decays.","In each analysis, the online and offline event selections, signal modeling, background suppression and estimation, and statistical interpretation are elucidated.","These searches involve various final state particles in a large transverse momentum range, showcasing the capability of the CMS experiment in exploring fundamental questions in particle physics."],"url":"http://arxiv.org/abs/2403.12817v1","category":"hep-ex"}
{"created":"2024-03-19 15:15:19","title":"Re-identification from histopathology images","abstract":"In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases. These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks. This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy. We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD). We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue. We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset. Based on our findings, we formulated a risk assessment scheme to estimate the risk to the patient's privacy prior to publication.","sentences":["In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases.","These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks.","This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy.","We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).","We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue.","We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset.","Based on our findings, we formulated a risk assessment scheme to estimate the risk to the patient's privacy prior to publication."],"url":"http://arxiv.org/abs/2403.12816v1","category":"cs.CV"}
{"created":"2024-03-19 15:07:22","title":"Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models","abstract":"In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.","sentences":["In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions.","Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction.","Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models.","On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored.","Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models.","We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.","Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers.","Our code is available: https://github.com/casszhao/multilingual-faith."],"url":"http://arxiv.org/abs/2403.12809v1","category":"cs.CL"}
{"created":"2024-03-19 15:06:53","title":"Contextual Moral Value Alignment Through Context-Based Aggregation","abstract":"Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.","sentences":["Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI.","Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance.","In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation.","Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input.","The proposed system shows better results in term of alignment to human value compared to the state of the art."],"url":"http://arxiv.org/abs/2403.12805v1","category":"cs.AI"}
{"created":"2024-03-19 15:01:19","title":"RelationVLM: Making Large Vision-Language Models Understand Visual Relations","abstract":"The development of Large Vision-Language Models (LVLMs) is striving to catch up with the success of Large Language Models (LLMs), yet it faces more challenges to be resolved. Very recent works enable LVLMs to localize object-level visual contents and ground text to them. Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data. In this work, we present RelationVLM, a large vision-language model capable of comprehending various levels and types of relations whether across multiple images or within a video. Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms. Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive in-context capability of reasoning from few-shot examples by comparison. This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence.","sentences":["The development of Large Vision-Language Models (LVLMs) is striving to catch up with the success of Large Language Models (LLMs), yet it faces more challenges to be resolved.","Very recent works enable LVLMs to localize object-level visual contents and ground text to them.","Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data.","In this work, we present RelationVLM, a large vision-language model capable of comprehending various levels and types of relations whether across multiple images or within a video.","Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms.","Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive in-context capability of reasoning from few-shot examples by comparison.","This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence."],"url":"http://arxiv.org/abs/2403.12801v1","category":"cs.CV"}
{"created":"2024-03-19 15:01:14","title":"Investigating Text Shortening Strategy in BERT: Truncation vs Summarization","abstract":"The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative. In this study, we investigate the performance of document truncation and summarization in text classification tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive summarization. This study explains what happened to the result, leading to further research in order to exploit the potential of document summarization as a shortening alternative. The code and data used in this work are publicly available in https://github.com/mirzaalimm/TruncationVsSummarization.","sentences":["The parallelism of Transformer-based models comes at the cost of their input max-length.","Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative.","In this study, we investigate the performance of document truncation and summarization in text classification tasks.","Each of the two was investigated with several variations.","This study also investigated how close their performances are to the performance of full-text.","We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests.","This study shows how the summaries outperform the majority of truncation method variations and lose to only one.","The best strategy obtained in this study is taking the head of the document.","The second is extractive summarization.","This study explains what happened to the result, leading to further research in order to exploit the potential of document summarization as a shortening alternative.","The code and data used in this work are publicly available in https://github.com/mirzaalimm/TruncationVsSummarization."],"url":"http://arxiv.org/abs/2403.12799v1","category":"cs.CL"}
{"created":"2024-03-19 14:52:53","title":"Modeling RIS from Electromagnetic Principles to Communication Systems--Part I: Synthesis and Characterization of a Scalable Anomalous Reflector","abstract":"This work aims to build connections between the electromagnetic and communication aspects of Reconfigurable Intelligent Surfaces (RIS) by proposing a methodology to combine outputs from electromagnetic RIS design into an RIS-tailored system-level simulator and a ray tracer. In this first part of the contribution, a periodic anomalous reflector is designed using an algebraic array antenna scattering synthesis technique that enables electromagnetically accurate modeling of scattering surfaces with both static and reconfigurable scattering characteristics. The multi-mode periodic structure, capable of scattering into several anomalous angles through manipulation of reactive loads, is then cropped into finite-sized arrays, and the quantization effects of the load reactances on the array scattering are analyzed. An experimental anomalous reflector is demonstrated with a comparison between simulated and measured scattering performance. In the second part, the simulated receiving and transmitting scattering patterns of the anomalous reflector are utilized to build an electromagnetically consistent path loss model of an RIS into a system-level simulator. Large-scale fading is analyzed in simple scenarios of RIS-assisted wireless networks to verify the communication model, and an indoor scenario measurement using the manufactured anomalous reflector sample to support the simulation analysis. After verifying the connections between electromagnetic and communication aspects through simulations and measurements, the proposed communication model can be used for a broad range of RIS designs to perform large-scale system-level and ray-tracing simulations in realistic scenarios.","sentences":["This work aims to build connections between the electromagnetic and communication aspects of Reconfigurable Intelligent Surfaces (RIS) by proposing a methodology to combine outputs from electromagnetic RIS design into an RIS-tailored system-level simulator and a ray tracer.","In this first part of the contribution, a periodic anomalous reflector is designed using an algebraic array antenna scattering synthesis technique that enables electromagnetically accurate modeling of scattering surfaces with both static and reconfigurable scattering characteristics.","The multi-mode periodic structure, capable of scattering into several anomalous angles through manipulation of reactive loads, is then cropped into finite-sized arrays, and the quantization effects of the load reactances on the array scattering are analyzed.","An experimental anomalous reflector is demonstrated with a comparison between simulated and measured scattering performance.","In the second part, the simulated receiving and transmitting scattering patterns of the anomalous reflector are utilized to build an electromagnetically consistent path loss model of an RIS into a system-level simulator.","Large-scale fading is analyzed in simple scenarios of RIS-assisted wireless networks to verify the communication model, and an indoor scenario measurement using the manufactured anomalous reflector sample to support the simulation analysis.","After verifying the connections between electromagnetic and communication aspects through simulations and measurements, the proposed communication model can be used for a broad range of RIS designs to perform large-scale system-level and ray-tracing simulations in realistic scenarios."],"url":"http://arxiv.org/abs/2403.12790v1","category":"physics.app-ph"}
{"created":"2024-03-19 14:48:10","title":"Large-Scale RIS Enabled Air-Ground Channels: Near-Field Modeling and Analysis","abstract":"Existing works mainly rely on the far-field planar-wave-based channel model to assess the performance of reconfigurable intelligent surface (RIS)-enabled wireless communication systems. However, when the transmitter and receiver are in near-field ranges, this will result in relatively low computing accuracy. To tackle this challenge, we initially develop an analytical framework for sub-array partitioning. This framework divides the large-scale RIS array into multiple sub-arrays, effectively reducing modeling complexity while maintaining acceptable accuracy. Then, we develop a beam domain channel model based on the proposed sub-array partition framework for large-scale RIS-enabled UAV-to-vehicle communication systems, which can be used to efficiently capture the sparse features in RIS-enabled UAV-to-vehicle channels in both near-field and far-field ranges. Furthermore, some important propagation characteristics of the proposed channel model, including the spatial cross-correlation functions (CCFs), temporal auto-correlation functions (ACFs), frequency correlation functions (CFs), and channel capacities with respect to the different physical features of the RIS and non-stationary properties of the channel model are derived and analyzed. Finally, simulation results are provided to demonstrate that the proposed framework is helpful to achieve a good tradeoff between model complexity and accuracy for investigating the channel propagation characteristics, and therefore providing highly-efficient communications in RIS-enabled UAV-to-vehicle wireless networks.","sentences":["Existing works mainly rely on the far-field planar-wave-based channel model to assess the performance of reconfigurable intelligent surface (RIS)-enabled wireless communication systems.","However, when the transmitter and receiver are in near-field ranges, this will result in relatively low computing accuracy.","To tackle this challenge, we initially develop an analytical framework for sub-array partitioning.","This framework divides the large-scale RIS array into multiple sub-arrays, effectively reducing modeling complexity while maintaining acceptable accuracy.","Then, we develop a beam domain channel model based on the proposed sub-array partition framework for large-scale RIS-enabled UAV-to-vehicle communication systems, which can be used to efficiently capture the sparse features in RIS-enabled UAV-to-vehicle channels in both near-field and far-field ranges.","Furthermore, some important propagation characteristics of the proposed channel model, including the spatial cross-correlation functions (CCFs), temporal auto-correlation functions (ACFs), frequency correlation functions (CFs), and channel capacities with respect to the different physical features of the RIS and non-stationary properties of the channel model are derived and analyzed.","Finally, simulation results are provided to demonstrate that the proposed framework is helpful to achieve a good tradeoff between model complexity and accuracy for investigating the channel propagation characteristics, and therefore providing highly-efficient communications in RIS-enabled UAV-to-vehicle wireless networks."],"url":"http://arxiv.org/abs/2403.12781v1","category":"eess.SP"}
{"created":"2024-03-19 14:44:54","title":"Discover and Mitigate Multiple Biased Subgroups in Image Classifiers","abstract":"Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier. We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models. Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups. Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers. The code is available at https://github.com/ZhangAIPI/DIM.","sentences":["Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications.","Such subgroups are typically unknown due to the absence of subgroup labels.","Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness.","Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   ","In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers.","Our approach decomposes the image features into multiple components that represent multiple subgroups.","This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier.","We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models.","Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies.","Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups.","Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers.","The code is available at https://github.com/ZhangAIPI/DIM."],"url":"http://arxiv.org/abs/2403.12777v1","category":"cs.CV"}
{"created":"2024-03-19 14:43:52","title":"Is open source software culture enough to make AI a common ?","abstract":"Language models (LM or LLM) are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users. Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation. In this contribution, we examine the concept of the commons and its relevance for thinking about LM. We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies. Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers.","sentences":["Language models (LM or LLM) are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users.","Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation.","In this contribution, we examine the concept of the commons and its relevance for thinking about LM.","We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies.","Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers."],"url":"http://arxiv.org/abs/2403.12774v1","category":"cs.CY"}
{"created":"2024-03-19 14:18:59","title":"Ring nebulae around Wolf-Rayet stars in M33 as seen by SITELLE","abstract":"We have conducted an analysis of nebulae around Wolf-Rayet (WR) stars in M33 using data collected by the imaging Fourier transform spectrometer SITELLE at the Canada-France-Hawaii telescope as part of the SIGNALS Large Program. Of the 211 known Wolf-Rayet stars in M33, 178 are located in the fields observed in this study. We present the results of this analysis in the form of a comprehensive summary of all nebulae found around the observed WR stars. Based on three criteria we find to be the most effective for their detection, we detect a clear association with a circumstellar bubble around 33 of them (19\\%). Our results show that the presence of bubbles does not correlate with the spectral type of the central star. The mean diameter of the WR nebulae we have found is 21 parsecs.","sentences":["We have conducted an analysis of nebulae around Wolf-Rayet (WR) stars in M33 using data collected by the imaging Fourier transform spectrometer SITELLE at the Canada-France-Hawaii telescope as part of the SIGNALS Large Program.","Of the 211 known Wolf-Rayet stars in M33, 178 are located in the fields observed in this study.","We present the results of this analysis in the form of a comprehensive summary of all nebulae found around the observed WR stars.","Based on three criteria we find to be the most effective for their detection, we detect a clear association with a circumstellar bubble around 33 of them (19\\%).","Our results show that the presence of bubbles does not correlate with the spectral type of the central star.","The mean diameter of the WR nebulae we have found is 21 parsecs."],"url":"http://arxiv.org/abs/2403.12754v1","category":"astro-ph.SR"}
{"created":"2024-03-19 14:11:26","title":"Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection","abstract":"Brain tumor image segmentation is a challenging research topic in which deep-learning models have presented the best results. However, the traditional way of training those models from many pre-annotated images leaves several unanswered questions. Hence methodologies, such as Feature Learning from Image Markers (FLIM), have involved an expert in the learning loop to reduce human effort in data annotation and build models sufficiently deep for a given problem. FLIM has been successfully used to create encoders, estimating the filters of all convolutional layers from patches centered at marker voxels. In this work, we present Multi-Step (MS) FLIM - a user-assisted approach to estimating and selecting the most relevant filters from multiple FLIM executions. MS-FLIM is used only for the first convolutional layer, and the results already indicate improvement over FLIM. For evaluation, we build a simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma segmentation using T1Gd and FLAIR MRI scans, varying the encoder's training method, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two datasets. The results show that the sU-Net based on MS-FLIM outperforms the other training methods and achieves effectiveness within the standard deviations of the SOTA models.","sentences":["Brain tumor image segmentation is a challenging research topic in which deep-learning models have presented the best results.","However, the traditional way of training those models from many pre-annotated images leaves several unanswered questions.","Hence methodologies, such as Feature Learning from Image Markers (FLIM), have involved an expert in the learning loop to reduce human effort in data annotation and build models sufficiently deep for a given problem.","FLIM has been successfully used to create encoders, estimating the filters of all convolutional layers from patches centered at marker voxels.","In this work, we present Multi-Step (MS) FLIM - a user-assisted approach to estimating and selecting the most relevant filters from multiple FLIM executions.","MS-FLIM is used only for the first convolutional layer, and the results already indicate improvement over FLIM.","For evaluation, we build a simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma segmentation using T1Gd and FLAIR MRI scans, varying the encoder's training method, using FLIM, MS-FLIM, and backpropagation algorithm.","Also, we compared these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two datasets.","The results show that the sU-Net based on MS-FLIM outperforms the other training methods and achieves effectiveness within the standard deviations of the SOTA models."],"url":"http://arxiv.org/abs/2403.12748v1","category":"cs.CV"}
{"created":"2024-03-19 13:48:30","title":"Small Scale Reflection for the Working Lean User","abstract":"We present the design and implementation of the Small Scale Reflection proof methodology and tactic language (a.k.a. SSR) for the Lean 4 proof assistant. Like its Coq predecessor SSReflect, our Lean 4 implementation, dubbed LeanSSR, provides powerful rewriting principles and means for effective management of hypotheses in the proof context. Unlike SSReflect for Coq, LeanSSR does not require explicit switching between the logical and symbolic representation of a goal, allowing for even more concise proof scripts that seamlessly combine deduction steps with proofs by computation.   In this paper, we first provide a gentle introduction to the principles of structuring mechanised proofs using LeanSSR. Next, we show how the native support for metaprogramming in Lean 4 makes it possible to develop LeanSSR entirely within the proof assistant, greatly improving the overall experience of both tactic implementers and proof engineers. Finally, we demonstrate the utility of LeanSSR by conducting two case studies: (a) porting a collection of Coq lemmas about sequences from the widely used Mathematical Components library and (b) reimplementing proofs in the finite set library of Lean's mathlib4. Both case studies show significant reduction in proof sizes.","sentences":["We present the design and implementation of the Small Scale Reflection proof methodology and tactic language (a.k.a. SSR) for the Lean 4 proof assistant.","Like its Coq predecessor SSReflect, our Lean 4 implementation, dubbed LeanSSR, provides powerful rewriting principles and means for effective management of hypotheses in the proof context.","Unlike SSReflect for Coq, LeanSSR does not require explicit switching between the logical and symbolic representation of a goal, allowing for even more concise proof scripts that seamlessly combine deduction steps with proofs by computation.   ","In this paper, we first provide a gentle introduction to the principles of structuring mechanised proofs using LeanSSR.","Next, we show how the native support for metaprogramming in Lean 4 makes it possible to develop LeanSSR entirely within the proof assistant, greatly improving the overall experience of both tactic implementers and proof engineers.","Finally, we demonstrate the utility of LeanSSR by conducting two case studies: (a) porting a collection of Coq lemmas about sequences from the widely used Mathematical Components library and (b) reimplementing proofs in the finite set library of Lean's mathlib4.","Both case studies show significant reduction in proof sizes."],"url":"http://arxiv.org/abs/2403.12733v1","category":"cs.PL"}
{"created":"2024-03-19 13:45:34","title":"What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks","abstract":"Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed -- accounting for a diverse range of their engineering and social aspects -- in view of the anticipated use case.","sentences":["Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging.","In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols.","While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs.","Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed -- accounting for a diverse range of their engineering and social aspects -- in view of the anticipated use case."],"url":"http://arxiv.org/abs/2403.12730v1","category":"cs.HC"}
{"created":"2024-03-19 13:41:11","title":"Python Fuzzing for Trustworthy Machine Learning Frameworks","abstract":"Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems. Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software. Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python. We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is integrated in GitLab CI. To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, and related projects such as h5py. Applying our dynamic analysis pipeline to these targets, we were able to discover 3 new bugs and propose fixes for them.","sentences":["Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems.","Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software.","Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python.","We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset.","Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection.","Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly.","Furthermore, the proposed pipeline is integrated in GitLab CI.","To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, and related projects such as h5py.","Applying our dynamic analysis pipeline to these targets, we were able to discover 3 new bugs and propose fixes for them."],"url":"http://arxiv.org/abs/2403.12723v1","category":"cs.CR"}
{"created":"2024-03-19 13:30:47","title":"CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation","abstract":"This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community. A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles. Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts.","sentences":["This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space.","The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents.","The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology.","All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment.","The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community.","A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles.","Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts."],"url":"http://arxiv.org/abs/2403.12721v1","category":"cs.CL"}
{"created":"2024-03-19 13:08:54","title":"AnimateDiff-Lightning: Cross-Model Diffusion Distillation","abstract":"We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility. We are pleased to release our distilled AnimateDiff-Lightning model for the community's use.","sentences":["We present AnimateDiff-Lightning for lightning-fast video generation.","Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation.","We discuss our modifications to adapt it for the video modality.","Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility.","We are pleased to release our distilled AnimateDiff-Lightning model for the community's use."],"url":"http://arxiv.org/abs/2403.12706v1","category":"cs.CV"}
{"created":"2024-03-19 13:01:58","title":"ReProbes: An Architecture for Reconfigurable and Adaptive Probes","abstract":"Modern distributed systems are highly dynamic and scalable, requiring monitoring solutions that can adapt to rapid changes. Monitoring systems that rely on external probes can only achieve adaptation through expensive operations such as deployment, undeployment, and reconfiguration. This poster paper introduces ReProbes, a class of adaptive monitoring probes that can handle rapid changes in data collection strategies. ReProbe offers controllable and configurable self-adaptive capabilities for data transmission, collection, and analysis methods. The resulting architecture can effectively enhance probe adaptability when qualitatively compared to state-of-the-art monitoring solutions.","sentences":["Modern distributed systems are highly dynamic and scalable, requiring monitoring solutions that can adapt to rapid changes.","Monitoring systems that rely on external probes can only achieve adaptation through expensive operations such as deployment, undeployment, and reconfiguration.","This poster paper introduces ReProbes, a class of adaptive monitoring probes that can handle rapid changes in data collection strategies.","ReProbe offers controllable and configurable self-adaptive capabilities for data transmission, collection, and analysis methods.","The resulting architecture can effectively enhance probe adaptability when qualitatively compared to state-of-the-art monitoring solutions."],"url":"http://arxiv.org/abs/2403.12703v1","category":"cs.SE"}
{"created":"2024-03-19 12:45:52","title":"Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision","abstract":"This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions. The source code is publicly available.","sentences":["This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition.","We propose a novel audio-visual method for compound expression recognition.","Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules.","Notably, our method does not use any training data specific to the target task.","The method is evaluated in multi-corpus training and cross-corpus validation setups.","Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions.","The source code is publicly available."],"url":"http://arxiv.org/abs/2403.12687v1","category":"cs.CV"}
{"created":"2024-03-19 12:45:00","title":"Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints","abstract":"Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects. However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits. These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical. To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive. Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics. This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements. We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening. Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag. See supplementary material at https://sites.google.com/view/bilbo-bag.","sentences":["Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects.","However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits.","These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical.","To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive.","Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics.","This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements.","We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening.","Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag.","See supplementary material at https://sites.google.com/view/bilbo-bag."],"url":"http://arxiv.org/abs/2403.12685v1","category":"cs.RO"}
{"created":"2024-03-19 12:39:37","title":"Concepts and methods for predicting viral evolution","abstract":"The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year. These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies. Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution. Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes. From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year. Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection. Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app.","sentences":["The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year.","These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies.","Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution.","Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes.","From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year.","Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection.","Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app."],"url":"http://arxiv.org/abs/2403.12684v1","category":"q-bio.PE"}
{"created":"2024-03-19 12:24:20","title":"Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights","abstract":"The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use. Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of chatbots and the accuracy of retrieval systems.","sentences":["The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights.","Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights.","Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations.","The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances.","The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions.","A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use.","Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of chatbots and the accuracy of retrieval systems."],"url":"http://arxiv.org/abs/2403.12678v1","category":"cs.CL"}
{"created":"2024-03-19 12:13:52","title":"Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine","abstract":"Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points. In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score based on simulated annealing, which is widely used for optimization problems. The proposed evaluation method was also validated using numerical experiments.","sentences":["Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points.","In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM.","However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted.","In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure.","The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points.","Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score based on simulated annealing, which is widely used for optimization problems.","The proposed evaluation method was also validated using numerical experiments."],"url":"http://arxiv.org/abs/2403.12672v1","category":"cs.LG"}
{"created":"2024-03-19 12:13:33","title":"Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering","abstract":"AI assistants for coding are on the rise. However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code. This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue. Then, we propose a systematic approach based on prompt-altering methods to achieve better code security of (even proprietary black-box) AI-based code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs. In sum, we propose and evaluate three prompt altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination. Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user. We assess the effectiveness of the proposed methods on the GitHub Copilot using the OpenVPN project in realistic scenarios, and we demonstrate that the proposed methods reduce the number of insecure generated code samples by up to 16\\% and increase the number of secure code by up to 8\\%. Since our approach does not require access to the internals of the AI models, it can be in general applied to any AI-based code synthesizer, not only GitHub Copilot.","sentences":["AI assistants for coding are on the rise.","However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code.","This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue.","Then, we propose a systematic approach based on prompt-altering methods to achieve better code security of (even proprietary black-box) AI-based code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs.","In sum, we propose and evaluate three prompt altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination.","Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user.","We assess the effectiveness of the proposed methods on the GitHub Copilot using the OpenVPN project in realistic scenarios, and we demonstrate that the proposed methods reduce the number of insecure generated code samples by up to 16\\% and increase the number of secure code by up to 8\\%.","Since our approach does not require access to the internals of the AI models, it can be in general applied to any AI-based code synthesizer, not only GitHub Copilot."],"url":"http://arxiv.org/abs/2403.12671v1","category":"cs.CR"}
{"created":"2024-03-19 11:56:21","title":"Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making","abstract":"In many applications, model ensembling proves to be better than a single predictive model. Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML). The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models. In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks. This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The given ensemble is analyzed from different perspectives. We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models. We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions. Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of variables. Summarizing obtained insights, we can investigate and adjust the weights with a modification tool to tune the ensemble in the desired way. The application provides the aforementioned aspects through dedicated interactive visualizations, making it accessible to a diverse audience. We believe the cattleia can support users in decision-making and deepen the comprehension of AutoML frameworks.","sentences":["In many applications, model ensembling proves to be better than a single predictive model.","Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML).","The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models.","In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks.","This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML.","The given ensemble is analyzed from different perspectives.","We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models.","We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions.","Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of variables.","Summarizing obtained insights, we can investigate and adjust the weights with a modification tool to tune the ensemble in the desired way.","The application provides the aforementioned aspects through dedicated interactive visualizations, making it accessible to a diverse audience.","We believe the cattleia can support users in decision-making and deepen the comprehension of AutoML frameworks."],"url":"http://arxiv.org/abs/2403.12664v1","category":"cs.LG"}
{"created":"2024-03-19 11:49:35","title":"ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems","abstract":"Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online for ease of reproduction.","sentences":["Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations.","Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands.","This research area, particularly in the context of DRS, is nascent and faces three core challenges.","Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights.","Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS.","Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods.","To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS.","ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement.","Our code is available online for ease of reproduction."],"url":"http://arxiv.org/abs/2403.12660v2","category":"cs.IR"}
{"created":"2024-03-19 11:34:15","title":"InBox: Recommendation with Knowledge Graph using Interest Box Embedding","abstract":"Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability. Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of knowledge graph data and interest connectivity. To address these limitations, we introduce a novel embedding-based model called InBox. Specifically, various knowledge graph entities and relations are embedded as points or boxes, while user interests are modeled as boxes encompassing interaction history. Representing interests as boxes enables containing collections of item points related to that interest. We further propose that an interest comprises diverse basic concepts, and box intersection naturally supports concept combination. Across three training steps, InBox significantly outperforms state-of-the-art methods like HAKG and KGIN on recommendation tasks. Further analysis provides meaningful insights into the variable value of different KG data for recommendations. In summary, InBox advances recommender systems through box-based interest and concept modeling for sophisticated knowledge graph exploitation.","sentences":["Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability.","Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items.","However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity.","This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way.","Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests.","This homogenization limits the precise exploitation of knowledge graph data and interest connectivity.","To address these limitations, we introduce a novel embedding-based model called InBox.","Specifically, various knowledge graph entities and relations are embedded as points or boxes, while user interests are modeled as boxes encompassing interaction history.","Representing interests as boxes enables containing collections of item points related to that interest.","We further propose that an interest comprises diverse basic concepts, and box intersection naturally supports concept combination.","Across three training steps, InBox significantly outperforms state-of-the-art methods like HAKG and KGIN on recommendation tasks.","Further analysis provides meaningful insights into the variable value of different KG data for recommendations.","In summary, InBox advances recommender systems through box-based interest and concept modeling for sophisticated knowledge graph exploitation."],"url":"http://arxiv.org/abs/2403.12649v1","category":"cs.IR"}
{"created":"2024-03-19 10:59:21","title":"PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic Glove Applications","abstract":"Controlling hand exoskeletons to assist individuals with grasping tasks poses a challenge due to the difficulty in understanding user intentions. We propose that most daily grasping tasks during activities of daily living (ADL) can be deduced by analyzing object geometries (simple and complex) from 3D point clouds. The study introduces PointGrasp, a real-time system designed for identifying household scenes semantically, aiming to support and enhance assistance during ADL for tailored end-to-end grasping tasks. The system comprises an RGB-D camera with an inertial measurement unit and a microprocessor integrated into a tendon-driven soft robotic glove. The RGB-D camera processes 3D scenes at a rate exceeding 30 frames per second. The proposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple and 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies and pinpoints reachable objects. This system shows promise in end-to-end vision-driven robotic-assisted rehabilitation manual tasks.","sentences":["Controlling hand exoskeletons to assist individuals with grasping tasks poses a challenge due to the difficulty in understanding user intentions.","We propose that most daily grasping tasks during activities of daily living (ADL) can be deduced by analyzing object geometries (simple and complex) from 3D point clouds.","The study introduces PointGrasp, a real-time system designed for identifying household scenes semantically, aiming to support and enhance assistance during ADL for tailored end-to-end grasping tasks.","The system comprises an RGB-D camera with an inertial measurement unit and a microprocessor integrated into a tendon-driven soft robotic glove.","The RGB-D camera processes 3D scenes at a rate exceeding 30 frames per second.","The proposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple and 0.11 $\\pm$ 0.06 cm for complex geometries.","Within each mode, it identifies and pinpoints reachable objects.","This system shows promise in end-to-end vision-driven robotic-assisted rehabilitation manual tasks."],"url":"http://arxiv.org/abs/2403.12631v1","category":"cs.RO"}
{"created":"2024-03-19 10:53:40","title":"Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code","abstract":"In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness. Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs). Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code. This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information. Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving. Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation. Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies. This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification. The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1","sentences":["In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness.","Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs).","Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code.","This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information.","Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving.","Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation.","Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies.","This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification.","The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1"],"url":"http://arxiv.org/abs/2403.12627v1","category":"cs.AI"}
{"created":"2024-03-19 10:11:14","title":"LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models","abstract":"Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams. Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs under the zero-shot setting, which aligns with real examinations, and compared their performance across different subjects. We also conduct an in-depth analysis to check whether GPT-4 can automatically score subjective predictions. Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese LLMs.","sentences":["Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications.","However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture.","Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue.","Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types.","To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper.","LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.","It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams.","Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of LLMs.","We have assessed 11 Chinese LLMs under the zero-shot setting, which aligns with real examinations, and compared their performance across different subjects.","We also conduct an in-depth analysis to check whether GPT-4 can automatically score subjective predictions.","Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese LLMs."],"url":"http://arxiv.org/abs/2403.12601v1","category":"cs.CL"}
{"created":"2024-03-19 09:48:18","title":"FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting","abstract":"Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a relevant discrete set of actions. Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets. This approach relies on inherent computations made by the actor-critic DRL architecture. We demonstrate the validity of our approach with simulation results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition.","sentences":["Designing a humanoid locomotion controller is challenging and classically split up in sub-problems.","Footstep planning is one of those, where the sequence of footsteps is defined.","Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem.","In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*).","However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters.","In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference.","Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps.","In contrast, other methods necessitate the selection of a relevant discrete set of actions.","Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets.","This approach relies on inherent computations made by the actor-critic DRL architecture.","We demonstrate the validity of our approach with simulation results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition."],"url":"http://arxiv.org/abs/2403.12589v1","category":"cs.RO"}
{"created":"2024-03-19 09:47:54","title":"Machine Learning of the Prime Distribution","abstract":"In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.","sentences":["In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem.","We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques.","Numerical experiments that we perform corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2403.12588v1","category":"cs.IT"}
{"created":"2024-03-19 09:34:11","title":"EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks","abstract":"Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection. Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules. Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps. For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps. Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models.","sentences":["Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions.","However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed.","Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge.","In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler.","Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection.","Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules.","Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps.","For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps.","Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models."],"url":"http://arxiv.org/abs/2403.12574v1","category":"cs.CV"}
{"created":"2024-03-19 09:30:56","title":"Compound Expression Recognition via Multi Model Ensemble","abstract":"Compound Expression Recognition (CER) plays a crucial role in interpersonal interactions. Due to the existence of Compound Expressions , human emotional expressions are complex, requiring consideration of both local and global facial expressions to make judgments. In this paper, to address this issue, we propose a solution based on ensemble learning methods for Compound Expression Recognition. Specifically, our task is classification, where we train three expression classification models based on convolutional networks, Vision Transformers, and multi-scale local attention networks. Then, through model ensemble using late fusion, we merge the outputs of multiple models to predict the final result. Our method achieves high accuracy on RAF-DB and is able to recognize expressions through zero-shot on certain portions of C-EXPR-DB.","sentences":["Compound Expression Recognition (CER) plays a crucial role in interpersonal interactions.","Due to the existence of Compound Expressions , human emotional expressions are complex, requiring consideration of both local and global facial expressions to make judgments.","In this paper, to address this issue, we propose a solution based on ensemble learning methods for Compound Expression Recognition.","Specifically, our task is classification, where we train three expression classification models based on convolutional networks, Vision Transformers, and multi-scale local attention networks.","Then, through model ensemble using late fusion, we merge the outputs of multiple models to predict the final result.","Our method achieves high accuracy on RAF-DB and is able to recognize expressions through zero-shot on certain portions of C-EXPR-DB."],"url":"http://arxiv.org/abs/2403.12572v1","category":"cs.CV"}
{"created":"2024-03-19 09:22:50","title":"Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices","abstract":"Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices. For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential. However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues. In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference. We design a memory-efficient management method to support memory-demanding inference in TEEs. By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in the trusted operating system. Additionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny deep learning library, and Tinylibm (827 LoCs), a tiny math library, to support efficient inference in TEEs. We implemented a prototype on Raspberry Pi 3B+ and evaluated it using three well-known lightweight DNN models. The experimental results demonstrate that our design significantly improves inference speed by 3.13 times and reduces power consumption by over 66.5% compared to non-memory optimization method in TEEs.","sentences":["Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices.","For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential.","However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues.","In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference.","We design a memory-efficient management method to support memory-demanding inference in TEEs.","By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in the trusted operating system.","Additionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny deep learning library, and Tinylibm (827 LoCs), a tiny math library, to support efficient inference in TEEs.","We implemented a prototype on Raspberry Pi 3B+ and evaluated it using three well-known lightweight DNN models.","The experimental results demonstrate that our design significantly improves inference speed by 3.13 times and reduces power consumption by over 66.5% compared to non-memory optimization method in TEEs."],"url":"http://arxiv.org/abs/2403.12568v1","category":"cs.CR"}
{"created":"2024-03-19 09:17:25","title":"Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service","abstract":"Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best hack found, we then compare 512, 256, and 128 tokens length. We find that removing stopwords while keeping punctuation and low-frequency words is the best hack. Some of our setups manage to outperform taking 512 first tokens using a smaller 128 or 256 first tokens which manage to represent the same information while requiring less computational resources. The findings could help developers to efficiently pursue optimal performance of the models using limited resources.","sentences":["Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource.","In Indonesian, only a few works were found on long-text classification using Transformers.","Most only use a small amount of data and do not report any HPO.","In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer.","We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words.","To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library.","Using the best hack found, we then compare 512, 256, and 128 tokens length.","We find that removing stopwords while keeping punctuation and low-frequency words is the best hack.","Some of our setups manage to outperform taking 512 first tokens using a smaller 128 or 256 first tokens which manage to represent the same information while requiring less computational resources.","The findings could help developers to efficiently pursue optimal performance of the models using limited resources."],"url":"http://arxiv.org/abs/2403.12563v1","category":"cs.CL"}
{"created":"2024-03-19 09:17:18","title":"Equity through Access: A Case for Small-scale Deep Learning","abstract":"The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using pretrained models can significantly reduce the computational resources and data required. We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.","sentences":["The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute.","These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions.","These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South.","In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited.","To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score.","Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs.","In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models.","Furthermore, we show that using pretrained models can significantly reduce the computational resources and data required.","We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints."],"url":"http://arxiv.org/abs/2403.12562v1","category":"cs.LG"}
{"created":"2024-03-19 08:54:52","title":"M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving","abstract":"End-to-end autonomous driving has witnessed remarkable progress. However, the extensive deployment of autonomous vehicles has yet to be realized, primarily due to 1) inefficient multi-modal environment perception: how to integrate data from multi-modal sensors more efficiently; 2) non-human-like scene understanding: how to effectively locate and predict critical risky agents in traffic scenarios like an experienced driver. To overcome these challenges, in this paper, we propose a Multi-Modal fusion transformer incorporating Driver Attention (M2DA) for autonomous driving. To better fuse multi-modal data and achieve higher alignment between different modalities, a novel Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By incorporating driver attention, we empower the human-like scene understanding ability to autonomous vehicles to identify crucial areas within complex scenarios precisely and ensure safety. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance with less data in closed-loop benchmarks. Source codes are available at https://anonymous.4open.science/r/M2DA-4772.","sentences":["End-to-end autonomous driving has witnessed remarkable progress.","However, the extensive deployment of autonomous vehicles has yet to be realized, primarily due to 1) inefficient multi-modal environment perception: how to integrate data from multi-modal sensors more efficiently; 2) non-human-like scene understanding: how to effectively locate and predict critical risky agents in traffic scenarios like an experienced driver.","To overcome these challenges, in this paper, we propose a Multi-Modal fusion transformer incorporating Driver Attention (M2DA) for autonomous driving.","To better fuse multi-modal data and achieve higher alignment between different modalities, a novel Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed.","By incorporating driver attention, we empower the human-like scene understanding ability to autonomous vehicles to identify crucial areas within complex scenarios precisely and ensure safety.","We conduct experiments on the CARLA simulator and achieve state-of-the-art performance with less data in closed-loop benchmarks.","Source codes are available at https://anonymous.4open.science/r/M2DA-4772."],"url":"http://arxiv.org/abs/2403.12552v1","category":"cs.CV"}
{"created":"2024-03-19 08:09:44","title":"To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions","abstract":"How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.","sentences":["How can a robot provide unobtrusive physical support within a group of humans?","We present Attentive Support, a novel interaction concept for robots to support a group of humans.","It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs).","In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group.","With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed."],"url":"http://arxiv.org/abs/2403.12533v1","category":"cs.RO"}
{"created":"2024-03-19 07:50:32","title":"GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings","abstract":"Events describe the state changes of entities. In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent). Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language. There are two main problems in the current ERE works: a. Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event. b. The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored. To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly extract multiple event relations, we use Node Transformer and construct Task-specific Dynamic Event Graphs for each type of relation. Finally, we used a multi-task learning strategy to train the whole framework. Experimental results on the latest MAVEN-ERE dataset validate that GraphERE significantly outperforms existing methods. Further analyses indicate the effectiveness of the graph-enhanced event embeddings and the joint extraction strategy.","sentences":["Events describe the state changes of entities.","In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent).","Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language.","There are two main problems in the current ERE works: a.","Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event.","b.","The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored.","To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings.","First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly extract multiple event relations, we use Node Transformer and construct Task-specific Dynamic Event Graphs for each type of relation.","Finally, we used a multi-task learning strategy to train the whole framework.","Experimental results on the latest MAVEN-ERE dataset validate that GraphERE significantly outperforms existing methods.","Further analyses indicate the effectiveness of the graph-enhanced event embeddings and the joint extraction strategy."],"url":"http://arxiv.org/abs/2403.12523v1","category":"cs.CL"}
{"created":"2024-03-19 07:44:18","title":"Multi-mode fault diagnosis datasets of gearbox under variable working conditions","abstract":"The gearbox is a critical component of electromechanical systems. The occurrence of multiple faults can significantly impact system accuracy and service life. The vibration signal of the gearbox is an effective indicator of its operational status and fault information. However, gearboxes in real industrial settings often operate under variable working conditions, such as varying speeds and loads. It is a significant and challenging research area to complete the gearbox fault diagnosis procedure under varying operating conditions using vibration signals. This data article presents vibration datasets collected from a gearbox exhibiting various fault degrees of severity and fault types, operating under diverse speed and load conditions. These faults are manually implanted into the gears or bearings through precise machining processes, which include health, missing teeth, wear, pitting, root cracks, and broken teeth. Several kinds of actual compound faults are also encompassed. The development of these datasets facilitates testing the effectiveness and reliability of newly developed fault diagnosis methods.","sentences":["The gearbox is a critical component of electromechanical systems.","The occurrence of multiple faults can significantly impact system accuracy and service life.","The vibration signal of the gearbox is an effective indicator of its operational status and fault information.","However, gearboxes in real industrial settings often operate under variable working conditions, such as varying speeds and loads.","It is a significant and challenging research area to complete the gearbox fault diagnosis procedure under varying operating conditions using vibration signals.","This data article presents vibration datasets collected from a gearbox exhibiting various fault degrees of severity and fault types, operating under diverse speed and load conditions.","These faults are manually implanted into the gears or bearings through precise machining processes, which include health, missing teeth, wear, pitting, root cracks, and broken teeth.","Several kinds of actual compound faults are also encompassed.","The development of these datasets facilitates testing the effectiveness and reliability of newly developed fault diagnosis methods."],"url":"http://arxiv.org/abs/2403.12521v1","category":"eess.SY"}
{"created":"2024-03-19 07:24:54","title":"Generalized Consistency Trajectory Models for Image Manipulation","abstract":"Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs. We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing. Code: \\url{https://github.com/1202kbs/GCTM}","sentences":["Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration.","The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks.","Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step.","However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations.","Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data.","Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs.","We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing.","Code: \\url{https://github.com/1202kbs/GCTM}"],"url":"http://arxiv.org/abs/2403.12510v1","category":"cs.CV"}
{"created":"2024-03-19 07:10:58","title":"Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices","abstract":"Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.","sentences":["Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP).","Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations.","Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations.","These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities.","This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies.","Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs."],"url":"http://arxiv.org/abs/2403.12503v1","category":"cs.CR"}
{"created":"2024-03-19 07:07:13","title":"A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems","abstract":"Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation. The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits. First, having access to large contradiction data enables a comprehensive examination of their characteristics. Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training. Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses. In this paper, we build a large dataset of response generation models' contradictions for the first time. Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods.","sentences":["Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation.","The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits.","First, having access to large contradiction data enables a comprehensive examination of their characteristics.","Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training.","Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses.","In this paper, we build a large dataset of response generation models' contradictions for the first time.","Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses.","Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods."],"url":"http://arxiv.org/abs/2403.12500v1","category":"cs.CL"}
{"created":"2024-03-19 07:04:51","title":"WMMSE-Based Rate Maximization for RIS-Assisted MU-MIMO Systems","abstract":"Reconfigurable intelligent surface (RIS) technology, given its ability to favorably modify wireless communication environments, will play a pivotal role in the evolution of future communication systems. This paper proposes rate maximization techniques for both single-user and multiuser MIMO systems, based on the well-known weighted minimum mean square error (WMMSE) criterion. Using a suitable weight matrix, the WMMSE algorithm tackles an equivalent weighted mean square error (WMSE) minimization problem to achieve the sum-rate maximization. By considering a more practical RIS system model that employs a tensor-based representation enforced by the electromagnetic behavior exhibited by the RIS panel, we detail both the sum-rate maximizing and WMSE minimizing strategies for RIS phase shift optimization by deriving the closed-form gradient of the WMSE and the sum-rate with respect to the RIS phase shift vector. Our simulations reveal that the proposed rate maximization technique, rooted in the WMMSE algorithm, exhibits superior performance when compared to other benchmarks.","sentences":["Reconfigurable intelligent surface (RIS) technology, given its ability to favorably modify wireless communication environments, will play a pivotal role in the evolution of future communication systems.","This paper proposes rate maximization techniques for both single-user and multiuser MIMO systems, based on the well-known weighted minimum mean square error (WMMSE) criterion.","Using a suitable weight matrix, the WMMSE algorithm tackles an equivalent weighted mean square error (WMSE) minimization problem to achieve the sum-rate maximization.","By considering a more practical RIS system model that employs a tensor-based representation enforced by the electromagnetic behavior exhibited by the RIS panel, we detail both the sum-rate maximizing and WMSE minimizing strategies for RIS phase shift optimization by deriving the closed-form gradient of the WMSE and the sum-rate with respect to the RIS phase shift vector.","Our simulations reveal that the proposed rate maximization technique, rooted in the WMMSE algorithm, exhibits superior performance when compared to other benchmarks."],"url":"http://arxiv.org/abs/2403.12498v1","category":"cs.IT"}
{"created":"2024-03-19 06:54:33","title":"DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM","abstract":"We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-the-art methods, GPT-4V with our DetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS COCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val set for zero-shot referring expression comprehension, +14.5% AP on D-cube describe object detection FULL setting.","sentences":["We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini.","Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts.","Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs).","Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements.","The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases.","Compared to existing state-of-the-art methods, GPT-4V with our DetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS COCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val set for zero-shot referring expression comprehension, +14.5% AP on D-cube describe object detection FULL setting."],"url":"http://arxiv.org/abs/2403.12488v1","category":"cs.CV"}
{"created":"2024-03-19 06:43:46","title":"NTK-Guided Few-Shot Class Incremental Learning","abstract":"While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial network weights. Then they are carefully refined through curricular alignment, followed by the application of dual NTK regularization tailored specifically for both convolutional and linear layers. Through the combined effects of these measures, our network acquires robust NTK properties, significantly enhancing its foundational generalization. On popular FSCIL benchmark datasets, our NTK-FSCIL surpasses contemporary state-of-the-art approaches, elevating end-session accuracy by 2.9% to 8.7%.","sentences":["While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition.","In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK).","Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization.","To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network.","Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss.","Specifically, we initiate self-supervised pre-training on the base session to shape the initial network weights.","Then they are carefully refined through curricular alignment, followed by the application of dual NTK regularization tailored specifically for both convolutional and linear layers.","Through the combined effects of these measures, our network acquires robust NTK properties, significantly enhancing its foundational generalization.","On popular FSCIL benchmark datasets, our NTK-FSCIL surpasses contemporary state-of-the-art approaches, elevating end-session accuracy by 2.9% to 8.7%."],"url":"http://arxiv.org/abs/2403.12486v1","category":"cs.LG"}
{"created":"2024-03-19 17:59:52","title":"Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment","abstract":"This paper introduces a novel framework for virtual try-on, termed Wear-Any-Way. Different from previous methods, Wear-Any-Way is a customizable solution. Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style. To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios. To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations. With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style. For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc. Wear-Any-Way enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry.","sentences":["This paper introduces a novel framework for virtual try-on, termed Wear-Any-Way.","Different from previous methods, Wear-Any-Way is a customizable solution.","Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style.","To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios.","To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations.","With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style.","For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc.","Wear-Any-Way enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry."],"url":"http://arxiv.org/abs/2403.12965v1","category":"cs.CV"}
{"created":"2024-03-19 17:59:39","title":"Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models","abstract":"Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/DualAdapter.","sentences":["Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning.","In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't.","Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples.","In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks.","Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency.","Code is available at https://github.com/zhangce01/DualAdapter."],"url":"http://arxiv.org/abs/2403.12964v1","category":"cs.CV"}
{"created":"2024-03-19 17:55:22","title":"FutureDepth: Learning to Predict the Future Improves Video Depth Estimation","abstract":"In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models","sentences":["In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training.","More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively.","In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process.","Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes.","At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network.","Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy.","Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models"],"url":"http://arxiv.org/abs/2403.12953v1","category":"cs.CV"}
{"created":"2024-03-19 17:50:55","title":"Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion","abstract":"In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable. This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task. Such a generalization was not previously known and is likely to be of independent interest.","sentences":["In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm.","The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023).","The goal is to design algorithms without foreknowledge of the amount of change.   ","The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times.","Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention.","In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   ","Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable.","This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task.","Such a generalization was not previously known and is likely to be of independent interest."],"url":"http://arxiv.org/abs/2403.12950v1","category":"cs.LG"}
{"created":"2024-03-19 17:50:40","title":"A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks","abstract":"The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT). However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG. This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase. Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse. Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation. Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage. To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching. Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark.","sentences":["The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT).","However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG.","This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase.","Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse.","Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation.","Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage.","To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching.","Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark."],"url":"http://arxiv.org/abs/2403.12949v1","category":"cs.NI"}
{"created":"2024-03-19 17:34:27","title":"You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs","abstract":"We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis. This is achieved by integrating the diffusion process with GANs. Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning. In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.","sentences":["We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis.","This is achieved by integrating the diffusion process with GANs.","Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning.","We show that our method can serve as a one-step generation model training from scratch with competitive performance.","Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning.","In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training.","Our code is provided at https://github.com/Luo-Yihong/YOSO."],"url":"http://arxiv.org/abs/2403.12931v1","category":"cs.CV"}
{"created":"2024-03-19 17:10:52","title":"Strangers in a foreign land: 'Yeastizing' plant enzymes","abstract":"Expressing plant metabolic pathways in microbial platforms is an efficient, cost-effective solution for producing many desired plant compounds. As eukaryotic organisms, yeasts are often the preferred platform. However, expression of plant enzymes in a yeast frequently leads to failure because the enzymes are poorly adapted to the foreign yeast cellular environment. Here we first summarize current engineering approaches for optimizing performance of plant enzymes in yeast. A critical limitation of these approaches is that they are labor-intensive and must be customized for each individual enzyme, which significantly hinders the establishment of plant pathways in cellular factories. In response to this challenge, we propose the development of a cost-effective computational pipeline to redesign plant enzymes for better adaptation to the yeast cellular milieu. This proposition is underpinned by compelling evidence that plant and yeast enzymes exhibit distinct sequence features that are generalizable across enzyme families. Consequently, we introduce a data-driven machine learning framework designed to extract 'yeastizing' rules from natural protein sequence variations, which can be broadly applied to all enzymes. Additionally, we discuss the potential to integrate the machine learning model into a full design-build-test-cycle.","sentences":["Expressing plant metabolic pathways in microbial platforms is an efficient, cost-effective solution for producing many desired plant compounds.","As eukaryotic organisms, yeasts are often the preferred platform.","However, expression of plant enzymes in a yeast frequently leads to failure because the enzymes are poorly adapted to the foreign yeast cellular environment.","Here we first summarize current engineering approaches for optimizing performance of plant enzymes in yeast.","A critical limitation of these approaches is that they are labor-intensive and must be customized for each individual enzyme, which significantly hinders the establishment of plant pathways in cellular factories.","In response to this challenge, we propose the development of a cost-effective computational pipeline to redesign plant enzymes for better adaptation to the yeast cellular milieu.","This proposition is underpinned by compelling evidence that plant and yeast enzymes exhibit distinct sequence features that are generalizable across enzyme families.","Consequently, we introduce a data-driven machine learning framework designed to extract 'yeastizing' rules from natural protein sequence variations, which can be broadly applied to all enzymes.","Additionally, we discuss the potential to integrate the machine learning model into a full design-build-test-cycle."],"url":"http://arxiv.org/abs/2403.12912v2","category":"q-bio.BM"}
{"created":"2024-03-19 17:02:07","title":"TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation","abstract":"Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets. We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model. Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability. Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds. Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions.","sentences":["Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV.","Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets.","We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model.","Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability.","Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds.","Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions."],"url":"http://arxiv.org/abs/2403.12906v1","category":"cs.CV"}
{"created":"2024-03-19 16:31:30","title":"HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning","abstract":"Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities. Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning. HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner. The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop. This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness. Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.","sentences":["Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities.","Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures.","To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning.","HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner.","The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop.","This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness.","Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets."],"url":"http://arxiv.org/abs/2403.12884v1","category":"cs.CV"}
{"created":"2024-03-19 16:29:59","title":"Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments","abstract":"In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation. In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance. Previous methods employed prototype methods for domain adaptation on robust feature spaces. However, these approaches struggle to effectively classify classes with similar features under noisy environments. To address this issue, we propose a new method to detect and correct confusing class pair. We first divide classes into easy and hard classes based on the small loss criterion. We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes. We apply label correction to the noisy samples within the confusing pair. With the proposed label correction method, we can train our model with more accurate labels. Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods. Our codes are publicly available at https://github.com/Hehxcf/CPC/.","sentences":["In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation.","In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance.","Previous methods employed prototype methods for domain adaptation on robust feature spaces.","However, these approaches struggle to effectively classify classes with similar features under noisy environments.","To address this issue, we propose a new method to detect and correct confusing class pair.","We first divide classes into easy and hard classes based on the small loss criterion.","We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes.","We apply label correction to the noisy samples within the confusing pair.","With the proposed label correction method, we can train our model with more accurate labels.","Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods.","Our codes are publicly available at https://github.com/Hehxcf/CPC/."],"url":"http://arxiv.org/abs/2403.12883v1","category":"cs.CV"}
{"created":"2024-03-19 16:09:30","title":"PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments","abstract":"The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight. In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances. The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization. Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control. Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner. Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario.","sentences":["The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight.","In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances.","The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization.","Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control.","Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner.","Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario."],"url":"http://arxiv.org/abs/2403.12865v1","category":"cs.RO"}
{"created":"2024-03-19 15:45:54","title":"Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering","abstract":"Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: https://shaomq2187.github.io/GF-NeRF/","sentences":["Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes.","However, their limited model capacity typically results in blurred rendering results.","Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs.","These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene.","Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity.","In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes.","Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy.","The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders.","Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency.","Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes.","Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes.","We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets.","Our project page: https://shaomq2187.github.io/GF-NeRF/"],"url":"http://arxiv.org/abs/2403.12839v1","category":"cs.CV"}
{"created":"2024-03-19 15:41:39","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","abstract":"Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions. Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning. Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text. An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering. We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents.","sentences":["Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios.","To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions.","Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning.","Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text.","An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering.","We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents."],"url":"http://arxiv.org/abs/2403.12835v1","category":"cs.CV"}
{"created":"2024-03-19 15:17:23","title":"Dynamic Survival Analysis for Early Event Prediction","abstract":"This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.","sentences":["This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics.","By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference).","This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management."],"url":"http://arxiv.org/abs/2403.12818v1","category":"cs.LG"}
{"created":"2024-03-19 15:07:08","title":"VisualCritic: Making LMMs Perceive Visual Quality Like Humans","abstract":"At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer \"Yes!\". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.","sentences":["At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals.","However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception.","Can LMMs achieve this and show the same degree of generalization in this regard?","If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed.","In this paper, we explore this question and provide the answer \"Yes!\".","As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment.","VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models.","As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic.","Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images."],"url":"http://arxiv.org/abs/2403.12806v1","category":"cs.CV"}
{"created":"2024-03-19 14:52:51","title":"Bivariate temporal dependence via mixtures of rotated copulas","abstract":"Parametric bivariate copula families have been known to flexibly capture enough various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions. However, to the best of our knowledge, there is not a single parametric model adaptable enough to capture several of these features simultaneously. To address this, we propose a mixture of 4-way rotations of a parametric copula that is able to capture all these features. We illustrate the construction using the Clayton family but the concept is general and can be applied to other families. In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average type of relationship. The properties of the proposed model and its performance are examined using simulated and real data sets.","sentences":["Parametric bivariate copula families have been known to flexibly capture enough various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions.","However, to the best of our knowledge, there is not a single parametric model adaptable enough to capture several of these features simultaneously.","To address this, we propose a mixture of 4-way rotations of a parametric copula that is able to capture all these features.","We illustrate the construction using the Clayton family but the concept is general and can be applied to other families.","In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average type of relationship.","The properties of the proposed model and its performance are examined using simulated and real data sets."],"url":"http://arxiv.org/abs/2403.12789v1","category":"stat.ME"}
{"created":"2024-03-19 14:34:44","title":"Multispectral Image Restoration by Generalized Opponent Transformation Total Variation","abstract":"Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks. Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization. The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain. Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images. We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration. To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM.","sentences":["Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks.","Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization.","The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain.","Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images.","We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration.","To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM."],"url":"http://arxiv.org/abs/2403.12770v1","category":"cs.CV"}
{"created":"2024-03-19 14:30:56","title":"Neural Parameter Regression for Explicit Representations of PDE Solution Operators","abstract":"We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.","sentences":["We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs).","Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters.","By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces.","Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability.","The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples."],"url":"http://arxiv.org/abs/2403.12764v1","category":"cs.LG"}
{"created":"2024-03-19 14:12:54","title":"Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data","abstract":"Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki. We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.","sentences":["Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects.","This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval.","The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information.","We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian.","Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet.","Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus.","Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki.","We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance."],"url":"http://arxiv.org/abs/2403.12749v1","category":"cs.CL"}
{"created":"2024-03-19 13:52:33","title":"To blow-up or not to blow-up for a granular kinetic equation","abstract":"A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a convolution integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics. While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial. The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport. We present a preliminary study through a meticulous numerical investigation and heuristic arguments. We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations. We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario.","sentences":["A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a convolution integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics.","While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial.","The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport.","We present a preliminary study through a meticulous numerical investigation and heuristic arguments.","We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations.","We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario."],"url":"http://arxiv.org/abs/2403.12735v1","category":"math.NA"}
{"created":"2024-03-19 13:29:44","title":"Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration","abstract":"This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot.","sentences":["This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture.","For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner.","Therefore, two key components are addressed in this work: motion generation and shared autonomy.","We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance.","Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority.","Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces.","System passivity is ensured by an energy-tank based task passivation strategy.","The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot."],"url":"http://arxiv.org/abs/2403.12720v1","category":"cs.RO"}
{"created":"2024-03-19 13:27:38","title":"Emergence of dynamical networks in termites","abstract":"Termites form complex dynamical trail networks from simple individual rules when exploring their environment. To help identify those simple rules, we reconstructed trail networks from time-lapse images of roaming termites. We quantified the trails' frequentations over time and compared them to the ones obtained by a null model. Arena borders were preferred in both simulated and observed data. Yet, the amplification phenomenon was higher with real termites, underlining the role of pheromones.","sentences":["Termites form complex dynamical trail networks from simple individual rules when exploring their environment.","To help identify those simple rules, we reconstructed trail networks from time-lapse images of roaming termites.","We quantified the trails' frequentations over time and compared them to the ones obtained by a null model.","Arena borders were preferred in both simulated and observed data.","Yet, the amplification phenomenon was higher with real termites, underlining the role of pheromones."],"url":"http://arxiv.org/abs/2403.12718v1","category":"nlin.AO"}
{"created":"2024-03-19 13:19:41","title":"Addressing Source Scale Bias via Image Warping for Domain Adaptation","abstract":"In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture. Highlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC. Our approach adds minimal memory during training and has no additional latency at inference time. Please see Appendix for more results and analysis.","sentences":["In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets.","Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference.","While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited.","Besides, they increase computational load during training and latency during inference.","In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training.","Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation.","Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture.","Highlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC.","Our approach adds minimal memory during training and has no additional latency at inference time.","Please see Appendix for more results and analysis."],"url":"http://arxiv.org/abs/2403.12712v1","category":"cs.CV"}
{"created":"2024-03-19 13:01:57","title":"Learning Cross-view Visual Geo-localization without Ground Truth","abstract":"Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image. Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens. In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels. We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references. To address these challenges, we propose a self-supervised learning framework to train a learnable adapter for a frozen Foundation Model (FM). This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively. To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter. To maintain the robustness of the FM's representation, we incorporate an information consistency module with a reconstruction loss, ensuring that adapted features retain strong discriminative ability across views. Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to supervised methods, while necessitating fewer training parameters and relying solely on unlabeled data. Evaluation of our adaptation for task-specific models further highlights its broad applicability.","sentences":["Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image.","Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens.","In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels.","We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references.","To address these challenges, we propose a self-supervised learning framework to train a learnable adapter for a frozen Foundation Model (FM).","This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively.","To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter.","To maintain the robustness of the FM's representation, we incorporate an information consistency module with a reconstruction loss, ensuring that adapted features retain strong discriminative ability across views.","Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to supervised methods, while necessitating fewer training parameters and relying solely on unlabeled data.","Evaluation of our adaptation for task-specific models further highlights its broad applicability."],"url":"http://arxiv.org/abs/2403.12702v1","category":"cs.CV"}
{"created":"2024-03-19 12:56:02","title":"System Support for Environmentally Sustainable Computing in Data Centers","abstract":"Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability. While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact. This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation. We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices. We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain.","sentences":["Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability.","While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact.","This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation.","We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices.","We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain."],"url":"http://arxiv.org/abs/2403.12698v1","category":"cs.AR"}
{"created":"2024-03-19 12:49:25","title":"Efficient thermalization and universal quantum computing with quantum Gibbs samplers","abstract":"The preparation of thermal states of matter is a crucial task in quantum simulation. In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures, and for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice. Furthermore, we show the efficient adiabatic preparation of the associated purifications or ``thermofield double'' states. To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications. In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures logarithmic in the system's size is polynomially equivalent to standard quantum computation. On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then analysing it as perturbation of the Hamiltonian corresponding to infinite temperature. For low temperature, we instead perform a perturbation at zero temperature of the Laplace transform of the energy observable at fixed runtime, and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing. Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems.","sentences":["The preparation of thermal states of matter is a crucial task in quantum simulation.","In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures, and for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice.","Furthermore, we show the efficient adiabatic preparation of the associated purifications or ``thermofield double'' states.","To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications.","In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures logarithmic in the system's size is polynomially equivalent to standard quantum computation.","On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then analysing it as perturbation of the Hamiltonian corresponding to infinite temperature.","For low temperature, we instead perform a perturbation at zero temperature of the Laplace transform of the energy observable at fixed runtime, and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing.","Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems."],"url":"http://arxiv.org/abs/2403.12691v1","category":"quant-ph"}
{"created":"2024-03-19 12:47:55","title":"Stabilizing DG Methods Using Dafermos' Entropy Rate Criterion: III -- Unstructured Grids","abstract":"The approach presented in the second installment of this series is extended to multidimensional systems of conservation laws that are approximated via a Discontinuous Galerkin method on unstructured (triangular) grids. Special attention is paid to predicting the entropy dissipation from boundaries. The resulting schemes are free of tunable viscosity parameters and tested on the Euler equations. The trinity of testcases is the spreading of thermal energy from a point source, transsonic and supersonic flows around airfoils, and supersonic air inlets.","sentences":["The approach presented in the second installment of this series is extended to multidimensional systems of conservation laws that are approximated via a Discontinuous Galerkin method on unstructured (triangular) grids.","Special attention is paid to predicting the entropy dissipation from boundaries.","The resulting schemes are free of tunable viscosity parameters and tested on the Euler equations.","The trinity of testcases is the spreading of thermal energy from a point source, transsonic and supersonic flows around airfoils, and supersonic air inlets."],"url":"http://arxiv.org/abs/2403.12689v1","category":"math.NA"}
{"created":"2024-03-19 12:45:18","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar","abstract":"The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments. Inspired by visual grounding, in this paper, we introduce WaterVG, the first visual grounding dataset designed for USV-based waterway perception based on human intention prompts. WaterVG encompasses prompts describing multiple targets, with annotations at the instance level including bounding boxes and masks. Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text prompts. Furthermore, we propose a novel multi-modal visual grounding model, Potamoi, which is a multi-modal and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained prompts. Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts.","sentences":["The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments.","Inspired by visual grounding, in this paper, we introduce WaterVG, the first visual grounding dataset designed for USV-based waterway perception based on human intention prompts.","WaterVG encompasses prompts describing multiple targets, with annotations at the instance level including bounding boxes and masks.","Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text prompts.","Furthermore, we propose a novel multi-modal visual grounding model, Potamoi, which is a multi-modal and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA).","In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained prompts.","Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts."],"url":"http://arxiv.org/abs/2403.12686v1","category":"cs.CV"}
{"created":"2024-03-19 12:21:20","title":"Pragmatic Competence Evaluation of Large Language Models for Korean","abstract":"The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4. Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference. Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.","sentences":["The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.","Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean.","We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options.","Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4.","Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.","Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations."],"url":"http://arxiv.org/abs/2403.12675v1","category":"cs.CL"}
{"created":"2024-03-19 11:34:40","title":"Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation","abstract":"To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented. It is constructed to map parameters of the model data to corresponding finite element solutions. To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM). It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network. The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network. This leads to a problem adapted representation of the solution on locally refined grids. Furthermore, each solution of the AFEM is discretized in a hierarchical basis. For the architecture, convolutional neural networks (CNNs) are chosen. The hierarchical basis then allows to handle sparse images for finely discretized meshes. Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively. This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs. The architecture is described and preliminary numerical examples are presented.","sentences":["To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented.","It is constructed to map parameters of the model data to corresponding finite element solutions.","To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM).","It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network.","The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network.","This leads to a problem adapted representation of the solution on locally refined grids.","Furthermore, each solution of the AFEM is discretized in a hierarchical basis.","For the architecture, convolutional neural networks (CNNs) are chosen.","The hierarchical basis then allows to handle sparse images for finely discretized meshes.","Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively.","This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs.","The architecture is described and preliminary numerical examples are presented."],"url":"http://arxiv.org/abs/2403.12650v1","category":"math.NA"}
{"created":"2024-03-19 10:50:34","title":"Large-scale metric objects filtering for binary classification with application to abnormal brain connectivity detection","abstract":"The classification of random objects within metric spaces without a vector structure has attracted increasing attention. However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications. To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space. Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility. It enjoys a model-free property, as its implementation does not rely on any specified classifier. Theoretically, it controls the false discovery rate while guaranteeing the sure screening property. Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors. When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition. Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities.","sentences":["The classification of random objects within metric spaces without a vector structure has attracted increasing attention.","However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications.","To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space.","Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility.","It enjoys a model-free property, as its implementation does not rely on any specified classifier.","Theoretically, it controls the false discovery rate while guaranteeing the sure screening property.","Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors.","When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition.","Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities."],"url":"http://arxiv.org/abs/2403.12624v1","category":"stat.ME"}
{"created":"2024-03-19 10:31:12","title":"Surfactant-laden liquid thread breakup driven by thermal fluctuations","abstract":"The breakup of liquid threads into droplets is crucial in various applications, such as nanoprinting, nanomanufacturing, and inkjet printing, where a detailed understanding of the thinning neck dynamics allows for a precise droplet control. Here, the role of surfactant in the breakup process is studied by many-body dissipative particle dynamics, in particular, the various regime transitions and thread profiles, shedding light on molecular-level intricacies of this process hitherto inaccessible to continuum theory and experiments. Moreover, the role of surfactant in the most unstable perturbation, the formed droplet size, and surfactant distributions have been unraveled. As surfactant concentration rises, both the wavelength and time to breakup steadily increase due to the lowering of surface tension below the critical micelle concentration (CMC) and viscous effects introduced by micelles above the CMC. These changes prior to the breakup lead to larger droplets being formed in cases with higher surfactant concentration. We also compared the thinning dynamics to existing theoretical predictions, revealing that the surfactant-laden breakup starts at the inertial regime and transitions into the thermal fluctuation regime when the concentration is increased. Thus, we illuminate the hitherto poorly investigated and intricate breakup process of surfactant-laden liquid threads driven by thermal fluctuations, contributing to a deeper understanding of this process at molecular scales.","sentences":["The breakup of liquid threads into droplets is crucial in various applications, such as nanoprinting, nanomanufacturing, and inkjet printing, where a detailed understanding of the thinning neck dynamics allows for a precise droplet control.","Here, the role of surfactant in the breakup process is studied by many-body dissipative particle dynamics, in particular, the various regime transitions and thread profiles, shedding light on molecular-level intricacies of this process hitherto inaccessible to continuum theory and experiments.","Moreover, the role of surfactant in the most unstable perturbation, the formed droplet size, and surfactant distributions have been unraveled.","As surfactant concentration rises, both the wavelength and time to breakup steadily increase due to the lowering of surface tension below the critical micelle concentration (CMC) and viscous effects introduced by micelles above the CMC.","These changes prior to the breakup lead to larger droplets being formed in cases with higher surfactant concentration.","We also compared the thinning dynamics to existing theoretical predictions, revealing that the surfactant-laden breakup starts at the inertial regime and transitions into the thermal fluctuation regime when the concentration is increased.","Thus, we illuminate the hitherto poorly investigated and intricate breakup process of surfactant-laden liquid threads driven by thermal fluctuations, contributing to a deeper understanding of this process at molecular scales."],"url":"http://arxiv.org/abs/2403.12614v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 09:51:20","title":"Simulation of the Wave Turbulence of a Liquid Surface Using the Dynamic Conformal Transformation Method","abstract":"The dynamic conformal transformation method has been generalized for the first time to numerically simulate the capillary wave turbulence of a liquid surface in the plane symmetric anisotropic geometry. The model is strongly nonlinear and involves effects of surface tension, as well as energy dissipation and pumping. Simulation results have shown that the system of nonlinear capillary waves can pass to the quasistationary chaotic motion regime (wave turbulence). The calculated exponents of spectra do not coincide with those for the classical Zakharov-Filonenko spectrum for isotropic capillary turbulence but are in good agreement with the estimate obtained under the assumption of the dominant effect of five-wave resonant interactions.","sentences":["The dynamic conformal transformation method has been generalized for the first time to numerically simulate the capillary wave turbulence of a liquid surface in the plane symmetric anisotropic geometry.","The model is strongly nonlinear and involves effects of surface tension, as well as energy dissipation and pumping.","Simulation results have shown that the system of nonlinear capillary waves can pass to the quasistationary chaotic motion regime (wave turbulence).","The calculated exponents of spectra do not coincide with those for the classical Zakharov-Filonenko spectrum for isotropic capillary turbulence but are in good agreement with the estimate obtained under the assumption of the dominant effect of five-wave resonant interactions."],"url":"http://arxiv.org/abs/2403.12592v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 09:47:16","title":"An Adaptive feature mode decomposition based on a novel health indicator for bearing fault diagnosis","abstract":"The vibration analysis of the bearing is very crucial because of its non-stationary nature and low signal-to-noise ratio. Therefore, a novel scheme for detecting bearing defects is put forward based on the extraction of single-valued neutrosophic cross-entropy (SVNCE) to address this issue. Initially, the artificial hummingbird algorithm (AHA) is used to make the feature mode decomposition (FMD) adaptive by optimizing its parameter based on a newly developed health indicator (HI) i.e. sparsity impact measure index (SIMI). This HI ensures full sparsity and impact properties simultaneously. The raw signals are disintegrated into different modes by adaptive FMD at optimal values of its parameters. The energy of these modes is calculated for different health conditions. The energy interval range has been decided based on energy eigen which are then transformed into single-valued neutrosophic sets (SVNSs) for unknown defect conditions. The minimum argument principle employs the least SVNCE values between SVNSs of testing samples (obtained from unknown bearing conditions) and SVNSs of training samples (obtained from known bearing conditions) to recognize the different defects in the bearing. It has been discovered that the suggested methodology is more adept at identifying the various bearing defects.","sentences":["The vibration analysis of the bearing is very crucial because of its non-stationary nature and low signal-to-noise ratio.","Therefore, a novel scheme for detecting bearing defects is put forward based on the extraction of single-valued neutrosophic cross-entropy (SVNCE) to address this issue.","Initially, the artificial hummingbird algorithm (AHA) is used to make the feature mode decomposition (FMD) adaptive by optimizing its parameter based on a newly developed health indicator (HI) i.e. sparsity impact measure index (SIMI).","This HI ensures full sparsity and impact properties simultaneously.","The raw signals are disintegrated into different modes by adaptive FMD at optimal values of its parameters.","The energy of these modes is calculated for different health conditions.","The energy interval range has been decided based on energy eigen which are then transformed into single-valued neutrosophic sets (SVNSs) for unknown defect conditions.","The minimum argument principle employs the least SVNCE values between SVNSs of testing samples (obtained from unknown bearing conditions) and SVNSs of training samples (obtained from known bearing conditions) to recognize the different defects in the bearing.","It has been discovered that the suggested methodology is more adept at identifying the various bearing defects."],"url":"http://arxiv.org/abs/2403.12586v1","category":"eess.SP"}
{"created":"2024-03-19 09:36:08","title":"Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains","abstract":"The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021. It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical. This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof. This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes. The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory. Five computational benchmarks display accurate reference eigenvalues up to 30 digits.","sentences":["The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021.","It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical.","This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof.","This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes.","The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory.","Five computational benchmarks display accurate reference eigenvalues up to 30 digits."],"url":"http://arxiv.org/abs/2403.12577v1","category":"math.NA"}
{"created":"2024-03-19 09:28:19","title":"Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images","abstract":"Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains. However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection. This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection. Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels. This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images. The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training. Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively. Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD","sentences":["Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains.","However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection.","This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection.","Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels.","This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images.","The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training.","Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively.","Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD"],"url":"http://arxiv.org/abs/2403.12570v1","category":"cs.CV"}
{"created":"2024-03-19 08:48:09","title":"Kinetically constrained models constructed from dissipative quantum dynamics","abstract":"We propose a construction of kinetically constrained models using the Markovian quantum dynamics under strong dissipation. Using the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) formalism, we show that strong dissipation leads to the emergent decoherence-free subspaces, within which constrained quantum many-body unitary dynamics can take place. We argue that the unitary dynamics constructed by the GKSL dynamics is more tightly constrained than that constructed by the strongly interacting Hamiltonian, where the interactions have the same form with the GKSL jump operators. As an example, we demonstrate that a one-dimensional spin system with two-site dissipation leads to the kinetically constrained ``PXQ\" model, which exhibits the free domain-wall motion with an additional frozen-block structure. Under the uniform magnetic field, the PXQ model shows the domain-wall localization, similar to the Wannier-Stark localization. We then couple two PXQ chains with the magnetic field and inter-chain interaction. We discover that, while localization of the domain walls persists despite the interactions for typical parameter regimes, a non-trivial partial delocalization appears for a certain parameter line.","sentences":["We propose a construction of kinetically constrained models using the Markovian quantum dynamics under strong dissipation.","Using the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) formalism, we show that strong dissipation leads to the emergent decoherence-free subspaces, within which constrained quantum many-body unitary dynamics can take place.","We argue that the unitary dynamics constructed by the GKSL dynamics is more tightly constrained than that constructed by the strongly interacting Hamiltonian, where the interactions have the same form with the GKSL jump operators.","As an example, we demonstrate that a one-dimensional spin system with two-site dissipation leads to the kinetically constrained ``PXQ\" model, which exhibits the free domain-wall motion with an additional frozen-block structure.","Under the uniform magnetic field, the PXQ model shows the domain-wall localization, similar to the Wannier-Stark localization.","We then couple two PXQ chains with the magnetic field and inter-chain interaction.","We discover that, while localization of the domain walls persists despite the interactions for typical parameter regimes, a non-trivial partial delocalization appears for a certain parameter line."],"url":"http://arxiv.org/abs/2403.12548v1","category":"quant-ph"}
{"created":"2024-03-19 08:37:57","title":"Attitude Tracking of Uncertain Flexible Spacecraft Systems Subject to Unknown External Disturbances","abstract":"In this paper, we investigate the attitude tracking problem of uncertain flexible spacecraft systems subject to external disturbances. In sharp contrast to existing results, the dynamics of flexible spacecraft systems and external disturbances are allowed to be unknown. To deal with the challenges by these unknown factors, we develop a class of nonlinear internal models which converts the attitude tracking problem of uncertain flexible spacecraft systems into a regulation problem of an augmented system. Furthermore, to overcome the difficulties caused by the unmeasurable modal variable, the uncertainty introduced by the internal model, and the cross-coupling of the uncertainties with the system state, we design an auxiliary dynamic system for auxiliary stabilization, a dynamic compensator for dynamic compensation, and a linearly parameterized transformation for adaptive regulation in sequence. By introducing a series of coordinate and input transformations, we propose an adaptive dynamic control law to achieve regulation of the augmented system and thus leading to the solution to the attitude tracking problem. In addition, we analyze the convergence issue of the estimated parameter to its true value by the persistently exciting condition. Finally, the effec tiveness of the developed approach is verified by its application to the attitude manoeuvre of a flexible spacecraft system in the presence of external disturbances.","sentences":["In this paper, we investigate the attitude tracking problem of uncertain flexible spacecraft systems subject to external disturbances.","In sharp contrast to existing results, the dynamics of flexible spacecraft systems and external disturbances are allowed to be unknown.","To deal with the challenges by these unknown factors, we develop a class of nonlinear internal models which converts the attitude tracking problem of uncertain flexible spacecraft systems into a regulation problem of an augmented system.","Furthermore, to overcome the difficulties caused by the unmeasurable modal variable, the uncertainty introduced by the internal model, and the cross-coupling of the uncertainties with the system state, we design an auxiliary dynamic system for auxiliary stabilization, a dynamic compensator for dynamic compensation, and a linearly parameterized transformation for adaptive regulation in sequence.","By introducing a series of coordinate and input transformations, we propose an adaptive dynamic control law to achieve regulation of the augmented system and thus leading to the solution to the attitude tracking problem.","In addition, we analyze the convergence issue of the estimated parameter to its true value by the persistently exciting condition.","Finally, the effec tiveness of the developed approach is verified by its application to the attitude manoeuvre of a flexible spacecraft system in the presence of external disturbances."],"url":"http://arxiv.org/abs/2403.12542v1","category":"math.OC"}
{"created":"2024-03-19 08:23:12","title":"Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification","abstract":"Multiple instance learning (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs). Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images. To address this issue, we propose PAMT, a novel Prompt-guided Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data. To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual Prompt (PVP) to reform the input data, building a compact while informative representation. Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features. We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models. Our findings affirm the potential of PAMT to set a new benchmark in WSI classification, underscoring the value of a targeted reprogramming approach.","sentences":["Multiple instance learning (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs).","Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images.","To address this issue, we propose PAMT, a novel Prompt-guided Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data.","To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual Prompt (PVP) to reform the input data, building a compact while informative representation.","Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features.","We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models.","Our findings affirm the potential of PAMT to set a new benchmark in WSI classification, underscoring the value of a targeted reprogramming approach."],"url":"http://arxiv.org/abs/2403.12537v1","category":"cs.CV"}
{"created":"2024-03-19 08:15:53","title":"ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More","abstract":"Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns. However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks. We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty. In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective. Our ExACT brings two technical contributions. Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones. This subtly enhances the performance of ExACT without extra computational cost. Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation. In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation. Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.","sentences":["Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns.","However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks.","We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty.","In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective.","Our ExACT brings two technical contributions.","Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones.","This subtly enhances the performance of ExACT without extra computational cost.","Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation.","In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation.","Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively."],"url":"http://arxiv.org/abs/2403.12534v1","category":"cs.CV"}
{"created":"2024-03-19 08:09:27","title":"UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All","abstract":"We present UniBind, a flexible and efficient approach that learns a unified representation space for seven diverse modalities -- images, text, audio, point cloud, thermal, video, and event data. Existing works, eg., ImageBind, treat the image as the central modality and build an image-centered representation space; however, the space may be sub-optimal as it leads to an unbalanced representation space among all modalities. Moreover, the category names are directly used to extract text embeddings for the downstream tasks, making it hardly possible to represent the semantics of multi-modal data. The 'out-of-the-box' insight of our UniBind is to make the alignment center modality-agnostic and further learn a unified and balanced representation space, empowered by the large language models (LLMs). UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts. To make this possible, we 1) construct a knowledge base of text embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build LLM-augmented class-wise embedding center on top of the knowledge base and encoded visual embeddings; 3) align all the embeddings to the LLM-augmented embedding center via contrastive learning to achieve a unified and balanced representation space. UniBind shows strong zero-shot recognition performance gains over prior arts by an average of 6.36%. Finally, we achieve new state-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal fine-tuning setting while reducing 90% of the learnable parameters.","sentences":["We present UniBind, a flexible and efficient approach that learns a unified representation space for seven diverse modalities -- images, text, audio, point cloud, thermal, video, and event data.","Existing works, eg., ImageBind, treat the image as the central modality and build an image-centered representation space; however, the space may be sub-optimal as it leads to an unbalanced representation space among all modalities.","Moreover, the category names are directly used to extract text embeddings for the downstream tasks, making it hardly possible to represent the semantics of multi-modal data.","The 'out-of-the-box' insight of our UniBind is to make the alignment center modality-agnostic and further learn a unified and balanced representation space, empowered by the large language models (LLMs).","UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts.","To make this possible, we 1) construct a knowledge base of text embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build LLM-augmented class-wise embedding center on top of the knowledge base and encoded visual embeddings; 3) align all the embeddings to the LLM-augmented embedding center via contrastive learning to achieve a unified and balanced representation space.","UniBind shows strong zero-shot recognition performance gains over prior arts by an average of 6.36%.","Finally, we achieve new state-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal fine-tuning setting while reducing 90% of the learnable parameters."],"url":"http://arxiv.org/abs/2403.12532v1","category":"cs.CV"}
{"created":"2024-03-19 08:08:12","title":"PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation","abstract":"Generating annotations for bird's-eye-view (BEV) segmentation presents significant challenges due to the scenes' complexity and the high manual annotation cost. In this work, we address these challenges by leveraging the abundance of unlabeled data available. We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets. PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels. Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures. PCT can be applied to various settings where unlabeled data is available. In this paper, we applied PCT for semi-supervised learning (SSL) and unsupervised domain adaptation (UDA). Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework. Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art.","sentences":["Generating annotations for bird's-eye-view (BEV) segmentation presents significant challenges due to the scenes' complexity and the high manual annotation cost.","In this work, we address these challenges by leveraging the abundance of unlabeled data available.","We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets.","PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels.","Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures.","PCT can be applied to various settings where unlabeled data is available.","In this paper, we applied PCT for semi-supervised learning (SSL) and unsupervised domain adaptation (UDA).","Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework.","Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art."],"url":"http://arxiv.org/abs/2403.12530v1","category":"cs.CV"}
{"created":"2024-03-19 07:11:53","title":"Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation","abstract":"This paper addresses an interesting yet challenging problem -- source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation -- given only a pinhole image-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is nontrivial due to the semantic mismatches, style discrepancies, and inevitable distortion of panoramic images. To this end, we propose a novel method that utilizes Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images. Both projections are shown effective in extracting knowledge from the source model. However, the distinct projection discrepancies between source and target domains impede the direct knowledge transfer; thus, we propose a panoramic prototype adaptation module (PPAM) to integrate panoramic prototypes from the extracted knowledge for adaptation. We then impose the loss constraints on both predictions and prototypes and propose a cross-dual attention module (CDAM) at the feature level to better align the spatial and channel characteristics across the domains and projections. Both knowledge extraction and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our method achieves significantly better performance than prior SFUDA methods for pinhole-to-panoramic adaptation.","sentences":["This paper addresses an interesting yet challenging problem -- source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation -- given only a pinhole image-trained model (i.e., source) and unlabeled panoramic images (i.e., target).","Tackling this problem is nontrivial due to the semantic mismatches, style discrepancies, and inevitable distortion of panoramic images.","To this end, we propose a novel method that utilizes Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.","Both projections are shown effective in extracting knowledge from the source model.","However, the distinct projection discrepancies between source and target domains impede the direct knowledge transfer; thus, we propose a panoramic prototype adaptation module (PPAM) to integrate panoramic prototypes from the extracted knowledge for adaptation.","We then impose the loss constraints on both predictions and prototypes and propose a cross-dual attention module (CDAM) at the feature level to better align the spatial and channel characteristics across the domains and projections.","Both knowledge extraction and transfer processes are synchronously updated to reach the best performance.","Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our method achieves significantly better performance than prior SFUDA methods for pinhole-to-panoramic adaptation."],"url":"http://arxiv.org/abs/2403.12505v1","category":"cs.CV"}
{"created":"2024-03-19 07:10:04","title":"Under-actuated Robotic Gripper with Multiple Grasping Modes Inspired by Human Finger","abstract":"Under-actuated robot grippers as a pervasive tool of robots have become a considerable research focus. Despite their simplicity of mechanical design and control strategy, they suffer from poor versatility and weak adaptability, making widespread applications limited. To better relieve relevant research gaps, we present a novel 3-finger linkage-based gripper that realizes retractable and reconfigurable multi-mode grasps driven by a single motor. Firstly, inspired by the changes that occurred in the contact surface with a human finger moving, we artfully design a slider-slide rail mechanism as the phalanx to achieve retraction of each finger, allowing for better performance in the enveloping grasping mode. Secondly, a reconfigurable structure is constructed to broaden the grasping range of objects' dimensions for the proposed gripper. By adjusting the configuration and gesture of each finger, the gripper can achieve five grasping modes. Thirdly, the proposed gripper is just actuated by a single motor, yet it can be capable of grasping and reconfiguring simultaneously. Finally, various experiments on grasps of slender, thin, and large-volume objects are implemented to evaluate the performance of the proposed gripper in practical scenarios, which demonstrates the excellent grasping capabilities of the gripper.","sentences":["Under-actuated robot grippers as a pervasive tool of robots have become a considerable research focus.","Despite their simplicity of mechanical design and control strategy, they suffer from poor versatility and weak adaptability, making widespread applications limited.","To better relieve relevant research gaps, we present a novel 3-finger linkage-based gripper that realizes retractable and reconfigurable multi-mode grasps driven by a single motor.","Firstly, inspired by the changes that occurred in the contact surface with a human finger moving, we artfully design a slider-slide rail mechanism as the phalanx to achieve retraction of each finger, allowing for better performance in the enveloping grasping mode.","Secondly, a reconfigurable structure is constructed to broaden the grasping range of objects' dimensions for the proposed gripper.","By adjusting the configuration and gesture of each finger, the gripper can achieve five grasping modes.","Thirdly, the proposed gripper is just actuated by a single motor, yet it can be capable of grasping and reconfiguring simultaneously.","Finally, various experiments on grasps of slender, thin, and large-volume objects are implemented to evaluate the performance of the proposed gripper in practical scenarios, which demonstrates the excellent grasping capabilities of the gripper."],"url":"http://arxiv.org/abs/2403.12502v1","category":"cs.RO"}
{"created":"2024-03-19 07:02:08","title":"Task-Customized Mixture of Adapters for General Image Fusion","abstract":"General image fusion aims at integrating important information from multi-source images. However, due to the significant cross-task gap, the respective fusion mechanism varies considerably in practice, resulting in limited performance across subtasks. To handle this problem, we propose a novel task-customized mixture of adapters (TC-MoA) for general image fusion, adaptively prompting various fusion tasks in a unified model. We borrow the insight from the mixture of experts (MoE), taking the experts as efficient tuning adapters to prompt a pre-trained foundation model. These adapters are shared across different tasks and constrained by mutual information regularization, ensuring compatibility with different tasks while complementarity for multi-source images. The task-specific routing networks customize these adapters to extract task-specific information from different sources with dynamic dominant intensity, performing adaptive visual feature prompt fusion. Notably, our TC-MoA controls the dominant intensity bias for different fusion tasks, successfully unifying multiple fusion tasks in a single model. Extensive experiments show that TC-MoA outperforms the competing approaches in learning commonalities while retaining compatibility for general image fusion (multi-modal, multi-exposure, and multi-focus), and also demonstrating striking controllability on more generalization experiments. The code is available at https://github.com/YangSun22/TC-MoA .","sentences":["General image fusion aims at integrating important information from multi-source images.","However, due to the significant cross-task gap, the respective fusion mechanism varies considerably in practice, resulting in limited performance across subtasks.","To handle this problem, we propose a novel task-customized mixture of adapters (TC-MoA) for general image fusion, adaptively prompting various fusion tasks in a unified model.","We borrow the insight from the mixture of experts (MoE), taking the experts as efficient tuning adapters to prompt a pre-trained foundation model.","These adapters are shared across different tasks and constrained by mutual information regularization, ensuring compatibility with different tasks while complementarity for multi-source images.","The task-specific routing networks customize these adapters to extract task-specific information from different sources with dynamic dominant intensity, performing adaptive visual feature prompt fusion.","Notably, our TC-MoA controls the dominant intensity bias for different fusion tasks, successfully unifying multiple fusion tasks in a single model.","Extensive experiments show that TC-MoA outperforms the competing approaches in learning commonalities while retaining compatibility for general image fusion (multi-modal, multi-exposure, and multi-focus), and also demonstrating striking controllability on more generalization experiments.","The code is available at https://github.com/YangSun22/TC-MoA ."],"url":"http://arxiv.org/abs/2403.12494v1","category":"cs.CV"}
{"created":"2024-03-19 07:02:06","title":"A Trainable Feature Extractor Module for Deep Neural Networks and Scanpath Classification","abstract":"Scanpath classification is an area in eye tracking research with possible applications in medicine, manufacturing as well as training systems for students in various domains. In this paper we propose a trainable feature extraction module for deep neural networks. The purpose of this module is to transform a scanpath into a feature vector which is directly useable for the deep neural network architecture. Based on the backpropagated error of the deep neural network, the feature extraction module adapts its parameters to improve the classification performance. Therefore, our feature extraction module is jointly trainable with the deep neural network. The motivation to this feature extraction module is based on classical histogram-based approaches which usually compute distributions over a scanpath. We evaluated our module on three public datasets and compared it to the state of the art approaches.","sentences":["Scanpath classification is an area in eye tracking research with possible applications in medicine, manufacturing as well as training systems for students in various domains.","In this paper we propose a trainable feature extraction module for deep neural networks.","The purpose of this module is to transform a scanpath into a feature vector which is directly useable for the deep neural network architecture.","Based on the backpropagated error of the deep neural network, the feature extraction module adapts its parameters to improve the classification performance.","Therefore, our feature extraction module is jointly trainable with the deep neural network.","The motivation to this feature extraction module is based on classical histogram-based approaches which usually compute distributions over a scanpath.","We evaluated our module on three public datasets and compared it to the state of the art approaches."],"url":"http://arxiv.org/abs/2403.12493v1","category":"cs.CV"}
{"created":"2024-03-19 06:07:01","title":"Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot","abstract":"Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures. However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation. To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming. Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces' frictional coefficients on displacements in different motion phases. Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained. The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with multiple joints. Simulation experiments have been conducted to illustrate the effectiveness of the proposed modeling.","sentences":["Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures.","However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation.","To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming.","Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces' frictional coefficients on displacements in different motion phases.","Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained.","The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with multiple joints.","Simulation experiments have been conducted to illustrate the effectiveness of the proposed modeling."],"url":"http://arxiv.org/abs/2403.12471v1","category":"cs.RO"}
{"created":"2024-03-19 05:52:56","title":"CrossTune: Black-Box Few-Shot Classification with Label Enhancement","abstract":"Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning. A switch mechanism is implemented to exclude low-quality ChatGPT-generated data. Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average. Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach.","sentences":["Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks.","One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them.","Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts.","Therefore, we are motivated to study black-box language model adaptation without prompt search.","Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions.","Its effectiveness is examined in the context of few-shot text classification.","To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning.","A switch mechanism is implemented to exclude low-quality ChatGPT-generated data.","Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average.","Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.12468v1","category":"cs.CL"}
{"created":"2024-03-19 05:27:04","title":"CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation","abstract":"Open-vocabulary video instance segmentation strives to segment and track instances belonging to an open set of categories in a video. The vision-language model Contrastive Language-Image Pre-training (CLIP) has shown strong zero-shot classification ability in image-level open-vocabulary task. In this paper, we propose a simple encoder-decoder network, called CLIP-VIS, to adapt CLIP for open-vocabulary video instance segmentation. Our CLIP-VIS adopts frozen CLIP image encoder and introduces three modules, including class-agnostic mask generation, temporal topK-enhanced matching, and weighted open-vocabulary classification. Given a set of initial queries, class-agnostic mask generation employs a transformer decoder to predict query masks and corresponding object scores and mask IoU scores. Then, temporal topK-enhanced matching performs query matching across frames by using K mostly matched frames. Finally, weighted open-vocabulary classification first generates query visual features with mask pooling, and second performs weighted classification using object scores and mask IoU scores. Our CLIP-VIS does not require the annotations of instance categories and identities. The experiments are performed on various video instance segmentation datasets, which demonstrate the effectiveness of our proposed method, especially on novel categories. When using ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and APn scores of 32.1% and 40.3% on validation set of LV-VIS dataset, which outperforms OV2Seg by 11.0% and 24.0% respectively. We will release the source code and models at https://github.com/zwq456/CLIP-VIS.git.","sentences":["Open-vocabulary video instance segmentation strives to segment and track instances belonging to an open set of categories in a video.","The vision-language model Contrastive Language-Image Pre-training (CLIP) has shown strong zero-shot classification ability in image-level open-vocabulary task.","In this paper, we propose a simple encoder-decoder network, called CLIP-VIS, to adapt CLIP for open-vocabulary video instance segmentation.","Our CLIP-VIS adopts frozen CLIP image encoder and introduces three modules, including class-agnostic mask generation, temporal topK-enhanced matching, and weighted open-vocabulary classification.","Given a set of initial queries, class-agnostic mask generation employs a transformer decoder to predict query masks and corresponding object scores and mask IoU scores.","Then, temporal topK-enhanced matching performs query matching across frames by using K mostly matched frames.","Finally, weighted open-vocabulary classification first generates query visual features with mask pooling, and second performs weighted classification using object scores and mask IoU scores.","Our CLIP-VIS does not require the annotations of instance categories and identities.","The experiments are performed on various video instance segmentation datasets, which demonstrate the effectiveness of our proposed method, especially on novel categories.","When using ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and APn scores of 32.1% and 40.3% on validation set of LV-VIS dataset, which outperforms OV2Seg by 11.0% and 24.0% respectively.","We will release the source code and models at https://github.com/zwq456/CLIP-VIS.git."],"url":"http://arxiv.org/abs/2403.12455v1","category":"cs.CV"}
{"created":"2024-03-19 05:17:47","title":"Do Generated Data Always Help Contrastive Learning?","abstract":"Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation. Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely data-centric strategy without introducing any extra computation cost. On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods. Notably, without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods. Code is available at https://github.com/PKU-ML/adainf.","sentences":["Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations.","With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized.","These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''.","However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning.","We investigate the causes behind this failure from the perspective of both data inflation and data augmentation.","For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa.","We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation.","Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely data-centric strategy without introducing any extra computation cost.","On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods.","Notably, without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods.","Code is available at https://github.com/PKU-ML/adainf."],"url":"http://arxiv.org/abs/2403.12448v1","category":"cs.LG"}
{"created":"2024-03-19 04:46:56","title":"Algorithmic Complexity Attacks on Dynamic Learned Indexes","abstract":"Learned Index Structures (LIS) view a sorted index as a model that learns the data distribution, takes a data element key as input, and outputs the predicted position of the key. The original LIS can only handle lookup operations with no support for updates, rendering it impractical to use for typical workloads. To address this limitation, recent studies have focused on designing efficient dynamic learned indexes. ALEX, as the pioneering dynamic learned index structures, enables dynamism by incorporating a series of design choices, including adaptive key space partitioning, dynamic model retraining, and sophisticated engineering and policies that prioritize read/write performance. While these design choices offer improved average-case performance, the emphasis on flexibility and performance increases the attack surface by allowing adversarial behaviors that maximize ALEX's memory space and time complexity in worst-case scenarios. In this work, we present the first systematic investigation of algorithmic complexity attacks (ACAs) targeting the worst-case scenarios of ALEX. We introduce new ACAs that fall into two categories, space ACAs and time ACAs, which target the memory space and time complexity, respectively. First, our space ACA on data nodes exploits ALEX's gapped array layout and uses Multiple-Choice Knapsack (MCK) to generate an optimal adversarial insertion plan for maximizing the memory consumption at the data node level. Second, our space ACA on internal nodes exploits ALEX's catastrophic cost mitigation mechanism, causing an out-of-memory error with only a few hundred adversarial insertions. Third, our time ACA generates pathological insertions to increase the disparity between the actual key distribution and the linear models of data nodes, deteriorating the runtime performance by up to 1,641X compared to ALEX operating under legitimate workloads.","sentences":["Learned Index Structures (LIS) view a sorted index as a model that learns the data distribution, takes a data element key as input, and outputs the predicted position of the key.","The original LIS can only handle lookup operations with no support for updates, rendering it impractical to use for typical workloads.","To address this limitation, recent studies have focused on designing efficient dynamic learned indexes.","ALEX, as the pioneering dynamic learned index structures, enables dynamism by incorporating a series of design choices, including adaptive key space partitioning, dynamic model retraining, and sophisticated engineering and policies that prioritize read/write performance.","While these design choices offer improved average-case performance, the emphasis on flexibility and performance increases the attack surface by allowing adversarial behaviors that maximize ALEX's memory space and time complexity in worst-case scenarios.","In this work, we present the first systematic investigation of algorithmic complexity attacks (ACAs) targeting the worst-case scenarios of ALEX.","We introduce new ACAs that fall into two categories, space ACAs and time ACAs, which target the memory space and time complexity, respectively.","First, our space ACA on data nodes exploits ALEX's gapped array layout and uses Multiple-Choice Knapsack (MCK) to generate an optimal adversarial insertion plan for maximizing the memory consumption at the data node level.","Second, our space ACA on internal nodes exploits ALEX's catastrophic cost mitigation mechanism, causing an out-of-memory error with only a few hundred adversarial insertions.","Third, our time ACA generates pathological insertions to increase the disparity between the actual key distribution and the linear models of data nodes, deteriorating the runtime performance by up to 1,641X compared to ALEX operating under legitimate workloads."],"url":"http://arxiv.org/abs/2403.12433v1","category":"cs.DB"}
{"created":"2024-03-19 04:36:41","title":"TransformMix: Learning Transformation and Mixing Strategies from Data","abstract":"Data augmentation improves the generalization power of deep learning models by synthesizing more training samples. Sample-mixing is a popular data augmentation approach that creates additional data by combining existing samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs. Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically. A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets. If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations. In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data. In particular, TransformMix applies learned transformations and mixing masks to create compelling mixed images that contain correct and important information for the target tasks. We demonstrate the effectiveness of TransformMix on multiple datasets in transfer learning, classification, object detection, and knowledge distillation settings. Experimental results show that our method achieves better performance as well as efficiency when compared with strong sample-mixing baselines.","sentences":["Data augmentation improves the generalization power of deep learning models by synthesizing more training samples.","Sample-mixing is a popular data augmentation approach that creates additional data by combining existing samples.","Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs.","Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically.","A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets.","If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations.","In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data.","In particular, TransformMix applies learned transformations and mixing masks to create compelling mixed images that contain correct and important information for the target tasks.","We demonstrate the effectiveness of TransformMix on multiple datasets in transfer learning, classification, object detection, and knowledge distillation settings.","Experimental results show that our method achieves better performance as well as efficiency when compared with strong sample-mixing baselines."],"url":"http://arxiv.org/abs/2403.12429v1","category":"cs.CV"}
{"created":"2024-03-19 04:02:57","title":"STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model","abstract":"Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of STG networks. STG-Mamba is formulated as an Encoder-Decoder architecture, which takes GS3B as the basic module, for efficient sequential data modeling. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph Neural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothly fits in the context of selective state space evolution, and at the same time keeps linear complexity. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time.","sentences":["Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning.","In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time.","In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension.","In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of STG networks.","STG-Mamba is formulated as an Encoder-Decoder architecture, which takes GS3B as the basic module, for efficient sequential data modeling.","Furthermore, to strengthen GNN's ability of modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph Neural Networks (KFGN) for adaptive graph structure upgrading.","KFGN smoothly fits in the context of selective state space evolution, and at the same time keeps linear complexity.","Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba.","It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time."],"url":"http://arxiv.org/abs/2403.12418v1","category":"cs.LG"}
{"created":"2024-03-19 04:02:31","title":"On Predictive planning and counterfactual learning in active inference","abstract":"Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.","sentences":["Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important.","Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making.","In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'.","Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making.","We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent.","Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making."],"url":"http://arxiv.org/abs/2403.12417v1","category":"cs.AI"}
{"created":"2024-03-19 03:53:47","title":"Third-Party Language Model Performance Prediction from Instruction","abstract":"Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions. However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task. We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time. We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, and prompt format. Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations of modern instruction-following natural language processing systems.","sentences":["Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions.","However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task.","We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time.","We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, and prompt format.","Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations of modern instruction-following natural language processing systems."],"url":"http://arxiv.org/abs/2403.12413v1","category":"cs.CL"}
{"created":"2024-03-19 03:19:07","title":"VQ-NeRV: A Vector Quantized Neural Representation for Videos","abstract":"Implicit neural representations (INR) excel in encoding videos within neural networks, showcasing promise in computer vision tasks like video compression and denoising. INR-based approaches reconstruct video frames from content-agnostic embeddings, which hampers their efficacy in video frame regression and restricts their generalization ability for video interpolation. To address these deficiencies, Hybrid Neural Representation for Videos (HNeRV) was introduced with content-adaptive embeddings. Nevertheless, HNeRV's compression ratios remain relatively low, attributable to an oversight in leveraging the network's shallow features and inter-frame residual information. In this work, we introduce an advanced U-shaped architecture, Vector Quantized-NeRV (VQ-NeRV), which integrates a novel component--the VQ-NeRV Block. This block incorporates a codebook mechanism to discretize the network's shallow residual features and inter-frame residual information effectively. This approach proves particularly advantageous in video compression, as it results in smaller size compared to quantized features. Furthermore, we introduce an original codebook optimization technique, termed shallow codebook optimization, designed to refine the utility and efficiency of the codebook. The experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video regression tasks, delivering superior reconstruction quality (with an increase of 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp) efficiency, and improved video inpainting outcomes.","sentences":["Implicit neural representations (INR) excel in encoding videos within neural networks, showcasing promise in computer vision tasks like video compression and denoising.","INR-based approaches reconstruct video frames from content-agnostic embeddings, which hampers their efficacy in video frame regression and restricts their generalization ability for video interpolation.","To address these deficiencies, Hybrid Neural Representation for Videos (HNeRV) was introduced with content-adaptive embeddings.","Nevertheless, HNeRV's compression ratios remain relatively low, attributable to an oversight in leveraging the network's shallow features and inter-frame residual information.","In this work, we introduce an advanced U-shaped architecture, Vector Quantized-NeRV (VQ-NeRV), which integrates a novel component--the VQ-NeRV Block.","This block incorporates a codebook mechanism to discretize the network's shallow residual features and inter-frame residual information effectively.","This approach proves particularly advantageous in video compression, as it results in smaller size compared to quantized features.","Furthermore, we introduce an original codebook optimization technique, termed shallow codebook optimization, designed to refine the utility and efficiency of the codebook.","The experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video regression tasks, delivering superior reconstruction quality (with an increase of 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp) efficiency, and improved video inpainting outcomes."],"url":"http://arxiv.org/abs/2403.12401v1","category":"cs.CV"}
{"created":"2024-03-19 03:11:42","title":"Hierarchical Digital Twin for Efficient 6G Network Orchestration via Adaptive Attribute Selection and Scalable Network Modeling","abstract":"Achieving a holistic and long-term understanding through accurate network modeling is essential for orchestrating future networks with increasing service diversity and infrastructure complexities. However, due to unselective data collection and uniform processing, traditional modeling approaches undermine the efficacy and timeliness of network orchestration. Additionally, temporal disparities arising from various modeling delays further impair the centralized decision-making with distributed models. In this paper, we propose a new hierarchical digital twin paradigm adapting to real-time network situations for problem-centered model construction. Specifically, we introduce an adaptive attribute selection mechanism that evaluates the distinct modeling values of diverse network attributes, considering their relevance to current network scenarios and inherent modeling complexity. By prioritizing critical attributes at higher layers, an efficient evaluation of network situations is achieved to identify target areas. Subsequently, scalable network modeling facilitates the inclusion of all identified elements at the lower layers, where more fine-grained digital twins are developed to generate targeted solutions for user association and power allocation. Furthermore, virtual-physical domain synchronization is implemented to maintain accurate temporal alignment between the digital twins and their physical counterparts, spanning from the construction to the utilization of the proposed paradigm. Extensive simulations validate the proposed approach, demonstrating its effectiveness in efficiently identifying pressing issues and delivering network orchestration solutions in complex 6G HetNets.","sentences":["Achieving a holistic and long-term understanding through accurate network modeling is essential for orchestrating future networks with increasing service diversity and infrastructure complexities.","However, due to unselective data collection and uniform processing, traditional modeling approaches undermine the efficacy and timeliness of network orchestration.","Additionally, temporal disparities arising from various modeling delays further impair the centralized decision-making with distributed models.","In this paper, we propose a new hierarchical digital twin paradigm adapting to real-time network situations for problem-centered model construction.","Specifically, we introduce an adaptive attribute selection mechanism that evaluates the distinct modeling values of diverse network attributes, considering their relevance to current network scenarios and inherent modeling complexity.","By prioritizing critical attributes at higher layers, an efficient evaluation of network situations is achieved to identify target areas.","Subsequently, scalable network modeling facilitates the inclusion of all identified elements at the lower layers, where more fine-grained digital twins are developed to generate targeted solutions for user association and power allocation.","Furthermore, virtual-physical domain synchronization is implemented to maintain accurate temporal alignment between the digital twins and their physical counterparts, spanning from the construction to the utilization of the proposed paradigm.","Extensive simulations validate the proposed approach, demonstrating its effectiveness in efficiently identifying pressing issues and delivering network orchestration solutions in complex 6G HetNets."],"url":"http://arxiv.org/abs/2403.12398v1","category":"cs.NI"}
{"created":"2024-03-19 03:08:08","title":"A kinetic-magnetohydrodynamic model with adaptive mesh refinement for modeling heliosphere neutral-plasma interaction","abstract":"The charge exchange between the interstellar medium (ISM) and the solar wind plasma is crucial for determining the structures of the heliosphere. Since both the neutral-ion and neutral-neutral collision mean free paths are either comparable to or larger than the size of the heliosphere, the neutral phase space distribution can deviate far away from the Maxwellian distribution. A kinetic description for the neutrals is crucial for accurately modeling the heliosphere. It is computationally challenging to run three-dimensional (3D) time-dependent kinetic simulations due to the large number of macro-particles. In this paper, we present the new highly efficient SHIELD-2 model with a kinetic model of neutrals and a magnetohydrodynamic (MHD) model for the ions and electrons. To improve the simulation efficiency, we implement adaptive mesh refinement (AMR) and particle splitting and merging algorithms for the neutral particles to reduce the particle number that is required for an accurate simulation. We present several tests to verify and demonstrate the capabilities of the model.","sentences":["The charge exchange between the interstellar medium (ISM) and the solar wind plasma is crucial for determining the structures of the heliosphere.","Since both the neutral-ion and neutral-neutral collision mean free paths are either comparable to or larger than the size of the heliosphere, the neutral phase space distribution can deviate far away from the Maxwellian distribution.","A kinetic description for the neutrals is crucial for accurately modeling the heliosphere.","It is computationally challenging to run three-dimensional (3D) time-dependent kinetic simulations due to the large number of macro-particles.","In this paper, we present the new highly efficient SHIELD-2 model with a kinetic model of neutrals and a magnetohydrodynamic (MHD) model for the ions and electrons.","To improve the simulation efficiency, we implement adaptive mesh refinement (AMR) and particle splitting and merging algorithms for the neutral particles to reduce the particle number that is required for an accurate simulation.","We present several tests to verify and demonstrate the capabilities of the model."],"url":"http://arxiv.org/abs/2403.12395v1","category":"physics.space-ph"}
{"created":"2024-03-19 02:56:18","title":"ProgrammableGrass: A Shape-Changing Artificial Grass Display Adapted for Dynamic and Interactive Display Features","abstract":"There are various proposals for employing grass materials as a green landscape-friendly display. However, it is difficult for current techniques to display smooth animations using 8-bit images and to adjust display resolution, similar to conventional displays. We present ProgrammableGrass, an artificial grass display with scalable resolution, capable of swiftly controlling grass color at 8-bit levels. This grass display can control grass colors linearly at the 8-bit level, similar to an LCD display, and can also display not only 8-bit-based images but also videos. This display enables pixel-by-pixel color transitions from yellow to green using fixed-length yellow and adjustable-length green grass. We designed a grass module that can be connected to other modules. Utilizing a proportional derivative control, the grass colors are manipulated to display animations at approximately 10 [fps]. Since the relationship between grass lengths and colors is nonlinear, we developed a calibration system for ProgrammableGrass. We revealed that this calibration system allows ProgrammableGrass to linearly control grass colors at 8-bit levels through experiments under multiple conditions. Lastly, we demonstrate ProgrammableGrass to show smooth animations with 8-bit grayscale images. Moreover, we show several application examples to illustrate the potential of ProgrammableGrass. With the advancement of this technology, users will be able to treat grass as a green-based interactive display device.","sentences":["There are various proposals for employing grass materials as a green landscape-friendly display.","However, it is difficult for current techniques to display smooth animations using 8-bit images and to adjust display resolution, similar to conventional displays.","We present ProgrammableGrass, an artificial grass display with scalable resolution, capable of swiftly controlling grass color at 8-bit levels.","This grass display can control grass colors linearly at the 8-bit level, similar to an LCD display, and can also display not only 8-bit-based images but also videos.","This display enables pixel-by-pixel color transitions from yellow to green using fixed-length yellow and adjustable-length green grass.","We designed a grass module that can be connected to other modules.","Utilizing a proportional derivative control, the grass colors are manipulated to display animations at approximately 10 [fps].","Since the relationship between grass lengths and colors is nonlinear, we developed a calibration system for ProgrammableGrass.","We revealed that this calibration system allows ProgrammableGrass to linearly control grass colors at 8-bit levels through experiments under multiple conditions.","Lastly, we demonstrate ProgrammableGrass to show smooth animations with 8-bit grayscale images.","Moreover, we show several application examples to illustrate the potential of ProgrammableGrass.","With the advancement of this technology, users will be able to treat grass as a green-based interactive display device."],"url":"http://arxiv.org/abs/2403.12387v1","category":"cs.GR"}
{"created":"2024-03-19 02:47:33","title":"Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising","abstract":"Deep learning-based denoiser has been the focus of recent development on image denoising. In the past few years, there has been increasing interest in developing self-supervised denoising networks that only require noisy images, without the need for clean ground truth for training. However, a performance gap remains between current self-supervised methods and their supervised counterparts. Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios. Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between self-supervised and supervised methods, thereby enhancing the performance of self-supervised learning. To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges the gap between self-supervised and supervised learning. Furthermore, we have discovered that several existing self-supervised denoising frameworks naturally fall within the proposed trace-constraint loss as subcases. Extensive experiments conducted on natural and confocal image datasets indicate that our method achieves state-of-the-art performance within the realm of zero-shot self-supervised image denoising approaches, without relying on any assumptions regarding the noise.","sentences":["Deep learning-based denoiser has been the focus of recent development on image denoising.","In the past few years, there has been increasing interest in developing self-supervised denoising networks that only require noisy images, without the need for clean ground truth for training.","However, a performance gap remains between current self-supervised methods and their supervised counterparts.","Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios.","Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between self-supervised and supervised methods, thereby enhancing the performance of self-supervised learning.","To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges the gap between self-supervised and supervised learning.","Furthermore, we have discovered that several existing self-supervised denoising frameworks naturally fall within the proposed trace-constraint loss as subcases.","Extensive experiments conducted on natural and confocal image datasets indicate that our method achieves state-of-the-art performance within the realm of zero-shot self-supervised image denoising approaches, without relying on any assumptions regarding the noise."],"url":"http://arxiv.org/abs/2403.12382v1","category":"eess.IV"}
{"created":"2024-03-19 02:47:10","title":"Explainable AutoML (xAutoML) with adaptive modeling for yield enhancement in semiconductor smart manufacturing","abstract":"Enhancing yield is recognized as a paramount driver to reducing production costs in semiconductor smart manufacturing. However, optimizing and ensuring high yield rates is a highly complex and technical challenge, especially while maintaining reliable yield diagnosis and prognosis, and this shall require understanding all the confounding factors in a complex condition. This study proposes a domain-specific explainable automated machine learning technique (termed xAutoML), which autonomously self-learns the optimal models for yield prediction, with an extent of explainability, and also provides insights on key diagnosis factors. The xAutoML incorporates tailored problem-solving functionalities in an auto-optimization pipeline to address the intricacies of semiconductor yield enhancement. Firstly, to capture the key diagnosis factors, knowledge-informed feature extraction coupled with model-agnostic key feature selection is designed. Secondly, combined algorithm selection and hyperparameter tuning with adaptive loss are developed to generate optimized classifiers for better defect prediction, and adaptively evolve in response to shifting data patterns. Moreover, a suite of explainability tools is provided throughout the AutoML pipeline, enhancing user understanding and fostering trust in the automated processes. The proposed xAutoML exhibits superior performance, with domain-specific refined countermeasures, adaptive optimization capabilities, and embedded explainability. Findings exhibit that the proposed xAutoML is a compelling solution for semiconductor yield improvement, defect diagnosis, and related applications.","sentences":["Enhancing yield is recognized as a paramount driver to reducing production costs in semiconductor smart manufacturing.","However, optimizing and ensuring high yield rates is a highly complex and technical challenge, especially while maintaining reliable yield diagnosis and prognosis, and this shall require understanding all the confounding factors in a complex condition.","This study proposes a domain-specific explainable automated machine learning technique (termed xAutoML), which autonomously self-learns the optimal models for yield prediction, with an extent of explainability, and also provides insights on key diagnosis factors.","The xAutoML incorporates tailored problem-solving functionalities in an auto-optimization pipeline to address the intricacies of semiconductor yield enhancement.","Firstly, to capture the key diagnosis factors, knowledge-informed feature extraction coupled with model-agnostic key feature selection is designed.","Secondly, combined algorithm selection and hyperparameter tuning with adaptive loss are developed to generate optimized classifiers for better defect prediction, and adaptively evolve in response to shifting data patterns.","Moreover, a suite of explainability tools is provided throughout the AutoML pipeline, enhancing user understanding and fostering trust in the automated processes.","The proposed xAutoML exhibits superior performance, with domain-specific refined countermeasures, adaptive optimization capabilities, and embedded explainability.","Findings exhibit that the proposed xAutoML is a compelling solution for semiconductor yield improvement, defect diagnosis, and related applications."],"url":"http://arxiv.org/abs/2403.12381v1","category":"cs.CE"}
{"created":"2024-03-19 02:19:57","title":"Class and Region-Adaptive Constraints for Network Calibration","abstract":"In this work, we present a novel approach to calibrate segmentation networks that considers the inherent challenges posed by different categories and object regions. In particular, we present a formulation that integrates class and region-wise constraints into the learning objective, with multiple penalty weights to account for class and region differences. Finding the optimal penalty weights manually, however, might be unfeasible, and potentially hinder the optimization process. To overcome this limitation, we propose an approach based on Class and Region-Adaptive constraints (CRaC), which allows to learn the class and region-wise penalty weights during training. CRaC is based on a general Augmented Lagrangian method, a well-established technique in constrained optimization. Experimental results on two popular segmentation benchmarks, and two well-known segmentation networks, demonstrate the superiority of CRaC compared to existing approaches. The code is available at: https://github.com/Bala93/CRac/","sentences":["In this work, we present a novel approach to calibrate segmentation networks that considers the inherent challenges posed by different categories and object regions.","In particular, we present a formulation that integrates class and region-wise constraints into the learning objective, with multiple penalty weights to account for class and region differences.","Finding the optimal penalty weights manually, however, might be unfeasible, and potentially hinder the optimization process.","To overcome this limitation, we propose an approach based on Class and Region-Adaptive constraints (CRaC), which allows to learn the class and region-wise penalty weights during training.","CRaC is based on a general Augmented Lagrangian method, a well-established technique in constrained optimization.","Experimental results on two popular segmentation benchmarks, and two well-known segmentation networks, demonstrate the superiority of CRaC compared to existing approaches.","The code is available at: https://github.com/Bala93/CRac/"],"url":"http://arxiv.org/abs/2403.12364v1","category":"cs.CV"}
{"created":"2024-03-19 02:07:22","title":"The Lyapunov exponent as a signature of dissipative many-body quantum chaos","abstract":"A distinct feature of Hermitian quantum chaotic dynamics is the exponential increase of certain out-of-time-order-correlation (OTOC) functions around the Ehrenfest time with a rate given by a Lyapunov exponent. Physically, the OTOCs describe the growth of quantum uncertainty that crucially depends on the nature of the quantum motion. Here, we employ the OTOC in order to provide a precise definition of dissipative quantum chaos. For this purpose, we compute analytically the Lyapunov exponent for the vectorized formulation of the large $q$-limit of a $q$-body Sachdev-Ye-Kitaev model coupled to a Markovian bath. These analytic results are confirmed by an explicit numerical calculation of the Lyapunov exponent for several values of $q \\geq 4$ based on the solutions of the Schwinger-Dyson and Bethe-Salpeter equations. We show that the Lyapunov exponent decreases monotonically as the coupling to the bath increases and eventually becomes negative at a critical value of the coupling signaling a transition to a dynamics which is no longer quantum chaotic. Therefore, a positive Lyapunov exponent is a defining feature of dissipative many-body quantum chaos. The observation of the breaking of the exponential growth for sufficiently strong coupling suggests that dissipative quantum chaos may require in certain cases a sufficiently weak coupling to the environment.","sentences":["A distinct feature of Hermitian quantum chaotic dynamics is the exponential increase of certain out-of-time-order-correlation (OTOC) functions around the Ehrenfest time with a rate given by a Lyapunov exponent.","Physically, the OTOCs describe the growth of quantum uncertainty that crucially depends on the nature of the quantum motion.","Here, we employ the OTOC in order to provide a precise definition of dissipative quantum chaos.","For this purpose, we compute analytically the Lyapunov exponent for the vectorized formulation of the large $q$-limit of a $q$-body Sachdev-Ye-Kitaev model coupled to a Markovian bath.","These analytic results are confirmed by an explicit numerical calculation of the Lyapunov exponent for several values of $q \\geq 4$ based on the solutions of the Schwinger-Dyson and Bethe-Salpeter equations.","We show that the Lyapunov exponent decreases monotonically as the coupling to the bath increases and eventually becomes negative at a critical value of the coupling signaling a transition to a dynamics which is no longer quantum chaotic.","Therefore, a positive Lyapunov exponent is a defining feature of dissipative many-body quantum chaos.","The observation of the breaking of the exponential growth for sufficiently strong coupling suggests that dissipative quantum chaos may require in certain cases a sufficiently weak coupling to the environment."],"url":"http://arxiv.org/abs/2403.12359v1","category":"hep-th"}
{"created":"2024-03-19 01:49:40","title":"Local well-posedness for dispersion generalized Benjamin-Ono equations in Fourier-Lebesgue spaces","abstract":"We prove that the Cauchy problem for the dispersion generalized Benjamin-Ono equation where $0<\\alpha \\leq 1$ \\begin{eqnarray*} \\left\\{ \\begin{array}{l} \\partial_t u+|\\partial_x|^{1+\\alpha}\\partial_x u+uu_x=0,\\\\ u(x,0)=u_0(x), \\end{array} \\right. \\end{eqnarray*} is $C^2$ locally well-posed in the Fourier-Lebesgue space $\\widehat{H}^{s}_{r}(\\mathbb{R})$. This is proved via Picard iteration arguments using $X^{s,b}$-type space adapted to the Fourier-Lebesgue space, inspired by the work of Gr\\\"unrock and Vega. We also proved the ill-posedness result that shows the range is sharp. Note that, previously, Molinet, Saut and Tzvetkov \\cite{MST2001} proved that the solution map is not $C^2$ in $H^s$ for any $s$ if $0\\leq \\alpha<1$. However, in the Fourier-Lebesgue space, we have a stronger smoothing effect to handle the $high\\times low$ interactions.","sentences":["We prove that the Cauchy problem for the dispersion generalized Benjamin-Ono equation where $0<\\alpha \\leq 1$ \\begin{eqnarray*} \\left\\{ \\begin{array}{l} \\partial_t u+|\\partial_x|^{1+\\alpha}\\partial_x u+uu_x=0,\\\\ u(x,0)=u_0(x), \\end{array} \\right.","\\end{eqnarray*} is $C^2$ locally well-posed in the Fourier-Lebesgue space $\\widehat{H}^{s}_{r}(\\mathbb{R})$.","This is proved via Picard iteration arguments using $X^{s,b}$-type space adapted to the Fourier-Lebesgue space, inspired by the work of Gr\\\"unrock and Vega.","We also proved the ill-posedness result that shows the range is sharp.","Note that, previously, Molinet, Saut and Tzvetkov \\cite{MST2001} proved that the solution map is not $C^2$ in $H^s$ for any $s$ if $0\\leq \\alpha<1$. However, in the Fourier-Lebesgue space, we have a stronger smoothing effect to handle the $high\\times low$ interactions."],"url":"http://arxiv.org/abs/2403.12353v1","category":"math.AP"}
{"created":"2024-03-19 01:27:15","title":"Human Factors in Space Exploration: Opportunities for International and Interdisciplinary Collaboration","abstract":"As humanity pushes the boundaries of space exploration, human factors research becomes more important. Human factors encompass a broad spectrum of psychological, physiological, and ergonomic factors that affect human performance, well-being, and safety in the unique and challenging space environment. This panel explores the multifaceted field of human factors in space exploration and highlights the opportunities that lie in fostering international and interdisciplinary cooperation. This exploration delves into the current state of research on human factors in space missions, addressing the physiological and psychological challenges astronauts face during long space flights. It emphasizes the importance of interdisciplinary collaboration, combining knowledge from fields such as psychology, medicine, engineering, and design to address the complex interaction of factors affecting human performance and adaptation to the space environment","sentences":["As humanity pushes the boundaries of space exploration, human factors research becomes more important.","Human factors encompass a broad spectrum of psychological, physiological, and ergonomic factors that affect human performance, well-being, and safety in the unique and challenging space environment.","This panel explores the multifaceted field of human factors in space exploration and highlights the opportunities that lie in fostering international and interdisciplinary cooperation.","This exploration delves into the current state of research on human factors in space missions, addressing the physiological and psychological challenges astronauts face during long space flights.","It emphasizes the importance of interdisciplinary collaboration, combining knowledge from fields such as psychology, medicine, engineering, and design to address the complex interaction of factors affecting human performance and adaptation to the space environment"],"url":"http://arxiv.org/abs/2403.12344v1","category":"cs.HC"}
{"created":"2024-03-19 00:09:50","title":"A maximum penalised likelihood approach for semiparametric accelerated failure time models with time-varying covariates and partly interval censoring","abstract":"Accelerated failure time (AFT) models are frequently used for modelling survival data. This approach is attractive as it quantifies the direct relationship between the time until an event occurs and various covariates. It asserts that the failure times experience either acceleration or deceleration through a multiplicative factor when these covariates are present. While existing literature provides numerous methods for fitting AFT models with time-fixed covariates, adapting these approaches to scenarios involving both time-varying covariates and partly interval-censored data remains challenging. In this paper, we introduce a maximum penalised likelihood approach to fit a semiparametric AFT model. This method, designed for survival data with partly interval-censored failure times, accommodates both time-fixed and time-varying covariates. We utilise Gaussian basis functions to construct a smooth approximation of the nonparametric baseline hazard and fit the model via a constrained optimisation approach. To illustrate the effectiveness of our proposed method, we conduct a comprehensive simulation study. We also present an implementation of our approach on a randomised clinical trial dataset on advanced melanoma patients.","sentences":["Accelerated failure time (AFT) models are frequently used for modelling survival data.","This approach is attractive as it quantifies the direct relationship between the time until an event occurs and various covariates.","It asserts that the failure times experience either acceleration or deceleration through a multiplicative factor when these covariates are present.","While existing literature provides numerous methods for fitting AFT models with time-fixed covariates, adapting these approaches to scenarios involving both time-varying covariates and partly interval-censored data remains challenging.","In this paper, we introduce a maximum penalised likelihood approach to fit a semiparametric AFT model.","This method, designed for survival data with partly interval-censored failure times, accommodates both time-fixed and time-varying covariates.","We utilise Gaussian basis functions to construct a smooth approximation of the nonparametric baseline hazard and fit the model via a constrained optimisation approach.","To illustrate the effectiveness of our proposed method, we conduct a comprehensive simulation study.","We also present an implementation of our approach on a randomised clinical trial dataset on advanced melanoma patients."],"url":"http://arxiv.org/abs/2403.12332v1","category":"stat.ME"}
{"created":"2024-03-18 23:20:08","title":"Improving LoRA in Privacy-preserving Federated Learning","abstract":"Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs. The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.","sentences":["Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency.","LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module.","However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters.","A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server.","Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs.","The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices.","Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL.","Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks."],"url":"http://arxiv.org/abs/2403.12313v1","category":"cs.LG"}
{"created":"2024-03-18 22:39:03","title":"Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach","abstract":"Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health. Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes. Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns. This study investigates the application of LLMs for extracting severity-related information for various SUD diagnoses from clinical notes. We propose a workflow employing zero-shot learning of LLMs with carefully crafted prompts and post-processing techniques. Through experimentation with Flan-T5, an open-source LLM, we demonstrate its superior recall compared to the rule-based approach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness of LLMs in extracting severity information, contributing to improved risk assessment and treatment planning for SUD patients.","sentences":["Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society.","SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health.","Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes.","Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language.","Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns.","This study investigates the application of LLMs for extracting severity-related information for various SUD diagnoses from clinical notes.","We propose a workflow employing zero-shot learning of LLMs with carefully crafted prompts and post-processing techniques.","Through experimentation with Flan-T5, an open-source LLM, we demonstrate its superior recall compared to the rule-based approach.","Focusing on 11 categories of SUD diagnoses, we show the effectiveness of LLMs in extracting severity information, contributing to improved risk assessment and treatment planning for SUD patients."],"url":"http://arxiv.org/abs/2403.12297v1","category":"cs.CL"}
{"created":"2024-03-18 22:09:48","title":"The Wreaths of KHAN: Uniform Graph Feature Selection with False Discovery Rate Control","abstract":"Graphical models find numerous applications in biology, chemistry, sociology, neuroscience, etc. While substantial progress has been made in graph estimation, it remains largely unexplored how to select significant graph signals with uncertainty assessment, especially those graph features related to topological structures including cycles (i.e., wreaths), cliques, hubs, etc. These features play a vital role in protein substructure analysis, drug molecular design, and brain network connectivity analysis. To fill the gap, we propose a novel inferential framework for general high dimensional graphical models to select graph features with false discovery rate controlled. Our method is based on the maximum of $p$-values from single edges that comprise the topological feature of interest, thus is able to detect weak signals. Moreover, we introduce the $K$-dimensional persistent Homology Adaptive selectioN (KHAN) algorithm to select all the homological features within $K$ dimensions with the uniform control of the false discovery rate over continuous filtration levels. The KHAN method applies a novel discrete Gram-Schmidt algorithm to select statistically significant generators from the homology group. We apply the structural screening method to identify the important residues of the SARS-CoV-2 spike protein during the binding process to the ACE2 receptors. We score the residues for all domains in the spike protein by the $p$-value weighted filtration level in the network persistent homology for the closed, partially open, and open states and identify the residues crucial for protein conformational changes and thus being potential targets for inhibition.","sentences":["Graphical models find numerous applications in biology, chemistry, sociology, neuroscience, etc.","While substantial progress has been made in graph estimation, it remains largely unexplored how to select significant graph signals with uncertainty assessment, especially those graph features related to topological structures including cycles (i.e., wreaths), cliques, hubs, etc.","These features play a vital role in protein substructure analysis, drug molecular design, and brain network connectivity analysis.","To fill the gap, we propose a novel inferential framework for general high dimensional graphical models to select graph features with false discovery rate controlled.","Our method is based on the maximum of $p$-values from single edges that comprise the topological feature of interest, thus is able to detect weak signals.","Moreover, we introduce the $K$-dimensional persistent Homology Adaptive selectioN (KHAN) algorithm to select all the homological features within $K$ dimensions with the uniform control of the false discovery rate over continuous filtration levels.","The KHAN method applies a novel discrete Gram-Schmidt algorithm to select statistically significant generators from the homology group.","We apply the structural screening method to identify the important residues of the SARS-CoV-2 spike protein during the binding process to the ACE2 receptors.","We score the residues for all domains in the spike protein by the $p$-value weighted filtration level in the network persistent homology for the closed, partially open, and open states and identify the residues crucial for protein conformational changes and thus being potential targets for inhibition."],"url":"http://arxiv.org/abs/2403.12284v1","category":"math.ST"}
{"created":"2024-03-18 21:07:57","title":"Adaptive LPD Radar Waveform Design with Generative Deep Learning","abstract":"We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing. To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can generate LPD waveforms that reduce detectability by up to 90% while simultaneously offering improved ambiguity function (sensing) characteristics. Our framework also provides a mechanism to trade-off detectability and sensing performance.","sentences":["We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment.","Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing.","To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background.","To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms.","We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network.","We find that our method can generate LPD waveforms that reduce detectability by up to 90% while simultaneously offering improved ambiguity function (sensing) characteristics.","Our framework also provides a mechanism to trade-off detectability and sensing performance."],"url":"http://arxiv.org/abs/2403.12254v1","category":"eess.SP"}
{"created":"2024-03-18 20:20:13","title":"Fusion Transformer with Object Mask Guidance for Image Forgery Analysis","abstract":"In this work, we introduce OMG-Fuser, a fusion transformer-based network designed to extract information from various forensic signals to enable robust image forgery detection and localization. Our approach can operate with an arbitrary number of forensic signals and leverages object information for their analysis -- unlike previous methods that rely on fusion schemes with few signals and often disregard image semantics. To this end, we design a forensic signal stream composed of a transformer guided by an object attention mechanism, associating patches that depict the same objects. In that way, we incorporate object-level information from the image. Each forensic signal is processed by a different stream that adapts to its peculiarities. Subsequently, a token fusion transformer efficiently aggregates the outputs of an arbitrary number of network streams and generates a fused representation for each image patch. These representations are finally processed by a long-range dependencies transformer that captures the intrinsic relations between the image patches. We assess two fusion variants on top of the proposed approach: (i) score-level fusion that fuses the outputs of multiple image forensics algorithms and (ii) feature-level fusion that fuses low-level forensic traces directly. Both variants exceed state-of-the-art performance on seven datasets for image forgery detection and localization, with a relative average improvement of 12.1% and 20.4% in terms of F1. Our network demonstrates robustness against traditional and novel forgery attacks and can be expanded with new signals without training from scratch.","sentences":["In this work, we introduce OMG-Fuser, a fusion transformer-based network designed to extract information from various forensic signals to enable robust image forgery detection and localization.","Our approach can operate with an arbitrary number of forensic signals and leverages object information for their analysis -- unlike previous methods that rely on fusion schemes with few signals and often disregard image semantics.","To this end, we design a forensic signal stream composed of a transformer guided by an object attention mechanism, associating patches that depict the same objects.","In that way, we incorporate object-level information from the image.","Each forensic signal is processed by a different stream that adapts to its peculiarities.","Subsequently, a token fusion transformer efficiently aggregates the outputs of an arbitrary number of network streams and generates a fused representation for each image patch.","These representations are finally processed by a long-range dependencies transformer that captures the intrinsic relations between the image patches.","We assess two fusion variants on top of the proposed approach: (i) score-level fusion that fuses the outputs of multiple image forensics algorithms and (ii) feature-level fusion that fuses low-level forensic traces directly.","Both variants exceed state-of-the-art performance on seven datasets for image forgery detection and localization, with a relative average improvement of 12.1% and 20.4% in terms of F1.","Our network demonstrates robustness against traditional and novel forgery attacks and can be expanded with new signals without training from scratch."],"url":"http://arxiv.org/abs/2403.12229v1","category":"cs.CV"}
{"created":"2024-03-18 20:18:32","title":"Large-scale flood modeling and forecasting with FloodCast","abstract":"Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators. GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling. Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods. Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling. Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors.","sentences":["Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost.","This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings.","In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast.","The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling.","In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction.","In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators.","GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains.","Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling.","Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods.","Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling.","Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors."],"url":"http://arxiv.org/abs/2403.12226v1","category":"cs.LG"}
{"created":"2024-03-18 20:02:31","title":"Secure Synchronization of Heterogeneous Pulse-Coupled Oscillators","abstract":"In this paper, we consider the synchronization of heterogeneous pulse-coupled oscillators (PCOs), where some of the oscillators might be faulty or malicious. The oscillators interact through identical pulses at discrete instants and evolve continuously with different frequencies otherwise. Despite the presence of misbehaviors, benign oscillators aim to reach synchronization. To achieve this objective, two resilient synchronization protocols are developed in this paper by adapting the real-valued mean-subsequence reduced (MSR) algorithm to pulse-based interactions. The first protocol relies on packet-based communication to transmit absolute frequencies, while the second protocol operates purely with pulses to calculate relative frequencies. In both protocols, each normal oscillator periodically counts the received pulses to detect possible malicious behaviors. By disregarding suspicious pulses from its neighbors, the oscillator updates both its phases and frequencies. The paper establishes sufficient conditions on the initial states and graph structure under which resilient synchronization is achieved in the PCO network. Specifically, the normal oscillators can either detect the presence of malicious nodes or synchronize in both phases and frequencies. Additionally, a comparison between the two algorithms reveals a trade-off between relaxed initial conditions and reduced communication burden.","sentences":["In this paper, we consider the synchronization of heterogeneous pulse-coupled oscillators (PCOs), where some of the oscillators might be faulty or malicious.","The oscillators interact through identical pulses at discrete instants and evolve continuously with different frequencies otherwise.","Despite the presence of misbehaviors, benign oscillators aim to reach synchronization.","To achieve this objective, two resilient synchronization protocols are developed in this paper by adapting the real-valued mean-subsequence reduced (MSR) algorithm to pulse-based interactions.","The first protocol relies on packet-based communication to transmit absolute frequencies, while the second protocol operates purely with pulses to calculate relative frequencies.","In both protocols, each normal oscillator periodically counts the received pulses to detect possible malicious behaviors.","By disregarding suspicious pulses from its neighbors, the oscillator updates both its phases and frequencies.","The paper establishes sufficient conditions on the initial states and graph structure under which resilient synchronization is achieved in the PCO network.","Specifically, the normal oscillators can either detect the presence of malicious nodes or synchronize in both phases and frequencies.","Additionally, a comparison between the two algorithms reveals a trade-off between relaxed initial conditions and reduced communication burden."],"url":"http://arxiv.org/abs/2403.12218v2","category":"eess.SY"}
{"created":"2024-03-18 19:25:57","title":"Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight","abstract":"We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL's advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tuning. Our experiments in both simulated and real-world environments demonstrate that our approach achieves superior performance and robustness than IL or RL alone in navigating a quadrotor through a racing course using only visual information without explicit state estimation.","sentences":["We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing.","We focus on directly processing visual input without explicit state estimation.","While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs.","Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift.","To overcome these limitations, we propose a novel training framework combining RL and IL's advantages.","Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tuning.","Our experiments in both simulated and real-world environments demonstrate that our approach achieves superior performance and robustness than IL or RL alone in navigating a quadrotor through a racing course using only visual information without explicit state estimation."],"url":"http://arxiv.org/abs/2403.12203v1","category":"cs.RO"}
{"created":"2024-03-18 18:59:42","title":"PETScML: Second-order solvers for training regression problems in Scientific Machine Learning","abstract":"In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications. At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods. However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization. We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization. We empirically demonstrate the superior efficacy of a trust region method based on the Gauss-Newton approximation of the Hessian in improving the generalization errors arising from regression tasks when learning surrogate models for a wide range of scientific machine-learning techniques and test cases. All the conventional second-order solvers tested, including L-BFGS and inexact Newton with line-search, compare favorably, either in terms of cost or accuracy, with the adaptive first-order methods used to validate the surrogate models.","sentences":["In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications.","At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods.","However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization.","We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization.","We empirically demonstrate the superior efficacy of a trust region method based on the Gauss-Newton approximation of the Hessian in improving the generalization errors arising from regression tasks when learning surrogate models for a wide range of scientific machine-learning techniques and test cases.","All the conventional second-order solvers tested, including L-BFGS and inexact Newton with line-search, compare favorably, either in terms of cost or accuracy, with the adaptive first-order methods used to validate the surrogate models."],"url":"http://arxiv.org/abs/2403.12188v1","category":"cs.LG"}
{"created":"2024-03-18 18:50:59","title":"AMReX and pyAMReX: Looking Beyond ECP","abstract":"AMReX is a software framework for the development of block-structured mesh applications with adaptive mesh refinement (AMR). AMReX was initially developed and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale Computing Project, and is continuing to grow post-ECP. In addition to adding new functionality and performance improvements to the core AMReX framework, we have also developed a Python binding, pyAMReX, that provides a bridge between AMReX-based application codes and the data science ecosystem. pyAMReX provides zero-copy application GPU data access for AI/ML, in situ analysis and application coupling, and enables rapid, massively parallel prototyping. In this paper we review the overall functionality of AMReX and pyAMReX, focusing on new developments, new functionality, and optimizations of key operations. We also summarize capabilities of ECP projects that used AMReX and provide an overview of new, non-ECP applications.","sentences":["AMReX is a software framework for the development of block-structured mesh applications with adaptive mesh refinement (AMR).","AMReX was initially developed and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale Computing Project, and is continuing to grow post-ECP.","In addition to adding new functionality and performance improvements to the core AMReX framework, we have also developed a Python binding, pyAMReX, that provides a bridge between AMReX-based application codes and the data science ecosystem.","pyAMReX provides zero-copy application GPU data access for AI/ML, in situ analysis and application coupling, and enables rapid, massively parallel prototyping.","In this paper we review the overall functionality of AMReX and pyAMReX, focusing on new developments, new functionality, and optimizations of key operations.","We also summarize capabilities of ECP projects that used AMReX and provide an overview of new, non-ECP applications."],"url":"http://arxiv.org/abs/2403.12179v1","category":"cs.DC"}
{"created":"2024-03-18 18:00:00","title":"Unveiling MOA-2007-BLG-192: An M Dwarf Hosting a Likely Super-Earth","abstract":"We present an analysis of high angular resolution images of the microlensing target MOA-2007-BLG-192 using Keck adaptive optics and the Hubble Space Telescope. The planetary host star is robustly detected as it separates from the background source star in nearly all of the Keck and Hubble data. The amplitude and direction of the lens-source separation allows us to break a degeneracy related to the microlensing parallax and source radius crossing time. Thus, we are able to reduce the number of possible solutions by a factor of ${\\sim}2$, demonstrating the power of high angular resolution follow-up imaging for events with sparse light curve coverage. Following Bennett et al. 2023, we apply constraints from the high resolution imaging on the light curve modeling to find host star and planet masses of $M_{\\textrm{host}} = 0.28 \\pm 0.04M_{\\odot}$ and $m_p = 12.49^{+65.47}_{-8.03}M_{\\oplus}$ at a distance from Earth of $D_L = 2.16 \\pm 0.30\\,$kpc. This work illustrates the necessity for the Nancy Grace Roman Galactic Exoplanet Survey (RGES) to use its own high resolution imaging to inform light curve modeling for microlensing planets that the mission discovers.","sentences":["We present an analysis of high angular resolution images of the microlensing target MOA-2007-BLG-192 using Keck adaptive optics and the Hubble Space Telescope.","The planetary host star is robustly detected as it separates from the background source star in nearly all of the Keck and Hubble data.","The amplitude and direction of the lens-source separation allows us to break a degeneracy related to the microlensing parallax and source radius crossing time.","Thus, we are able to reduce the number of possible solutions by a factor of ${\\sim}2$, demonstrating the power of high angular resolution follow-up imaging for events with sparse light curve coverage.","Following Bennett et al. 2023, we apply constraints from the high resolution imaging on the light curve modeling to find host star and planet masses of $M_{\\textrm{host}} = 0.28 \\pm 0.04M_{\\odot}$ and $m_p = 12.49^{+65.47}_{-8.03}M_{\\oplus}$ at a distance from Earth of $D_L = 2.16 \\pm 0.30\\,$kpc.","This work illustrates the necessity for the Nancy Grace Roman Galactic Exoplanet Survey (RGES) to use its own high resolution imaging to inform light curve modeling for microlensing planets that the mission discovers."],"url":"http://arxiv.org/abs/2403.12118v1","category":"astro-ph.EP"}
{"created":"2024-03-18 18:00:00","title":"Low-overhead non-Clifford topological fault-tolerant circuits for all non-chiral abelian topological phases","abstract":"We propose a family of explicit geometrically local circuits realizing any abelian non-chiral topological phase as an actively error-corrected fault-tolerant memory. These circuits are constructed from measuring 1-form symmetries in discrete fixed-point path integrals, which we express through cellular cohomology and higher-order cup products. The specific path integral we use is the abelian Dijkgraaf-Witten state sum on a 3-dimensional cellulation, which is a spacetime representation of the twisted quantum double model. The resulting circuits are based on a syndrome extraction circuit of the (qudit) stabilizer toric code, into which we insert non-Clifford phase gates that implement the ``twist''. The overhead compared to the toric code is moderate, in contrast to known constructions for twisted abelian phases. We also show that other architectures for the (qudit) toric code phase, like measurement-based topological quantum computation or Floquet codes, can be enriched with phase gates to implement twisted quantum doubles instead of their untwisted versions. As a further result, we prove fault tolerance under arbitrary local (including non-Pauli) noise for a very general class of topological circuits that we call 1-form symmetric fixed-point circuits. This notion unifies the circuits in this paper as well as the stabilizer toric code, subsystem toric code, measurement-based topological quantum computation, or the (CSS) honeycomb Floquet code. We also demonstrate how our method can be adapted to construct fault-tolerant circuits for specific non-Abelian phases. In the appendix we present an explicit combinatorial procedure to define formulas for higher cup products on arbitrary cellulations, which might be interesting in its own right to the TQFT and topological-phases community.","sentences":["We propose a family of explicit geometrically local circuits realizing any abelian non-chiral topological phase as an actively error-corrected fault-tolerant memory.","These circuits are constructed from measuring 1-form symmetries in discrete fixed-point path integrals, which we express through cellular cohomology and higher-order cup products.","The specific path integral we use is the abelian Dijkgraaf-Witten state sum on a 3-dimensional cellulation, which is a spacetime representation of the twisted quantum double model.","The resulting circuits are based on a syndrome extraction circuit of the (qudit) stabilizer toric code, into which we insert non-Clifford phase gates that implement the ``twist''.","The overhead compared to the toric code is moderate, in contrast to known constructions for twisted abelian phases.","We also show that other architectures for the (qudit) toric code phase, like measurement-based topological quantum computation or Floquet codes, can be enriched with phase gates to implement twisted quantum doubles instead of their untwisted versions.","As a further result, we prove fault tolerance under arbitrary local (including non-Pauli) noise for a very general class of topological circuits that we call 1-form symmetric fixed-point circuits.","This notion unifies the circuits in this paper as well as the stabilizer toric code, subsystem toric code, measurement-based topological quantum computation, or the (CSS) honeycomb Floquet code.","We also demonstrate how our method can be adapted to construct fault-tolerant circuits for specific non-Abelian phases.","In the appendix we present an explicit combinatorial procedure to define formulas for higher cup products on arbitrary cellulations, which might be interesting in its own right to the TQFT and topological-phases community."],"url":"http://arxiv.org/abs/2403.12119v1","category":"quant-ph"}
{"created":"2024-03-18 17:59:40","title":"One-Step Image Translation with Text-to-Image Models","abstract":"In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference. This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives. Our code and models are available at https://github.com/GaParmar/img2img-turbo.","sentences":["In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning.","To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives.","Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting.","We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain.","We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference.","This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives.","Our code and models are available at https://github.com/GaParmar/img2img-turbo."],"url":"http://arxiv.org/abs/2403.12036v1","category":"cs.CV"}
{"created":"2024-03-18 17:59:09","title":"Generic 3D Diffusion Adapter Using Controlled Multi-View Editing","abstract":"Open-domain 3D object synthesis has been lagging behind image synthesis due to limited data and higher computational complexity. To bridge this gap, recent works have investigated multi-view diffusion but often fall short in either 3D consistency, visual quality, or efficiency. This paper proposes MVEdit, which functions as a 3D counterpart of SDEdit, employing ancestral sampling to jointly denoise multi-view images and output high-quality textured meshes. Built on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency through a training-free 3D Adapter, which lifts the 2D views of the last timestep into a coherent 3D representation, then conditions the 2D views of the next timestep using rendered views, without uncompromising visual quality. With an inference time of only 2-5 minutes, this framework achieves better trade-off between quality and speed than score distillation. MVEdit is highly versatile and extendable, with a wide range of applications including text/image-to-3D generation, 3D-to-3D editing, and high-quality texture synthesis. In particular, evaluations demonstrate state-of-the-art performance in both image-to-3D and text-guided texture generation tasks. Additionally, we introduce a method for fine-tuning 2D latent diffusion models on small 3D datasets with limited resources, enabling fast low-resolution text-to-3D initialization.","sentences":["Open-domain 3D object synthesis has been lagging behind image synthesis due to limited data and higher computational complexity.","To bridge this gap, recent works have investigated multi-view diffusion but often fall short in either 3D consistency, visual quality, or efficiency.","This paper proposes MVEdit, which functions as a 3D counterpart of SDEdit, employing ancestral sampling to jointly denoise multi-view images and output high-quality textured meshes.","Built on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency through a training-free 3D Adapter, which lifts the 2D views of the last timestep into a coherent 3D representation, then conditions the 2D views of the next timestep using rendered views, without uncompromising visual quality.","With an inference time of only 2-5 minutes, this framework achieves better trade-off between quality and speed than score distillation.","MVEdit is highly versatile and extendable, with a wide range of applications including text/image-to-3D generation, 3D-to-3D editing, and high-quality texture synthesis.","In particular, evaluations demonstrate state-of-the-art performance in both image-to-3D and text-guided texture generation tasks.","Additionally, we introduce a method for fine-tuning 2D latent diffusion models on small 3D datasets with limited resources, enabling fast low-resolution text-to-3D initialization."],"url":"http://arxiv.org/abs/2403.12032v2","category":"cs.CV"}
{"created":"2024-03-18 17:58:13","title":"Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning","abstract":"Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new-stage spaces. Correspondingly, we design a semantic-guided prototype complement strategy that synthesizes old classes' new features without using any old class instance. Extensive experiments on seven benchmark datasets verify EASE's state-of-the-art performance. Code is available at: https://github.com/sun-hailong/CVPR24-Ease","sentences":["Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting.","Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones.","Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes.","As a result, it is desired to figure out a way of efficient model updating without harming former knowledge.","In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL.","To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces.","These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces.","As data evolves, the expanding subspaces render the old class classifiers incompatible with new-stage spaces.","Correspondingly, we design a semantic-guided prototype complement strategy that synthesizes old classes' new features without using any old class instance.","Extensive experiments on seven benchmark datasets verify EASE's state-of-the-art performance.","Code is available at: https://github.com/sun-hailong/CVPR24-Ease"],"url":"http://arxiv.org/abs/2403.12030v1","category":"cs.CV"}
{"created":"2024-03-18 17:58:02","title":"Align and Distill: Unifying and Improving Domain Adaptive Object Detection","abstract":"Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin. ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel. Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research. Code and data are available: https://github.com/justinkay/aldi and https://github.com/visipedia/caltech-fish-counting.","sentences":["Object detectors often perform poorly on data that differs from their training set.","Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge.","Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks.","We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin.","ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel.","Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research.","Code and data are available: https://github.com/justinkay/aldi and https://github.com/visipedia/caltech-fish-counting."],"url":"http://arxiv.org/abs/2403.12029v1","category":"cs.CV"}
{"created":"2024-03-18 17:51:16","title":"EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents","abstract":"Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We show qualitatively how the LLM adapts training environments to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls. Lastly, we present detailed ablation studies for our design choices.","sentences":["Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment.","Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.","Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at?","We propose EnvGen, a novel framework to address this question.","First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel.","Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.).","Next, we train a small RL agent in a mixture of the original and LLM-generated environments.","Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance.","We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments.","We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster.","We show qualitatively how the LLM adapts training environments to help improve RL agents' weaker skills over time.","Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls.","Lastly, we present detailed ablation studies for our design choices."],"url":"http://arxiv.org/abs/2403.12014v1","category":"cs.CL"}
{"created":"2024-03-18 17:46:06","title":"SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion","abstract":"We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.","sentences":["We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object.","Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization.","However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation.","In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS.","We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation.","Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works."],"url":"http://arxiv.org/abs/2403.12008v1","category":"cs.CV"}
{"created":"2024-03-18 17:41:26","title":"GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning","abstract":"Self-supervised learning has achieved remarkable success in acquiring high-quality representations from unlabeled data. The widely adopted contrastive learning framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image. However, existing techniques to construct positive views highly rely on manual transformations, resulting in limited diversity and potentially false positive pairs. To tackle these challenges, we present GenView, a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics. We develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability. Additionally, we introduce a quality-driven contrastive loss, which assesses the quality of positive pairs by considering both foreground similarity and background diversity. This loss prioritizes the high-quality positive pairs we construct while reducing the influence of low-quality pairs, thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive data augmentation. Thanks to the improved positive view quality and the quality-driven contrastive loss, GenView significantly improves self-supervised learning across various tasks. For instance, GenView improves MoCov2 performance by 2.5%/2.2% on ImageNet linear/semi-supervised classification. Moreover, GenView even performs much better than naively augmenting the ImageNet dataset with Laion400M or ImageNet21K. Code is available at https://github.com/xiaojieli0903/genview.","sentences":["Self-supervised learning has achieved remarkable success in acquiring high-quality representations from unlabeled data.","The widely adopted contrastive learning framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image.","However, existing techniques to construct positive views highly rely on manual transformations, resulting in limited diversity and potentially false positive pairs.","To tackle these challenges, we present GenView, a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics.","We develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability.","Additionally, we introduce a quality-driven contrastive loss, which assesses the quality of positive pairs by considering both foreground similarity and background diversity.","This loss prioritizes the high-quality positive pairs we construct while reducing the influence of low-quality pairs, thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive data augmentation.","Thanks to the improved positive view quality and the quality-driven contrastive loss, GenView significantly improves self-supervised learning across various tasks.","For instance, GenView improves MoCov2 performance by 2.5%/2.2% on ImageNet linear/semi-supervised classification.","Moreover, GenView even performs much better than naively augmenting the ImageNet dataset with Laion400M or ImageNet21K. Code is available at https://github.com/xiaojieli0903/genview."],"url":"http://arxiv.org/abs/2403.12003v1","category":"cs.CV"}
{"created":"2024-03-18 17:32:23","title":"Learning Useful Representations of Recurrent Neural Network Weight Matrices","abstract":"Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority.","sentences":["Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers.","The program of an RNN is its weight matrix.","How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks?","While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping.","We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs.","Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs.","We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior.","We create and release the first two 'model zoo' datasets for RNN weight representation learning.","One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits.","With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications.","On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority."],"url":"http://arxiv.org/abs/2403.11998v1","category":"cs.LG"}
{"created":"2024-03-18 17:27:00","title":"Adaptive stepsize algorithms for Langevin dynamics","abstract":"We discuss the design of an invariant measure-preserving transformed dynamics for the numerical treatment of Langevin dynamics based on rescaling of time, with the goal of sampling from an invariant measure. Given an appropriate monitor function which characterizes the numerical difficulty of the problem as a function of the state of the system, this method allows the stepsizes to be reduced only when necessary, facilitating efficient recovery of long-time behavior. We study both the overdamped and underdamped Langevin dynamics. We investigate how an appropriate correction term that ensures preservation of the invariant measure should be incorporated into a numerical splitting scheme. Finally, we demonstrate the use of the technique in several model systems, including a Bayesian sampling problem with a steep prior.","sentences":["We discuss the design of an invariant measure-preserving transformed dynamics for the numerical treatment of Langevin dynamics based on rescaling of time, with the goal of sampling from an invariant measure.","Given an appropriate monitor function which characterizes the numerical difficulty of the problem as a function of the state of the system, this method allows the stepsizes to be reduced only when necessary, facilitating efficient recovery of long-time behavior.","We study both the overdamped and underdamped Langevin dynamics.","We investigate how an appropriate correction term that ensures preservation of the invariant measure should be incorporated into a numerical splitting scheme.","Finally, we demonstrate the use of the technique in several model systems, including a Bayesian sampling problem with a steep prior."],"url":"http://arxiv.org/abs/2403.11993v2","category":"math.NA"}
{"created":"2024-03-18 17:23:50","title":"Suppression of the superfluid Kelvin-Helmholtz instability due to massive vortex cores, friction and confinement","abstract":"We characterize the dynamical instability responsible for the breakdown of regular rows and necklaces of quantized vortices that appear at the interface between two superfluids in relative motion. Making use of a generalized point-vortex model, we identify several mechanisms leading to the suppression of this instability. They include a non-zero mass of the vortex cores, dissipative processes resulting from the interaction between the vortices and the excitations of the superfluid, and the proximity of the vortex array to the sample boundaries. We show that massive vortex cores not only have a mitigating effect on the dynamical instability, but also change the associated scaling law and affect the direction along which it develops. The predictions of our massive and dissipative point-vortex model are eventually compared against recent experimental measurements of the maximum instability growth rate relevant to vortex necklaces in a cold-atom platform.","sentences":["We characterize the dynamical instability responsible for the breakdown of regular rows and necklaces of quantized vortices that appear at the interface between two superfluids in relative motion.","Making use of a generalized point-vortex model, we identify several mechanisms leading to the suppression of this instability.","They include a non-zero mass of the vortex cores, dissipative processes resulting from the interaction between the vortices and the excitations of the superfluid, and the proximity of the vortex array to the sample boundaries.","We show that massive vortex cores not only have a mitigating effect on the dynamical instability, but also change the associated scaling law and affect the direction along which it develops.","The predictions of our massive and dissipative point-vortex model are eventually compared against recent experimental measurements of the maximum instability growth rate relevant to vortex necklaces in a cold-atom platform."],"url":"http://arxiv.org/abs/2403.11987v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-18 17:12:00","title":"OUCopula: Bi-Channel Multi-Label Copula-Enhanced Adapter-Based CNN for Myopia Screening Based on OU-UWF Images","abstract":"Myopia screening using cutting-edge ultra-widefield (UWF) fundus imaging is potentially significant for ophthalmic outcomes. Current multidisciplinary research between ophthalmology and deep learning (DL) concentrates primarily on disease classification and diagnosis using single-eye images, largely ignoring joint modeling and prediction for Oculus Uterque (OU, both eyes). Inspired by the complex relationships between OU and the high correlation between the (continuous) outcome labels (Spherical Equivalent and Axial Length), we propose a framework of copula-enhanced adapter convolutional neural network (CNN) learning with OU UWF fundus images (OUCopula) for joint prediction of multiple clinical scores. We design a novel bi-channel multi-label CNN that can (1) take bi-channel image inputs subject to both high correlation and heterogeneity (by sharing the same backbone network and employing adapters to parameterize the channel-wise discrepancy), and (2) incorporate correlation information between continuous output labels (using a copula). Solid experiments show that OUCopula achieves satisfactory performance in myopia score prediction compared to backbone models. Moreover, OUCopula can far exceed the performance of models constructed for single-eye inputs. Importantly, our study also hints at the potential extension of the bi-channel model to a multi-channel paradigm and the generalizability of OUCopula across various backbone CNNs.","sentences":["Myopia screening using cutting-edge ultra-widefield (UWF) fundus imaging is potentially significant for ophthalmic outcomes.","Current multidisciplinary research between ophthalmology and deep learning (DL) concentrates primarily on disease classification and diagnosis using single-eye images, largely ignoring joint modeling and prediction for Oculus Uterque (OU, both eyes).","Inspired by the complex relationships between OU and the high correlation between the (continuous) outcome labels (Spherical Equivalent and Axial Length), we propose a framework of copula-enhanced adapter convolutional neural network (CNN) learning with OU UWF fundus images (OUCopula) for joint prediction of multiple clinical scores.","We design a novel bi-channel multi-label CNN that can (1) take bi-channel image inputs subject to both high correlation and heterogeneity (by sharing the same backbone network and employing adapters to parameterize the channel-wise discrepancy), and (2) incorporate correlation information between continuous output labels (using a copula).","Solid experiments show that OUCopula achieves satisfactory performance in myopia score prediction compared to backbone models.","Moreover, OUCopula can far exceed the performance of models constructed for single-eye inputs.","Importantly, our study also hints at the potential extension of the bi-channel model to a multi-channel paradigm and the generalizability of OUCopula across various backbone CNNs."],"url":"http://arxiv.org/abs/2403.11974v1","category":"eess.IV"}
{"created":"2024-03-18 17:08:24","title":"Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory","abstract":"Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation.","sentences":["Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning.","In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties.","Despite the empirical success, theory of conditional diffusion models is largely missing.","This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models.","Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound.","The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique.","Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation."],"url":"http://arxiv.org/abs/2403.11968v1","category":"cs.LG"}
{"created":"2024-03-18 17:08:04","title":"Probing Site-Resolved Current in Strongly Interacting Superconducting Circuit Lattices","abstract":"Transport measurements are fundamental for understanding condensed matter phenomena, from superconductivity to the fractional quantum Hall effect. Analogously, they can be powerful tools for probing synthetic quantum matter in quantum simulators. Here we demonstrate the measurement of in-situ particle current in a superconducting circuit lattice and apply it to study transport in both coherent and bath-coupled lattices. Our method utilizes controlled tunneling in a double-well potential to map current to on-site density, revealing site-resolved current and current statistics. We prepare a strongly interacting Bose-Hubbard lattice at different lattice fillings, and observe the change in current statistics as the many-body states transition from superfluid to Mott insulator. Furthermore, we explore non-equilibrium current dynamics by coupling the lattice to engineered driven-dissipative baths that serve as tunable particle source and drain. We observe steady-state current in discrete conduction channels and interaction-assisted transport. These results establish a versatile platform to investigate microscopic quantum transport in superconducting circuits.","sentences":["Transport measurements are fundamental for understanding condensed matter phenomena, from superconductivity to the fractional quantum Hall effect.","Analogously, they can be powerful tools for probing synthetic quantum matter in quantum simulators.","Here we demonstrate the measurement of in-situ particle current in a superconducting circuit lattice and apply it to study transport in both coherent and bath-coupled lattices.","Our method utilizes controlled tunneling in a double-well potential to map current to on-site density, revealing site-resolved current and current statistics.","We prepare a strongly interacting Bose-Hubbard lattice at different lattice fillings, and observe the change in current statistics as the many-body states transition from superfluid to Mott insulator.","Furthermore, we explore non-equilibrium current dynamics by coupling the lattice to engineered driven-dissipative baths that serve as tunable particle source and drain.","We observe steady-state current in discrete conduction channels and interaction-assisted transport.","These results establish a versatile platform to investigate microscopic quantum transport in superconducting circuits."],"url":"http://arxiv.org/abs/2403.11967v1","category":"quant-ph"}
{"created":"2024-03-18 16:42:39","title":"Learning Dynamical Systems Encoding Non-Linearity within Space Curvature","abstract":"Dynamical Systems (DS) are an effective and powerful means of shaping high-level policies for robotics control. They provide robust and reactive control while ensuring the stability of the driving vector field. The increasing complexity of real-world scenarios necessitates DS with a higher degree of non-linearity, along with the ability to adapt to potential changes in environmental conditions, such as obstacles. Current learning strategies for DSs often involve a trade-off, sacrificing either stability guarantees or offline computational efficiency in order to enhance the capabilities of the learned DS. Online local adaptation to environmental changes is either not taken into consideration or treated as a separate problem. In this paper, our objective is to introduce a method that enhances the complexity of the learned DS without compromising efficiency during training or stability guarantees. Furthermore, we aim to provide a unified approach for seamlessly integrating the initially learned DS's non-linearity with any local non-linearities that may arise due to changes in the environment. We propose a geometrical approach to learn asymptotically stable non-linear DS for robotics control. Each DS is modeled as a harmonic damped oscillator on a latent manifold. By learning the manifold's Euclidean embedded representation, our approach encodes the non-linearity of the DS within the curvature of the space. Having an explicit embedded representation of the manifold allows us to showcase obstacle avoidance by directly inducing local deformations of the space. We demonstrate the effectiveness of our methodology through two scenarios: first, the 2D learning of synthetic vector fields, and second, the learning of 3D robotic end-effector motions in real-world settings.","sentences":["Dynamical Systems (DS) are an effective and powerful means of shaping high-level policies for robotics control.","They provide robust and reactive control while ensuring the stability of the driving vector field.","The increasing complexity of real-world scenarios necessitates DS with a higher degree of non-linearity, along with the ability to adapt to potential changes in environmental conditions, such as obstacles.","Current learning strategies for DSs often involve a trade-off, sacrificing either stability guarantees or offline computational efficiency in order to enhance the capabilities of the learned DS.","Online local adaptation to environmental changes is either not taken into consideration or treated as a separate problem.","In this paper, our objective is to introduce a method that enhances the complexity of the learned DS without compromising efficiency during training or stability guarantees.","Furthermore, we aim to provide a unified approach for seamlessly integrating the initially learned DS's non-linearity with any local non-linearities that may arise due to changes in the environment.","We propose a geometrical approach to learn asymptotically stable non-linear DS for robotics control.","Each DS is modeled as a harmonic damped oscillator on a latent manifold.","By learning the manifold's Euclidean embedded representation, our approach encodes the non-linearity of the DS within the curvature of the space.","Having an explicit embedded representation of the manifold allows us to showcase obstacle avoidance by directly inducing local deformations of the space.","We demonstrate the effectiveness of our methodology through two scenarios: first, the 2D learning of synthetic vector fields, and second, the learning of 3D robotic end-effector motions in real-world settings."],"url":"http://arxiv.org/abs/2403.11948v1","category":"cs.RO"}
{"created":"2024-03-18 16:40:41","title":"Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees","abstract":"With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources. Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs. However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses. We aim to address this challenging problem and introduce a reinforcement learning-based approach using differentiable decision trees. This approach integrates the scalability of data-driven reinforcement learning with the explainability of (differentiable) decision trees. This leads to a controller that can be easily adapted across different houses and provides a simple control policy that can be explained to end-users, further improving user acceptance. As a proof-of-concept, we analyze our method using a home energy management problem, comparing its performance with commercially available rule-based baseline and standard neural network-based RL controllers. Through this preliminary study, we show that the performance of our proposed method is comparable to standard RL-based controllers, outperforming baseline controllers by ~20% in terms of daily cost savings while being straightforward to explain.","sentences":["With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources.","Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs.","However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses.","We aim to address this challenging problem and introduce a reinforcement learning-based approach using differentiable decision trees.","This approach integrates the scalability of data-driven reinforcement learning with the explainability of (differentiable) decision trees.","This leads to a controller that can be easily adapted across different houses and provides a simple control policy that can be explained to end-users, further improving user acceptance.","As a proof-of-concept, we analyze our method using a home energy management problem, comparing its performance with commercially available rule-based baseline and standard neural network-based RL controllers.","Through this preliminary study, we show that the performance of our proposed method is comparable to standard RL-based controllers, outperforming baseline controllers by ~20% in terms of daily cost savings while being straightforward to explain."],"url":"http://arxiv.org/abs/2403.11947v1","category":"eess.SY"}
{"created":"2024-03-18 16:36:09","title":"Perfect Zero-Knowledge PCPs for #P","abstract":"We construct perfect zero-knowledge probabilistically checkable proofs (PZK-PCPs) for every language in #P. This is the first construction of a PZK-PCP for any language outside BPP. Furthermore, unlike previous constructions of (statistical) zero-knowledge PCPs, our construction simultaneously achieves non-adaptivity and zero knowledge against arbitrary (adaptive) polynomial-time malicious verifiers.   Our construction consists of a novel masked sumcheck PCP, which uses the combinatorial nullstellensatz to obtain antisymmetric structure within the hypercube and randomness outside of it. To prove zero knowledge, we introduce the notion of locally simulatable encodings: randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message. We show that the code arising from the sumcheck protocol (the Reed-Muller code augmented with subcube sums) admits a locally simulatable encoding. This reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions.","sentences":["We construct perfect zero-knowledge probabilistically checkable proofs (PZK-PCPs) for every language in #P. This is the first construction of a PZK-PCP for any language outside BPP.","Furthermore, unlike previous constructions of (statistical) zero-knowledge PCPs, our construction simultaneously achieves non-adaptivity and zero knowledge against arbitrary (adaptive) polynomial-time malicious verifiers.   ","Our construction consists of a novel masked sumcheck PCP, which uses the combinatorial nullstellensatz to obtain antisymmetric structure within the hypercube and randomness outside of it.","To prove zero knowledge, we introduce the notion of locally simulatable encodings: randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message.","We show that the code arising from the sumcheck protocol (the Reed-Muller code augmented with subcube sums) admits a locally simulatable encoding.","This reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions."],"url":"http://arxiv.org/abs/2403.11941v2","category":"cs.CC"}
{"created":"2024-03-19 17:55:52","title":"Damped energy-norm a posteriori error estimates for fully discrete approximations of the wave equation using C2-reconstructions","abstract":"We derive a posteriori error estimates for the the scalar wave equation discretized in space by continuous finite elements and in time by the explicit leapfrog scheme. Our analysis combines the idea of invoking extra time-regularity for the right-hand side, as previously introduced in the space semi-discrete setting, with a novel, piecewise quartic, globally twice-differentiable time-reconstruction of the fully discrete solution. Our main results show that the proposed estimator is reliable and efficient in a damped energy norm. These properties are illustrated in a series of numerical examples.","sentences":["We derive a posteriori error estimates for the the scalar wave equation discretized in space by continuous finite elements and in time by the explicit leapfrog scheme.","Our analysis combines the idea of invoking extra time-regularity for the right-hand side, as previously introduced in the space semi-discrete setting, with a novel, piecewise quartic, globally twice-differentiable time-reconstruction of the fully discrete solution.","Our main results show that the proposed estimator is reliable and efficient in a damped energy norm.","These properties are illustrated in a series of numerical examples."],"url":"http://arxiv.org/abs/2403.12954v1","category":"math.NA"}
{"created":"2024-03-19 17:43:57","title":"Neural Differential Algebraic Equations","abstract":"Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system.","sentences":["Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints.","Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships.","Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs.","This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains.","In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks.","Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes.","Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system."],"url":"http://arxiv.org/abs/2403.12938v1","category":"cs.LG"}
{"created":"2024-03-19 17:36:34","title":"Chemical differentiation and gas kinematics around massive young stellar objects in RCW 120","abstract":"We present results of a spectral survey towards a dense molecular condensation and young stellar objects (YSOs) projected on the border of the HII region RCW 120 and discuss emission of 20 molecules which produce the brightest lines. The survey was performed with the APEX telescope in the frequency range 200 -- 260 GHz. We provide evidences for two outflows in the dense gas. The first one is powered by the RCW 120 S2 YSO and oriented along the line of sight. The second outflow around RCW 120 S1 is aligned almost perpendicular to the line of sight. We show that area with bright emission of CH$_3$OH, CH$_3$CCH and CH$_3$CN are organised into an onion-like structure where CH$_3$CN traces warmer regions around the YSOs than the other molecules. Methanol seems to be released to the gas phase by shock waves in the vicinity of the outflows while thermal evaporation still does not work towards the YSOs. We find only a single manifestation of the UV radiation to the molecules, namely, enhanced abundances of small hydrocarbons CCH and c-C$_3$H$_2$ in the photo-dissociation region.","sentences":["We present results of a spectral survey towards a dense molecular condensation and young stellar objects (YSOs) projected on the border of the HII region RCW 120 and discuss emission of 20 molecules which produce the brightest lines.","The survey was performed with the APEX telescope in the frequency range 200 -- 260 GHz.","We provide evidences for two outflows in the dense gas.","The first one is powered by the RCW 120 S2 YSO and oriented along the line of sight.","The second outflow around RCW 120 S1 is aligned almost perpendicular to the line of sight.","We show that area with bright emission of CH$_3$OH, CH$_3$CCH and CH$_3$CN are organised into an onion-like structure where CH$_3$CN traces warmer regions around the YSOs than the other molecules.","Methanol seems to be released to the gas phase by shock waves in the vicinity of the outflows while thermal evaporation still does not work towards the YSOs.","We find only a single manifestation of the UV radiation to the molecules, namely, enhanced abundances of small hydrocarbons CCH and c-C$_3$H$_2$ in the photo-dissociation region."],"url":"http://arxiv.org/abs/2403.12934v1","category":"astro-ph.GA"}
{"created":"2024-03-19 17:32:53","title":"Identifiability and Observability of Nonsmooth Systems via Taylor-like Approximations","abstract":"New sensitivity-based methods are developed for determining identifiability and observability of nonsmooth input-output systems. More specifically, lexicographic calculus is used to construct nonsmooth sensitivity rank condition (SERC) tests, which we call lexicographic SERC (L-SERC) tests. The introduced L-SERC tests are: (i) practically implementable and amenable to large-scale problems; (ii) accurate since they directly treat the nonsmoothness while avoiding, e.g., smoothing approximations; and (iii) analogous to (and indeed recover) their smooth counterparts. To accomplish this, a first-order Taylor-like approximation theory is developed using lexicographic differentiation to directly treat nonsmooth functions. A practically implementable algorithm is proposed that determines partial structural identifiability or observability, a useful characterization in the nonsmooth setting. Lastly, the theory is illustrated through an application in climate modeling.","sentences":["New sensitivity-based methods are developed for determining identifiability and observability of nonsmooth input-output systems.","More specifically, lexicographic calculus is used to construct nonsmooth sensitivity rank condition (SERC) tests, which we call lexicographic SERC (L-SERC) tests.","The introduced L-SERC tests are: (i) practically implementable and amenable to large-scale problems; (ii) accurate since they directly treat the nonsmoothness while avoiding, e.g., smoothing approximations; and (iii) analogous to (and indeed recover) their smooth counterparts.","To accomplish this, a first-order Taylor-like approximation theory is developed using lexicographic differentiation to directly treat nonsmooth functions.","A practically implementable algorithm is proposed that determines partial structural identifiability or observability, a useful characterization in the nonsmooth setting.","Lastly, the theory is illustrated through an application in climate modeling."],"url":"http://arxiv.org/abs/2403.12930v1","category":"math.OC"}
{"created":"2024-03-19 17:29:05","title":"On the $N$-waves hierarchy with constant boundary conditions. Spectral properties","abstract":"The paper is devoted to $N$-wave equations with constant boundary conditions related to symplectic Lie algebras. We study the spectral properties of a class of Lax operators $L$, whose potentials $Q(x,t)$ tend to constants $Q_\\pm$ for $x\\to \\pm \\infty$. For special choices of $Q_\\pm$ we outline the spectral properties of $L$, the direct scattering transform and construct its fundamental analytic solutions. We generalise Wronskian relations for the case of CBC -- this allows us to analyse the mapping between the scattering data and the $x$-derivative of the potential $Q_x$. Next, using the Wronskian relations we derive the dispersion laws for the $N$-wave hierarchy and describe the NLEE related to the given Lax operator.","sentences":["The paper is devoted to $N$-wave equations with constant boundary conditions related to symplectic Lie algebras.","We study the spectral properties of a class of Lax operators $L$, whose potentials $Q(x,t)$ tend to constants $Q_\\pm$ for $x\\to \\pm \\infty$. For special choices of $Q_\\pm$ we outline the spectral properties of $L$, the direct scattering transform and construct its fundamental analytic solutions.","We generalise Wronskian relations for the case of CBC -- this allows us to analyse the mapping between the scattering data and the $x$-derivative of the potential $Q_x$. Next, using the Wronskian relations we derive the dispersion laws for the $N$-wave hierarchy and describe the NLEE related to the given Lax operator."],"url":"http://arxiv.org/abs/2403.12925v1","category":"nlin.SI"}
{"created":"2024-03-19 17:06:04","title":"Regularised Spectral Estimation for High Dimensional Point Processes","abstract":"Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process. We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix. In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons. However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics. In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting. We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes. Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer connectivity in the brain network.","sentences":["Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process.","We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix.","In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons.","However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics.","In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting.","We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes.","Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer connectivity in the brain network."],"url":"http://arxiv.org/abs/2403.12908v1","category":"stat.ME"}
{"created":"2024-03-19 16:58:09","title":"Geodesic vector fields, induced contact structures and tightness in dimension three","abstract":"In this paper, we provide new and simpler proofs of two theorems of Gluck and Harrison on contact structures induced by great circle or line fibrations. Furthermore, we prove that a geodesic vector field whose Jacobi tensor is parallel along flow lines (e.g. if the underlying manifold is locally symmetric) induces a contact structure if the 'mixed' sectional curvatures are nonnegative, and if a certain nondegeneracy condition holds. Additionally, we prove that in dimension three, contact structures admitting a Reeb flow which is either periodic, isometric, or free and proper, must be universally tight. In particular, we generalise an earlier result of Geiges and the author, by showing that every contact form on $\\mathbb{R}^3$ whose Reeb vector field spans a line fibration is necessarily tight. Furthermore, we provide a characterisation of isometric Reeb vector fields. As an application, we recover a result of Kegel and Lange on Seifert fibrations spanned by Reeb vector fields, and we classify closed contact $3$-manifolds with isometric Reeb flows (also known as $R$-contact manifolds) up to diffeomorphism.","sentences":["In this paper, we provide new and simpler proofs of two theorems of Gluck and Harrison on contact structures induced by great circle or line fibrations.","Furthermore, we prove that a geodesic vector field whose Jacobi tensor is parallel along flow lines (e.g. if the underlying manifold is locally symmetric) induces a contact structure if the 'mixed' sectional curvatures are nonnegative, and if a certain nondegeneracy condition holds.","Additionally, we prove that in dimension three, contact structures admitting a Reeb flow which is either periodic, isometric, or free and proper, must be universally tight.","In particular, we generalise an earlier result of Geiges and the author, by showing that every contact form on $\\mathbb{R}^3$ whose Reeb vector field spans a line fibration is necessarily tight.","Furthermore, we provide a characterisation of isometric Reeb vector fields.","As an application, we recover a result of Kegel and Lange on Seifert fibrations spanned by Reeb vector fields, and we classify closed contact $3$-manifolds with isometric Reeb flows (also known as $R$-contact manifolds) up to diffeomorphism."],"url":"http://arxiv.org/abs/2403.12903v1","category":"math.SG"}
{"created":"2024-03-19 16:50:11","title":"Quantum Onsager relations","abstract":"Using quantum information geometry, I derive quantum generalizations of the Onsager rate equations, which model the dynamics of a system near a steady state. By redefining the currents, I propose a version of the equations with a symmetric transport tensor without assuming any detailed balance. With a more conventional definition of the currents, I propose another version where the transport tensor may not be symmetric and detailed balance is a sufficient, though not necessary, condition for the symmetry of the tensor. The relations and relative merits of the two versions are discussed.","sentences":["Using quantum information geometry, I derive quantum generalizations of the Onsager rate equations, which model the dynamics of a system near a steady state.","By redefining the currents, I propose a version of the equations with a symmetric transport tensor without assuming any detailed balance.","With a more conventional definition of the currents, I propose another version where the transport tensor may not be symmetric and detailed balance is a sufficient, though not necessary, condition for the symmetry of the tensor.","The relations and relative merits of the two versions are discussed."],"url":"http://arxiv.org/abs/2403.12896v1","category":"quant-ph"}
{"created":"2024-03-19 16:34:31","title":"Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport","abstract":"We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, we propose to train our model with gradient flow w.r.t. the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition. Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width. Performing a local Polyak-\\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer. This is the first result of this type for infinitely deep and arbitrarily wide ResNets.","sentences":["We study the convergence of gradient flow for the training of deep neural networks.","If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective.","Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent.","To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers.","Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures.","Motivated by this approach, we propose to train our model with gradient flow w.r.t.","the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition.","Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width.","Performing a local Polyak-\\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer.","This is the first result of this type for infinitely deep and arbitrarily wide ResNets."],"url":"http://arxiv.org/abs/2403.12887v1","category":"cs.LG"}
{"created":"2024-03-19 16:33:03","title":"Improved decay results for micropolar flows with nonlinear damping","abstract":"We examine the long-time behavior of solutions (and their derivatives) to the micropolar equations with nonlinear velocity damping. Additionally, we get a speed-up gain of $ t^{1/2} $ for the angular velocity, consistent with established findings for classic micropolar flows lacking nonlinear damping. Consequently, we also obtain a sharper result regarding the asymptotic stability of the micro-rotational velocity $\\ww(\\cdot,t)$. Related results of independent interest are also included.","sentences":["We examine the long-time behavior of solutions (and their derivatives) to the micropolar equations with nonlinear velocity damping.","Additionally, we get a speed-up gain of $ t^{1/2} $ for the angular velocity, consistent with established findings for classic micropolar flows lacking nonlinear damping.","Consequently, we also obtain a sharper result regarding the asymptotic stability of the micro-rotational velocity $\\ww(\\cdot,t)$. Related results of independent interest are also included."],"url":"http://arxiv.org/abs/2403.12885v1","category":"math.AP"}
{"created":"2024-03-19 16:18:12","title":"Markovian lifting and optimal control for integral stochastic Volterra equations with completely monotone kernels","abstract":"In this paper, we focus on solving the optimal control problem for integral stochastic Volterra equations in a finite dimensional setting. In our setting, the noise term is driven by a pure jump L\\'evy noise and the control acts on the intensity of the jumps.   We use recent techniques proposed by Hamaguchi, where a crucial requirement is that the convolution kernel should be a completely monotone function. This allows us to use Bernstein's representation and the machinery of Laplace transform to obtain a Markovian lift.   It is natural that the Markovian lift, in whatever form constructed, transforms the state equation into a stochastic differential equation in an infinite-dimensional space. This space should be large enough to contain all the information about the history of the process. Hence, although the original equation is taken in a finite dimensional space, the resulting lift is always infinite dimensional.   We solve the problem by using the forward-backward approach in the infinite-dimensional setting and prove the existence of the optimal control for the original problem. Under additional assumptions on the coefficients, we see that a control in closed-loop form can be achieved.","sentences":["In this paper, we focus on solving the optimal control problem for integral stochastic Volterra equations in a finite dimensional setting.","In our setting, the noise term is driven by a pure jump L\\'evy noise and the control acts on the intensity of the jumps.   ","We use recent techniques proposed by Hamaguchi, where a crucial requirement is that the convolution kernel should be a completely monotone function.","This allows us to use Bernstein's representation and the machinery of Laplace transform to obtain a Markovian lift.   ","It is natural that the Markovian lift, in whatever form constructed, transforms the state equation into a stochastic differential equation in an infinite-dimensional space.","This space should be large enough to contain all the information about the history of the process.","Hence, although the original equation is taken in a finite dimensional space, the resulting lift is always infinite dimensional.   ","We solve the problem by using the forward-backward approach in the infinite-dimensional setting and prove the existence of the optimal control for the original problem.","Under additional assumptions on the coefficients, we see that a control in closed-loop form can be achieved."],"url":"http://arxiv.org/abs/2403.12875v1","category":"math.OC"}
{"created":"2024-03-19 16:15:44","title":"Wildfire danger prediction optimization with transfer learning","abstract":"Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection. This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas. Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions. The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns. Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\\% in identifying burnt areas. This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires. By combining transfer learning and CNNs, this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations.","sentences":["Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection.","This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas.","Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions.","The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns.","Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\\% in identifying burnt areas.","This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires.","By combining transfer learning and CNNs, this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations."],"url":"http://arxiv.org/abs/2403.12871v1","category":"cs.LG"}
{"created":"2024-03-19 16:15:08","title":"PoNQ: a Neural QEM-based Mesh Representation","abstract":"Although polygon meshes have been a standard representation in geometry processing, their irregular and combinatorial nature hinders their suitability for learning-based applications. In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape, which we denote PoNQ. A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors. Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume. Notably, our representation does not rely on a regular grid, is supervised directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics.","sentences":["Although polygon meshes have been a standard representation in geometry processing, their irregular and combinatorial nature hinders their suitability for learning-based applications.","In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t.","the underlying shape, which we denote PoNQ.","A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors.","Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume.","Notably, our representation does not rely on a regular grid, is supervised directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features.","We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics."],"url":"http://arxiv.org/abs/2403.12870v1","category":"cs.CV"}
{"created":"2024-03-19 16:08:27","title":"A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection","abstract":"Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry. The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations. Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated. While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources. In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints.","sentences":["Spacecraft operations are highly critical, demanding impeccable reliability and safety.","Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures.","With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations.","This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data.","The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures.","Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types.","Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry.","The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations.","Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated.","While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources.","In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints."],"url":"http://arxiv.org/abs/2403.12864v1","category":"cs.LG"}
{"created":"2024-03-19 16:01:25","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning","abstract":"In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.","sentences":["In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance.","However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge.","Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks.","This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles.","We further add a regularization term for adding inductive bias during training.","In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance."],"url":"http://arxiv.org/abs/2403.12856v1","category":"cs.LG"}
{"created":"2024-03-19 15:59:47","title":"An inhomogeneous porous medium equation with non-integrable data: asymptotics","abstract":"We investigate the asymptotic behavior as $t\\to+\\infty$ of solutions to a weighted porous medium equation in $ \\mathbb{R}^N $, whose weight $\\rho(x)$ behaves at spatial infinity like $ |x|^{-\\gamma} $ with subcritical power, namely $ \\gamma \\in [0,2) $. Inspired by some results by Alikakos-Rostamian and Kamin-Ughi from the 1980s on the unweighted problem, we focus on solutions whose initial data $u_0(x)$ are not globally integrable with respect to the weight and behave at infinity like $ |x|^{-\\alpha} $, for $\\alpha\\in(0,N-\\gamma)$. In the special case $ \\rho(x)=|x|^{-\\gamma} $ and $ u_0(x)=|x|^{-\\alpha} $ we show that self-similar solutions of Barenblatt type, i.e. reminiscent of the usual source-type solutions, still exist, although they are no longer compactly supported. Moreover, they exhibit a transition phenomenon which is new even for the unweighted equation. We prove that such self-similar solutions are attractors for the original problem, and convergence takes place globally in suitable weighted $ L^p $ spaces for $p\\in[1,\\infty)$ and even globally in $L^\\infty$ under some mild additional regularity assumptions on the weight. Among the fundamental tools that we exploit, it is worth mentioning a global smoothing effect for non-integrable data.","sentences":["We investigate the asymptotic behavior as $t\\to+\\infty$ of solutions to a weighted porous medium equation in $ \\mathbb{R}^N $, whose weight $\\rho(x)$ behaves at spatial infinity like $ |x|^{-\\gamma} $ with subcritical power, namely $ \\gamma \\in [0,2) $.","Inspired by some results by Alikakos-Rostamian and Kamin-Ughi from the 1980s on the unweighted problem, we focus on solutions whose initial data $u_0(x)$ are not globally integrable with respect to the weight and behave at infinity like $ |x|^{-\\alpha} $, for $\\alpha\\in(0,N-\\gamma)$. In the special case $ \\rho(x)=|x|^{-\\gamma} $ and $ u_0(x)=|x|^{-\\alpha} $ we show that self-similar solutions of Barenblatt type, i.e. reminiscent of the usual source-type solutions, still exist, although they are no longer compactly supported.","Moreover, they exhibit a transition phenomenon which is new even for the unweighted equation.","We prove that such self-similar solutions are attractors for the original problem, and convergence takes place globally in suitable weighted $ L^p $ spaces for $p\\in[1,\\infty)$ and even globally in $L^\\infty$ under some mild additional regularity assumptions on the weight.","Among the fundamental tools that we exploit, it is worth mentioning a global smoothing effect for non-integrable data."],"url":"http://arxiv.org/abs/2403.12854v1","category":"math.AP"}
{"created":"2024-03-19 15:51:29","title":"Giant electrode effect on tunneling magnetoresistance and electroresistance in van der Waals intrinsic multiferroic tunnel junctions","abstract":"Van der Waals multiferroic tunnel junctions (vdW-MFTJs) with multiple nonvolatile resistive states are highly suitable for new physics and next-generation storage electronics. However, currently reported vdW-MFTJs are based on two types of materials, i.e., vdW ferromagnetic and ferroelectric materials, forming a multiferroic system. This undoubtedly introduces additional interfaces, increasing the complexity of experimental preparation. Herein, we engineer vdW intrinsic MFTJs utilizing bilayer VS$_2$. By employing the nonequilibrium Green's function combined with density functional theory, we systematically investigate the influence of three types of electrodes (including non-vdW pure metal Ag/Au, vdW metallic 1T-MoS$_2$/2H-PtTe$_2$, and vdW ferromagnetic metallic Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$) on the electronic transport properties of VS$_2$-based intrinsic MFTJs. We demonstrate that these MFTJs manifest a giant electrode-dependent electronic transport characteristic effect. Comprehensively comparing these electrode pairs, the Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$ electrode combination exhibits optimal transport properties, the maximum TMR (TER) can reach 10949\\% (69\\%) and the minimum resistance-area product (RA) is 0.45 $\\Omega$$\\mu$m$^{2}$, as well as the perfect spin filtering and negative differential resistance effects. More intriguingly, TMR (TER) can be further enhanced to 34000\\% (380\\%) by applying an external bias voltage (0.1 V), while RA can be reduced to 0.16 $\\Omega$$\\mu$m$^{2}$ under the influence of biaxial stress (-3\\%). Our proposed concept of designing vdW-MFTJs using intrinsic multiferroic materials points towards new avenues in experimental exploration.","sentences":["Van der Waals multiferroic tunnel junctions (vdW-MFTJs) with multiple nonvolatile resistive states are highly suitable for new physics and next-generation storage electronics.","However, currently reported vdW-MFTJs are based on two types of materials, i.e., vdW ferromagnetic and ferroelectric materials, forming a multiferroic system.","This undoubtedly introduces additional interfaces, increasing the complexity of experimental preparation.","Herein, we engineer vdW intrinsic MFTJs utilizing bilayer VS$_2$. By employing the nonequilibrium Green's function combined with density functional theory, we systematically investigate the influence of three types of electrodes (including non-vdW pure metal Ag/Au, vdW metallic 1T-MoS$_2$/2H-PtTe$_2$, and vdW ferromagnetic metallic Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$) on the electronic transport properties of VS$_2$-based intrinsic MFTJs.","We demonstrate that these MFTJs manifest a giant electrode-dependent electronic transport characteristic effect.","Comprehensively comparing these electrode pairs, the Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$ electrode combination exhibits optimal transport properties, the maximum TMR (TER) can reach 10949\\% (69\\%) and the minimum resistance-area product (RA) is 0.45 $\\Omega$$\\mu$m$^{2}$, as well as the perfect spin filtering and negative differential resistance effects.","More intriguingly, TMR (TER) can be further enhanced to 34000\\% (380\\%) by applying an external bias voltage (0.1 V), while RA can be reduced to 0.16 $\\Omega$$\\mu$m$^{2}$ under the influence of biaxial stress (-3\\%).","Our proposed concept of designing vdW-MFTJs using intrinsic multiferroic materials points towards new avenues in experimental exploration."],"url":"http://arxiv.org/abs/2403.12845v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 15:49:32","title":"The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems","abstract":"Hybrid systems are dynamical systems with continuous-time and discrete-time components in their dynamics. When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the discrete-time transition of the dynamics: interior impacts and exterior impacts. In this paper we define hybrid systems on principal bundles, study the underlying geometry on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle.","sentences":["Hybrid systems are dynamical systems with continuous-time and discrete-time components in their dynamics.","When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the discrete-time transition of the dynamics: interior impacts and exterior impacts.","In this paper we define hybrid systems on principal bundles, study the underlying geometry on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle."],"url":"http://arxiv.org/abs/2403.12842v1","category":"cs.RO"}
{"created":"2024-03-19 15:32:23","title":"Semilinear elliptic degenerate equations with critical exponent","abstract":"In this paper we are mainly concerned with nontrivial positive solutions to the Dirichlet problem for the degenerate elliptic equation   \\begin{gather}   -\\frac{\\partial^2 u}{\\partial x^2} -\\left|x\\right|^{2k}\\frac{\\partial^2 u}{\\partial y^2}=|x|^{2k}u^p+f(x,y,u) \\quad\\text{ in }\\Omega, \\ u=0 \\quad\\text{ on }\\partial\\Omega,\\label{equ0}   \\end{gather} where $\\Omega$ is a bounded domain with smooth boundary in $\\mathbb{R}^2, \\Omega \\cap \\{x=0\\}\\ne \\emptyset,$ $k\\in\\mathbb N,$ $f(x,y,0)=0,$ and $p=(4+5k)/k$ is the critical exponent. Recently, the equation (1) was investigated in [12] for the subcritical case based on a new result obtained in [17] on embedding theorem of weighted Sobolev spaces. In the critical case considered in this paper we will essentially use the optimal functions and constants found in [17]","sentences":["In this paper we are mainly concerned with nontrivial positive solutions to the Dirichlet problem for the degenerate elliptic equation   \\begin{gather}   -\\frac{\\partial^2 u}{\\partial x^2} -\\left|x\\right|^{2k}\\frac{\\partial^2 u}{\\partial y^2}=|x|^{2k}u^p+f(x,y,u) \\quad\\text{ in }\\Omega, \\ u=0 \\quad\\text{ on }\\partial\\Omega,\\label{equ0}   \\end{gather} where $\\Omega$ is a bounded domain with smooth boundary in $\\mathbb{R}^2, \\Omega \\cap \\{x=0\\}\\ne \\emptyset,$","$k\\in\\mathbb N,$ $f(x,y,0)=0,$ and $p=(4+5k)/k$ is the critical exponent.","Recently, the equation (1) was investigated in [12] for the subcritical case based on a new result obtained in [17] on embedding theorem of weighted Sobolev spaces.","In the critical case considered in this paper we will essentially use the optimal functions and constants found in [17]"],"url":"http://arxiv.org/abs/2403.12828v1","category":"math.AP"}
{"created":"2024-03-19 15:31:24","title":"Cross Algorithms for Cost-Effective Time Integration of Nonlinear Tensor Differential Equations on Low-Rank Tucker Tensor and Tensor Train Manifolds","abstract":"Dynamical low-rank approximation (DLRA) provides a rigorous, cost-effective mathematical framework for solving high-dimensional tensor differential equations (TDEs) on low-rank tensor manifolds. Despite their effectiveness, DLRA-based low-rank approximations lose their computational efficiency when applied to nonlinear TDEs, particularly those exhibiting non-polynomial nonlinearity. In this paper, we present a novel algorithm for the time integration of TDEs on the tensor train and Tucker tensor low-rank manifolds, which are the building blocks of many tensor network decompositions. This paper builds on our previous work (Donello et al., Proceedings of the Royal Society A, Vol. 479, 2023) on solving nonlinear matrix differential equations on low-rank matrix manifolds using CUR decompositions. The methodology we present offers multiple advantages: (i) it leverages cross algorithms based on the discrete empirical interpolation method to strategically sample sparse entries of the time-discrete TDEs to advance the solution in low-rank form. As a result, it offers near-optimal computational savings both in terms of memory and floating-point operations. (ii) The time integration is robust in the presence of small or zero singular values. (iii) The algorithm is remarkably easy to implement, as it requires the evaluation of the full-order model TDE at strategically selected entries and it does not use tangent space projections, whose efficient implementation is intrusive and time-consuming. (iv) We develop high-order explicit Runge-Kutta schemes for the time integration of TDEs on low-rank manifolds. We demonstrate the efficiency of the presented algorithm for several test cases, including a 100-dimensional TDE with non-polynomial nonlinearity.","sentences":["Dynamical low-rank approximation (DLRA) provides a rigorous, cost-effective mathematical framework for solving high-dimensional tensor differential equations (TDEs) on low-rank tensor manifolds.","Despite their effectiveness, DLRA-based low-rank approximations lose their computational efficiency when applied to nonlinear TDEs, particularly those exhibiting non-polynomial nonlinearity.","In this paper, we present a novel algorithm for the time integration of TDEs on the tensor train and Tucker tensor low-rank manifolds, which are the building blocks of many tensor network decompositions.","This paper builds on our previous work (Donello et al., Proceedings of the Royal Society A, Vol. 479, 2023) on solving nonlinear matrix differential equations on low-rank matrix manifolds using CUR decompositions.","The methodology we present offers multiple advantages: (i) it leverages cross algorithms based on the discrete empirical interpolation method to strategically sample sparse entries of the time-discrete TDEs to advance the solution in low-rank form.","As a result, it offers near-optimal computational savings both in terms of memory and floating-point operations.","(ii) The time integration is robust in the presence of small or zero singular values.","(iii) The algorithm is remarkably easy to implement, as it requires the evaluation of the full-order model TDE at strategically selected entries and it does not use tangent space projections, whose efficient implementation is intrusive and time-consuming.","(iv) We develop high-order explicit Runge-Kutta schemes for the time integration of TDEs on low-rank manifolds.","We demonstrate the efficiency of the presented algorithm for several test cases, including a 100-dimensional TDE with non-polynomial nonlinearity."],"url":"http://arxiv.org/abs/2403.12826v1","category":"math.NA"}
{"created":"2024-03-19 15:28:27","title":"Well-posedness and no-uniform dependence for the Euler-Poincar\u00e9 equations in Triebel-Lizorkin spaces","abstract":"In this paper, we study the Cauchy problem of the Euler-Poincar\\'{e} equations in $\\R^d$ with initial data belonging to the Triebel-Lizorkin spaces. We prove the local-in-time unique existence of solutions to the Euler-Poincar\\'{e} equations in $F^s_{p,r}(\\R^d)$. Furthermore, we obtain that the data-to-solution of this equation is continuous but not uniformly continuous in these spaces.","sentences":["In this paper, we study the Cauchy problem of the Euler-Poincar\\'{e} equations in $\\R^d$ with initial data belonging to the Triebel-Lizorkin spaces.","We prove the local-in-time unique existence of solutions to the Euler-Poincar\\'{e} equations in $F^s_{p,r}(\\R^d)$.","Furthermore, we obtain that the data-to-solution of this equation is continuous but not uniformly continuous in these spaces."],"url":"http://arxiv.org/abs/2403.12824v1","category":"math.AP"}
{"created":"2024-03-19 15:21:00","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","abstract":"Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselines and predictive realism successfully validate its generalization ability. Inference efficiency of the proposed model also defeats traditional physics simulation. This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination.","sentences":["Delicate cloth simulations have long been desired in computer graphics.","Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations.","Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics.","This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation.","The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics.","The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks.","The model is tested across different cloth animation cases, without training with new data.","Agreement with baselines and predictive realism successfully validate its generalization ability.","Inference efficiency of the proposed model also defeats traditional physics simulation.","This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination."],"url":"http://arxiv.org/abs/2403.12820v1","category":"cs.GR"}
{"created":"2024-03-19 15:12:56","title":"Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect","abstract":"Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance. To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks. Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated. A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers. In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively. Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional quantized bit vector for feedback, thereby reducing the feedback overhead substantially. Simulation results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios.","sentences":["Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance.","To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks.","Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated.","A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers.","In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively.","Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional quantized bit vector for feedback, thereby reducing the feedback overhead substantially.","Simulation results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios."],"url":"http://arxiv.org/abs/2403.12813v1","category":"cs.IT"}
{"created":"2024-03-19 15:01:18","title":"Learning Neural Volumetric Pose Features for Camera Localization","abstract":"We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses. Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module. This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features. Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework. Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy.","sentences":["We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses.","Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module.","This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features.","Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework.","Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy."],"url":"http://arxiv.org/abs/2403.12800v1","category":"cs.CV"}
{"created":"2024-03-19 14:54:50","title":"Importance sampling for rare event tracking within the ensemble Kalman filtering framework","abstract":"In this work we employ importance sampling (IS) techniques to track a small over-threshold probability of a running maximum associated with the solution of a stochastic differential equation (SDE) within the framework of ensemble Kalman filtering (EnKF). Between two observation times of the EnKF, we propose to use IS with respect to the initial condition of the SDE, IS with respect to the Wiener process via a stochastic optimal control formulation, and combined IS with respect to both initial condition and Wiener process. Both IS strategies require the approximation of the solution of Kolmogorov Backward equation (KBE) with boundary conditions. In multidimensional settings, we employ a Markovian projection dimension reduction technique to obtain an approximation of the solution of the KBE by just solving a one dimensional PDE. The proposed ideas are tested on two illustrative examples: Double Well SDE and Langevin dynamics, and showcase a significant variance reduction compared to the standard Monte Carlo method and another sampling-based IS technique, namely, multilevel cross entropy.","sentences":["In this work we employ importance sampling (IS) techniques to track a small over-threshold probability of a running maximum associated with the solution of a stochastic differential equation (SDE) within the framework of ensemble Kalman filtering (EnKF).","Between two observation times of the EnKF, we propose to use IS with respect to the initial condition of the SDE, IS with respect to the Wiener process via a stochastic optimal control formulation, and combined IS with respect to both initial condition and Wiener process.","Both IS strategies require the approximation of the solution of Kolmogorov Backward equation (KBE) with boundary conditions.","In multidimensional settings, we employ a Markovian projection dimension reduction technique to obtain an approximation of the solution of the KBE by just solving a one dimensional PDE.","The proposed ideas are tested on two illustrative examples: Double Well SDE and Langevin dynamics, and showcase a significant variance reduction compared to the standard Monte Carlo method and another sampling-based IS technique, namely, multilevel cross entropy."],"url":"http://arxiv.org/abs/2403.12793v1","category":"math.NA"}
{"created":"2024-03-19 14:50:13","title":"Total Disentanglement of Font Images into Style and Character Class Features","abstract":"In this paper, we demonstrate a total disentanglement of font images. Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features. It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts. These disentangled features guarantee the reconstruction of the original font image. Various experiments have been conducted to understand the performance of total disentanglement. First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?'' Hofstadter (1985). Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation.","sentences":["In this paper, we demonstrate a total disentanglement of font images.","Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features.","It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts.","These disentangled features guarantee the reconstruction of the original font image.","Various experiments have been conducted to understand the performance of total disentanglement.","First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?''","Hofstadter (1985).","Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation."],"url":"http://arxiv.org/abs/2403.12784v1","category":"cs.CV"}
{"created":"2024-03-19 14:31:59","title":"Dynamic Density Functional Theory with Inertia and Background Flow","abstract":"We present dynamic density functional theory (DDFT) incorporating general inhomogeneous, incompressible, time dependent background flows and inertia, describing externally driven passive colloidal systems out of equilibrium. We start by considering the underlying nonequilibrium Langevin dynamics, including the effect of the local velocity of the surrounding liquid bath, to obtain the nonlinear, nonlocal partial differential equations governing the evolution of the (coarse--grained) density and velocity fields describing the dynamics of colloids. Additionally, we show both with heuristic arguments, and by numerical solution, that our equations and solutions agree with existing DDFTs in the overdamped (high friction) limit. We provide numerical solutions that model the flow of hard spheres, in both unbounded and confined domains, and compare to previously--derived DDFTs with and without the background flow.","sentences":["We present dynamic density functional theory (DDFT) incorporating general inhomogeneous, incompressible, time dependent background flows and inertia, describing externally driven passive colloidal systems out of equilibrium.","We start by considering the underlying nonequilibrium Langevin dynamics, including the effect of the local velocity of the surrounding liquid bath, to obtain the nonlinear, nonlocal partial differential equations governing the evolution of the (coarse--grained) density and velocity fields describing the dynamics of colloids.","Additionally, we show both with heuristic arguments, and by numerical solution, that our equations and solutions agree with existing DDFTs in the overdamped (high friction) limit.","We provide numerical solutions that model the flow of hard spheres, in both unbounded and confined domains, and compare to previously--derived DDFTs with and without the background flow."],"url":"http://arxiv.org/abs/2403.12765v1","category":"cond-mat.soft"}
{"created":"2024-03-19 14:30:13","title":"A smoothing effect for the fractional Schr\u00f6dinger equations on the circle and observability","abstract":"We show that, after a renormalisation, one can define the square of the modulus of the solution of the fractional Schr\\\"odinger equations on the circle with data in Sobolev spaces of arbitrary negative index. As an application, we obtain observability estimates with rough controls.","sentences":["We show that, after a renormalisation, one can define the square of the modulus of the solution of the fractional Schr\\\"odinger equations on the circle with data in Sobolev spaces of arbitrary negative index.","As an application, we obtain observability estimates with rough controls."],"url":"http://arxiv.org/abs/2403.12763v1","category":"math.AP"}
{"created":"2024-03-19 14:29:57","title":"Smooth helically symmetric transonic flows with nonzero vorticity in a concentric cylinder","abstract":"This paper concerns the structural stability of smooth cylindrical symmetric transonic flows in a concentric cylinder under helically symmetric perturbation of suitable boundary conditions. The deformation-curl decomposition developed by the second author and his collaborator is utilized to effectively decouple the elliptic-hyperbolic mixed structure in the steady compressible Euler equation. A key parameter in the helical symmetry is the step (denoted by $\\sigma$), which denotes the magnitude of the translation along the symmetry axis after rotating one full turn. It is shown that the step determines the type of the first order partial differential system satisfied by the radial and vertical velocity. There exists a critical number $\\sigma_{*}$ depending only on the background transonic flows, such that if $0<\\sigma<\\sigma_{*}$, one can prove the existence and uniqueness of smooth helically symmetric transonic flows with nonzero vorticity.","sentences":["This paper concerns the structural stability of smooth cylindrical symmetric transonic flows in a concentric cylinder under helically symmetric perturbation of suitable boundary conditions.","The deformation-curl decomposition developed by the second author and his collaborator is utilized to effectively decouple the elliptic-hyperbolic mixed structure in the steady compressible Euler equation.","A key parameter in the helical symmetry is the step (denoted by $\\sigma$), which denotes the magnitude of the translation along the symmetry axis after rotating one full turn.","It is shown that the step determines the type of the first order partial differential system satisfied by the radial and vertical velocity.","There exists a critical number $\\sigma_{*}$ depending only on the background transonic flows, such that if $0<\\sigma<\\sigma_{*}$, one can prove the existence and uniqueness of smooth helically symmetric transonic flows with nonzero vorticity."],"url":"http://arxiv.org/abs/2403.12762v1","category":"math.AP"}
{"created":"2024-03-19 14:00:15","title":"Controllability and diffeomorphism groups on manifolds with boundary","abstract":"In this article we consider diffeomorphism groups of manifolds with smooth boundary. We show that the diffeomorphism groups of the manifold and its boundary fit into a short exact sequence which admits local sections. In other words, they form an infinite-dimensional fibre bundle. Manifolds with boundary are of interest in numerical analysis and with a view towards applications in machine learning we establish controllability results for families of vector fields. This generalises older results due to Agrachev and Caponigro in the boundary-less case. Our results show in particular that the diffeomorphism group of a manifold with smooth boundary is generated by the image of the exponential map.","sentences":["In this article we consider diffeomorphism groups of manifolds with smooth boundary.","We show that the diffeomorphism groups of the manifold and its boundary fit into a short exact sequence which admits local sections.","In other words, they form an infinite-dimensional fibre bundle.","Manifolds with boundary are of interest in numerical analysis and with a view towards applications in machine learning we establish controllability results for families of vector fields.","This generalises older results due to Agrachev and Caponigro in the boundary-less case.","Our results show in particular that the diffeomorphism group of a manifold with smooth boundary is generated by the image of the exponential map."],"url":"http://arxiv.org/abs/2403.12742v1","category":"math.DG"}
{"created":"2024-03-19 13:59:09","title":"Two-level systems and harmonic excitations in a mean-field anharmonic quantum glass","abstract":"Structural glasses display at low temperature a set of anomalies in thermodynamic observables. The prominent example is the linear-in-temperature scaling of the specific heat, at odds with the Debye cubic scaling found in crystals, due to acoustic phonons. Such an excess of specific heat in amorphous solids is thought of arising from phenomenological soft excitations dubbed tunneling two-level systems (TTLS). Their nature as well as their statistical properties remain elusive from a first-principle viewpoint. In this work we investigate the canonically quantized version of the KHGPS model, a mean-field glass model of coupled anharmonic oscillators, across its phase diagram, with an emphasis on the specific heat. The thermodynamics is solved in a semiclassical expansion. We show that in the replica-symmetric region of the model, up to the marginal glass transition line where replica symmetry gets continuously broken, a disordered version of the Debye approximation holds: the specific heat is dominated by harmonic vibrational excitations inducing a power-law scaling at the transition, ruled by random matrix theory. This mechanism generalizes a previous semiclassical argument in the literature. We then study the marginal glass phase where the semiclassical expansion becomes non-perturbative due to the emergence of instantons that overcome disordered Debye behavior. Inside the glass phase, a variational solution to the instanton approach provides the prevailing excitations as TTLS, which generate a linear specific heat. This phase thus hosts a mix of TTLS and harmonic excitations generated by interactions. We finally suggest to go beyond the variational approximation through an analogy with the spin-boson model.","sentences":["Structural glasses display at low temperature a set of anomalies in thermodynamic observables.","The prominent example is the linear-in-temperature scaling of the specific heat, at odds with the Debye cubic scaling found in crystals, due to acoustic phonons.","Such an excess of specific heat in amorphous solids is thought of arising from phenomenological soft excitations dubbed tunneling two-level systems (TTLS).","Their nature as well as their statistical properties remain elusive from a first-principle viewpoint.","In this work we investigate the canonically quantized version of the KHGPS model, a mean-field glass model of coupled anharmonic oscillators, across its phase diagram, with an emphasis on the specific heat.","The thermodynamics is solved in a semiclassical expansion.","We show that in the replica-symmetric region of the model, up to the marginal glass transition line where replica symmetry gets continuously broken, a disordered version of the Debye approximation holds: the specific heat is dominated by harmonic vibrational excitations inducing a power-law scaling at the transition, ruled by random matrix theory.","This mechanism generalizes a previous semiclassical argument in the literature.","We then study the marginal glass phase where the semiclassical expansion becomes non-perturbative due to the emergence of instantons that overcome disordered Debye behavior.","Inside the glass phase, a variational solution to the instanton approach provides the prevailing excitations as TTLS, which generate a linear specific heat.","This phase thus hosts a mix of TTLS and harmonic excitations generated by interactions.","We finally suggest to go beyond the variational approximation through an analogy with the spin-boson model."],"url":"http://arxiv.org/abs/2403.12740v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-19 13:56:34","title":"Path Planning in a dynamic environment using Spherical Particle Swarm Optimization","abstract":"Efficiently planning an Unmanned Aerial Vehicle (UAV) path is crucial, especially in dynamic settings where potential threats are prevalent. A Dynamic Path Planner (DPP) for UAV using the Spherical Vector-based Particle Swarm Optimisation (SPSO) technique is proposed in this study. The UAV is supposed to go from a starting point to an end point through an optimal path according to some flight criteria. Path length, Safety, Attitude and Path Smoothness are all taken into account upon deciding how an optimal path should be. The path is constructed as a set of way-points that stands as re-planning checkpoints. At each path way-point, threats are allowed some constrained random motion, where their exact positions are updated and fed to the SPSO-solver. Four test scenarios are carried out using real digital elevation models. Each test gives different priorities to path length and safety, in order to show how well the SPSO-DPP is capable of generating a safe yet efficient path segments. Finally, a comparison is made to reveal the persistent overall superior performance of the SPSO, in a dynamic environment, over both the Particle Swarm Optimisation (PSO) and the Genetic Algorithm (GA). The methods are compared directly, by averaging costs over multiple runs, and by considering different challenging levels of obstacle motion. SPSO outperformed both PSO and GA, showcasing cost reductions ranging from 330\\% to 675\\% compared to both algorithms.","sentences":["Efficiently planning an Unmanned Aerial Vehicle (UAV) path is crucial, especially in dynamic settings where potential threats are prevalent.","A Dynamic Path Planner (DPP) for UAV using the Spherical Vector-based Particle Swarm Optimisation (SPSO) technique is proposed in this study.","The UAV is supposed to go from a starting point to an end point through an optimal path according to some flight criteria.","Path length, Safety, Attitude and Path Smoothness are all taken into account upon deciding how an optimal path should be.","The path is constructed as a set of way-points that stands as re-planning checkpoints.","At each path way-point, threats are allowed some constrained random motion, where their exact positions are updated and fed to the SPSO-solver.","Four test scenarios are carried out using real digital elevation models.","Each test gives different priorities to path length and safety, in order to show how well the SPSO-DPP is capable of generating a safe yet efficient path segments.","Finally, a comparison is made to reveal the persistent overall superior performance of the SPSO, in a dynamic environment, over both the Particle Swarm Optimisation (PSO) and the Genetic Algorithm (GA).","The methods are compared directly, by averaging costs over multiple runs, and by considering different challenging levels of obstacle motion.","SPSO outperformed both PSO and GA, showcasing cost reductions ranging from 330\\% to 675\\% compared to both algorithms."],"url":"http://arxiv.org/abs/2403.12739v1","category":"cs.NE"}
{"created":"2024-03-19 12:57:29","title":"A second-order iterative time integration scheme for linear poroelasticity","abstract":"We propose a novel time stepping method for linear poroelasticity by extending a recent iterative decoupling approach to the second-order case. This results in a two-step scheme with an inner iteration and a relaxation step. We prove second-order convergence for a prescribed number of inner iteration steps, only depending on the coupling strength of the elastic and the flow equation. The efficiency of the scheme is illustrated by a number of numerical experiments, including a simulation of three-dimensional brain tissue.","sentences":["We propose a novel time stepping method for linear poroelasticity by extending a recent iterative decoupling approach to the second-order case.","This results in a two-step scheme with an inner iteration and a relaxation step.","We prove second-order convergence for a prescribed number of inner iteration steps, only depending on the coupling strength of the elastic and the flow equation.","The efficiency of the scheme is illustrated by a number of numerical experiments, including a simulation of three-dimensional brain tissue."],"url":"http://arxiv.org/abs/2403.12699v1","category":"math.NA"}
{"created":"2024-03-19 12:55:18","title":"Optimal estimate of electromagnetic field concentration between two nearly-touching inclusions in the quasi-static regime","abstract":"We investigate the electromagnetic field concentration between two nearly-touching inclusions that possess high-contrast electric permittivities in the quasi-static regime. By using layer potential techniques and asymptotic analysis in the low-frequency regime, we derive low-frequency expansions that provide integral representations for the solutions of the Maxwell equations. For the leading-order term $\\bE_0$ of the asymptotic expansion of the electric field, we prove that it has the blow up order of $\\epsilon^{-1} |\\ln \\epsilon|^{-1}$ within the radial geometry, where $\\epsilon$ signifies the asymptotic distance between the inclusions. By delicate analysis of the integral operators involved, we further prove the boundedness of the first-order term $\\bE_1$. We also conduct extensive numerical experiments which not only corroborate the theoretical findings but also provide more discoveries on the field concentration in the general geometric setup. Our study provides the first treatment in the literature on field concentration between nearly-touching material inclusions for the full Maxwell system.","sentences":["We investigate the electromagnetic field concentration between two nearly-touching inclusions that possess high-contrast electric permittivities in the quasi-static regime.","By using layer potential techniques and asymptotic analysis in the low-frequency regime, we derive low-frequency expansions that provide integral representations for the solutions of the Maxwell equations.","For the leading-order term $\\bE_0$ of the asymptotic expansion of the electric field, we prove that it has the blow up order of $\\epsilon^{-1} |\\ln \\epsilon|^{-1}$ within the radial geometry, where $\\epsilon$ signifies the asymptotic distance between the inclusions.","By delicate analysis of the integral operators involved, we further prove the boundedness of the first-order term $\\bE_1$. We also conduct extensive numerical experiments which not only corroborate the theoretical findings but also provide more discoveries on the field concentration in the general geometric setup.","Our study provides the first treatment in the literature on field concentration between nearly-touching material inclusions for the full Maxwell system."],"url":"http://arxiv.org/abs/2403.12697v1","category":"math.AP"}
{"created":"2024-03-19 12:49:09","title":"LNPT: Label-free Network Pruning and Training","abstract":"Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online guidance for network pruning and learning on smart devices with unlabeled data. Our results demonstrate the superiority of this approach over supervised training.","sentences":["Pruning before training enables the deployment of neural networks on smart devices.","By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices.","It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance.","However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance.","In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization.","Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance.","We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online guidance for network pruning and learning on smart devices with unlabeled data.","Our results demonstrate the superiority of this approach over supervised training."],"url":"http://arxiv.org/abs/2403.12690v2","category":"cs.LG"}
{"created":"2024-03-19 12:47:43","title":"SEVEN: Pruning Transformer Model by Reserving Sentinels","abstract":"Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved by SEVEN. Extensive experiments on various TM in natural language, question-answering, and image classification domains are conducted to validate the effectiveness of SEVEN. The results demonstrate significant improvements of SEVEN in multiple pruning scenarios and across different sparsity levels. Additionally, SEVEN exhibits robust performance under various fine-tuning strategies. The code is publicly available at https://github.com/xiaojinying/SEVEN.","sentences":["Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks.","However, their considerable parameter size restricts their applicability, particularly on mobile devices.","Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise.","This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance.","Symbolic Descent (SD) is a general approach for training and fine-tuning TM.","In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD.","We utilize this design to dynamically assess the importance scores of weights.","SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise.","These weights are tended to be preserved by SEVEN.","Extensive experiments on various TM in natural language, question-answering, and image classification domains are conducted to validate the effectiveness of SEVEN.","The results demonstrate significant improvements of SEVEN in multiple pruning scenarios and across different sparsity levels.","Additionally, SEVEN exhibits robust performance under various fine-tuning strategies.","The code is publicly available at https://github.com/xiaojinying/SEVEN."],"url":"http://arxiv.org/abs/2403.12688v1","category":"cs.LG"}
{"created":"2024-03-19 12:36:51","title":"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single image and a NeRF model","abstract":"We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation. IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model. From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis. The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle. We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image. Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess.","sentences":["We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation.","IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution.","IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model.","From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis.","The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle.","We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image.","Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess."],"url":"http://arxiv.org/abs/2403.12682v1","category":"cs.CV"}
{"created":"2024-03-19 11:56:15","title":"Renormalization of networks with weak geometric coupling","abstract":"The Renormalization Group is crucial for understanding systems across scales, including complex networks. Renormalizing networks via network geometry, a framework in which their topology is based on the location of nodes in a hidden metric space, is one of the foundational approaches. However, the current methods assume that the geometric coupling is strong, neglecting weak coupling in many real networks. This paper extends renormalization to weak geometric coupling, showing that geometric information is essential to preserve self-similarity. Our results underline the importance of geometric effects on network topology even when the coupling to the underlying space is weak.","sentences":["The Renormalization Group is crucial for understanding systems across scales, including complex networks.","Renormalizing networks via network geometry, a framework in which their topology is based on the location of nodes in a hidden metric space, is one of the foundational approaches.","However, the current methods assume that the geometric coupling is strong, neglecting weak coupling in many real networks.","This paper extends renormalization to weak geometric coupling, showing that geometric information is essential to preserve self-similarity.","Our results underline the importance of geometric effects on network topology even when the coupling to the underlying space is weak."],"url":"http://arxiv.org/abs/2403.12663v1","category":"physics.soc-ph"}
{"created":"2024-03-19 11:50:36","title":"Reflected Brownian Motion in a wedge: sum-of-exponential absorption probability at the vertex and differential properties","abstract":"We study a Brownian motion with drift in a wedge of angle $\\beta$ which is obliquely reflected on each edge along angles $\\varepsilon$ and $\\delta$. We assume that the classical parameter $\\alpha=\\frac{\\delta+\\varepsilon - \\pi}{\\beta}$ is greater than $1$ and we focus on transient cases where the process can either be absorbed at the vertex or escape to infinity. We show that $\\alpha\\in\\mathbb{N}^*$ is a necessary and sufficient condition for the absorption probability, seen as a function of the starting point, to be written as a finite sum of terms of exponential product form. In such cases, we give expressions for the absorption probability and its Laplace transform. When $\\alpha\\in\\mathbb{Z}+\\frac{\\pi}{\\beta}\\mathbb{Z}$ we find explicit D-algebraic expression for the Laplace transform. Our results rely on Tutte's invariant method and on a recursive compensation approach.","sentences":["We study a Brownian motion with drift in a wedge of angle $\\beta$ which is obliquely reflected on each edge along angles $\\varepsilon$ and $\\delta$.","We assume that the classical parameter $\\alpha=\\frac{\\delta+\\varepsilon - \\pi}{\\beta}$ is greater than $1$ and we focus on transient cases where the process can either be absorbed at the vertex or escape to infinity.","We show that $\\alpha\\in\\mathbb{N}^*$ is a necessary and sufficient condition for the absorption probability, seen as a function of the starting point, to be written as a finite sum of terms of exponential product form.","In such cases, we give expressions for the absorption probability and its Laplace transform.","When $\\alpha\\in\\mathbb{Z}+\\frac{\\pi}{\\beta}\\mathbb{Z}$ we find explicit D-algebraic expression for the Laplace transform.","Our results rely on Tutte's invariant method and on a recursive compensation approach."],"url":"http://arxiv.org/abs/2403.12661v1","category":"math.PR"}
{"created":"2024-03-19 11:36:13","title":"Well-posedness of the stochastic thin-film equation with an interface potential","abstract":"We consider strictly positive solutions to a class of fourth-order conservative quasilinear SPDEs on the $d$-dimensional torus modeled after the stochastic thin-film equation. Using recent results on quasilinear stochastic evolution equations, we show local well-posedness, blow-up criteria and instantaneous regularization in suitable function spaces under corresponding smoothness conditions on the noise. As a key ingredient, we prove stochastic maximal $L^p$-regularity estimates for thin film-type operators with measurable in time coefficients. With the aid of the above-mentioned results, we obtain global well-posedness of the stochastic thin-film equation with an interface potential by closing $\\alpha$-entropy estimates and subsequently an energy estimate in dimension one. In particular, we can treat a wide range of mobility functions including the power laws $u^n$ for $n\\in [0,6)$ as long as the interface potential is sufficiently repulsive.","sentences":["We consider strictly positive solutions to a class of fourth-order conservative quasilinear SPDEs on the $d$-dimensional torus modeled after the stochastic thin-film equation.","Using recent results on quasilinear stochastic evolution equations, we show local well-posedness, blow-up criteria and instantaneous regularization in suitable function spaces under corresponding smoothness conditions on the noise.","As a key ingredient, we prove stochastic maximal $L^p$-regularity estimates for thin film-type operators with measurable in time coefficients.","With the aid of the above-mentioned results, we obtain global well-posedness of the stochastic thin-film equation with an interface potential by closing $\\alpha$-entropy estimates and subsequently an energy estimate in dimension one.","In particular, we can treat a wide range of mobility functions including the power laws $u^n$ for $n\\in [0,6)$ as long as the interface potential is sufficiently repulsive."],"url":"http://arxiv.org/abs/2403.12652v1","category":"math.AP"}
{"created":"2024-03-19 11:35:59","title":"Mean-field derivation of Landau-like equations","abstract":"We derive a class of space homogeneous Landau-like equations from stochastic interacting particles. Through the use of relative entropy, we obtain quantitative bounds on the distance between the solution of the N-particle Liouville equation and the tensorised solution of the limiting Landau-like equation.","sentences":["We derive a class of space homogeneous Landau-like equations from stochastic interacting particles.","Through the use of relative entropy, we obtain quantitative bounds on the distance between the solution of the N-particle Liouville equation and the tensorised solution of the limiting Landau-like equation."],"url":"http://arxiv.org/abs/2403.12651v1","category":"math.AP"}
{"created":"2024-03-19 11:16:14","title":"A Practical Guide to Statistical Distances for Evaluating Generative Models in Science","abstract":"Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision making and a model generating medical images. We showcase that distinct distances can give different results on similar data. Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science.","sentences":["Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes.","How do we evaluate the samples these models generate?","This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics.","We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\\'echet Inception Distance; FID).","We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls.","To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision making and a model generating medical images.","We showcase that distinct distances can give different results on similar data.","Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science."],"url":"http://arxiv.org/abs/2403.12636v1","category":"cs.LG"}
{"created":"2024-03-19 10:53:34","title":"On an integrable class of Chebyshev nets","abstract":"We consider integrable curve nets in Euclidean space as a particular integrable geometry invariant with respect to rigid motions and net-preserving reparameterisations. For the purpose of their description, we first give an overview of the most important second-order invariants and relations among them. As a particular integrable example, we study curve nets satisfying an $\\mathbb R$-linear relation between the Schief curvature of the net and the Gauss curvature of the supporting surface. Starting with an $\\mathfrak{so}(3)$-valued zero-curvature representation, associated with an elliptic spectral curve, we reveal two cases when the curve degenerates. In one of these cases, when the curvatures are proportional (concordant nets), we find a correspondence to pairs of pseudospherical surfaces of equal negative constant Gaussian curvatures. The construction generalises the well-known correspondence between translation surfaces and pairs of curves.","sentences":["We consider integrable curve nets in Euclidean space as a particular integrable geometry invariant with respect to rigid motions and net-preserving reparameterisations.","For the purpose of their description, we first give an overview of the most important second-order invariants and relations among them.","As a particular integrable example, we study curve nets satisfying an $\\mathbb R$-linear relation between the Schief curvature of the net and the Gauss curvature of the supporting surface.","Starting with an $\\mathfrak{so}(3)$-valued zero-curvature representation, associated with an elliptic spectral curve, we reveal two cases when the curve degenerates.","In one of these cases, when the curvatures are proportional (concordant nets), we find a correspondence to pairs of pseudospherical surfaces of equal negative constant Gaussian curvatures.","The construction generalises the well-known correspondence between translation surfaces and pairs of curves."],"url":"http://arxiv.org/abs/2403.12626v1","category":"math.DG"}
{"created":"2024-03-19 10:52:12","title":"Existence, uniqueness and characterisation of local minimisers in higher order Calculus of Variations in $\\mathrm L^{\\infty}$","abstract":"We study variational problems for second order supremal functionals $\\mathrm F_\\infty(u)= \\|F(\\cdot,u,\\mathrm D u,\\mathrm{A}\\!:\\!\\mathrm D^2u)\\|_{\\mathrm L^{\\infty}(\\Omega)}$, where $F$ satisfies certain natural assumptions, $\\mathrm A$ is a positive matrix, and $\\Omega \\Subset \\mathbb R^n$. Higher order problems are very novel in the Calculus of Variations in $\\mathrm L^{\\infty}$, and exhibit a strikingly different behaviour compared to first order problems, for which there exists an established theory, pioneered by Aronsson in 1960s. The aim of this paper is to develop a complete theory for $\\mathrm F_\\infty$. We prove that, under appropriate conditions, ``localised\" minimisers can be characterised as solutions to a nonlinear system of PDEs, which is different from the corresponding Aronsson equation for $\\mathrm F_\\infty$; the latter is only a necessary, but not a sufficient condition for minimality. We also establish the existence and uniqueness of localised minimisers subject to Dirichlet conditions on $\\partial \\Omega$, and also their partial regularity outside a singular set of codimension one, which may be non-empty even if $n=1$.","sentences":["We study variational problems for second order supremal functionals $\\mathrm F_\\infty(u)= \\|F(\\cdot,u,\\mathrm D u,\\mathrm{A}\\!:\\!\\mathrm D^2u)\\|_{\\mathrm L^{\\infty}(\\Omega)}$, where $F$ satisfies certain natural assumptions, $\\mathrm A$ is a positive matrix, and $\\Omega \\Subset \\mathbb R^n$. Higher order problems are very novel in the Calculus of Variations in $\\mathrm L^{\\infty}$, and exhibit a strikingly different behaviour compared to first order problems, for which there exists an established theory, pioneered by Aronsson in 1960s.","The aim of this paper is to develop a complete theory for $\\mathrm F_\\infty$. We prove that, under appropriate conditions, ``localised\" minimisers can be characterised as solutions to a nonlinear system of PDEs, which is different from the corresponding Aronsson equation for $\\mathrm F_\\infty$; the latter is only a necessary, but not a sufficient condition for minimality.","We also establish the existence and uniqueness of localised minimisers subject to Dirichlet conditions on $\\partial \\Omega$, and also their partial regularity outside a singular set of codimension one, which may be non-empty even if $n=1$."],"url":"http://arxiv.org/abs/2403.12625v1","category":"math.AP"}
{"created":"2024-03-19 10:46:44","title":"Phase Transition and Thermodynamic Stability in an Entropy-driven Universe","abstract":"Motivated by the notion that the mathematics of gravity can be reproduced from a statistical requirement of maximal entropy, we study the consequence of introducing an entropic source term in the Einstein-Hilbert action. For a spatially homogeneous cosmological system driven by this entropic source and enveloped by a time evolving apparent horizon, we formulate a modified version of the second law of thermodynamics. An explicit differential equation governing the internal entropy profile is found. Using a Hessian matrix analysis of the internal entropy we check the thermodynamic stability for a {\\Lambda}CDM cosmology, a unified cosmic expanson and a non-singular ekpyrotic bounce. We find the mathematical condition for a second order phase transition during these evolutions from the divergence of specific heat at constant volume. The condition is purely kinematic and quadratic in nature, relating the deceleration parameter and the jerk parameter that chalks out an interesting curve on the parameter space. This condition is valid even without the entropic source term and may be a general property of any phase transition.","sentences":["Motivated by the notion that the mathematics of gravity can be reproduced from a statistical requirement of maximal entropy, we study the consequence of introducing an entropic source term in the Einstein-Hilbert action.","For a spatially homogeneous cosmological system driven by this entropic source and enveloped by a time evolving apparent horizon, we formulate a modified version of the second law of thermodynamics.","An explicit differential equation governing the internal entropy profile is found.","Using a Hessian matrix analysis of the internal entropy we check the thermodynamic stability for a {\\Lambda}CDM cosmology, a unified cosmic expanson and a non-singular ekpyrotic bounce.","We find the mathematical condition for a second order phase transition during these evolutions from the divergence of specific heat at constant volume.","The condition is purely kinematic and quadratic in nature, relating the deceleration parameter and the jerk parameter that chalks out an interesting curve on the parameter space.","This condition is valid even without the entropic source term and may be a general property of any phase transition."],"url":"http://arxiv.org/abs/2403.12622v1","category":"gr-qc"}
{"created":"2024-03-19 10:34:39","title":"Quantitative homogenization of the compressible Navier-Stokes equations towards Darcy's law","abstract":"We consider the solutions $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ to the compressible Navier-Stokes equations (NSE) in a domain periodically perforated by holes of diameter $\\varepsilon>0$. We focus on the case where the diameter of the holes is of the same order as the distance between neighboring holes. This is the same setting investigated in the paper by Masmoudi [\\url{http://www.numdam.org/article/COCV_2002__8__885_0.pdf}], where convergence $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ of the system to the porous medium equation has been shown. We prove a quantitative version of this convergence result provided that the solution of the limiting system is sufficiently regular. The proof builds on the relative energy inequality satisfied by the compressible NSE.","sentences":["We consider the solutions $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ to the compressible Navier-Stokes equations (NSE) in a domain periodically perforated by holes of diameter $\\varepsilon>0$. We focus on the case where the diameter of the holes is of the same order as the distance between neighboring holes.","This is the same setting investigated in the paper by Masmoudi [\\url{http://www.numdam.org/article/COCV_2002__8__885_0.pdf}], where convergence $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ of the system to the porous medium equation has been shown.","We prove a quantitative version of this convergence result provided that the solution of the limiting system is sufficiently regular.","The proof builds on the relative energy inequality satisfied by the compressible NSE."],"url":"http://arxiv.org/abs/2403.12616v1","category":"math.AP"}
{"created":"2024-03-19 10:26:01","title":"Fixing a Minor Mistake in the Theory of Stochastic Integration and Differential Equations","abstract":"When considering stochastic integration and the theory of stochastic differential equations, P. Protter's textbook \\cite{protter} undoubtedly is a main piece of standard literature. Not only is it well-written, but it also contains various profound results regarding these fields. Unfortunately, Theorem 12 of Chapter V, which presents an equivalence to uniform convergence on compacts in probability, is found to be incorrect. Given that numerous important results rely on this theorem, this paper aims to present a corrected version of it.","sentences":["When considering stochastic integration and the theory of stochastic differential equations, P. Protter's textbook \\cite{protter} undoubtedly is a main piece of standard literature.","Not only is it well-written, but it also contains various profound results regarding these fields.","Unfortunately, Theorem 12 of Chapter V, which presents an equivalence to uniform convergence on compacts in probability, is found to be incorrect.","Given that numerous important results rely on this theorem, this paper aims to present a corrected version of it."],"url":"http://arxiv.org/abs/2403.12613v1","category":"math.PR"}
{"created":"2024-03-19 10:24:15","title":"SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition","abstract":"As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.","sentences":["As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades.","While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data.","This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem.","We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively.","We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN).","We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol."],"url":"http://arxiv.org/abs/2403.12609v1","category":"cs.LG"}
{"created":"2024-03-19 10:17:26","title":"On the Effectiveness of Heterogeneous Ensemble Methods for Re-identification","abstract":"In this contribution, we introduce a novel ensemble method for the re-identification of industrial entities, using images of chipwood pallets and galvanized metal plates as dataset examples. Our algorithms replace commonly used, complex siamese neural networks with an ensemble of simplified, rudimentary models, providing wider applicability, especially in hardware-restricted scenarios. Each ensemble sub-model uses different types of extracted features of the given data as its input, allowing for the creation of effective ensembles in a fraction of the training duration needed for more complex state-of-the-art models. We reach state-of-the-art performance at our task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%, and introduce five distinct feature extraction approaches, and study their combination using different ensemble methods.","sentences":["In this contribution, we introduce a novel ensemble method for the re-identification of industrial entities, using images of chipwood pallets and galvanized metal plates as dataset examples.","Our algorithms replace commonly used, complex siamese neural networks with an ensemble of simplified, rudimentary models, providing wider applicability, especially in hardware-restricted scenarios.","Each ensemble sub-model uses different types of extracted features of the given data as its input, allowing for the creation of effective ensembles in a fraction of the training duration needed for more complex state-of-the-art models.","We reach state-of-the-art performance at our task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%, and introduce five distinct feature extraction approaches, and study their combination using different ensemble methods."],"url":"http://arxiv.org/abs/2403.12606v1","category":"cs.LG"}
{"created":"2024-03-19 10:08:14","title":"Effects of forward disorder on quasi-1D superconductors","abstract":"We study the competition between disorder and singlet superconductivity in a quasi-1d system. We investigate the applicability of the Anderson theorem, namely that time-reversal conserving (non-magnetic) disorder does not impact the critical temperature, by opposition to time-reversal breaking disorder (magnetic). To do so we examine a quasi-1d system of spin 1/2 fermions with attractive interactions and forward scattering disorder using field theory (bosonization). By computing the superconducting critical temperature ($T_c$), we find that for non-magnetic disorder the Anderson theorem also holds in the quasi-1D geometry. On the contrary, magnetic disorder has an impact on the critical temperature, that we investigate by deriving renormalization group (RG) equations describing the competition between the disorder and the interactions. Computing the critical temperature as a function of disorder strength, we see that different regimes arise depending on the strength of interactions. We discuss possible platforms where to observe this in cold atoms and condensed matter.","sentences":["We study the competition between disorder and singlet superconductivity in a quasi-1d system.","We investigate the applicability of the Anderson theorem, namely that time-reversal conserving (non-magnetic) disorder does not impact the critical temperature, by opposition to time-reversal breaking disorder (magnetic).","To do so we examine a quasi-1d system of spin 1/2 fermions with attractive interactions and forward scattering disorder using field theory (bosonization).","By computing the superconducting critical temperature ($T_c$), we find that for non-magnetic disorder the Anderson theorem also holds in the quasi-1D geometry.","On the contrary, magnetic disorder has an impact on the critical temperature, that we investigate by deriving renormalization group (RG) equations describing the competition between the disorder and the interactions.","Computing the critical temperature as a function of disorder strength, we see that different regimes arise depending on the strength of interactions.","We discuss possible platforms where to observe this in cold atoms and condensed matter."],"url":"http://arxiv.org/abs/2403.12597v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-19 10:01:25","title":"Ensuring Solution Uniqueness in Fixed-Point-Based Harmonic Power Flow Analysis with Converter-Interfaced Resources: Ex-post Conditions","abstract":"Recently, the authors of this paper proposed a method for the Harmonic Power-Flow (HPF) calculus in polyphase grids with widespread deployment of Converter-Interfaced Distributed Energy Resources (CIDERs). The HPF problem was formulated by integrating the hybrid nodal equations of the grid with a detailed representation of the CIDERs hardware, sensing, and controls as Linear Time-Periodic (LTP) systems, and solving the resulting mismatch equations using the Newton-Raphson (NR) method. This work introduces a novel problem formulation based on the fixed-point algorithm that, combined with the contraction property of the HPF problem, provides insights into the uniqueness of its solution. Notably, the effectiveness of the fixed-point formulation and the uniqueness of the solution are evaluated through numerical analyses conducted on a modified version of the CIGRE low-voltage benchmark microgrid.","sentences":["Recently, the authors of this paper proposed a method for the Harmonic Power-Flow (HPF) calculus in polyphase grids with widespread deployment of Converter-Interfaced Distributed Energy Resources (CIDERs).","The HPF problem was formulated by integrating the hybrid nodal equations of the grid with a detailed representation of the CIDERs hardware, sensing, and controls as Linear Time-Periodic (LTP) systems, and solving the resulting mismatch equations using the Newton-Raphson (NR) method.","This work introduces a novel problem formulation based on the fixed-point algorithm that, combined with the contraction property of the HPF problem, provides insights into the uniqueness of its solution.","Notably, the effectiveness of the fixed-point formulation and the uniqueness of the solution are evaluated through numerical analyses conducted on a modified version of the CIGRE low-voltage benchmark microgrid."],"url":"http://arxiv.org/abs/2403.12595v1","category":"eess.SY"}
{"created":"2024-03-19 09:21:43","title":"NN-ETM: Enabling safe neural network-based event-triggering mechanisms for consensus problems","abstract":"In networked control applications, event-triggering mechanisms (ETMs) reduce the communication load while ensuring performance guarantees. However, the design of ETMs is becoming increasingly complex, particularly for decentralized multi-agent and consensus setups, where the condition used to trigger communication might incorporate the agent's local information and the information received from neighbors. This typically results in ad-hoc solutions, which may only work for the consensus protocols under consideration. In this work, we aim to safely incorporate neural networks in the ETM to provide a general and flexible solution while ensuring guaranteed performance. To decouple the stability analysis of the consensus protocol from the abstraction of the neural network in the ETM, we first derive design criteria for the consensus and ETM pair, which allow independent analysis of each element under mild constraints. As a result, we propose NN-ETM, a novel ETM featuring a neural network, which provides an all-purpose solution to optimize communication in consensus problems while preserving the stability guarantees of the consensus protocol.","sentences":["In networked control applications, event-triggering mechanisms (ETMs) reduce the communication load while ensuring performance guarantees.","However, the design of ETMs is becoming increasingly complex, particularly for decentralized multi-agent and consensus setups, where the condition used to trigger communication might incorporate the agent's local information and the information received from neighbors.","This typically results in ad-hoc solutions, which may only work for the consensus protocols under consideration.","In this work, we aim to safely incorporate neural networks in the ETM to provide a general and flexible solution while ensuring guaranteed performance.","To decouple the stability analysis of the consensus protocol from the abstraction of the neural network in the ETM, we first derive design criteria for the consensus and ETM pair, which allow independent analysis of each element under mild constraints.","As a result, we propose NN-ETM, a novel ETM featuring a neural network, which provides an all-purpose solution to optimize communication in consensus problems while preserving the stability guarantees of the consensus protocol."],"url":"http://arxiv.org/abs/2403.12567v1","category":"eess.SY"}
{"created":"2024-03-19 09:15:20","title":"A Bayesian multilevel hidden Markov model with Poisson-lognormal emissions for intense longitudinal count data","abstract":"Hidden Markov models (HMMs) are probabilistic methods in which observations are seen as realizations of a latent Markov process with discrete states that switch over time. Moving beyond standard statistical tests, HMMs offer a statistical environment to optimally exploit the information present in multivariate time series, uncovering the latent dynamics that rule them. Here, we extend the Poisson HMM to the multilevel framework, accommodating variability between individuals with continuously distributed individual random effects following a lognormal distribution, and describe how to estimate the model in a fully parametric Bayesian framework. The proposed multilevel HMM enables probabilistic decoding of hidden state sequences from multivariate count time-series based on individual-specific parameters, and offers a framework to quantificate between-individual variability formally. Through a Monte Carlo study we show that the multilevel HMM outperforms the HMM for scenarios involving heterogeneity between individuals, demonstrating improved decoding accuracy and estimation performance of parameters of the emission distribution, and performs equally well when not between heterogeneity is present. Finally, we illustrate how to use our model to explore the latent dynamics governing complex multivariate count data in an empirical application concerning pilot whale diving behaviour in the wild, and how to identify neural states from multi-electrode recordings of motor neural cortex activity in a macaque monkey in an experimental set up. We make the multilevel HMM introduced in this study publicly available in the R-package mHMMbayes in CRAN.","sentences":["Hidden Markov models (HMMs) are probabilistic methods in which observations are seen as realizations of a latent Markov process with discrete states that switch over time.","Moving beyond standard statistical tests, HMMs offer a statistical environment to optimally exploit the information present in multivariate time series, uncovering the latent dynamics that rule them.","Here, we extend the Poisson HMM to the multilevel framework, accommodating variability between individuals with continuously distributed individual random effects following a lognormal distribution, and describe how to estimate the model in a fully parametric Bayesian framework.","The proposed multilevel HMM enables probabilistic decoding of hidden state sequences from multivariate count time-series based on individual-specific parameters, and offers a framework to quantificate between-individual variability formally.","Through a Monte Carlo study we show that the multilevel HMM outperforms the HMM for scenarios involving heterogeneity between individuals, demonstrating improved decoding accuracy and estimation performance of parameters of the emission distribution, and performs equally well when not between heterogeneity is present.","Finally, we illustrate how to use our model to explore the latent dynamics governing complex multivariate count data in an empirical application concerning pilot whale diving behaviour in the wild, and how to identify neural states from multi-electrode recordings of motor neural cortex activity in a macaque monkey in an experimental set up.","We make the multilevel HMM introduced in this study publicly available in the R-package mHMMbayes in CRAN."],"url":"http://arxiv.org/abs/2403.12561v1","category":"stat.ME"}
{"created":"2024-03-19 17:59:56","title":"LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression","abstract":"This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.   We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.","sentences":["This paper focuses on task-agnostic prompt compression for better generalizability and efficiency.","Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   ","To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset.","We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context.","Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.   ","We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH.","Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs.","Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x."],"url":"http://arxiv.org/abs/2403.12968v1","category":"cs.CL"}
{"created":"2024-03-19 17:57:52","title":"GVGEN: Text-to-3D Generation with Volumetric Representation","abstract":"In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency.","sentences":["In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities.","To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input.","We propose two innovative techniques:(1) Structured Volumetric Representation.","We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume.","This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians.","To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization.","(2) Coarse-to-fine Generation Pipeline.","To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline.","It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes.","Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods.","Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency."],"url":"http://arxiv.org/abs/2403.12957v1","category":"cs.CV"}
{"created":"2024-03-19 17:50:32","title":"On Safety in Safe Bayesian Optimization","abstract":"Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage. To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes. Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems. To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety.","sentences":["Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this.","Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world.","In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms.","First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees.","We provide a detailed analysis of this problem and introduce Real-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees.","Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage.","To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes.","Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems.","To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety."],"url":"http://arxiv.org/abs/2403.12948v1","category":"cs.LG"}
{"created":"2024-03-19 17:28:48","title":"Solving Combinatorial Pricing Problems using Embedded Dynamic Programming Models","abstract":"The combinatorial pricing problem (CPP) is a bilevel problem in which the leader maximizes their revenue by imposing tolls on certain items that they can control. Based on the tolls set by the leader, the follower selects a subset of items corresponding to an optimal solution of a combinatorial optimization problem. To accomplish the leader's goal, the tolls need to be sufficiently low to discourage the follower from choosing the items offered by the competitors. In this paper, we derive a single-level reformulation for the CPP by rewriting the follower's problem as a longest path problem using a dynamic programming model, and then taking its dual and applying strong duality. We proceed to solve the reformulation in a dynamic fashion with a cutting plane method. We apply this methodology to 2 distinct dynamic programming models, namely, a novel formulation designated as selection diagram and the well-known decision diagram. We also produce numerical results to evaluate their performances across 3 different specializations of the CPP and a closely related problem that is the knapsack interdiction problem. Our results showcase the potential of the 2 proposed reformulations over the natural value function approach, expanding the set of tools to solve combinatorial bilevel programs.","sentences":["The combinatorial pricing problem (CPP) is a bilevel problem in which the leader maximizes their revenue by imposing tolls on certain items that they can control.","Based on the tolls set by the leader, the follower selects a subset of items corresponding to an optimal solution of a combinatorial optimization problem.","To accomplish the leader's goal, the tolls need to be sufficiently low to discourage the follower from choosing the items offered by the competitors.","In this paper, we derive a single-level reformulation for the CPP by rewriting the follower's problem as a longest path problem using a dynamic programming model, and then taking its dual and applying strong duality.","We proceed to solve the reformulation in a dynamic fashion with a cutting plane method.","We apply this methodology to 2 distinct dynamic programming models, namely, a novel formulation designated as selection diagram and the well-known decision diagram.","We also produce numerical results to evaluate their performances across 3 different specializations of the CPP and a closely related problem that is the knapsack interdiction problem.","Our results showcase the potential of the 2 proposed reformulations over the natural value function approach, expanding the set of tools to solve combinatorial bilevel programs."],"url":"http://arxiv.org/abs/2403.12923v1","category":"math.OC"}
{"created":"2024-03-19 17:10:17","title":"Boundary Layer Estimates in Stochastic Homogenization","abstract":"We prove quantitative decay estimates for the boundary layer corrector in stochastic homogenization in the case of a half-space boundary. Our estimates are of optimal order and show that the gradient of the boundary layer corrector features nearly fluctuation-order decay; its expected value decays even one order faster. As a corollary, we deduce estimates on the accuracy of the representative volume element method for the computation of effective coefficients: our understanding of the decay of boundary layers enables us to improve the order of convergence of the RVE method for $d\\geq 3$.","sentences":["We prove quantitative decay estimates for the boundary layer corrector in stochastic homogenization in the case of a half-space boundary.","Our estimates are of optimal order and show that the gradient of the boundary layer corrector features nearly fluctuation-order decay; its expected value decays even one order faster.","As a corollary, we deduce estimates on the accuracy of the representative volume element method for the computation of effective coefficients: our understanding of the decay of boundary layers enables us to improve the order of convergence of the RVE method for $d\\geq 3$."],"url":"http://arxiv.org/abs/2403.12911v1","category":"math.AP"}
{"created":"2024-03-19 17:01:09","title":"Ziv-Zakai-Optimal OFDM Resource Allocation for Time-of-Arrival Estimation","abstract":"This paper presents methods of optimizing the placement and power allocations of pilots in an orthogonal frequency-division multiplexing (OFDM) signal to minimize time-of-arrival (TOA) estimation errors under power and resource allocation constraints. TOA errors in this optimization are quantified through the Ziv-Zakai bound (ZZB), which captures error thresholding effects caused by sidelobes in the signal's autocorrelation function (ACF) which are not captured by the Cramer-Rao lower bound. This paper is the first to solve for these ZZB-optimal allocations in the context of OFDM signals, under integer resource allocation constraints, and under both coherent and noncoherent reception. Under convex constraints, the optimization of the ZZB is proven to be convex; under integer constraints, the optimization is lower bounded by a convex relaxation and a branch-and-bound algorithm is proposed for efficiently allocating pilot resources. These allocations are evaluated by their ZZBs and ACFs, compared against a typical uniform allocation, and deployed on a software-defined radio TOA measurement platform to demonstrate their applicability in real-world systems.","sentences":["This paper presents methods of optimizing the placement and power allocations of pilots in an orthogonal frequency-division multiplexing (OFDM) signal to minimize time-of-arrival (TOA) estimation errors under power and resource allocation constraints.","TOA errors in this optimization are quantified through the Ziv-Zakai bound (ZZB), which captures error thresholding effects caused by sidelobes in the signal's autocorrelation function (ACF) which are not captured by the Cramer-Rao lower bound.","This paper is the first to solve for these ZZB-optimal allocations in the context of OFDM signals, under integer resource allocation constraints, and under both coherent and noncoherent reception.","Under convex constraints, the optimization of the ZZB is proven to be convex; under integer constraints, the optimization is lower bounded by a convex relaxation and a branch-and-bound algorithm is proposed for efficiently allocating pilot resources.","These allocations are evaluated by their ZZBs and ACFs, compared against a typical uniform allocation, and deployed on a software-defined radio TOA measurement platform to demonstrate their applicability in real-world systems."],"url":"http://arxiv.org/abs/2403.12905v1","category":"eess.SP"}
{"created":"2024-03-19 16:44:53","title":"Uoc luong kenh truyen trong he thong da robot su dung SDR","abstract":"This study focuses on developing an experimental system for estimating communication channels in a multi-robot mobile system using software-defined radio (SDR) devices. The system consists of two mobile robots programmed for two scenarios: one where the robot remains stationary and another where it follows a predefined trajectory. Communication within the system is conducted through orthogonal frequency-division multiplexing (OFDM) to mitigate the effects of multipath propagation in indoor environments. The system's performance is evaluated using the bit error rate (BER). Connections related to robot motion and communication are implemented using Raspberry Pi 3 and BladeRF x115, respectively. The least squares (LS) technique is employed to estimate the channel with a bit error rate of approximately 10^(-2).","sentences":["This study focuses on developing an experimental system for estimating communication channels in a multi-robot mobile system using software-defined radio (SDR) devices.","The system consists of two mobile robots programmed for two scenarios: one where the robot remains stationary and another where it follows a predefined trajectory.","Communication within the system is conducted through orthogonal frequency-division multiplexing (OFDM) to mitigate the effects of multipath propagation in indoor environments.","The system's performance is evaluated using the bit error rate (BER).","Connections related to robot motion and communication are implemented using Raspberry Pi 3 and BladeRF x115, respectively.","The least squares (LS) technique is employed to estimate the channel with a bit error rate of approximately 10^(-2)."],"url":"http://arxiv.org/abs/2403.12892v1","category":"cs.RO"}
{"created":"2024-03-19 16:24:54","title":"General properties of the electric Penrose process","abstract":"We consider the Penrose process with the charged particles in the Reissner-Nordstr\\\"{o}m background. Let parent particle 0 decay to particles 1 and 2. With the assumption that all three particles move in the same plane, the exact formulas for characteristics of particles 1 and 2 in terms of those of particle 0 are derived. We concentrate on scenarios in which particle 1 and 2 are ejected along the trajectory of particle 0. It is shown that such scenarios correspond to the extrema of energies $E_{1}$ or $E_{2}$ of daughter particles with respect to the angular momentum $L_{1}$ or $L_{2}$ of one of them. We derive bounds on the values of angular momenta $L_{1}$ and $L_{2}$. We give classification of these scenarios and discuss their properties including decay in the near-horizon region. The results are reformulated in terms of velocities of daughter particles in the center of mass frame. The approach is applicable also to collisional Penrose process, in which a combination of particles 1 and 2 is considered as one effective particle. If the mass of particle 0 $m_{0}\\rightarrow \\infty $, the situation corresponds to the Ba\\~{n}ados-Silk-West effect, the results agree with the ones known in literature before. In addition, we consider special cases when decay occurs in the turning point for one or all three particles.","sentences":["We consider the Penrose process with the charged particles in the Reissner-Nordstr\\\"{o}m background.","Let parent particle 0 decay to particles 1 and 2.","With the assumption that all three particles move in the same plane, the exact formulas for characteristics of particles 1 and 2 in terms of those of particle 0 are derived.","We concentrate on scenarios in which particle 1 and 2 are ejected along the trajectory of particle 0.","It is shown that such scenarios correspond to the extrema of energies $E_{1}$ or $E_{2}$ of daughter particles with respect to the angular momentum $L_{1}$ or $L_{2}$ of one of them.","We derive bounds on the values of angular momenta $L_{1}$ and $L_{2}$. We give classification of these scenarios and discuss their properties including decay in the near-horizon region.","The results are reformulated in terms of velocities of daughter particles in the center of mass frame.","The approach is applicable also to collisional Penrose process, in which a combination of particles 1 and 2 is considered as one effective particle.","If the mass of particle 0 $m_{0}\\rightarrow \\infty $, the situation corresponds to the Ba\\~{n}ados-Silk-West effect, the results agree with the ones known in literature before.","In addition, we consider special cases when decay occurs in the turning point for one or all three particles."],"url":"http://arxiv.org/abs/2403.12879v1","category":"gr-qc"}
{"created":"2024-03-19 16:21:40","title":"LAVA: Long-horizon Visual Action based Food Acquisition","abstract":"Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves. The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table. Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods. This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods. Long-horizon refers to the goal of \"clearing the bowl\" by sequentially acquiring the food from the bowl. LAVA employs a hierarchical policy for long-horizon food acquisition tasks. The framework uses high-level policy to determine primitives by leveraging ScoopNet. At the mid-level, LAVA finds parameters for primitives using vision. To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution. We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition. Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl. Code, datasets, videos, and supplementary materials can be found on our website.","sentences":["Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves.","The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table.","Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods.","This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods.","Long-horizon refers to the goal of \"clearing the bowl\" by sequentially acquiring the food from the bowl.","LAVA employs a hierarchical policy for long-horizon food acquisition tasks.","The framework uses high-level policy to determine primitives by leveraging ScoopNet.","At the mid-level, LAVA finds parameters for primitives using vision.","To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution.","We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition.","Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl.","Code, datasets, videos, and supplementary materials can be found on our website."],"url":"http://arxiv.org/abs/2403.12876v1","category":"cs.RO"}
{"created":"2024-03-19 16:17:21","title":"Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints","abstract":"We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance. The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints. The output irradiance values are transformed to focus on unknown short-term dynamics. Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably. Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features). For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model.","sentences":["We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance.","The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints.","The output irradiance values are transformed to focus on unknown short-term dynamics.","Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably.","Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features).","For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model."],"url":"http://arxiv.org/abs/2403.12873v1","category":"cs.LG"}
{"created":"2024-03-19 16:05:51","title":"D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation","abstract":"Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications. Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation. Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process. Through empirical evaluation on a public benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin. We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task.","sentences":["Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications.","Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function.","In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks.","D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset.","To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process.","In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation.","Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process.","This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process.","Through empirical evaluation on a public benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin.","We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task."],"url":"http://arxiv.org/abs/2403.12861v1","category":"cs.RO"}
{"created":"2024-03-19 16:03:03","title":"Primal Methods for Variational Inequality Problems with Functional Constraints","abstract":"Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequality problems, without necessitating any information on the optimal Lagrange multipliers. We establish a non-asymptotic convergence analysis of the algorithm for variational inequality problems with monotone operators under smooth constraints. Remarkably, our algorithms match the complexity of projection-based methods in terms of operator queries for both monotone and strongly monotone settings, while utilizing significantly cheaper oracles based on quadratic programming. Furthermore, we provide several numerical examples to evaluate the efficacy of our algorithms.","sentences":["Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research.","First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability.","However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints.","Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function.","These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers.","In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequality problems, without necessitating any information on the optimal Lagrange multipliers.","We establish a non-asymptotic convergence analysis of the algorithm for variational inequality problems with monotone operators under smooth constraints.","Remarkably, our algorithms match the complexity of projection-based methods in terms of operator queries for both monotone and strongly monotone settings, while utilizing significantly cheaper oracles based on quadratic programming.","Furthermore, we provide several numerical examples to evaluate the efficacy of our algorithms."],"url":"http://arxiv.org/abs/2403.12859v1","category":"math.OC"}
{"created":"2024-03-19 15:54:56","title":"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a Multi-Objective Genetic Algorithm","abstract":"Augmented Reality (AR) and Virtual Reality (VR) systems involve computationally intensive image processing algorithms that can burden end-devices with limited resources, leading to poor performance in providing low latency services. Edge-to-cloud computing overcomes the limitations of end-devices by offloading their computations to nearby edge devices or remote cloud servers. Although this proves to be sufficient for many applications, optimal placement of latency sensitive AR/VR services in edge-to-cloud infrastructures (to provide desirable service response times and reliability) remain a formidable challenging. To address this challenge, this paper develops a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of AR/VR-based services in multi-tier edge-to-cloud environments. The primary objective of the proposed MOGA is to minimize the response time of all running services, while maximizing the reliability of the underlying system from both software and hardware perspectives. To evaluate its performance, we mathematically modeled all components and developed a tailor-made simulator to assess its effectiveness on various scales. MOGA was compared with several heuristics to prove that intuitive solutions, which are usually assumed sufficient, are not efficient enough for the stated problem. The experimental results indicated that MOGA can significantly reduce the response time of deployed services by an average of 67\\% on different scales, compared to other heuristic methods. MOGA also ensures reliability of the 97\\% infrastructure (hardware) and 95\\% services (software).","sentences":["Augmented Reality (AR) and Virtual Reality (VR) systems involve computationally intensive image processing algorithms that can burden end-devices with limited resources, leading to poor performance in providing low latency services.","Edge-to-cloud computing overcomes the limitations of end-devices by offloading their computations to nearby edge devices or remote cloud servers.","Although this proves to be sufficient for many applications, optimal placement of latency sensitive AR/VR services in edge-to-cloud infrastructures (to provide desirable service response times and reliability) remain a formidable challenging.","To address this challenge, this paper develops a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of AR/VR-based services in multi-tier edge-to-cloud environments.","The primary objective of the proposed MOGA is to minimize the response time of all running services, while maximizing the reliability of the underlying system from both software and hardware perspectives.","To evaluate its performance, we mathematically modeled all components and developed a tailor-made simulator to assess its effectiveness on various scales.","MOGA was compared with several heuristics to prove that intuitive solutions, which are usually assumed sufficient, are not efficient enough for the stated problem.","The experimental results indicated that MOGA can significantly reduce the response time of deployed services by an average of 67\\% on different scales, compared to other heuristic methods.","MOGA also ensures reliability of the 97\\% infrastructure (hardware) and 95\\% services (software)."],"url":"http://arxiv.org/abs/2403.12849v1","category":"cs.DC"}
{"created":"2024-03-19 15:54:38","title":"Policy Bifurcation in Safe Reinforcement Learning","abstract":"Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifurcated policy, we propose a safe RL algorithm called multimodal policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output. The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient. Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy's capability of exploring different modes. Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations.","sentences":["Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems.","Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations.","We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple.","Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state.","To train such a bifurcated policy, we propose a safe RL algorithm called multimodal policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output.","The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient.","Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy's capability of exploring different modes.","Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations."],"url":"http://arxiv.org/abs/2403.12847v2","category":"cs.LG"}
{"created":"2024-03-19 15:46:18","title":"Optical properties of Euler-Heisenberg black hole in the Cold Dark Matter Halo","abstract":"The optical properties of Euler-Heisenberg (EH) black hole (BH) surrounded by Cold Dark Matter (CDM) halo are investigated. By changing BH's parameters, we found that the radius of horizon r_{h} and radius of photon sphere r_{ph} will transparently increase as CDM halo parameters R and \\rho increase. To show the influence of CDM halo on the BH's optical characteristics, we took two sets of R and \\rho with prominent differences and plot the first four orders of images for thin accretion disk with different angle of inclination \\theta of observer. The images with light intensity distributions using Novikov-Thorne (N-T) model are also derived, as well as the effective potential, photon orbits. Especially, analysis of intersection behaviors between photon trajectories with different impact parameters and circular time-like orbits in accretion disk will help better understand the image of thin accretion disk. Our results showed that CDM halo will make BH become more larger and dimmer distinctly.","sentences":["The optical properties of Euler-Heisenberg (EH) black hole (BH) surrounded by Cold Dark Matter (CDM) halo are investigated.","By changing BH's parameters, we found that the radius of horizon r_{h} and radius of photon sphere r_{ph} will transparently increase as CDM halo parameters R and \\rho increase.","To show the influence of CDM halo on the BH's optical characteristics, we took two sets of R and \\rho with prominent differences and plot the first four orders of images for thin accretion disk with different angle of inclination \\theta of observer.","The images with light intensity distributions using Novikov-Thorne (N-T) model are also derived, as well as the effective potential, photon orbits.","Especially, analysis of intersection behaviors between photon trajectories with different impact parameters and circular time-like orbits in accretion disk will help better understand the image of thin accretion disk.","Our results showed that CDM halo will make BH become more larger and dimmer distinctly."],"url":"http://arxiv.org/abs/2403.12840v1","category":"gr-qc"}
{"created":"2024-03-19 15:41:16","title":"Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation","abstract":"Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field. Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets. Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative. We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems. To address these issues, we propose a comprehensive scribble supervision benchmark consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities. We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels. Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations. Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance. Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision. Source code as well as our extensive scribble benchmarking suite will be made publicly available upon publication.","sentences":["Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field.","Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets.","Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative.","We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems.","To address these issues, we propose a comprehensive scribble supervision benchmark consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities.","We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels.","Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations.","Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance.","Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision.","Source code as well as our extensive scribble benchmarking suite will be made publicly available upon publication."],"url":"http://arxiv.org/abs/2403.12834v1","category":"cs.CV"}
{"created":"2024-03-19 15:36:22","title":"Kinetic-type Mean Field Games with Non-separable Local Hamiltonians","abstract":"We prove well-posedness of a class of kinetic-type Mean Field Games, which typically arise when agents control their acceleration. Such systems include independent variables representing the spatial position as well as velocity. We consider non-separable Hamiltonians without any structural conditions, which depend locally on the density variable. Our analysis is based on two main ingredients: an energy method for the forward-backward system in Sobolev spaces, on the one hand and on a suitable vector field method to control derivatives with respect to the velocity variable, on the other hand. The careful combination of these two techniques reveals interesting phenomena applicable for Mean Field Games involving general classes of drift-diffusion operators and nonlinearities. While many prior existence theories for general Mean Field Games systems take the final datum function to be smoothing, we can allow this function to be non-smoothing, i.e. also depending locally on the final measure. Our well-posedness results hold under an appropriate smallness condition, assumed jointly on the data.","sentences":["We prove well-posedness of a class of kinetic-type Mean Field Games, which typically arise when agents control their acceleration.","Such systems include independent variables representing the spatial position as well as velocity.","We consider non-separable Hamiltonians without any structural conditions, which depend locally on the density variable.","Our analysis is based on two main ingredients: an energy method for the forward-backward system in Sobolev spaces, on the one hand and on a suitable vector field method to control derivatives with respect to the velocity variable, on the other hand.","The careful combination of these two techniques reveals interesting phenomena applicable for Mean Field Games involving general classes of drift-diffusion operators and nonlinearities.","While many prior existence theories for general Mean Field Games systems take the final datum function to be smoothing, we can allow this function to be non-smoothing, i.e. also depending locally on the final measure.","Our well-posedness results hold under an appropriate smallness condition, assumed jointly on the data."],"url":"http://arxiv.org/abs/2403.12829v1","category":"math.AP"}
{"created":"2024-03-19 15:31:02","title":"Oriented and Non-oriented Cubical Surfaces in The Penteract","abstract":"Which surfaces can be realized with two-dimensional faces of the five-dimensional cube (the penteract)? How can we visualize them? In recent work, Aveni, Govc, and Roldan, show that there exist 2690 connected closed cubical surfaces up to isomorphism in the 5-cube. They give a classification in terms of their genus $g$ for closed orientable cubical surfaces and their demigenus $k$ for a closed non-orientable cubical surface. In this paper, we explain the main idea behind the exhaustive search and we visualize the projection to $\\mathbb{R}^3$ of a torus, a genus two torus, the projective plane, and the Klein bottle. We use reinforcement learning techniques to obtain configurations optimized for 3D printing.","sentences":["Which surfaces can be realized with two-dimensional faces of the five-dimensional cube (the penteract)?","How can we visualize them?","In recent work, Aveni, Govc, and Roldan, show that there exist 2690 connected closed cubical surfaces up to isomorphism in the 5-cube.","They give a classification in terms of their genus $g$ for closed orientable cubical surfaces and their demigenus $k$ for a closed non-orientable cubical surface.","In this paper, we explain the main idea behind the exhaustive search and we visualize the projection to $\\mathbb{R}^3$ of a torus, a genus two torus, the projective plane, and the Klein bottle.","We use reinforcement learning techniques to obtain configurations optimized for 3D printing."],"url":"http://arxiv.org/abs/2403.12825v1","category":"math.GT"}
{"created":"2024-03-19 15:13:44","title":"A Unified Framework for Rerandomization using Quadratic Forms","abstract":"In the design stage of a randomized experiment, one way to ensure treatment and control groups exhibit similar covariate distributions is to randomize treatment until some prespecified level of covariate balance is satisfied. This experimental design strategy is known as rerandomization. Most rerandomization methods utilize balance metrics based on a quadratic form $v^TAv$ , where $v$ is a vector of covariate mean differences and $A$ is a positive semi-definite matrix. In this work, we derive general results for treatment-versus-control rerandomization schemes that employ quadratic forms for covariate balance. In addition to allowing researchers to quickly derive properties of rerandomization schemes not previously considered, our theoretical results provide guidance on how to choose the matrix $A$ in practice. We find the Mahalanobis and Euclidean distances optimize different measures of covariate balance. Furthermore, we establish how the covariates' eigenstructure and their relationship to the outcomes dictates which matrix $A$ yields the most precise mean-difference estimator for the average treatment effect. We find that the Euclidean distance is minimax optimal, in the sense that the mean-difference estimator's precision is never too far from the optimal choice, regardless of the relationship between covariates and outcomes. Our theoretical results are verified via simulation, where we find that rerandomization using the Euclidean distance has better performance in high-dimensional settings and typically achieves greater variance reduction to the mean-difference estimator than other quadratic forms.","sentences":["In the design stage of a randomized experiment, one way to ensure treatment and control groups exhibit similar covariate distributions is to randomize treatment until some prespecified level of covariate balance is satisfied.","This experimental design strategy is known as rerandomization.","Most rerandomization methods utilize balance metrics based on a quadratic form $v^TAv$ , where $v$ is a vector of covariate mean differences and $A$ is a positive semi-definite matrix.","In this work, we derive general results for treatment-versus-control rerandomization schemes that employ quadratic forms for covariate balance.","In addition to allowing researchers to quickly derive properties of rerandomization schemes not previously considered, our theoretical results provide guidance on how to choose the matrix $A$ in practice.","We find the Mahalanobis and Euclidean distances optimize different measures of covariate balance.","Furthermore, we establish how the covariates' eigenstructure and their relationship to the outcomes dictates which matrix $A$ yields the most precise mean-difference estimator for the average treatment effect.","We find that the Euclidean distance is minimax optimal, in the sense that the mean-difference estimator's precision is never too far from the optimal choice, regardless of the relationship between covariates and outcomes.","Our theoretical results are verified via simulation, where we find that rerandomization using the Euclidean distance has better performance in high-dimensional settings and typically achieves greater variance reduction to the mean-difference estimator than other quadratic forms."],"url":"http://arxiv.org/abs/2403.12815v1","category":"stat.ME"}
{"created":"2024-03-19 15:07:12","title":"Freshness-aware Block Propagation Optimization in 6G-based Web 3.0: An Evolutionary Game Approach","abstract":"Driven by the aspiration to establish a decentralized digital economy, Web 3.0 is emerging as the fundamental technology for digital transformation. Incorporating the promising sixth-generation (6G) technology with large bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds great potential in empowering users with enhanced data control and facilitating secure peer-to-peer transactions, especially in consumer electronics, through the utilization of blockchain technologies. However, 6G-based Web 3.0 is still in its infancy, such as ensuring block freshness and optimizing block propagation to improve blockchain performance. In this paper, we develop a freshness-aware block propagation optimization framework for 6G-based Web 3.0. We first propose a novel metric called Age of Block Information (AoBI) based on the concept of age of information to quantify block freshness. To make block propagation optimization tractable, we classify miners into five different states and propose a block propagation model for public blockchains inspired by epidemic models. Moreover, considering that the miners are bounded rational, we propose an incentive mechanism based on the evolutionary game for block propagation to improve block propagation efficiency. Numerical results demonstrate that compared with other block propagation mechanisms, the proposed scheme has a higher block forwarding probability, which improves block propagation efficiency.","sentences":["Driven by the aspiration to establish a decentralized digital economy, Web 3.0 is emerging as the fundamental technology for digital transformation.","Incorporating the promising sixth-generation (6G) technology with large bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds great potential in empowering users with enhanced data control and facilitating secure peer-to-peer transactions, especially in consumer electronics, through the utilization of blockchain technologies.","However, 6G-based Web 3.0 is still in its infancy, such as ensuring block freshness and optimizing block propagation to improve blockchain performance.","In this paper, we develop a freshness-aware block propagation optimization framework for 6G-based Web 3.0.","We first propose a novel metric called Age of Block Information (AoBI) based on the concept of age of information to quantify block freshness.","To make block propagation optimization tractable, we classify miners into five different states and propose a block propagation model for public blockchains inspired by epidemic models.","Moreover, considering that the miners are bounded rational, we propose an incentive mechanism based on the evolutionary game for block propagation to improve block propagation efficiency.","Numerical results demonstrate that compared with other block propagation mechanisms, the proposed scheme has a higher block forwarding probability, which improves block propagation efficiency."],"url":"http://arxiv.org/abs/2403.12807v1","category":"cs.GT"}
{"created":"2024-03-19 15:00:53","title":"Introducing Combi-Stations in Robotic Mobile Fulfilment Systems: A Queueing-Theory-Based Efficiency Analysis","abstract":"In the era of digital commerce, the surge in online shopping and the expectation for rapid delivery have placed unprecedented demands on warehouse operations. The traditional method of order fulfilment, where human order pickers traverse large storage areas to pick items, has become a bottleneck, consuming valuable time and resources. Robotic Mobile Fulfilment Systems (RMFS) offer a solution by using robots to transport storage racks directly to human-operated picking stations, eliminating the need for pickers to travel. This paper introduces combi-stations, a novel type of station that enables both item picking and replenishment, as opposed to traditional separate stations. We analyse the efficiency of combi-stations using queueing theory and demonstrate their potential to streamline warehouse operations. Our results suggest that combi-stations can reduce the number of robots required for stability and significantly reduce order turnover time, indicating a promising direction for future warehouse automation.","sentences":["In the era of digital commerce, the surge in online shopping and the expectation for rapid delivery have placed unprecedented demands on warehouse operations.","The traditional method of order fulfilment, where human order pickers traverse large storage areas to pick items, has become a bottleneck, consuming valuable time and resources.","Robotic Mobile Fulfilment Systems (RMFS) offer a solution by using robots to transport storage racks directly to human-operated picking stations, eliminating the need for pickers to travel.","This paper introduces combi-stations, a novel type of station that enables both item picking and replenishment, as opposed to traditional separate stations.","We analyse the efficiency of combi-stations using queueing theory and demonstrate their potential to streamline warehouse operations.","Our results suggest that combi-stations can reduce the number of robots required for stability and significantly reduce order turnover time, indicating a promising direction for future warehouse automation."],"url":"http://arxiv.org/abs/2403.12798v1","category":"cs.RO"}
{"created":"2024-03-19 13:54:34","title":"A new framework for constrained optimization via feedback control of Lagrange multipliers","abstract":"The continuous-time analysis of existing iterative algorithms for optimization has a long history. This work proposes a novel continuous-time control-theoretic framework for equality-constrained optimization. The key idea is to design a feedback control system where the Lagrange multipliers are the control input, and the output represents the constraints. The system converges to a stationary point of the constrained optimization problem through suitable regulation. Regarding the Lagrange multipliers, we consider two control laws: proportional-integral control and feedback linearization. These choices give rise to a family of different methods. We rigorously develop the related algorithms, theoretically analyze their convergence and present several numerical experiments to support their effectiveness concerning the state-of-the-art approaches.","sentences":["The continuous-time analysis of existing iterative algorithms for optimization has a long history.","This work proposes a novel continuous-time control-theoretic framework for equality-constrained optimization.","The key idea is to design a feedback control system where the Lagrange multipliers are the control input, and the output represents the constraints.","The system converges to a stationary point of the constrained optimization problem through suitable regulation.","Regarding the Lagrange multipliers, we consider two control laws: proportional-integral control and feedback linearization.","These choices give rise to a family of different methods.","We rigorously develop the related algorithms, theoretically analyze their convergence and present several numerical experiments to support their effectiveness concerning the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.12738v1","category":"math.OC"}
{"created":"2024-03-19 13:46:59","title":"Extension of the characterization method for non-Gaussianity in gravitational wave detector with statistical hypothesis test","abstract":"In gravitational wave astronomy, non-Gaussian noise, such as scattered light noise disturbs stable interferometer operation, limiting the interferometer's sensitivity, and reducing the reliability of the analyses. In scattered light noise, the non-Gaussian noise dominates the sensitivity in a low frequency range of less than a few hundred Hz, which is sensitive to gravitational waves from compact binary coalescence. This non-Gaussian noise prevents reliable parameter estimation, since several analysis methods are optimized only for Gaussian noise. Therefore, identifying data contaminated by non-Gaussian noise is important. In this work, we extended the conventional method to evaluate non-Gaussian noise, Rayleigh statistic, by using a statistical hypothesis test to determine a threshold for non-Gaussian noise. First, we estimated the distribution of the Rayleigh statistic against Gaussian noise, called the background distribution, and validated that our extension serves as the hypothetical test. Moreover, we investigated the detection efficiency by assuming two non-Gaussian noise models. For example, for the model with strong scattered light noise, the true positive rate was always above 0.7 when the significance level was 0.05. The results showed that our extension can contribute to an initial detection of non-Gaussian noise and lead to further investigation of the origin of the non-Gaussian noise.","sentences":["In gravitational wave astronomy, non-Gaussian noise, such as scattered light noise disturbs stable interferometer operation, limiting the interferometer's sensitivity, and reducing the reliability of the analyses.","In scattered light noise, the non-Gaussian noise dominates the sensitivity in a low frequency range of less than a few hundred Hz, which is sensitive to gravitational waves from compact binary coalescence.","This non-Gaussian noise prevents reliable parameter estimation, since several analysis methods are optimized only for Gaussian noise.","Therefore, identifying data contaminated by non-Gaussian noise is important.","In this work, we extended the conventional method to evaluate non-Gaussian noise, Rayleigh statistic, by using a statistical hypothesis test to determine a threshold for non-Gaussian noise.","First, we estimated the distribution of the Rayleigh statistic against Gaussian noise, called the background distribution, and validated that our extension serves as the hypothetical test.","Moreover, we investigated the detection efficiency by assuming two non-Gaussian noise models.","For example, for the model with strong scattered light noise, the true positive rate was always above 0.7 when the significance level was 0.05.","The results showed that our extension can contribute to an initial detection of non-Gaussian noise and lead to further investigation of the origin of the non-Gaussian noise."],"url":"http://arxiv.org/abs/2403.12731v1","category":"gr-qc"}
{"created":"2024-03-19 13:43:00","title":"Optimizing Leapover Lengths of L\u00e9vy Flights with Resetting","abstract":"We consider a one-dimensional search process under stochastic resetting conditions. A target is located at $b\\geq0$ and a searcher, starting from the origin, performs a discrete-time random walk with independent jumps drawn from a heavy-tailed distribution. Before each jump, there is a given probability $r$ of restarting the walk from the initial position. The efficiency of a \"myopic search\" - in which the search stops upon crossing the target for the first time - is usually characterized in terms of the first-passage time $\\tau$. On the other hand, great relevance is encapsulated by the leapover length $l = x_{\\tau} - b$, which measures how far from the target the search ends. For symmetric heavy-tailed jump distributions, in the absence of resetting the average leapover is always infinite. Here we show instead that resetting induces a finite average leapover $\\ell_b(r)$ if the mean jump length is finite. We compute exactly $\\ell_b(r)$ and determine the condition under which resetting allows for nontrivial optimization, i.e., for the existence of $r^*$ such that $\\ell_b(r^*)$ is minimal and smaller than the average leapover of the single jump.","sentences":["We consider a one-dimensional search process under stochastic resetting conditions.","A target is located at $b\\geq0$ and a searcher, starting from the origin, performs a discrete-time random walk with independent jumps drawn from a heavy-tailed distribution.","Before each jump, there is a given probability $r$ of restarting the walk from the initial position.","The efficiency of a \"myopic search\" - in which the search stops upon crossing the target for the first time - is usually characterized in terms of the first-passage time $\\tau$. On the other hand, great relevance is encapsulated by the leapover length $l = x_{\\tau} - b$, which measures how far from the target the search ends.","For symmetric heavy-tailed jump distributions, in the absence of resetting the average leapover is always infinite.","Here we show instead that resetting induces a finite average leapover $\\ell_b(r)$ if the mean jump length is finite.","We compute exactly $\\ell_b(r)$ and determine the condition under which resetting allows for nontrivial optimization, i.e., for the existence of $r^*$ such that $\\ell_b(r^*)$ is minimal and smaller than the average leapover of the single jump."],"url":"http://arxiv.org/abs/2403.12727v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-19 13:41:49","title":"Some geometric and topological data-driven methods in robot motion path planning","abstract":"Motion path planning is an intrinsically geometric problem which is central for design of robot systems. Since the early years of AI, robotics together with computer vision have been the areas of computer science that drove its development. Many questions that arise, such as existence, optimality, and diversity of motion paths in the configuration space that describes feasible robot configurations, are of topological nature. The recent advances in topological data analysis and related metric geometry, topology and combinatorics have provided new tools to address these engineering tasks. We will survey some questions, issues, recent work and promising directions in data-driven geometric and topological methods with some emphasis on the use of discrete Morse theory.","sentences":["Motion path planning is an intrinsically geometric problem which is central for design of robot systems.","Since the early years of AI, robotics together with computer vision have been the areas of computer science that drove its development.","Many questions that arise, such as existence, optimality, and diversity of motion paths in the configuration space that describes feasible robot configurations, are of topological nature.","The recent advances in topological data analysis and related metric geometry, topology and combinatorics have provided new tools to address these engineering tasks.","We will survey some questions, issues, recent work and promising directions in data-driven geometric and topological methods with some emphasis on the use of discrete Morse theory."],"url":"http://arxiv.org/abs/2403.12725v1","category":"math.AT"}
{"created":"2024-03-19 13:39:05","title":"HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting","abstract":"Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.","sentences":["Holistic understanding of urban scenes based on RGB images is a challenging yet important problem.","It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects.","Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes.","In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding.","Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints.","Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy.","Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.12722v1","category":"cs.CV"}
{"created":"2024-03-19 12:05:09","title":"ICE: Interactive 3D Game Character Editing via Dialogue","abstract":"Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters. However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification. In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process. In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players. Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round. To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner. Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results. Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE. Project page: https://iceedit.github.io/.","sentences":["Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters.","However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification.","In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process.","In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players.","Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round.","To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner.","Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results.","Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE.","Project page: https://iceedit.github.io/."],"url":"http://arxiv.org/abs/2403.12667v2","category":"cs.MM"}
{"created":"2024-03-19 11:31:28","title":"Revisiting Local Computation of PageRank: Simple and Optimal","abstract":"We revisit the classic local graph exploration algorithm ApproxContributions proposed by Andersen, Borgs, Chayes, Hopcroft, Mirrokni, and Teng (WAW '07, Internet Math. '08) for computing an $\\epsilon$-approximation of the PageRank contribution vector for a target node $t$ on a graph with $n$ nodes and $m$ edges. We give a worst-case complexity bound of ApproxContributions as $O(n\\pi(t)/\\epsilon\\cdot\\min(\\Delta_{in},\\Delta_{out},\\sqrt{m}))$, where $\\pi(t)$ is the PageRank score of $t$, and $\\Delta_{in}$ and $\\Delta_{out}$ are the maximum in-degree and out-degree of the graph, resp. We also give a lower bound of $\\Omega(\\min(\\Delta_{in}/\\delta,\\Delta_{out}/\\delta,\\sqrt{m}/\\delta,m))$ for detecting the $\\delta$-contributing set of $t$, showing that the simple ApproxContributions algorithm is already optimal.   We also investigate the computational complexity of locally estimating a node's PageRank centrality. We improve the best-known upper bound of $\\widetilde{O}(n^{2/3}\\cdot\\min(\\Delta_{out}^{1/3},m^{1/6}))$ given by Bressan, Peserico, and Pretto (SICOMP '23) to $O(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ by simply combining ApproxContributions with the Monte Carlo simulation method. We also improve their lower bound of $\\Omega(\\min(n^{1/2}\\Delta_{out}^{1/2},n^{1/3}m^{1/3}))$ to $\\Omega(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ if $\\min(\\Delta_{in},\\Delta_{out})=\\Omega(n^{1/3})$, and to $\\Omega(n^{1/2-\\gamma}(\\min(\\Delta_{in},\\Delta_{out}))^{1/2+\\gamma})$ if $\\min(\\Delta_{in},\\Delta_{out})=o(n^{1/3})$, where $\\gamma>0$ is an arbitrarily small constant. Our matching upper and lower bounds resolve the open problem of whether one can tighten the bounds given by Bressan, Peserico, and Pretto (FOCS '18, SICOMP '23). Remarkably, the techniques and analyses for proving all our results are surprisingly simple.","sentences":["We revisit the classic local graph exploration algorithm ApproxContributions proposed by Andersen, Borgs, Chayes, Hopcroft, Mirrokni, and Teng (WAW '07, Internet Math.","'08) for computing an $\\epsilon$-approximation of the PageRank contribution vector for a target node $t$ on a graph with $n$ nodes and $m$ edges.","We give a worst-case complexity bound of ApproxContributions as $O(n\\pi(t)/\\epsilon\\cdot\\min(\\Delta_{in},\\Delta_{out},\\sqrt{m}))$, where $\\pi(t)$ is the PageRank score of $t$, and $\\Delta_{in}$ and $\\Delta_{out}$ are the maximum in-degree and out-degree of the graph, resp.","We also give a lower bound of $\\Omega(\\min(\\Delta_{in}/\\delta,\\Delta_{out}/\\delta,\\sqrt{m}/\\delta,m))$ for detecting the $\\delta$-contributing set of $t$, showing that the simple ApproxContributions algorithm is already optimal.   ","We also investigate the computational complexity of locally estimating a node's PageRank centrality.","We improve the best-known upper bound of $\\widetilde{O}(n^{2/3}\\cdot\\min(\\Delta_{out}^{1/3},m^{1/6}))$ given by Bressan, Peserico, and Pretto (SICOMP '23) to $O(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ by simply combining ApproxContributions with the Monte Carlo simulation method.","We also improve their lower bound of $\\Omega(\\min(n^{1/2}\\Delta_{out}^{1/2},n^{1/3}m^{1/3}))$ to $\\Omega(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ if $\\min(\\Delta_{in},\\Delta_{out})=\\Omega(n^{1/3})$, and to $\\Omega(n^{1/2-\\gamma}(\\min(\\Delta_{in},\\Delta_{out}))^{1/2+\\gamma})$ if $\\min(\\Delta_{in},\\Delta_{out})=o(n^{1/3})$, where $\\gamma>0$ is an arbitrarily small constant.","Our matching upper and lower bounds resolve the open problem of whether one can tighten the bounds given by Bressan, Peserico, and Pretto (FOCS '18, SICOMP '23).","Remarkably, the techniques and analyses for proving all our results are surprisingly simple."],"url":"http://arxiv.org/abs/2403.12648v1","category":"cs.DS"}
{"created":"2024-03-19 11:30:03","title":"When Does Your Brain Know You? Segment Length and Its Impact on EEG-based Biometric Authentication Accuracy","abstract":"In the quest for optimal EEG-based biometric authentication, this study investigates the pivotal balance for accurate identification without sacrificing performance or adding unnecessary computational complexity. Through a methodical exploration of segment durations, and employing a variety of sophisticated machine learning models, the research seeks to pinpoint a threshold where EEG data provides maximum informational yield for authentication purposes. The findings are set to advance the field of non-invasive biometric technologies, proposing a practical approach to secure and user-friendly identity verification systems while also raising considerations for the real-world application of EEG-based biometric authentication beyond controlled environments.","sentences":["In the quest for optimal EEG-based biometric authentication, this study investigates the pivotal balance for accurate identification without sacrificing performance or adding unnecessary computational complexity.","Through a methodical exploration of segment durations, and employing a variety of sophisticated machine learning models, the research seeks to pinpoint a threshold where EEG data provides maximum informational yield for authentication purposes.","The findings are set to advance the field of non-invasive biometric technologies, proposing a practical approach to secure and user-friendly identity verification systems while also raising considerations for the real-world application of EEG-based biometric authentication beyond controlled environments."],"url":"http://arxiv.org/abs/2403.12644v1","category":"cs.CR"}
{"created":"2024-03-19 11:24:14","title":"Automated Contrastive Learning Strategy Search for Time Series","abstract":"In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive pair construction and contrastive losses. Further, we introduce an efficient reinforcement learning algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space. Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task. From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets. We also provide empirical analysis as a guidance for future design of CLS.","sentences":["In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series.","Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks.","However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations.","In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL).","We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive pair construction and contrastive losses.","Further, we introduce an efficient reinforcement learning algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space.","Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task.","From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets.","We also provide empirical analysis as a guidance for future design of CLS."],"url":"http://arxiv.org/abs/2403.12641v1","category":"cs.LG"}
{"created":"2024-03-19 11:20:18","title":"Hardy inequalities for large fermionic systems","abstract":"Given $0<s<\\frac d2$ with $s\\leq 1$, we are interested in the large $N$-behavior of the optimal constant $\\kappa_N$ in the Hardy inequality $\\sum_{n=1}^N (-\\Delta_n)^s \\geq \\kappa_N \\sum_{n<m} |X_n-X_m|^{-2s}$, when restricted to antisymmetric functions. We show that $N^{1-\\frac{2s}d}\\kappa_N$ has a positive, finite limit given by a certain variational problem, thereby generalizing a result of Lieb and Yau related to the Chandrasekhar theory of gravitational collapse.","sentences":["Given $0<s<\\frac d2$ with $s\\leq 1$, we are interested in the large $N$-behavior of the optimal constant $\\kappa_N$ in the Hardy inequality $\\sum_{n=1}^N (-\\Delta_n)^s \\geq \\kappa_N \\sum_{n<m} |X_n-X_m|^{-2s}$, when restricted to antisymmetric functions.","We show that $N^{1-\\frac{2s}d}\\kappa_N$ has a positive, finite limit given by a certain variational problem, thereby generalizing a result of Lieb and Yau related to the Chandrasekhar theory of gravitational collapse."],"url":"http://arxiv.org/abs/2403.12640v1","category":"math.AP"}
{"created":"2024-03-19 10:45:19","title":"Optimisation of Gyrokinetic Microstability Using Adjoint Methods","abstract":"Microinstabilities drive turbulent fluctuations in inhomogeneous, magnetized plasmas. In the context of magnetic confinement fusion devices, this leads to an enhanced transport of particles, momentum, and energy, thereby degrading confinement. In this work, we elaborate on the application of the adjoint method to efficiently determine the variation of linear growth rates for plasma microstabilities concerning a general set of external parameters within the local $\\delta \\! f$-gyrokinetic model. We then offer numerical verification of this approach. When coupled with gradient-based techniques, this methodology can facilitate the optimization process for the microstability of the confined plasmas across a high-dimensional parameter space. We present a numerical demonstration wherein the ion-temperature gradient (ITG) instability growth rate in a tokamak plasma is minimized with respect to flux surface shaping parameters. The adjoint method approach demonstrates a significant computational speed-up compared to a finite-difference gradient calculation.","sentences":["Microinstabilities drive turbulent fluctuations in inhomogeneous, magnetized plasmas.","In the context of magnetic confinement fusion devices, this leads to an enhanced transport of particles, momentum, and energy, thereby degrading confinement.","In this work, we elaborate on the application of the adjoint method to efficiently determine the variation of linear growth rates for plasma microstabilities concerning a general set of external parameters within the local $\\delta \\!","f$-gyrokinetic model.","We then offer numerical verification of this approach.","When coupled with gradient-based techniques, this methodology can facilitate the optimization process for the microstability of the confined plasmas across a high-dimensional parameter space.","We present a numerical demonstration wherein the ion-temperature gradient (ITG) instability growth rate in a tokamak plasma is minimized with respect to flux surface shaping parameters.","The adjoint method approach demonstrates a significant computational speed-up compared to a finite-difference gradient calculation."],"url":"http://arxiv.org/abs/2403.12621v1","category":"physics.plasm-ph"}
{"created":"2024-03-19 09:47:08","title":"LASPA: Latent Spatial Alignment for Fast Training-free Single Image Editing","abstract":"We present a novel, training-free approach for textual editing of real images using diffusion models. Unlike prior methods that rely on computationally expensive finetuning, our approach leverages LAtent SPatial Alignment (LASPA) to efficiently preserve image details. We demonstrate how the diffusion process is amenable to spatial guidance using a reference image, leading to semantically coherent edits. This eliminates the need for complex optimization and costly model finetuning, resulting in significantly faster editing compared to previous methods. Additionally, our method avoids the storage requirements associated with large finetuned models. These advantages make our approach particularly well-suited for editing on mobile devices and applications demanding rapid response times. While simple and fast, our method achieves 62-71\\% preference in a user-study and significantly better model-based editing strength and image preservation scores.","sentences":["We present a novel, training-free approach for textual editing of real images using diffusion models.","Unlike prior methods that rely on computationally expensive finetuning, our approach leverages LAtent SPatial Alignment (LASPA) to efficiently preserve image details.","We demonstrate how the diffusion process is amenable to spatial guidance using a reference image, leading to semantically coherent edits.","This eliminates the need for complex optimization and costly model finetuning, resulting in significantly faster editing compared to previous methods.","Additionally, our method avoids the storage requirements associated with large finetuned models.","These advantages make our approach particularly well-suited for editing on mobile devices and applications demanding rapid response times.","While simple and fast, our method achieves 62-71\\% preference in a user-study and significantly better model-based editing strength and image preservation scores."],"url":"http://arxiv.org/abs/2403.12585v1","category":"cs.CV"}
{"created":"2024-03-19 09:46:17","title":"Robust Fuel-Optimal Landing Guidance for Hazardous Terrain using Multiple Sliding Surfaces","abstract":"In any spacecraft landing mission, precision soft landing in a fuel-efficient way while also avoiding nearby hazardous terrain is of utmost importance. Very few existing literature have attempted addressing both the problems of precision soft landing and terrain avoidance simultaneously. To this end, an optimal terrain avoidance landing guidance (OTALG) was recently developed, which showed promising performance in avoiding the terrain while consuming near-minimum fuel. However, its performance significantly degrades in the face of external disturbances, indicating lack of robustness. In order to mitigate this problem, in this paper, a novel near fuel-optimal guidance law is developed to avoid terrain and land precisely and softly at the desired landing site, under atmospheric perturbations and thrust deviations and constraints. Expanding the OTALG formulation by using sliding mode control with multiple sliding surfaces (MSS), the presented guidance law, named `MSS-OTALG', improves in terms of precision soft landing accuracy. Further, the sliding parameter is designed to allow the lander to avoid terrain by leaving the trajectory enforced by the sliding mode, and eventually returning to it when the terrain avoidance phase is completed. And finally, the robustness of the MSS-OTALG is established by proving practical fixed-time stability. Extensive numerical simulations are also presented to showcase its performance in terms of terrain avoidance, low fuel consumption and accuracy of precision soft landing. Comparative studies against existing relevant literature validates a balanced trade-off of all these performance measures achieved by the developed MSS-OTALG.","sentences":["In any spacecraft landing mission, precision soft landing in a fuel-efficient way while also avoiding nearby hazardous terrain is of utmost importance.","Very few existing literature have attempted addressing both the problems of precision soft landing and terrain avoidance simultaneously.","To this end, an optimal terrain avoidance landing guidance (OTALG) was recently developed, which showed promising performance in avoiding the terrain while consuming near-minimum fuel.","However, its performance significantly degrades in the face of external disturbances, indicating lack of robustness.","In order to mitigate this problem, in this paper, a novel near fuel-optimal guidance law is developed to avoid terrain and land precisely and softly at the desired landing site, under atmospheric perturbations and thrust deviations and constraints.","Expanding the OTALG formulation by using sliding mode control with multiple sliding surfaces (MSS), the presented guidance law, named `MSS-OTALG', improves in terms of precision soft landing accuracy.","Further, the sliding parameter is designed to allow the lander to avoid terrain by leaving the trajectory enforced by the sliding mode, and eventually returning to it when the terrain avoidance phase is completed.","And finally, the robustness of the MSS-OTALG is established by proving practical fixed-time stability.","Extensive numerical simulations are also presented to showcase its performance in terms of terrain avoidance, low fuel consumption and accuracy of precision soft landing.","Comparative studies against existing relevant literature validates a balanced trade-off of all these performance measures achieved by the developed MSS-OTALG."],"url":"http://arxiv.org/abs/2403.12584v1","category":"eess.SY"}
{"created":"2024-03-19 09:38:57","title":"The Smoothed Duality Gap as a Stopping Criterion","abstract":"We optimize the running time of the primal-dual algorithms by optimizing their stopping criteria for solving convex optimization problems under affine equality constraints, which means terminating the algorithm earlier with fewer iterations. We study the relations between four stopping criteria and show under which conditions they are accurate to detect optimal solutions. The uncomputable one: ''Optimality gap and Feasibility error'', and the computable ones: the ''Karush-Kuhn-Tucker error'', the ''Projected Duality Gap'', and the ''Smoothed Duality Gap''. Assuming metric sub-regularity or quadratic error bound, we establish that all of the computable criteria provide practical upper bounds for the optimality gap, and approximate it effectively. Furthermore, we establish comparability between some of the computable criteria under certain conditions. Numerical experiments on basis pursuit, and quadratic programs with(out) non-negative weights corroborate these findings and show the superior stability of the smoothed duality gap over the rest.","sentences":["We optimize the running time of the primal-dual algorithms by optimizing their stopping criteria for solving convex optimization problems under affine equality constraints, which means terminating the algorithm earlier with fewer iterations.","We study the relations between four stopping criteria and show under which conditions they are accurate to detect optimal solutions.","The uncomputable one: ''Optimality gap and Feasibility error'', and the computable ones: the ''Karush-Kuhn-Tucker error'', the ''Projected Duality Gap'', and the ''Smoothed Duality Gap''.","Assuming metric sub-regularity or quadratic error bound, we establish that all of the computable criteria provide practical upper bounds for the optimality gap, and approximate it effectively.","Furthermore, we establish comparability between some of the computable criteria under certain conditions.","Numerical experiments on basis pursuit, and quadratic programs with(out) non-negative weights corroborate these findings and show the superior stability of the smoothed duality gap over the rest."],"url":"http://arxiv.org/abs/2403.12579v1","category":"math.OC"}
