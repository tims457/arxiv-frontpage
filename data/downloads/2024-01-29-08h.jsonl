{"created":"2024-01-26 18:59:30","title":"Physical Yukawa Couplings in Heterotic String Compactifications","abstract":"One of the challenges of heterotic compactification on a Calabi-Yau threefold is to determine the physical $(\\mathbf{27})^3$ Yukawa couplings of the resulting four-dimensional $\\mathcal{N}=1$ theory. In general, the calculation necessitates knowledge of the Ricci-flat metric. However, in the standard embedding, which references the tangent bundle, we can compute normalized Yukawa couplings from the Weil-Petersson metric on the moduli space of complex structure deformations of the Calabi-Yau manifold. In various examples (the Fermat quintic, the intersection of two cubics in $\\mathbb{P}^5$, and the Tian-Yau manifold), we calculate the normalized Yukawa couplings for $(2,1)$-forms using the Weil-Petersson metric obtained from the Kodaira-Spencer map. In cases where $h^{1,1}=1$, this is compared to a complementary calculation based on performing period integrals. A third expression for the normalized Yukawa couplings is obtained from a machine learned approximate Ricci-flat metric making use of explicit harmonic representatives. The excellent agreement between the different approaches opens the door to precision string phenomenology.","sentences":["One of the challenges of heterotic compactification on a Calabi-Yau threefold is to determine the physical $(\\mathbf{27})^3$ Yukawa couplings of the resulting four-dimensional $\\mathcal{N}=1$ theory.","In general, the calculation necessitates knowledge of the Ricci-flat metric.","However, in the standard embedding, which references the tangent bundle, we can compute normalized Yukawa couplings from the Weil-Petersson metric on the moduli space of complex structure deformations of the Calabi-Yau manifold.","In various examples (the Fermat quintic, the intersection of two cubics in $\\mathbb{P}^5$, and the Tian-Yau manifold), we calculate the normalized Yukawa couplings for $(2,1)$-forms using the Weil-Petersson metric obtained from the Kodaira-Spencer map.","In cases where $h^{1,1}=1$, this is compared to a complementary calculation based on performing period integrals.","A third expression for the normalized Yukawa couplings is obtained from a machine learned approximate Ricci-flat metric making use of explicit harmonic representatives.","The excellent agreement between the different approaches opens the door to precision string phenomenology."],"url":"http://arxiv.org/abs/2401.15078v1","category":"hep-th"}
{"created":"2024-01-26 18:59:01","title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty","abstract":"Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface's implementations.","sentences":["Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming.","We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration.","Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead.","The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding.","As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family.","On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa.","Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface's implementations."],"url":"http://arxiv.org/abs/2401.15077v1","category":"cs.LG"}
{"created":"2024-01-26 18:57:54","title":"Annotated Hands for Generative Models","abstract":"Generative models such as GANs and diffusion models have demonstrated impressive image generation capabilities. Despite these successes, these systems are surprisingly poor at creating images with hands. We propose a novel training framework for generative models that substantially improves the ability of such systems to create hand images. Our approach is to augment the training images with three additional channels that provide annotations to hands in the image. These annotations provide additional structure that coax the generative model to produce higher quality hand images. We demonstrate this approach on two different generative models: a generative adversarial network and a diffusion model. We demonstrate our method both on a new synthetic dataset of hand images and also on real photographs that contain hands. We measure the improved quality of the generated hands through higher confidence in finger joint identification using an off-the-shelf hand detector.","sentences":["Generative models such as GANs and diffusion models have demonstrated impressive image generation capabilities.","Despite these successes, these systems are surprisingly poor at creating images with hands.","We propose a novel training framework for generative models that substantially improves the ability of such systems to create hand images.","Our approach is to augment the training images with three additional channels that provide annotations to hands in the image.","These annotations provide additional structure that coax the generative model to produce higher quality hand images.","We demonstrate this approach on two different generative models: a generative adversarial network and a diffusion model.","We demonstrate our method both on a new synthetic dataset of hand images and also on real photographs that contain hands.","We measure the improved quality of the generated hands through higher confidence in finger joint identification using an off-the-shelf hand detector."],"url":"http://arxiv.org/abs/2401.15075v1","category":"cs.CV"}
{"created":"2024-01-26 18:54:35","title":"Quantum types: going beyond qubits and quantum gates","abstract":"Quantum computing is a growing field with significant potential applications. Learning how to code quantum programs means understanding how qubits work and learning to use quantum gates. This is analogous to creating classical algorithms using logic gates and bits. Even after learning all concepts, it is difficult to create new algorithms, which hinders the acceptance of quantum programming by most developers. This article outlines the need for higher-level abstractions and proposes some of them in a developer-friendly programming language called Rhyme. The new quantum types are extensions of classical types, including bits, integers, floats, characters, arrays, and strings. We show how to use such types with code snippets.","sentences":["Quantum computing is a growing field with significant potential applications.","Learning how to code quantum programs means understanding how qubits work and learning to use quantum gates.","This is analogous to creating classical algorithms using logic gates and bits.","Even after learning all concepts, it is difficult to create new algorithms, which hinders the acceptance of quantum programming by most developers.","This article outlines the need for higher-level abstractions and proposes some of them in a developer-friendly programming language called Rhyme.","The new quantum types are extensions of classical types, including bits, integers, floats, characters, arrays, and strings.","We show how to use such types with code snippets."],"url":"http://arxiv.org/abs/2401.15073v1","category":"quant-ph"}
{"created":"2024-01-26 18:53:03","title":"From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities","abstract":"Multi-modal Large Language Models (MLLMs) have shown impressive abilities in generating reasonable responses with respect to multi-modal contents. However, there is still a wide gap between the performance of recent MLLM-based applications and the expectation of the broad public, even though the most powerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper strives to enhance understanding of the gap through the lens of a qualitative study on the generalizability, trustworthiness, and causal reasoning capabilities of recent proprietary and open-source MLLMs across four modalities: ie, text, code, image, and video, ultimately aiming to improve the transparency of MLLMs. We believe these properties are several representative factors that define the reliability of MLLMs, in supporting various downstream applications. To be specific, we evaluate the closed-source GPT-4 and Gemini and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed cases, where the qualitative results are then summarized into 12 scores (ie, 4 modalities times 3 properties). In total, we uncover 14 empirical findings that are useful to understand the capabilities and limitations of both proprietary and open-source MLLMs, towards more reliable downstream multi-modal applications.","sentences":["Multi-modal Large Language Models (MLLMs) have shown impressive abilities in generating reasonable responses with respect to multi-modal contents.","However, there is still a wide gap between the performance of recent MLLM-based applications and the expectation of the broad public, even though the most powerful OpenAI's GPT-4 and Google's Gemini have been deployed.","This paper strives to enhance understanding of the gap through the lens of a qualitative study on the generalizability, trustworthiness, and causal reasoning capabilities of recent proprietary and open-source MLLMs across four modalities: ie, text, code, image, and video, ultimately aiming to improve the transparency of MLLMs.","We believe these properties are several representative factors that define the reliability of MLLMs, in supporting various downstream applications.","To be specific, we evaluate the closed-source GPT-4 and Gemini and 6 open-source LLMs and MLLMs.","Overall we evaluate 230 manually designed cases, where the qualitative results are then summarized into 12 scores (ie, 4 modalities times 3 properties).","In total, we uncover 14 empirical findings that are useful to understand the capabilities and limitations of both proprietary and open-source MLLMs, towards more reliable downstream multi-modal applications."],"url":"http://arxiv.org/abs/2401.15071v1","category":"cs.CV"}
{"created":"2024-01-26 18:49:34","title":"Pairing Orthographically Variant Literary Words to Standard Equivalents Using Neural Edit Distance Models","abstract":"We present a novel corpus consisting of orthographically variant words found in works of 19th century U.S. literature annotated with their corresponding \"standard\" word pair. We train a set of neural edit distance models to pair these variants with their standard forms, and compare the performance of these models to the performance of a set of neural edit distance models trained on a corpus of orthographic errors made by L2 English learners. Finally, we analyze the relative performance of these models in the light of different negative training sample generation strategies, and offer concluding remarks on the unique challenge literary orthographic variation poses to string pairing methodologies.","sentences":["We present a novel corpus consisting of orthographically variant words found in works of 19th century U.S. literature annotated with their corresponding \"standard\" word pair.","We train a set of neural edit distance models to pair these variants with their standard forms, and compare the performance of these models to the performance of a set of neural edit distance models trained on a corpus of orthographic errors made by L2 English learners.","Finally, we analyze the relative performance of these models in the light of different negative training sample generation strategies, and offer concluding remarks on the unique challenge literary orthographic variation poses to string pairing methodologies."],"url":"http://arxiv.org/abs/2401.15068v1","category":"cs.CL"}
{"created":"2024-01-26 18:48:23","title":"Universality conditions of unified classical and quantum reservoir computing","abstract":"Reservoir computing is a versatile paradigm in computational neuroscience and machine learning, that exploits the non-linear dynamics of a dynamical system - the reservoir - to efficiently process time-dependent information. Since its introduction, it has exhibited remarkable capabilities in various applications. As widely known, classes of reservoir computers serve as universal approximators of functionals with fading memory. The construction of such universal classes often appears context-specific, but in fact, they follow the same principles. Here we present a unified theoretical framework and we propose a ready-made setting to secure universality. We test the result in the arising context of quantum reservoir computing. Guided by such a unified theorem we suggest why spatial multiplexing may serve as a computational resource when dealing with quantum registers, as empirically observed in specific implementations on quantum hardware. The analysis sheds light on a unified view of classical and quantum reservoir computing.","sentences":["Reservoir computing is a versatile paradigm in computational neuroscience and machine learning, that exploits the non-linear dynamics of a dynamical system - the reservoir - to efficiently process time-dependent information.","Since its introduction, it has exhibited remarkable capabilities in various applications.","As widely known, classes of reservoir computers serve as universal approximators of functionals with fading memory.","The construction of such universal classes often appears context-specific, but in fact, they follow the same principles.","Here we present a unified theoretical framework and we propose a ready-made setting to secure universality.","We test the result in the arising context of quantum reservoir computing.","Guided by such a unified theorem we suggest why spatial multiplexing may serve as a computational resource when dealing with quantum registers, as empirically observed in specific implementations on quantum hardware.","The analysis sheds light on a unified view of classical and quantum reservoir computing."],"url":"http://arxiv.org/abs/2401.15067v1","category":"quant-ph"}
{"created":"2024-01-26 18:47:55","title":"Efficient High-Dimensional Entangled State Analyzer with Linear Optics","abstract":"The use of higher-dimensional photonic encodings (qudits) instead of two-dimensional encodings (qubits) can improve the loss tolerance and reduce the computational resources of photonic-based quantum information processing. To harness this potential, efficient schemes for entangling operations such as the high-dimensional generalization of a linear optics Bell measurement will be required. We show how an efficient high-dimensional entangled state analyzer can be implemented with linear optics and auxiliary photonic states. The Schmidt rank of the auxiliary state in our protocol scales only linearly with the dimensions of the input states instead of more than exponentially, as in previous proposals. In addition, we outline how the state can be generated deterministically from a single quantum emitter coupled to a small qubit processor. Our protocol thus outlines an experimentally feasible route for efficient, high-dimensional entangled state analyzers with linear optics.","sentences":["The use of higher-dimensional photonic encodings (qudits) instead of two-dimensional encodings (qubits) can improve the loss tolerance and reduce the computational resources of photonic-based quantum information processing.","To harness this potential, efficient schemes for entangling operations such as the high-dimensional generalization of a linear optics Bell measurement will be required.","We show how an efficient high-dimensional entangled state analyzer can be implemented with linear optics and auxiliary photonic states.","The Schmidt rank of the auxiliary state in our protocol scales only linearly with the dimensions of the input states instead of more than exponentially, as in previous proposals.","In addition, we outline how the state can be generated deterministically from a single quantum emitter coupled to a small qubit processor.","Our protocol thus outlines an experimentally feasible route for efficient, high-dimensional entangled state analyzers with linear optics."],"url":"http://arxiv.org/abs/2401.15066v1","category":"quant-ph"}
{"created":"2024-01-26 18:44:49","title":"Expert with Clustering: Hierarchical Online Preference Learning Framework","abstract":"Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users. We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids. In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our algorithm achieves a regret bound of $O(N\\sqrt{T\\log K} + NT)$. This bound consists of two parts: the first term is the regret from the Hedge algorithm, and the second term depends on the average loss from clustering. The algorithm performs with low regret, especially when a latent hierarchical structure exists among users. This regret bound underscores the theoretical and experimental efficacy of EWC, particularly in scenarios that demand rapid learning and adaptation. Experimental results highlight that EWC can substantially reduce regret by 27.57% compared to the LinUCB baseline. Our work offers a data-efficient approach to capturing both individual and collective behaviors, making it highly applicable to contexts with hierarchical structures. We expect the algorithm to be applicable to other settings with layered nuances of user preferences and information.","sentences":["Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes.","Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation.","In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users.","We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice.","EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric.","This metric is instrumental in generating more representative cluster centroids.","In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our algorithm achieves a regret bound of $O(N\\sqrt{T\\log K} +","NT)$. This bound consists of two parts: the first term is the regret from the Hedge algorithm, and the second term depends on the average loss from clustering.","The algorithm performs with low regret, especially when a latent hierarchical structure exists among users.","This regret bound underscores the theoretical and experimental efficacy of EWC, particularly in scenarios that demand rapid learning and adaptation.","Experimental results highlight that EWC can substantially reduce regret by 27.57% compared to the LinUCB baseline.","Our work offers a data-efficient approach to capturing both individual and collective behaviors, making it highly applicable to contexts with hierarchical structures.","We expect the algorithm to be applicable to other settings with layered nuances of user preferences and information."],"url":"http://arxiv.org/abs/2401.15062v1","category":"cs.LG"}
{"created":"2024-01-26 18:42:57","title":"Digital-analog hybrid matrix multiplication processor for optical neural networks","abstract":"The computational demands of modern AI have spurred interest in optical neural networks (ONNs) which offer the potential benefits of increased speed and lower power consumption. However, current ONNs face various challenges,most significantly a limited calculation precision (typically around 4 bits) and the requirement for high-resolution signal format converters (digital-to-analogue conversions (DACs) and analogue-to-digital conversions (ADCs)). These challenges are inherent to their analog computing nature and pose significant obstacles in practical implementation. Here, we propose a digital-analog hybrid optical computing architecture for ONNs, which utilizes digital optical inputs in the form of binary words. By introducing the logic levels and decisions based on thresholding, the calculation precision can be significantly enhanced. The DACs for input data can be removed and the resolution of the ADCs can be greatly reduced. This can increase the operating speed at a high calculation precision and facilitate the compatibility with microelectronics. To validate our approach, we have fabricated a proof-of-concept photonic chip and built up a hybrid optical processor (HOP) system for neural network applications. We have demonstrated an unprecedented 16-bit calculation precision for high-definition image processing, with a pixel error rate (PER) as low as $1.8\\times10^{-3}$ at an signal-to-noise ratio (SNR) of 18.2 dB. We have also implemented a convolutional neural network for handwritten digit recognition that shows the same accuracy as the one achieved by a desktop computer. The concept of the digital-analog hybrid optical computing architecture offers a methodology that could potentially be applied to various ONN implementations and may intrigue new research into efficient and accurate domain-specific optical computing architectures for neural networks.","sentences":["The computational demands of modern AI have spurred interest in optical neural networks (ONNs) which offer the potential benefits of increased speed and lower power consumption.","However, current ONNs face various challenges,most significantly a limited calculation precision (typically around 4 bits) and the requirement for high-resolution signal format converters (digital-to-analogue conversions (DACs) and analogue-to-digital conversions (ADCs)).","These challenges are inherent to their analog computing nature and pose significant obstacles in practical implementation.","Here, we propose a digital-analog hybrid optical computing architecture for ONNs, which utilizes digital optical inputs in the form of binary words.","By introducing the logic levels and decisions based on thresholding, the calculation precision can be significantly enhanced.","The DACs for input data can be removed and the resolution of the ADCs can be greatly reduced.","This can increase the operating speed at a high calculation precision and facilitate the compatibility with microelectronics.","To validate our approach, we have fabricated a proof-of-concept photonic chip and built up a hybrid optical processor (HOP) system for neural network applications.","We have demonstrated an unprecedented 16-bit calculation precision for high-definition image processing, with a pixel error rate (PER) as low as $1.8\\times10^{-3}$ at an signal-to-noise ratio (SNR) of 18.2 dB. We have also implemented a convolutional neural network for handwritten digit recognition that shows the same accuracy as the one achieved by a desktop computer.","The concept of the digital-analog hybrid optical computing architecture offers a methodology that could potentially be applied to various ONN implementations and may intrigue new research into efficient and accurate domain-specific optical computing architectures for neural networks."],"url":"http://arxiv.org/abs/2401.15061v1","category":"cs.NE"}
{"created":"2024-01-26 18:42:01","title":"Fully Independent Communication in Multi-Agent Reinforcement Learning","abstract":"Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication may not always be needed and that the chosen agent network sizes need to be considered when used together with communication in order to achieve efficient learning.","sentences":["Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems.","Several recent works have focused specifically on the study of communication approaches in MARL.","While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts.","One of the reasons for that is due to the use of the famous parameter sharing trick.","In this paper, we investigate how independent learners in MARL that do not share parameters can communicate.","We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution.","Our results show that, despite the challenges, independent agents can still learn communication strategies following our method.","Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters.","We observe that communication may not always be needed and that the chosen agent network sizes need to be considered when used together with communication in order to achieve efficient learning."],"url":"http://arxiv.org/abs/2401.15059v1","category":"cs.LG"}
{"created":"2024-01-26 18:39:07","title":"Concurrence distribution in excited states of the 1D spin-1/2 transverse field XY model: two different regions","abstract":"We investigate the variation of concurrence in a spin-1/2 transverse field XY chain system in an excited state. Initially, we precisely solve the eigenvalue problem of the system Hamiltonian using the fermionization technique. Subsequently, we calculate the concurrence between nearest-neighbor pairs of spins in all excited states with higher energy than the ground state. Below the factorized field, denoted as $h_f=\\sqrt{J^2-(J \\delta)^2}$, we find no pairwise entanglement between nearest neighbors in excited states. At the factorized field, corresponding to a factorized state, we observe weak concurrence in very low energy states. Beyond $h_f$, the concurrence strengthens, entangling all excited states. The density of entangled states peaks at the center of the excited spectrum. Additionally, the distribution of concurrence reveals that the midpoint of the non-zero concurrence range harbors the most entangled excited states.","sentences":["We investigate the variation of concurrence in a spin-1/2 transverse field XY chain system in an excited state.","Initially, we precisely solve the eigenvalue problem of the system Hamiltonian using the fermionization technique.","Subsequently, we calculate the concurrence between nearest-neighbor pairs of spins in all excited states with higher energy than the ground state.","Below the factorized field, denoted as $h_f=\\sqrt{J^2-(J \\delta)^2}$, we find no pairwise entanglement between nearest neighbors in excited states.","At the factorized field, corresponding to a factorized state, we observe weak concurrence in very low energy states.","Beyond $h_f$, the concurrence strengthens, entangling all excited states.","The density of entangled states peaks at the center of the excited spectrum.","Additionally, the distribution of concurrence reveals that the midpoint of the non-zero concurrence range harbors the most entangled excited states."],"url":"http://arxiv.org/abs/2401.15057v1","category":"quant-ph"}
{"created":"2024-01-26 18:36:45","title":"Subset Adaptive Relaying for Streaming Erasure Codes","abstract":"This paper investigates adaptive streaming codes over a three-node relayed network. In this setting, a source transmits a sequence of message packets through a relay under a delay constraint of $T$ time slots per packet. The source-to-relay and relay-to-destination links are unreliable and introduce a maximum of $N_1$ and $N_2$ packet erasures respectively. Recent work has proposed adaptive (time variant) and nonadaptive (time invariant) code constructions for this setting and has shown that adaptive codes can achieve higher rates. However, the adaptive construction deals with many possibilities, leading to an impractical code with very large block lengths. In this work, we propose a simplified adaptive code construction which greatly improves the practicality of the code, with only a small cost to the achievable rates. We analyze the construction in terms of the achievable rates and field size requirements, and perform numerical simulations over statistical channels to estimate packet loss probabilities.","sentences":["This paper investigates adaptive streaming codes over a three-node relayed network.","In this setting, a source transmits a sequence of message packets through a relay under a delay constraint of $T$ time slots per packet.","The source-to-relay and relay-to-destination links are unreliable and introduce a maximum of $N_1$ and $N_2$ packet erasures respectively.","Recent work has proposed adaptive (time variant) and nonadaptive (time invariant) code constructions for this setting and has shown that adaptive codes can achieve higher rates.","However, the adaptive construction deals with many possibilities, leading to an impractical code with very large block lengths.","In this work, we propose a simplified adaptive code construction which greatly improves the practicality of the code, with only a small cost to the achievable rates.","We analyze the construction in terms of the achievable rates and field size requirements, and perform numerical simulations over statistical channels to estimate packet loss probabilities."],"url":"http://arxiv.org/abs/2401.15056v1","category":"cs.IT"}
{"created":"2024-01-26 18:33:57","title":"Deep learning-based approach for tomato classification in complex scenes","abstract":"Tracking ripening tomatoes is time consuming and labor intensive. Artificial intelligence technologies combined with those of computer vision can help users optimize the process of monitoring the ripening status of plants. To this end, we have proposed a tomato ripening monitoring approach based on deep learning in complex scenes. The objective is to detect mature tomatoes and harvest them in a timely manner. The proposed approach is declined in two parts. Firstly, the images of the scene are transmitted to the pre-processing layer. This process allows the detection of areas of interest (area of the image containing tomatoes). Then, these images are used as input to the maturity detection layer. This layer, based on a deep neural network learning algorithm, classifies the tomato thumbnails provided to it in one of the following five categories: green, brittle, pink, pale red, mature red. The experiments are based on images collected from the internet gathered through searches using tomato state across diverse languages including English, German, French, and Spanish. The experimental results of the maturity detection layer on a dataset composed of images of tomatoes taken under the extreme conditions, gave a good classification rate.","sentences":["Tracking ripening tomatoes is time consuming and labor intensive.","Artificial intelligence technologies combined with those of computer vision can help users optimize the process of monitoring the ripening status of plants.","To this end, we have proposed a tomato ripening monitoring approach based on deep learning in complex scenes.","The objective is to detect mature tomatoes and harvest them in a timely manner.","The proposed approach is declined in two parts.","Firstly, the images of the scene are transmitted to the pre-processing layer.","This process allows the detection of areas of interest (area of the image containing tomatoes).","Then, these images are used as input to the maturity detection layer.","This layer, based on a deep neural network learning algorithm, classifies the tomato thumbnails provided to it in one of the following five categories: green, brittle, pink, pale red, mature red.","The experiments are based on images collected from the internet gathered through searches using tomato state across diverse languages including English, German, French, and Spanish.","The experimental results of the maturity detection layer on a dataset composed of images of tomatoes taken under the extreme conditions, gave a good classification rate."],"url":"http://arxiv.org/abs/2401.15055v1","category":"cs.CV"}
{"created":"2024-01-26 18:23:45","title":"LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents","abstract":"Document AI is a growing research field that focuses on the comprehension and extraction of information from scanned and digital documents to make everyday business operations more efficient. Numerous downstream tasks and datasets have been introduced to facilitate the training of AI models capable of parsing and extracting information from various document types such as receipts and scanned forms. Despite these advancements, both existing datasets and models fail to address critical challenges that arise in industrial contexts. Existing datasets primarily comprise short documents consisting of a single page, while existing models are constrained by a limited maximum length, often set at 512 tokens. Consequently, the practical application of these methods in financial services, where documents can span multiple pages, is severely impeded. To overcome these challenges, we introduce LongFin, a multimodal document AI model capable of encoding up to 4K tokens. We also propose the LongForms dataset, a comprehensive financial dataset that encapsulates several industrial challenges in financial documents. Through an extensive evaluation, we demonstrate the effectiveness of the LongFin model on the LongForms dataset, surpassing the performance of existing public models while maintaining comparable results on existing single-page benchmarks.","sentences":["Document AI is a growing research field that focuses on the comprehension and extraction of information from scanned and digital documents to make everyday business operations more efficient.","Numerous downstream tasks and datasets have been introduced to facilitate the training of AI models capable of parsing and extracting information from various document types such as receipts and scanned forms.","Despite these advancements, both existing datasets and models fail to address critical challenges that arise in industrial contexts.","Existing datasets primarily comprise short documents consisting of a single page, while existing models are constrained by a limited maximum length, often set at 512 tokens.","Consequently, the practical application of these methods in financial services, where documents can span multiple pages, is severely impeded.","To overcome these challenges, we introduce LongFin, a multimodal document AI model capable of encoding up to 4K tokens.","We also propose the LongForms dataset, a comprehensive financial dataset that encapsulates several industrial challenges in financial documents.","Through an extensive evaluation, we demonstrate the effectiveness of the LongFin model on the LongForms dataset, surpassing the performance of existing public models while maintaining comparable results on existing single-page benchmarks."],"url":"http://arxiv.org/abs/2401.15050v1","category":"cs.CL"}
{"created":"2024-01-26 18:20:53","title":"Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings","abstract":"In the realm of security applications, biometric authentication systems play a crucial role, yet one often encounters challenges concerning privacy and security while developing one. One of the most fundamental challenges lies in avoiding storing biometrics directly in the storage but still achieving decently high accuracy. Addressing this issue, we contribute to both artificial intelligence and engineering fields. We introduce an innovative image distortion technique that effectively renders facial images unrecognizable to the eye while maintaining their identifiability by neural network models. From the theoretical perspective, we explore how reliable state-of-the-art biometrics recognition neural networks are by checking the maximal degree of image distortion, which leaves the predicted identity unchanged. On the other hand, applying this technique demonstrates a practical solution to the engineering challenge of balancing security, precision, and performance in biometric authentication systems. Through experimenting on the widely used datasets, we assess the effectiveness of our method in preserving AI feature representation and distorting relative to conventional metrics. We also compare our method with previously used approaches.","sentences":["In the realm of security applications, biometric authentication systems play a crucial role, yet one often encounters challenges concerning privacy and security while developing one.","One of the most fundamental challenges lies in avoiding storing biometrics directly in the storage but still achieving decently high accuracy.","Addressing this issue, we contribute to both artificial intelligence and engineering fields.","We introduce an innovative image distortion technique that effectively renders facial images unrecognizable to the eye while maintaining their identifiability by neural network models.","From the theoretical perspective, we explore how reliable state-of-the-art biometrics recognition neural networks are by checking the maximal degree of image distortion, which leaves the predicted identity unchanged.","On the other hand, applying this technique demonstrates a practical solution to the engineering challenge of balancing security, precision, and performance in biometric authentication systems.","Through experimenting on the widely used datasets, we assess the effectiveness of our method in preserving AI feature representation and distorting relative to conventional metrics.","We also compare our method with previously used approaches."],"url":"http://arxiv.org/abs/2401.15048v1","category":"cs.CV"}
{"created":"2024-01-26 18:16:06","title":"Emulating Complex Synapses Using Interlinked Proton Conductors","abstract":"In terms of energy efficiency and computational speed, neuromorphic electronics based on non-volatile memory devices is expected to be one of most promising hardware candidates for future artificial intelligence (AI). However, catastrophic forgetting, networks rapidly overwriting previously learned weights when learning new tasks, remains as a pivotal hurdle in either digital or analog AI chips for unleashing the true power of brain-like computing. To address catastrophic forgetting in the context of online memory storage, a complex synapse model (the Benna-Fusi model) has been proposed recently[1], whose synaptic weight and internal variables evolve following a diffusion dynamics. In this work, by designing a proton transistor with a series of charge-diffusion-controlled storage components, we have experimentally realized the Benna-Fusi artificial complex synapse. The memory consolidation from coupled storage components is revealed by both numerical simulations and experimental observations. Different memory timescales for the complex synapse are engineered by the diffusion length of charge carriers, the capacity and number of coupled storage components. The advantage of the demonstrated complex synapse in both memory capacity and memory consolidation is revealed by neural network simulations of face familiarity detection. Our experimental realization of the complex synapse suggests a promising approach to enhance memory capacity and to enable continual learning.","sentences":["In terms of energy efficiency and computational speed, neuromorphic electronics based on non-volatile memory devices is expected to be one of most promising hardware candidates for future artificial intelligence (AI).","However, catastrophic forgetting, networks rapidly overwriting previously learned weights when learning new tasks, remains as a pivotal hurdle in either digital or analog AI chips for unleashing the true power of brain-like computing.","To address catastrophic forgetting in the context of online memory storage, a complex synapse model (the Benna-Fusi model) has been proposed recently[1], whose synaptic weight and internal variables evolve following a diffusion dynamics.","In this work, by designing a proton transistor with a series of charge-diffusion-controlled storage components, we have experimentally realized the Benna-Fusi artificial complex synapse.","The memory consolidation from coupled storage components is revealed by both numerical simulations and experimental observations.","Different memory timescales for the complex synapse are engineered by the diffusion length of charge carriers, the capacity and number of coupled storage components.","The advantage of the demonstrated complex synapse in both memory capacity and memory consolidation is revealed by neural network simulations of face familiarity detection.","Our experimental realization of the complex synapse suggests a promising approach to enhance memory capacity and to enable continual learning."],"url":"http://arxiv.org/abs/2401.15045v1","category":"cs.NE"}
{"created":"2024-01-26 18:13:57","title":"Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning","abstract":"Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.   Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based approaches. Our experimentation encompasses Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a lightweight model adept at distinguishing between original and simplified texts, thereby enhancing the model's effectiveness with unlabeled data.   Results: Fine-tuned Llama 2 models demonstrated high performance across various metrics. Our innovative RLHF reward function surpassed existing RL text simplification reward functions in effectiveness. The results underscore that RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text and improving performance. Additionally, these methods effectively adapt out-of-domain text simplification models to targeted domains.","sentences":["Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations.","Many patient educational resources surpass the reading level and complexity of widely accepted standards.","There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy.","This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.   ","Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research.","Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based approaches.","Our experimentation encompasses Llama 2 and GPT-4.","A novel RLHF reward function is introduced, featuring a lightweight model adept at distinguishing between original and simplified texts, thereby enhancing the model's effectiveness with unlabeled data.   ","Results: Fine-tuned Llama 2 models demonstrated high performance across various metrics.","Our innovative RLHF reward function surpassed existing RL text simplification reward functions in effectiveness.","The results underscore that RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text and improving performance.","Additionally, these methods effectively adapt out-of-domain text simplification models to targeted domains."],"url":"http://arxiv.org/abs/2401.15043v1","category":"cs.CL"}
{"created":"2024-01-26 18:12:25","title":"PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models","abstract":"Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \\textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \\textit{meta-questions} spanning various domains. Each meta-question contains corresponding \\textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \\textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \\textit{proxy-questions}. We examine multiple LLMs, emphasizing \\textsc{ProxyQA}'s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that evaluating through \\textit{proxy-questions} is a highly self-consistent and human-criteria-correlated validation method. The dataset and leaderboard will be available at \\url{https://github.com/Namco0816/ProxyQA}.","sentences":["Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks.","However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored.","Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach.","In this study, we introduce \\textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \\textit{meta-questions} spanning various domains.","Each meta-question contains corresponding \\textit{proxy-questions} with annotated answers.","LLMs are prompted to generate extensive content in response to these meta-questions.","Utilizing an evaluator and incorporating generated content as background context, \\textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \\textit{proxy-questions}.","We examine multiple LLMs, emphasizing \\textsc{ProxyQA}'s demanding nature as a high-quality assessment tool.","Human evaluation demonstrates that evaluating through \\textit{proxy-questions} is a highly self-consistent and human-criteria-correlated validation method.","The dataset and leaderboard will be available at \\url{https://github.com/Namco0816/ProxyQA}."],"url":"http://arxiv.org/abs/2401.15042v1","category":"cs.CL"}
{"created":"2024-01-26 18:09:47","title":"Computationally Bounded Robust Compilation and Universally Composable Security","abstract":"Universal Composability (UC) is the gold standard for cryptographic security, but mechanizing proofs of UC is notoriously difficult. A recently-discovered connection between UC and Robust Compilation (RC)$\\unicode{x2014}$a novel theory of secure compilation$\\unicode{x2014}$provides a means to verify UC proofs using tools that mechanize equality results. Unfortunately, the existing methods apply only to perfect UC security, and real-world protocols relying on cryptography are only computationally secure.   This paper addresses this gap by lifting the connection between UC and RC to the computational setting, extending techniques from the RC setting to apply to computational UC security. Moreover, it further generalizes the UC$\\unicode{x2013}$RC connection beyond computational security to arbitrary equalities, providing a framework to subsume the existing perfect case, and to instantiate future theories with more complex notions of security. This connection allows the use of tools for proofs of computational indistinguishability to properly mechanize proofs of computational UC security. We demonstrate this power by using CryptoVerif to mechanize a proof that parts of the Wireguard protocol are computationally UC secure. Finally, all proofs of the framework itself are verified in Isabelle/HOL.","sentences":["Universal Composability (UC) is the gold standard for cryptographic security, but mechanizing proofs of UC is notoriously difficult.","A recently-discovered connection between UC and Robust Compilation (RC)$\\unicode{x2014}$a novel theory of secure compilation$\\unicode{x2014}$provides a means to verify UC proofs using tools that mechanize equality results.","Unfortunately, the existing methods apply only to perfect UC security, and real-world protocols relying on cryptography are only computationally secure.   ","This paper addresses this gap by lifting the connection between UC and RC to the computational setting, extending techniques from the RC setting to apply to computational UC security.","Moreover, it further generalizes the UC$\\unicode{x2013}$RC connection beyond computational security to arbitrary equalities, providing a framework to subsume the existing perfect case, and to instantiate future theories with more complex notions of security.","This connection allows the use of tools for proofs of computational indistinguishability to properly mechanize proofs of computational UC security.","We demonstrate this power by using CryptoVerif to mechanize a proof that parts of the Wireguard protocol are computationally UC secure.","Finally, all proofs of the framework itself are verified in Isabelle/HOL."],"url":"http://arxiv.org/abs/2401.15041v1","category":"cs.CR"}
{"created":"2024-01-26 18:05:39","title":"Inequivalence of mimetic gravity with models of loop quantum gravity","abstract":"Certain versions of mimetic gravity have recently been claimed to present potential covariant theories of canonically modified spherically symmetric gravity, motivated by ingredients from loop quantum gravity. If such an equivalence were to hold, it would demonstrate general covariance of a large class of models considered in loop quantum gravity. However, the relationship with mimetic gravity as presented so far is incomplete because it has been proposed only in preferred space-time slicings of uniform scalar fields. Here, several independent arguments are used to show that neither an equivalence nor a covariance claim are correct for models of loop quantum gravity. The framework of emergent modified gravity is found to present a broad setting in which such questions can be analyzed efficiently. As an additional result, the discussion sheds light on the co-existence of different and mutually inequivalent approaches to an implementation of the gravitational dynamics within loop quantum gravity.","sentences":["Certain versions of mimetic gravity have recently been claimed to present potential covariant theories of canonically modified spherically symmetric gravity, motivated by ingredients from loop quantum gravity.","If such an equivalence were to hold, it would demonstrate general covariance of a large class of models considered in loop quantum gravity.","However, the relationship with mimetic gravity as presented so far is incomplete because it has been proposed only in preferred space-time slicings of uniform scalar fields.","Here, several independent arguments are used to show that neither an equivalence nor a covariance claim are correct for models of loop quantum gravity.","The framework of emergent modified gravity is found to present a broad setting in which such questions can be analyzed efficiently.","As an additional result, the discussion sheds light on the co-existence of different and mutually inequivalent approaches to an implementation of the gravitational dynamics within loop quantum gravity."],"url":"http://arxiv.org/abs/2401.15040v1","category":"gr-qc"}
{"created":"2024-01-26 18:05:29","title":"Chaotic Encryption Applied to Optical Ethernet in Industrial Control Systems","abstract":"In the past decades, Ethernet has become an alternative technology for the field buses traditionally used in industrial control systems and distributed measurement systems. Among different transmission media in Ethernet standards, optical fiber provides the best bandwidth, excellent immunity to electromagnetic interference, and less signal loses than other wired media. Due to the absence of a standard that provides security at the physical layer of optical Ethernet links, the main motivation of this paper is to propose and implement the necessary modifications to introduce encryption in Ethernet 1000Base-X standard. This has consisted of symmetric streaming encryption of the 8b10b symbols flow at physical coding sublayer level, thanks to a keystream generator based on chaotic algorithm. The overall system has been implemented and tested in an field programmable gate array and Ethernet traffic has been encrypted and transmitted over an optical link. The experimental results show that it is possible to cipher traffic at this level and hide the complete Ethernet traffic pattern from passive eavesdroppers. In addition, no space overhead is introduced in data frames during encryption, achieving the maximum throughput.","sentences":["In the past decades, Ethernet has become an alternative technology for the field buses traditionally used in industrial control systems and distributed measurement systems.","Among different transmission media in Ethernet standards, optical fiber provides the best bandwidth, excellent immunity to electromagnetic interference, and less signal loses than other wired media.","Due to the absence of a standard that provides security at the physical layer of optical Ethernet links, the main motivation of this paper is to propose and implement the necessary modifications to introduce encryption in Ethernet 1000Base-X standard.","This has consisted of symmetric streaming encryption of the 8b10b symbols flow at physical coding sublayer level, thanks to a keystream generator based on chaotic algorithm.","The overall system has been implemented and tested in an field programmable gate array and Ethernet traffic has been encrypted and transmitted over an optical link.","The experimental results show that it is possible to cipher traffic at this level and hide the complete Ethernet traffic pattern from passive eavesdroppers.","In addition, no space overhead is introduced in data frames during encryption, achieving the maximum throughput."],"url":"http://arxiv.org/abs/2401.15039v1","category":"cs.CR"}
{"created":"2024-01-26 18:01:59","title":"Physical Layer Encryption for Industrial Ethernet in Gigabit Optical Links","abstract":"Industrial Ethernet is a technology widely spread in factory floors and critical infrastructures where a high amount of data need to be collected and transported. Fiber optic networks at gigabit rates fit well with that type of environments where speed, system performance and reliability are critical. In this work a new encryption method for high speed optical communications suitable for such kind of networks is proposed. This new encryption method consists of a symmetric streaming encryption of the 8b/10b data flow at PCS (Physical Coding Sublayer) level. It is carried out thanks to an FPE (Format Preserving Encryption) blockcipher working in CTR (Counter) mode. The overall system has been simulated and implemented in an FPGA (Field Programmable Gate Array). Thanks to experimental results it can be concluded that it is possible to cipher traffic at this physical level in a secure way. In addition, no overhead is introduced during encryption, getting minimum latency and maximum throughput.","sentences":["Industrial Ethernet is a technology widely spread in factory floors and critical infrastructures where a high amount of data need to be collected and transported.","Fiber optic networks at gigabit rates fit well with that type of environments where speed, system performance and reliability are critical.","In this work a new encryption method for high speed optical communications suitable for such kind of networks is proposed.","This new encryption method consists of a symmetric streaming encryption of the 8b/10b data flow at PCS (Physical Coding Sublayer) level.","It is carried out thanks to an FPE (Format Preserving Encryption) blockcipher working in CTR (Counter) mode.","The overall system has been simulated and implemented in an FPGA (Field Programmable Gate Array).","Thanks to experimental results it can be concluded that it is possible to cipher traffic at this physical level in a secure way.","In addition, no overhead is introduced during encryption, getting minimum latency and maximum throughput."],"url":"http://arxiv.org/abs/2401.15038v1","category":"cs.CR"}
{"created":"2024-01-26 17:59:55","title":"On symmetries of spheres in univalent foundations","abstract":"Working in univalent foundations, we investigate the symmetries of spheres, i.e., the types of the form $\\mathbb{S}^n = \\mathbb{S}^n$. The case of the circle has a slick answer: the symmetries of the circle form two copies of the circle. For higher-dimensional spheres, the type of symmetries has again two connected components, namely the components of the maps of degree plus or minus one. Each of the two components has $\\mathbb{Z}/2\\mathbb{Z}$ as fundamental group. For the latter result, we develop an EHP long exact sequence.","sentences":["Working in univalent foundations, we investigate the symmetries of spheres, i.e., the types of the form $\\mathbb{S}^n = \\mathbb{S}^n$.","The case of the circle has a slick answer: the symmetries of the circle form two copies of the circle.","For higher-dimensional spheres, the type of symmetries has again two connected components, namely the components of the maps of degree plus or minus one.","Each of the two components has $\\mathbb{Z}/2\\mathbb{Z}$ as fundamental group.","For the latter result, we develop an EHP long exact sequence."],"url":"http://arxiv.org/abs/2401.15037v1","category":"cs.LO"}
{"created":"2024-01-26 17:54:55","title":"Distributed Simultaneous Localisation and Auto-Calibration using Gaussian Belief Propagation","abstract":"We present a novel scalable, fully distributed, and online method for simultaneous localisation and extrinsic calibration for multi-robot setups. Individual a priori unknown robot poses are probabilistically inferred as robots sense each other while simultaneously calibrating their sensors and markers extrinsic using Gaussian Belief Propagation. In the presented experiments, we show how our method not only yields accurate robot localisation and auto-calibration but also is able to perform under challenging circumstances such as highly noisy measurements, significant communication failures or limited communication range.","sentences":["We present a novel scalable, fully distributed, and online method for simultaneous localisation and extrinsic calibration for multi-robot setups.","Individual a priori unknown robot poses are probabilistically inferred as robots sense each other while simultaneously calibrating their sensors and markers extrinsic using Gaussian Belief Propagation.","In the presented experiments, we show how our method not only yields accurate robot localisation and auto-calibration but also is able to perform under challenging circumstances such as highly noisy measurements, significant communication failures or limited communication range."],"url":"http://arxiv.org/abs/2401.15036v1","category":"cs.RO"}
{"created":"2024-01-26 17:53:34","title":"Explicit Subcodes of Reed-Solomon Codes that Efficiently Achieve List Decoding Capacity","abstract":"In this paper, we introduce a novel explicit family of subcodes of Reed-Solomon (RS) codes that efficiently achieve list decoding capacity with a constant output list size. Our approach builds upon the idea of large linear subcodes of RS codes evaluated on a subfield, similar to the method employed by Guruswami and Xing (STOC 2013). However, our approach diverges by leveraging the idea of {\\it permuted product codes}, thereby simplifying the construction by avoiding the need of {\\it subspace designs}.   Specifically, the codes are constructed by initially forming the tensor product of two RS codes with carefully selected evaluation sets, followed by specific cyclic shifts to the codeword rows. This process results in each codeword column being treated as an individual coordinate, reminiscent of prior capacity-achieving codes, such as folded RS codes and univariate multiplicity codes. This construction is easily shown to be a subcode of an interleaved RS code, equivalently, an RS code evaluated on a subfield.   Alternatively, the codes can be constructed by the evaluation of bivariate polynomials over orbits generated by \\emph{two} affine transformations with coprime orders, extending the earlier use of a single affine transformation in folded RS codes and the recent affine folded RS codes introduced by Bhandari {\\it et al.} (IEEE T-IT, Feb.~2024). While our codes require large, yet constant characteristic, the two affine transformations facilitate achieving code length equal to the field size, without the restriction of the field being prime, contrasting with univariate multiplicity codes.","sentences":["In this paper, we introduce a novel explicit family of subcodes of Reed-Solomon (RS) codes that efficiently achieve list decoding capacity with a constant output list size.","Our approach builds upon the idea of large linear subcodes of RS codes evaluated on a subfield, similar to the method employed by Guruswami and Xing (STOC 2013).","However, our approach diverges by leveraging the idea of {\\it permuted product codes}, thereby simplifying the construction by avoiding the need of {\\it subspace designs}.   ","Specifically, the codes are constructed by initially forming the tensor product of two RS codes with carefully selected evaluation sets, followed by specific cyclic shifts to the codeword rows.","This process results in each codeword column being treated as an individual coordinate, reminiscent of prior capacity-achieving codes, such as folded RS codes and univariate multiplicity codes.","This construction is easily shown to be a subcode of an interleaved RS code, equivalently, an RS code evaluated on a subfield.   ","Alternatively, the codes can be constructed by the evaluation of bivariate polynomials over orbits generated by \\emph{two} affine transformations with coprime orders, extending the earlier use of a single affine transformation in folded RS codes and the recent affine folded RS codes introduced by Bhandari {\\it et al.}","(IEEE T-IT, Feb.~2024).","While our codes require large, yet constant characteristic, the two affine transformations facilitate achieving code length equal to the field size, without the restriction of the field being prime, contrasting with univariate multiplicity codes."],"url":"http://arxiv.org/abs/2401.15034v1","category":"cs.IT"}
{"created":"2024-01-26 17:48:28","title":"Color Maker: a Mixed-Initiative Approach to Creating Accessible Color Maps","abstract":"Quantitative data is frequently represented using color, yet designing effective color mappings is a challenging task, requiring one to balance perceptual standards with personal color preference. Current design tools either overwhelm novices with complexity or offer limited customization options. We present ColorMaker, a mixed-initiative approach for creating colormaps. ColorMaker combines fluid user interaction with real-time optimization to generate smooth, continuous color ramps. Users specify their loose color preferences while leaving the algorithm to generate precise color sequences, meeting both designer needs and established guidelines. ColorMaker can create new colormaps, including designs accessible for people with color-vision deficiencies, starting from scratch or with only partial input, thus supporting ideation and iterative refinement. We show that our approach can generate designs with similar or superior perceptual characteristics to standard colormaps. A user study demonstrates how designers of varying skill levels can use this tool to create custom, high-quality colormaps. ColorMaker is available at https://colormaker.org","sentences":["Quantitative data is frequently represented using color, yet designing effective color mappings is a challenging task, requiring one to balance perceptual standards with personal color preference.","Current design tools either overwhelm novices with complexity or offer limited customization options.","We present ColorMaker, a mixed-initiative approach for creating colormaps.","ColorMaker combines fluid user interaction with real-time optimization to generate smooth, continuous color ramps.","Users specify their loose color preferences while leaving the algorithm to generate precise color sequences, meeting both designer needs and established guidelines.","ColorMaker can create new colormaps, including designs accessible for people with color-vision deficiencies, starting from scratch or with only partial input, thus supporting ideation and iterative refinement.","We show that our approach can generate designs with similar or superior perceptual characteristics to standard colormaps.","A user study demonstrates how designers of varying skill levels can use this tool to create custom, high-quality colormaps.","ColorMaker is available at https://colormaker.org"],"url":"http://arxiv.org/abs/2401.15032v1","category":"cs.HC"}
{"created":"2024-01-26 17:44:53","title":"Tensor product algorithms for inference of contact network from epidemiological data","abstract":"We consider a problem of inferring contact network from nodal states observed during an epidemiological process. In a black-box Bayesian optimisation framework this problem reduces to a discrete likelihood optimisation over the set of possible networks. The high dimensionality of this set, which grows quadratically with the number of network nodes, makes this optimisation computationally challenging. Moreover, the computation of the likelihood of a network requires estimating probabilities of the observed data to realise during the evolution of the epidemiological process on this network. A stochastic simulation algorithm struggles to estimate rare events of observing the data (corresponding to the ground truth network) during the evolution with a significantly different network, and hence prevents optimisation of the likelihood. We replace the stochastic simulation with solving the chemical master equation for the probabilities of all network states. This equation also suffers from the curse of dimensionality due to the exponentially large number of network states. We overcome this by approximating the probability of states in the tensor-train decomposition. This enables fast and accurate computation of small probabilities and likelihoods. Numerical simulations demonstrate efficient black-box Bayesian inference of the network.","sentences":["We consider a problem of inferring contact network from nodal states observed during an epidemiological process.","In a black-box Bayesian optimisation framework this problem reduces to a discrete likelihood optimisation over the set of possible networks.","The high dimensionality of this set, which grows quadratically with the number of network nodes, makes this optimisation computationally challenging.","Moreover, the computation of the likelihood of a network requires estimating probabilities of the observed data to realise during the evolution of the epidemiological process on this network.","A stochastic simulation algorithm struggles to estimate rare events of observing the data (corresponding to the ground truth network) during the evolution with a significantly different network, and hence prevents optimisation of the likelihood.","We replace the stochastic simulation with solving the chemical master equation for the probabilities of all network states.","This equation also suffers from the curse of dimensionality due to the exponentially large number of network states.","We overcome this by approximating the probability of states in the tensor-train decomposition.","This enables fast and accurate computation of small probabilities and likelihoods.","Numerical simulations demonstrate efficient black-box Bayesian inference of the network."],"url":"http://arxiv.org/abs/2401.15031v1","category":"stat.CO"}
{"created":"2024-01-26 17:42:59","title":"On the generalization capacity of neural networks during generic multimodal reasoning","abstract":"The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, either cross-modal attention or models with deeper attention layers are key architectural features required to integrate multimodal inputs. On the other hand, neither of these architectural features led to productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide Generic COG (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.","sentences":["The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities.","To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization.","We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures).","We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better.","Our positive results demonstrate that for multimodal distractor and systematic generalization, either cross-modal attention or models with deeper attention layers are key architectural features required to integrate multimodal inputs.","On the other hand, neither of these architectural features led to productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal generalization.","These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning.","Finally, we provide Generic COG (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore."],"url":"http://arxiv.org/abs/2401.15030v1","category":"cs.LG"}
{"created":"2024-01-26 17:42:52","title":"Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring","abstract":"This work leverages neural radiance fields and remote sensing for forestry applications. Here, we show neural radiance fields offer a wide range of possibilities to improve upon existing remote sensing methods in forest monitoring. We present experiments that demonstrate their potential to: (1) express fine features of forest 3D structure, (2) fuse available remote sensing modalities and (3), improve upon 3D structure derived forest metrics. Altogether, these properties make neural fields an attractive computational tool with great potential to further advance the scalability and accuracy of forest monitoring programs.","sentences":["This work leverages neural radiance fields and remote sensing for forestry applications.","Here, we show neural radiance fields offer a wide range of possibilities to improve upon existing remote sensing methods in forest monitoring.","We present experiments that demonstrate their potential to: (1) express fine features of forest 3D structure, (2) fuse available remote sensing modalities and (3), improve upon 3D structure derived forest metrics.","Altogether, these properties make neural fields an attractive computational tool with great potential to further advance the scalability and accuracy of forest monitoring programs."],"url":"http://arxiv.org/abs/2401.15029v1","category":"cs.CV"}
{"created":"2024-01-26 17:37:25","title":"Multi-Agent Coordination for a Partially Observable and Dynamic Robot Soccer Environment with Limited Communication","abstract":"RoboCup represents an International testbed for advancing research in AI and robotics, focusing on a definite goal: developing a robot team that can win against the human world soccer champion team by the year 2050. To achieve this goal, autonomous humanoid robots' coordination is crucial. This paper explores novel solutions within the RoboCup Standard Platform League (SPL), where a reduction in WiFi communication is imperative, leading to the development of new coordination paradigms. The SPL has experienced a substantial decrease in network packet rate, compelling the need for advanced coordination architectures to maintain optimal team functionality in dynamic environments. Inspired by market-based task assignment, we introduce a novel distributed coordination system to orchestrate autonomous robots' actions efficiently in low communication scenarios. This approach has been tested with NAO robots during official RoboCup competitions and in the SimRobot simulator, demonstrating a notable reduction in task overlaps in limited communication settings.","sentences":["RoboCup represents an International testbed for advancing research in AI and robotics, focusing on a definite goal: developing a robot team that can win against the human world soccer champion team by the year 2050.","To achieve this goal, autonomous humanoid robots' coordination is crucial.","This paper explores novel solutions within the RoboCup Standard Platform League (SPL), where a reduction in WiFi communication is imperative, leading to the development of new coordination paradigms.","The SPL has experienced a substantial decrease in network packet rate, compelling the need for advanced coordination architectures to maintain optimal team functionality in dynamic environments.","Inspired by market-based task assignment, we introduce a novel distributed coordination system to orchestrate autonomous robots' actions efficiently in low communication scenarios.","This approach has been tested with NAO robots during official RoboCup competitions and in the SimRobot simulator, demonstrating a notable reduction in task overlaps in limited communication settings."],"url":"http://arxiv.org/abs/2401.15026v1","category":"cs.RO"}
{"created":"2024-01-26 17:35:45","title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns","abstract":"Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression","sentences":["Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.","Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc.","Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware.","In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network.","Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively.","Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%.","We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.","Code is available at: https://github.com/microsoft/TransformerCompression"],"url":"http://arxiv.org/abs/2401.15024v1","category":"cs.LG"}
{"created":"2024-01-26 17:29:01","title":"Machine learning-based analysis of glioma tissue sections: a review","abstract":"In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in isolation (10/70) or in addition to the TCGA datasets (11/70). Current approaches mostly rely on convolutional neural networks (53/70) for analyzing tissue at 20x magnification (30/70). A new field of research is the integration of clinical data, omics data, or magnetic resonance imaging (27/70). So far, machine learning-based methods have achieved promising results, but are not yet used in real clinical settings. Future work should focus on the independent validation of methods on larger, multi-site datasets with high-quality and up-to-date clinical and molecular pathology annotations to demonstrate routine applicability.","sentences":["In recent years, the diagnosis of gliomas has become increasingly complex.","Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction.","To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70).","All studies were reviewed with regard to methodological aspects as well as clinical applicability.","It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas.","The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in isolation (10/70) or in addition to the TCGA datasets (11/70).","Current approaches mostly rely on convolutional neural networks (53/70) for analyzing tissue at 20x magnification (30/70).","A new field of research is the integration of clinical data, omics data, or magnetic resonance imaging (27/70).","So far, machine learning-based methods have achieved promising results, but are not yet used in real clinical settings.","Future work should focus on the independent validation of methods on larger, multi-site datasets with high-quality and up-to-date clinical and molecular pathology annotations to demonstrate routine applicability."],"url":"http://arxiv.org/abs/2401.15022v1","category":"eess.IV"}
{"created":"2024-01-26 17:28:01","title":"A Nonlinear Journey from Structural Phase Transitions to Quantum Annealing","abstract":"Motivated by an exact mapping between equilibrium properties of a 1-dimensional chain of quantum Ising spins in a transverse field (the transverse field Ising (TFI) model) and a 2-dimensional classical array of particles in double-well potentials (the \"$\\phi^4$ model\") with weak inter-chain coupling, we explore connections between the driven variants of the two systems. We argue that coupling between the fundamental topological solitary waves in the form of kinks between neighboring chains in the classical $\\phi^4$ system is the analogue of the competing effect of the transverse field on spin flips in the quantum TFI model. As an example application, we mimic simplified measurement protocols in a closed quantum model system by studying the classical $\\phi^4$ model subjected to periodic perturbations. This reveals memory/loss of memory and coherence/decoherence regimes, whose quantum analogues are essential in annealing phenomena. In particular, we examine regimes where the topological excitations control the thermal equilibration following perturbations. This paves the way for further explorations of the analogy between lower-dimensional linear quantum and higher-dimensional classical nonlinear systems.","sentences":["Motivated by an exact mapping between equilibrium properties of a 1-dimensional chain of quantum Ising spins in a transverse field (the transverse field Ising (TFI) model) and a 2-dimensional classical array of particles in double-well potentials (the \"$\\phi^4$ model\") with weak inter-chain coupling, we explore connections between the driven variants of the two systems.","We argue that coupling between the fundamental topological solitary waves in the form of kinks between neighboring chains in the classical $\\phi^4$ system is the analogue of the competing effect of the transverse field on spin flips in the quantum TFI model.","As an example application, we mimic simplified measurement protocols in a closed quantum model system by studying the classical $\\phi^4$ model subjected to periodic perturbations.","This reveals memory/loss of memory and coherence/decoherence regimes, whose quantum analogues are essential in annealing phenomena.","In particular, we examine regimes where the topological excitations control the thermal equilibration following perturbations.","This paves the way for further explorations of the analogy between lower-dimensional linear quantum and higher-dimensional classical nonlinear systems."],"url":"http://arxiv.org/abs/2401.15020v1","category":"nlin.PS"}
{"created":"2024-01-26 17:23:48","title":"Symmetries in the many-body problems, a method to find its ayalytical solution, and Helium atom spectrum","abstract":"In this work it is shown that there are symmetries beyond the Euclidean group $E\\left(3\\right)$ in 3-body problem, and by extension in many-body problem, with inverse squared distance inter particle force. The symmetries in 3-body problem form a group: $SO\\left(4\\times3,2\\times3\\right)/\\left(C\\left(3\\times2\\right)\\right)$, where $C\\left(n\\right)$ is the planar translation group in n dimensions, which forms its Spectrum-Generating group. Some of these quantities commute with the Hamiltonian. The existence of these conserved quantities was verified by calculating energy spectrum of the Helium atom. This method can also be used to find symmetries in many-body problem, and to calculate energy levels, and wave-functions of more complicated systems, which include every possible atomic and molecular systems in chemistry.","sentences":["In this work it is shown that there are symmetries beyond the Euclidean group $E\\left(3\\right)$ in 3-body problem, and by extension in many-body problem, with inverse squared distance inter particle force.","The symmetries in 3-body problem form a group: $SO\\left(4\\times3,2\\times3\\right)/\\left(C\\left(3\\times2\\right)\\right)$, where $C\\left(n\\right)$ is the planar translation group in n dimensions, which forms its Spectrum-Generating group.","Some of these quantities commute with the Hamiltonian.","The existence of these conserved quantities was verified by calculating energy spectrum of the Helium atom.","This method can also be used to find symmetries in many-body problem, and to calculate energy levels, and wave-functions of more complicated systems, which include every possible atomic and molecular systems in chemistry."],"url":"http://arxiv.org/abs/2401.15019v1","category":"physics.gen-ph"}
{"created":"2024-01-26 17:19:59","title":"Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers","abstract":"Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification. In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions. On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification. The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform - Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison of different combinations of the previous features is discussed. On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points. However, the kernel function of an SVM has great influence on its performance. In this work, we propose the combination of two SVM-based classifiers with different kernel functions: Linear kernel and Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR) classifier. The combination is carried out by means of a parallel structure approach, in which different voting rules to take the final decision are considered. Results show that significant improvement in the performance of the SV system is achieved by using the combined features with the combined classifiers either with clean speech or in the presence of noise. Finally, to enhance the system more in noisy environments, the inclusion of the multiband noise removal technique as a preprocessing stage is proposed.","sentences":["Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification.","In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions.","On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification.","The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform - Perceptual Linear Predictive (RASTA-PLP).","In this paper, a complete comparison of different combinations of the previous features is discussed.","On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points.","However, the kernel function of an SVM has great influence on its performance.","In this work, we propose the combination of two SVM-based classifiers with different kernel functions: Linear kernel and Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR) classifier.","The combination is carried out by means of a parallel structure approach, in which different voting rules to take the final decision are considered.","Results show that significant improvement in the performance of the SV system is achieved by using the combined features with the combined classifiers either with clean speech or in the presence of noise.","Finally, to enhance the system more in noisy environments, the inclusion of the multiband noise removal technique as a preprocessing stage is proposed."],"url":"http://arxiv.org/abs/2401.15018v1","category":"eess.AS"}
{"created":"2024-01-26 17:19:56","title":"Protocol for certifying entanglement in surface spin systems using a scanning tunneling microscope","abstract":"Certifying quantum entanglement is a critical step towards realizing quantum-coherent applications of surface spin systems. In this work, we show that entanglement can be unambiguously shown in a scanning tunneling microscope (STM) with electron spin resonance by exploiting the fact that entangled states undergo a free time evolution with a distinct characteristic time constant that clearly distinguishes it from any other time evolution in the system. By implementing a suitable phase control scheme, the phase of this time evolution can be mapped back onto the population of one entangled spin in a pair, which can then be read out reliably using a weakly coupled sensor spin in the junction of the scanning tunneling microscope. We demonstrate through open quantum system simulations with realistic spin systems, which are currently available with spin coherence times of $T_2\\approx$ 300 ns, that a signal directly correlated with the degree of entanglement can be measured at a temperature range of 100$-$400 mK accessible in sub-Kelvin cryogenic STM systems.","sentences":["Certifying quantum entanglement is a critical step towards realizing quantum-coherent applications of surface spin systems.","In this work, we show that entanglement can be unambiguously shown in a scanning tunneling microscope (STM) with electron spin resonance by exploiting the fact that entangled states undergo a free time evolution with a distinct characteristic time constant that clearly distinguishes it from any other time evolution in the system.","By implementing a suitable phase control scheme, the phase of this time evolution can be mapped back onto the population of one entangled spin in a pair, which can then be read out reliably using a weakly coupled sensor spin in the junction of the scanning tunneling microscope.","We demonstrate through open quantum system simulations with realistic spin systems, which are currently available with spin coherence times of $T_2\\approx$ 300 ns, that a signal directly correlated with the degree of entanglement can be measured at a temperature range of 100$-$400 mK accessible in sub-Kelvin cryogenic STM systems."],"url":"http://arxiv.org/abs/2401.15017v1","category":"cond-mat.mes-hall"}
{"created":"2024-01-26 17:18:25","title":"KPZ fluctuations in finite volume","abstract":"These lecture notes, adapted from the habilitation thesis of the author, survey in a first part various exact results obtained in the past few decades about KPZ fluctuations in one dimension, with a special focus on finite volume effects describing the relaxation to its stationary state of a finite system starting from a given initial condition. The second part is more specifically devoted to an approach allowing to express in a simple way the statistics of the current in the totally asymmetric simple exclusion process in terms of a contour integral on a compact Riemann surface, whose infinite genus limit leads to KPZ fluctuations in finite volume.","sentences":["These lecture notes, adapted from the habilitation thesis of the author, survey in a first part various exact results obtained in the past few decades about KPZ fluctuations in one dimension, with a special focus on finite volume effects describing the relaxation to its stationary state of a finite system starting from a given initial condition.","The second part is more specifically devoted to an approach allowing to express in a simple way the statistics of the current in the totally asymmetric simple exclusion process in terms of a contour integral on a compact Riemann surface, whose infinite genus limit leads to KPZ fluctuations in finite volume."],"url":"http://arxiv.org/abs/2401.15016v1","category":"math.PR"}
{"created":"2024-01-26 17:14:08","title":"Simulating interfacial flows: a farewell to planes","abstract":"Over the past decades, the volume-of-fluid (VOF) method has been the method of choice for simulating atomization processes, owing to its unique ability to discretely conserve mass. Current state-of-the-art VOF methods, however, rely on the piecewise-linear interface calculation (PLIC) to represent the interface used when calculating advection fluxes. This renders the estimated curvature of the transported interface zeroth-order accurate at best, adversely impacting the simulation of surface-tension-driven flows.   In the past few years, there have been several attempts at using piecewise-parabolic interface approximations instead of piecewise-linear ones for computing advection fluxes, albeit all limited to two-dimensional cases or not inherently mass conservative. In this contribution, we present our most recent work on three-dimensional piecewise-parabolic interface reconstruction and apply it in the context of the VOF method. As a result of increasing the order of the interface representation, the reconstruction of the interface and the estimation of its curvature now become a single step instead of two separate ones. The performance of this new approach is assessed both in terms of accuracy and stability and compared to the classical PLIC-VOF approach on a range of canonical test-cases and cases of surface-tension-driven instabilities.","sentences":["Over the past decades, the volume-of-fluid (VOF) method has been the method of choice for simulating atomization processes, owing to its unique ability to discretely conserve mass.","Current state-of-the-art VOF methods, however, rely on the piecewise-linear interface calculation (PLIC) to represent the interface used when calculating advection fluxes.","This renders the estimated curvature of the transported interface zeroth-order accurate at best, adversely impacting the simulation of surface-tension-driven flows.   ","In the past few years, there have been several attempts at using piecewise-parabolic interface approximations instead of piecewise-linear ones for computing advection fluxes, albeit all limited to two-dimensional cases or not inherently mass conservative.","In this contribution, we present our most recent work on three-dimensional piecewise-parabolic interface reconstruction and apply it in the context of the VOF method.","As a result of increasing the order of the interface representation, the reconstruction of the interface and the estimation of its curvature now become a single step instead of two separate ones.","The performance of this new approach is assessed both in terms of accuracy and stability and compared to the classical PLIC-VOF approach on a range of canonical test-cases and cases of surface-tension-driven instabilities."],"url":"http://arxiv.org/abs/2401.15012v1","category":"physics.comp-ph"}
{"created":"2024-01-26 17:09:21","title":"Reinforcement Learning-based Relay Selection for Cooperative WSNs in the Presence of Bursty Impulsive Noise","abstract":"The problem of relay selection is pivotal in the realm of cooperative communication. However, this issue has not been thoroughly examined, particularly when the background noise is assumed to possess an impulsive characteristic with consistent memory as observed in smart grid communications and some other wireless communication scenarios. In this paper, we investigate the impact of this specific type of noise on the performance of cooperative Wireless Sensor Networks (WSNs) with the Decode and Forward (DF) relaying scheme, considering Symbol-Error-Rate (SER) and battery power consumption fairness across all nodes as the performance metrics. We introduce two innovative relay selection methods that depend on noise state detection and the residual battery power of each relay. The first method encompasses the adaptation of the Max-Min criterion to this specific context, whereas the second employs Reinforcement Learning (RL) to surmount this challenge. Our empirical outcomes demonstrate that the impacts of bursty impulsive noise on the SER performance can be effectively mitigated and that a balance in battery power consumption among all nodes can be established using the proposed methods.","sentences":["The problem of relay selection is pivotal in the realm of cooperative communication.","However, this issue has not been thoroughly examined, particularly when the background noise is assumed to possess an impulsive characteristic with consistent memory as observed in smart grid communications and some other wireless communication scenarios.","In this paper, we investigate the impact of this specific type of noise on the performance of cooperative Wireless Sensor Networks (WSNs) with the Decode and Forward (DF) relaying scheme, considering Symbol-Error-Rate (SER) and battery power consumption fairness across all nodes as the performance metrics.","We introduce two innovative relay selection methods that depend on noise state detection and the residual battery power of each relay.","The first method encompasses the adaptation of the Max-Min criterion to this specific context, whereas the second employs Reinforcement Learning (RL) to surmount this challenge.","Our empirical outcomes demonstrate that the impacts of bursty impulsive noise on the SER performance can be effectively mitigated and that a balance in battery power consumption among all nodes can be established using the proposed methods."],"url":"http://arxiv.org/abs/2401.15008v1","category":"cs.NI"}
{"created":"2024-01-26 17:07:08","title":"Airavata: Introducing Hindi Instruction-tuned LLM","abstract":"We announce the initial release of \"Airavata,\" an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.","sentences":["We announce the initial release of \"Airavata,\" an instruction-tuned LLM for Hindi.","Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks.","Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs.","Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi.","Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages.","You can access all artifacts at https://ai4bharat.github.io/airavata."],"url":"http://arxiv.org/abs/2401.15006v1","category":"cs.CL"}
{"created":"2024-01-26 17:05:27","title":"Comparison between two approaches to classify topological insulators using K-theory","abstract":"We compare two approaches which use K-theory for C*-algebras to classify symmetry protected topological phases of quantum systems described in the one particle approximation. In the approach by Kellendonk, which is more abstract and more general, the algebra remains unspecified and the symmetries are defined using gradings and real structures. In the approach by Alldridge et al., the algebra is physically motivated and the symmetries implemented by generators which commute with the Hamiltonian. Both approaches use van Daele's version of K-theory. We show that the second approach is a special case of the first one. We highlight the role played by two of the symmetries: charge conservation and spin rotation symmetry.","sentences":["We compare two approaches which use K-theory for C*-algebras to classify symmetry protected topological phases of quantum systems described in the one particle approximation.","In the approach by Kellendonk, which is more abstract and more general, the algebra remains unspecified and the symmetries are defined using gradings and real structures.","In the approach by Alldridge et al., the algebra is physically motivated and the symmetries implemented by generators which commute with the Hamiltonian.","Both approaches use van Daele's version of K-theory.","We show that the second approach is a special case of the first one.","We highlight the role played by two of the symmetries: charge conservation and spin rotation symmetry."],"url":"http://arxiv.org/abs/2401.15004v1","category":"math.OA"}
{"created":"2024-01-26 17:04:59","title":"Superradiant clouds may be relevant for close compact object binaries","abstract":"Bosonic fields (within suitable mass range) may be collectively generated by rotating black holes through the black hole superradiance process. The resulting black hole is surrounded by a ``cloud\" of particles whose wave function populates the superradiant energy level of the black hole. For comparable mass ratio binary black hole systems, it has been suggested that these clouds are mostly depleted at large binary separations because of a resonant level mixing effect. As a result, these clouds may not be dynamically relevant for black hole and neutron star binaries that enter the LIGO and LISA detection frequency band. In this work, we point out that the common envelope process during a compact binary evolution may bring the binary to $\\sim 0.01$AU in hundreds to thousands of years, so that the resonant level mixing of hyperfine levels are no longer important. We derive a relevant regime of binary parameters where the clouds are still present for binary entering the LISA band. When the binary separation further decreases due to gravitational wave radiation, we discuss the impact of non-resonant level mixing for cloud depletion, as well as possible cloud mass transfer between the binary objects.","sentences":["Bosonic fields (within suitable mass range) may be collectively generated by rotating black holes through the black hole superradiance process.","The resulting black hole is surrounded by a ``cloud\" of particles whose wave function populates the superradiant energy level of the black hole.","For comparable mass ratio binary black hole systems, it has been suggested that these clouds are mostly depleted at large binary separations because of a resonant level mixing effect.","As a result, these clouds may not be dynamically relevant for black hole and neutron star binaries that enter the LIGO and LISA detection frequency band.","In this work, we point out that the common envelope process during a compact binary evolution may bring the binary to $\\sim 0.01$AU in hundreds to thousands of years, so that the resonant level mixing of hyperfine levels are no longer important.","We derive a relevant regime of binary parameters where the clouds are still present for binary entering the LISA band.","When the binary separation further decreases due to gravitational wave radiation, we discuss the impact of non-resonant level mixing for cloud depletion, as well as possible cloud mass transfer between the binary objects."],"url":"http://arxiv.org/abs/2401.15003v1","category":"gr-qc"}
{"created":"2024-01-26 17:03:38","title":"BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning","abstract":"As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total. 3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at http://backdoorbench.com, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.","sentences":["As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race.","However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions).","Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature.","To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench.","Our benchmark makes three valuable contributions to the research community.","1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase.","2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total.","3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning.","We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning.","Finally, we have created a user-friendly website at http://backdoorbench.com, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo."],"url":"http://arxiv.org/abs/2401.15002v1","category":"cs.CV"}
{"created":"2024-01-26 16:55:38","title":"The causal role of the Reddit collective action on the GameStop short squeeze","abstract":"In early 2021, the stock prices of GameStop, AMC, Nokia, and BlackBerry experienced dramatic increases, triggered by short squeeze operations that have been largely attributed to Reddit's retail investors. These events showcased, for the first time, the potential of online social networks to catalyze financial collective action. How, when and to what extent Reddit users played a causal role in driving up these prices, however, remains unclear. To address these questions, we employ causal inference techniques, leveraging data capturing activity on Reddit and Twitter, and trading volume with a high temporal resolution. We find that Reddit discussions foreshadowed trading volume before the GameStop short squeeze, with their predictive power being particularly strong on hourly time scales. This effect emerged abruptly and became prominent a few weeks before the event, but waned once the community of investors gained widespread visibility through Twitter. As the causal link unfolded, the collective investment of the Reddit community, quantified through each user's financial position on GameStop, closely mirrored the market capitalization of the stock. The evidence from our study suggests that Reddit users fueled the GameStop short squeeze, and thereby Reddit served as a coordination hub for a shared financial strategy. Towards the end of January, users talking about GameStop contributed to raise the popularity of BlackBerry, AMC and Nokia, which emerged as the most popular stocks as the community gained global recognition. Overall, our results shed light on the dynamics behind the first large-scale financial collective action driven by social media users.","sentences":["In early 2021, the stock prices of GameStop, AMC, Nokia, and BlackBerry experienced dramatic increases, triggered by short squeeze operations that have been largely attributed to Reddit's retail investors.","These events showcased, for the first time, the potential of online social networks to catalyze financial collective action.","How, when and to what extent Reddit users played a causal role in driving up these prices, however, remains unclear.","To address these questions, we employ causal inference techniques, leveraging data capturing activity on Reddit and Twitter, and trading volume with a high temporal resolution.","We find that Reddit discussions foreshadowed trading volume before the GameStop short squeeze, with their predictive power being particularly strong on hourly time scales.","This effect emerged abruptly and became prominent a few weeks before the event, but waned once the community of investors gained widespread visibility through Twitter.","As the causal link unfolded, the collective investment of the Reddit community, quantified through each user's financial position on GameStop, closely mirrored the market capitalization of the stock.","The evidence from our study suggests that Reddit users fueled the GameStop short squeeze, and thereby Reddit served as a coordination hub for a shared financial strategy.","Towards the end of January, users talking about GameStop contributed to raise the popularity of BlackBerry, AMC and Nokia, which emerged as the most popular stocks as the community gained global recognition.","Overall, our results shed light on the dynamics behind the first large-scale financial collective action driven by social media users."],"url":"http://arxiv.org/abs/2401.14999v1","category":"physics.soc-ph"}
{"created":"2024-01-26 16:52:22","title":"Geometric measure of entanglement of quantum graph states prepared with controlled phase shift operators","abstract":"We consider graph states generated by the action of controlled phase shift operators on a separable state of a multi-qubit system. The case when all the qubits are initially prepared in arbitrary states is investigated. We obtain the geometric measure of entanglement of a qubit with the remaining system in graph states represented by arbitrary weighted graphs and establish its relationship with state parameters. For two-qubit graph states, the geometric measure of entanglement is also quantified on IBM's simulator Qiskit Aer and quantum processor ibmq lima based on auxiliary mean spin measurements. The results of quantum computations verify our analytical predictions.","sentences":["We consider graph states generated by the action of controlled phase shift operators on a separable state of a multi-qubit system.","The case when all the qubits are initially prepared in arbitrary states is investigated.","We obtain the geometric measure of entanglement of a qubit with the remaining system in graph states represented by arbitrary weighted graphs and establish its relationship with state parameters.","For two-qubit graph states, the geometric measure of entanglement is also quantified on IBM's simulator Qiskit Aer and quantum processor ibmq lima based on auxiliary mean spin measurements.","The results of quantum computations verify our analytical predictions."],"url":"http://arxiv.org/abs/2401.14997v1","category":"quant-ph"}
{"created":"2024-01-26 16:52:08","title":"A Resolution-Based Interactive Proof System for UNSAT","abstract":"Modern SAT or QBF solvers are expected to produce correctness certificates. However, certificates have worst-case exponential size (unless $\\textsf{NP}=\\textsf{coNP}$), and at recent SAT competitions the largest certificates of unsatisfiability are starting to reach terabyte size.   Recently, Couillard, Czerner, Esparza, and Majumdar have suggested to replace certificates with interactive proof systems based on the $\\textsf{IP}=\\textsf{PSPACE}$ theorem. They have presented an interactive protocol between a prover and a verifier for an extension of QBF. The overall running time of the protocol is linear in the time needed by a standard BDD-based algorithm, and the time invested by the verifier is polynomial in the size of the formula. (So, in particular, the verifier never has to read or process exponentially long certificates). We call such an interactive protocol competitive with the BDD algorithm for solving QBF.   While BDD-algorithms are state-of-the-art for certain classes of QBF instances, no modern (UN)SAT solver is based on BDDs. For this reason, we initiate the study of interactive certification for more practical SAT algorithms. In particular, we address the question whether interactive protocols can be competitive with some variant of resolution. We present two contributions. First, we prove a theorem that reduces the problem of finding competitive interactive protocols to finding an arithmetisation of formulas satisfying certain commutativity properties. (Arithmetisation is the fundamental technique underlying the $\\textsf{IP}=\\textsf{PSPACE}$ theorem.) Then, we apply the theorem to give the first interactive protocol for the Davis-Putnam resolution procedure.","sentences":["Modern SAT or QBF solvers are expected to produce correctness certificates.","However, certificates have worst-case exponential size (unless $\\textsf{NP}=\\textsf{coNP}$), and at recent SAT competitions the largest certificates of unsatisfiability are starting to reach terabyte size.   ","Recently, Couillard, Czerner, Esparza, and Majumdar have suggested to replace certificates with interactive proof systems based on the $\\textsf{IP}=\\textsf{PSPACE}$ theorem.","They have presented an interactive protocol between a prover and a verifier for an extension of QBF.","The overall running time of the protocol is linear in the time needed by a standard BDD-based algorithm, and the time invested by the verifier is polynomial in the size of the formula.","(So, in particular, the verifier never has to read or process exponentially long certificates).","We call such an interactive protocol competitive with the BDD algorithm for solving QBF.   ","While BDD-algorithms are state-of-the-art for certain classes of QBF instances, no modern (UN)SAT solver is based on BDDs.","For this reason, we initiate the study of interactive certification for more practical SAT algorithms.","In particular, we address the question whether interactive protocols can be competitive with some variant of resolution.","We present two contributions.","First, we prove a theorem that reduces the problem of finding competitive interactive protocols to finding an arithmetisation of formulas satisfying certain commutativity properties.","(Arithmetisation is the fundamental technique underlying the $\\textsf{IP}=\\textsf{PSPACE}$ theorem.)","Then, we apply the theorem to give the first interactive protocol for the Davis-Putnam resolution procedure."],"url":"http://arxiv.org/abs/2401.14996v1","category":"cs.LO"}
{"created":"2024-01-26 16:47:26","title":"Liouvillian Exceptional Points of Non-Hermitian Systems via Quantum Process Tomography","abstract":"Hamiltonian exceptional points (HEPs) are spectral degeneracies of non-Hermitian Hamiltonians describing classical and semiclassical open systems with gain and/or loss. However, this definition overlooks the occurrence of quantum jumps in the evolution of open quantum systems. These quantum effects are properly accounted for by considering Liouvillians and their exceptional points (LEPs) [Minganti et al., Phys. Rev. A {\\bf 100}, 062131 (2019)]. Here, we explicitly describe how standard quantum process tomography, which reveals the dynamics of a quantum system, can be readily applied to reveal and characterize LEPs of non-Hermitian systems. We conducted experiments on an IBM quantum processor to implement a prototype model simulating the decay of a single qubit through three competing channels. Subsequently, we performed tomographic reconstruction of the corresponding experimental Liouvillians and their LEPs using both single- and two-qubit operations. This example underscores the efficacy of process tomography in tuning and observing LEPs, despite the absence of HEPs in the model.","sentences":["Hamiltonian exceptional points (HEPs) are spectral degeneracies of non-Hermitian Hamiltonians describing classical and semiclassical open systems with gain and/or loss.","However, this definition overlooks the occurrence of quantum jumps in the evolution of open quantum systems.","These quantum effects are properly accounted for by considering Liouvillians and their exceptional points (LEPs)","[Minganti et al., Phys.","Rev. A {\\bf 100}, 062131 (2019)].","Here, we explicitly describe how standard quantum process tomography, which reveals the dynamics of a quantum system, can be readily applied to reveal and characterize LEPs of non-Hermitian systems.","We conducted experiments on an IBM quantum processor to implement a prototype model simulating the decay of a single qubit through three competing channels.","Subsequently, we performed tomographic reconstruction of the corresponding experimental Liouvillians and their LEPs using both single- and two-qubit operations.","This example underscores the efficacy of process tomography in tuning and observing LEPs, despite the absence of HEPs in the model."],"url":"http://arxiv.org/abs/2401.14993v1","category":"quant-ph"}
{"created":"2024-01-26 16:42:49","title":"Graph-based Active Learning for Entity Cluster Repair","abstract":"Cluster repair methods aim to determine errors in clusters and modify them so that each cluster consists of records representing the same entity. Current cluster repair methodologies primarily assume duplicate-free data sources, where each record from one source corresponds to a unique record from another. However, real-world data often deviates from this assumption due to quality issues. Recent approaches apply clustering methods in combination with link categorization methods so they can be applied to data sources with duplicates. Nevertheless, the results do not show a clear picture since the quality highly varies depending on the configuration and dataset. In this study, we introduce a novel approach for cluster repair that utilizes graph metrics derived from the underlying similarity graphs. These metrics are pivotal in constructing a classification model to distinguish between correct and incorrect edges. To address the challenge of limited training data, we integrate an active learning mechanism tailored to cluster-specific attributes. The evaluation shows that the method outperforms existing cluster repair methods without distinguishing between duplicate-free or dirty data sources. Notably, our modified active learning strategy exhibits enhanced performance when dealing with datasets containing duplicates, showcasing its effectiveness in such scenarios.","sentences":["Cluster repair methods aim to determine errors in clusters and modify them so that each cluster consists of records representing the same entity.","Current cluster repair methodologies primarily assume duplicate-free data sources, where each record from one source corresponds to a unique record from another.","However, real-world data often deviates from this assumption due to quality issues.","Recent approaches apply clustering methods in combination with link categorization methods so they can be applied to data sources with duplicates.","Nevertheless, the results do not show a clear picture since the quality highly varies depending on the configuration and dataset.","In this study, we introduce a novel approach for cluster repair that utilizes graph metrics derived from the underlying similarity graphs.","These metrics are pivotal in constructing a classification model to distinguish between correct and incorrect edges.","To address the challenge of limited training data, we integrate an active learning mechanism tailored to cluster-specific attributes.","The evaluation shows that the method outperforms existing cluster repair methods without distinguishing between duplicate-free or dirty data sources.","Notably, our modified active learning strategy exhibits enhanced performance when dealing with datasets containing duplicates, showcasing its effectiveness in such scenarios."],"url":"http://arxiv.org/abs/2401.14992v1","category":"cs.LG"}
{"created":"2024-01-26 16:41:11","title":"Stokes graphs of the Rabi problem with real parameters","abstract":"The goal of this paper is to study the geometry of the Stokes graphs associated with the problem, which was introduced by Isidor Rabi in 1937 to model reactions of atoms to the harmonic electric field with frequency close to the natural frequency of the atoms. In the standard Garnier form, the Rabi model is a matrix linear differential equation with three physical parameters, which are: the level of separation of the fermion mode $\\Delta$, the boson-fermion coupling $g$, and the eigenvalue $E$ of the Hamiltonian relevant to this model. The qualitative behavior of solutions of this type of problems is often described in terms of the Stokes graphs of associated quadratic differential, which in the case of Rabi problem can be represented in the form $Q_0(z)\\, dz^2 = -\\frac{z^4+c_3z^3+c_2z^2+c_1z+c_0}{(z-1)^2(z+1)^2}\\, dz^2$ with the coefficients $c_k$, $k=0,1,2,3$, depending on the parameters $\\Delta$, $g$, and $E$. In this paper, we first give a complete classification of possible generic topological types of domain configurations and Stokes graphs of this quadratic differential assuming that its coefficients $c_k$ are real and the zeros of its numerator are distinct from its poles. Then we identify the set of coefficients $(c_3,c_2,c_1,c_0)\\in \\mathbb{R}^4$, which correspond to particular choices of the physical parameters $\\Delta$, $g$, and $E$. The structure of Stokes graphs and domain configurations of quadratic differentials, which appear as asymptotic cases when the parameters of the Rabi problem tend to infinity, also will be discussed.","sentences":["The goal of this paper is to study the geometry of the Stokes graphs associated with the problem, which was introduced by Isidor Rabi in 1937 to model reactions of atoms to the harmonic electric field with frequency close to the natural frequency of the atoms.","In the standard Garnier form, the Rabi model is a matrix linear differential equation with three physical parameters, which are: the level of separation of the fermion mode $\\Delta$, the boson-fermion coupling $g$, and the eigenvalue $E$ of the Hamiltonian relevant to this model.","The qualitative behavior of solutions of this type of problems is often described in terms of the Stokes graphs of associated quadratic differential, which in the case of Rabi problem can be represented in the form $Q_0(z)\\, dz^2 = -\\frac{z^4+c_3z^3+c_2z^2+c_1z+c_0}{(z-1)^2(z+1)^2}\\, dz^2$ with the coefficients $c_k$, $k=0,1,2,3$, depending on the parameters $\\Delta$, $g$, and $E$. In this paper, we first give a complete classification of possible generic topological types of domain configurations and Stokes graphs of this quadratic differential assuming that its coefficients $c_k$ are real and the zeros of its numerator are distinct from its poles.","Then we identify the set of coefficients $(c_3,c_2,c_1,c_0)\\in \\mathbb{R}^4$, which correspond to particular choices of the physical parameters $\\Delta$, $g$, and $E$. The structure of Stokes graphs and domain configurations of quadratic differentials, which appear as asymptotic cases when the parameters of the Rabi problem tend to infinity, also will be discussed."],"url":"http://arxiv.org/abs/2401.14991v1","category":"math-ph"}
{"created":"2024-01-26 16:35:48","title":"Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline Free Knot Placement Algorithm","abstract":"We propose a novel approach to nonlinear functional regression, called the Mapping-to-Parameter function model, which addresses complex and nonlinear functional regression problems in parameter space by employing any supervised learning technique. Central to this model is the mapping of function data from an infinite-dimensional function space to a finite-dimensional parameter space. This is accomplished by concurrently approximating multiple functions with a common set of B-spline basis functions by any chosen order, with their knot distribution determined by the Iterative Local Placement Algorithm, a newly proposed free knot placement algorithm. In contrast to the conventional equidistant knot placement strategy that uniformly distributes knot locations based on a predefined number of knots, our proposed algorithms determine knot location according to the local complexity of the input or output functions. The performance of our knot placement algorithms is shown to be robust in both single-function approximation and multiple-function approximation contexts. Furthermore, the effectiveness and advantage of the proposed prediction model in handling both function-on-scalar regression and function-on-function regression problems are demonstrated through several real data applications, in comparison with four groups of state-of-the-art methods.","sentences":["We propose a novel approach to nonlinear functional regression, called the Mapping-to-Parameter function model, which addresses complex and nonlinear functional regression problems in parameter space by employing any supervised learning technique.","Central to this model is the mapping of function data from an infinite-dimensional function space to a finite-dimensional parameter space.","This is accomplished by concurrently approximating multiple functions with a common set of B-spline basis functions by any chosen order, with their knot distribution determined by the Iterative Local Placement Algorithm, a newly proposed free knot placement algorithm.","In contrast to the conventional equidistant knot placement strategy that uniformly distributes knot locations based on a predefined number of knots, our proposed algorithms determine knot location according to the local complexity of the input or output functions.","The performance of our knot placement algorithms is shown to be robust in both single-function approximation and multiple-function approximation contexts.","Furthermore, the effectiveness and advantage of the proposed prediction model in handling both function-on-scalar regression and function-on-function regression problems are demonstrated through several real data applications, in comparison with four groups of state-of-the-art methods."],"url":"http://arxiv.org/abs/2401.14989v1","category":"cs.LG"}
{"created":"2024-01-26 16:25:56","title":"Integrability and chaos in the quantum brachistochrone problem","abstract":"The quantum brachistochrone problem addresses the fundamental challenge of achieving the quantum speed limit in applications aiming to realize a given unitary operation in a quantum system. Specifically, it looks into optimization of the transformation of quantum states through controlled Hamiltonians, which form a small subset in the space of the system's observables. Here we introduce a broad family of completely integrable brachistochrone protocols, which arise from a judicious choice of the control Hamiltonian subset. Furthermore, we demonstrate how the inherent stability of the completely integrable protocols makes them numerically tractable and therefore practicable as opposed to their non-integrable counterparts.","sentences":["The quantum brachistochrone problem addresses the fundamental challenge of achieving the quantum speed limit in applications aiming to realize a given unitary operation in a quantum system.","Specifically, it looks into optimization of the transformation of quantum states through controlled Hamiltonians, which form a small subset in the space of the system's observables.","Here we introduce a broad family of completely integrable brachistochrone protocols, which arise from a judicious choice of the control Hamiltonian subset.","Furthermore, we demonstrate how the inherent stability of the completely integrable protocols makes them numerically tractable and therefore practicable as opposed to their non-integrable counterparts."],"url":"http://arxiv.org/abs/2401.14986v1","category":"quant-ph"}
{"created":"2024-01-26 16:16:51","title":"Quota management in dCache or making a perfectly normal file system normal","abstract":"dCache (https://dcache.org) is a highly scalable storage system providing location-independent access to data. The data are stored across multiple data servers as complete files presented to the end-user via a single-rooted namespace. From its inception, dCache has been designed as a caching disk buffer to a tertiary tape storage system with the assumption that the latter has virtually unlimited capacity. dCache can also be configured as a disk-only storage system with no tape backend. Owing to the idea that a tape resource is infinite, or purely physically limited by budget considerations, the system has never provided for any restrictions on how much data can be stored on tape. Likewise, in the disk-only configuration, the capacity of the system is only limited by the aggregate disk capacity of the data servers. In a multi-user environment, however, this has become problematic. This presentation will describe the design and implementation of a user- and group-based quota system, that allows to manage tape and disk space allocations, as part of dCache namespace.","sentences":["dCache (https://dcache.org) is a highly scalable storage system providing location-independent access to data.","The data are stored across multiple data servers as complete files presented to the end-user via a single-rooted namespace.","From its inception, dCache has been designed as a caching disk buffer to a tertiary tape storage system with the assumption that the latter has virtually unlimited capacity.","dCache can also be configured as a disk-only storage system with no tape backend.","Owing to the idea that a tape resource is infinite, or purely physically limited by budget considerations, the system has never provided for any restrictions on how much data can be stored on tape.","Likewise, in the disk-only configuration, the capacity of the system is only limited by the aggregate disk capacity of the data servers.","In a multi-user environment, however, this has become problematic.","This presentation will describe the design and implementation of a user- and group-based quota system, that allows to manage tape and disk space allocations, as part of dCache namespace."],"url":"http://arxiv.org/abs/2401.14983v1","category":"cs.DB"}
{"created":"2024-01-26 16:11:00","title":"Creating a vulnerable node based on the vulnerability MS17-010","abstract":"The creation of a vulnerable node has been demonstrated through the analysis and implementation of the MS17-010 (CVE-2017-0144) vulnerability, affecting the SMBv1 protocol on various Windows operating systems. The principle and methodology of exploiting the vulnerability are described, with a formalized representation of the exploitation in the form of a Meta Attack Language (MAL) graph. Additionally, the attacker's implementation is outlined as the execution of an automated script in Python using the Metasploit Framework. Basic security measures for systems utilizing the SMBv1 protocol are provided.","sentences":["The creation of a vulnerable node has been demonstrated through the analysis and implementation of the MS17-010 (CVE-2017-0144) vulnerability, affecting the SMBv1 protocol on various Windows operating systems.","The principle and methodology of exploiting the vulnerability are described, with a formalized representation of the exploitation in the form of a Meta Attack Language (MAL) graph.","Additionally, the attacker's implementation is outlined as the execution of an automated script in Python using the Metasploit Framework.","Basic security measures for systems utilizing the SMBv1 protocol are provided."],"url":"http://arxiv.org/abs/2401.14979v1","category":"cs.CR"}
{"created":"2024-01-26 16:09:18","title":"Robust Dual-Modal Speech Keyword Spotting for XR Headsets","abstract":"While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby. These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information. Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets. We devise two different modal fusion approches and conduct experiments to test the system's performance across diverse scenarios. The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances. Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results. The code is available at https://github.com/caizhuojiang/VE-KWS.","sentences":["While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby.","These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information.","Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets.","We devise two different modal fusion approches and conduct experiments to test the system's performance across diverse scenarios.","The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances.","Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results.","The code is available at https://github.com/caizhuojiang/VE-KWS."],"url":"http://arxiv.org/abs/2401.14978v1","category":"cs.HC"}
{"created":"2024-01-26 16:07:50","title":"Spin Noise Spectroscopy of a Single Spin using Single Detected Photons","abstract":"Spin noise spectroscopy has become a widespread technique to extract information on spin dynamics in atomic and solid-state systems, in a potentially non-invasive way, through the optical probing of spin fluctuations. Here we experimentally demonstrate a new approach in spin noise spectroscopy, based on the detection of single photons. Due to the large spin-dependent polarization rotations provided by a deterministically-coupled quantum dot-micropillar device, giant spin noise signals induced by a single-hole spin are extracted in the form of photon-photon cross-correlations. Ultimately, such a technique can be extended to an ultrafast regime probing mechanisms down to few tens of picoseconds.","sentences":["Spin noise spectroscopy has become a widespread technique to extract information on spin dynamics in atomic and solid-state systems, in a potentially non-invasive way, through the optical probing of spin fluctuations.","Here we experimentally demonstrate a new approach in spin noise spectroscopy, based on the detection of single photons.","Due to the large spin-dependent polarization rotations provided by a deterministically-coupled quantum dot-micropillar device, giant spin noise signals induced by a single-hole spin are extracted in the form of photon-photon cross-correlations.","Ultimately, such a technique can be extended to an ultrafast regime probing mechanisms down to few tens of picoseconds."],"url":"http://arxiv.org/abs/2401.14976v1","category":"quant-ph"}
{"created":"2024-01-26 16:07:42","title":"Embedding-based search in JetBrains IDEs","abstract":"Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project. In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point. However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos. In this work, we describe the machine learning approach we implemented to improve the discoverability of search items. We also share the obstacles encountered during this process and how we overcame them.","sentences":["Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project.","In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point.","However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos.","In this work, we describe the machine learning approach we implemented to improve the discoverability of search items.","We also share the obstacles encountered during this process and how we overcame them."],"url":"http://arxiv.org/abs/2401.14975v1","category":"cs.SE"}
{"created":"2024-01-26 16:06:01","title":"Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models","abstract":"We seek to model a collection of time series arising from multiple entities interacting over the same time period. Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of individual time series. This is asymptotically no more costly than fitting separate models for each entity. Experiments on synthetic and real datasets show that our model can produce better forecasts of future entity behavior than existing methods. Moreover, the availability of latent state chains at both the entity and system level enables interpretation of group dynamics.","sentences":["We seek to model a collection of time series arising from multiple entities interacting over the same time period.","Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities.","To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics.","We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series.","Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions.","Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of individual time series.","This is asymptotically no more costly than fitting separate models for each entity.","Experiments on synthetic and real datasets show that our model can produce better forecasts of future entity behavior than existing methods.","Moreover, the availability of latent state chains at both the entity and system level enables interpretation of group dynamics."],"url":"http://arxiv.org/abs/2401.14973v1","category":"stat.ML"}
{"created":"2024-01-26 16:05:47","title":"\"It's Sink or Swim'': Exploring Patients' Challenges and Tool Needs for Self-Management of Postoperative Acute Pain","abstract":"Poorly managed postoperative acute pain can have long-lasting negative impacts and pose a major healthcare issue. There is limited investigation to understand and address the unique needs of patients experiencing acute pain. In this paper, we tackle this gap through an interview study with 14 patients who recently underwent postoperative acute pain to understand their challenges in pain self-management and their need for supportive tools. Our analysis identified various factors associated with the major aspects of acute pain self-management. Together, our findings indicated that tools for supporting these patients need to carefully consider information and support delivery to adapt to rapid changes in pain experiences, offer personalized and dynamic assistance that adapts to individual situations in context, and monitor emotion when promoting motivation. Overall, our work provided valuable knowledge to address the less-investigated but highly-needed problem of designing technology for the self-management of acute pain and similar health conditions.","sentences":["Poorly managed postoperative acute pain can have long-lasting negative impacts and pose a major healthcare issue.","There is limited investigation to understand and address the unique needs of patients experiencing acute pain.","In this paper, we tackle this gap through an interview study with 14 patients who recently underwent postoperative acute pain to understand their challenges in pain self-management and their need for supportive tools.","Our analysis identified various factors associated with the major aspects of acute pain self-management.","Together, our findings indicated that tools for supporting these patients need to carefully consider information and support delivery to adapt to rapid changes in pain experiences, offer personalized and dynamic assistance that adapts to individual situations in context, and monitor emotion when promoting motivation.","Overall, our work provided valuable knowledge to address the less-investigated but highly-needed problem of designing technology for the self-management of acute pain and similar health conditions."],"url":"http://arxiv.org/abs/2401.14972v1","category":"cs.HC"}
{"created":"2024-01-26 16:01:09","title":"Atmosphere: Context and situational-aware collaborative IoT architecture for edge-fog-cloud computing","abstract":"The Internet of Things (IoT) has grown significantly in popularity, accompanied by increased capacity and lower cost of communications, and overwhelming development of technologies. At the same time, big data and real-time data analysis have taken on great importance and have been accompanied by unprecedented interest in sharing data among citizens, public administrations and other organisms, giving rise to what is known as the Collaborative Internet of Things. This growth in data and infrastructure must be accompanied by a software architecture that allows its exploitation. Although there are various proposals focused on the exploitation of the IoT at edge, fog and/or cloud levels, it is not easy to find a software solution that exploits the three tiers together, taking maximum advantage not only of the analysis of contextual and situational data at each tier, but also of two-way communications between adjacent ones. In this paper, we propose an architecture that solves these deficiencies by proposing novel technologies which are appropriate for managing the resources of each tier: edge, fog and cloud. In addition, the fact that two-way communications along the three tiers of the architecture is allowed considerably enriches the contextual and situational information in each layer, and substantially assists decision making in real time. The paper illustrates the proposed software architecture through a case study of respiratory disease surveillance in hospitals. As a result, the proposed architecture permits efficient communications between the different tiers responding to the needs of these types of IoT scenarios.","sentences":["The Internet of Things (IoT) has grown significantly in popularity, accompanied by increased capacity and lower cost of communications, and overwhelming development of technologies.","At the same time, big data and real-time data analysis have taken on great importance and have been accompanied by unprecedented interest in sharing data among citizens, public administrations and other organisms, giving rise to what is known as the Collaborative Internet of Things.","This growth in data and infrastructure must be accompanied by a software architecture that allows its exploitation.","Although there are various proposals focused on the exploitation of the IoT at edge, fog and/or cloud levels, it is not easy to find a software solution that exploits the three tiers together, taking maximum advantage not only of the analysis of contextual and situational data at each tier, but also of two-way communications between adjacent ones.","In this paper, we propose an architecture that solves these deficiencies by proposing novel technologies which are appropriate for managing the resources of each tier: edge, fog and cloud.","In addition, the fact that two-way communications along the three tiers of the architecture is allowed considerably enriches the contextual and situational information in each layer, and substantially assists decision making in real time.","The paper illustrates the proposed software architecture through a case study of respiratory disease surveillance in hospitals.","As a result, the proposed architecture permits efficient communications between the different tiers responding to the needs of these types of IoT scenarios."],"url":"http://arxiv.org/abs/2401.14968v1","category":"cs.DC"}
{"created":"2024-01-26 16:01:06","title":"Superradiant instability of a charged regular black hole","abstract":"We show that a charged, massive scalar field in the vicinity of an electrically-charged Ay\\'on-Beato-Garc\\'ia (ABG) regular black hole has a spectrum of quasibound states that (in a certain parameter regime) grow exponentially with time, due to black hole superradiance. Superradiant quasibound states are made possible by the enhancement of the electrostatic potential at the horizon in nonlinear electrodynamics; in contrast, the Reissner-Nordstr\\\"om black hole does not possess such superradiant quasibound states. Here we compute the spectrum for a range of multipoles $\\ell$ across the parameter space, and we find the fastest growth rate in the monopole mode. We find that a regular black hole with a small charge can still trigger a significant superradiant instability if the charge-to-mass ratio of the field is compensatingly large. Finally, we consider the stationary bound states at the superradiant threshold, and we conjecture that, due to this instability, the ABG black hole will evolve towards a configuration with charged scalar hair.","sentences":["We show that a charged, massive scalar field in the vicinity of an electrically-charged Ay\\'on-Beato-Garc\\'ia (ABG) regular black hole has a spectrum of quasibound states that (in a certain parameter regime) grow exponentially with time, due to black hole superradiance.","Superradiant quasibound states are made possible by the enhancement of the electrostatic potential at the horizon in nonlinear electrodynamics; in contrast, the Reissner-Nordstr\\\"om black hole does not possess such superradiant quasibound states.","Here we compute the spectrum for a range of multipoles $\\ell$ across the parameter space, and we find the fastest growth rate in the monopole mode.","We find that a regular black hole with a small charge can still trigger a significant superradiant instability if the charge-to-mass ratio of the field is compensatingly large.","Finally, we consider the stationary bound states at the superradiant threshold, and we conjecture that, due to this instability, the ABG black hole will evolve towards a configuration with charged scalar hair."],"url":"http://arxiv.org/abs/2401.14967v1","category":"gr-qc"}
{"created":"2024-01-26 15:58:57","title":"Masked Pre-trained Model Enables Universal Zero-shot Denoiser","abstract":"In this work, we observe that the model, which is trained on vast general images using masking strategy, has been naturally embedded with the distribution knowledge regarding natural images, and thus spontaneously attains the underlying potential for strong image denoising. Based on this observation, we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it for denoising of a single image with unseen noise degradation. Concretely, the proposed MPI comprises two key procedures: 1) Masked Pre-training involves training a model on multiple natural images with random masks to gather generalizable representations, allowing for practical applications in varying noise degradation and even in distinct image types. 2) Iterative filling is devised to efficiently fuse pre-trained knowledge for denoising. Similar to but distinct from pre-training, random masking is retained to bridge the gap, but only the predicted parts covered by masks are assembled for efficiency, which enables high-quality denoising within a limited number of iterations. Comprehensive experiments across various noisy scenarios underscore the notable advances of proposed MPI over previous approaches with a marked reduction in inference time. Code is available at https://github.com/krennic999/MPI.git.","sentences":["In this work, we observe that the model, which is trained on vast general images using masking strategy, has been naturally embedded with the distribution knowledge regarding natural images, and thus spontaneously attains the underlying potential for strong image denoising.","Based on this observation, we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI).","MPI pre-trains a model with masking and fine-tunes it for denoising of a single image with unseen noise degradation.","Concretely, the proposed MPI comprises two key procedures: 1) Masked Pre-training involves training a model on multiple natural images with random masks to gather generalizable representations, allowing for practical applications in varying noise degradation and even in distinct image types.","2) Iterative filling is devised to efficiently fuse pre-trained knowledge for denoising.","Similar to but distinct from pre-training, random masking is retained to bridge the gap, but only the predicted parts covered by masks are assembled for efficiency, which enables high-quality denoising within a limited number of iterations.","Comprehensive experiments across various noisy scenarios underscore the notable advances of proposed MPI over previous approaches with a marked reduction in inference time.","Code is available at https://github.com/krennic999/MPI.git."],"url":"http://arxiv.org/abs/2401.14966v1","category":"cs.CV"}
{"created":"2024-01-26 15:58:47","title":"An Improved Lower Bound on Oblivious Transfer Capacity via Interactive Erasure Emulation","abstract":"We revisit the oblivious transfer (OT) capacities of noisy channels against the passive adversary, which have been identified only for a limited class of channels. In the literature, the general construction of oblivious transfer has been known only for generalized erasure channels (GECs); for other channels, we first convert a given channel to a GEC via alphabet extension and erasure emulation, and then apply the general construction for GEC. In this paper, we derive an improved lower bound on the OT capacity of the binary symmetric channel (BSC) and binary symmetric erasure channel (BSEC) by proposing a new protocol; by using interactive communication between the sender and the receiver, our protocol emulates erasure events recursively in multiple rounds. We also discuss a potential necessity of multiple rounds interactive communication to attain the OT capacity.","sentences":["We revisit the oblivious transfer (OT) capacities of noisy channels against the passive adversary, which have been identified only for a limited class of channels.","In the literature, the general construction of oblivious transfer has been known only for generalized erasure channels (GECs); for other channels, we first convert a given channel to a GEC via alphabet extension and erasure emulation, and then apply the general construction for GEC.","In this paper, we derive an improved lower bound on the OT capacity of the binary symmetric channel (BSC) and binary symmetric erasure channel (BSEC) by proposing a new protocol; by using interactive communication between the sender and the receiver, our protocol emulates erasure events recursively in multiple rounds.","We also discuss a potential necessity of multiple rounds interactive communication to attain the OT capacity."],"url":"http://arxiv.org/abs/2401.14965v1","category":"cs.IT"}
{"created":"2024-01-26 15:55:24","title":"AiRLIHockey: Highly Reactive Contact Control and Stochastic Optimal Shooting","abstract":"Air hockey is a highly reactive game which requires the player to quickly reason over stochastic puck and contact dynamics. We implement a hierarchical framework which combines stochastic optimal control for planning shooting angles and sampling-based model-predictive control for continuously generating constrained mallet trajectories. Our agent was deployed and evaluated in simulation and on a physical setup as part of the Robot Air-Hockey challenge competition at NeurIPS 2023.","sentences":["Air hockey is a highly reactive game which requires the player to quickly reason over stochastic puck and contact dynamics.","We implement a hierarchical framework which combines stochastic optimal control for planning shooting angles and sampling-based model-predictive control for continuously generating constrained mallet trajectories.","Our agent was deployed and evaluated in simulation and on a physical setup as part of the Robot Air-Hockey challenge competition at NeurIPS 2023."],"url":"http://arxiv.org/abs/2401.14964v1","category":"cs.RO"}
{"created":"2024-01-26 15:54:35","title":"On the Hardness of Gray Code Problems for Combinatorial Objects","abstract":"Can a list of binary strings be ordered so that consecutive strings differ in a single bit? Can a list of permutations be ordered so that consecutive permutations differ by a swap? Can a list of non-crossing set partitions be ordered so that consecutive partitions differ by refinement? These are examples of Gray coding problems: Can a list of combinatorial objects (of a particular type and size) be ordered so that consecutive objects differ by a flip (of a particular type)? For example, 000, 001, 010, 100 is a no instance of the first question, while 1234, 1324, 1243 is a yes instance of the second question due to the order 1243, 1234, 1324. We prove that a variety of Gray coding problems are NP-complete using a new tool we call a Gray code reduction.","sentences":["Can a list of binary strings be ordered so that consecutive strings differ in a single bit?","Can a list of permutations be ordered so that consecutive permutations differ by a swap?","Can a list of non-crossing set partitions be ordered so that consecutive partitions differ by refinement?","These are examples of Gray coding problems: Can a list of combinatorial objects (of a particular type and size) be ordered so that consecutive objects differ by a flip (of a particular type)?","For example, 000, 001, 010, 100 is a no instance of the first question, while 1234, 1324, 1243 is a yes instance of the second question due to the order 1243, 1234, 1324.","We prove that a variety of Gray coding problems are NP-complete using a new tool we call a Gray code reduction."],"url":"http://arxiv.org/abs/2401.14963v1","category":"cs.DM"}
{"created":"2024-01-26 15:53:51","title":"Joint Data and Semantics Lossy Compression: Nonasymptotic and Second-Order Achievability Bounds","abstract":"This paper studies a joint data and semantics lossy compression problem in the finite blocklength regime, where the data and semantic sources are correlated, and only the data source can be observed by the encoder. We first introduce an information-theoretic nonasymptotic analysis framework to investigate the nonasymptotic fundamental limits of our studied problem. Within this framework, general nonasymptotic achievability bounds valid for general sources and distortion measures are derived. Moreover, we provide a second-order achievability bound in the standard block coding setting by applying the two-dimensional Berry-Esseen theorem to our nonasymptotic bounds. Compared with first-order asymptotic bounds, our results have the potential to provide unique insights for the design of practical semantic communication systems.","sentences":["This paper studies a joint data and semantics lossy compression problem in the finite blocklength regime, where the data and semantic sources are correlated, and only the data source can be observed by the encoder.","We first introduce an information-theoretic nonasymptotic analysis framework to investigate the nonasymptotic fundamental limits of our studied problem.","Within this framework, general nonasymptotic achievability bounds valid for general sources and distortion measures are derived.","Moreover, we provide a second-order achievability bound in the standard block coding setting by applying the two-dimensional Berry-Esseen theorem to our nonasymptotic bounds.","Compared with first-order asymptotic bounds, our results have the potential to provide unique insights for the design of practical semantic communication systems."],"url":"http://arxiv.org/abs/2401.14962v1","category":"cs.IT"}
{"created":"2024-01-26 15:52:41","title":"End-To-End Set-Based Training for Neural Network Verification","abstract":"Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trained neural networks outperform neural networks trained with state-of-the-art adversarial attacks.","sentences":["Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network.","Safety-critical environments require neural networks that are robust against input perturbations.","However, training and formally verifying robust neural networks is challenging.","We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification.","Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network.","While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs.","Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify.","In many cases, set-based trained neural networks outperform neural networks trained with state-of-the-art adversarial attacks."],"url":"http://arxiv.org/abs/2401.14961v1","category":"cs.LG"}
{"created":"2024-01-26 15:46:34","title":"Declassification Policy for Program Complexity Analysis","abstract":"In automated complexity analysis, noninterference-based type systems statically guarantee, via soundness, the property that well-typed programs compute functions of a given complexity class, e.g., the class FP of functions computable in polynomial time. These characterizations are also extensionally complete -- they capture all functions -- but are not intensionally complete as some polytime algorithms are rejected. This impact on expressive power is an unavoidable cost of achieving a tractable characterization. To overcome this issue, an avenue arising from security applications is to find a relaxation of noninterference based on a declassification mechanism that allows critical data to be released in a safe and controlled manner. Following this path, we present a new and intuitive declassification policy preserving FP-soundness and capturing strictly more programs than existing noninterference-based systems. We show the versatility of the approach: it also provides a new characterization of the class BFF of second-order polynomial time computable functions in a second-order imperative language, with first-order procedure calls. Type inference is tractable: it can be done in polynomial time.","sentences":["In automated complexity analysis, noninterference-based type systems statically guarantee, via soundness, the property that well-typed programs compute functions of a given complexity class, e.g., the class FP of functions computable in polynomial time.","These characterizations are also extensionally complete -- they capture all functions -- but are not intensionally complete as some polytime algorithms are rejected.","This impact on expressive power is an unavoidable cost of achieving a tractable characterization.","To overcome this issue, an avenue arising from security applications is to find a relaxation of noninterference based on a declassification mechanism that allows critical data to be released in a safe and controlled manner.","Following this path, we present a new and intuitive declassification policy preserving FP-soundness and capturing strictly more programs than existing noninterference-based systems.","We show the versatility of the approach: it also provides a new characterization of the class BFF of second-order polynomial time computable functions in a second-order imperative language, with first-order procedure calls.","Type inference is tractable: it can be done in polynomial time."],"url":"http://arxiv.org/abs/2401.14957v1","category":"cs.LO"}
{"created":"2024-01-26 15:42:52","title":"Non-Relativistic M2-Branes and the AdS/CFT Correspondence","abstract":"A non-relativistic limit of the AdS/CFT correspondence is studied in the context of M2-branes. On the field theory side this corresponds to a near-BPS limit of ABJM that localises onto solutions of Hitchin's equations. It is shown that the symmetries of the theory include an infinite-dimensional enhancement of the spatial symmetry algebra corresponding to time-dependent holomorphic transformations. Taking the limit of the gravitational dual splits the geometry into three 'large' directions and eight 'small' directions, which for the near-horizon limit of the M2-brane metric has the effect of reducing the $AdS_4$ factor to an $AdS_2$ factor. Evidence is presented that the duality is maintained after the limit.","sentences":["A non-relativistic limit of the AdS/CFT correspondence is studied in the context of M2-branes.","On the field theory side this corresponds to a near-BPS limit of ABJM that localises onto solutions of Hitchin's equations.","It is shown that the symmetries of the theory include an infinite-dimensional enhancement of the spatial symmetry algebra corresponding to time-dependent holomorphic transformations.","Taking the limit of the gravitational dual splits the geometry into three 'large' directions and eight 'small' directions, which for the near-horizon limit of the M2-brane metric has the effect of reducing the $AdS_4$ factor to an $AdS_2$ factor.","Evidence is presented that the duality is maintained after the limit."],"url":"http://arxiv.org/abs/2401.14955v1","category":"hep-th"}
{"created":"2024-01-26 15:37:16","title":"Learning Universal Predictors","abstract":"Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies.","sentences":["Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data.","Broad exposure to different tasks leads to versatile representations enabling general problem solving.","But, what are the limits of meta-learning?","In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits.","We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns.","We provide theoretical analysis of the UTM data generation processes and meta-training protocols.","We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality.","Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies."],"url":"http://arxiv.org/abs/2401.14953v1","category":"cs.LG"}
{"created":"2024-01-26 15:33:39","title":"Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training","abstract":"Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating \"robust overfitting\". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.","sentences":["Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization.","To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting.","Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity.","We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights.","Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios.","It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating \"robust overfitting\".","Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research."],"url":"http://arxiv.org/abs/2401.14948v1","category":"cs.LG"}
{"created":"2024-01-26 15:31:07","title":"Confinement Induced Resonances in Spherical Shell Traps","abstract":"We have computed exactly the energy spectrum and corresponding wave functions of two bosonic particles, which are confined in a spherically symmetric shell-shaped trap and interact with each other via a three-dimensional zero-range potential. Confinement induced resonances (CIRs), originating entirely from the strong coupling of the relative and center-of-mass motions of the two particles, are identified as avoided crossings at certain values of the shell radius. By working close to the found CIRs, these results offer a new way to enhance the atom-atom interaction in the atomic gas by tuning only the geometrical parameters of the shell.","sentences":["We have computed exactly the energy spectrum and corresponding wave functions of two bosonic particles, which are confined in a spherically symmetric shell-shaped trap and interact with each other via a three-dimensional zero-range potential.","Confinement induced resonances (CIRs), originating entirely from the strong coupling of the relative and center-of-mass motions of the two particles, are identified as avoided crossings at certain values of the shell radius.","By working close to the found CIRs, these results offer a new way to enhance the atom-atom interaction in the atomic gas by tuning only the geometrical parameters of the shell."],"url":"http://arxiv.org/abs/2401.14946v1","category":"quant-ph"}
{"created":"2024-01-26 15:25:27","title":"Analysing the Influence of Macroeconomic Factors on Credit Risk in the UK Banking Sector","abstract":"Macroeconomic factors have a critical impact on banking credit risk, which cannot be directly controlled by banks, and therefore, there is a need for an early credit risk warning system based on the macroeconomy. By comparing different predictive models (traditional statistical and machine learning algorithms), this study aims to examine the macroeconomic determinants impact on the UK banking credit risk and assess the most accurate credit risk estimate using predictive analytics. This study found that the variance-based multi-split decision tree algorithm is the most precise predictive model with interpretable, reliable, and robust results. Our model performance achieved 95% accuracy and evidenced that unemployment and inflation rate are significant credit risk predictors in the UK banking context. Our findings provided valuable insights such as a positive association between credit risk and inflation, the unemployment rate, and national savings, as well as a negative relationship between credit risk and national debt, total trade deficit, and national income. In addition, we empirically showed the relationship between national savings and non-performing loans, thus proving the paradox of thrift. These findings benefit the credit risk management team in monitoring the macroeconomic factors thresholds and implementing critical reforms to mitigate credit risk.","sentences":["Macroeconomic factors have a critical impact on banking credit risk, which cannot be directly controlled by banks, and therefore, there is a need for an early credit risk warning system based on the macroeconomy.","By comparing different predictive models (traditional statistical and machine learning algorithms), this study aims to examine the macroeconomic determinants impact on the UK banking credit risk and assess the most accurate credit risk estimate using predictive analytics.","This study found that the variance-based multi-split decision tree algorithm is the most precise predictive model with interpretable, reliable, and robust results.","Our model performance achieved 95% accuracy and evidenced that unemployment and inflation rate are significant credit risk predictors in the UK banking context.","Our findings provided valuable insights such as a positive association between credit risk and inflation, the unemployment rate, and national savings, as well as a negative relationship between credit risk and national debt, total trade deficit, and national income.","In addition, we empirically showed the relationship between national savings and non-performing loans, thus proving the paradox of thrift.","These findings benefit the credit risk management team in monitoring the macroeconomic factors thresholds and implementing critical reforms to mitigate credit risk."],"url":"http://arxiv.org/abs/2401.14943v1","category":"cs.IR"}
{"created":"2024-01-26 15:24:48","title":"Noise-like analytic properties of imaginary chaos","abstract":"In this note we continue the study of imaginary multiplicative chaos $\\mu_\\beta := \\exp(i \\beta \\Gamma)$, where $\\Gamma$ is a two-dimensional continuum Gaussian free field. We concentrate here on the fine-scale analytic properties of $|\\mu_\\beta(Q(x,r))|$ as $r \\to 0$, where $Q(x,r)$ is a square of side-length $2r$ centred at $x$. More precisely, we prove monofractality of this process, a law of the iterated logarithm as $r \\to 0$ and analyse its exceptional points, which have a close connection to fast points of Brownian motion. Some of the technical ideas developed to address these questions also help us pin down the exact Besov regularity of imaginary chaos, a question left open in [JSW20]. All the mentioned properties illustrate the noise-like behaviour of the imaginary chaos. We conclude by proving that the processes $x \\mapsto |\\mu_\\beta(Q(x,r))|^2$, when normalised additively and multiplicatively, converge as $r \\to 0$ in law, but not in probability, to white noise; this suggests that all the information of the multiplicative chaos is contained in the angular parts of $\\mu_\\beta(Q(x,r))$.","sentences":["In this note we continue the study of imaginary multiplicative chaos $\\mu_\\beta := \\exp(i \\beta \\Gamma)$, where $\\Gamma$ is a two-dimensional continuum Gaussian free field.","We concentrate here on the fine-scale analytic properties of $|\\mu_\\beta(Q(x,r))|$ as $r \\to 0$, where $Q(x,r)$ is a square of side-length $2r$ centred at $x$. More precisely, we prove monofractality of this process, a law of the iterated logarithm as $r \\to 0$ and analyse its exceptional points, which have a close connection to fast points of Brownian motion.","Some of the technical ideas developed to address these questions also help us pin down the exact Besov regularity of imaginary chaos, a question left open in [JSW20].","All the mentioned properties illustrate the noise-like behaviour of the imaginary chaos.","We conclude by proving that the processes $x \\mapsto |\\mu_\\beta(Q(x,r))|^2$, when normalised additively and multiplicatively, converge as $r \\to 0$ in law, but not in probability, to white noise; this suggests that all the information of the multiplicative chaos is contained in the angular parts of $\\mu_\\beta(Q(x,r))$."],"url":"http://arxiv.org/abs/2401.14942v1","category":"math.PR"}
{"created":"2024-01-26 15:23:25","title":"Macro Graph Neural Networks for Online Billion-Scale Recommender Systems","abstract":"Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation graph\" and introduce a more suitable MAcro Recommendation Graph (MAG) for billion-scale recommendations. MAG resolves the computational complexity problems in the infrastructure by reducing the node count from billions to hundreds. Specifically, MAG groups micro nodes (users and items) with similar behavior patterns to form macro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.","sentences":["Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors.","To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations.","However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns.","To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation graph\" and introduce a more suitable MAcro Recommendation Graph (MAG) for billion-scale recommendations.","MAG resolves the computational complexity problems in the infrastructure by reducing the node count from billions to hundreds.","Specifically, MAG groups micro nodes (users and items) with similar behavior patterns to form macro nodes.","Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes.","MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users.","Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient.","Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems."],"url":"http://arxiv.org/abs/2401.14939v1","category":"cs.IR"}
{"created":"2024-01-26 15:22:06","title":"DAM: Diffusion Activation Maximization for 3D Global Explanations","abstract":"In recent years, the performance of point cloud models has been rapidly improved. However, due to the limited amount of relevant explainability studies, the unreliability and opacity of these black-box models may lead to potential risks in applications where human lives are at stake, e.g. autonomous driving or healthcare. This work proposes a DDPM-based point cloud global explainability method (DAM) that leverages Point Diffusion Transformer (PDT), a novel point-wise symmetric model, with dual-classifier guidance to generate high-quality global explanations. In addition, an adapted path gradient integration method for DAM is proposed, which not only provides a global overview of the saliency maps for point cloud categories, but also sheds light on how the attributions of the explanations vary during the generation process. Extensive experiments indicate that our method outperforms existing ones in terms of perceptibility, representativeness, and diversity, with a significant reduction in generation time. Our code is available at: https://github.com/Explain3D/DAM","sentences":["In recent years, the performance of point cloud models has been rapidly improved.","However, due to the limited amount of relevant explainability studies, the unreliability and opacity of these black-box models may lead to potential risks in applications where human lives are at stake, e.g. autonomous driving or healthcare.","This work proposes a DDPM-based point cloud global explainability method (DAM) that leverages Point Diffusion Transformer (PDT), a novel point-wise symmetric model, with dual-classifier guidance to generate high-quality global explanations.","In addition, an adapted path gradient integration method for DAM is proposed, which not only provides a global overview of the saliency maps for point cloud categories, but also sheds light on how the attributions of the explanations vary during the generation process.","Extensive experiments indicate that our method outperforms existing ones in terms of perceptibility, representativeness, and diversity, with a significant reduction in generation time.","Our code is available at: https://github.com/Explain3D/DAM"],"url":"http://arxiv.org/abs/2401.14938v1","category":"cs.CV"}
{"created":"2024-01-26 15:18:22","title":"Reassessing Java Code Readability Models with a Human-Centered Approach","abstract":"To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted. Existing Code Readability (CR) models can guide this alignment. However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code. This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code. Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets. Our findings indicate that when AI generates concise and executable code, it is often considered readable by CR models and developers. However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models.","sentences":["To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted.","Existing Code Readability (CR) models can guide this alignment.","However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code.","This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code.","Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets.","Our findings indicate that when AI generates concise and executable code, it is often considered readable by CR models and developers.","However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models."],"url":"http://arxiv.org/abs/2401.14936v1","category":"cs.SE"}
{"created":"2024-01-26 15:17:28","title":"Appropriateness of LLM-equipped Robotic Well-being Coach Language in the Workplace: A Qualitative Evaluation","abstract":"Robotic coaches have been recently investigated to promote mental well-being in various contexts such as workplaces and homes. With the widespread use of Large Language Models (LLMs), HRI researchers are called to consider language appropriateness when using such generated language for robotic mental well-being coaches in the real world. Therefore, this paper presents the first work that investigated the language appropriateness of robot mental well-being coach in the workplace. To this end, we conducted an empirical study that involved 17 employees who interacted over 4 weeks with a robotic mental well-being coach equipped with LLM-based capabilities. After the study, we individually interviewed them and we conducted a focus group of 1.5 hours with 11 of them. The focus group consisted of: i) an ice-breaking activity, ii) evaluation of robotic coach language appropriateness in various scenarios, and iii) listing shoulds and shouldn'ts for designing appropriate robotic coach language for mental well-being. From our qualitative evaluation, we found that a language-appropriate robotic coach should (1) ask deep questions which explore feelings of the coachees, rather than superficial questions, (2) express and show emotional and empathic understanding of the context, and (3) not make any assumptions without clarifying with follow-up questions to avoid bias and stereotyping. These results can inform the design of language-appropriate robotic coach to promote mental well-being in real-world contexts.","sentences":["Robotic coaches have been recently investigated to promote mental well-being in various contexts such as workplaces and homes.","With the widespread use of Large Language Models (LLMs), HRI researchers are called to consider language appropriateness when using such generated language for robotic mental well-being coaches in the real world.","Therefore, this paper presents the first work that investigated the language appropriateness of robot mental well-being coach in the workplace.","To this end, we conducted an empirical study that involved 17 employees who interacted over 4 weeks with a robotic mental well-being coach equipped with LLM-based capabilities.","After the study, we individually interviewed them and we conducted a focus group of 1.5 hours with 11 of them.","The focus group consisted of: i) an ice-breaking activity, ii) evaluation of robotic coach language appropriateness in various scenarios, and iii) listing shoulds and shouldn'ts for designing appropriate robotic coach language for mental well-being.","From our qualitative evaluation, we found that a language-appropriate robotic coach should (1) ask deep questions which explore feelings of the coachees, rather than superficial questions, (2) express and show emotional and empathic understanding of the context, and (3) not make any assumptions without clarifying with follow-up questions to avoid bias and stereotyping.","These results can inform the design of language-appropriate robotic coach to promote mental well-being in real-world contexts."],"url":"http://arxiv.org/abs/2401.14935v1","category":"cs.HC"}
{"created":"2024-01-26 15:13:43","title":"Shadow simulation of quantum processes","abstract":"We introduce the task of shadow process simulation, where the goal is to reproduce the expectation values of arbitrary quantum observables at the output of a target physical process. When the sender and receiver share classical random bits, we show that the performance of shadow process simulation exceeds that of conventional process simulation protocols in a variety of scenarios including communication, noise simulation, and data compression. Remarkably, shadow simulation provides increased accuracy without any increase in the sampling cost. Overall, shadow simulation provides a unified framework for a variety of quantum protocols, including probabilistic error cancellation and circuit knitting in quantum computing.","sentences":["We introduce the task of shadow process simulation, where the goal is to reproduce the expectation values of arbitrary quantum observables at the output of a target physical process.","When the sender and receiver share classical random bits, we show that the performance of shadow process simulation exceeds that of conventional process simulation protocols in a variety of scenarios including communication, noise simulation, and data compression.","Remarkably, shadow simulation provides increased accuracy without any increase in the sampling cost.","Overall, shadow simulation provides a unified framework for a variety of quantum protocols, including probabilistic error cancellation and circuit knitting in quantum computing."],"url":"http://arxiv.org/abs/2401.14934v1","category":"quant-ph"}
{"created":"2024-01-26 15:11:31","title":"SSDOnt: an Ontology for representing Single-Subject Design Studies","abstract":"Background: Single-Subject Design is used in several areas such as education and biomedicine. However, no suited formal vocabulary exists for annotating the detailed configuration and the results of this type of research studies with the appropriate granularity for looking for information about them. Therefore, the search for those study designs relies heavily on a syntactical search on the abstract, keywords or full text of the publications about the study, which entails some limitations. Objective: To present SSDOnt, a specific purpose ontology for describing and annotating single-subject design studies, so that complex questions can be asked about them afterwards. Methods: The ontology was developed following the NeOn methodology. Once the requirements of the ontology were defined, a formal model was described in a Description Logic and later implemented in the ontology language OWL 2 DL. Results: We show how the ontology provides a reference model with a suitable terminology for the annotation and searching of single-subject design studies and their main components, such as the phases, the intervention types, the outcomes and the results. Some mappings with terms of related ontologies have been established. We show as proof-of-concept that classes in the ontology can be easily extended to annotate more precise information about specific interventions and outcomes such as those related to autism. Moreover, we provide examples of some types of queries that can be posed to the ontology. Conclusions: SSDOnt has achieved the purpose of covering the descriptions of the domain of single-subject research studies.","sentences":["Background: Single-Subject Design is used in several areas such as education and biomedicine.","However, no suited formal vocabulary exists for annotating the detailed configuration and the results of this type of research studies with the appropriate granularity for looking for information about them.","Therefore, the search for those study designs relies heavily on a syntactical search on the abstract, keywords or full text of the publications about the study, which entails some limitations.","Objective: To present SSDOnt, a specific purpose ontology for describing and annotating single-subject design studies, so that complex questions can be asked about them afterwards.","Methods: The ontology was developed following the NeOn methodology.","Once the requirements of the ontology were defined, a formal model was described in a Description Logic and later implemented in the ontology language OWL 2 DL.","Results:","We show how the ontology provides a reference model with a suitable terminology for the annotation and searching of single-subject design studies and their main components, such as the phases, the intervention types, the outcomes and the results.","Some mappings with terms of related ontologies have been established.","We show as proof-of-concept that classes in the ontology can be easily extended to annotate more precise information about specific interventions and outcomes such as those related to autism.","Moreover, we provide examples of some types of queries that can be posed to the ontology.","Conclusions: SSDOnt has achieved the purpose of covering the descriptions of the domain of single-subject research studies."],"url":"http://arxiv.org/abs/2401.14933v1","category":"cs.AI"}
{"created":"2024-01-26 15:10:44","title":"Super-exponential quantum advantage for finding the center of a sphere","abstract":"This article considers the geometric problem of finding the center of a sphere in vector space over finite fields, given samples of random points on the sphere. We propose a quantum algorithm based on continuous-time quantum walks that needs only a constant number of samples to find the center. We also prove that any classical algorithm for the same task requires approximately as many samples as the dimension of the vector space, by a reduction to an old and basic algebraic result -- Warning's second theorem. Thus, a super-exponential quantum advantage is revealed for the first time for a natural and intuitive geometric problem.","sentences":["This article considers the geometric problem of finding the center of a sphere in vector space over finite fields, given samples of random points on the sphere.","We propose a quantum algorithm based on continuous-time quantum walks that needs only a constant number of samples to find the center.","We also prove that any classical algorithm for the same task requires approximately as many samples as the dimension of the vector space, by a reduction to an old and basic algebraic result -- Warning's second theorem.","Thus, a super-exponential quantum advantage is revealed for the first time for a natural and intuitive geometric problem."],"url":"http://arxiv.org/abs/2401.14932v1","category":"quant-ph"}
{"created":"2024-01-26 15:10:23","title":"Do LLMs Dream of Ontologies?","abstract":"Large language models (LLMs) have recently revolutionized automated text understanding and generation. The performance of these models relies on the high number of parameters of the underlying neural architectures, which allows LLMs to memorize part of the vast quantity of data seen during the training. This paper investigates whether and to what extent general-purpose pre-trained LLMs have memorized information from known ontologies. Our results show that LLMs partially know ontologies: they can, and do indeed, memorize concepts from ontologies mentioned in the text, but the level of memorization of their concepts seems to vary proportionally to their popularity on the Web, the primary source of their training material. We additionally propose new metrics to estimate the degree of memorization of ontological information in LLMs by measuring the consistency of the output produced across different prompt repetitions, query languages, and degrees of determinism.","sentences":["Large language models (LLMs) have recently revolutionized automated text understanding and generation.","The performance of these models relies on the high number of parameters of the underlying neural architectures, which allows LLMs to memorize part of the vast quantity of data seen during the training.","This paper investigates whether and to what extent general-purpose pre-trained LLMs have memorized information from known ontologies.","Our results show that LLMs partially know ontologies: they can, and do indeed, memorize concepts from ontologies mentioned in the text, but the level of memorization of their concepts seems to vary proportionally to their popularity on the Web, the primary source of their training material.","We additionally propose new metrics to estimate the degree of memorization of ontological information in LLMs by measuring the consistency of the output produced across different prompt repetitions, query languages, and degrees of determinism."],"url":"http://arxiv.org/abs/2401.14931v1","category":"cs.CL"}
{"created":"2024-01-26 15:09:43","title":"Neutron-star Measurements in the Multi-messenger Era","abstract":"Neutron stars are compact and dense celestial objects that offer the unique opportunity to explore matter and its interactions under conditions that cannot be reproduced elsewhere in the Universe. Their extreme gravitational, rotational and magnetic energy reservoirs fuel the large variety of their emission, which encompasses all available multi-messenger tracers: electromagnetic and gravitational waves, neutrinos, and cosmic rays. However, accurately measuring global neutron-star properties such as mass, radius, and moment of inertia poses significant challenges. Probing internal characteristics such as the crustal composition or superfluid physics is even more complex. This article provides a comprehensive review of the different methods employed to measure neutron-star characteristics and the level of reliance on theoretical models. Understanding these measurement techniques is crucial for advancing our knowledge of neutron-star physics. We also highlight the importance of employing independent methods and adopting a multi-messenger approach to gather complementary data from various observable phenomena as exemplified by the recent breakthroughs in gravitational-wave astronomy and the landmark detection of a binary neutron-star merger. Consolidating the current state of knowledge on neutron-star measurements will enable an accurate interpretation of the current data and errors, and better planning for future observations and experiments.","sentences":["Neutron stars are compact and dense celestial objects that offer the unique opportunity to explore matter and its interactions under conditions that cannot be reproduced elsewhere in the Universe.","Their extreme gravitational, rotational and magnetic energy reservoirs fuel the large variety of their emission, which encompasses all available multi-messenger tracers: electromagnetic and gravitational waves, neutrinos, and cosmic rays.","However, accurately measuring global neutron-star properties such as mass, radius, and moment of inertia poses significant challenges.","Probing internal characteristics such as the crustal composition or superfluid physics is even more complex.","This article provides a comprehensive review of the different methods employed to measure neutron-star characteristics and the level of reliance on theoretical models.","Understanding these measurement techniques is crucial for advancing our knowledge of neutron-star physics.","We also highlight the importance of employing independent methods and adopting a multi-messenger approach to gather complementary data from various observable phenomena as exemplified by the recent breakthroughs in gravitational-wave astronomy and the landmark detection of a binary neutron-star merger.","Consolidating the current state of knowledge on neutron-star measurements will enable an accurate interpretation of the current data and errors, and better planning for future observations and experiments."],"url":"http://arxiv.org/abs/2401.14930v1","category":"astro-ph.HE"}
{"created":"2024-01-26 15:01:19","title":"Frictional contact of soft polymeric shells","abstract":"The classical Hertzian contact model establishes a monotonic correlation between contact force and area. Here, we showed that the interplay between local friction and structural instability can deliberately lead to unconventional contact behavior when a soft elastic shell comes into contact with a flat surface. The deviation from Hertzian contact first arises from bending within the contact area, followed by the second transition induced by buckling, resulting in a notable decrease in the contact area despite increased contact force. Friction delays both transitions and introduces hysteresis during unloading. However, a high amount of friction suppresses both buckling and dissipation. Different contact regimes are discussed in terms of rolling and sliding mechanisms, providing insights for tailoring contact behaviors in soft shells.","sentences":["The classical Hertzian contact model establishes a monotonic correlation between contact force and area.","Here, we showed that the interplay between local friction and structural instability can deliberately lead to unconventional contact behavior when a soft elastic shell comes into contact with a flat surface.","The deviation from Hertzian contact first arises from bending within the contact area, followed by the second transition induced by buckling, resulting in a notable decrease in the contact area despite increased contact force.","Friction delays both transitions and introduces hysteresis during unloading.","However, a high amount of friction suppresses both buckling and dissipation.","Different contact regimes are discussed in terms of rolling and sliding mechanisms, providing insights for tailoring contact behaviors in soft shells."],"url":"http://arxiv.org/abs/2401.14926v1","category":"cs.CE"}
{"created":"2024-01-26 15:00:28","title":"More inclusive and on wider sources: A Comparative Analysis of Data and Political Journalists on Twitter in Germany","abstract":"Women are underrepresented in many areas of journalistic newsrooms. In this paper, we examine if this established effect continues in the new forms of journalistic communication, Social Media Networks. We used mentions, retweets, and hashtags as journalistic amplification and legitimation measures. Furthermore, we compared two groups of journalists in different stages of development: political and data journalists in Germany in 2021. Our results show that journalists regarded as women tend to favor their other women in mentions and retweets on Twitter, compared to men. While both professions are dominated by many men and a high share of men-authored tweets, women are mentioning and retweeting other women to a more extensive degree than their male colleagues. Women data journalists also leveraged different sources than men. In addition, we have found data journalists to be more inclusive towards non-member sources in their network compared to political journalists.","sentences":["Women are underrepresented in many areas of journalistic newsrooms.","In this paper, we examine if this established effect continues in the new forms of journalistic communication, Social Media Networks.","We used mentions, retweets, and hashtags as journalistic amplification and legitimation measures.","Furthermore, we compared two groups of journalists in different stages of development: political and data journalists in Germany in 2021.","Our results show that journalists regarded as women tend to favor their other women in mentions and retweets on Twitter, compared to men.","While both professions are dominated by many men and a high share of men-authored tweets, women are mentioning and retweeting other women to a more extensive degree than their male colleagues.","Women data journalists also leveraged different sources than men.","In addition, we have found data journalists to be more inclusive towards non-member sources in their network compared to political journalists."],"url":"http://arxiv.org/abs/2401.14925v1","category":"cs.SI"}
{"created":"2024-01-26 14:59:48","title":"Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks","abstract":"Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans.","sentences":["Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification.","Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals.","In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions.","In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent.","Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor.","Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks.","Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans."],"url":"http://arxiv.org/abs/2401.14923v1","category":"cs.AI"}
{"created":"2024-01-26 14:55:21","title":"Hold Tight: Identifying Behavioral Patterns During Prolonged Work in VR through Video Analysis","abstract":"VR devices have recently been actively promoted as tools for knowledge workers and prior work has demonstrated that VR can support some knowledge worker tasks. However, only a few studies have explored the effects of prolonged use of VR such as a study observing 16 participant working in VR and a physical environment for one work-week each and reporting mainly on subjective feedback. As a nuanced understanding of participants' behavior in VR and how it evolves over time is still missing, we report on the results from an analysis of 559 hours of video material obtained in this prior study. Among other findings, we report that (1) the frequency of actions related to adjusting the headset reduced by 46% and the frequency of actions related to supporting the headset reduced by 42% over the five days; (2) the HMD was removed 31% less frequently over the five days but for 41% longer periods; (3) wearing an HMD is disruptive to normal patterns of eating and drinking, but not to social interactions, such as talking. The combined findings in this work demonstrate the value of long-term studies of deployed VR systems and can be used to inform the design of better, more ergonomic VR systems as tools for knowledge workers.","sentences":["VR devices have recently been actively promoted as tools for knowledge workers and prior work has demonstrated that VR can support some knowledge worker tasks.","However, only a few studies have explored the effects of prolonged use of VR such as a study observing 16 participant working in VR and a physical environment for one work-week each and reporting mainly on subjective feedback.","As a nuanced understanding of participants' behavior in VR and how it evolves over time is still missing, we report on the results from an analysis of 559 hours of video material obtained in this prior study.","Among other findings, we report that (1) the frequency of actions related to adjusting the headset reduced by 46% and the frequency of actions related to supporting the headset reduced by 42% over the five days; (2) the HMD was removed 31% less frequently over the five days but for 41% longer periods; (3) wearing an HMD is disruptive to normal patterns of eating and drinking, but not to social interactions, such as talking.","The combined findings in this work demonstrate the value of long-term studies of deployed VR systems and can be used to inform the design of better, more ergonomic VR systems as tools for knowledge workers."],"url":"http://arxiv.org/abs/2401.14920v1","category":"cs.HC"}
{"created":"2024-01-26 14:54:56","title":"PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus","abstract":"We present a real-time method for robust estimation of multiple instances of geometric models from noisy data. Geometric models such as vanishing points, planar homographies or fundamental matrices are essential for 3D scene analysis. Previous approaches discover distinct model instances in an iterative manner, thus limiting their potential for speedup via parallel computation. In contrast, our method detects all model instances independently and in parallel. A neural network segments the input data into clusters representing potential model instances by predicting multiple sets of sample and inlier weights. Using the predicted weights, we determine the model parameters for each potential instance separately in a RANSAC-like fashion. We train the neural network via task-specific loss functions, i.e. we do not require a ground-truth segmentation of the input data. As suitable training data for homography and fundamental matrix fitting is scarce, we additionally present two new synthetic datasets. We demonstrate state-of-the-art performance on these as well as multiple established datasets, with inference times as small as five milliseconds per image.","sentences":["We present a real-time method for robust estimation of multiple instances of geometric models from noisy data.","Geometric models such as vanishing points, planar homographies or fundamental matrices are essential for 3D scene analysis.","Previous approaches discover distinct model instances in an iterative manner, thus limiting their potential for speedup via parallel computation.","In contrast, our method detects all model instances independently and in parallel.","A neural network segments the input data into clusters representing potential model instances by predicting multiple sets of sample and inlier weights.","Using the predicted weights, we determine the model parameters for each potential instance separately in a RANSAC-like fashion.","We train the neural network via task-specific loss functions, i.e. we do not require a ground-truth segmentation of the input data.","As suitable training data for homography and fundamental matrix fitting is scarce, we additionally present two new synthetic datasets.","We demonstrate state-of-the-art performance on these as well as multiple established datasets, with inference times as small as five milliseconds per image."],"url":"http://arxiv.org/abs/2401.14919v1","category":"cs.CV"}
{"created":"2024-01-26 14:51:29","title":"Proving Information Inequalities by Gaussian Elimination","abstract":"The proof of information inequalities and identities under linear constraints on the information measures is an important problem in information theory. For this purpose, ITIP and other variant algorithms have been developed and implemented, which are all based on solving a linear program (LP). In this paper, we develop a method with symbolic computation. Compared with the known methods, our approach can completely avoids the use of linear programming which may cause numerical errors. Our procedures are also more efficient computationally.","sentences":["The proof of information inequalities and identities under linear constraints on the information measures is an important problem in information theory.","For this purpose, ITIP and other variant algorithms have been developed and implemented, which are all based on solving a linear program (LP).","In this paper, we develop a method with symbolic computation.","Compared with the known methods, our approach can completely avoids the use of linear programming which may cause numerical errors.","Our procedures are also more efficient computationally."],"url":"http://arxiv.org/abs/2401.14916v1","category":"cs.IT"}
{"created":"2024-01-26 14:49:29","title":"Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students","abstract":"The increasing use of Artificial Intelligence (AI) by students in learning presents new challenges for assessing their learning outcomes in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students' AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students' use of AI in PBL and ways of analyzing these uses grounded by students' vision of education goal transformation. We also found students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand the use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.","sentences":["The increasing use of Artificial Intelligence (AI) by students in learning presents new challenges for assessing their learning outcomes in project-based learning (PBL).","This paper introduces a co-design study to explore the potential of students' AI usage data as a novel material for PBL assessment.","We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions.","Our workshops yielded various scenarios of students' use of AI in PBL and ways of analyzing these uses grounded by students' vision of education goal transformation.","We also found students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand the use of AI.","Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning."],"url":"http://arxiv.org/abs/2401.14915v1","category":"cs.HC"}
{"created":"2024-01-26 14:46:53","title":"On Repairing Quantum Programs Using ChatGPT","abstract":"Automated Program Repair (APR) is a vital area in software engineering aimed at generating automatic patches for vulnerable programs. While numerous techniques have been proposed for repairing classical programs, the realm of quantum programming lacks a comparable automated repair technique. In this initial exploration, we investigate the use of ChatGPT for quantum program repair and evaluate its performance on Bugs4Q, a benchmark suite of quantum program bugs. Our findings demonstrate the feasibility of employing ChatGPT for quantum program repair. Specifically, we assess ChatGPT's ability to address bugs within the Bugs4Q benchmark, revealing its success in repairing 29 out of 38 bugs. This research represents a promising step towards automating the repair process for quantum programs.","sentences":["Automated Program Repair (APR) is a vital area in software engineering aimed at generating automatic patches for vulnerable programs.","While numerous techniques have been proposed for repairing classical programs, the realm of quantum programming lacks a comparable automated repair technique.","In this initial exploration, we investigate the use of ChatGPT for quantum program repair and evaluate its performance on Bugs4Q, a benchmark suite of quantum program bugs.","Our findings demonstrate the feasibility of employing ChatGPT for quantum program repair.","Specifically, we assess ChatGPT's ability to address bugs within the Bugs4Q benchmark, revealing its success in repairing 29 out of 38 bugs.","This research represents a promising step towards automating the repair process for quantum programs."],"url":"http://arxiv.org/abs/2401.14913v1","category":"cs.SE"}
{"created":"2024-01-26 14:46:22","title":"Many-excitation removal of a transmon qubit using a single-junction quantum-circuit refrigerator and a two-tone microwave drive","abstract":"Achieving fast and precise initialization of qubits is a critical requirement for the successful operation of quantum computers. The combination of engineered environments with all-microwave techniques has recently emerged as a promising approach for the reset of superconducting quantum devices. In this work, we experimentally demonstrate the utilization of a single-junction quantum-circuit refrigerator (QCR) for an expeditious removal of several excitations from a transmon qubit. The QCR is indirectly coupled to the transmon through a resonator in the dispersive regime, constituting a carefully engineered environmental spectrum for the transmon. Using single-shot readout, we observe excitation stabilization times down to roughly $500$ ns, a $20$-fold speedup with QCR and a simultaneous two-tone drive addressing the $e$-$f$ and $f0$-$g1$ transitions of the system. Our results are obtained at a $48$-mK fridge temperature and without postselection, fully capturing the advantage of the protocol for the short-time dynamics and the drive-induced detrimental asymptotic behavior in the presence of relatively hot other baths of the transmon. We validate our results with a detailed Liouvillian model truncated up to the three-excitation subspace, from which we estimate the performance of the protocol in optimized scenarios, such as cold transmon baths and fine-tuned driving frequencies. These results pave the way for optimized reset of quantum-electric devices using engineered environments and for dissipation-engineered state preparation.","sentences":["Achieving fast and precise initialization of qubits is a critical requirement for the successful operation of quantum computers.","The combination of engineered environments with all-microwave techniques has recently emerged as a promising approach for the reset of superconducting quantum devices.","In this work, we experimentally demonstrate the utilization of a single-junction quantum-circuit refrigerator (QCR) for an expeditious removal of several excitations from a transmon qubit.","The QCR is indirectly coupled to the transmon through a resonator in the dispersive regime, constituting a carefully engineered environmental spectrum for the transmon.","Using single-shot readout, we observe excitation stabilization times down to roughly $500$ ns, a $20$-fold speedup with QCR and a simultaneous two-tone drive addressing the $e$-$f$ and $f0$-$g1$ transitions of the system.","Our results are obtained at a $48$-mK fridge temperature and without postselection, fully capturing the advantage of the protocol for the short-time dynamics and the drive-induced detrimental asymptotic behavior in the presence of relatively hot other baths of the transmon.","We validate our results with a detailed Liouvillian model truncated up to the three-excitation subspace, from which we estimate the performance of the protocol in optimized scenarios, such as cold transmon baths and fine-tuned driving frequencies.","These results pave the way for optimized reset of quantum-electric devices using engineered environments and for dissipation-engineered state preparation."],"url":"http://arxiv.org/abs/2401.14912v1","category":"quant-ph"}
{"created":"2024-01-26 14:45:03","title":"The excitation spectrum of a dilute Bose gas with an impurity","abstract":"We study a dilute system of $N$ interacting bosons coupled to an impurity particle via a pair potential in the Gross--Pitaevskii regime. We derive an expansion of the ground state energy up to order one in the boson number, and show that the difference of excited eigenvalues to the ground state is given by the eigenvalues of the renormalized Bogoliubov--Fr\\\"ohlich Hamiltonian in the limit $N\\to \\infty$.","sentences":["We study a dilute system of $N$ interacting bosons coupled to an impurity particle via a pair potential in the Gross--Pitaevskii regime.","We derive an expansion of the ground state energy up to order one in the boson number, and show that the difference of excited eigenvalues to the ground state is given by the eigenvalues of the renormalized Bogoliubov--Fr\\\"ohlich Hamiltonian in the limit $N\\to \\infty$."],"url":"http://arxiv.org/abs/2401.14911v1","category":"math-ph"}
{"created":"2024-01-26 14:38:54","title":"A Framework for Assurance Audits of Algorithmic Systems","abstract":"An increasing number of regulations propose the notion of AI audits as an enforcement mechanism for achieving transparency and accountability for AI systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently have little to no agreed upon practices, procedures, taxonomies, and standards. We propose the criterion audit as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations' ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit, and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which bias audits for hiring algorithms can be performed, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion as informed by experiences in performing these audits in practice highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of such methodology.","sentences":["An increasing number of regulations propose the notion of AI audits as an enforcement mechanism for achieving transparency and accountability for AI systems.","Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently have little to no agreed upon practices, procedures, taxonomies, and standards.","We propose the criterion audit as an operationalizable compliance and assurance external audit framework.","We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations' ability to govern their algorithms in ways that mitigate harms and uphold human values.","We discuss the necessary conditions for the criterion audit, and provide a procedural blueprint for performing an audit engagement in practice.","We illustrate how this framework can be adapted to current regulations by deriving the criteria on which bias audits for hiring algorithms can be performed, as required by the recently effective New York City Local Law 144 of 2021.","We conclude by offering critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge.","Our discussion as informed by experiences in performing these audits in practice highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of such methodology."],"url":"http://arxiv.org/abs/2401.14908v1","category":"cs.CY"}
{"created":"2024-01-26 14:38:43","title":"Learning Local Control Barrier Functions for Safety Control of Hybrid Systems","abstract":"Hybrid dynamical systems are ubiquitous as practical robotic applications often involve both continuous states and discrete switchings. Safety is a primary concern for hybrid robotic systems. Existing safety-critical control approaches for hybrid systems are either computationally inefficient, detrimental to system performance, or limited to small-scale systems. To amend these drawbacks, in this paper, we propose a learningenabled approach to construct local Control Barrier Functions (CBFs) to guarantee the safety of a wide class of nonlinear hybrid dynamical systems. The end result is a safe neural CBFbased switching controller. Our approach is computationally efficient, minimally invasive to any reference controller, and applicable to large-scale systems. We empirically evaluate our framework and demonstrate its efficacy and flexibility through two robotic examples including a high-dimensional autonomous racing case, against other CBF-based approaches and model predictive control.","sentences":["Hybrid dynamical systems are ubiquitous as practical robotic applications often involve both continuous states and discrete switchings.","Safety is a primary concern for hybrid robotic systems.","Existing safety-critical control approaches for hybrid systems are either computationally inefficient, detrimental to system performance, or limited to small-scale systems.","To amend these drawbacks, in this paper, we propose a learningenabled approach to construct local Control Barrier Functions (CBFs) to guarantee the safety of a wide class of nonlinear hybrid dynamical systems.","The end result is a safe neural CBFbased switching controller.","Our approach is computationally efficient, minimally invasive to any reference controller, and applicable to large-scale systems.","We empirically evaluate our framework and demonstrate its efficacy and flexibility through two robotic examples including a high-dimensional autonomous racing case, against other CBF-based approaches and model predictive control."],"url":"http://arxiv.org/abs/2401.14907v1","category":"cs.RO"}
{"created":"2024-01-26 14:38:24","title":"A High-Performance SurfaceNets Discrete Isocontouring Algorithm","abstract":"Isocontouring is one of the most widely used visualization techniques. However, many popular contouring algorithms were created prior to the advent of ubiquitous parallel approaches, such as multi-core, shared memory computing systems. With increasing data sizes and computational loads, it is essential to reimagine such algorithms to leverage the increased computing capabilities available today. To this end we have redesigned the SurfaceNets algorithm, a powerful technique which is often employed to isocontour non-continuous, discrete, volumetric scalar fields such as segmentation label maps. Label maps are ubiquitous to medical computing and biological analysis, used in applications ranging from anatomical atlas creation to brain connectomics. This novel Parallel SurfaceNets algorithm has been redesigned using concepts from the high-performance Flying Edges continuous isocontouring algorrithm. It consists of two basic steps, surface extraction followed by constrained smoothing, parallelized over volume edges and employing a double-buffering smoothing approach to guarantee determinism. The algorithm can extract and smooth multiple segmented objects in a single execution, producing a polygonal (triangular/quadrilateral) mesh with points and polygons fully shared between neighboring objects. Performance is typically one to two orders of magnitude faster than the current sequential algorithms for discrete isosurface extraction on small core-count commodity CPU hardware. We demonstrate the effectiveness of the algorithm on five different datasets including human torso and brain atlases, mouse brain segmentation, and electron microscopy connectomics. The software is currently available under a permissive, open source license in the VTK visualization system.","sentences":["Isocontouring is one of the most widely used visualization techniques.","However, many popular contouring algorithms were created prior to the advent of ubiquitous parallel approaches, such as multi-core, shared memory computing systems.","With increasing data sizes and computational loads, it is essential to reimagine such algorithms to leverage the increased computing capabilities available today.","To this end we have redesigned the SurfaceNets algorithm, a powerful technique which is often employed to isocontour non-continuous, discrete, volumetric scalar fields such as segmentation label maps.","Label maps are ubiquitous to medical computing and biological analysis, used in applications ranging from anatomical atlas creation to brain connectomics.","This novel Parallel SurfaceNets algorithm has been redesigned using concepts from the high-performance Flying Edges continuous isocontouring algorrithm.","It consists of two basic steps, surface extraction followed by constrained smoothing, parallelized over volume edges and employing a double-buffering smoothing approach to guarantee determinism.","The algorithm can extract and smooth multiple segmented objects in a single execution, producing a polygonal (triangular/quadrilateral) mesh with points and polygons fully shared between neighboring objects.","Performance is typically one to two orders of magnitude faster than the current sequential algorithms for discrete isosurface extraction on small core-count commodity CPU hardware.","We demonstrate the effectiveness of the algorithm on five different datasets including human torso and brain atlases, mouse brain segmentation, and electron microscopy connectomics.","The software is currently available under a permissive, open source license in the VTK visualization system."],"url":"http://arxiv.org/abs/2401.14906v1","category":"cs.GR"}
{"created":"2024-01-26 14:38:22","title":"Social norms and cooperation in higher-order networks","abstract":"Recent research has focused on understanding how cooperation is fostered through various mechanisms in cognitive settings, particularly through pairwise interactions. However, real-world interactions often extend beyond simple dyads, including multiple cliques with both pairwise and higher-order interactions. These complex interactions influence how individuals perceive and adapt their strategies based on social norms. We here introduce a model that explores the evolution of collective strategies and social norms within a heterogeneous environment, encompassing both dyadic and three-body interactions. We find that social norms play a crucial role in promoting cooperation in comparison to simply imitating the most successful neighbor. We also show that the rise of prosocial norms leads to increased cooperation across various social dilemmas, often resulting in shifts from defective to cooperative behavior. Additionally, we observe that a moderate level of information privacy helps sustaining prosocial norms and curtails antisocial tendencies, even in situations where mutual defection might seem advantageous. Our research thus offers insights into the evolution of cooperation through the lens of social norm diffusion in higher-order networks.","sentences":["Recent research has focused on understanding how cooperation is fostered through various mechanisms in cognitive settings, particularly through pairwise interactions.","However, real-world interactions often extend beyond simple dyads, including multiple cliques with both pairwise and higher-order interactions.","These complex interactions influence how individuals perceive and adapt their strategies based on social norms.","We here introduce a model that explores the evolution of collective strategies and social norms within a heterogeneous environment, encompassing both dyadic and three-body interactions.","We find that social norms play a crucial role in promoting cooperation in comparison to simply imitating the most successful neighbor.","We also show that the rise of prosocial norms leads to increased cooperation across various social dilemmas, often resulting in shifts from defective to cooperative behavior.","Additionally, we observe that a moderate level of information privacy helps sustaining prosocial norms and curtails antisocial tendencies, even in situations where mutual defection might seem advantageous.","Our research thus offers insights into the evolution of cooperation through the lens of social norm diffusion in higher-order networks."],"url":"http://arxiv.org/abs/2401.14905v1","category":"physics.soc-ph"}
{"created":"2024-01-26 14:32:35","title":"Energy Flexibility Potential in the Brewery Sector: A Multi-agent Based Simulation of 239 Danish Breweries","abstract":"The beverage industry is a typical food processing industry, accounts for significant energy consumption, and has flexible demands. However, the deployment of energy flexibility in the beverage industry is complex and challenging. Furthermore, activation of energy flexibility from the whole brewery industry is necessary to ensure grid stability. Therefore, this paper assesses the energy flexibility potential of Denmark's brewery sector based on a multi-agent-based simulation. 239 individual brewery facilities are simulated, and each facility, as an agent, can interact with the energy system market and make decisions based on its underlying parameters and operational restrictions. The results show that the Danish breweries could save 1.56 % of electricity costs annually while maintaining operational security and reducing approximately 1745 tonnes of CO2 emissions. Furthermore, medium-size breweries could obtain higher relative benefits by providing energy flexibility, especially those producing lager and ale. The result also shows that the breweries' relative saving potential is electricity market-dependent.","sentences":["The beverage industry is a typical food processing industry, accounts for significant energy consumption, and has flexible demands.","However, the deployment of energy flexibility in the beverage industry is complex and challenging.","Furthermore, activation of energy flexibility from the whole brewery industry is necessary to ensure grid stability.","Therefore, this paper assesses the energy flexibility potential of Denmark's brewery sector based on a multi-agent-based simulation.","239 individual brewery facilities are simulated, and each facility, as an agent, can interact with the energy system market and make decisions based on its underlying parameters and operational restrictions.","The results show that the Danish breweries could save 1.56 % of electricity costs annually while maintaining operational security and reducing approximately 1745 tonnes of CO2 emissions.","Furthermore, medium-size breweries could obtain higher relative benefits by providing energy flexibility, especially those producing lager and ale.","The result also shows that the breweries' relative saving potential is electricity market-dependent."],"url":"http://arxiv.org/abs/2401.14903v1","category":"cs.MA"}
{"created":"2024-01-26 14:30:32","title":"Augmenting Bankruptcy Prediction using Reported Behavior of Corporate Restructuring","abstract":"Credit risk assessment of a company is commonly conducted by utilizing financial ratios that are derived from its financial statements. However, this approach may not fully encompass other significant aspects of a company. We propose the utilization of a hybrid dataset that combines financial statements with information about corporate restructuring behavior in order to construct diverse machine learning models to predict bankruptcy. Utilizing a hybrid data set provides a more comprehensive and holistic perspective on a company's financial position and the dynamics of its business operations. The experiments were carried out using publicly available records of all the files submitted by small and medium-sized enterprises to Luxembourg Business Registers. We conduct a comparative analysis of bankruptcy prediction using six machine learning models. Furthermore, we validate the effectiveness of the hybrid dataset. In addition to the conventional testing set, we deliberately chose the timeframe encompassing the years of the Covid-19 pandemic as an additional testing set in order to evaluate the robustness of the models. The experimental results demonstrate that the hybrid data set can improve the performance of the model by 4%-13% compared to a single source data set. We also identify suitable models for predicting bankruptcy.","sentences":["Credit risk assessment of a company is commonly conducted by utilizing financial ratios that are derived from its financial statements.","However, this approach may not fully encompass other significant aspects of a company.","We propose the utilization of a hybrid dataset that combines financial statements with information about corporate restructuring behavior in order to construct diverse machine learning models to predict bankruptcy.","Utilizing a hybrid data set provides a more comprehensive and holistic perspective on a company's financial position and the dynamics of its business operations.","The experiments were carried out using publicly available records of all the files submitted by small and medium-sized enterprises to Luxembourg Business Registers.","We conduct a comparative analysis of bankruptcy prediction using six machine learning models.","Furthermore, we validate the effectiveness of the hybrid dataset.","In addition to the conventional testing set, we deliberately chose the timeframe encompassing the years of the Covid-19 pandemic as an additional testing set in order to evaluate the robustness of the models.","The experimental results demonstrate that the hybrid data set can improve the performance of the model by 4%-13% compared to a single source data set.","We also identify suitable models for predicting bankruptcy."],"url":"http://arxiv.org/abs/2401.14901v1","category":"cs.CE"}
{"created":"2024-01-26 14:29:31","title":"Benchmarking Bayesian quantum estimation","abstract":"The quest for precision in parameter estimation is a fundamental task in different scientific areas. The relevance of this problem thus provided the motivation to develop methods for the application of quantum resources to estimation protocols. Within this context, Bayesian estimation offers a complete framework for optimal quantum metrology techniques, such as adaptive protocols. However, the use of the Bayesian approach requires extensive computational resources, especially in the multiparameter estimations that represent the typical operational scenario for quantum sensors. Hence, the requirement to characterize protocols implementing Bayesian estimations can become a significant challenge. This work focuses on the crucial task of robustly benchmarking the performances of these protocols in both single and multiple-parameter scenarios. By comparing different figures of merits, evidence is provided in favor of using the median of the quadratic error in the estimations in order to mitigate spurious effects due to the numerical discretization of the parameter space, the presence of limited data, and numerical instabilities. These results, providing a robust and reliable characterization of Bayesian protocols, find natural applications to practical problems within the quantum estimation framework.","sentences":["The quest for precision in parameter estimation is a fundamental task in different scientific areas.","The relevance of this problem thus provided the motivation to develop methods for the application of quantum resources to estimation protocols.","Within this context, Bayesian estimation offers a complete framework for optimal quantum metrology techniques, such as adaptive protocols.","However, the use of the Bayesian approach requires extensive computational resources, especially in the multiparameter estimations that represent the typical operational scenario for quantum sensors.","Hence, the requirement to characterize protocols implementing Bayesian estimations can become a significant challenge.","This work focuses on the crucial task of robustly benchmarking the performances of these protocols in both single and multiple-parameter scenarios.","By comparing different figures of merits, evidence is provided in favor of using the median of the quadratic error in the estimations in order to mitigate spurious effects due to the numerical discretization of the parameter space, the presence of limited data, and numerical instabilities.","These results, providing a robust and reliable characterization of Bayesian protocols, find natural applications to practical problems within the quantum estimation framework."],"url":"http://arxiv.org/abs/2401.14900v1","category":"quant-ph"}
{"created":"2024-01-26 14:25:15","title":"MPTQ-ViT:Mixed-PrecisionPost-TrainingQuantizationforVisionTransformer","abstract":"While vision transformers (ViTs) have shown great potential in computer vision tasks, their intense computation and memory requirements pose challenges for practical applications. Existing post-training quantization methods leverage value redistribution or specialized quantizers to address the non-normal distribution in ViTs. However, without considering the asymmetry in activations and relying on hand-crafted settings, these methods often struggle to maintain performance under low-bit quantization. To overcome these challenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate the asymmetry issue and reduce the clamping loss. We also introduce optimal scaling factor ratio search (OPT-m) to determine quantization parameters by a data-dependent mechanism automatically. To further enhance the compressibility, we incorporate the above-mentioned techniques and propose a mixed-precision post-training quantization framework for vision transformers (MPTQ-ViT). We develop greedy mixed-precision quantization (Greedy MP) to allocate layer-wise bit-width considering both model performance and compressibility. Our experiments on ViT, DeiT, and Swin demonstrate significant accuracy improvements compared with SOTA on the ImageNet dataset. Specifically, our proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on 4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully quantized ViTs with mixed-precision.","sentences":["While vision transformers (ViTs) have shown great potential in computer vision tasks, their intense computation and memory requirements pose challenges for practical applications.","Existing post-training quantization methods leverage value redistribution or specialized quantizers to address the non-normal distribution in ViTs.","However, without considering the asymmetry in activations and relying on hand-crafted settings, these methods often struggle to maintain performance under low-bit quantization.","To overcome these challenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate the asymmetry issue and reduce the clamping loss.","We also introduce optimal scaling factor ratio search (OPT-m) to determine quantization parameters by a data-dependent mechanism automatically.","To further enhance the compressibility, we incorporate the above-mentioned techniques and propose a mixed-precision post-training quantization framework for vision transformers (MPTQ-ViT).","We develop greedy mixed-precision quantization (Greedy MP) to allocate layer-wise bit-width considering both model performance and compressibility.","Our experiments on ViT, DeiT, and Swin demonstrate significant accuracy improvements compared with SOTA on the ImageNet dataset.","Specifically, our proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on 4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully quantized ViTs with mixed-precision."],"url":"http://arxiv.org/abs/2401.14895v1","category":"cs.CV"}
{"created":"2024-01-26 14:21:45","title":"A structured regression approach for evaluating model performance across intersectional subgroups","abstract":"Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and goodness-of-fit testing helps identify the key factors that drive differences in performance.","sentences":["Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes.","The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group.","However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations.","In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups.","We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups.","We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data.","The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and goodness-of-fit testing helps identify the key factors that drive differences in performance."],"url":"http://arxiv.org/abs/2401.14893v1","category":"cs.LG"}
{"created":"2024-01-26 14:15:20","title":"Comparison of parameters of vowel sounds of russian and english languages","abstract":"In multilingual speech recognition systems, a situation can often arise when the language is not known in advance, but the signal has already been received and is being processed. For such cases, some generalized model is needed that will be able to respond to phonetic differences and, depending on them, correctly recog-nize speech in the desired language. To build such a model, it is necessary to set the values of phonetic parameters, and then compare similar sounds, establishing significant differences.","sentences":["In multilingual speech recognition systems, a situation can often arise when the language is not known in advance, but the signal has already been received and is being processed.","For such cases, some generalized model is needed that will be able to respond to phonetic differences and, depending on them, correctly recog-nize speech in the desired language.","To build such a model, it is necessary to set the values of phonetic parameters, and then compare similar sounds, establishing significant differences."],"url":"http://arxiv.org/abs/2401.14890v1","category":"cs.SD"}
{"created":"2024-01-26 14:15:02","title":"HOPE: Holistic STT-RAM Architecture Exploration Framework for Future Cross-Platform Analysis","abstract":"Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile Memory (NVM) technology that has garnered attention to overcome the drawbacks of conventional CMOS-based technologies. However, such technologies must be evaluated before deployment under real workloads and architecture. But there is a lack of available open-source STT-RAM-based system evaluation framework, which hampers research and experimentation and impacts the adoption of STT- RAM in a system. This paper proposes a novel, extendable STT-RAM memory controller design integrated inside the gem5 simulator. Our framework enables understanding various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system throughput. We will open-source our HOPE framework, which will fuel research and aid in accelerating the development of future system architectures based on STT-RAM. It will also facilitate the user for further tool enhancement.","sentences":["Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile Memory (NVM) technology that has garnered attention to overcome the drawbacks of conventional CMOS-based technologies.","However, such technologies must be evaluated before deployment under real workloads and architecture.","But there is a lack of available open-source STT-RAM-based system evaluation framework, which hampers research and experimentation and impacts the adoption of STT- RAM in a system.","This paper proposes a novel, extendable STT-RAM memory controller design integrated inside the gem5 simulator.","Our framework enables understanding various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system throughput.","We will open-source our HOPE framework, which will fuel research and aid in accelerating the development of future system architectures based on STT-RAM.","It will also facilitate the user for further tool enhancement."],"url":"http://arxiv.org/abs/2401.14888v1","category":"cs.AR"}
{"created":"2024-01-26 14:14:59","title":"The Power of Noise: Redefining Retrieval for RAG Systems","abstract":"Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting our initial assumption of diminished quality. These findings call for developing specialized approaches tailored to the specific demands of integrating retrieval with language generation models and pave the way for future research. These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field.","sentences":["Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs).","RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window.","Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems.","Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems.","This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved.","We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context.","Our findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting our initial assumption of diminished quality.","These findings call for developing specialized approaches tailored to the specific demands of integrating retrieval with language generation models and pave the way for future research.","These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field."],"url":"http://arxiv.org/abs/2401.14887v1","category":"cs.IR"}
{"created":"2024-01-26 14:14:52","title":"Coca: Improving and Explaining Graph Neural Network-Based Vulnerability Detection Systems","abstract":"Recently, Graph Neural Network (GNN)-based vulnerability detection systems have achieved remarkable success. However, the lack of explainability poses a critical challenge to deploy black-box models in security-related domains. For this reason, several approaches have been proposed to explain the decision logic of the detection model by providing a set of crucial statements positively contributing to its predictions. Unfortunately, due to the weakly-robust detection models and suboptimal explanation strategy, they have the danger of revealing spurious correlations and redundancy issue.   In this paper, we propose Coca, a general framework aiming to 1) enhance the robustness of existing GNN-based vulnerability detection models to avoid spurious explanations; and 2) provide both concise and effective explanations to reason about the detected vulnerabilities. \\sysname consists of two core parts referred to as Trainer and Explainer. The former aims to train a detection model which is robust to random perturbation based on combinatorial contrastive learning, while the latter builds an explainer to derive crucial code statements that are most decisive to the detected vulnerability via dual-view causal inference as explanations. We apply Coca over three typical GNN-based vulnerability detectors. Experimental results show that Coca can effectively mitigate the spurious correlation issue, and provide more useful high-quality explanations.","sentences":["Recently, Graph Neural Network (GNN)-based vulnerability detection systems have achieved remarkable success.","However, the lack of explainability poses a critical challenge to deploy black-box models in security-related domains.","For this reason, several approaches have been proposed to explain the decision logic of the detection model by providing a set of crucial statements positively contributing to its predictions.","Unfortunately, due to the weakly-robust detection models and suboptimal explanation strategy, they have the danger of revealing spurious correlations and redundancy issue.   ","In this paper, we propose Coca, a general framework aiming to 1) enhance the robustness of existing GNN-based vulnerability detection models to avoid spurious explanations; and 2) provide both concise and effective explanations to reason about the detected vulnerabilities.","\\sysname consists of two core parts referred to as Trainer and Explainer.","The former aims to train a detection model which is robust to random perturbation based on combinatorial contrastive learning, while the latter builds an explainer to derive crucial code statements that are most decisive to the detected vulnerability via dual-view causal inference as explanations.","We apply Coca over three typical GNN-based vulnerability detectors.","Experimental results show that Coca can effectively mitigate the spurious correlation issue, and provide more useful high-quality explanations."],"url":"http://arxiv.org/abs/2401.14886v1","category":"cs.CR"}
{"created":"2024-01-26 14:12:35","title":"Neuromorphic quadratic programming for efficient and scalable model predictive control","abstract":"Applications in robotics or other size-, weight- and power-constrained autonomous systems at the edge often require real-time and low-energy solutions to large optimization problems. Event-based and memory-integrated neuromorphic architectures promise to solve such optimization problems with superior energy efficiency and performance compared to conventional von Neumann architectures. Here, we present a method to solve convex continuous optimization problems with quadratic cost functions and linear constraints on Intel's scalable neuromorphic research chip Loihi 2. When applied to model predictive control (MPC) problems for the quadruped robotic platform ANYmal, this method achieves over two orders of magnitude reduction in combined energy-delay product compared to the state-of-the-art solver, OSQP, on (edge) CPUs and GPUs with solution times under ten milliseconds for various problem sizes. These results demonstrate the benefit of non-von-Neumann architectures for robotic control applications.","sentences":["Applications in robotics or other size-, weight- and power-constrained autonomous systems at the edge often require real-time and low-energy solutions to large optimization problems.","Event-based and memory-integrated neuromorphic architectures promise to solve such optimization problems with superior energy efficiency and performance compared to conventional von Neumann architectures.","Here, we present a method to solve convex continuous optimization problems with quadratic cost functions and linear constraints on Intel's scalable neuromorphic research chip Loihi 2.","When applied to model predictive control (MPC) problems for the quadruped robotic platform ANYmal, this method achieves over two orders of magnitude reduction in combined energy-delay product compared to the state-of-the-art solver, OSQP, on (edge) CPUs and GPUs with solution times under ten milliseconds for various problem sizes.","These results demonstrate the benefit of non-von-Neumann architectures for robotic control applications."],"url":"http://arxiv.org/abs/2401.14885v1","category":"cs.NE"}
{"created":"2024-01-26 14:08:43","title":"P3LS: Partial Least Squares under Privacy Preservation","abstract":"Modern manufacturing value chains require intelligent orchestration of processes across company borders in order to maximize profits while fostering social and environmental sustainability. However, the implementation of integrated, systems-level approaches for data-informed decision-making along value chains is currently hampered by privacy concerns associated with cross-organizational data exchange and integration. We here propose Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated learning technique that enables cross-organizational data integration and process modeling with privacy guarantees. P3LS involves a singular value decomposition (SVD) based PLS algorithm and employs removable, random masks generated by a trusted authority in order to protect the privacy of the data contributed by each data holder. We demonstrate the capability of P3LS to vertically integrate process data along a hypothetical value chain consisting of three parties and to improve the prediction performance on several process-related key performance indicators. Furthermore, we show the numerical equivalence of P3LS and PLS model components on simulated data and provide a thorough privacy analysis of the former. Moreover, we propose a mechanism for determining the relevance of the contributed data to the problem being addressed, thus creating a basis for quantifying the contribution of participants.","sentences":["Modern manufacturing value chains require intelligent orchestration of processes across company borders in order to maximize profits while fostering social and environmental sustainability.","However, the implementation of integrated, systems-level approaches for data-informed decision-making along value chains is currently hampered by privacy concerns associated with cross-organizational data exchange and integration.","We here propose Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated learning technique that enables cross-organizational data integration and process modeling with privacy guarantees.","P3LS involves a singular value decomposition (SVD) based PLS algorithm and employs removable, random masks generated by a trusted authority in order to protect the privacy of the data contributed by each data holder.","We demonstrate the capability of P3LS to vertically integrate process data along a hypothetical value chain consisting of three parties and to improve the prediction performance on several process-related key performance indicators.","Furthermore, we show the numerical equivalence of P3LS and PLS model components on simulated data and provide a thorough privacy analysis of the former.","Moreover, we propose a mechanism for determining the relevance of the contributed data to the problem being addressed, thus creating a basis for quantifying the contribution of participants."],"url":"http://arxiv.org/abs/2401.14884v1","category":"stat.ML"}
{"created":"2024-01-26 14:07:18","title":"Analyzing the concept of technical debt in the context of agile software development: A systematic literature review","abstract":"Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. In recent years, it has gained significant attention in agile software development (ASD). The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD. Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed. We found five research areas of interest related to the literature of TD in ASD. Among those areas, managing TD in ASD received the highest attention, followed by architecture in ASD and its relationship with TD. In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified. Focus on quick delivery and architectural and design issues were the most popular causes of incurring TD in ASD. Reduced productivity, system degradation and increased maintenance cost were identified as significant consequences of incurring TD in ASD. Additionally, we found 12 strategies for managing TD in the context of ASD, out of which refactoring and enhancing the visibility of TD were the most significant. The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation.","sentences":["Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders.","In recent years, it has gained significant attention in agile software development (ASD).","The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD.","Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed.","We found five research areas of interest related to the literature of TD in ASD.","Among those areas, managing TD in ASD received the highest attention, followed by architecture in ASD and its relationship with TD.","In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified.","Focus on quick delivery and architectural and design issues were the most popular causes of incurring TD in ASD.","Reduced productivity, system degradation and increased maintenance cost were identified as significant consequences of incurring TD in ASD.","Additionally, we found 12 strategies for managing TD in the context of ASD, out of which refactoring and enhancing the visibility of TD were the most significant.","The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation."],"url":"http://arxiv.org/abs/2401.14882v1","category":"cs.SE"}
{"created":"2024-01-26 14:07:16","title":"Online Bin Covering with Frequency Predictions","abstract":"We study the discrete bin covering problem where a multiset of items from a fixed set $S \\subseteq (0,1]$ must be split into disjoint subsets while maximizing the number of subsets whose contents sum to at least $1$. We study the online discrete variant, where $S$ is finite, and items arrive sequentially. In the purely online setting, we show that the competitive ratios of best deterministic (and randomized) algorithms converge to $\\frac{1}{2}$ for large $S$, similar to the continuous setting. Therefore, we consider the problem under the prediction setting, where algorithms may access a vector of frequencies predicting the frequency of items of each size in the instance. In this setting, we introduce a family of online algorithms that perform near-optimally when the predictions are correct. Further, we introduce a second family of more robust algorithms that presents a tradeoff between the performance guarantees when the predictions are perfect and when predictions are adversarial. Finally, we consider a stochastic setting where items are drawn independently from any fixed but unknown distribution of $S$. Using results from the PAC-learnability of probabilities in discrete distributions, we also introduce a purely online algorithm whose average-case performance is near-optimal with high probability for all finite sets $S$ and all distributions of $S$.","sentences":["We study the discrete bin covering problem where a multiset of items from a fixed set $S \\subseteq (0,1]$ must be split into disjoint subsets while maximizing the number of subsets whose contents sum to at least $1$. We study the online discrete variant, where $S$ is finite, and items arrive sequentially.","In the purely online setting, we show that the competitive ratios of best deterministic (and randomized) algorithms converge to $\\frac{1}{2}$ for large $S$, similar to the continuous setting.","Therefore, we consider the problem under the prediction setting, where algorithms may access a vector of frequencies predicting the frequency of items of each size in the instance.","In this setting, we introduce a family of online algorithms that perform near-optimally when the predictions are correct.","Further, we introduce a second family of more robust algorithms that presents a tradeoff between the performance guarantees when the predictions are perfect and when predictions are adversarial.","Finally, we consider a stochastic setting where items are drawn independently from any fixed but unknown distribution of $S$. Using results from the PAC-learnability of probabilities in discrete distributions, we also introduce a purely online algorithm whose average-case performance is near-optimal with high probability for all finite sets $S$ and all distributions of $S$."],"url":"http://arxiv.org/abs/2401.14881v1","category":"cs.DS"}
{"created":"2024-01-26 14:05:29","title":"Reduction of self-heating effects in GaN HEMT via h-BN passivation and lift-off transfer to diamond substrate: a simulation study","abstract":"In this article, we investigate through numerical simulation the reduction of self-heating effects (SHEs) in GaN HEMT via the integration of hexagonal boron nitride (h-BN) as a passivation layer and as a release layer to transfer GaN HEMT to diamond substrate. The obtained devices exhibit improved thermal performance compared to SiO2/GaN/sapphire HEMT. The lattice temperature was reduced from 507 K in SiO2/GaN/sapphire to 372 K in h-BN/GaN/diamond HEMT. The temperature decrease enhances the drain current and transconductance to 900 mA/mm and 250 mS/mm, corresponding to a 47 % improvement. In addition, the total thermal resistance Rth is reduced by a factor of 5 from 27 K.mm/W in GaN/sapphire HEMT to 5.5 K.mm/W in GaN/diamond HEMT. This study indicates that h-BN integration in GaN HEMT as a top heat spreader and a release layer for transfer to diamond substrate can be a promising solution to reduce self-heating effects and extend the device lifetime and reliability.","sentences":["In this article, we investigate through numerical simulation the reduction of self-heating effects (SHEs) in GaN HEMT via the integration of hexagonal boron nitride (h-BN) as a passivation layer and as a release layer to transfer GaN HEMT to diamond substrate.","The obtained devices exhibit improved thermal performance compared to SiO2/GaN/sapphire HEMT.","The lattice temperature was reduced from 507 K in SiO2/GaN/sapphire to 372 K in h-BN/GaN/diamond HEMT.","The temperature decrease enhances the drain current and transconductance to 900 mA/mm and 250 mS/mm, corresponding to a 47 % improvement.","In addition, the total thermal resistance Rth is reduced by a factor of 5 from 27 K.mm/W in GaN/sapphire HEMT to 5.5 K.mm/W in GaN/diamond HEMT.","This study indicates that h-BN integration in GaN HEMT as a top heat spreader and a release layer for transfer to diamond substrate can be a promising solution to reduce self-heating effects and extend the device lifetime and reliability."],"url":"http://arxiv.org/abs/2401.14880v1","category":"physics.comp-ph"}
{"created":"2024-01-26 14:05:10","title":"Fast Long-Term Multi-Scenario Prediction for Maneuver Planning at Unsignalized Intersections","abstract":"Motion prediction for intelligent vehicles typically focuses on estimating the most probable future evolutions of a traffic scenario. Estimating the gap acceptance, i.e., whether a vehicle merges or crosses before another vehicle with the right of way, is often handled implicitly in the prediction. However, an infrastructure-based maneuver planning can assign artificial priorities between cooperative vehicles, so it needs to evaluate many more potential scenarios. Additionally, the prediction horizon has to be long enough to assess the impact of a maneuver. We, therefore, present a novel long-term prediction approach handling the gap acceptance estimation and the velocity prediction in two separate stages. Thereby, the behavior of regular vehicles as well as priority assignments of cooperative vehicles can be considered. We train both stages on real-world traffic observations to achieve realistic prediction results. Our method has a competitive accuracy and is fast enough to predict a multitude of scenarios in a short time, making it suitable to be used in a maneuver planning framework.","sentences":["Motion prediction for intelligent vehicles typically focuses on estimating the most probable future evolutions of a traffic scenario.","Estimating the gap acceptance, i.e., whether a vehicle merges or crosses before another vehicle with the right of way, is often handled implicitly in the prediction.","However, an infrastructure-based maneuver planning can assign artificial priorities between cooperative vehicles, so it needs to evaluate many more potential scenarios.","Additionally, the prediction horizon has to be long enough to assess the impact of a maneuver.","We, therefore, present a novel long-term prediction approach handling the gap acceptance estimation and the velocity prediction in two separate stages.","Thereby, the behavior of regular vehicles as well as priority assignments of cooperative vehicles can be considered.","We train both stages on real-world traffic observations to achieve realistic prediction results.","Our method has a competitive accuracy and is fast enough to predict a multitude of scenarios in a short time, making it suitable to be used in a maneuver planning framework."],"url":"http://arxiv.org/abs/2401.14879v1","category":"cs.RO"}
{"created":"2024-01-26 14:04:24","title":"Hades Again and Again: A Study on Frustration Tolerance, Physiology and Player Experience","abstract":"Accurately quantifying player experience is challenging for many reasons: identifying a ground truth and building validated and reliable scales are both challenging tasks; on top of that, empirical results are often moderated by individual factors. In this article, we present a study on the rogue-like game Hades designed to investigate the impact of individual differences in the operationalisation of player experience by cross-referencing multiple modalities (i.e., questionnaires, gameplay, and heart rate) and identifying the interplay between their scales.","sentences":["Accurately quantifying player experience is challenging for many reasons: identifying a ground truth and building validated and reliable scales are both challenging tasks; on top of that, empirical results are often moderated by individual factors.","In this article, we present a study on the rogue-like game Hades designed to investigate the impact of individual differences in the operationalisation of player experience by cross-referencing multiple modalities (i.e., questionnaires, gameplay, and heart rate) and identifying the interplay between their scales."],"url":"http://arxiv.org/abs/2401.14878v1","category":"cs.HC"}
{"created":"2024-01-26 14:02:29","title":"Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem","abstract":"The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the context of GCNs. This serves as a foundation for combining it with the attribute-based filter to capture the adaptive-frequency information. Finally, we derive the cross-space filter via an effective multiple-kernel learning strategy, which unifies the attribute-based high-pass filter and the topology-based low-pass filter. This helps to address the over-smoothing problem while maintaining effectiveness. Extensive experiments demonstrate that CSF not only successfully alleviates the over-smoothing problem but also promotes the effectiveness of the node classification task.","sentences":["The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep.","To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology.","However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs.","In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces.","Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression.","Then, we cast the topology-based low-pass filter as a Mercer's kernel within the context of GCNs.","This serves as a foundation for combining it with the attribute-based filter to capture the adaptive-frequency information.","Finally, we derive the cross-space filter via an effective multiple-kernel learning strategy, which unifies the attribute-based high-pass filter and the topology-based low-pass filter.","This helps to address the over-smoothing problem while maintaining effectiveness.","Extensive experiments demonstrate that CSF not only successfully alleviates the over-smoothing problem but also promotes the effectiveness of the node classification task."],"url":"http://arxiv.org/abs/2401.14876v1","category":"cs.LG"}
{"created":"2024-01-26 13:59:38","title":"Lessons from discrete light-cone quantization for physics at null infinity: Bosons in two dimensions","abstract":"Motivated by issues in the context of asymptotically flat spacetimes at null infinity, we discuss in the simplest example of a massless scalar field in two dimensions several subtleties that arise when setting up the canonical formulation on a single or on two intersecting null hyperplanes with a special emphasis on the infinite-dimensional global and conformal symmetries and their canonical generators, the free data, a consistent treatment of zero modes, matching conditions, and implications for quantization of massless versus massive fields.","sentences":["Motivated by issues in the context of asymptotically flat spacetimes at null infinity, we discuss in the simplest example of a massless scalar field in two dimensions several subtleties that arise when setting up the canonical formulation on a single or on two intersecting null hyperplanes with a special emphasis on the infinite-dimensional global and conformal symmetries and their canonical generators, the free data, a consistent treatment of zero modes, matching conditions, and implications for quantization of massless versus massive fields."],"url":"http://arxiv.org/abs/2401.14873v1","category":"hep-th"}
{"created":"2024-01-26 13:55:32","title":"F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods","abstract":"Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities.","sentences":["Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs.","However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage.","Previous subjective evaluation methods mainly reply on scoring by API models.","However, in the absence of references, large models have shown limited ability to discern subtle differences.","To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic.","The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks.","For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models.","We conduct evaluations on 13 advanced LLMs.","Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators.","Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods.","We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities."],"url":"http://arxiv.org/abs/2401.14869v1","category":"cs.CL"}
{"created":"2024-01-26 13:42:12","title":"Implicit Neural Representation for Physics-driven Actuated Soft Bodies","abstract":"Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.","sentences":["Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation.","Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks.","Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value.","This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable.","We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems.","We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time."],"url":"http://arxiv.org/abs/2401.14861v1","category":"cs.CV"}
{"created":"2024-01-26 13:38:28","title":"RESPRECT: Speeding-up Multi-fingered Grasping with Residual Reinforcement Learning","abstract":"Deep Reinforcement Learning (DRL) has proven effective in learning control policies using robotic grippers, but much less practical for solving the problem of grasping with dexterous hands -- especially on real robotic platforms -- due to the high dimensionality of the problem. In this work, we focus on the multi-fingered grasping task with the anthropomorphic hand of the iCub humanoid. We propose the RESidual learning with PREtrained CriTics (RESPRECT) method that, starting from a policy pre-trained on a large set of objects, can learn a residual policy to grasp a novel object in a fraction ($\\sim 5 \\times$ faster) of the timesteps required to train a policy from scratch, without requiring any task demonstration. To our knowledge, this is the first Residual Reinforcement Learning (RRL) approach that learns a residual policy on top of another policy pre-trained with DRL. We exploit some components of the pre-trained policy during residual learning that further speed-up the training. We benchmark our results in the iCub simulated environment, and we show that RESPRECT can be effectively used to learn a multi-fingered grasping policy on the real iCub robot. The code to reproduce the experiments is released together with the paper with an open source license.","sentences":["Deep Reinforcement Learning (DRL) has proven effective in learning control policies using robotic grippers, but much less practical for solving the problem of grasping with dexterous hands -- especially on real robotic platforms -- due to the high dimensionality of the problem.","In this work, we focus on the multi-fingered grasping task with the anthropomorphic hand of the iCub humanoid.","We propose the RESidual learning with PREtrained CriTics (RESPRECT) method that, starting from a policy pre-trained on a large set of objects, can learn a residual policy to grasp a novel object in a fraction ($\\sim 5 \\times$ faster) of the timesteps required to train a policy from scratch, without requiring any task demonstration.","To our knowledge, this is the first Residual Reinforcement Learning (RRL) approach that learns a residual policy on top of another policy pre-trained with DRL.","We exploit some components of the pre-trained policy during residual learning that further speed-up the training.","We benchmark our results in the iCub simulated environment, and we show that RESPRECT can be effectively used to learn a multi-fingered grasping policy on the real iCub robot.","The code to reproduce the experiments is released together with the paper with an open source license."],"url":"http://arxiv.org/abs/2401.14858v1","category":"cs.RO"}
{"created":"2024-01-26 13:36:46","title":"LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering","abstract":"We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion.   This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels. Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements.   Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains.   We release our software and hardware and self-collected datasets on Github\\footnote[3]{https://github.com/sheng00125/LIV-GaussMap} to benefit the community.","sentences":["We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy.","Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion.   ","This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity.","The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels.","Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements.   ","Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes.","bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets.","It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains.   ","We release our software and hardware and self-collected datasets on Github\\footnote[3]{https://github.com/sheng00125/LIV-GaussMap} to benefit the community."],"url":"http://arxiv.org/abs/2401.14857v1","category":"cs.RO"}
{"created":"2024-01-26 13:36:12","title":"Memory-Inspired Temporal Prompt Interaction for Text-Image Classification","abstract":"In recent years, large-scale pre-trained multimodal models (LMM) generally emerge to integrate the vision and language modalities, achieving considerable success in various natural language processing and computer vision tasks. The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks. Hence, prompt-based interaction strategy is studied to align modalities more efficiently. In this contex, we propose a novel prompt-based multimodal interaction strategy inspired by human memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our proposed method involves in two stages as in human memory strategy: the acquiring stage, and the consolidation and activation stage. We utilize temporal prompts on intermediate layers to imitate the acquiring stage, leverage similarity-based prompt interaction to imitate memory consolidation, and employ prompt generation strategy to imitate memory activation. The main strength of our paper is that we interact the prompt vectors on intermediate layers to leverage sufficient information exchange between modalities, with compressed trainable parameters and memory usage. We achieve competitive results on several datasets with relatively small memory usage and 2.0M of trainable parameters (about 1% of the pre-trained foundation model).","sentences":["In recent years, large-scale pre-trained multimodal models (LMM) generally emerge to integrate the vision and language modalities, achieving considerable success in various natural language processing and computer vision tasks.","The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks.","Hence, prompt-based interaction strategy is studied to align modalities more efficiently.","In this contex, we propose a novel prompt-based multimodal interaction strategy inspired by human memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP).","Our proposed method involves in two stages as in human memory strategy: the acquiring stage, and the consolidation and activation stage.","We utilize temporal prompts on intermediate layers to imitate the acquiring stage, leverage similarity-based prompt interaction to imitate memory consolidation, and employ prompt generation strategy to imitate memory activation.","The main strength of our paper is that we interact the prompt vectors on intermediate layers to leverage sufficient information exchange between modalities, with compressed trainable parameters and memory usage.","We achieve competitive results on several datasets with relatively small memory usage and 2.0M of trainable parameters (about 1% of the pre-trained foundation model)."],"url":"http://arxiv.org/abs/2401.14856v1","category":"cs.CV"}
{"created":"2024-01-26 13:35:21","title":"Dimensional gain in sensing through higher-dimensional quantum spin chain","abstract":"Recent breakthroughs in quantum technology pave the way for extensive utilization of higher-dimensional quantum systems, which outperform their qubit counterparts in terms of capabilities and versatility. We present a framework for accurately predicting weak external magnetic fields using a higher-dimensional many-body quantum probe. We demonstrate that dimension serves as a valuable resource for quantum sensing when a transverse spin-s Ising chain interacts locally with a magnetic field whose strength has to be determined. We observe the distinct performance of sensors for spin chains with half-integer and integer spins. Furthermore, we highlight that the time duration appropriate for quantum-enhanced sensing increases with the increase of dimension. Additionally, we observe that, in addition to nearest-neighbor interactions, incorporating interactions between the next nearest-neighbor sites increases sensing precision, particularly for spin chains with integer spins. We also prove the dimensional-dependence of the bound on quantum Fisher information which provides the limit on the precision in estimating parameters.","sentences":["Recent breakthroughs in quantum technology pave the way for extensive utilization of higher-dimensional quantum systems, which outperform their qubit counterparts in terms of capabilities and versatility.","We present a framework for accurately predicting weak external magnetic fields using a higher-dimensional many-body quantum probe.","We demonstrate that dimension serves as a valuable resource for quantum sensing when a transverse spin-s Ising chain interacts locally with a magnetic field whose strength has to be determined.","We observe the distinct performance of sensors for spin chains with half-integer and integer spins.","Furthermore, we highlight that the time duration appropriate for quantum-enhanced sensing increases with the increase of dimension.","Additionally, we observe that, in addition to nearest-neighbor interactions, incorporating interactions between the next nearest-neighbor sites increases sensing precision, particularly for spin chains with integer spins.","We also prove the dimensional-dependence of the bound on quantum Fisher information which provides the limit on the precision in estimating parameters."],"url":"http://arxiv.org/abs/2401.14853v1","category":"quant-ph"}
{"created":"2024-01-26 13:30:50","title":"Observing Kinematic Anisotropies of the Stochastic Background with LISA","abstract":"We propose a diagnostic tool for future analyses of stochastic gravitational wave background signals of extra-galactic origin in LISA data. Next-generation gravitational wave detectors hold the capability to track unresolved gravitational waves bundled into a stochastic background. This composite background contains cosmological and astrophysical contributions, the exploration of which offers promising avenues for groundbreaking new insights into very early universe cosmology as well as late-time structure formation. In this article, we develop a full end-to-end pipeline for the extraction of extra-galactic signals, based on kinematic anisotropies arising from the galactic motion, via full-time-domain simulations of LISA's response to the gravitational wave anisotropic sky. Employing a Markov-Chain-Monte-Carlo map-making scheme, multipoles up to $\\ell=2$ are recovered for scale-free spectra that support an interpretation as signals originating from cosmic strings in the case of a high signal-to-noise ratio. We demonstrate that our analysis is consistently beating cosmic variance and is robust against statistical and systematic errors. The impact of instrumental noise on the extraction of kinematic anisotropies is investigated, and we establish a detection threshold of $\\Omega_{GW}\\gtrsim 5\\times 10^{-8}$ in the presence of instrument-induced noise. Potential avenues for improvement in our methodology are highlighted.","sentences":["We propose a diagnostic tool for future analyses of stochastic gravitational wave background signals of extra-galactic origin in LISA data.","Next-generation gravitational wave detectors hold the capability to track unresolved gravitational waves bundled into a stochastic background.","This composite background contains cosmological and astrophysical contributions, the exploration of which offers promising avenues for groundbreaking new insights into very early universe cosmology as well as late-time structure formation.","In this article, we develop a full end-to-end pipeline for the extraction of extra-galactic signals, based on kinematic anisotropies arising from the galactic motion, via full-time-domain simulations of LISA's response to the gravitational wave anisotropic sky.","Employing a Markov-Chain-Monte-Carlo map-making scheme, multipoles up to $\\ell=2$ are recovered for scale-free spectra that support an interpretation as signals originating from cosmic strings in the case of a high signal-to-noise ratio.","We demonstrate that our analysis is consistently beating cosmic variance and is robust against statistical and systematic errors.","The impact of instrumental noise on the extraction of kinematic anisotropies is investigated, and we establish a detection threshold of $\\Omega_{GW}\\gtrsim 5\\times 10^{-8}$ in the presence of instrument-induced noise.","Potential avenues for improvement in our methodology are highlighted."],"url":"http://arxiv.org/abs/2401.14849v1","category":"gr-qc"}
{"created":"2024-01-26 13:27:35","title":"Extracting Process-Aware Decision Models from Object-Centric Process Data","abstract":"Organizations execute decisions within business processes on a daily basis whilst having to take into account multiple stakeholders who might require multiple point of views of the same process. Moreover, the complexity of the information systems running these business processes is generally high as they are linked to databases storing all the relevant data and aspects of the processes. Given the presence of multiple objects within an information system which support the processes in their enactment, decisions are naturally influenced by both these perspectives, logged in object-centric process logs. However, the discovery of such decisions from object-centric process logs is not straightforward as it requires to correctly link the involved objects whilst considering the sequential constraints that business processes impose as well as correctly discovering what a decision actually does. This paper proposes the first object-centric decision-mining algorithm called Integrated Object-centric Decision Discovery Algorithm (IODDA). IODDA is able to discover how a decision is structured as well as how a decision is made. Moreover, IODDA is able to discover which activities and object types are involved in the decision-making process. Next, IODDA is demonstrated with the first artificial knowledge-intensive process logs whose log generators are provided to the research community.","sentences":["Organizations execute decisions within business processes on a daily basis whilst having to take into account multiple stakeholders who might require multiple point of views of the same process.","Moreover, the complexity of the information systems running these business processes is generally high as they are linked to databases storing all the relevant data and aspects of the processes.","Given the presence of multiple objects within an information system which support the processes in their enactment, decisions are naturally influenced by both these perspectives, logged in object-centric process logs.","However, the discovery of such decisions from object-centric process logs is not straightforward as it requires to correctly link the involved objects whilst considering the sequential constraints that business processes impose as well as correctly discovering what a decision actually does.","This paper proposes the first object-centric decision-mining algorithm called Integrated Object-centric Decision Discovery Algorithm (IODDA).","IODDA is able to discover how a decision is structured as well as how a decision is made.","Moreover, IODDA is able to discover which activities and object types are involved in the decision-making process.","Next, IODDA is demonstrated with the first artificial knowledge-intensive process logs whose log generators are provided to the research community."],"url":"http://arxiv.org/abs/2401.14847v1","category":"cs.LG"}
{"created":"2024-01-26 13:27:15","title":"Understanding Domain Generalization: A Noise Robustness Perspective","abstract":"Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice.","sentences":["Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks.","To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise.","Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization.","Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present.","Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments.","However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM.","We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice."],"url":"http://arxiv.org/abs/2401.14846v1","category":"cs.LG"}
{"created":"2024-01-26 13:24:45","title":"Adaptive Point Transformer","abstract":"The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces computational complexity while maintaining competitive accuracy compared to standard PTs. The code for AdaPT is made publicly available.","sentences":["The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing.","While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications.","To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism.","AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds.","Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models.","Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces computational complexity while maintaining competitive accuracy compared to standard PTs.","The code for AdaPT is made publicly available."],"url":"http://arxiv.org/abs/2401.14845v1","category":"cs.CV"}
{"created":"2024-01-26 13:12:52","title":"GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption","abstract":"Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all while preserving the privacy of the input data and ML model. We demonstrate the real-world applicability of our construction by developing and evaluating an HHE-based PPML application for classifying heart disease based on sensitive ECG data. Notably, our evaluations revealed a slight reduction in accuracy compared to inference on plaintext data. Additionally, both the analyst and end devices experience minimal communication and computation costs, underscoring the practical viability of our approach. The successful integration of HHE into PPML provides a glimpse into a more secure and privacy-conscious future for machine learning on relatively constrained end devices.","sentences":["Machine Learning (ML) has emerged as one of data science's most transformative and influential domains.","However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models.","To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models.","One such approach is the use of Homomorphic Encryption (HE).","However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios.","Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges.","Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices.","We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all while preserving the privacy of the input data and ML model.","We demonstrate the real-world applicability of our construction by developing and evaluating an HHE-based PPML application for classifying heart disease based on sensitive ECG data.","Notably, our evaluations revealed a slight reduction in accuracy compared to inference on plaintext data.","Additionally, both the analyst and end devices experience minimal communication and computation costs, underscoring the practical viability of our approach.","The successful integration of HHE into PPML provides a glimpse into a more secure and privacy-conscious future for machine learning on relatively constrained end devices."],"url":"http://arxiv.org/abs/2401.14840v1","category":"cs.LG"}
{"created":"2024-01-26 13:07:59","title":"Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring","abstract":"Driver Action Recognition (DAR) is crucial in vehicle cabin monitoring systems. In real-world applications, it is common for vehicle cabins to be equipped with cameras featuring different modalities. However, multi-modality fusion strategies for the DAR task within car cabins have rarely been studied. In this paper, we propose a novel yet efficient multi-modality driver action recognition method based on dual feature shift, named DFS. DFS first integrates complementary features across modalities by performing modality feature interaction. Meanwhile, DFS achieves the neighbour feature propagation within single modalities, by feature shifting among temporal frames. To learn common patterns and improve model efficiency, DFS shares feature extracting stages among multiple modalities. Extensive experiments have been carried out to verify the effectiveness of the proposed DFS model on the Drive\\&Act dataset. The results demonstrate that DFS achieves good performance and improves the efficiency of multi-modality driver action recognition.","sentences":["Driver Action Recognition (DAR) is crucial in vehicle cabin monitoring systems.","In real-world applications, it is common for vehicle cabins to be equipped with cameras featuring different modalities.","However, multi-modality fusion strategies for the DAR task within car cabins have rarely been studied.","In this paper, we propose a novel yet efficient multi-modality driver action recognition method based on dual feature shift, named DFS.","DFS first integrates complementary features across modalities by performing modality feature interaction.","Meanwhile, DFS achieves the neighbour feature propagation within single modalities, by feature shifting among temporal frames.","To learn common patterns and improve model efficiency, DFS shares feature extracting stages among multiple modalities.","Extensive experiments have been carried out to verify the effectiveness of the proposed DFS model on the Drive\\&Act dataset.","The results demonstrate that DFS achieves good performance and improves the efficiency of multi-modality driver action recognition."],"url":"http://arxiv.org/abs/2401.14838v1","category":"cs.CV"}
{"created":"2024-01-26 13:02:09","title":"Hot electrons and singlet-fission dark excitons modulate strong-coupling conditions in metal-organic optical microcavities","abstract":"Polaritons, formed as a result of strong mixing between light and matter, are promising for numerous applications including organic solar cells, optical logic gates, and qubits. In low-Q organic optical microcavities, polaritonic signatures due to strong hybridization between photons and Frenkel excitons were found to decay together with the dynamics of dark excitons. A conundrum, however, remained whether dark excitons modulate exciton-photon coupling strength. It also remained unclear how dark excitons in the organic layer and hot electrons in the metal layers contribute to shaping the long-lived optical response at the energies of the hybrid states. Here, we identified that due to delocalization of polaritons over both organic and metal layers, they are sensitive to the effects of both dark excitons and hot electrons. We observed that the dynamics of dark excitons modulate exciton-photon strong coupling (Rabi energy). The role of metal layers is to contribute absorptive components near the energies of the polaritonic branches; contributions from hot electrons have also been detected. These and other mechanistic insights into the dynamics of strong-coupling conditions were supported by theoretical analysis based on non-Hermitian Hamiltonian mechanics, axially-resolved transfer-matrix simulations, global analysis of pump-probe spectra, and statistical correlation analysis. The developed methodology can be applied to other microcavity structures. Our findings pave the way for disentangling pure polaritonic effects from other excitations in organic and metal layers, with the ultimate aim of achieving photonic control over photophysical and photochemical processes.","sentences":["Polaritons, formed as a result of strong mixing between light and matter, are promising for numerous applications including organic solar cells, optical logic gates, and qubits.","In low-Q organic optical microcavities, polaritonic signatures due to strong hybridization between photons and Frenkel excitons were found to decay together with the dynamics of dark excitons.","A conundrum, however, remained whether dark excitons modulate exciton-photon coupling strength.","It also remained unclear how dark excitons in the organic layer and hot electrons in the metal layers contribute to shaping the long-lived optical response at the energies of the hybrid states.","Here, we identified that due to delocalization of polaritons over both organic and metal layers, they are sensitive to the effects of both dark excitons and hot electrons.","We observed that the dynamics of dark excitons modulate exciton-photon strong coupling (Rabi energy).","The role of metal layers is to contribute absorptive components near the energies of the polaritonic branches; contributions from hot electrons have also been detected.","These and other mechanistic insights into the dynamics of strong-coupling conditions were supported by theoretical analysis based on non-Hermitian Hamiltonian mechanics, axially-resolved transfer-matrix simulations, global analysis of pump-probe spectra, and statistical correlation analysis.","The developed methodology can be applied to other microcavity structures.","Our findings pave the way for disentangling pure polaritonic effects from other excitations in organic and metal layers, with the ultimate aim of achieving photonic control over photophysical and photochemical processes."],"url":"http://arxiv.org/abs/2401.14835v1","category":"physics.app-ph"}
{"created":"2024-01-26 13:02:02","title":"From Differential Linear Logic to Coherent Differentiation","abstract":"In this survey, we present in a unified way the categorical and syntactical settings of coherent differentiation introduced recently, which shows that the basic ideas of differential linear logic and of the differential lambda-calculus are compatible with determinism. Indeed, due to the Leibniz rule of the differential calculus, differential linear logic and the differential lambda-calculus feature an operation of addition of proofs or terms operationally interpreted as a strong form of nondeterminism. The main idea of coherent differentiation is that these sums can be controlled and kept in the realm of determinism by means of a notion of summability, upon enforcing summability restrictions on the derivatives which can be written in the models and in the syntax.","sentences":["In this survey, we present in a unified way the categorical and syntactical settings of coherent differentiation introduced recently, which shows that the basic ideas of differential linear logic and of the differential lambda-calculus are compatible with determinism.","Indeed, due to the Leibniz rule of the differential calculus, differential linear logic and the differential lambda-calculus feature an operation of addition of proofs or terms operationally interpreted as a strong form of nondeterminism.","The main idea of coherent differentiation is that these sums can be controlled and kept in the realm of determinism by means of a notion of summability, upon enforcing summability restrictions on the derivatives which can be written in the models and in the syntax."],"url":"http://arxiv.org/abs/2401.14834v1","category":"cs.LO"}
{"created":"2024-01-26 13:01:55","title":"Accretion, greybody factor, quasinormal modes, power spectrum, sparsity of Hawking radiation, and weak gravitational lensing of a minimum measurable length inspired Schwarzchild black hole","abstract":"In this manuscript, we delve into an analytic and numerical probe of shadow with different accretion models, quasinormal modes, Hawking radiation, and gravitational lensing to study observational impacts of quantum effect introduced throughh linear-quadratic GUP(LQG). Our investigation reveals that the shadows of LQG modified black holes are smaller and brighter than Schwarzschild black holes. To examine the impact of the quantum correction on the quasinormal mode, linear-quadratic GUP modified black holes are explored under scalar and electromagnetic field perturbation. Here, linear-quadratic GUP is used to capture quantum corrections. It is observed that the incorporation of quantum correction by linear-quadratic GUP alters the singularity structure of the black hole. To compute the quasinormal modes of this linear-quadratic GUP-inspired quantum-corrected black holes, we compute the effective potential generated under the perturbation of scalar and electromagnetic field, and then we use the sixth-order WKB approach in conjunction with the appropriate numerical analysis. We find that the greybody factor decreases with the GUP parameter $\\alpha$ implying that the probability of transmission decreases with the GUP parameter. The total power emitted by LQG modified black hole is found to be greater than that emitted by Schwarzschild black hole. Finally, we study weak gravitational lensing and make a comparison with quadratic GUP and linear GUP modified black holes.","sentences":["In this manuscript, we delve into an analytic and numerical probe of shadow with different accretion models, quasinormal modes, Hawking radiation, and gravitational lensing to study observational impacts of quantum effect introduced throughh linear-quadratic GUP(LQG).","Our investigation reveals that the shadows of LQG modified black holes are smaller and brighter than Schwarzschild black holes.","To examine the impact of the quantum correction on the quasinormal mode, linear-quadratic GUP modified black holes are explored under scalar and electromagnetic field perturbation.","Here, linear-quadratic GUP is used to capture quantum corrections.","It is observed that the incorporation of quantum correction by linear-quadratic GUP alters the singularity structure of the black hole.","To compute the quasinormal modes of this linear-quadratic GUP-inspired quantum-corrected black holes, we compute the effective potential generated under the perturbation of scalar and electromagnetic field, and then we use the sixth-order WKB approach in conjunction with the appropriate numerical analysis.","We find that the greybody factor decreases with the GUP parameter $\\alpha$ implying that the probability of transmission decreases with the GUP parameter.","The total power emitted by LQG modified black hole is found to be greater than that emitted by Schwarzschild black hole.","Finally, we study weak gravitational lensing and make a comparison with quadratic GUP and linear GUP modified black holes."],"url":"http://arxiv.org/abs/2401.14833v1","category":"gr-qc"}
{"created":"2024-01-26 13:01:28","title":"Text Image Inpainting via Global Structure-Guided Diffusion Models","abstract":"Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM.","sentences":["Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure.","These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification.","Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles.","Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study.","In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively.","Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information.","On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution.","Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts.","The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality.","These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing.","Code and datasets are available at: https://github.com/blackprotoss/GSDM."],"url":"http://arxiv.org/abs/2401.14832v1","category":"cs.CV"}
{"created":"2024-01-26 12:59:26","title":"The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances","abstract":"Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of our model delivers a structured exploration of entities in a specific domain, their relationships and assigning results of a MV-under-test to construct an entity-relationship graph. Through clustering patterns of relations in the graph general MV deficits are arguable. In Summary, our work contributes to a more nuanced and systematized identification of deficits of a MV test object in correlation to holistic circumstances in HAD operating domains.","sentences":["Are we heading for an iceberg with the current testing of machine vision?","This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems.","Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies.","We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes.","As overlooked considerations can cost lives.","Our main contribution is the hierarchical level model, which we call Granularity Grades.","The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate.","This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes.","The application of our model delivers a structured exploration of entities in a specific domain, their relationships and assigning results of a MV-under-test to construct an entity-relationship graph.","Through clustering patterns of relations in the graph general MV deficits are arguable.","In Summary, our work contributes to a more nuanced and systematized identification of deficits of a MV test object in correlation to holistic circumstances in HAD operating domains."],"url":"http://arxiv.org/abs/2401.14831v1","category":"cs.RO"}
{"created":"2024-01-26 12:57:18","title":"UMBRELLA: A One-stop Shop Bridging the Gap from Lab to Real-World IoT Experimentation","abstract":"UMBRELLA is an open, large-scale IoT ecosystem deployed across South Gloucestershire, UK. It is intended to accelerate innovation across multiple technology domains. UMBRELLA is built to bridge the gap between existing specialised testbeds and address holistically real-world technological challenges in a System-of-Systems (SoS) fashion. UMBRELLA provides open access to real-world devices and infrastructure, enabling researchers and the industry to evaluate solutions for Smart Cities, Robotics, Wireless Communications, Edge Intelligence, and more. Key features include over 200 multi-sensor nodes installed on public infrastructure, a robotics arena with 20 mobile robots, a 5G network-in-a-box solution, and a unified backend platform for management, control and secure user access. The heterogeneity of hardware components, including diverse sensors, communication interfaces, and GPU-enabled edge devices, coupled with tools like digital twins, allows for comprehensive experimentation and benchmarking of innovative solutions not viable in lab environments. This paper provides a comprehensive overview of UMBRELLA's multi-domain architecture and capabilities, making it an ideal playground for Internet of Things (IoT) and Industrial IoT (IIoT) innovation. It discusses the challenges in designing, developing and operating UMBRELLA as an open, sustainable testbed and shares lessons learned to guide similar future initiatives. With its unique openness, heterogeneity, realism and tools, UMBRELLA aims to continue accelerating cutting-edge technology research, development and translation into real-world progress.","sentences":["UMBRELLA is an open, large-scale IoT ecosystem deployed across South Gloucestershire, UK.","It is intended to accelerate innovation across multiple technology domains.","UMBRELLA is built to bridge the gap between existing specialised testbeds and address holistically real-world technological challenges in a System-of-Systems (SoS) fashion.","UMBRELLA provides open access to real-world devices and infrastructure, enabling researchers and the industry to evaluate solutions for Smart Cities, Robotics, Wireless Communications, Edge Intelligence, and more.","Key features include over 200 multi-sensor nodes installed on public infrastructure, a robotics arena with 20 mobile robots, a 5G network-in-a-box solution, and a unified backend platform for management, control and secure user access.","The heterogeneity of hardware components, including diverse sensors, communication interfaces, and GPU-enabled edge devices, coupled with tools like digital twins, allows for comprehensive experimentation and benchmarking of innovative solutions not viable in lab environments.","This paper provides a comprehensive overview of UMBRELLA's multi-domain architecture and capabilities, making it an ideal playground for Internet of Things (IoT) and Industrial IoT (IIoT) innovation.","It discusses the challenges in designing, developing and operating UMBRELLA as an open, sustainable testbed and shares lessons learned to guide similar future initiatives.","With its unique openness, heterogeneity, realism and tools, UMBRELLA aims to continue accelerating cutting-edge technology research, development and translation into real-world progress."],"url":"http://arxiv.org/abs/2401.14829v1","category":"cs.NI"}
{"created":"2024-01-26 12:57:05","title":"TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts","abstract":"Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.","sentences":["Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness.","However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description.","To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region.","With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance.","Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box.","Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged.","Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2401.14828v1","category":"cs.CV"}
{"created":"2024-01-26 12:52:56","title":"Expressivity-aware Music Performance Retrieval using Mid-level Perceptual Features and Emotion Word Embeddings","abstract":"This paper explores a specific sub-task of cross-modal music retrieval. We consider the delicate task of retrieving a performance or rendition of a musical piece based on a description of its style, expressive character, or emotion from a set of different performances of the same piece. We observe that a general purpose cross-modal system trained to learn a common text-audio embedding space does not yield optimal results for this task. By introducing two changes -- one each to the text encoder and the audio encoder -- we demonstrate improved performance on a dataset of piano performances and associated free-text descriptions. On the text side, we use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings. Our results highlight the effectiveness of mid-level perceptual features learnt from music and emotion enriched word embeddings learnt from emotion-labelled text in capturing musical expression in a cross-modal setting. Additionally, our interpretable mid-level features provide a route for introducing explainability in the retrieval and downstream recommendation processes.","sentences":["This paper explores a specific sub-task of cross-modal music retrieval.","We consider the delicate task of retrieving a performance or rendition of a musical piece based on a description of its style, expressive character, or emotion from a set of different performances of the same piece.","We observe that a general purpose cross-modal system trained to learn a common text-audio embedding space does not yield optimal results for this task.","By introducing two changes -- one each to the text encoder and the audio encoder -- we demonstrate improved performance on a dataset of piano performances and associated free-text descriptions.","On the text side, we use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings.","Our results highlight the effectiveness of mid-level perceptual features learnt from music and emotion enriched word embeddings learnt from emotion-labelled text in capturing musical expression in a cross-modal setting.","Additionally, our interpretable mid-level features provide a route for introducing explainability in the retrieval and downstream recommendation processes."],"url":"http://arxiv.org/abs/2401.14826v1","category":"cs.SD"}
{"created":"2024-01-26 12:52:49","title":"Keeping the Harmony Between Neighbors: Local Fairness in Graph Fair Division","abstract":"We study the problem of allocating indivisible resources under the connectivity constraints of a graph $G$. This model, initially introduced by Bouveret et al. (published in IJCAI, 2017), effectively encompasses a diverse array of scenarios characterized by spatial or temporal limitations, including the division of land plots and the allocation of time plots. In this paper, we introduce a novel fairness concept that integrates local comparisons within the social network formed by a connected allocation of the item graph. Our particular focus is to achieve pairwise-maximin fair share (PMMS) among the \"neighbors\" within this network. For any underlying graph structure, we show that a connected allocation that maximizes Nash welfare guarantees a $(1/2)$-PMMS fairness. Moreover, for two agents, we establish that a $(3/4)$-PMMS allocation can be efficiently computed. Additionally, we demonstrate that for three agents and the items aligned on a path, a PMMS allocation is always attainable and can be computed in polynomial time. Lastly, when agents have identical additive utilities, we present a pseudo-polynomial-time algorithm for a $(3/4)$-PMMS allocation, irrespective of the underlying graph $G$. Furthermore, we provide a polynomial-time algorithm for obtaining a PMMS allocation when $G$ is a tree.","sentences":["We study the problem of allocating indivisible resources under the connectivity constraints of a graph $G$.","This model, initially introduced by Bouveret et al. (published in IJCAI, 2017), effectively encompasses a diverse array of scenarios characterized by spatial or temporal limitations, including the division of land plots and the allocation of time plots.","In this paper, we introduce a novel fairness concept that integrates local comparisons within the social network formed by a connected allocation of the item graph.","Our particular focus is to achieve pairwise-maximin fair share (PMMS) among the \"neighbors\" within this network.","For any underlying graph structure, we show that a connected allocation that maximizes Nash welfare guarantees a $(1/2)$-PMMS fairness.","Moreover, for two agents, we establish that a $(3/4)$-PMMS allocation can be efficiently computed.","Additionally, we demonstrate that for three agents and the items aligned on a path, a PMMS allocation is always attainable and can be computed in polynomial time.","Lastly, when agents have identical additive utilities, we present a pseudo-polynomial-time algorithm for a $(3/4)$-PMMS allocation, irrespective of the underlying graph $G$.","Furthermore, we provide a polynomial-time algorithm for obtaining a PMMS allocation when $G$ is a tree."],"url":"http://arxiv.org/abs/2401.14825v1","category":"cs.GT"}
{"created":"2024-01-26 12:51:26","title":"A Deep Reinforcement Learning-based Approach for Adaptive Handover Protocols in Mobile Networks","abstract":"Due to an ever-increasing number of participants and new areas of application, the demands on mobile communications systems are continually increasing. In order to deliver higher data rates, enable mobility and guarantee QoS requirements of subscribers, these systems and the protocols used are becoming more complex. By using higher frequency spectrums, cells become smaller and more base stations have to be deployed. This leads to an increased number of handovers of user equipments between base stations in order to enable mobility, resulting in potentially more frequent radio link failures and rate reduction. The persistent switching between the same base stations, commonly referred to as \"ping-pong\", leads to a consistent reduction of data rates. In this work, we propose a method for handover optimization by using proximal policy optimization in mobile communications to learn an adaptive handover protocol. The resulting agent is highly flexible regarding different travelling speeds of user equipments, while outperforming the standard 5G NR handover protocol by 3GPP in terms of average data rate and number of radio link failures. Furthermore, the design of the proposed environment demonstrates remarkable accuracy, ensuring a fair comparison with the standard 3GPP protocol.","sentences":["Due to an ever-increasing number of participants and new areas of application, the demands on mobile communications systems are continually increasing.","In order to deliver higher data rates, enable mobility and guarantee QoS requirements of subscribers, these systems and the protocols used are becoming more complex.","By using higher frequency spectrums, cells become smaller and more base stations have to be deployed.","This leads to an increased number of handovers of user equipments between base stations in order to enable mobility, resulting in potentially more frequent radio link failures and rate reduction.","The persistent switching between the same base stations, commonly referred to as \"ping-pong\", leads to a consistent reduction of data rates.","In this work, we propose a method for handover optimization by using proximal policy optimization in mobile communications to learn an adaptive handover protocol.","The resulting agent is highly flexible regarding different travelling speeds of user equipments, while outperforming the standard 5G NR handover protocol by 3GPP in terms of average data rate and number of radio link failures.","Furthermore, the design of the proposed environment demonstrates remarkable accuracy, ensuring a fair comparison with the standard 3GPP protocol."],"url":"http://arxiv.org/abs/2401.14823v1","category":"cs.NI"}
{"created":"2024-01-26 12:47:54","title":"Endowing Protein Language Models with Structural Knowledge","abstract":"Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules. This refined model, termed Protein Structure Transformer (PST), is further pretrained on a small protein structure database, using the same masked language modeling objective as traditional protein language models. Empirical evaluations of PST demonstrate its superior parameter efficiency relative to protein language models, despite being pretrained on a dataset comprising only 542K structures. Notably, PST consistently outperforms the state-of-the-art foundation model for protein sequences, ESM-2, setting a new benchmark in protein function prediction. Our findings underscore the potential of integrating structural information into protein language models, paving the way for more effective and efficient protein modeling Code and pretrained models are available at https://github.com/BorgwardtLab/PST.","sentences":["Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution.","Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases.","Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios.","Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning.","While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications.","To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data.","Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules.","This refined model, termed Protein Structure Transformer (PST), is further pretrained on a small protein structure database, using the same masked language modeling objective as traditional protein language models.","Empirical evaluations of PST demonstrate its superior parameter efficiency relative to protein language models, despite being pretrained on a dataset comprising only 542K structures.","Notably, PST consistently outperforms the state-of-the-art foundation model for protein sequences, ESM-2, setting a new benchmark in protein function prediction.","Our findings underscore the potential of integrating structural information into protein language models, paving the way for more effective and efficient protein modeling Code and pretrained models are available at https://github.com/BorgwardtLab/PST."],"url":"http://arxiv.org/abs/2401.14819v1","category":"q-bio.QM"}
{"created":"2024-01-26 12:45:55","title":"ChemDFM: Dialogue Foundation Model for Chemistry","abstract":"Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference. Further qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios. We will open-source the ChemDFM model soon.","sentences":["Large language models (LLMs) have established great success in the general domain of natural language processing.","Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry.","However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry.","To this end, we develop ChemDFM, the first LLM towards CGI.","ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain.","Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities.","Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs.","Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference.","Further qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios.","We will open-source the ChemDFM model soon."],"url":"http://arxiv.org/abs/2401.14818v1","category":"cs.CL"}
{"created":"2024-01-26 12:39:54","title":"Unleashing Data Journalism's Potential: COVID-19 as Catalyst for Newsroom Transformation","abstract":"In the context of journalism, the COVID-19 pandemic brought unprecedented challenges, necessitating rapid adaptations in newsrooms. Data journalism emerged as a pivotal approach for effectively conveying complex information to the public. Here, we show the profound impact of COVID-19 on data journalism, revealing a surge in data-driven publications and heightened collaboration between data and science journalists. Employing a quantitative methodology, including negative binomial regression and Relational hyperevent models (RHEM), on byline data of articles co-authored by data journalists, we comprehensively analyze data journalism outputs, authorship trends, and collaboration networks to address five key research questions.   The findings reveal a significant increase in data journalistic pieces during and after the pandemic, in particular with a rise in publications within scientific departments. Collaborative efforts among data and science journalists intensified, evident through increased authorship and co-authorship trends. Prior common authorship experiences somewhat influenced the likelihood of future co-authorships, underscoring the importance of building collaborative communities of practice.   These quantitative insights provide an understanding of the transformational role of data journalism during COVID-19, contributing to the growing body of literature in computational communication science and journalism practice.","sentences":["In the context of journalism, the COVID-19 pandemic brought unprecedented challenges, necessitating rapid adaptations in newsrooms.","Data journalism emerged as a pivotal approach for effectively conveying complex information to the public.","Here, we show the profound impact of COVID-19 on data journalism, revealing a surge in data-driven publications and heightened collaboration between data and science journalists.","Employing a quantitative methodology, including negative binomial regression and Relational hyperevent models (RHEM), on byline data of articles co-authored by data journalists, we comprehensively analyze data journalism outputs, authorship trends, and collaboration networks to address five key research questions.   ","The findings reveal a significant increase in data journalistic pieces during and after the pandemic, in particular with a rise in publications within scientific departments.","Collaborative efforts among data and science journalists intensified, evident through increased authorship and co-authorship trends.","Prior common authorship experiences somewhat influenced the likelihood of future co-authorships, underscoring the importance of building collaborative communities of practice.   ","These quantitative insights provide an understanding of the transformational role of data journalism during COVID-19, contributing to the growing body of literature in computational communication science and journalism practice."],"url":"http://arxiv.org/abs/2401.14816v1","category":"cs.SI"}
{"created":"2024-01-26 12:35:03","title":"Faster Fr\u00e9chet Distance Approximation through Truncated Smoothing","abstract":"The Fr\\'echet distance is a popular distance measure for curves. Computing the Fr\\'echet distance between two polygonal curves of $n$ vertices takes roughly quadratic time, and conditional lower bounds suggest that even approximating to within a factor $3$ cannot be done in strongly-subquadratic time, even in one dimension. The current best approximation algorithms present trade-offs between approximation quality and running time. Recently, van der Horst $\\textit{et al.}$ (SODA, 2023) presented an $O((n^2 / \\alpha) \\log^3 n)$ time $\\alpha$-approximate algorithm for curves in arbitrary dimensions, for any $\\alpha \\in [1, n]$. Our main contribution is an approximation algorithm for curves in one dimension, with a significantly faster running time of $O(n \\log^3 n + (n^2 / \\alpha^3) \\log^2 n \\log \\log n)$. Additionally, we give an algorithm for curves in arbitrary dimensions that improves upon the state-of-the-art running time by a logarithmic factor, to $O((n^2 / \\alpha) \\log^2 n)$. Both of our algorithms rely on a linear-time simplification procedure that in one dimension reduces the complexity of the reachable free space to $O(n^2 / \\alpha)$ without making sacrifices in the asymptotic approximation factor.","sentences":["The Fr\\'echet distance is a popular distance measure for curves.","Computing the Fr\\'echet distance between two polygonal curves of $n$ vertices takes roughly quadratic time, and conditional lower bounds suggest that even approximating to within a factor $3$ cannot be done in strongly-subquadratic time, even in one dimension.","The current best approximation algorithms present trade-offs between approximation quality and running time.","Recently, van der Horst $\\textit{et al.}$ (SODA, 2023) presented an $O((n^2 / \\alpha) \\log^3","n)$ time $\\alpha$-approximate algorithm for curves in arbitrary dimensions, for any $\\alpha \\in [1, n]$. Our main contribution is an approximation algorithm for curves in one dimension, with a significantly faster running time of $O(n \\log^3 n + (n^2 / \\alpha^3) \\log^2 n \\log \\log n)$. Additionally, we give an algorithm for curves in arbitrary dimensions that improves upon the state-of-the-art running time by a logarithmic factor, to $O((n^2 / \\alpha) \\log^2 n)$. Both of our algorithms rely on a linear-time simplification procedure that in one dimension reduces the complexity of the reachable free space to $O(n^2 / \\alpha)$ without making sacrifices in the asymptotic approximation factor."],"url":"http://arxiv.org/abs/2401.14815v1","category":"cs.CG"}
{"created":"2024-01-26 12:31:30","title":"Symbol-Specific Sparsification of Interprocedural Distributive Environment Problems","abstract":"Previous work has shown that one can often greatly speed up static analysis by computing data flows not for every edge in the program's control-flow graph but instead only along definition-use chains. This yields a so-called sparse static analysis. Recent work on SparseDroid has shown that specifically taint analysis can be \"sparsified\" with extraordinary effectiveness because the taint state of one variable does not depend on those of others. This allows one to soundly omit more flow-function computations than in the general case.   In this work, we now assess whether this result carries over to the more generic setting of so-called Interprocedural Distributive Environment (IDE) problems. Opposed to taint analysis, IDE comprises distributive problems with large or even infinitely broad domains, such as typestate analysis or linear constant propagation. Specifically, this paper presents Sparse IDE, a framework that realizes sparsification for any static analysis that fits the IDE framework.   We implement Sparse IDE in SparseHeros, as an extension to the popular Heros IDE solver, and evaluate its performance on real-world Java libraries by comparing it to the baseline IDE algorithm. To this end, we design, implement and evaluate a linear constant propagation analysis client on top of SparseHeros. Our experiments show that, although IDE analyses can only be sparsified with respect to symbols and not (numeric) values, Sparse IDE can nonetheless yield significantly lower runtimes and often also memory consumptions compared to the original IDE.","sentences":["Previous work has shown that one can often greatly speed up static analysis by computing data flows not for every edge in the program's control-flow graph but instead only along definition-use chains.","This yields a so-called sparse static analysis.","Recent work on SparseDroid has shown that specifically taint analysis can be \"sparsified\" with extraordinary effectiveness because the taint state of one variable does not depend on those of others.","This allows one to soundly omit more flow-function computations than in the general case.   ","In this work, we now assess whether this result carries over to the more generic setting of so-called Interprocedural Distributive Environment (IDE) problems.","Opposed to taint analysis, IDE comprises distributive problems with large or even infinitely broad domains, such as typestate analysis or linear constant propagation.","Specifically, this paper presents Sparse IDE, a framework that realizes sparsification for any static analysis that fits the IDE framework.   ","We implement Sparse IDE in SparseHeros, as an extension to the popular Heros IDE solver, and evaluate its performance on real-world Java libraries by comparing it to the baseline IDE algorithm.","To this end, we design, implement and evaluate a linear constant propagation analysis client on top of SparseHeros.","Our experiments show that, although IDE analyses can only be sparsified with respect to symbols and not (numeric) values, Sparse IDE can nonetheless yield significantly lower runtimes and often also memory consumptions compared to the original IDE."],"url":"http://arxiv.org/abs/2401.14813v1","category":"cs.SE"}
{"created":"2024-01-26 12:18:29","title":"On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks","abstract":"In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.","sentences":["In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express.","Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL.","For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward.","Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes.","We thereby contribute to a more complete understanding of what standard reward functions can and cannot express.","In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature.","We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms."],"url":"http://arxiv.org/abs/2401.14811v1","category":"cs.AI"}
{"created":"2024-01-26 12:16:55","title":"Cyclic Group Projection for Enumerating Quasi-Cyclic Codes Trapping Sets","abstract":"This paper introduces a novel approach to enumerate and assess Trapping sets in quasi-cyclic codes, those with circulant sizes that are non-prime numbers. Leveraging the quasi-cyclic properties, the method employs a tabular technique to streamline the importance sampling step for estimating the pseudo-codeword weight of Trapping sets. The presented methodology draws on the mathematical framework established in the provided theorem, which elucidates the behavior of projection and lifting transformations on pseudo-codewords","sentences":["This paper introduces a novel approach to enumerate and assess Trapping sets in quasi-cyclic codes, those with circulant sizes that are non-prime numbers.","Leveraging the quasi-cyclic properties, the method employs a tabular technique to streamline the importance sampling step for estimating the pseudo-codeword weight of Trapping sets.","The presented methodology draws on the mathematical framework established in the provided theorem, which elucidates the behavior of projection and lifting transformations on pseudo-codewords"],"url":"http://arxiv.org/abs/2401.14810v1","category":"cs.IT"}
{"created":"2024-01-26 12:15:35","title":"Ground state energy of Bogoliubov energy functional in the high density limit","abstract":"We consider the Bogoliubov energy functional proposed by Napi\\'orkowski, Reuvers and Solovej and analize it in the high density regime. We derive a two term asymptotic expansion of the ground state energy.","sentences":["We consider the Bogoliubov energy functional proposed by Napi\\'orkowski, Reuvers and Solovej and analize it in the high density regime.","We derive a two term asymptotic expansion of the ground state energy."],"url":"http://arxiv.org/abs/2401.14809v1","category":"math-ph"}
{"created":"2024-01-26 12:11:28","title":"Raman spectra of amino acids and peptides from machine learning polarizabilities","abstract":"Raman spectroscopy is an important tool in the study of vibrational properties and composition of molecules, peptides and even proteins. Raman spectra can be simulated based on the change of the electronic polarizability with vibrations, which can nowadays be efficiently obtained via machine learning models trained on first-principles data. However, the transferability of the models trained on small molecules to larger structures is unclear and direct training on large structures in prohibitively expensive. In this work, we first train two machine learning models to predict polarizabilities of all 20 amino acids. Both models are carefully benchmarked and compared to DFT calculations, with neural network method found to offer better transferability. By combining machine learning models with classical force field molecular dynamics, Raman spectra of all amino acids are also obtained and investigated, showing good agreement with experiments. The models are further extended to small peptides. We find that adding structures containing peptide bonds to the training set greatly improves predictions even for peptides not included in training sets.","sentences":["Raman spectroscopy is an important tool in the study of vibrational properties and composition of molecules, peptides and even proteins.","Raman spectra can be simulated based on the change of the electronic polarizability with vibrations, which can nowadays be efficiently obtained via machine learning models trained on first-principles data.","However, the transferability of the models trained on small molecules to larger structures is unclear and direct training on large structures in prohibitively expensive.","In this work, we first train two machine learning models to predict polarizabilities of all 20 amino acids.","Both models are carefully benchmarked and compared to DFT calculations, with neural network method found to offer better transferability.","By combining machine learning models with classical force field molecular dynamics, Raman spectra of all amino acids are also obtained and investigated, showing good agreement with experiments.","The models are further extended to small peptides.","We find that adding structures containing peptide bonds to the training set greatly improves predictions even for peptides not included in training sets."],"url":"http://arxiv.org/abs/2401.14808v1","category":"physics.comp-ph"}
{"created":"2024-01-26 12:11:04","title":"PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning","abstract":"Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural networks to learn new tasks incrementally from a small number of labeled samples without forgetting previously learned tasks, closely mimicking human learning patterns. In this paper, we propose a novel approach called Prompt Learning for FSCIL (PL-FSCIL), which harnesses the power of prompts in conjunction with a pre-trained Vision Transformer (ViT) model to address the challenges of FSCIL effectively. Our work pioneers the use of visual prompts in FSCIL, which is characterized by its notable simplicity. PL-FSCIL consists of two distinct prompts: the Domain Prompt and the FSCIL Prompt. Both are vectors that augment the model by embedding themselves into the attention layer of the ViT model. Specifically, the Domain Prompt assists the ViT model in adapting to new data domains. The task-specific FSCIL Prompt, coupled with a prototype classifier, amplifies the model's ability to effectively handle FSCIL tasks. We validate the efficacy of PL-FSCIL on widely used benchmark datasets such as CIFAR-100 and CUB-200. The results showcase competitive performance, underscoring its promising potential for real-world applications where high-quality data is often scarce. The source code is available at: https://github.com/TianSongS/PL-FSCIL.","sentences":["Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural networks to learn new tasks incrementally from a small number of labeled samples without forgetting previously learned tasks, closely mimicking human learning patterns.","In this paper, we propose a novel approach called Prompt Learning for FSCIL (PL-FSCIL), which harnesses the power of prompts in conjunction with a pre-trained Vision Transformer (ViT) model to address the challenges of FSCIL effectively.","Our work pioneers the use of visual prompts in FSCIL, which is characterized by its notable simplicity.","PL-FSCIL consists of two distinct prompts: the Domain Prompt and the FSCIL Prompt.","Both are vectors that augment the model by embedding themselves into the attention layer of the ViT model.","Specifically, the Domain Prompt assists the ViT model in adapting to new data domains.","The task-specific FSCIL Prompt, coupled with a prototype classifier, amplifies the model's ability to effectively handle FSCIL tasks.","We validate the efficacy of PL-FSCIL on widely used benchmark datasets such as CIFAR-100 and CUB-200.","The results showcase competitive performance, underscoring its promising potential for real-world applications where high-quality data is often scarce.","The source code is available at: https://github.com/TianSongS/PL-FSCIL."],"url":"http://arxiv.org/abs/2401.14807v1","category":"cs.CV"}
{"created":"2024-01-26 11:56:04","title":"Pointwise Redundancy in One-Shot Lossy Compression via Poisson Functional Representation","abstract":"We study different notions of pointwise redundancy in variable-length lossy source coding. We present a construction of one-shot variable-length lossy source coding schemes using the Poisson functional representation, and give bounds on its pointwise redundancy for various definitions of pointwise redundancy. This allows us to describe the distribution of the encoding length in a precise manner. We also generalize the result to the one-shot lossy Gray-Wyner system.","sentences":["We study different notions of pointwise redundancy in variable-length lossy source coding.","We present a construction of one-shot variable-length lossy source coding schemes using the Poisson functional representation, and give bounds on its pointwise redundancy for various definitions of pointwise redundancy.","This allows us to describe the distribution of the encoding length in a precise manner.","We also generalize the result to the one-shot lossy Gray-Wyner system."],"url":"http://arxiv.org/abs/2401.14805v1","category":"cs.IT"}
{"created":"2024-01-26 11:46:38","title":"Exploring exact-factorization-based trajectories for low-energy dynamics near a conical intersection","abstract":"We study low-energy dynamics generated by a two-dimensional two-state Jahn-Teller Hamiltonian in the vicinity of a conical intersection using quantum wavepacket and trajectories dynamics. Recently, these dynamics were studied by comparing the adiabatic representation and the exact factorization, with the purpose to highlight the different nature of topological- and geometric-phase effects arising in the two theoretical representation of the same problem. Here, we employ the exact factorization to understand how to model accurately low-energy dynamics in the vicinity of a conical intersection using an approximate description of the nuclear motion that uses trajectories. We find that, since nonadiabatic effects are weak but non-negligible, the trajectory-based description that invokes the classical approximation struggles to capture the correct behavior.","sentences":["We study low-energy dynamics generated by a two-dimensional two-state Jahn-Teller Hamiltonian in the vicinity of a conical intersection using quantum wavepacket and trajectories dynamics.","Recently, these dynamics were studied by comparing the adiabatic representation and the exact factorization, with the purpose to highlight the different nature of topological- and geometric-phase effects arising in the two theoretical representation of the same problem.","Here, we employ the exact factorization to understand how to model accurately low-energy dynamics in the vicinity of a conical intersection using an approximate description of the nuclear motion that uses trajectories.","We find that, since nonadiabatic effects are weak but non-negligible, the trajectory-based description that invokes the classical approximation struggles to capture the correct behavior."],"url":"http://arxiv.org/abs/2401.14801v1","category":"physics.chem-ph"}
{"created":"2024-01-26 11:41:00","title":"Linearity and Classification of $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-Linear Hadamard Codes","abstract":"The $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive codes are subgroups of $\\mathbb{Z}_2^{\\alpha_1} \\times \\mathbb{Z}_4^{\\alpha_2} \\times \\mathbb{Z}_8^{\\alpha_3}$. A $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard code is a Hadamard code which is the Gray map image of a $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive code. A recursive construction of $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive Hadamard codes of type $(\\alpha_1,\\alpha_2, \\alpha_3;t_1,t_2, t_3)$ with $\\alpha_1 \\neq 0$, $\\alpha_2 \\neq 0$, $\\alpha_3 \\neq 0$, $t_1\\geq 1$, $t_2 \\geq 0$, and $t_3\\geq 1$ is known. In this paper, we generalize some known results for $\\mathbb{Z}_2\\mathbb{Z}_4$-linear Hadamard codes to $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes with $\\alpha_1 \\neq 0$, $\\alpha_2 \\neq 0$, and $\\alpha_3 \\neq 0$. First, we show for which types the corresponding $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes of length $2^t$ are nonlinear. For these codes, we compute the kernel and its dimension, which allows us to give a partial classification of these codes. Moreover, for $3 \\leq t \\leq 11$, we give a complete classification by providing the exact amount of nonequivalent such codes. We also prove the existence of several families of infinite such nonlinear $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes, which are not equivalent to any other constructed $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard code, nor to any $\\mathbb{Z}_2\\mathbb{Z}_4$-linear Hadamard code, nor to any previously constructed $\\mathbb{Z}_{2^s}$-linear Hadamard code with $s\\geq 2$, with the same length $2^t$.","sentences":["The $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive codes are subgroups of $\\mathbb{Z}_2^{\\alpha_1} \\times \\mathbb{Z}_4^{\\alpha_2} \\times \\mathbb{Z}_8^{\\alpha_3}$. A $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard code is a Hadamard code which is the Gray map image of a $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive code.","A recursive construction of $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive Hadamard codes of type $(\\alpha_1,\\alpha_2, \\alpha_3;t_1,t_2, t_3)$ with $\\alpha_1 \\neq 0$, $\\alpha_2 \\neq 0$, $\\alpha_3 \\neq 0$, $t_1\\geq 1$, $t_2 \\geq 0$, and $t_3\\geq 1$ is known.","In this paper, we generalize some known results for $\\mathbb{Z}_2\\mathbb{Z}_4$-linear Hadamard codes to $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes with $\\alpha_1 \\neq 0$, $\\alpha_2 \\neq 0$, and $\\alpha_3 \\neq 0$.","First, we show for which types the corresponding $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes of length $2^t$ are nonlinear.","For these codes, we compute the kernel and its dimension, which allows us to give a partial classification of these codes.","Moreover, for $3 \\leq t \\leq 11$, we give a complete classification by providing the exact amount of nonequivalent such codes.","We also prove the existence of several families of infinite such nonlinear $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes, which are not equivalent to any other constructed $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard code, nor to any $\\mathbb{Z}_2\\mathbb{Z}_4$-linear Hadamard code, nor to any previously constructed $\\mathbb{Z}_{2^s}$-linear Hadamard code with $s\\geq 2$, with the same length $2^t$."],"url":"http://arxiv.org/abs/2401.14799v1","category":"cs.IT"}
{"created":"2024-01-26 11:37:45","title":"Soliton sheets formed by interference of Bose-Einstein condensates in optical lattices","abstract":"Soliton sheets which are formed by interference of Bose Einstein condensates occupying different single-particle states are observed in optical lattice potential. This structure consists of one-dimensional stationary solitons arranged periodically along the peaks of optical lattice (y direction) with the phase difference between the two sides of the soliton sheets is a linear function of y in each period, so we call it soliton sheet. A y component velocity difference exists between the two sides of the soliton sheet. Similar velocity distributions can be produced by the alignment of an infinite number of isotropic vortices along the peaks of the optical lattice. Their difference is that the soliton sheet structure is not limited by the number of phase singularities and can be generated even without phase singularities.","sentences":["Soliton sheets which are formed by interference of Bose Einstein condensates occupying different single-particle states are observed in optical lattice potential.","This structure consists of one-dimensional stationary solitons arranged periodically along the peaks of optical lattice (y direction) with the phase difference between the two sides of the soliton sheets is a linear function of y in each period, so we call it soliton sheet.","A y component velocity difference exists between the two sides of the soliton sheet.","Similar velocity distributions can be produced by the alignment of an infinite number of isotropic vortices along the peaks of the optical lattice.","Their difference is that the soliton sheet structure is not limited by the number of phase singularities and can be generated even without phase singularities."],"url":"http://arxiv.org/abs/2401.14796v1","category":"cond-mat.quant-gas"}
{"created":"2024-01-26 11:36:18","title":"Generalization of nonlocally related partial differential equation systems: unknown symmetric properties and analytical solutions","abstract":"Symmetry, which describes invariance, is an eternal concern in mathematics and physics, especially in the investigation of solutions to the partial differential equation (PDE). A PDE's nonlocally related PDE systems provide excellent approaches to search for various symmetries that expand the range of its known solutions. They composed of potential systems based on conservation laws and inverse potential systems (IPS) based on differential invariants. Our study is devoted to generalizing their construction and application in three-dimensional circumstances. Concretely, the potential of the algebraic gauge-constrained potential system is simplified without weakening its solution space. The potential system is extended via nonlocal conservation laws and double reductions. Afterwards, nonlocal symmetries are identified in the IPS.\\@ The IPS is extended by the solvable Lie algebra and type \\Rmnum{2} hidden symmetries. Besides, systems among equations can be connected via Cole-Hopf transformation.\\@ Ultimately, established and extended systems embody rich symmetric properties and unprecedented analytical solutions, and may even further facilitate general coordinate-independent analysis in qualitative, numerical, perturbation, etc., this can be illustrated by several Burgers-type equations.","sentences":["Symmetry, which describes invariance, is an eternal concern in mathematics and physics, especially in the investigation of solutions to the partial differential equation (PDE).","A PDE's nonlocally related PDE systems provide excellent approaches to search for various symmetries that expand the range of its known solutions.","They composed of potential systems based on conservation laws and inverse potential systems (IPS) based on differential invariants.","Our study is devoted to generalizing their construction and application in three-dimensional circumstances.","Concretely, the potential of the algebraic gauge-constrained potential system is simplified without weakening its solution space.","The potential system is extended via nonlocal conservation laws and double reductions.","Afterwards, nonlocal symmetries are identified in the IPS.\\@ The IPS is extended by the solvable Lie algebra and type \\Rmnum{2} hidden symmetries.","Besides, systems among equations can be connected via Cole-Hopf transformation.\\@ Ultimately, established and extended systems embody rich symmetric properties and unprecedented analytical solutions, and may even further facilitate general coordinate-independent analysis in qualitative, numerical, perturbation, etc., this can be illustrated by several Burgers-type equations."],"url":"http://arxiv.org/abs/2401.14795v1","category":"math-ph"}
{"created":"2024-01-26 11:36:13","title":"Imaging thick accretion disks and jets surrounding black holes","abstract":"Based on the horizon-scale magnetofluid model developed in [arXiv:2309.13304], we investigate the millimeter-wave images of a geometrically thick accretion disk or a funnel wall jet around a Kerr black hole. By employing the numerical method to solve the null geodesic and radiative transfer equations, we obtain the optical appearances at various observational angles and frequencies, generated by the thermal synchrotron radiation within the magnetofluid. For the thick disk, we specifically examine the impact of emission anisotropy on images, concluding that anisotropic synchrotron radiation could play an important role in the observability of the photon ring. For the funnel wall jet, we find that both the outflow and inflow funnel walls exhibit annular structures on the imaging plane. The outflow jet yields a brighter primary image than the photon ring, whereas the inflow jet does not. Based on our investigation, the inflow funnel wall model can not be ruled out by current observations of M87*.","sentences":["Based on the horizon-scale magnetofluid model developed in [arXiv:2309.13304], we investigate the millimeter-wave images of a geometrically thick accretion disk or a funnel wall jet around a Kerr black hole.","By employing the numerical method to solve the null geodesic and radiative transfer equations, we obtain the optical appearances at various observational angles and frequencies, generated by the thermal synchrotron radiation within the magnetofluid.","For the thick disk, we specifically examine the impact of emission anisotropy on images, concluding that anisotropic synchrotron radiation could play an important role in the observability of the photon ring.","For the funnel wall jet, we find that both the outflow and inflow funnel walls exhibit annular structures on the imaging plane.","The outflow jet yields a brighter primary image than the photon ring, whereas the inflow jet does not.","Based on our investigation, the inflow funnel wall model can not be ruled out by current observations of M87*."],"url":"http://arxiv.org/abs/2401.14794v1","category":"astro-ph.HE"}
{"created":"2024-01-26 11:32:53","title":"Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition","abstract":"In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework. We rigorously address the trade-off between obfuscation and utility. Both are quantified through the logarithmic loss, a measure also recognized as self-information loss. This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models. Importantly, we apply our model to state-of-the-art face recognition systems. The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation.","sentences":["In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework.","We rigorously address the trade-off between obfuscation and utility.","Both are quantified through the logarithmic loss, a measure also recognized as self-information loss.","This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models.","Importantly, we apply our model to state-of-the-art face recognition systems.","The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation."],"url":"http://arxiv.org/abs/2401.14792v1","category":"cs.CV"}
{"created":"2024-01-26 11:23:23","title":"A Numerical System For Nested Spaces -- Defining An Intuitive, Universal Coordinate System For Self-Similarity","abstract":"Analysis on fractals is a growing field, with hints of potential for widespread applicability across all of STEM. One of the most heavily researched type of fractals are the nested fractals, fractal shapes defined by virtue of being made of smaller copies of themselves. In this paper I introduce a simple, universal numerical system for describing all self-similar nested spaces. For a nested space made of $N>1$ smaller copies of itself, the location of all points can be specified using the double shift space $\\lbrace 0,1,...,N-1 \\rbrace^{\\mathbb{Z}}$, endowed with the appropriate nested topology. Connectivity rules are established to fully define the topology of the nested space, and immediate consequences are investigated using the numerical system defined. I introduce the numerical system and demonstrate how it unifies previous existing works under a single, simple framework, to argue for its adoption as standard in discussions of nested fractals.","sentences":["Analysis on fractals is a growing field, with hints of potential for widespread applicability across all of STEM.","One of the most heavily researched type of fractals are the nested fractals, fractal shapes defined by virtue of being made of smaller copies of themselves.","In this paper I introduce a simple, universal numerical system for describing all self-similar nested spaces.","For a nested space made of $N>1$ smaller copies of itself, the location of all points can be specified using the double shift space $\\lbrace 0,1,...,N-1 \\rbrace^{\\mathbb{Z}}$, endowed with the appropriate nested topology.","Connectivity rules are established to fully define the topology of the nested space, and immediate consequences are investigated using the numerical system defined.","I introduce the numerical system and demonstrate how it unifies previous existing works under a single, simple framework, to argue for its adoption as standard in discussions of nested fractals."],"url":"http://arxiv.org/abs/2401.14787v1","category":"math-ph"}
{"created":"2024-01-26 11:20:11","title":"Study of the gOMP Algorithm for Recovery of Compressed Sensed Hyperspectral Images","abstract":"Hyperspectral Imaging (HSI) is used in a wide range of applications such as remote sensing, yet the transmission of the HS images by communication data links becomes challenging due to the large number of spectral bands that the HS images contain together with the limited data bandwidth available in real applications. Compressive Sensing reduces the images by randomly subsampling the spectral bands of each spatial pixel and then it performs the image reconstruction of all the bands using recovery algorithms which impose sparsity in a certain transform domain. Since the image pixels are not strictly sparse, this work studies a data sparsification pre-processing stage prior to compression to ensure the sparsity of the pixels. The sparsified images are compressed $2.5\\times$ and then recovered using the Generalized Orthogonal Matching Pursuit algorithm (gOMP) characterized by high accuracy, low computational requirements and fast convergence. The experiments are performed in five conventional hyperspectral images where the effect of different sparsification levels in the quality of the uncompressed as well as the recovered images is studied. It is concluded that the gOMP algorithm reconstructs the hyperspectral images with higher accuracy as well as faster convergence when the pixels are highly sparsified and hence at the expense of reducing the quality of the recovered images with respect to the original images.","sentences":["Hyperspectral Imaging (HSI) is used in a wide range of applications such as remote sensing, yet the transmission of the HS images by communication data links becomes challenging due to the large number of spectral bands that the HS images contain together with the limited data bandwidth available in real applications.","Compressive Sensing reduces the images by randomly subsampling the spectral bands of each spatial pixel and then it performs the image reconstruction of all the bands using recovery algorithms which impose sparsity in a certain transform domain.","Since the image pixels are not strictly sparse, this work studies a data sparsification pre-processing stage prior to compression to ensure the sparsity of the pixels.","The sparsified images are compressed $2.5\\times$ and then recovered using the Generalized Orthogonal Matching Pursuit algorithm (gOMP) characterized by high accuracy, low computational requirements and fast convergence.","The experiments are performed in five conventional hyperspectral images where the effect of different sparsification levels in the quality of the uncompressed as well as the recovered images is studied.","It is concluded that the gOMP algorithm reconstructs the hyperspectral images with higher accuracy as well as faster convergence when the pixels are highly sparsified and hence at the expense of reducing the quality of the recovered images with respect to the original images."],"url":"http://arxiv.org/abs/2401.14786v1","category":"cs.CV"}
{"created":"2024-01-26 11:19:13","title":"SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras","abstract":"Our work addresses the problem of egocentric human pose estimation from downwards-facing cameras on head-mounted devices (HMD). This presents a challenging scenario, as parts of the body often fall outside of the image or are occluded. Previous solutions minimize this problem by using fish-eye camera lenses to capture a wider view, but these can present hardware design issues. They also predict 2D heat-maps per joint and lift them to 3D space to deal with self-occlusions, but this requires large network architectures which are impractical to deploy on resource-constrained HMDs. We predict pose from images captured with conventional rectilinear camera lenses. This resolves hardware design issues, but means body parts are often out of frame. As such, we directly regress probabilistic joint rotations represented as matrix Fisher distributions for a parameterized body model. This allows us to quantify pose uncertainties and explain out-of-frame or occluded joints. This also removes the need to compute 2D heat-maps and allows for simplified DNN architectures which require less compute. Given the lack of egocentric datasets using rectilinear camera lenses, we introduce the SynthEgo dataset, a synthetic dataset with 60K stereo images containing high diversity of pose, shape, clothing and skin tone. Our approach achieves state-of-the-art results for this challenging configuration, reducing mean per-joint position error by 23% overall and 58% for the lower body. Our architecture also has eight times fewer parameters and runs twice as fast as the current state-of-the-art. Experiments show that training on our synthetic dataset leads to good generalization to real world images without fine-tuning.","sentences":["Our work addresses the problem of egocentric human pose estimation from downwards-facing cameras on head-mounted devices (HMD).","This presents a challenging scenario, as parts of the body often fall outside of the image or are occluded.","Previous solutions minimize this problem by using fish-eye camera lenses to capture a wider view, but these can present hardware design issues.","They also predict 2D heat-maps per joint and lift them to 3D space to deal with self-occlusions, but this requires large network architectures which are impractical to deploy on resource-constrained HMDs.","We predict pose from images captured with conventional rectilinear camera lenses.","This resolves hardware design issues, but means body parts are often out of frame.","As such, we directly regress probabilistic joint rotations represented as matrix Fisher distributions for a parameterized body model.","This allows us to quantify pose uncertainties and explain out-of-frame or occluded joints.","This also removes the need to compute 2D heat-maps and allows for simplified DNN architectures which require less compute.","Given the lack of egocentric datasets using rectilinear camera lenses, we introduce the SynthEgo dataset, a synthetic dataset with 60K stereo images containing high diversity of pose, shape, clothing and skin tone.","Our approach achieves state-of-the-art results for this challenging configuration, reducing mean per-joint position error by 23% overall and 58% for the lower body.","Our architecture also has eight times fewer parameters and runs twice as fast as the current state-of-the-art.","Experiments show that training on our synthetic dataset leads to good generalization to real world images without fine-tuning."],"url":"http://arxiv.org/abs/2401.14785v1","category":"cs.CV"}
{"created":"2024-01-26 11:11:06","title":"Adversarial Attacks and Defenses in 6G Network-Assisted IoT Systems","abstract":"The Internet of Things (IoT) and massive IoT systems are key to sixth-generation (6G) networks due to dense connectivity, ultra-reliability, low latency, and high throughput. Artificial intelligence, including deep learning and machine learning, offers solutions for optimizing and deploying cutting-edge technologies for future radio communications. However, these techniques are vulnerable to adversarial attacks, leading to degraded performance and erroneous predictions, outcomes unacceptable for ubiquitous networks. This survey extensively addresses adversarial attacks and defense methods in 6G network-assisted IoT systems. The theoretical background and up-to-date research on adversarial attacks and defenses are discussed. Furthermore, we provide Monte Carlo simulations to validate the effectiveness of adversarial attacks compared to jamming attacks. Additionally, we examine the vulnerability of 6G IoT systems by demonstrating attack strategies applicable to key technologies, including reconfigurable intelligent surfaces, massive multiple-input multiple-output (MIMO)/cell-free massive MIMO, satellites, the metaverse, and semantic communications. Finally, we outline the challenges and future developments associated with adversarial attacks and defenses in 6G IoT systems.","sentences":["The Internet of Things (IoT) and massive IoT systems are key to sixth-generation (6G) networks due to dense connectivity, ultra-reliability, low latency, and high throughput.","Artificial intelligence, including deep learning and machine learning, offers solutions for optimizing and deploying cutting-edge technologies for future radio communications.","However, these techniques are vulnerable to adversarial attacks, leading to degraded performance and erroneous predictions, outcomes unacceptable for ubiquitous networks.","This survey extensively addresses adversarial attacks and defense methods in 6G network-assisted IoT systems.","The theoretical background and up-to-date research on adversarial attacks and defenses are discussed.","Furthermore, we provide Monte Carlo simulations to validate the effectiveness of adversarial attacks compared to jamming attacks.","Additionally, we examine the vulnerability of 6G IoT systems by demonstrating attack strategies applicable to key technologies, including reconfigurable intelligent surfaces, massive multiple-input multiple-output (MIMO)/cell-free massive MIMO, satellites, the metaverse, and semantic communications.","Finally, we outline the challenges and future developments associated with adversarial attacks and defenses in 6G IoT systems."],"url":"http://arxiv.org/abs/2401.14780v1","category":"cs.IT"}
