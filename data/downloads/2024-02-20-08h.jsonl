{"created":"2024-02-19 18:59:07","title":"FiT: Flexible Vision Transformer for Diffusion Model","abstract":"Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.","sentences":["Nature is infinitely resolution-free.","In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain.","To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios.","Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens.","This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping.","Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation.","Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution.","Repository available at https://github.com/whlzy/FiT."],"url":"http://arxiv.org/abs/2402.12376v1","category":"cs.CV"}
{"created":"2024-02-19 18:58:26","title":"LTL learning on GPUs","abstract":"Linear temporal logic (LTL) is widely used in industrial verification. LTL formulae can be learned from traces. Scaling LTL formula learning is an open problem. We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis. The learner is sound and complete. Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners. This is achieved with, among others, novel branch-free LTL semantics that has $O(\\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors).","sentences":["Linear temporal logic (LTL) is widely used in industrial verification.","LTL formulae can be learned from traces.","Scaling LTL formula learning is an open problem.","We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis.","The learner is sound and complete.","Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners.","This is achieved with, among others, novel branch-free LTL semantics that has $O(\\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors)."],"url":"http://arxiv.org/abs/2402.12373v1","category":"cs.PL"}
{"created":"2024-02-19 18:56:44","title":"AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies","abstract":"Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.","sentences":["Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$).","Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively.","Can language models (LMs) do the same?","To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs.","Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios.","We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2.","As in prior results, scaling up LMs results in some performance boosts.","Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack.","We hope these observations encourage further research in this field."],"url":"http://arxiv.org/abs/2402.12370v1","category":"cs.CL"}
{"created":"2024-02-19 18:55:16","title":"A synthetic data approach for domain generalization of NLI models","abstract":"Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE benchmark, a T5-small model trained with our data improves around $7\\%$ on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data.","sentences":["Natural Language Inference (NLI) remains an important benchmark task for LLMs.","NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text.","There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections.","Yet their realistic performance on out-of-distribution/domain data is less well-understood.","We present an in-depth exploration of the problem of domain generalization of NLI models.","We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets.","The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy.","We show that models trained on this data ($685$K synthetic examples) have the best generalization to completely new downstream test settings.","On the TRUE benchmark, a T5-small model trained with our data improves around $7\\%$ on average compared to training on the best alternative dataset.","The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model.","We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data."],"url":"http://arxiv.org/abs/2402.12368v1","category":"cs.CL"}
{"created":"2024-02-19 18:53:54","title":"A Critical Evaluation of AI Feedback for Aligning Large Language Models","abstract":"Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice.","sentences":["Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models.","RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model.","While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback.","We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation.","Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines.","More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models.","Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice."],"url":"http://arxiv.org/abs/2402.12366v1","category":"cs.LG"}
{"created":"2024-02-19 18:52:13","title":"Universal Physics Transformers","abstract":"Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics. Project page: https://ml-jku.github.io/UPT","sentences":["Deep neural network based surrogates for partial differential equations have recently gained increased interest.","However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar.","A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics.","We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes.","UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles.","UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques.","Finally, UPTs allow for queries of the latent space representation at any point in space-time.","We demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.","Project page: https://ml-jku.github.io/UPT"],"url":"http://arxiv.org/abs/2402.12365v1","category":"cs.LG"}
{"created":"2024-02-19 18:50:53","title":"Almost-linear time parameterized algorithm for rankwidth via dynamic rankwidth","abstract":"We give an algorithm that given a graph $G$ with $n$ vertices and $m$ edges and an integer $k$, in time $O_k(n^{1+o(1)}) + O(m)$ either outputs a rank decomposition of $G$ of width at most $k$ or determines that the rankwidth of $G$ is larger than $k$; the $O_k(\\cdot)$-notation hides factors depending on $k$. Our algorithm returns also a $(2^{k+1}-1)$-expression for cliquewidth, yielding a $(2^{k+1}-1)$-approximation algorithm for cliquewidth with the same running time. This improves upon the $O_k(n^2)$ time algorithm of Fomin and Korhonen [STOC 2022].   The main ingredient of our algorithm is a fully dynamic algorithm for maintaining rank decompositions of bounded width: We give a data structure that for a dynamic $n$-vertex graph $G$ that is updated by edge insertions and deletions maintains a rank decomposition of $G$ of width at most $4k$ under the promise that the rankwidth of $G$ never grows above $k$. The amortized running time of each update is $O_k(2^{\\sqrt{\\log n} \\log \\log n})$. The data structure furthermore can maintain whether $G$ satisfies some fixed ${\\sf CMSO}_1$ property within the same running time. We also give a framework for performing ``dense'' edge updates inside a given set of vertices $X$, where the new edges inside $X$ are described by a given ${\\sf CMSO}_1$ sentence and vertex labels, in amortized $O_k(|X| \\cdot 2^{\\sqrt{\\log n} \\log \\log n})$ time. Our dynamic algorithm generalizes the dynamic treewidth algorithm of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\\l}owski [FOCS 2023].","sentences":["We give an algorithm that given a graph $G$ with $n$ vertices and $m$ edges and an integer $k$, in time $O_k(n^{1+o(1)})","+ O(m)$ either outputs a rank decomposition of $G$ of width at most $k$ or determines that the rankwidth of $G$ is larger than $k$; the $O_k(\\cdot)$-notation hides factors depending on $k$. Our algorithm returns also a $(2^{k+1}-1)$-expression for cliquewidth, yielding a $(2^{k+1}-1)$-approximation algorithm for cliquewidth with the same running time.","This improves upon the $O_k(n^2)$ time algorithm of Fomin and Korhonen","[STOC 2022].   ","The main ingredient of our algorithm is a fully dynamic algorithm for maintaining rank decompositions of bounded width: We give a data structure that for a dynamic $n$-vertex graph $G$ that is updated by edge insertions and deletions maintains a rank decomposition of $G$ of width at most $4k$ under the promise that the rankwidth of $G$ never grows above $k$.","The amortized running time of each update is $O_k(2^{\\sqrt{\\log n} \\log \\log","n})$.","The data structure furthermore can maintain whether $G$ satisfies some fixed ${\\sf CMSO}_1$ property within the same running time.","We also give a framework for performing ``dense'' edge updates inside a given set of vertices $X$, where the new edges inside $X$ are described by a given ${\\sf CMSO}_1$ sentence and vertex labels, in amortized $O_k(|X| \\cdot 2^{\\sqrt{\\log n} \\log \\log n})$ time.","Our dynamic algorithm generalizes the dynamic treewidth algorithm of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\\l}owski","[FOCS 2023]."],"url":"http://arxiv.org/abs/2402.12364v1","category":"cs.DS"}
{"created":"2024-02-19 18:47:56","title":"Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks","abstract":"We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem. Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations. The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically. We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution.","sentences":["We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem.","Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations.","The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically.","We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution."],"url":"http://arxiv.org/abs/2402.12360v1","category":"math.NA"}
{"created":"2024-02-19 18:39:53","title":"Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4","abstract":"A pseudo-triangle is a simple polygon with exactly three convex vertices, and all other vertices (if any) are distributed on three concave chains. A pseudo-triangulation~$\\mathcal{T}$ of a point set~$P$ in~$\\mathbb{R}^2$ is a partitioning of the convex hull of~$P$ into pseudo-triangles, such that the union of the vertices of the pseudo-triangles is exactly~$P$. We call a size-4 pseudo-triangle a dart. For a fixed $k\\geq 1$, we study $k$-dart pseudo-triangulations ($k$-DPTs), that is, pseudo-triangulations in which exactly $k$ faces are darts and all other faces are triangles. We study the flip graph for such pseudo-triangulations, in which a flip exchanges the diagonals of a pseudo-quadrilatral. Our results are as follows. We prove that the flip graph of $1$-DPTs is generally not connected, and show how to compute its connected components. Furthermore, for $k$-DPTs on a point configuration called the double chain we analyze the structure of the flip graph on a more fine-grained level.","sentences":["A pseudo-triangle is a simple polygon with exactly three convex vertices, and all other vertices (if any) are distributed on three concave chains.","A pseudo-triangulation~$\\mathcal{T}$ of a point set~$P$ in~$\\mathbb{R}^2$ is a partitioning of the convex hull of~$P$ into pseudo-triangles, such that the union of the vertices of the pseudo-triangles is exactly~$P$.","We call a size-4 pseudo-triangle a dart.","For a fixed $k\\geq 1$, we study $k$-dart pseudo-triangulations ($k$-DPTs), that is, pseudo-triangulations in which exactly $k$ faces are darts and all other faces are triangles.","We study the flip graph for such pseudo-triangulations, in which a flip exchanges the diagonals of a pseudo-quadrilatral.","Our results are as follows.","We prove that the flip graph of $1$-DPTs is generally not connected, and show how to compute its connected components.","Furthermore, for $k$-DPTs on a point configuration called the double chain we analyze the structure of the flip graph on a more fine-grained level."],"url":"http://arxiv.org/abs/2402.12357v1","category":"cs.CG"}
{"created":"2024-02-19 18:35:33","title":"Signature of the atmospheric asymmetries of hot and ultra-hot Jupiters in lightcurves","abstract":"With the new generation of space telescopes such as the James Webb Space Telescope (JWST), it is possible to better characterize the atmospheres of exoplanets. The atmospheres of Hot and Ultra Hot Jupiters are highly heterogeneous and asymmetrical. The difference between the temperatures on the day-side and the night-side is especially extreme in the case of Ultra Hot Jupiters. We introduce a new tool to compute synthetic lightcurves from 3D GCM simulations, developed in the Pytmosph3R framework. We show how rotation induces a variation of the flux during the transit that is a source of information on the chemical and thermal distribution of the atmosphere. We find that the day-night gradient linked to Ultra Hot Jupiters has an effect close to the stellar limb-darkening, but opposite to tidal deformation. We confirm the impact of the atmospheric and chemical distribution on variations of the central transit time, though the variations found are smaller than that of available observational data, which could indicate that the east-west asymmetries are underestimated, due to the chemistry or clouds.","sentences":["With the new generation of space telescopes such as the James Webb Space Telescope (JWST), it is possible to better characterize the atmospheres of exoplanets.","The atmospheres of Hot and Ultra Hot Jupiters are highly heterogeneous and asymmetrical.","The difference between the temperatures on the day-side and the night-side is especially extreme in the case of Ultra Hot Jupiters.","We introduce a new tool to compute synthetic lightcurves from 3D GCM simulations, developed in the Pytmosph3R framework.","We show how rotation induces a variation of the flux during the transit that is a source of information on the chemical and thermal distribution of the atmosphere.","We find that the day-night gradient linked to Ultra Hot Jupiters has an effect close to the stellar limb-darkening, but opposite to tidal deformation.","We confirm the impact of the atmospheric and chemical distribution on variations of the central transit time, though the variations found are smaller than that of available observational data, which could indicate that the east-west asymmetries are underestimated, due to the chemistry or clouds."],"url":"http://arxiv.org/abs/2402.12355v1","category":"astro-ph.EP"}
{"created":"2024-02-19 18:33:49","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","abstract":"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","sentences":["In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al.","(2021) leads to suboptimal finetuning of models with large width (embedding dimension).","This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate.","Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning.","We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA","adapter matrices A and B with a well-chosen ratio.","We call this proposed algorithm LoRA$+$.","In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA."],"url":"http://arxiv.org/abs/2402.12354v1","category":"cs.LG"}
{"created":"2024-02-19 18:31:11","title":"Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge","abstract":"Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the context via maximum similarity search over text embeddings. In this study, we show that RAG methods leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel information-retrieval method that leverages a knowledge graph to downsample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be advantageously combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models.","sentences":["Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations.","Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones.","In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem).","Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production.","To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets.","RAG methods typically select the context via maximum similarity search over text embeddings.","In this study, we show that RAG methods leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature.","We introduce a novel information-retrieval method that leverages a knowledge graph to downsample these clusters and mitigate the information overload problem.","Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall.","Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be advantageously combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models."],"url":"http://arxiv.org/abs/2402.12352v1","category":"cs.CL"}
{"created":"2024-02-19 18:24:01","title":"An optimal replacement policy under variable shocks and self-healing patterns","abstract":"We study a system that experiences damaging external shocks at stochastic intervals, continuous degradation, and self-healing. The motivation for such a system comes from real-life applications based on micro-electro-mechanical systems (MEMS). The system fails if the cumulative damage exceeds a time-dependent threshold. We develop a preventive maintenance policy to replace the system such that its lifetime is prudently utilized. Further, three variations on the healing pattern have been considered: (i) shocks heal for a fixed duration $\\tau$; (ii) a fixed proportion of shocks are non-healable (that is, $\\tau=0$); (iii) there are two types of shocks -- self healable shocks heal for a finite duration, and nonhealable shocks inflict a random system degradation. We implement a proposed preventive maintenance policy and compare the optimal replacement times in these new cases to that of the original case where all shocks heal indefinitely and thereby enable the system manager to take necessary decisions in generalized system set-ups.","sentences":["We study a system that experiences damaging external shocks at stochastic intervals, continuous degradation, and self-healing.","The motivation for such a system comes from real-life applications based on micro-electro-mechanical systems (MEMS).","The system fails if the cumulative damage exceeds a time-dependent threshold.","We develop a preventive maintenance policy to replace the system such that its lifetime is prudently utilized.","Further, three variations on the healing pattern have been considered: (i) shocks heal for a fixed duration $\\tau$; (ii) a fixed proportion of shocks are non-healable (that is, $\\tau=0$); (iii) there are two types of shocks -- self healable shocks heal for a finite duration, and nonhealable shocks inflict a random system degradation.","We implement a proposed preventive maintenance policy and compare the optimal replacement times in these new cases to that of the original case where all shocks heal indefinitely and thereby enable the system manager to take necessary decisions in generalized system set-ups."],"url":"http://arxiv.org/abs/2402.12349v1","category":"stat.ME"}
{"created":"2024-02-19 18:23:36","title":"GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations","abstract":"As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs, e.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are also provided for a better understanding of LLMs' behavior.","sentences":["As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial.","This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents.","We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios.","Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation.","We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs, e.g., GPT-4, in complex games.","In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help.","Detailed error profiles are also provided for a better understanding of LLMs' behavior."],"url":"http://arxiv.org/abs/2402.12348v1","category":"cs.CL"}
{"created":"2024-02-19 18:20:44","title":"SymTFTs for Continuous non-Abelian Symmetries","abstract":"Topological defects and operators give a far-reaching generalization of symmetries of quantum fields. An auxiliary topological field theory in one dimension higher than the QFT of interest, known as the SymTFT, provides a natural way for capturing such operators. This gives a new perspective on several applications of symmetries, but fails to capture continuous non-Abelian symmetries. The main aim of this work is to fill this gap. Guided by geometric engineering and holography, we recover various known features of representation theory of the non-Abelian symmetry from a SymTFT viewpoint. Central to our approach is a duality between (flat) free Yang-Mills and non-Abelian BF theories. Our results extend naturally to models without supersymmetry.","sentences":["Topological defects and operators give a far-reaching generalization of symmetries of quantum fields.","An auxiliary topological field theory in one dimension higher than the QFT of interest, known as the SymTFT, provides a natural way for capturing such operators.","This gives a new perspective on several applications of symmetries, but fails to capture continuous non-Abelian symmetries.","The main aim of this work is to fill this gap.","Guided by geometric engineering and holography, we recover various known features of representation theory of the non-Abelian symmetry from a SymTFT viewpoint.","Central to our approach is a duality between (flat) free Yang-Mills and non-Abelian BF theories.","Our results extend naturally to models without supersymmetry."],"url":"http://arxiv.org/abs/2402.12347v1","category":"hep-th"}
{"created":"2024-02-19 18:20:12","title":"Homoclinic Floer homology via direct limits","abstract":"Let $(M \\omega)$ be a two dimensional symplectic manifold, $\\phi: M \\to M$ a symplectomorphism with hyperbolic fixed point $x$ and transversely intersecting stable and unstable manifolds $W^s(\\phi, x) \\cap\\ W^u(\\phi, x)=:\\mathcal{H}(\\phi, x)$. The intersection points are called homoclinic points, and the stable and unstable manifold are in this situation Lagrangian submanifolds. For this Lagrangian intersection problem with its infinite number of intersection points and wild oscillation behavior, we first define a Floer homology generated by finite sets of so-called contractible homoclinic points. This generalizes very significantly the Floer homologies generated by (semi)primary points defined by us in earlier works. Nevertheless these Floer homologies only consider quite `local' aspects of $W^s(\\phi, x) \\cap\\ W^u(\\phi, x)$ since their generator sets are finite, but the number of all contractible homoclinic points is infinite.   To overcome this issue, we construct a direct limit of these `local' homoclinic Floer homologies over suitable index sets. These direct limits thus accumulate the information gathered by the finitely generated local' homoclinic Floer homologies.","sentences":["Let $(M \\omega)$ be a two dimensional symplectic manifold, $\\phi",": M \\to M$ a symplectomorphism with hyperbolic fixed point $x$ and transversely intersecting stable and unstable manifolds $W^s(\\phi, x) \\cap\\ W^u(\\phi, x)=:\\mathcal{H}(\\phi, x)$.","The intersection points are called homoclinic points, and the stable and unstable manifold are in this situation Lagrangian submanifolds.","For this Lagrangian intersection problem with its infinite number of intersection points and wild oscillation behavior, we first define a Floer homology generated by finite sets of so-called contractible homoclinic points.","This generalizes very significantly the Floer homologies generated by (semi)primary points defined by us in earlier works.","Nevertheless these Floer homologies only consider quite `local' aspects of $W^s(\\phi, x) \\cap\\ W^u(\\phi, x)$ since their generator sets are finite, but the number of all contractible homoclinic points is infinite.   ","To overcome this issue, we construct a direct limit of these `local' homoclinic Floer homologies over suitable index sets.","These direct limits thus accumulate the information gathered by the finitely generated local' homoclinic Floer homologies."],"url":"http://arxiv.org/abs/2402.12345v1","category":"math.SG"}
{"created":"2024-02-19 18:15:09","title":"Post-Minkowskian Theory Meets the Spinning Effective-One-Body Approach for Two-Body Scattering","abstract":"Effective-one-body (EOB) waveforms employed by the LIGO-Virgo-KAGRA Collaboration have primarily been developed by resumming the post-Newtonian expansion of the relativistic two-body problem. Given the recent significant advancements in post-Minkowskian (PM) theory and gravitational self-force formalism, there is considerable interest in creating waveform models that integrate information from various perturbative methods in innovative ways. This becomes particularly crucial when tackling the accuracy challenge posed by upcoming ground-based detectors (such as the Einstein Telescope and Cosmic Explorer) and space-based detectors (such as LISA, TianQin or Taiji) expected to operate in the next decade. In this context, we present the derivation of the first spinning EOB Hamiltonian that incorporates PM results up to three-loop order: the SEOB-PM model. The model accounts for the complete hyperbolic motion, encompassing nonlocal-in-time tails. To evaluate its accuracy, we compare its predictions for the conservative scattering angle, augmented with dissipative contributions, against numerical-relativity data of non-spinning and spinning equal-mass black holes. We observe very good agreement, comparable, and in some cases slightly better to the recently proposed $w_{\\rm EOB}$-potential model, of which the SEOB-PM model is a resummation around the probe limit. Indeed, in the probe limit, the SEOB-PM Hamiltonian and scattering angles reduce to the one of a test mass in Kerr spacetime. Once complemented with nonlocal-in-time contributions for bound orbits, the SEOB-PM Hamiltonian can be utilized to generate waveform models for spinning black holes on quasi-circular orbits.","sentences":["Effective-one-body (EOB) waveforms employed by the LIGO-Virgo-KAGRA Collaboration have primarily been developed by resumming the post-Newtonian expansion of the relativistic two-body problem.","Given the recent significant advancements in post-Minkowskian (PM) theory and gravitational self-force formalism, there is considerable interest in creating waveform models that integrate information from various perturbative methods in innovative ways.","This becomes particularly crucial when tackling the accuracy challenge posed by upcoming ground-based detectors (such as the Einstein Telescope and Cosmic Explorer) and space-based detectors (such as LISA, TianQin or Taiji) expected to operate in the next decade.","In this context, we present the derivation of the first spinning EOB Hamiltonian that incorporates PM results up to three-loop order: the SEOB-PM model.","The model accounts for the complete hyperbolic motion, encompassing nonlocal-in-time tails.","To evaluate its accuracy, we compare its predictions for the conservative scattering angle, augmented with dissipative contributions, against numerical-relativity data of non-spinning and spinning equal-mass black holes.","We observe very good agreement, comparable, and in some cases slightly better to the recently proposed $w_{\\rm EOB}$-potential model, of which the SEOB-PM model is a resummation around the probe limit.","Indeed, in the probe limit, the SEOB-PM Hamiltonian and scattering angles reduce to the one of a test mass in Kerr spacetime.","Once complemented with nonlocal-in-time contributions for bound orbits, the SEOB-PM Hamiltonian can be utilized to generate waveform models for spinning black holes on quasi-circular orbits."],"url":"http://arxiv.org/abs/2402.12342v1","category":"gr-qc"}
{"created":"2024-02-19 18:11:58","title":"Path spaces of pushouts","abstract":"Given a span of spaces, one can form the homotopy pushout and then take the homotopy pullback of the resulting cospan. We give a concrete description of this pullback as the colimit of a sequence of approximations, using what we call the zigzag construction. We also obtain a description of loop spaces of homotopy pushouts. Using the zigzag construction, we reproduce generalisations of the Blakers-Massey theorem and fundamental results from Bass-Serre theory. We also describe the loop space of a wedge and show that it splits after suspension. Our construction can be interpreted in a large class of $(\\infty,1)$-categories and in homotopy type theory, where it resolves the long-standing open problem of showing that a pushout of 0-types is 1-truncated. The zigzag construction is closely related to the James construction, but works in greater generality.","sentences":["Given a span of spaces, one can form the homotopy pushout and then take the homotopy pullback of the resulting cospan.","We give a concrete description of this pullback as the colimit of a sequence of approximations, using what we call the zigzag construction.","We also obtain a description of loop spaces of homotopy pushouts.","Using the zigzag construction, we reproduce generalisations of the Blakers-Massey theorem and fundamental results from Bass-Serre theory.","We also describe the loop space of a wedge and show that it splits after suspension.","Our construction can be interpreted in a large class of $(\\infty,1)$-categories and in homotopy type theory, where it resolves the long-standing open problem of showing that a pushout of 0-types is 1-truncated.","The zigzag construction is closely related to the James construction, but works in greater generality."],"url":"http://arxiv.org/abs/2402.12339v1","category":"math.AT"}
{"created":"2024-02-19 18:11:37","title":"An Adversarial Approach to Evaluating the Robustness of Event Identification Models","abstract":"Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model. Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as logistic regression is more susceptible to adversarial attacks than gradient boosting.","sentences":["Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness.","Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data.","This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss.","The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness.","The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model.","Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as logistic regression is more susceptible to adversarial attacks than gradient boosting."],"url":"http://arxiv.org/abs/2402.12338v1","category":"eess.SY"}
{"created":"2024-02-19 18:10:48","title":"Hot carrier distribution engineering by alloying: picking elements for the desired purposes","abstract":"Metal alloys hold the promise of providing hot carrier generation distributions superior to pure metals in applications such as sensing, catalysis and solar energy harvesting. Guidelines for finding the optimal alloy configuration for a target application require understanding the connection between alloy composition and hot carrier distribution. Here we present a DFT-based computational approach to investigate the photo-generated hot carrier distribution of metal alloys based on the joint density of states and the electronic structure. We classified the metals by their electronic structure into closed d-shell, open d-shell, p-block and s-block elements. It is shown that combining closed d-shell elements enables modulating the distribution of highly energetic holes typical of pure metals but also leads to hot carrier production by IR light excitation and the appearance of highly energetic electrons due to band folding and splitting. This feature arises as an emergent property of alloying and is only unveiled when the hot carrier distribution computation takes momentum conservation into account. The combination of closed d-shell with open d-shell elements allows an abundant production of hot carriers in a broad energy range, while alloying a closed d-shell elements with an s-block element opens the door to hot electron distribution skewed toward high energy electrons. The combination of d-shell with p-block elements results in moderate hot carrier distribution whose asymmetry can be tuned by composition. Overall, the obtained insights that connect alloy composition, band structure and resulting carrier distribution provide a toolkit to match elements in an alloy for the deliberate engineering of hot carrier distribution.","sentences":["Metal alloys hold the promise of providing hot carrier generation distributions superior to pure metals in applications such as sensing, catalysis and solar energy harvesting.","Guidelines for finding the optimal alloy configuration for a target application require understanding the connection between alloy composition and hot carrier distribution.","Here we present a DFT-based computational approach to investigate the photo-generated hot carrier distribution of metal alloys based on the joint density of states and the electronic structure.","We classified the metals by their electronic structure into closed d-shell, open d-shell, p-block and s-block elements.","It is shown that combining closed d-shell elements enables modulating the distribution of highly energetic holes typical of pure metals but also leads to hot carrier production by IR light excitation and the appearance of highly energetic electrons due to band folding and splitting.","This feature arises as an emergent property of alloying and is only unveiled when the hot carrier distribution computation takes momentum conservation into account.","The combination of closed d-shell with open d-shell elements allows an abundant production of hot carriers in a broad energy range, while alloying a closed d-shell elements with an s-block element opens the door to hot electron distribution skewed toward high energy electrons.","The combination of d-shell with p-block elements results in moderate hot carrier distribution whose asymmetry can be tuned by composition.","Overall, the obtained insights that connect alloy composition, band structure and resulting carrier distribution provide a toolkit to match elements in an alloy for the deliberate engineering of hot carrier distribution."],"url":"http://arxiv.org/abs/2402.12337v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 18:09:48","title":"Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models","abstract":"Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM","sentences":["Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks.","Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality.","These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem.","The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo.","We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP.","In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one.","No retraining or fine-tuning of the VLM is required.","The code and robust models are available at https://github.com/chs20/RobustVLM"],"url":"http://arxiv.org/abs/2402.12336v1","category":"cs.LG"}
{"created":"2024-02-19 18:09:43","title":"Image Super-resolution Inspired Electron Density Prediction","abstract":"Drawing inspiration from the domain of image super-resolution, we view the electron density as a 3D grayscale image and use a convolutional residual network to transform a crude and trivially generated guess of the molecular density into an accurate ground-state quantum mechanical density. We find that this model outperforms all prior density prediction approaches. Because the input is itself a real-space density, the predictions are equivariant to molecular symmetry transformations even though the model is not constructed to be. Due to its simplicity, the model is directly applicable to unseen molecular conformations and chemical elements. We show that fine-tuning on limited new data provides high accuracy even in challenging cases of exotic elements and charge states. Our work suggests new routes to learning real-space physical quantities drawing from the established ideas of image processing.","sentences":["Drawing inspiration from the domain of image super-resolution, we view the electron density as a 3D grayscale image and use a convolutional residual network to transform a crude and trivially generated guess of the molecular density into an accurate ground-state quantum mechanical density.","We find that this model outperforms all prior density prediction approaches.","Because the input is itself a real-space density, the predictions are equivariant to molecular symmetry transformations even though the model is not constructed to be.","Due to its simplicity, the model is directly applicable to unseen molecular conformations and chemical elements.","We show that fine-tuning on limited new data provides high accuracy even in challenging cases of exotic elements and charge states.","Our work suggests new routes to learning real-space physical quantities drawing from the established ideas of image processing."],"url":"http://arxiv.org/abs/2402.12335v1","category":"physics.chem-ph"}
{"created":"2024-02-19 18:09:11","title":"Insights into the mechanics of sessile whole blood droplet evaporation","abstract":"We study the mechanics of sessile whole blood drop evaporation using direct experimental color visualization and theoretical methods. We show that the transient evaporation process can be subdivided into three major phases (A, B, and C) based on the evaporation rate. Phase A is the fastest where edge evaporation dominates and leads to the formation of a gelated three phase contact line. Gelation is a result of sol-gel phase transition that occurs due to the accumulation of RBCs (red blood cells) as they get transported due to outward capillary flow generated during drop evaporation. The intermediate phase B consists of gelation front propagating radially inwards due to a combined effect of outward capillary flow and drop height reduction evaporating in CCR (constant contact radius) mode leading to a formation of a wet gel phase. We unearth that the gelation of the entire droplet occurs in Phase B and the gel formed contains trace amounts of water that is detectable from our experiments. Phase C is the final slowest stage of evaporation, where the wet gel transforms into dry-gel and leads to the formation of dessication stress concentrations resulting in the formation of various kinds of crack patterns in the dried blood drop precipitate. Using profilometry and scanning electron microscopy we observe radial cracks in the thicker region of the precipitate and mud flat type cracks are observed in the center part of the evaporating droplet where the drop thickness is relatively small and curvature is negligible. We also study the evaporation of bacteria laden droplets to simulate bacterial infection in human blood and show that the drop evaporation rate and final dried residue pattern does not change appreciably within the parameter variation of the bacterial concentration typically found in bacterial infection of living organisms.","sentences":["We study the mechanics of sessile whole blood drop evaporation using direct experimental color visualization and theoretical methods.","We show that the transient evaporation process can be subdivided into three major phases (A, B, and C) based on the evaporation rate.","Phase A is the fastest where edge evaporation dominates and leads to the formation of a gelated three phase contact line.","Gelation is a result of sol-gel phase transition that occurs due to the accumulation of RBCs (red blood cells) as they get transported due to outward capillary flow generated during drop evaporation.","The intermediate phase B consists of gelation front propagating radially inwards due to a combined effect of outward capillary flow and drop height reduction evaporating in CCR (constant contact radius) mode leading to a formation of a wet gel phase.","We unearth that the gelation of the entire droplet occurs in Phase B and the gel formed contains trace amounts of water that is detectable from our experiments.","Phase C is the final slowest stage of evaporation, where the wet gel transforms into dry-gel and leads to the formation of dessication stress concentrations resulting in the formation of various kinds of crack patterns in the dried blood drop precipitate.","Using profilometry and scanning electron microscopy we observe radial cracks in the thicker region of the precipitate and mud flat type cracks are observed in the center part of the evaporating droplet where the drop thickness is relatively small and curvature is negligible.","We also study the evaporation of bacteria laden droplets to simulate bacterial infection in human blood and show that the drop evaporation rate and final dried residue pattern does not change appreciably within the parameter variation of the bacterial concentration typically found in bacterial infection of living organisms."],"url":"http://arxiv.org/abs/2402.12334v1","category":"physics.flu-dyn"}
{"created":"2024-02-19 18:06:05","title":"An explicit result for short sums of positive arithmetic functions","abstract":"We prove a totally explicit bound for short sums of certain non-negative arithmetic functions satisfying a general growth condition, and apply this result to derive two explicit estimates for the Erd\\H{o}s-Hooley $\\Delta$-function in short intervals.","sentences":["We prove a totally explicit bound for short sums of certain non-negative arithmetic functions satisfying a general growth condition, and apply this result to derive two explicit estimates for the Erd\\H{o}s-Hooley $\\Delta$-function in short intervals."],"url":"http://arxiv.org/abs/2402.12333v1","category":"math.NT"}
{"created":"2024-02-19 18:06:02","title":"Triple-Encoders: Representations That Fire Together, Wire Together","abstract":"Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model is publicly available.","sentences":["Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost.","Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency.","While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization.","To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights.","Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding.","Our code/model is publicly available."],"url":"http://arxiv.org/abs/2402.12332v1","category":"cs.CL"}
{"created":"2024-02-19 18:02:10","title":"Generating Survival Interpretable Trajectories and Data","abstract":"A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classification task. The paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets. The code of the algorithm implementing the proposed model is publicly available.","sentences":["A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed.","It solves three tasks.","First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator.","Second, the model generates additional data based on a given training set that would supplement the original dataset.","Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event.","The trajectory can be viewed as a type of the counterfactual explanation.","The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder.","The model also determines the censored indicators of new generated data by solving a classification task.","The paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets.","The code of the algorithm implementing the proposed model is publicly available."],"url":"http://arxiv.org/abs/2402.12331v1","category":"cs.LG"}
{"created":"2024-02-19 18:01:36","title":"Query-Based Adversarial Prompt Generation","abstract":"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.","sentences":["Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior.","Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models.","We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.","We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability."],"url":"http://arxiv.org/abs/2402.12329v1","category":"cs.CL"}
{"created":"2024-02-19 18:01:16","title":"Observations of compact stars and fermion-boson stars with a quartic self-interaction","abstract":"We investigated the possibility that compact stars could be described by a fermion-boson star with a quartic self-interaction in the boson sector. Specifically, by varying the polytropic constant $K$ and adiabatic index $\\Gamma$ in the polytropic equation of state, the boson mass $\\mu$, and the self-interaction parameter $\\Lambda$, we construct equilibrium configurations of these mixed-stars with total mass compatible with the mass constraints obtained from observational data of the collaborations NICE, NICER/XMN-Newton, and LIGO. Our work confirms that the addition of a boson sector eases the comparison of neutron star models with gravitational events related to compact objects and that in such a case observations may have preference for a positive self-interaction in the boson sector.","sentences":["We investigated the possibility that compact stars could be described by a fermion-boson star with a quartic self-interaction in the boson sector.","Specifically, by varying the polytropic constant $K$ and adiabatic index $\\Gamma$ in the polytropic equation of state, the boson mass $\\mu$, and the self-interaction parameter $\\Lambda$, we construct equilibrium configurations of these mixed-stars with total mass compatible with the mass constraints obtained from observational data of the collaborations NICE, NICER/XMN-Newton, and LIGO.","Our work confirms that the addition of a boson sector eases the comparison of neutron star models with gravitational events related to compact objects and that in such a case observations may have preference for a positive self-interaction in the boson sector."],"url":"http://arxiv.org/abs/2402.12328v1","category":"gr-qc"}
{"created":"2024-02-19 18:00:53","title":"Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents","abstract":"Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWeTalk .","sentences":["Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics.","However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied.","To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings.","This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science.","Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena.","The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWeTalk ."],"url":"http://arxiv.org/abs/2402.12327v1","category":"cs.AI"}
{"created":"2024-02-19 18:00:30","title":"LLM Agents for Psychology: A Study on Gamified Assessments","abstract":"Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.","sentences":["Psychological measurement is essential for mental health, self-understanding, and personal development.","Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility.","While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability.","In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment.","The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers.","By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games.","To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits.","Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity.","Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction."],"url":"http://arxiv.org/abs/2402.12326v1","category":"cs.CL"}
{"created":"2024-02-19 17:57:32","title":"Investigating $\u03c9(z)$ Parametrizations in Horava-Lifshitz Gravity: Observational Constraints with Covariance Matrix Simulation","abstract":"This study investigates accelerated cosmic expansion using various cosmological models within Horava-Lifshitz gravity, including BBCCFHKO, Seljak, ASSS, PADE-I, PADE-II, and BAZS, utilizing Equation of State Parametrization. To constrain the cosmological parameters of each model, we incorporate 24 Baryon Acoustic Oscillation points, 30 Cosmic Chronometer points, 40 Type Ia Supernovae points, 24 quasar Hubble diagram points, and 162 Gamma Ray Bursts points, along with the latest Hubble constant measurement (R22). We treat $r_{d}$ as a free parameter, aiming to extract $H_{0}$ and $r_{d}$ using late-time datasets and obtain optimal fitting values for each parameter in every model. The benefits of treating $r_{d}$ as a free parameter include reduced bias, improved precision, and enhanced dataset compatibility. The resulting values of $H_{0}$ and $r_{d}$ are relative to the $\\Lambda$CDM model, showcasing alignment with early Planck and SDSS estimations. We additionally minimize errors by simulating random correlations in the covariance matrix. Furthermore, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) support all models, with the $\\Lambda$CDM model exhibiting the lowest AIC. Evaluation via reduced chi-square statistic confirms reasonable fits for all models. While $\\Lambda$CDM remains favored, extensions warrant further investigation. This study underscores the importance of exploring alternative cosmological models to deepen our understanding of the universe's fundamental properties and evolution. Continued refinement of models is essential for advancing cosmological research.","sentences":["This study investigates accelerated cosmic expansion using various cosmological models within Horava-Lifshitz gravity, including BBCCFHKO, Seljak, ASSS, PADE-I, PADE-II, and BAZS, utilizing Equation of State Parametrization.","To constrain the cosmological parameters of each model, we incorporate 24 Baryon Acoustic Oscillation points, 30 Cosmic Chronometer points, 40 Type Ia Supernovae points, 24 quasar Hubble diagram points, and 162 Gamma Ray Bursts points, along with the latest Hubble constant measurement (R22).","We treat $r_{d}$ as a free parameter, aiming to extract $H_{0}$ and $r_{d}$ using late-time datasets and obtain optimal fitting values for each parameter in every model.","The benefits of treating $r_{d}$ as a free parameter include reduced bias, improved precision, and enhanced dataset compatibility.","The resulting values of $H_{0}$ and $r_{d}$ are relative to the $\\Lambda$CDM model, showcasing alignment with early Planck and SDSS estimations.","We additionally minimize errors by simulating random correlations in the covariance matrix.","Furthermore, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) support all models, with the $\\Lambda$CDM model exhibiting the lowest AIC.","Evaluation via reduced chi-square statistic confirms reasonable fits for all models.","While $\\Lambda$CDM remains favored, extensions warrant further investigation.","This study underscores the importance of exploring alternative cosmological models to deepen our understanding of the universe's fundamental properties and evolution.","Continued refinement of models is essential for advancing cosmological research."],"url":"http://arxiv.org/abs/2402.12324v1","category":"astro-ph.CO"}
{"created":"2024-02-19 17:49:23","title":"Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment","abstract":"In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1. We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node. In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method.","sentences":["In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment.","The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm.","We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself.","We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1.","We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @","[0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1.","The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node.","In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method."],"url":"http://arxiv.org/abs/2402.12320v1","category":"cs.CV"}
{"created":"2024-02-19 17:44:35","title":"Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness","abstract":"The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.","sentences":["The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning.","In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks.","A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework.","Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions.","In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework.","Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML.","This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy.","The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively.","Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints.","Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods."],"url":"http://arxiv.org/abs/2402.12319v1","category":"cs.LG"}
{"created":"2024-02-19 17:37:28","title":"ARKS: Active Retrieval in Knowledge Soup for Code Generation","abstract":"Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs. The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines. Our model, code, and data are available at https://arks-codegen.github.io.","sentences":["Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training.","While widely explored in natural language applications, its utilization in code generation remains under-explored.","In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code.","In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets.","We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup.","To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages.","Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs.","The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines.","Our model, code, and data are available at https://arks-codegen.github.io."],"url":"http://arxiv.org/abs/2402.12317v1","category":"cs.CL"}
{"created":"2024-02-19 17:34:20","title":"A new approach to universal $F$-inverse monoids in enriched signature","abstract":"We show that the universal $X$-generated $F$-inverse monoid $F(G)$, where $G$ is an $X$-generated group, introduced by Auinger, Szendrei and the first-named author, arises as a quotient inverse monoid of the Margolis-Meakin expansion $M(G, X\\cup \\overline{G})$ of $G$, with respect to the extended generating set $X\\cup \\overline{G}$, where $\\overline{G}$ is a bijective copy of $G$ which encodes the $m$-operation in $F(G)$. The construction relies on a certain closure operator on the semilattice of all finite and connected subgraphs containing the origin of the Cayley graph $Cay(G, X\\cup {\\overline{G}})$ and leads to a new and simpler proof of the universal property of $F(G)$.","sentences":["We show that the universal $X$-generated $F$-inverse monoid $F(G)$, where $G$ is an $X$-generated group, introduced by Auinger, Szendrei and the first-named author, arises as a quotient inverse monoid of the Margolis-Meakin expansion $M(G, X\\cup \\overline{G})$ of $G$, with respect to the extended generating set $X\\cup \\overline{G}$, where $\\overline{G}$ is a bijective copy of $G$ which encodes the $m$-operation in $F(G)$. The construction relies on a certain closure operator on the semilattice of all finite and connected subgraphs containing the origin of the Cayley graph $Cay(G, X\\cup {\\overline{G}})$ and leads to a new and simpler proof of the universal property of $F(G)$."],"url":"http://arxiv.org/abs/2402.12313v1","category":"math.GR"}
{"created":"2024-02-19 17:32:27","title":"Derived equivalence of algebras induced by their trivial extensions","abstract":"The bounded derived category of a finite dimensional algebra of finite global dimension is equivalent the stable category of $\\mathbb{Z}$-graded modules over its trivial extension \\cite{Happel}. In particular, given two derived equivalent finite dimensional algebras $\\Lambda_{1}$ and $\\Lambda_{2}$ of finite global dimension, their trivial extensions are stable equivalent. The converse is not true in general. The goal of this paper is to study cases where derived equivalences between $\\Lambda_{1}$ and $\\Lambda_{2}$ arise from an equivalence of categories involving their trivial extension. Thanks to a graded version of Happel's theorem, we show that one can construct a $\\mathbb{Z}$-grading on the $\\Lambda_{i}$ so that such an equivalence involving their trivial extension yield a derived equivalence between the category of $\\mathbb{Z}$-graded module over $\\Lambda_{i}$. We describe explicitly the tilting object associated to this derived equivalence (in the non-graded and in the graded case) for triangular matrix algebras. Finally, we apply these results to the particular case of gentle algebras. In this context, we study how one can obtain a derived equivalence between $\\Lambda_{1}$ and $\\Lambda_{2}$ (in the non-graded and the graded case) from graded generalized Kauer moves.","sentences":["The bounded derived category of a finite dimensional algebra of finite global dimension is equivalent the stable category of $\\mathbb{Z}$-graded modules over its trivial extension \\cite{Happel}.","In particular, given two derived equivalent finite dimensional algebras $\\Lambda_{1}$ and $\\Lambda_{2}$ of finite global dimension, their trivial extensions are stable equivalent.","The converse is not true in general.","The goal of this paper is to study cases where derived equivalences between $\\Lambda_{1}$ and $\\Lambda_{2}$ arise from an equivalence of categories involving their trivial extension.","Thanks to a graded version of Happel's theorem, we show that one can construct a $\\mathbb{Z}$-grading on the $\\Lambda_{i}$ so that such an equivalence involving their trivial extension yield a derived equivalence between the category of $\\mathbb{Z}$-graded module over $\\Lambda_{i}$. We describe explicitly the tilting object associated to this derived equivalence (in the non-graded and in the graded case) for triangular matrix algebras.","Finally, we apply these results to the particular case of gentle algebras.","In this context, we study how one can obtain a derived equivalence between $\\Lambda_{1}$ and $\\Lambda_{2}$ (in the non-graded and the graded case) from graded generalized Kauer moves."],"url":"http://arxiv.org/abs/2402.12312v1","category":"math.RT"}
{"created":"2024-02-19 17:32:04","title":"Free probability, path developments and signature kernels as universal scaling limits","abstract":"Random developments of a path into a matrix Lie group $G_N$ have recently been used to construct signature-based kernels on path space. Two examples include developments into GL$(N;\\mathbb{R})$ and $U(N;\\mathbb{C})$, the general linear and unitary groups of dimension $N$. For the former, [MLS23] showed that the signature kernel is obtained via a scaling limit of developments with Gaussian vector fields. The second instance was used in [LLN23] to construct a metric between probability measures on path space. We present a unified treatment to obtaining large $N$ limits by leveraging the tools of free probability theory. An important conclusion is that the limiting kernels, while dependent on the choice of Lie group, are nonetheless universal limits with respect to how the development map is randomised. For unitary developments, the limiting kernel is given by the contraction of a signature against the monomials of freely independent semicircular random variables. Using the Schwinger-Dyson equations, we show that this kernel can be obtained by solving a novel quadratic functional equation. We provide a convergent numerical scheme for this equation, together with rates, which does not require computation of signatures themselves.","sentences":["Random developments of a path into a matrix Lie group $G_N$ have recently been used to construct signature-based kernels on path space.","Two examples include developments into GL$(N;\\mathbb{R})$ and $U(N;\\mathbb{C})$, the general linear and unitary groups of dimension $N$. For the former, [MLS23] showed that the signature kernel is obtained via a scaling limit of developments with Gaussian vector fields.","The second instance was used in [LLN23] to construct a metric between probability measures on path space.","We present a unified treatment to obtaining large $N$ limits by leveraging the tools of free probability theory.","An important conclusion is that the limiting kernels, while dependent on the choice of Lie group, are nonetheless universal limits with respect to how the development map is randomised.","For unitary developments, the limiting kernel is given by the contraction of a signature against the monomials of freely independent semicircular random variables.","Using the Schwinger-Dyson equations, we show that this kernel can be obtained by solving a novel quadratic functional equation.","We provide a convergent numerical scheme for this equation, together with rates, which does not require computation of signatures themselves."],"url":"http://arxiv.org/abs/2402.12311v1","category":"math.PR"}
{"created":"2024-02-19 17:30:44","title":"TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs","abstract":"Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods.","sentences":["Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general.","However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem.","In this paper, we propose TILP, a differentiable framework for temporal logical rules learning.","By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model.","We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process.","We compare TILP with state-of-the-art methods on two benchmark datasets.","We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results.","In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different.","In all these cases, TILP works much better than the state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.12309v1","category":"cs.CL"}
{"created":"2024-02-19 17:30:39","title":"Distribution of distance-based quantum resources outside a radiating Schwarzschild black hole","abstract":"We obtain analytical expressions for distance-based quantum resources and examine their distribution in the proximity of a Schwarzschild black hole (SBH) within a curved background. For an observer in free fall and their stationary counterpart sharing the Gisin state, the quantum resources are degraded at an infinite Hawking temperature. The extent of this degradation that occurs as the SBH evaporates is contingent upon the fermionic frequency mode, Gisin state parameters, and the distance between the observer and the event horizon (EH). In the case of two accelerating detectors in Minkowski spacetime interacting with quantum fluctuating scalar fields (QFSF), we find that quantum coherence and discord exhibit sudden disappearance for certain initial states and sudden reappearance for others except entanglement, regardless of the Unruh temperature. We also discover that the quantum resources of one detector can be transferred to another in the case of two stationary detectors through a fluctuating quantum field outside the SBH. We demonstrate that, in contrast to coherence and discord, we are unable to regenerate entanglement for a given initial state and that they are equal for different vacuum states. In certain circumstances, the presence of EHs does not significantly reduce the available resources, as it turns out that all interesting phenomena occur within EHs. Since the world is basically non-inertial, it is necessary to understand the distribution of quantum resources within a relativistic framework.","sentences":["We obtain analytical expressions for distance-based quantum resources and examine their distribution in the proximity of a Schwarzschild black hole (SBH) within a curved background.","For an observer in free fall and their stationary counterpart sharing the Gisin state, the quantum resources are degraded at an infinite Hawking temperature.","The extent of this degradation that occurs as the SBH evaporates is contingent upon the fermionic frequency mode, Gisin state parameters, and the distance between the observer and the event horizon (EH).","In the case of two accelerating detectors in Minkowski spacetime interacting with quantum fluctuating scalar fields (QFSF), we find that quantum coherence and discord exhibit sudden disappearance for certain initial states and sudden reappearance for others except entanglement, regardless of the Unruh temperature.","We also discover that the quantum resources of one detector can be transferred to another in the case of two stationary detectors through a fluctuating quantum field outside the SBH.","We demonstrate that, in contrast to coherence and discord, we are unable to regenerate entanglement for a given initial state and that they are equal for different vacuum states.","In certain circumstances, the presence of EHs does not significantly reduce the available resources, as it turns out that all interesting phenomena occur within EHs.","Since the world is basically non-inertial, it is necessary to understand the distribution of quantum resources within a relativistic framework."],"url":"http://arxiv.org/abs/2402.12308v1","category":"quant-ph"}
{"created":"2024-02-19 17:30:09","title":"Multi-View Conformal Learning for Heterogeneous Sensor Fusion","abstract":"Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comprehensive experimentation, we show that multi-view models perform better than single-view models not only in terms of accuracy-based performance metrics (as it has already been shown in several previous works) but also in conformal measures that provide uncertainty estimation. Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models.","sentences":["Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios.","Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few.","In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day.","While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion.","To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion.","Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework.","We also propose a multi-view semi-conformal model based on sets intersection.","Through comprehensive experimentation, we show that multi-view models perform better than single-view models not only in terms of accuracy-based performance metrics (as it has already been shown in several previous works) but also in conformal measures that provide uncertainty estimation.","Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models."],"url":"http://arxiv.org/abs/2402.12307v1","category":"cs.LG"}
{"created":"2024-02-19 17:25:12","title":"Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering","abstract":"The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.","sentences":["The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now.","In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime.","This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering.","The proposed proof is very general and relies solely on the rotational invariance of the noise.","Numerical experiments on synthetic and real data illustrate the universality of this phenomenon."],"url":"http://arxiv.org/abs/2402.12302v1","category":"stat.ML"}
{"created":"2024-02-19 17:23:10","title":"Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports","abstract":"Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text reports using different prompting techniques.   Results: On the ImaGenome dataset, the best performing open-source model was Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot prompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984, respectively. On the institutional dataset, the best performing open-source model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and few-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.973, respectively.   Conclusion: In this paper, we show that while GPT-4 is superior to open-source models in zero-shot report labeling, the implementation of few-shot prompting can bring open-source models on par with GPT-4. This shows that open-source models could be a performant and privacy preserving alternative to GPT-4 for the task of radiology report classification.","sentences":["Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models.","While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   ","Materials and Methods: Two different and independent datasets were used.","The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021.","The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset.","We then compared the commercial models GPT-3.5","Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text reports using different prompting techniques.   ","Results: On the ImaGenome dataset, the best performing open-source model was Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot prompts, respectively.","GPT-4 achieved micro F1-scores of 0.975 and 0.984, respectively.","On the institutional dataset, the best performing open-source model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and few-shot prompting, respectively.","GPT-4 achieved micro F1-scores of 0.975 and 0.973, respectively.   ","Conclusion: In this paper, we show that while GPT-4 is superior to open-source models in zero-shot report labeling, the implementation of few-shot prompting can bring open-source models on par with GPT-4.","This shows that open-source models could be a performant and privacy preserving alternative to GPT-4 for the task of radiology report classification."],"url":"http://arxiv.org/abs/2402.12298v1","category":"cs.CL"}
{"created":"2024-02-19 17:18:32","title":"Cryo-Near-Field Photovoltage Microscopy of Heavy-Fermion Twisted Symmetric Trilayer Graphene","abstract":"Ever since the initial experimental observation of correlated insulators and superconductivity in the flat Dirac bands of magic angle twisted bilayer graphene, a search for the microscopic description that explains its strong electronic interactions has begun. While the seemingly disagreeing electronic transport and scanning tunneling microscopy experiments suggest a dichotomy between local and extended electronic orbitals, definitive experimental evidence merging the two patterns together has been much sought after. Here, we report on the local photothermoelectric measurements in the flat electronic bands of twisted symmetric trilayer graphene (TSTG). We use a cryogenic scanning near-field optical microscope with an oscillating atomic force microscopy (AFM) tip irradiated by the infrared photons to create a nanoscopic hot spot in the planar samples, which generates a photocurrent that we probe globally. We observe a breakdown of the non-interacting Mott formalism at low temperatures (10K), signaling the importance of the electronic interactions. Our measurements reveal an overall negative offset of the Seebeck coefficient and significant peaks of the local photovoltage values at all positive integer fillings of the TSTG's moir\\'e superlattice, further indicating a substantial deviation from the classical two-band semiconductor Seebeck response. We explain these observations using the interacting topological heavy-fermion model. In addition, our data reveal a spatial variation of the relative interaction strength dependent on the measured local twist angle (1.2{\\deg} - 1.6{\\deg}). Our findings provide experimental evidence of heavy fermion behaviour in the topological flat bands of moir\\'e graphene and epitomize an avenue to apply local thermoelectric measurements to other strongly correlated materials in the disorder-free limit.","sentences":["Ever since the initial experimental observation of correlated insulators and superconductivity in the flat Dirac bands of magic angle twisted bilayer graphene, a search for the microscopic description that explains its strong electronic interactions has begun.","While the seemingly disagreeing electronic transport and scanning tunneling microscopy experiments suggest a dichotomy between local and extended electronic orbitals, definitive experimental evidence merging the two patterns together has been much sought after.","Here, we report on the local photothermoelectric measurements in the flat electronic bands of twisted symmetric trilayer graphene (TSTG).","We use a cryogenic scanning near-field optical microscope with an oscillating atomic force microscopy (AFM) tip irradiated by the infrared photons to create a nanoscopic hot spot in the planar samples, which generates a photocurrent that we probe globally.","We observe a breakdown of the non-interacting Mott formalism at low temperatures (10K), signaling the importance of the electronic interactions.","Our measurements reveal an overall negative offset of the Seebeck coefficient and significant peaks of the local photovoltage values at all positive integer fillings of the TSTG's moir\\'e superlattice, further indicating a substantial deviation from the classical two-band semiconductor Seebeck response.","We explain these observations using the interacting topological heavy-fermion model.","In addition, our data reveal a spatial variation of the relative interaction strength dependent on the measured local twist angle (1.2{\\deg} - 1.6{\\deg}).","Our findings provide experimental evidence of heavy fermion behaviour in the topological flat bands of moir\\'e graphene and epitomize an avenue to apply local thermoelectric measurements to other strongly correlated materials in the disorder-free limit."],"url":"http://arxiv.org/abs/2402.12296v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 17:17:53","title":"Convex-cocompact representations into the isometry group of the infinite-dimensional hyperbolic space","abstract":"We construct convex-cocompact representations of fundamental groups of closed hyperbolic surfaces into the isometry group of the infinite-dimensional hyperbolic space using bendings. We prove that convex-cocompact representations of finitely generated groups in the group of isometries of the infinite-dimensional hyperbolic space form an open set in the space of representations and that the space of deformations (up to conjugation) obtained by bending an irreducible representation of a surface group is infinite-dimensional.","sentences":["We construct convex-cocompact representations of fundamental groups of closed hyperbolic surfaces into the isometry group of the infinite-dimensional hyperbolic space using bendings.","We prove that convex-cocompact representations of finitely generated groups in the group of isometries of the infinite-dimensional hyperbolic space form an open set in the space of representations and that the space of deformations (up to conjugation) obtained by bending an irreducible representation of a surface group is infinite-dimensional."],"url":"http://arxiv.org/abs/2402.12294v1","category":"math.GT"}
{"created":"2024-02-19 17:04:44","title":"A Lower Bound for Estimating Fr\u00e9chet Means","abstract":"Fr\\'echet means, conceptually appealing, generalize the Euclidean expectation to general metric spaces. We explore how well Fr\\'echet means can be estimated from independent and identically distributed samples and uncover a fundamental limitation: In the vicinity of a probability distribution $P$ with nonunique means, independent of sample size, it is not possible to uniformly estimate Fr\\'echet means below a precision determined by the diameter of the set of Fr\\'echet means of $P$. Implications were previously identified for empirical plug-in estimators as part of the phenomenon \\emph{finite sample smeariness}. Our findings thus confirm inevitable statistical challenges in the estimation of Fr\\'echet means on metric spaces for which there exist distributions with nonunique means. Illustrating the relevance of our lower bound, examples of extrinsic, intrinsic, Procrustes, diffusion and Wasserstein means showcase either deteriorating constants or slow convergence rates of empirical Fr\\'echet means for samples near the regime of nonunique means.","sentences":["Fr\\'echet means, conceptually appealing, generalize the Euclidean expectation to general metric spaces.","We explore how well Fr\\'echet means can be estimated from independent and identically distributed samples and uncover a fundamental limitation: In the vicinity of a probability distribution $P$ with nonunique means, independent of sample size, it is not possible to uniformly estimate Fr\\'echet means below a precision determined by the diameter of the set of Fr\\'echet means of $P$. Implications were previously identified for empirical plug-in estimators as part of the phenomenon \\emph{finite sample smeariness}.","Our findings thus confirm inevitable statistical challenges in the estimation of Fr\\'echet means on metric spaces for which there exist distributions with nonunique means.","Illustrating the relevance of our lower bound, examples of extrinsic, intrinsic, Procrustes, diffusion and Wasserstein means showcase either deteriorating constants or slow convergence rates of empirical Fr\\'echet means for samples near the regime of nonunique means."],"url":"http://arxiv.org/abs/2402.12290v1","category":"math.ST"}
{"created":"2024-02-19 17:00:54","title":"Revisiting registration-based synthesis: A focus on unsupervised MR image synthesis","abstract":"Deep learning (DL) has led to significant improvements in medical image synthesis, enabling advanced image-to-image translation to generate synthetic images. However, DL methods face challenges such as domain shift and high demands for training data, limiting their generalizability and applicability. Historically, image synthesis was also carried out using deformable image registration (DIR), a method that warps moving images of a desired modality to match the anatomy of a fixed image. However, concerns about its speed and accuracy led to its decline in popularity. With the recent advances of DL-based DIR, we now revisit and reinvigorate this line of research. In this paper, we propose a fast and accurate synthesis method based on DIR. We use the task of synthesizing a rare magnetic resonance (MR) sequence, white matter nulled (WMn) T1-weighted (T1-w) images, to demonstrate the potential of our approach. During training, our method learns a DIR model based on the widely available MPRAGE sequence, which is a cerebrospinal fluid nulled (CSFn) T1-w inversion recovery gradient echo pulse sequence. During testing, the trained DIR model is first applied to estimate the deformation between moving and fixed CSFn images. Subsequently, this estimated deformation is applied to align the paired WMn counterpart of the moving CSFn image, yielding a synthetic WMn image for the fixed CSFn image. Our experiments demonstrate promising results for unsupervised image synthesis using DIR. These findings highlight the potential of our technique in contexts where supervised synthesis methods are constrained by limited training data.","sentences":["Deep learning (DL) has led to significant improvements in medical image synthesis, enabling advanced image-to-image translation to generate synthetic images.","However, DL methods face challenges such as domain shift and high demands for training data, limiting their generalizability and applicability.","Historically, image synthesis was also carried out using deformable image registration (DIR), a method that warps moving images of a desired modality to match the anatomy of a fixed image.","However, concerns about its speed and accuracy led to its decline in popularity.","With the recent advances of DL-based DIR, we now revisit and reinvigorate this line of research.","In this paper, we propose a fast and accurate synthesis method based on DIR.","We use the task of synthesizing a rare magnetic resonance (MR) sequence, white matter nulled (WMn) T1-weighted (T1-w) images, to demonstrate the potential of our approach.","During training, our method learns a DIR model based on the widely available MPRAGE sequence, which is a cerebrospinal fluid nulled (CSFn) T1-w inversion recovery gradient echo pulse sequence.","During testing, the trained DIR model is first applied to estimate the deformation between moving and fixed CSFn images.","Subsequently, this estimated deformation is applied to align the paired WMn counterpart of the moving CSFn image, yielding a synthetic WMn image for the fixed CSFn image.","Our experiments demonstrate promising results for unsupervised image synthesis using DIR.","These findings highlight the potential of our technique in contexts where supervised synthesis methods are constrained by limited training data."],"url":"http://arxiv.org/abs/2402.12288v1","category":"eess.IV"}
{"created":"2024-02-19 16:58:03","title":"Statistical evaluation and optimization of entanglement purification protocols","abstract":"Quantitative characterization of two-qubit entanglement purification protocols is introduced. Our approach is based on the concurrence and the hit-and-run algorithm applied to the convex set of all two-qubit states. We demonstrate that pioneering protocols are unable to improve the estimated initial average concurrence of almost uniformly sampled density matrices, however, as it is known, they still generate pairs of qubits in a state that is close to a Bell state. We also develop a more efficient protocol and investigate it numerically together with a recent proposal based on an entangling rank-two projector. Furthermore, we present a class of variational purification protocols with continuous parameters and optimize their output concurrence. These optimized algorithms turn out to surpass former proposals and our new protocol by means of not wasting too many entangled states.","sentences":["Quantitative characterization of two-qubit entanglement purification protocols is introduced.","Our approach is based on the concurrence and the hit-and-run algorithm applied to the convex set of all two-qubit states.","We demonstrate that pioneering protocols are unable to improve the estimated initial average concurrence of almost uniformly sampled density matrices, however, as it is known, they still generate pairs of qubits in a state that is close to a Bell state.","We also develop a more efficient protocol and investigate it numerically together with a recent proposal based on an entangling rank-two projector.","Furthermore, we present a class of variational purification protocols with continuous parameters and optimize their output concurrence.","These optimized algorithms turn out to surpass former proposals and our new protocol by means of not wasting too many entangled states."],"url":"http://arxiv.org/abs/2402.12287v1","category":"quant-ph"}
{"created":"2024-02-19 16:51:29","title":"Refining Minimax Regret for Unsupervised Environment Design","abstract":"In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence. We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning.","sentences":["In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective.","Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded.","However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced.","Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates.","In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation.","We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels.","We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence.","We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning."],"url":"http://arxiv.org/abs/2402.12284v1","category":"cs.LG"}
{"created":"2024-02-19 16:51:24","title":"Solution Polishing via Path Relinking for Continuous Black-Box Optimization","abstract":"When faced with a limited budget of function evaluations, state-of-the-art black-box optimization (BBO) solvers struggle to obtain globally, or sometimes even locally, optimal solutions. In such cases, one may pursue solution polishing, i.e., a computational method to improve (or ``polish'') an incumbent solution, typically via some sort of evolutionary algorithm involving two or more solutions. While solution polishing in ``white-box'' optimization has existed for years, relatively little has been published regarding its application in costly-to-evaluate BBO. To fill this void, we explore two novel methods for performing solution polishing along one-dimensional curves rather than along straight lines. We introduce a convex quadratic program that can generate promising curves through multiple elite solutions, i.e., via path relinking, or around a single elite solution. In comparing four solution polishing techniques for continuous BBO, we show that solution polishing along a curve is competitive with solution polishing using a state-of-the-art BBO solver.","sentences":["When faced with a limited budget of function evaluations, state-of-the-art black-box optimization (BBO) solvers struggle to obtain globally, or sometimes even locally, optimal solutions.","In such cases, one may pursue solution polishing, i.e., a computational method to improve (or ``polish'') an incumbent solution, typically via some sort of evolutionary algorithm involving two or more solutions.","While solution polishing in ``white-box'' optimization has existed for years, relatively little has been published regarding its application in costly-to-evaluate BBO.","To fill this void, we explore two novel methods for performing solution polishing along one-dimensional curves rather than along straight lines.","We introduce a convex quadratic program that can generate promising curves through multiple elite solutions, i.e., via path relinking, or around a single elite solution.","In comparing four solution polishing techniques for continuous BBO, we show that solution polishing along a curve is competitive with solution polishing using a state-of-the-art BBO solver."],"url":"http://arxiv.org/abs/2402.12283v1","category":"math.OC"}
{"created":"2024-02-19 16:47:04","title":"Adaptive Skeleton Graph Decoding","abstract":"Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality. Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%.","sentences":["Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs.","Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality.","Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance.","In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems.","Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality.","Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%."],"url":"http://arxiv.org/abs/2402.12280v1","category":"cs.CL"}
{"created":"2024-02-19 16:43:57","title":"Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks","abstract":"Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases. Our final models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual generation.","sentences":["Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages.","Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model.","In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200.","We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language.","Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements.","Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases.","Our final models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual generation."],"url":"http://arxiv.org/abs/2402.12279v1","category":"cs.CL"}
{"created":"2024-02-19 16:43:16","title":"Topological changes and deformation mechanisms of nanoporous Ta under compression","abstract":"While the mechanical behavior of noble nanoporous metals has been the subject of numerous studies, less is known about their recently developed refractory-based counterparts. Here we report on the mechanical properties, deformation mechanisms and topological changes of nanoporous tantalum, a prototypical refractory metal, by means of atomistic simulations of compression tests. An open-source multi-cpu and gpu-capable software is presented and used for the generation of computational samples. The stress strain curves show a non-linear elastic response, with early yielding. The plastic regime is first characterized by a linear hardening followed by an exponential hardening at large strains, associated with a high degree of densification. Plasticity is dominated by dislocation activity, with twinning and vacancy formation appearing as complementary deformation mechanisms. In order to study the mechanical response from a topological perspective, we track the evolution of the genus throughout the tests, finding direct correlations with each regime of the stress strain curves. The results are in agreement with previous studies of plasticity in nanoporous metals and highlight the importance of using topological metrics, for gaining insights into complex aspects of the deformation of nanoporous metals.","sentences":["While the mechanical behavior of noble nanoporous metals has been the subject of numerous studies, less is known about their recently developed refractory-based counterparts.","Here we report on the mechanical properties, deformation mechanisms and topological changes of nanoporous tantalum, a prototypical refractory metal, by means of atomistic simulations of compression tests.","An open-source multi-cpu and gpu-capable software is presented and used for the generation of computational samples.","The stress strain curves show a non-linear elastic response, with early yielding.","The plastic regime is first characterized by a linear hardening followed by an exponential hardening at large strains, associated with a high degree of densification.","Plasticity is dominated by dislocation activity, with twinning and vacancy formation appearing as complementary deformation mechanisms.","In order to study the mechanical response from a topological perspective, we track the evolution of the genus throughout the tests, finding direct correlations with each regime of the stress strain curves.","The results are in agreement with previous studies of plasticity in nanoporous metals and highlight the importance of using topological metrics, for gaining insights into complex aspects of the deformation of nanoporous metals."],"url":"http://arxiv.org/abs/2402.12278v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 16:42:08","title":"Correlations equalities and some upper bounds for the coupling constant implying area decay of Wilson loop for $Z_3$ lattice gauge theories","abstract":"Correlation identities are obtained for $Z_3$ lattice gauge theory where the bonds of the plaquettes are decorated by generalized three-state Ising variables. Making use of correlation inequalities we obtain the area decay of the Wilson loop observable in a range of the coupling parameter larger than those obtained from mean field theory considerations.","sentences":["Correlation identities are obtained for $Z_3$ lattice gauge theory where the bonds of the plaquettes are decorated by generalized three-state Ising variables.","Making use of correlation inequalities we obtain the area decay of the Wilson loop observable in a range of the coupling parameter larger than those obtained from mean field theory considerations."],"url":"http://arxiv.org/abs/2402.12277v1","category":"hep-lat"}
{"created":"2024-02-19 16:39:18","title":"WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment","abstract":"We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.","sentences":["We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment.","The world model tries to explain its interactions, while also being optimistic about what reward it can achieve.","We do this by extending work on program synthesis via LLMs.","We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents."],"url":"http://arxiv.org/abs/2402.12275v1","category":"cs.AI"}
{"created":"2024-02-19 16:39:14","title":"Designing and Prototyping Extensions to MPI in MPICH","abstract":"As HPC system architectures and the applications running on them continue to evolve, the MPI standard itself must evolve. The trend in current and future HPC systems toward powerful nodes with multiple CPU cores and multiple GPU accelerators makes efficient support for hybrid programming critical for applications to achieve high performance. However, the support for hybrid programming in the MPI standard has not kept up with recent trends. The MPICH implementation of MPI provides a platform for implementing and experimenting with new proposals and extensions to fill this gap and to gain valuable experience and feedback before the MPI Forum can consider them for standardization. In this work, we detail six extensions implemented in MPICH to increase MPI interoperability with other runtimes, with a specific focus on heterogeneous architectures. First, the extension to MPI generalized requests lets applications integrate asynchronous tasks into MPI's progress engine. Second, the iovec extension to datatypes lets applications use MPI datatypes as a general-purpose data layout API beyond just MPI communications. Third, a new MPI object, MPIX stream, can be used by applications to identify execution contexts beyond MPI processes, including threads and GPU streams. MPIX stream communicators can be created to make existing MPI functions thread-aware and GPU-aware, thus providing applications with explicit ways to achieve higher performance. Fourth, MPIX Streams are extended to support the enqueue semantics for offloading MPI communications onto a GPU stream context. Fifth, thread communicators allow MPI communicators to be constructed with individual threads, thus providing a new level of interoperability between MPI and on-node runtimes such as OpenMP. Lastly, we present an extension to invoke MPI progress, which lets users spawn progress threads with fine-grained control.","sentences":["As HPC system architectures and the applications running on them continue to evolve, the MPI standard itself must evolve.","The trend in current and future HPC systems toward powerful nodes with multiple CPU cores and multiple GPU accelerators makes efficient support for hybrid programming critical for applications to achieve high performance.","However, the support for hybrid programming in the MPI standard has not kept up with recent trends.","The MPICH implementation of MPI provides a platform for implementing and experimenting with new proposals and extensions to fill this gap and to gain valuable experience and feedback before the MPI Forum can consider them for standardization.","In this work, we detail six extensions implemented in MPICH to increase MPI interoperability with other runtimes, with a specific focus on heterogeneous architectures.","First, the extension to MPI generalized requests lets applications integrate asynchronous tasks into MPI's progress engine.","Second, the iovec extension to datatypes lets applications use MPI datatypes as a general-purpose data layout API beyond just MPI communications.","Third, a new MPI object, MPIX stream, can be used by applications to identify execution contexts beyond MPI processes, including threads and GPU streams.","MPIX stream communicators can be created to make existing MPI functions thread-aware and GPU-aware, thus providing applications with explicit ways to achieve higher performance.","Fourth, MPIX Streams are extended to support the enqueue semantics for offloading MPI communications onto a GPU stream context.","Fifth, thread communicators allow MPI communicators to be constructed with individual threads, thus providing a new level of interoperability between MPI and on-node runtimes such as OpenMP.","Lastly, we present an extension to invoke MPI progress, which lets users spawn progress threads with fine-grained control."],"url":"http://arxiv.org/abs/2402.12274v1","category":"cs.DC"}
{"created":"2024-02-19 16:38:57","title":"Exact Ansatz of Fermion-Boson Systems for a Quantum Device","abstract":"We present an exact ansatz for the eigenstate problem of mixed fermion-boson systems that can be implemented on quantum devices. Based on a generalization of the electronic contracted Schr\\\"odinger equation (CSE), our approach guides a trial wave function to the ground state of any arbitrary mixed Hamiltonian by directly measuring residuals of the mixed CSE on a quantum device. Unlike density-functional and coupled-cluster theories applied to electron-phonon or electron-photon systems, the accuracy of our approach is not limited by the unknown exchange-correlation functional or the uncontrolled form of the exponential ansatz. To test the performance of the method, we study the Tavis-Cummings model, commonly used in polaritonic quantum chemistry. Our results demonstrate that the CSE is a powerful tool in the development of quantum algorithms for solving general fermion-boson many-body problems.","sentences":["We present an exact ansatz for the eigenstate problem of mixed fermion-boson systems that can be implemented on quantum devices.","Based on a generalization of the electronic contracted Schr\\\"odinger equation (CSE), our approach guides a trial wave function to the ground state of any arbitrary mixed Hamiltonian by directly measuring residuals of the mixed CSE on a quantum device.","Unlike density-functional and coupled-cluster theories applied to electron-phonon or electron-photon systems, the accuracy of our approach is not limited by the unknown exchange-correlation functional or the uncontrolled form of the exponential ansatz.","To test the performance of the method, we study the Tavis-Cummings model, commonly used in polaritonic quantum chemistry.","Our results demonstrate that the CSE is a powerful tool in the development of quantum algorithms for solving general fermion-boson many-body problems."],"url":"http://arxiv.org/abs/2402.12273v1","category":"quant-ph"}
{"created":"2024-02-19 16:34:05","title":"Dynamics, quantum states and Compton scattering in nonlinear gravitational waves","abstract":"The classical dynamics and the construction of quantum states in a plane wave curved spacetime are examined, paying particular attention to the similarities with the case of an electromagnetic plane wave in flat spacetime. A natural map connecting the dynamics of a particle in the Rosen metric and the motion of a charged particle in an electromagnetic plane wave is unveiled. We then discuss how this map can be translated into the quantum description by exploiting the large number of underlying symmetries. We examine the complete analogy between Volkov solutions and fermion states in the Rosen chart and properly extend this to massive vector bosons. We finally report the squared S-matrix element of Compton scattering in a sandwich plane wave spacetime in the form of a two-dimensional integral.","sentences":["The classical dynamics and the construction of quantum states in a plane wave curved spacetime are examined, paying particular attention to the similarities with the case of an electromagnetic plane wave in flat spacetime.","A natural map connecting the dynamics of a particle in the Rosen metric and the motion of a charged particle in an electromagnetic plane wave is unveiled.","We then discuss how this map can be translated into the quantum description by exploiting the large number of underlying symmetries.","We examine the complete analogy between Volkov solutions and fermion states in the Rosen chart and properly extend this to massive vector bosons.","We finally report the squared S-matrix element of Compton scattering in a sandwich plane wave spacetime in the form of a two-dimensional integral."],"url":"http://arxiv.org/abs/2402.12270v1","category":"gr-qc"}
{"created":"2024-02-19 16:29:40","title":"High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models","abstract":"The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric's suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.","sentences":["The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages.","We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese.","We test LLMs on these under-resourced languages and English, in a range of scenarios.","We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations.","For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric's suitability for evaluating non-task-specific systems.","Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages."],"url":"http://arxiv.org/abs/2402.12267v1","category":"cs.CL"}
{"created":"2024-02-19 16:28:16","title":"L-QLES: Sparse Laplacian generator for evaluating Quantum Linear Equation Solvers","abstract":"L-QLES is an open source python code for generating 1D, 2D and 3D Laplacian operators and associated Poisson equations and their classical solutions. Its goal is to provide quantum algorithm developers with a flexible test case framework where features of industrial applications can be incorporated without the need for end-user domain knowledge or reliance on inflexible one-off industry supplied matrix sets. A sample set of 1, 2, and 3 dimensional Laplacians are suggested and used to compare the performance of the Prepare-Select and FABLE block encoding techniques. Results show that large matrices are not needed to investigate industrial characteristics. A matrix with a condition number of 17,000 can be encoded using 13 qubits. L-QLES has also been produced to enable algorithm developers to investigate and optimise both the classical and quantum aspects of the inevitable hybrid nature of quantum linear equation solvers. Prepare-Select encoding that takes over an hour of classical preprocessing time to decompose a 4,096x4,096 matrix into Pauli strings can be can investigated using L-QLES matrices. Similarly, row-column query oracles that have success probabilities $\\le 10^{-7}$ for the same matrix can be investigated.","sentences":["L-QLES is an open source python code for generating 1D, 2D and 3D Laplacian operators and associated Poisson equations and their classical solutions.","Its goal is to provide quantum algorithm developers with a flexible test case framework where features of industrial applications can be incorporated without the need for end-user domain knowledge or reliance on inflexible one-off industry supplied matrix sets.","A sample set of 1, 2, and 3 dimensional Laplacians are suggested and used to compare the performance of the Prepare-Select and FABLE block encoding techniques.","Results show that large matrices are not needed to investigate industrial characteristics.","A matrix with a condition number of 17,000 can be encoded using 13 qubits.","L-QLES has also been produced to enable algorithm developers to investigate and optimise both the classical and quantum aspects of the inevitable hybrid nature of quantum linear equation solvers.","Prepare-Select encoding that takes over an hour of classical preprocessing time to decompose a 4,096x4,096 matrix into Pauli strings can be can investigated using L-QLES matrices.","Similarly, row-column query oracles that have success probabilities $\\le 10^{-7}$ for the same matrix can be investigated."],"url":"http://arxiv.org/abs/2402.12266v1","category":"quant-ph"}
{"created":"2024-02-19 16:26:40","title":"On the Byzantine-Resilience of Distillation-Based Federated Learning","abstract":"Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of KD-based FL algorithms and demonstrate its efficacy. Finally, we provide a general method to make attacks harder to detect, improving their effectiveness.","sentences":["Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost.","These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset.","In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process.","We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging.","Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods.","Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of KD-based FL algorithms and demonstrate its efficacy.","Finally, we provide a general method to make attacks harder to detect, improving their effectiveness."],"url":"http://arxiv.org/abs/2402.12265v1","category":"cs.LG"}
{"created":"2024-02-19 16:26:00","title":"Uncertainty quantification in fine-tuned LLMs using LoRA ensembles","abstract":"Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.","sentences":["Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing.","We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles.","We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning.","In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn."],"url":"http://arxiv.org/abs/2402.12264v1","category":"cs.LG"}
{"created":"2024-02-19 16:20:05","title":"BINGO innovative assembly for background reduction in bolometric $0\u03bd\u03b2\u03b2$ experiments","abstract":"BINGO is a project aiming to set the grounds for large-scale bolometric neutrinoless double-beta-decay experiments capable of investigating the effective Majorana neutrino mass at a few meV level. It focuses on developing innovative technologies to achieve a very low background index, of the order of $10^{-5}$ counts/(keV kg yr) in the region of interest. The BINGO demonstrator, called MINI-BINGO, will be composed of Li$_2$MoO$_4$ and TeO$_2$ crystals coupled to bolometric light detectors designed to investigate the promising double-beta-decay isotopes $^{100}$Mo and $^{130}$Te. This will allow us to reject a significant background in bolometers caused by surface contamination from $\\alpha$-active radionuclides by means of light yield selection. In addition, BINGO introduces new methods to mitigate other sources of background, such as surface radioactive contamination, external $\\gamma$ radioactivity, and pile-up due to random coincidence of background events. This paper focuses on the description of an innovative assembly designed to reduce the passive materials in line of sight of the detectors, which is expected to be a dominant source of background in next-generation bolometric experiments. We present the performance of two prototype modules -- housing four Li$_2$MoO$_4$ crystals in total -- operated in the Canfranc underground laboratory in Spain within a facility developed for the CROSS double-beta-decay experiment.","sentences":["BINGO is a project aiming to set the grounds for large-scale bolometric neutrinoless double-beta-decay experiments capable of investigating the effective Majorana neutrino mass at a few meV level.","It focuses on developing innovative technologies to achieve a very low background index, of the order of $10^{-5}$ counts/(keV kg yr) in the region of interest.","The BINGO demonstrator, called MINI-BINGO, will be composed of Li$_2$MoO$_4$ and TeO$_2$ crystals coupled to bolometric light detectors designed to investigate the promising double-beta-decay isotopes $^{100}$Mo and $^{130}$Te.","This will allow us to reject a significant background in bolometers caused by surface contamination from $\\alpha$-active radionuclides by means of light yield selection.","In addition, BINGO introduces new methods to mitigate other sources of background, such as surface radioactive contamination, external $\\gamma$ radioactivity, and pile-up due to random coincidence of background events.","This paper focuses on the description of an innovative assembly designed to reduce the passive materials in line of sight of the detectors, which is expected to be a dominant source of background in next-generation bolometric experiments.","We present the performance of two prototype modules -- housing four Li$_2$MoO$_4$ crystals in total -- operated in the Canfranc underground laboratory in Spain within a facility developed for the CROSS double-beta-decay experiment."],"url":"http://arxiv.org/abs/2402.12262v1","category":"physics.ins-det"}
{"created":"2024-02-19 16:19:15","title":"NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms","abstract":"The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.","sentences":["The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference.","One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time.","We create a diverse resource of recent English neologisms by using several popular collection methods.","We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words.","Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence.","Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity.","Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks.","LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address.","We will release our benchmark and code for reproducing our experiments."],"url":"http://arxiv.org/abs/2402.12261v1","category":"cs.CL"}
{"created":"2024-02-19 16:14:04","title":"Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition","abstract":"Numerous AI-assisted scholarly applications have been developed to aid different stages of the research process. We present an analysis of AI-assisted scholarly writing generated with ScholaCite, a tool we built that is designed for organizing literature and composing Related Work sections for academic papers. Our evaluation method focuses on the analysis of citation graphs to assess the structural complexity and inter-connectedness of citations in texts and involves a three-way comparison between (1) original human-written texts, (2) purely GPT-generated texts, and (3) human-AI collaborative texts. We find that GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention. We suggest that future writing assistant tools should not be used to draft text independently of the human author.","sentences":["Numerous AI-assisted scholarly applications have been developed to aid different stages of the research process.","We present an analysis of AI-assisted scholarly writing generated with ScholaCite, a tool we built that is designed for organizing literature and composing Related Work sections for academic papers.","Our evaluation method focuses on the analysis of citation graphs to assess the structural complexity and inter-connectedness of citations in texts and involves a three-way comparison between (1) original human-written texts, (2) purely GPT-generated texts, and (3) human-AI collaborative texts.","We find that GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention.","We suggest that future writing assistant tools should not be used to draft text independently of the human author."],"url":"http://arxiv.org/abs/2402.12255v1","category":"cs.CL"}
{"created":"2024-02-19 16:09:43","title":"Water Vapour Transit Ambiguities for Habitable M-Earths","abstract":"We have shown in a recent study, using 3D climate simulations, that dayside land cover has a substantial impact on the climate of a synchronously rotating temperate rocky planet such as Proxima Centauri b. Building on that result, we generate synthetic transit spectra from our simulations to assess the impact of these land-induced climate uncertainties on water vapour transit signals. We find that distinct climate regimes will likely be very difficult to differentiate in transit spectra, even under the more favourable conditions of smaller planets orbiting ultracool dwarfs. Further, we show that additional climate ambiguities arise when both land cover and atmosphere mass are unknown, as is likely to be the case for transiting planets. While water vapour may be detectable under favourable conditions, it may be nearly impossible to infer a rocky planet's surface conditions or climate state from its transit spectrum due to the interdependent effects of land cover and atmosphere mass on surface temperature, humidity, and terminator cloud cover.","sentences":["We have shown in a recent study, using 3D climate simulations, that dayside land cover has a substantial impact on the climate of a synchronously rotating temperate rocky planet such as Proxima Centauri b. Building on that result, we generate synthetic transit spectra from our simulations to assess the impact of these land-induced climate uncertainties on water vapour transit signals.","We find that distinct climate regimes will likely be very difficult to differentiate in transit spectra, even under the more favourable conditions of smaller planets orbiting ultracool dwarfs.","Further, we show that additional climate ambiguities arise when both land cover and atmosphere mass are unknown, as is likely to be the case for transiting planets.","While water vapour may be detectable under favourable conditions, it may be nearly impossible to infer a rocky planet's surface conditions or climate state from its transit spectrum due to the interdependent effects of land cover and atmosphere mass on surface temperature, humidity, and terminator cloud cover."],"url":"http://arxiv.org/abs/2402.12253v1","category":"astro-ph.EP"}
{"created":"2024-02-19 16:05:43","title":"Derivative-Free iterative One-Step Reconstruction for Multispectral CT","abstract":"Image reconstruction in Multispectral Computed Tomography (MSCT) requires solving a challenging nonlinear inverse problem, commonly tackled via iterative optimization algorithms. Existing methods necessitate computing the derivative of the forward map and potentially its regularized inverse. In this work, we present a simple yet highly effective algorithm for MSCT image reconstruction, utilizing iterative update mechanisms that leverage the full forward model in the forward step and a derivative-free adjoint problem. Our approach demonstrates both fast convergence and superior performance compared to existing algorithms, making it an interesting candidate for future work. We also discuss further generalizations of our method and its combination with additional regularization and other data discrepancy terms.","sentences":["Image reconstruction in Multispectral Computed Tomography (MSCT) requires solving a challenging nonlinear inverse problem, commonly tackled via iterative optimization algorithms.","Existing methods necessitate computing the derivative of the forward map and potentially its regularized inverse.","In this work, we present a simple yet highly effective algorithm for MSCT image reconstruction, utilizing iterative update mechanisms that leverage the full forward model in the forward step and a derivative-free adjoint problem.","Our approach demonstrates both fast convergence and superior performance compared to existing algorithms, making it an interesting candidate for future work.","We also discuss further generalizations of our method and its combination with additional regularization and other data discrepancy terms."],"url":"http://arxiv.org/abs/2402.12250v1","category":"math.NA"}
{"created":"2024-02-19 16:05:28","title":"Analysis of Levenshtein Transformer's Decoder and Its Variants","abstract":"Levenshtein transformer (LevT) is a non-autoregressive machine translation model with high decoding efficiency and comparable translation quality in terms of bleu score, due to its parallel decoding and iterative refinement procedure. Are there any deficiencies of its translations and what improvements could be made? In this report, we focus on LevT's decoder and analyse the decoding results length, subword generation, and deletion module's capability. We hope to identify weaknesses of the decoder for future improvements.   We also compare translations of the original LevT, knowledge-distilled LevT, LevT with translation memory, and the KD-LevT with translation memory to see how KD and translation memory can help.","sentences":["Levenshtein transformer (LevT) is a non-autoregressive machine translation model with high decoding efficiency and comparable translation quality in terms of bleu score, due to its parallel decoding and iterative refinement procedure.","Are there any deficiencies of its translations and what improvements could be made?","In this report, we focus on LevT's decoder and analyse the decoding results length, subword generation, and deletion module's capability.","We hope to identify weaknesses of the decoder for future improvements.   ","We also compare translations of the original LevT, knowledge-distilled LevT, LevT with translation memory, and the KD-LevT with translation memory to see how KD and translation memory can help."],"url":"http://arxiv.org/abs/2402.12249v1","category":"cs.CL"}
{"created":"2024-02-19 16:03:18","title":"A high-order, fully well-balanced, unconditionally positivity-preserving finite volume framework for flood simulations","abstract":"In this work, we present a high-order finite volume framework for the numerical simulation of shallow water flows. The method is designed to accurately capture complex dynamics inherent in shallow water systems, particularly suited for applications such as tsunami simulations. The arbitrarily high-order framework ensures precise representation of flow behaviors, crucial for simulating phenomena characterized by rapid changes and fine-scale features. Thanks to an {\\it ad-hoc} reformulation in terms of production-destruction terms, the time integration ensures positivity preservation without any time-step restrictions, a vital attribute for physical consistency, especially in scenarios where negative water depth reconstructions could lead to unrealistic results. In order to introduce the preservation of general steady equilibria dictated by the underlying balance law, the high-order reconstruction and numerical flux are blended in a convex fashion with a well-balanced approximation, which is able to provide exact preservation of both static and moving equilibria. Through numerical experiments, we demonstrate the effectiveness and robustness of the proposed approach in capturing the intricate dynamics of shallow water flows, while preserving key physical properties essential for flood simulations.","sentences":["In this work, we present a high-order finite volume framework for the numerical simulation of shallow water flows.","The method is designed to accurately capture complex dynamics inherent in shallow water systems, particularly suited for applications such as tsunami simulations.","The arbitrarily high-order framework ensures precise representation of flow behaviors, crucial for simulating phenomena characterized by rapid changes and fine-scale features.","Thanks to an {\\it ad-hoc} reformulation in terms of production-destruction terms, the time integration ensures positivity preservation without any time-step restrictions, a vital attribute for physical consistency, especially in scenarios where negative water depth reconstructions could lead to unrealistic results.","In order to introduce the preservation of general steady equilibria dictated by the underlying balance law, the high-order reconstruction and numerical flux are blended in a convex fashion with a well-balanced approximation, which is able to provide exact preservation of both static and moving equilibria.","Through numerical experiments, we demonstrate the effectiveness and robustness of the proposed approach in capturing the intricate dynamics of shallow water flows, while preserving key physical properties essential for flood simulations."],"url":"http://arxiv.org/abs/2402.12248v1","category":"math.NA"}
{"created":"2024-02-19 15:59:48","title":"Unconditional quantum MAGIC advantage in shallow circuit computation","abstract":"Quantum theory promises computation speed-ups than classical means. The full power is believed to reside in \"magic\" states, or equivalently non-Clifford operations -- the secret sauce to establish universal quantum computing. Despite the celebrated Gottesman-Knill Theorem stating that magic-free computation can be efficiently simulated by a classical computer, it is still questionable whether \"magic\" is really magical. Indeed, all the existing results establish its supremacy for efficient computation upon unproven complexity assumptions or queries to black-box oracles. In this work, we show that the magic advantage can be unconditionally established, at least in a shallow circuit with a constant depth. For this purpose, we first construct a specific nonlocal game inspired by the linear binary constraint system, which requires the magic resource to generate the desired nonlocal statistics or quantum \"pseudo telepathy.\" For a relation problem targeting generating such correlations between arbitrary nonlocal computation sites, we construct a shallow circuit with bounded fan-in gates that takes the strategy for quantum pseudo telepathy as a sub-routine to solve the problem with certainty. In contrast, magic-free counterparts inevitably require a logarithmic circuit depth to the input size, and the separation is proven optimal. As by-products, we prove that the nonlocal game we construct has non-unique perfect winning strategies, answering an open problem in quantum self-testing. We also provide an efficient algorithm to aid the search for potential magic-requiring nonlocal games similar to the current one. We anticipate our results to enlighten the ultimate establishment of the unconditional advantage of universal quantum computation.","sentences":["Quantum theory promises computation speed-ups than classical means.","The full power is believed to reside in \"magic\" states, or equivalently non-Clifford operations -- the secret sauce to establish universal quantum computing.","Despite the celebrated Gottesman-Knill Theorem stating that magic-free computation can be efficiently simulated by a classical computer, it is still questionable whether \"magic\" is really magical.","Indeed, all the existing results establish its supremacy for efficient computation upon unproven complexity assumptions or queries to black-box oracles.","In this work, we show that the magic advantage can be unconditionally established, at least in a shallow circuit with a constant depth.","For this purpose, we first construct a specific nonlocal game inspired by the linear binary constraint system, which requires the magic resource to generate the desired nonlocal statistics or quantum \"pseudo telepathy.\"","For a relation problem targeting generating such correlations between arbitrary nonlocal computation sites, we construct a shallow circuit with bounded fan-in gates that takes the strategy for quantum pseudo telepathy as a sub-routine to solve the problem with certainty.","In contrast, magic-free counterparts inevitably require a logarithmic circuit depth to the input size, and the separation is proven optimal.","As by-products, we prove that the nonlocal game we construct has non-unique perfect winning strategies, answering an open problem in quantum self-testing.","We also provide an efficient algorithm to aid the search for potential magic-requiring nonlocal games similar to the current one.","We anticipate our results to enlighten the ultimate establishment of the unconditional advantage of universal quantum computation."],"url":"http://arxiv.org/abs/2402.12246v1","category":"quant-ph"}
{"created":"2024-02-19 15:58:15","title":"Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark","abstract":"Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the benchmark's reliability. Surprisingly, when evaluating models on corrected SQL queries, zero-shot baselines surpassed the performance of state-of-the-art prompting methods. We conclude that informative noise labels and reliable benchmarks are crucial to developing new Text-to-SQL methods that can handle varying types of noise.","sentences":["Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge.","However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors.","This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models.","While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries.","We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types.","The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the benchmark's reliability.","Surprisingly, when evaluating models on corrected SQL queries, zero-shot baselines surpassed the performance of state-of-the-art prompting methods.","We conclude that informative noise labels and reliable benchmarks are crucial to developing new Text-to-SQL methods that can handle varying types of noise."],"url":"http://arxiv.org/abs/2402.12243v1","category":"cs.CL"}
{"created":"2024-02-19 15:57:39","title":"Synthetic location trajectory generation using categorical diffusion models","abstract":"Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation. Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals. ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making. We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space. We demonstrate that our model can synthesize realistic ILPs by comparing conditionally and unconditionally generated sequences to real-world ILPs from a GNSS tracking data set which suggests the potential use of our model for synthetic data generation, for example, for benchmarking models used in mobility research.","sentences":["Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation.","Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals.","ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making.","We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space.","We demonstrate that our model can synthesize realistic ILPs by comparing conditionally and unconditionally generated sequences to real-world ILPs from a GNSS tracking data set which suggests the potential use of our model for synthetic data generation, for example, for benchmarking models used in mobility research."],"url":"http://arxiv.org/abs/2402.12242v1","category":"cs.LG"}
{"created":"2024-02-19 15:54:36","title":"BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts","abstract":"Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs. We show empirically that bears improves RS-awareness of several state-of-the-art NeSy models, and also facilitates acquiring informative dense annotations for mitigation purposes.","sentences":["Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics.","RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts.","Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts.","Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts.","Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs.","We show empirically that bears improves RS-awareness of several state-of-the-art NeSy models, and also facilitates acquiring informative dense annotations for mitigation purposes."],"url":"http://arxiv.org/abs/2402.12240v1","category":"cs.LG"}
{"created":"2024-02-19 15:48:55","title":"Mixed Gaussian Flow for Diverse Trajectory Prediction","abstract":"Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories. Code is available at https://github.com/mulplue/MGF.","sentences":["Existing trajectory prediction studies intensively leverage generative models.","Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories.","However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data.","To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold.","The model shows a better capacity for generating diverse trajectory patterns.","Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions.","In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability.","Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates.","We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories.","Code is available at https://github.com/mulplue/MGF."],"url":"http://arxiv.org/abs/2402.12238v1","category":"cs.CV"}
{"created":"2024-02-19 15:47:47","title":"Learning to Defer in Content Moderation: The Human-AI Interplay","abstract":"Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to capture this human-AI interplay is via the framework of learning to defer, where the algorithm has the option to defer a classification task to humans for a fixed cost and immediately receive feedback. Our model contributes to this literature by introducing congestion in the human review system. Moreover, unlike work on online learning with delayed feedback where the delay in the feedback is exogenous to the algorithm's decisions, the delay in our model is endogenous to both the admission and the scheduling decisions.   We propose a near-optimal learning algorithm that carefully balances the classification loss from a selectively sampled dataset, the idiosyncratic loss of non-reviewed posts, and the delay loss of having congestion in the human review system. To the best of our knowledge, this is the first result for online learning in contextual queueing systems and hence our analytical framework may be of independent interest.","sentences":["Successful content moderation in online platforms relies on a human-AI collaboration approach.","A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review.","This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   ","In this paper, we introduce a model to capture the human-AI interplay in content moderation.","The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review.","Only admitted posts receive human reviews on their harmfulness.","These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system.","The classical learning-theoretic way to capture this human-AI interplay is via the framework of learning to defer, where the algorithm has the option to defer a classification task to humans for a fixed cost and immediately receive feedback.","Our model contributes to this literature by introducing congestion in the human review system.","Moreover, unlike work on online learning with delayed feedback where the delay in the feedback is exogenous to the algorithm's decisions, the delay in our model is endogenous to both the admission and the scheduling decisions.   ","We propose a near-optimal learning algorithm that carefully balances the classification loss from a selectively sampled dataset, the idiosyncratic loss of non-reviewed posts, and the delay loss of having congestion in the human review system.","To the best of our knowledge, this is the first result for online learning in contextual queueing systems and hence our analytical framework may be of independent interest."],"url":"http://arxiv.org/abs/2402.12237v1","category":"cs.LG"}
{"created":"2024-02-19 15:39:39","title":"Kernel KMeans clustering splits for end-to-end unsupervised decision trees","abstract":"Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.","sentences":["Trees are convenient models for obtaining explainable predictions on relatively small datasets.","Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge.","As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri.","This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids.","We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel.","For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree."],"url":"http://arxiv.org/abs/2402.12232v1","category":"stat.ML"}
{"created":"2024-02-19 15:34:35","title":"Variational properties of local functionals driven by arbitrary anisotropies","abstract":"We provide integral representation and $\\Gamma$-compactness results for anisotropic local functionals depending on arbitrary Lipschitz continuous vector fields. In particular, neither bracket-generating assumptions nor linear independence conditions are required.","sentences":["We provide integral representation and $\\Gamma$-compactness results for anisotropic local functionals depending on arbitrary Lipschitz continuous vector fields.","In particular, neither bracket-generating assumptions nor linear independence conditions are required."],"url":"http://arxiv.org/abs/2402.12227v1","category":"math.AP"}
{"created":"2024-02-19 15:33:10","title":"AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling","abstract":"We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/","sentences":["We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music.","AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms.","Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages.","We build a multimodal text-centric dataset for multimodal alignment pre-training.","Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset.","It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs.","Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model.","Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"],"url":"http://arxiv.org/abs/2402.12226v1","category":"cs.CL"}
{"created":"2024-02-19 15:33:09","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability","abstract":"Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space. In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations. However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a novel framework Argus3D in terms of capacity. Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation. Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance.","sentences":["Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space.","In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously.","Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models.","It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions.","This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations.","However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes.","To this end, we then present a novel framework Argus3D in terms of capacity.","Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order.","The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts.","In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation.","Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance."],"url":"http://arxiv.org/abs/2402.12225v1","category":"cs.CV"}
{"created":"2024-02-19 15:30:40","title":"CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation","abstract":"Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving vulnerability detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results indicate that CovRL-Fuzz outperforms the state-of-the-art fuzzers in terms of code coverage and bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related bugs in the latest JavaScript engines, including 39 previously unknown vulnerabilities and 11 CVEs.","sentences":["Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input.","Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem.","However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner.","This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback.","Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map.","This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning.","CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving vulnerability detection while minimizing syntax and semantic errors, all without needing extra post-processing.","Our evaluation results indicate that CovRL-Fuzz outperforms the state-of-the-art fuzzers in terms of code coverage and bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related bugs in the latest JavaScript engines, including 39 previously unknown vulnerabilities and 11 CVEs."],"url":"http://arxiv.org/abs/2402.12222v1","category":"cs.CR"}
{"created":"2024-02-19 15:26:19","title":"Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting","abstract":"Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones.","sentences":["Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation.","However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities.","We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably.","In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation.","Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones."],"url":"http://arxiv.org/abs/2402.12220v1","category":"eess.AS"}
{"created":"2024-02-19 15:21:58","title":"Reformatted Alignment","abstract":"The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.","sentences":["The quality of finetuning data is crucial for aligning large language models (LLMs) with human values.","Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations.","This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence.","This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques.","Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   ","Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.","Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset.","This work highlights the need for further research into the science and mechanistic interpretability of LLMs.","We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign."],"url":"http://arxiv.org/abs/2402.12219v1","category":"cs.CL"}
{"created":"2024-02-19 15:20:35","title":"Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications","abstract":"As AIGC has impacted our society profoundly in the past years, ethical issues have received tremendous attention. The most urgent one is the AIGC copyright dilemma, which can immensely stifle the development of AIGC and greatly cost the entire society. Given the complexity of AIGC copyright governance and the fact that no perfect solution currently exists, previous work advocated copyleft on AI governance but without substantive analysis. In this paper, we take a step further to explore the feasibility of copyleft to alleviate the AIGC copyright dilemma. We conduct a mixed-methods study from two aspects: qualitatively, we use a formal what-if analysis to clarify the dilemma and provide case studies to show the feasibility of copyleft; quantitatively, we perform a carefully designed survey to find out how the public feels about copylefting AIGC. The key findings include: a) people generally perceive the dilemma, b) they prefer to use authorized AIGC under loose restriction, and c) they are positive to copyleft in AIGC and willing to use it in the future.","sentences":["As AIGC has impacted our society profoundly in the past years, ethical issues have received tremendous attention.","The most urgent one is the AIGC copyright dilemma, which can immensely stifle the development of AIGC and greatly cost the entire society.","Given the complexity of AIGC copyright governance and the fact that no perfect solution currently exists, previous work advocated copyleft on AI governance but without substantive analysis.","In this paper, we take a step further to explore the feasibility of copyleft to alleviate the AIGC copyright dilemma.","We conduct a mixed-methods study from two aspects: qualitatively, we use a formal what-if analysis to clarify the dilemma and provide case studies to show the feasibility of copyleft; quantitatively, we perform a carefully designed survey to find out how the public feels about copylefting AIGC.","The key findings include: a) people generally perceive the dilemma, b) they prefer to use authorized AIGC under loose restriction, and c) they are positive to copyleft in AIGC and willing to use it in the future."],"url":"http://arxiv.org/abs/2402.12216v1","category":"cs.CY"}
{"created":"2024-02-19 15:17:16","title":"SlopeSeeker: A Search Tool for Exploring a Dataset of Quantifiable Trends","abstract":"Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets. However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like \"bump\" and \"spike\" in the context of COVID cases, for example. We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends. We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels. The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like \"sharply\" and \"gradually,\" as well as multi-line trends (e.g., \"peak,\" \"valley\"). We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as \"show me stocks that tanked in 2010.\" The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts. In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., \"increase\") to more specific ones (e.g., \"sharp increase\"). A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends. We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces.","sentences":["Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets.","However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like \"bump\" and \"spike\" in the context of COVID cases, for example.","We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends.","We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels.","The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like \"sharply\" and \"gradually,\" as well as multi-line trends (e.g., \"peak,\" \"valley\").","We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as \"show me stocks that tanked in 2010.\"","The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts.","In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., \"increase\") to more specific ones (e.g., \"sharp increase\").","A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends.","We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces."],"url":"http://arxiv.org/abs/2402.12214v1","category":"cs.HC"}
{"created":"2024-02-19 15:14:15","title":"Polarization of Autonomous Generative AI Agents Under Echo Chambers","abstract":"Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group's opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is caused by ChatGPT's high prompt understanding ability to update its opinion by considering its own and surrounding agents' opinions. We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize. As a result, we identified factors that strongly influence polarization, such as the agent's persona. These factors should be monitored to prevent the polarization of AI agents.","sentences":["Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs.","An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol.","The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities.","In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment.","We had AI agents discuss specific topics and analyzed how the group's opinions changed as the discussion progressed.","As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments.","The analysis of opinion transitions shows that this result is caused by ChatGPT's high prompt understanding ability to update its opinion by considering its own and surrounding agents' opinions.","We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize.","As a result, we identified factors that strongly influence polarization, such as the agent's persona.","These factors should be monitored to prevent the polarization of AI agents."],"url":"http://arxiv.org/abs/2402.12212v1","category":"cs.CL"}
{"created":"2024-02-19 15:12:12","title":"Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models","abstract":"In recent years, large language models have achieved significant success in generative tasks (e.g., speech cloning and audio generation) related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serves as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) most codec models are trained on only 1,000 hours of data, whereas most speech language models are trained on 60,000 hours; 2) Achieving good reconstruction performance requires the utilization of numerous codebooks, which increases the burden on downstream speech language models; 3) The initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with improved Fourier transform structures and larger training datasets to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/speechnovateur/languagecodec_tmp .","sentences":["In recent years, large language models have achieved significant success in generative tasks (e.g., speech cloning and audio generation) related to speech, audio, music, and other signal domains.","A crucial element of these models is the discrete acoustic codecs, which serves as an intermediate representation replacing the mel-spectrogram.","However, there exist several gaps between discrete codecs and downstream speech language models.","Specifically, 1) most codec models are trained on only 1,000 hours of data, whereas most speech language models are trained on 60,000 hours; 2) Achieving good reconstruction performance requires the utilization of numerous codebooks, which increases the burden on downstream speech language models; 3) The initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks.","Consequently, leveraging the characteristics of speech language models, we propose Language-Codec.","In the Language-Codec, we introduce a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with improved Fourier transform structures and larger training datasets to address the aforementioned gaps.","We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations.","Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models.","The source code and pre-trained models can be accessed at https://github.com/speechnovateur/languagecodec_tmp ."],"url":"http://arxiv.org/abs/2402.12208v1","category":"eess.AS"}
{"created":"2024-02-19 15:12:10","title":"Dislike of general opinion makes for tight elections","abstract":"In modern democracies, the outcome of elections and referendums is often remarkably tight. The repetition of these divisive events are the hallmark of a split society; to the physicist, however, it is an astonishing feat for such large collections of diverse individuals. Many sociophysics models reproduce the emergence of collective human behavior with interacting agents, which respond to their environment according to simple rules, modulated by random fluctuations. A paragon of this class is the Ising model which, when interactions are strong, predicts that order can emerge from a chaotic initial state. In contrast with many elections, however, this model favors a strong majority. Here, we introduce a new element to this classical theory, which accounts for the influence of opinion polls on the electorate. This brings about a new phase in which two groups divide the opinion equally. These political camps are spatially segregated, and the sharp boundary that separates them makes the system size-dependent, even in the limit of a large electorate. Election data show that, over the last 30 years, countries with more than about a million voters often found themselves in this state, whereas elections in smaller countries yielded more consensual results. We suggest that this transition hinges on the electorate's awareness of the general opinion.","sentences":["In modern democracies, the outcome of elections and referendums is often remarkably tight.","The repetition of these divisive events are the hallmark of a split society; to the physicist, however, it is an astonishing feat for such large collections of diverse individuals.","Many sociophysics models reproduce the emergence of collective human behavior with interacting agents, which respond to their environment according to simple rules, modulated by random fluctuations.","A paragon of this class is the Ising model which, when interactions are strong, predicts that order can emerge from a chaotic initial state.","In contrast with many elections, however, this model favors a strong majority.","Here, we introduce a new element to this classical theory, which accounts for the influence of opinion polls on the electorate.","This brings about a new phase in which two groups divide the opinion equally.","These political camps are spatially segregated, and the sharp boundary that separates them makes the system size-dependent, even in the limit of a large electorate.","Election data show that, over the last 30 years, countries with more than about a million voters often found themselves in this state, whereas elections in smaller countries yielded more consensual results.","We suggest that this transition hinges on the electorate's awareness of the general opinion."],"url":"http://arxiv.org/abs/2402.12207v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 15:07:32","title":"Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages","abstract":"While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.","sentences":["While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages.","One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training.","However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer.","In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages.","We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages."],"url":"http://arxiv.org/abs/2402.12204v1","category":"cs.CL"}
{"created":"2024-02-19 15:07:24","title":"MPI Implementation Profiling for Better Application Performance","abstract":"While application profiling has been a mainstay in the HPC community for years, profiling of MPI and other communication middleware has not received the same degree of exploration. This paper adds to the discussion of MPI profiling, contributing two general-purpose profiling methods as well as practical applications of these methods to an existing implementation. The ability to detect performance defects in MPI codes using these methods increases the potential of further research and development in communication optimization.","sentences":["While application profiling has been a mainstay in the HPC community for years, profiling of MPI and other communication middleware has not received the same degree of exploration.","This paper adds to the discussion of MPI profiling, contributing two general-purpose profiling methods as well as practical applications of these methods to an existing implementation.","The ability to detect performance defects in MPI codes using these methods increases the potential of further research and development in communication optimization."],"url":"http://arxiv.org/abs/2402.12203v1","category":"cs.DC"}
{"created":"2024-02-19 15:06:04","title":"Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach","abstract":"In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec model demonstrates its effectiveness in providing personalized elective recommendations while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets.","sentences":["In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection.","However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations.","In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations.","The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information.","We design an attention mechanism to capture heterogeneity-aware representations.","Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives.","Our HFRec model demonstrates its effectiveness in providing personalized elective recommendations while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets."],"url":"http://arxiv.org/abs/2402.12202v1","category":"cs.IR"}
{"created":"2024-02-19 15:04:05","title":"The matching problem with linear transfers is equivalent to a hide-and-seek game","abstract":"Matching problems with linearly transferable utility (LTU) generalize the well-studied transferable utility (TU) case by relaxing the assumption that utility is transferred one-for-one within matched pairs. We show that LTU matching problems can be reframed as nonzero-sum games between two players, thus generalizing a result from von Neumann. The underlying linear programming structure of TU matching problems, however, is lost when moving to LTU. These results draw a new bridge between non-TU matching problems and the theory of bimatrix games, with consequences notably regarding the computation of stable outcomes.","sentences":["Matching problems with linearly transferable utility (LTU) generalize the well-studied transferable utility (TU) case by relaxing the assumption that utility is transferred one-for-one within matched pairs.","We show that LTU matching problems can be reframed as nonzero-sum games between two players, thus generalizing a result from von Neumann.","The underlying linear programming structure of TU matching problems, however, is lost when moving to LTU.","These results draw a new bridge between non-TU matching problems and the theory of bimatrix games, with consequences notably regarding the computation of stable outcomes."],"url":"http://arxiv.org/abs/2402.12200v1","category":"econ.TH"}
{"created":"2024-02-19 15:03:13","title":"Molecular Dynamics Simulations of Anisotropic Particles Accelerated by Neural-Net Predicted Interactions","abstract":"Rigid bodies, made of smaller composite beads, are commonly used to simulate anisotropic particles with molecular dynamics or Monte Carlo methods. To accurately represent the particle shape and to obtain smooth and realistic effective pair interactions between two rigid bodies, each body may need to contain hundreds of spherical beads. Given an interacting pair of particles, traditional MD methods calculate the inter-body distances between the beads of the rigid bodies within a certain distance. For a system containing many anisotropic particles, distance calculations are computationally costly and limit the attainable system size and simulation time. However, the effective interaction between two rigid particles only depends on the distance between their center of masses and their relative orientation. Therefore, a function directly mapping the center of mass distance and orientation to the interaction energy between the two rigid bodies, would completely bypass inter-bead distance calculations. It is challenging to derive such a general function analytically for most non-spherical rigid bodies. We have trained neural nets, powerful tools to fit nonlinear functions to complex datasets, to achieve this task. The pair configuration is taken as input and the energy, forces and torques between two rigid particles are predicted directly. We show that MD simulations of cubes and cylinders performed with forces and torques obtained from the gradients of the energy neural-nets quantitatively match traditional simulations that uses composite rigid bodies. Both structural quantities and dynamic measures are in agreement, while achieving up to 23 times speed up over traditional molecular dynamics, depending on hardware and system size. The method can, in principle, be applied to any irregular shape with any pair interaction, provided that sufficient training data can be obtained.","sentences":["Rigid bodies, made of smaller composite beads, are commonly used to simulate anisotropic particles with molecular dynamics or Monte Carlo methods.","To accurately represent the particle shape and to obtain smooth and realistic effective pair interactions between two rigid bodies, each body may need to contain hundreds of spherical beads.","Given an interacting pair of particles, traditional MD methods calculate the inter-body distances between the beads of the rigid bodies within a certain distance.","For a system containing many anisotropic particles, distance calculations are computationally costly and limit the attainable system size and simulation time.","However, the effective interaction between two rigid particles only depends on the distance between their center of masses and their relative orientation.","Therefore, a function directly mapping the center of mass distance and orientation to the interaction energy between the two rigid bodies, would completely bypass inter-bead distance calculations.","It is challenging to derive such a general function analytically for most non-spherical rigid bodies.","We have trained neural nets, powerful tools to fit nonlinear functions to complex datasets, to achieve this task.","The pair configuration is taken as input and the energy, forces and torques between two rigid particles are predicted directly.","We show that MD simulations of cubes and cylinders performed with forces and torques obtained from the gradients of the energy neural-nets quantitatively match traditional simulations that uses composite rigid bodies.","Both structural quantities and dynamic measures are in agreement, while achieving up to 23 times speed up over traditional molecular dynamics, depending on hardware and system size.","The method can, in principle, be applied to any irregular shape with any pair interaction, provided that sufficient training data can be obtained."],"url":"http://arxiv.org/abs/2402.12199v1","category":"cond-mat.soft"}
{"created":"2024-02-19 15:02:27","title":"Concentration inequalities, dynamical activity, and trade-off relations","abstract":"We present a concentration inequality in stochastic thermodynamics that provides a lower bound for the probability distribution of observables in Markov processes, where the lower bound includes the dynamical activity, a central thermodynamic quantity in trade-off relations. The derived inequality is called the thermodynamic concentration inequality. As a first corollary of the thermodynamic concentration inequality, by combining it with the Markov inequality, we derive an upper bound on the expectation of an observable given its maximum. Furthermore, as a second corollary, we obtain a generalization of the thermodynamic uncertainty relation, where the first and second moments of the original relation are replaced by the $p$-norm. These corollaries are valid for an arbitrary Markov process with time-dependent transition rates and for an arbitrary initial probability distribution. The thermodynamic concentration inequality provides the foundation for discovering trade-off relations.","sentences":["We present a concentration inequality in stochastic thermodynamics that provides a lower bound for the probability distribution of observables in Markov processes, where the lower bound includes the dynamical activity, a central thermodynamic quantity in trade-off relations.","The derived inequality is called the thermodynamic concentration inequality.","As a first corollary of the thermodynamic concentration inequality, by combining it with the Markov inequality, we derive an upper bound on the expectation of an observable given its maximum.","Furthermore, as a second corollary, we obtain a generalization of the thermodynamic uncertainty relation, where the first and second moments of the original relation are replaced by the $p$-norm.","These corollaries are valid for an arbitrary Markov process with time-dependent transition rates and for an arbitrary initial probability distribution.","The thermodynamic concentration inequality provides the foundation for discovering trade-off relations."],"url":"http://arxiv.org/abs/2402.12197v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 14:54:54","title":"Pan-Mamba: Effective pan-sharpening with State Space Model","abstract":"Pan-sharpening involves integrating information from lowresolution multi-spectral and high-resolution panchromatic images to generate high-resolution multi-spectral counterparts. While recent advancements in the state space model, particularly the efficient long-range dependency modeling achieved by Mamba, have revolutionized computer vision community, its untapped potential in pan-sharpening motivates our exploration. Our contribution, Pan-Mamba, represents a novel pansharpening network that leverages the efficiency of the Mamba model in global information modeling. In Pan-Mamba, we customize two core components: channel swapping Mamba and cross-modal Mamba, strategically designed for efficient cross-modal information exchange and fusion. The former initiates a lightweight cross-modal interaction through the exchange of partial panchromatic and multispectral channels, while the latter facilities the information representation capability by exploiting inherent cross-modal relationships. Through extensive experiments across diverse datasets, our proposed approach surpasses state-of-theart methods, showcasing superior fusion results in pan-sharpening. To the best of our knowledge, this work is the first attempt in exploring the potential of the Mamba model and establishes a new frontier in the pan-sharpening techniques. The source code is available at https://github.com/alexhe101/Pan-Mamba .","sentences":["Pan-sharpening involves integrating information from lowresolution multi-spectral and high-resolution panchromatic images to generate high-resolution multi-spectral counterparts.","While recent advancements in the state space model, particularly the efficient long-range dependency modeling achieved by Mamba, have revolutionized computer vision community, its untapped potential in pan-sharpening motivates our exploration.","Our contribution, Pan-Mamba, represents a novel pansharpening network that leverages the efficiency of the Mamba model in global information modeling.","In Pan-Mamba, we customize two core components: channel swapping Mamba and cross-modal Mamba, strategically designed for efficient cross-modal information exchange and fusion.","The former initiates a lightweight cross-modal interaction through the exchange of partial panchromatic and multispectral channels, while the latter facilities the information representation capability by exploiting inherent cross-modal relationships.","Through extensive experiments across diverse datasets, our proposed approach surpasses state-of-theart methods, showcasing superior fusion results in pan-sharpening.","To the best of our knowledge, this work is the first attempt in exploring the potential of the Mamba model and establishes a new frontier in the pan-sharpening techniques.","The source code is available at https://github.com/alexhe101/Pan-Mamba ."],"url":"http://arxiv.org/abs/2402.12192v1","category":"cs.CV"}
{"created":"2024-02-19 14:54:23","title":"On Coupling Constraints in Linear Bilevel Optimization","abstract":"It is well-known that coupling constraints in linear bilevel optimization can lead to disconnected feasible sets, which is not possible without coupling constraints. However, there is no difference between linear bilevel problems with and without coupling constraints w.r.t. their complexity-theoretical hardness. In this note, we prove that, although there is a clear difference between these two classes of problems in terms of their feasible sets, the classes are equivalent on the level of optimal solutions. To this end, given a general linear bilevel problem with coupling constraints, we derive a respective problem without coupling constraints and prove that it has the same optimal solutions (when projected back to the original variable space).","sentences":["It is well-known that coupling constraints in linear bilevel optimization can lead to disconnected feasible sets, which is not possible without coupling constraints.","However, there is no difference between linear bilevel problems with and without coupling constraints w.r.t.","their complexity-theoretical hardness.","In this note, we prove that, although there is a clear difference between these two classes of problems in terms of their feasible sets, the classes are equivalent on the level of optimal solutions.","To this end, given a general linear bilevel problem with coupling constraints, we derive a respective problem without coupling constraints and prove that it has the same optimal solutions (when projected back to the original variable space)."],"url":"http://arxiv.org/abs/2402.12191v1","category":"math.OC"}
{"created":"2024-02-19 14:52:50","title":"Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships","abstract":"Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions.","sentences":["Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization.","This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data.","This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset.","To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data.","However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging.","To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM.","We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities.","Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure.","We discuss potential mitigations and suggest future research directions."],"url":"http://arxiv.org/abs/2402.12189v1","category":"cs.CL"}
{"created":"2024-02-19 14:51:55","title":"Structure of activity in multiregion recurrent neural networks","abstract":"Neural circuits are composed of multiple regions, each with rich dynamics and engaging in communication with other regions. The combination of local, within-region dynamics and global, network-level dynamics is thought to provide computational flexibility. However, the nature of such multiregion dynamics and the underlying synaptic connectivity patterns remain poorly understood. Here, we study the dynamics of recurrent neural networks with multiple interconnected regions. Within each region, neurons have a combination of random and structured recurrent connections. Motivated by experimental evidence of communication subspaces between cortical areas, these networks have low-rank connectivity between regions, enabling selective routing of activity. These networks exhibit two interacting forms of dynamics: high-dimensional fluctuations within regions and low-dimensional signal transmission between regions. To characterize this interaction, we develop a dynamical mean-field theory to analyze such networks in the limit where each region contains infinitely many neurons, with cross-region currents as key order parameters. Regions can act as both generators and transmitters of activity, roles that we show are in conflict. Specifically, taming the complexity of activity within a region is necessary for it to route signals to and from other regions. Unlike previous models of routing in neural circuits, which suppressed the activities of neuronal groups to control signal flow, routing in our model is achieved by exciting different high-dimensional activity patterns through a combination of connectivity structure and nonlinear recurrent dynamics. This theory provides insight into the interpretation of both multiregion neural data and trained neural networks.","sentences":["Neural circuits are composed of multiple regions, each with rich dynamics and engaging in communication with other regions.","The combination of local, within-region dynamics and global, network-level dynamics is thought to provide computational flexibility.","However, the nature of such multiregion dynamics and the underlying synaptic connectivity patterns remain poorly understood.","Here, we study the dynamics of recurrent neural networks with multiple interconnected regions.","Within each region, neurons have a combination of random and structured recurrent connections.","Motivated by experimental evidence of communication subspaces between cortical areas, these networks have low-rank connectivity between regions, enabling selective routing of activity.","These networks exhibit two interacting forms of dynamics: high-dimensional fluctuations within regions and low-dimensional signal transmission between regions.","To characterize this interaction, we develop a dynamical mean-field theory to analyze such networks in the limit where each region contains infinitely many neurons, with cross-region currents as key order parameters.","Regions can act as both generators and transmitters of activity, roles that we show are in conflict.","Specifically, taming the complexity of activity within a region is necessary for it to route signals to and from other regions.","Unlike previous models of routing in neural circuits, which suppressed the activities of neuronal groups to control signal flow, routing in our model is achieved by exciting different high-dimensional activity patterns through a combination of connectivity structure and nonlinear recurrent dynamics.","This theory provides insight into the interpretation of both multiregion neural data and trained neural networks."],"url":"http://arxiv.org/abs/2402.12188v1","category":"q-bio.NC"}
{"created":"2024-02-19 14:45:46","title":"MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data","abstract":"In the health domain, decisions are often based on different data modalities. Thus, when creating prediction models, multimodal fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial. Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner. We propose MultiFIX: a new interpretability-focused multimodal data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction. An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality. Each part of the model is then explained using explainable artificial intelligence techniques. Attention maps are used to highlight important regions in image inputs. Inherently interpretable symbolic expressions, learned with GP-GOMEA, are used to describe the contribution of tabular inputs. The fusion of the extracted features to predict the target label is also replaced by a symbolic expression, learned with GP-GOMEA. Results on synthetic problems demonstrate the strengths and limitations of MultiFIX. Lastly, we apply MultiFIX to a publicly available dataset for the detection of malignant skin lesions.","sentences":["In the health domain, decisions are often based on different data modalities.","Thus, when creating prediction models, multimodal fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial.","Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner.","We propose MultiFIX: a new interpretability-focused multimodal data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction.","An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality.","Each part of the model is then explained using explainable artificial intelligence techniques.","Attention maps are used to highlight important regions in image inputs.","Inherently interpretable symbolic expressions, learned with GP-GOMEA, are used to describe the contribution of tabular inputs.","The fusion of the extracted features to predict the target label is also replaced by a symbolic expression, learned with GP-GOMEA.","Results on synthetic problems demonstrate the strengths and limitations of MultiFIX.","Lastly, we apply MultiFIX to a publicly available dataset for the detection of malignant skin lesions."],"url":"http://arxiv.org/abs/2402.12183v1","category":"cs.AI"}
{"created":"2024-02-19 14:42:10","title":"Revisiting Data Augmentation in Deep Reinforcement Learning","abstract":"Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.","sentences":["Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL).","Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear.","To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected.","Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them.","We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values.","This analysis suggests recommendations on how to exploit data augmentation in a more principled way.","In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge.","We evaluate our proposition and validate our analysis in several domains.","Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments."],"url":"http://arxiv.org/abs/2402.12181v1","category":"cs.LG"}
{"created":"2024-02-19 14:37:17","title":"Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations","abstract":"Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic. To address this issue of academic dishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations\" is designed to assist proctors in identifying unusual student behavior. Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making. This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams.","sentences":["Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic.","To address this issue of academic dishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations\" is designed to assist proctors in identifying unusual student behavior.","Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making.","This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams."],"url":"http://arxiv.org/abs/2402.12179v1","category":"cs.CV"}
{"created":"2024-02-19 14:33:55","title":"On dual risk models with proportional gains and dependencies","abstract":"In this work, we consider extensions of the dual risk model with proportional gains by introducing a dependence structure between gain sizes and gain interrarrival times. Among others, we further consider the case where the proportional parameter is randomly chosen, the case where it is a uniformly random variable, as well as the case where we may have upwards as well as downwards jumps. Moreover, we consider the case with causal dependence structure, as well as the case where the dependence is based on the generalized Farlie-Gumbel-Morgenstern copula. The ruin probability and the distribution of the time to ruin are investigated.","sentences":["In this work, we consider extensions of the dual risk model with proportional gains by introducing a dependence structure between gain sizes and gain interrarrival times.","Among others, we further consider the case where the proportional parameter is randomly chosen, the case where it is a uniformly random variable, as well as the case where we may have upwards as well as downwards jumps.","Moreover, we consider the case with causal dependence structure, as well as the case where the dependence is based on the generalized Farlie-Gumbel-Morgenstern copula.","The ruin probability and the distribution of the time to ruin are investigated."],"url":"http://arxiv.org/abs/2402.12178v1","category":"math.PR"}
{"created":"2024-02-19 14:33:24","title":"Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning","abstract":"Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.","sentences":["Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs).","The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics.","However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning.","This paper addresses scenarios where the embeddings are only available from a black-box model.","We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model.","Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model.","We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency."],"url":"http://arxiv.org/abs/2402.12177v1","category":"cs.LG"}
{"created":"2024-02-19 14:29:35","title":"Learning Discretized Bayesian Networks with GOMEA","abstract":"Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. The proposed Discretized Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks. Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively. We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network.","sentences":["Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence.","From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact.","Moreover, captured relations can be directly inspected by domain experts.","In practice, data is often real-valued.","Unless assumptions of normality can be made, discretization is often required.","The optimal discretization, however, depends on the relations modelled between the variables.","This complicates learning Bayesian networks from data.","For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning.","In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations.","The proposed Discretized Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks.","Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively.","We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network."],"url":"http://arxiv.org/abs/2402.12175v1","category":"cs.LG"}
{"created":"2024-02-19 14:23:18","title":"Automating Boundary Filling in Cubical Agda","abstract":"When working in a proof assistant, automation is key to discharging routine proof goals such as equations between algebraic expressions. Homotopy Type Theory allows the user to reason about higher structures, such as topological spaces, using higher inductive types (HITs) and univalence. Cubical Agda is an extension of Agda with computational support for HITs and univalence. A difficulty when working in Cubical Agda is dealing with the complex combinatorics of higher structures, an infinite-dimensional generalisation of equational reasoning. To solve these higher-dimensional equations consists in constructing cubes with specified boundaries.   We develop a simplified cubical language in which we isolate and study two automation problems: contortion solving, where we attempt to \"contort\" a cube to fit a given boundary, and the more general Kan solving, where we search for solutions that involve pasting multiple cubes together. Both problems are difficult in the general case - Kan solving is even undecidable - so we focus on heuristics that perform well on practical examples. We provide a solver for the contortion problem using a reformulation of contortions in terms of poset maps, while we solve Kan problems using constraint satisfaction programming. We have implemented our algorithms in an experimental Haskell solver that can be used to automatically solve goals presented by Cubical Agda. We illustrate this with a case study establishing the Eckmann-Hilton theorem using our solver, as well as various benchmarks - providing the ground for further study of proof automation in cubical type theories.","sentences":["When working in a proof assistant, automation is key to discharging routine proof goals such as equations between algebraic expressions.","Homotopy Type Theory allows the user to reason about higher structures, such as topological spaces, using higher inductive types (HITs) and univalence.","Cubical Agda is an extension of Agda with computational support for HITs and univalence.","A difficulty when working in Cubical Agda is dealing with the complex combinatorics of higher structures, an infinite-dimensional generalisation of equational reasoning.","To solve these higher-dimensional equations consists in constructing cubes with specified boundaries.   ","We develop a simplified cubical language in which we isolate and study two automation problems: contortion solving, where we attempt to \"contort\" a cube to fit a given boundary, and the more general Kan solving, where we search for solutions that involve pasting multiple cubes together.","Both problems are difficult in the general case - Kan solving is even undecidable - so we focus on heuristics that perform well on practical examples.","We provide a solver for the contortion problem using a reformulation of contortions in terms of poset maps, while we solve Kan problems using constraint satisfaction programming.","We have implemented our algorithms in an experimental Haskell solver that can be used to automatically solve goals presented by Cubical Agda.","We illustrate this with a case study establishing the Eckmann-Hilton theorem using our solver, as well as various benchmarks - providing the ground for further study of proof automation in cubical type theories."],"url":"http://arxiv.org/abs/2402.12169v1","category":"cs.LO"}
{"created":"2024-02-19 14:22:54","title":"Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning","abstract":"Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.","sentences":["Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented.","However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks.","In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning.","Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks.","Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels.","During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean.","We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods.","Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT.","Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks."],"url":"http://arxiv.org/abs/2402.12168v1","category":"cs.CR"}
{"created":"2024-02-19 14:22:11","title":"Towards Cosmography of the Local Universe","abstract":"Anisotropies in the distance-redshift relation of cosmological sources are expected due to large-scale inhomogeneities in the local Universe. When the observed sources are tracing a large-scale matter flow in a general spacetime geometry, the distance-redshift relation with its anisotropies can be described with a geometrical prediction that generalises the well-known Friedmann-Lema\\^itre-Robertson-Walker result. Furthermore, it turns out that a finite set of multipole coefficients contain the full information about a finite-order truncation of the distance-redshift relation of a given observer. The multipoles of the distance-redshift relation are interesting new cosmological observables that have a direct physical interpretation in terms of kinematical quantities of the underlying matter flow. Using light cones extracted from $N$-body simulations we quantify the anisotropies expected in a $\\Lambda$ cold dark matter cosmology by running a Markov chain Monte Carlo analysis on the observed data. In this observational approach the survey selection implements an implicit smoothing scale over which the effective rest frame of matter is fitted. The perceived anisotropy therefore depends significantly on the redshift range and distribution of sources. We find that the multipoles of the expansion rate, as well as the observer's velocity with respect to the large-scale matter flow, can be determined robustly with our approach.","sentences":["Anisotropies in the distance-redshift relation of cosmological sources are expected due to large-scale inhomogeneities in the local Universe.","When the observed sources are tracing a large-scale matter flow in a general spacetime geometry, the distance-redshift relation with its anisotropies can be described with a geometrical prediction that generalises the well-known Friedmann-Lema\\^itre-Robertson-Walker result.","Furthermore, it turns out that a finite set of multipole coefficients contain the full information about a finite-order truncation of the distance-redshift relation of a given observer.","The multipoles of the distance-redshift relation are interesting new cosmological observables that have a direct physical interpretation in terms of kinematical quantities of the underlying matter flow.","Using light cones extracted from $N$-body simulations we quantify the anisotropies expected in a $\\Lambda$ cold dark matter cosmology by running a Markov chain Monte Carlo analysis on the observed data.","In this observational approach the survey selection implements an implicit smoothing scale over which the effective rest frame of matter is fitted.","The perceived anisotropy therefore depends significantly on the redshift range and distribution of sources.","We find that the multipoles of the expansion rate, as well as the observer's velocity with respect to the large-scale matter flow, can be determined robustly with our approach."],"url":"http://arxiv.org/abs/2402.12165v1","category":"astro-ph.CO"}
{"created":"2024-02-19 14:16:08","title":"Endowing Pre-trained Graph Models with Provable Fairness","abstract":"Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \\textbf{Graph} models with \\textbf{P}rovable f\\textbf{A}i\\textbf{R}ness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness.","sentences":["Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks.","Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications.","The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs.","However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient.","Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario.","To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \\textbf{Graph} models with \\textbf{P}rovable f\\textbf{A}i\\textbf{R}ness (called GraphPAR).","GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks.","Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node.","The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions.","Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics.","Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task.","Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness."],"url":"http://arxiv.org/abs/2402.12161v1","category":"cs.LG"}
{"created":"2024-02-19 14:02:31","title":"Transformer-based Causal Language Models Perform Clustering","abstract":"Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.","sentences":["Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern.","Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks.","However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood.","Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model.","Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning.","We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting."],"url":"http://arxiv.org/abs/2402.12151v1","category":"cs.CL"}
{"created":"2024-02-19 14:02:22","title":"Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One","abstract":"The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performance.","sentences":["The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness.","However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases.","We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data.","In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints.","Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions.","To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performance."],"url":"http://arxiv.org/abs/2402.12150v1","category":"cs.CL"}
{"created":"2024-02-19 14:00:35","title":"End-to-end multilingual fact-checking at scale","abstract":"In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.","sentences":["In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models.","We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b."],"url":"http://arxiv.org/abs/2402.12147v1","category":"cs.CL"}
{"created":"2024-02-19 13:57:55","title":"Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement","abstract":"Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\\textit{Meta}$ $\\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be used to enhance the performance of LLMs in two practical applications: query routing and iterative training data filtering. The former achieves GPT-4-turbo comparable performance with less than half the token consumption, while the latter makes the instruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpass Alpaca-13B over fewer training samples, underscoring the high potential of our proposed method.","sentences":["Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination.","Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query.","To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\\textit{Meta}$ $\\textit{Ranking}$ (MR).","Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs.","We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning.","We further demonstrate that MR can be used to enhance the performance of LLMs in two practical applications: query routing and iterative training data filtering.","The former achieves GPT-4-turbo comparable performance with less than half the token consumption, while the latter makes the instruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpass Alpaca-13B over fewer training samples, underscoring the high potential of our proposed method."],"url":"http://arxiv.org/abs/2402.12146v1","category":"cs.CL"}
{"created":"2024-02-19 13:52:40","title":"Joint mode switching and resource allocation in wireless-powered RIS-aided multiuser communication systems","abstract":"This paper investigates a wireless-powered hybrid reflecting intelligent surface (hybrid RIS)-assisted multiple access system, where the RIS can harvest energy from energy station (ES) transmitted radio frequency signal (RF), and each reflecting element can flexibly switch between active mode, passive mode, and idle mode. The objective is to minimize the maximum energy consumption of the users by jointly optimizing the operating modes of each reflecting element, the amplification factor of active elements, the transmit power, and transmission time allocation, subject to quality-of-service (QoS) of each user and the available energy constraint of RIS. In the formulated optimization problem, the operating modes of each reflecting element are highly coupled with the amplification coefficient of the active reflecting elements, making it a challenging mixed-integer programming problem. To solve this problem, a hierarchical optimization method based on deep reinforcement learning is proposed, where the operating modes of each reflecting element and the amplification coefficient of active elements are obtained by solving the outer sub-problem using proximal policy optimization (PPO), and the transmit power and transmission time allocation are obtained by solving the inner sub-problem using convex optimization methods. Simulation results show that compared to the baseline scheme, the proposed scheme can reduce user energy consumption by $70 \\%$.","sentences":["This paper investigates a wireless-powered hybrid reflecting intelligent surface (hybrid RIS)-assisted multiple access system, where the RIS can harvest energy from energy station (ES) transmitted radio frequency signal (RF), and each reflecting element can flexibly switch between active mode, passive mode, and idle mode.","The objective is to minimize the maximum energy consumption of the users by jointly optimizing the operating modes of each reflecting element, the amplification factor of active elements, the transmit power, and transmission time allocation, subject to quality-of-service (QoS) of each user and the available energy constraint of RIS.","In the formulated optimization problem, the operating modes of each reflecting element are highly coupled with the amplification coefficient of the active reflecting elements, making it a challenging mixed-integer programming problem.","To solve this problem, a hierarchical optimization method based on deep reinforcement learning is proposed, where the operating modes of each reflecting element and the amplification coefficient of active elements are obtained by solving the outer sub-problem using proximal policy optimization (PPO), and the transmit power and transmission time allocation are obtained by solving the inner sub-problem using convex optimization methods.","Simulation results show that compared to the baseline scheme, the proposed scheme can reduce user energy consumption by $70 \\%$."],"url":"http://arxiv.org/abs/2402.12143v1","category":"eess.SP"}
{"created":"2024-02-19 13:46:49","title":"Many-Stage Optimal Stabilized Runge-Kutta Methods for Hyperbolic Partial Differential Equations","abstract":"A novel optimization procedure for the generation of stability polynomials of stabilized explicit Runge-Kutta method is devised. Intended for semidiscretizations of hyperbolic partial differential equations, the herein developed approach allows the optimization of stability polynomials with more than hundred stages. A potential application of these high degree stability polynomials are problems with locally varying characteristic speeds as found in non-uniformly refined meshes and different wave speeds.   To demonstrate the applicability of the stability polynomials we construct 2N storage many-stage Runge-Kutta methods that match their designed second order of accuracy when applied to a range of linear and nonlinear hyperbolic PDEs with smooth solutions. The methods are constructed to reduce the amplification of round off errors which becomes a significant concern for these many-stage methods.","sentences":["A novel optimization procedure for the generation of stability polynomials of stabilized explicit Runge-Kutta method is devised.","Intended for semidiscretizations of hyperbolic partial differential equations, the herein developed approach allows the optimization of stability polynomials with more than hundred stages.","A potential application of these high degree stability polynomials are problems with locally varying characteristic speeds as found in non-uniformly refined meshes and different wave speeds.   ","To demonstrate the applicability of the stability polynomials we construct 2N storage many-stage Runge-Kutta methods that match their designed second order of accuracy when applied to a range of linear and nonlinear hyperbolic PDEs with smooth solutions.","The methods are constructed to reduce the amplification of round off errors which becomes a significant concern for these many-stage methods."],"url":"http://arxiv.org/abs/2402.12140v1","category":"math.NA"}
{"created":"2024-02-19 13:38:15","title":"Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers","abstract":"We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (`what') and location (`where') to develop alongside each other over multiple layers -- allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation and image classification.","sentences":["We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches.","BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two.","This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (`what') and location (`where') to develop alongside each other over multiple layers -- allowing its direct application to dense and instance-based tasks alike.","By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation and image classification."],"url":"http://arxiv.org/abs/2402.12138v1","category":"cs.CV"}
{"created":"2024-02-19 13:37:59","title":"A search for top-squark pair production, in final states containing a top quark, a charm quark and missing transverse momentum, using the 139 fb$^{-1}$ of $pp$ collision data collected by the ATLAS detector","abstract":"This paper presents a search for top-squark pair production in final states with a top quark, a charm quark and missing transverse momentum. The data were collected with the ATLAS detector during LHC Run 2 and corresponds to an integrated luminosity of 139fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}$ = 13 TeV. The analysis is motivated by an extended Minimal Supersymmetric Standard Model featuring a non-minimal flavour violation in the second- and third-generation squark sector. The top squark in this model has two possible decay modes, either $\\tilde{t}_1 \\rightarrow c\\tilde{\\chi}_1^0$ or $\\tilde{t}_1\\rightarrow t\\tilde{\\chi}_1^0$, where the $\\tilde{\\chi}_1^0$ is undetected. The analysis is optimised assuming that both of the decay modes are equally probable, leading to the most likely final state of $tc + E_{\\text{T}}^{\\text{miss}}$. Good agreement is found between the Standard Model expectation and the data in the search regions. Exclusion limits at 95% CL are obtained in the $m(\\tilde{t}_1)$ vs $m(\\tilde{\\chi}_1^0)$ plane and, in addition, limits on the branching ratio of the $\\tilde{t}_1\\rightarrow t\\tilde{\\chi}_1^0$ decay as a function of $m(\\tilde{t}_1)$ are also produced. Top-squark masses of up to 800 GeV are excluded for scenarios with light neutralinos, and top-squark masses up to 600 GeV are excluded in scenarios where the neutralino and the top squark are almost mass degenerate.","sentences":["This paper presents a search for top-squark pair production in final states with a top quark, a charm quark and missing transverse momentum.","The data were collected with the ATLAS detector during LHC Run 2 and corresponds to an integrated luminosity of 139fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}$ = 13 TeV.","The analysis is motivated by an extended Minimal Supersymmetric Standard Model featuring a non-minimal flavour violation in the second- and third-generation squark sector.","The top squark in this model has two possible decay modes, either $\\tilde{t}_1 \\rightarrow c\\tilde{\\chi}_1^0$ or $\\tilde{t}_1\\rightarrow t\\tilde{\\chi}_1^0$, where the $\\tilde{\\chi}_1^0$ is undetected.","The analysis is optimised assuming that both of the decay modes are equally probable, leading to the most likely final state of $tc +","E_{\\text{T}}^{\\text{miss}}$. Good agreement is found between the Standard Model expectation and the data in the search regions.","Exclusion limits at 95% CL are obtained in the $m(\\tilde{t}_1)$ vs $m(\\tilde{\\chi}_1^0)$ plane and, in addition, limits on the branching ratio of the $\\tilde{t}_1\\rightarrow t\\tilde{\\chi}_1^0$ decay as a function of $m(\\tilde{t}_1)$ are also produced.","Top-squark masses of up to 800 GeV are excluded for scenarios with light neutralinos, and top-squark masses up to 600 GeV are excluded in scenarios where the neutralino and the top squark are almost mass degenerate."],"url":"http://arxiv.org/abs/2402.12137v1","category":"hep-ex"}
{"created":"2024-02-19 13:37:30","title":"Gel'fand-LevitanTransformations to remove and to add bound states for the half-line matrix Schr\u00f6dinger equation with the general selfadjoint boundary condition","abstract":"We develop transformations to remove and to add bound states, and to decrease or increase the multiplicity of a bound state,for the half-line matrix Schr\\\"odinger equation with the general selfadjoint boundary condition. We use the Gel'fand-Levitan method. When a bound state is added or removed from the spectrum, and when the multiplicity of a bound state is decreased or increased, it is shown how the following quantities transform: the regular and the Jost solutions, the potential, the boundary matrices, the Jost matrix, the scattering matrix, and other relevant quantities. The Gel'fand-Levitan normalization matrices are defined for the bound states by taking into consideration the multiplicities of the bound states.","sentences":["We develop transformations to remove and to add bound states, and to decrease or increase the multiplicity of a bound state,for the half-line matrix Schr\\\"odinger equation with the general selfadjoint boundary condition.","We use the Gel'fand-Levitan method.","When a bound state is added or removed from the spectrum, and when the multiplicity of a bound state is decreased or increased, it is shown how the following quantities transform: the regular and the Jost solutions, the potential, the boundary matrices, the Jost matrix, the scattering matrix, and other relevant quantities.","The Gel'fand-Levitan normalization matrices are defined for the bound states by taking into consideration the multiplicities of the bound states."],"url":"http://arxiv.org/abs/2402.12136v1","category":"math-ph"}
{"created":"2024-02-19 13:32:30","title":"Molecule Generation and Optimization for Efficient Fragrance Creation","abstract":"This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception. Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception. This model includes an AI-driven molecule generator (utilizing Graph and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances. Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations. The methodology employs Transfer Learning and selects the most suitable molecules based on vapor pressure and fragrance notes. Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences. The methodology is validated by reproducing two distinct olfactory experiences using available experimental data.","sentences":["This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception.","Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception.","This model includes an AI-driven molecule generator (utilizing Graph and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances.","Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations.","The methodology employs Transfer Learning and selects the most suitable molecules based on vapor pressure and fragrance notes.","Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences.","The methodology is validated by reproducing two distinct olfactory experiences using available experimental data."],"url":"http://arxiv.org/abs/2402.12134v1","category":"physics.chem-ph"}
{"created":"2024-02-19 13:28:43","title":"SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding","abstract":"Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.","sentences":["Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets.","However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios.","This often results in suboptimal predictions and recommendations.","Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance.","To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs.","To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method.","Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction.","Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations.","This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors."],"url":"http://arxiv.org/abs/2402.12132v1","category":"cs.AI"}
{"created":"2024-02-19 13:27:18","title":"Gravitational vacua in Newman-Penrose formalism","abstract":"In this paper, we derive the generic solution of the Newman-Penrose equations in the Newman-Unti gauge with vanishing curvature tensor. The obtained solutions are the vacua of the gravitational theory which are connected to the derivations in metric formalism from exponentiating the infinitesimal BMS generators in the BMS gauge in \\cite{Compere:2016jwb,Compere:2016hzt} by a radial transformation. The coordinate transformations in the Newman-Unti gauge connecting each vacuum are also obtained. We confirm that the supertranslation charge of the gravitational vacua with respect to global considerations vanishes exactly not only in the Einstein theory but also when including the Holst, Pontryagin, and Gauss-Bonnet terms which verifies that the gravitational vacua are not affected by those trivial or boundary terms.","sentences":["In this paper, we derive the generic solution of the Newman-Penrose equations in the Newman-Unti gauge with vanishing curvature tensor.","The obtained solutions are the vacua of the gravitational theory which are connected to the derivations in metric formalism from exponentiating the infinitesimal BMS generators in the BMS gauge in \\cite{Compere:2016jwb,Compere:2016hzt} by a radial transformation.","The coordinate transformations in the Newman-Unti gauge connecting each vacuum are also obtained.","We confirm that the supertranslation charge of the gravitational vacua with respect to global considerations vanishes exactly not only in the Einstein theory but also when including the Holst, Pontryagin, and Gauss-Bonnet terms which verifies that the gravitational vacua are not affected by those trivial or boundary terms."],"url":"http://arxiv.org/abs/2402.12131v1","category":"hep-th"}
{"created":"2024-02-19 13:26:42","title":"Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based Computing","abstract":"This paper proposes the design and implementation strategy of a novel computing architecture, the Factor Machine. The work is a step towards a general-purpose parallel system operating in a non-sequential manner, exploiting processing/memory co-integration and replacing the traditional Turing/von Neumann model of a computer system with a framework based on \"factorised computation\". This architecture is inspired by neural information processing principles and aims to progress the development of brain-like machine intelligence systems, through providing a computing substrate designed from the ground up to enable efficient implementations of algorithms based on relational networks. The paper provides a rationale for such machine, in the context of the history of computing, and more recent developments in neuromorphic hardware, reviews its general features, and proposes a mixed-signal hardware implementation, based on using analogue circuits to carry out computation and localised and sparse communication between the compute units.","sentences":["This paper proposes the design and implementation strategy of a novel computing architecture, the Factor Machine.","The work is a step towards a general-purpose parallel system operating in a non-sequential manner, exploiting processing/memory co-integration and replacing the traditional Turing/von Neumann model of a computer system with a framework based on \"factorised computation\".","This architecture is inspired by neural information processing principles and aims to progress the development of brain-like machine intelligence systems, through providing a computing substrate designed from the ground up to enable efficient implementations of algorithms based on relational networks.","The paper provides a rationale for such machine, in the context of the history of computing, and more recent developments in neuromorphic hardware, reviews its general features, and proposes a mixed-signal hardware implementation, based on using analogue circuits to carry out computation and localised and sparse communication between the compute units."],"url":"http://arxiv.org/abs/2402.12130v1","category":"cs.AR"}
{"created":"2024-02-19 13:26:38","title":"Modified RRT* for Path Planning in Autonomous Driving","abstract":"Essential tasks in autonomous driving includes environment perception, detection and tracking, path planning and action control. This paper focus on path planning, which is one of the challenging task as it needs to find optimal path in highly complex and dynamic environments. Usually, a driving scenario has large number of obstacles in their route. In this paper, we propose a two-stage path planning algorithm named Angle-based Directed Rapidly exploring Random Trees (AD-RRT*) to address the problem of optimal path in complex environment. The proposed algorithm uses A* algorithm for global path planning and modifies RRT* to bound the samples using angle. The efficiency of the proposed algorithm is evaluated through experiments in different scenarios based on the location and number of obstacles. The proposed algorithm showed higher rate of convergence with reduced time and less number of nodes than the base RRT* algorithm.","sentences":["Essential tasks in autonomous driving includes environment perception, detection and tracking, path planning and action control.","This paper focus on path planning, which is one of the challenging task as it needs to find optimal path in highly complex and dynamic environments.","Usually, a driving scenario has large number of obstacles in their route.","In this paper, we propose a two-stage path planning algorithm named Angle-based Directed Rapidly exploring Random Trees (AD-RRT*) to address the problem of optimal path in complex environment.","The proposed algorithm uses A* algorithm for global path planning and modifies RRT* to bound the samples using angle.","The efficiency of the proposed algorithm is evaluated through experiments in different scenarios based on the location and number of obstacles.","The proposed algorithm showed higher rate of convergence with reduced time and less number of nodes than the base RRT* algorithm."],"url":"http://arxiv.org/abs/2402.12129v1","category":"cs.RO"}
{"created":"2024-02-19 13:24:46","title":"3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection","abstract":"Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP.","sentences":["Vascular structure segmentation plays a crucial role in medical analysis and clinical applications.","The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space.","This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations.","Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure.","To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model.","Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections.","Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance.","Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network.","Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation.","Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP."],"url":"http://arxiv.org/abs/2402.12128v1","category":"cs.CV"}
{"created":"2024-02-19 13:23:17","title":"Rate-Splitting Multiple Access for Transmissive Reconfigurable Intelligent Surface Transceiver Empowered ISAC System","abstract":"In this paper, a novel transmissive reconfigurable intelligent surface (TRIS) transceiver empowered integrated sensing and communications (ISAC) system is proposed for future multi-demand terminals. To address interference management, we implement rate-splitting multiple access (RSMA), where the common stream is independently designed for the sensing service. We introduce the sensing quality of service (QoS) criteria based on this structure and construct an optimization problem with the sensing QoS criteria as the objective function to optimize the sensing stream precoding matrix and the communication stream precoding matrix. Due to the coupling of optimization variables, the formulated problem is a non-convex optimization problem that cannot be solved directly. To tackle the above-mentioned challenging problem, alternating optimization (AO) is utilized to decouple the optimization variables. Specifically, the problem is decoupled into three subproblems about the sensing stream precoding matrix, the communication stream precoding matrix, and the auxiliary variables, which is solved alternatively through AO until the convergence is reached. For solving the problem, successive convex approximation (SCA) is applied to deal with the sum-rate threshold constraints on communications, and difference-of-convex (DC) programming is utilized to solve rank-one non-convex constraints. Numerical simulation results verify the superiority of the proposed scheme in terms of improving the communication and sensing QoS.","sentences":["In this paper, a novel transmissive reconfigurable intelligent surface (TRIS) transceiver empowered integrated sensing and communications (ISAC) system is proposed for future multi-demand terminals.","To address interference management, we implement rate-splitting multiple access (RSMA), where the common stream is independently designed for the sensing service.","We introduce the sensing quality of service (QoS) criteria based on this structure and construct an optimization problem with the sensing QoS criteria as the objective function to optimize the sensing stream precoding matrix and the communication stream precoding matrix.","Due to the coupling of optimization variables, the formulated problem is a non-convex optimization problem that cannot be solved directly.","To tackle the above-mentioned challenging problem, alternating optimization (AO) is utilized to decouple the optimization variables.","Specifically, the problem is decoupled into three subproblems about the sensing stream precoding matrix, the communication stream precoding matrix, and the auxiliary variables, which is solved alternatively through AO until the convergence is reached.","For solving the problem, successive convex approximation (SCA) is applied to deal with the sum-rate threshold constraints on communications, and difference-of-convex (DC) programming is utilized to solve rank-one non-convex constraints.","Numerical simulation results verify the superiority of the proposed scheme in terms of improving the communication and sensing QoS."],"url":"http://arxiv.org/abs/2402.12127v1","category":"cs.IT"}
{"created":"2024-02-19 13:17:09","title":"On General fiber product rings, Poincar\u00e9 series and their structure","abstract":"The present paper deals with the investigation of the structure of general fiber product rings $R\\times_TS$, where $R$, $S$ and $T$ are local rings with common residue field. We show that the Poincar\\'e series of any $R$-module over the fiber product ring $R\\times_TS$ is bounded by a rational function. In addition, we give a description of ${\\rm depth}(R\\times_TS)$, which is an open problem in this theory. As a biproduct, using the characterization of the Betti numbers over $R\\times_TS$ obtained, we provide certain cases of the Cohen-Macaulayness of $R\\times_TS$ and, in particular, we show that $R\\times_TS$ is always non-regular. Some positive answers for the Buchsbaum-Eisenbud-Horrocks and Total rank conjectures over $R\\times_TS$ are also established.","sentences":["The present paper deals with the investigation of the structure of general fiber product rings $R\\times_TS$, where $R$, $S$ and $T$ are local rings with common residue field.","We show that the Poincar\\'e series of any $R$-module over the fiber product ring $R\\times_TS$ is bounded by a rational function.","In addition, we give a description of ${\\rm depth}(R\\times_TS)$, which is an open problem in this theory.","As a biproduct, using the characterization of the Betti numbers over $R\\times_TS$ obtained, we provide certain cases of the Cohen-Macaulayness of $R\\times_TS$ and, in particular, we show that $R\\times_TS$ is always non-regular.","Some positive answers for the Buchsbaum-Eisenbud-Horrocks and Total rank conjectures over $R\\times_TS$ are also established."],"url":"http://arxiv.org/abs/2402.12125v1","category":"math.AC"}
{"created":"2024-02-19 13:16:10","title":"Evaluating Image Review Ability of Vision Language Models","abstract":"Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset reveal that LVLMs, particularly those with proven superiority in other evaluative contexts, excel at distinguishing between high-quality and substandard image reviews.","sentences":["Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model.","This paper explores the use of LVLMs to generate review texts for images.","The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities.","Unlike image captions, review texts can be written from various perspectives such as image composition and exposure.","This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image.","To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings.","We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs.","Our experiments with the dataset reveal that LVLMs, particularly those with proven superiority in other evaluative contexts, excel at distinguishing between high-quality and substandard image reviews."],"url":"http://arxiv.org/abs/2402.12121v1","category":"cs.CL"}
{"created":"2024-02-19 13:13:16","title":"DualView: Data Attribution from the Dual Perspective","abstract":"Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to competing approaches across evaluation metrics. Futhermore, our proposed method produces sparse explanations, where sparseness can be tuned via a hyperparameter. Finally, we showcase that with DualView, we can now render explanations from local data attributions compatible with established local feature attribution methods: For each prediction on (test) data points explained in terms of impactful samples from the training set, we are able to compute and visualize how the prediction on (test) sample relates to each influential training sample in terms of features recognized and by the model. We provide an Open Source implementation of DualView online, together with implementations for all other local data attribution methods we compare against, as well as the metrics reported here, for full reproducibility.","sentences":["Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time.","Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both.","In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results.","With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods.","We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to competing approaches across evaluation metrics.","Futhermore, our proposed method produces sparse explanations, where sparseness can be tuned via a hyperparameter.","Finally, we showcase that with DualView, we can now render explanations from local data attributions compatible with established local feature attribution methods: For each prediction on (test) data points explained in terms of impactful samples from the training set, we are able to compute and visualize how the prediction on (test) sample relates to each influential training sample in terms of features recognized and by the model.","We provide an Open Source implementation of DualView online, together with implementations for all other local data attribution methods we compare against, as well as the metrics reported here, for full reproducibility."],"url":"http://arxiv.org/abs/2402.12118v1","category":"cs.LG"}
{"created":"2024-02-19 13:10:34","title":"Intrinsic anomalous and crystal Hall effects in altermagnets","abstract":"We construct the tight-binding model of the altermagnet in the presence of a finite spin-orbit interaction. As a concrete example, we have studied the collinear FeSb$_2$. The Fe ions form two sublattices with opposite spin polarization. Inclusion of the intra-sublattice hopping amplitudes up to the next-to-nearest neighbor results in the generic model free of accidental degeneracies. The zero spin-orbit interaction limit is achieved by elimination of spin dependent hopping amplitudes. The additional band degeneracies in this limit are shown to coincide with the prediction of spin symmetry. The minimal one-orbital model gives rise to four bands due to two sublattices and two spin orientations. We have constructed a simplified two-band description valid in the limit of large exchange splitting. Close to the center of the Brillouin Zone, it reduces to the $\\mathbf{k}\\cdot \\mathbf{p}$-type Hamiltonian. Based on this Hamiltonian, we have computed the intrinsic contribution to the anomalous Hall conductivity. In a metallic limit, the result can be expressed analytically in terms of the two-kinds of spin splitting: one associated with the altermagnetism, and the other originating from the spin orbit interaction.","sentences":["We construct the tight-binding model of the altermagnet in the presence of a finite spin-orbit interaction.","As a concrete example, we have studied the collinear FeSb$_2$. The Fe ions form two sublattices with opposite spin polarization.","Inclusion of the intra-sublattice hopping amplitudes up to the next-to-nearest neighbor results in the generic model free of accidental degeneracies.","The zero spin-orbit interaction limit is achieved by elimination of spin dependent hopping amplitudes.","The additional band degeneracies in this limit are shown to coincide with the prediction of spin symmetry.","The minimal one-orbital model gives rise to four bands due to two sublattices and two spin orientations.","We have constructed a simplified two-band description valid in the limit of large exchange splitting.","Close to the center of the Brillouin Zone, it reduces to the $\\mathbf{k}\\cdot \\mathbf{p}$-type Hamiltonian.","Based on this Hamiltonian, we have computed the intrinsic contribution to the anomalous Hall conductivity.","In a metallic limit, the result can be expressed analytically in terms of the two-kinds of spin splitting: one associated with the altermagnetism, and the other originating from the spin orbit interaction."],"url":"http://arxiv.org/abs/2402.12115v1","category":"cond-mat.str-el"}
{"created":"2024-02-19 13:08:31","title":"A Spatiotemporal Illumination Model for 3D Image Fusion in Optical Coherence Tomography","abstract":"Optical coherence tomography (OCT) is a non-invasive, micrometer-scale imaging modality that has become a clinical standard in ophthalmology. By raster-scanning the retina, sequential cross-sectional image slices are acquired to generate volumetric data. In-vivo imaging suffers from discontinuities between slices that show up as motion and illumination artifacts. We present a new illumination model that exploits continuity in orthogonally raster-scanned volume data. Our novel spatiotemporal parametrization adheres to illumination continuity both temporally, along the imaged slices, as well as spatially, in the transverse directions. Yet, our formulation does not make inter-slice assumptions, which could have discontinuities. This is the first optimization of a 3D inverse model in an image reconstruction context in OCT. Evaluation in 68 volumes from eyes with pathology showed reduction of illumination artifacts in 88\\% of the data, and only 6\\% showed moderate residual illumination artifacts. The method enables the use of forward-warped motion corrected data, which is more accurate, and enables supersampling and advanced 3D image reconstruction in OCT.","sentences":["Optical coherence tomography (OCT) is a non-invasive, micrometer-scale imaging modality that has become a clinical standard in ophthalmology.","By raster-scanning the retina, sequential cross-sectional image slices are acquired to generate volumetric data.","In-vivo imaging suffers from discontinuities between slices that show up as motion and illumination artifacts.","We present a new illumination model that exploits continuity in orthogonally raster-scanned volume data.","Our novel spatiotemporal parametrization adheres to illumination continuity both temporally, along the imaged slices, as well as spatially, in the transverse directions.","Yet, our formulation does not make inter-slice assumptions, which could have discontinuities.","This is the first optimization of a 3D inverse model in an image reconstruction context in OCT.","Evaluation in 68 volumes from eyes with pathology showed reduction of illumination artifacts in 88\\% of the data, and only 6\\% showed moderate residual illumination artifacts.","The method enables the use of forward-warped motion corrected data, which is more accurate, and enables supersampling and advanced 3D image reconstruction in OCT."],"url":"http://arxiv.org/abs/2402.12114v1","category":"eess.IV"}
{"created":"2024-02-19 13:01:46","title":"The Complexity of Geodesic Spanners using Steiner Points","abstract":"A geometric $t$-spanner $\\mathcal{G}$ on a set $S$ of $n$ point sites in a metric space $P$ is a subgraph of the complete graph on $S$ such that for every pair of sites $p,q$ the distance in $\\mathcal{G}$ is a most $t$ times the distance $d(p,q)$ in $P$. We call a connection between two sites in the spanner a link. In some settings, such as when $P$ is a simple polygon with $m$ vertices and a link is a shortest path in $P$, links can consist of $\\Theta (m)$ segments and thus have non-constant complexity. The total spanner complexity is a recently-introduced measure of how compact a spanner is. In this paper, we study what happens if we are allowed to introduce $k$ Steiner points to reduce the spanner complexity. We study such Steiner spanners in simple polygons, polygonal domains, and edge-weighted trees.   Surprisingly, we show that Steiner points have only limited utility. For a spanner that uses $k$ Steiner points, we provide an $\\Omega(nm/k)$ lower bound on the worst-case complexity of any $(3-\\varepsilon)$-spanner, and an $\\Omega(mn^{1/(t+1)}/k^{1/(t+1)})$ lower bound on the worst-case complexity of any $(t-\\varepsilon)$-spanner, for any constant $\\varepsilon\\in (0,1)$ and integer constant $t \\geq 2$. These lower bounds hold in all settings. Additionally, we show NP-hardness for the problem of deciding whether a set of sites in a polygonal domain admits a $3$-spanner with a given maximum complexity using $k$ Steiner points.   On the positive side, for trees we show how to build a $2t$-spanner that uses $k$ Steiner points and of complexity $O(mn^{1/t}/k^{1/t} + n \\log (n/k))$, for any integer $t \\geq 1$. We generalize this result to forests, and apply it to obtain a $2\\sqrt{2}t$-spanner in a simple polygon or a $6t$-spanner in a polygonal domain, with total complexity $O(mn^{1/t}(\\log k)^{1+1/t}/k^{1/t} + n\\log^2 n)$.","sentences":["A geometric $t$-spanner $\\mathcal{G}$ on a set $S$ of $n$ point sites in a metric space $P$ is a subgraph of the complete graph on $S$ such that for every pair of sites $p,q$ the distance in $\\mathcal{G}$ is a most $t$ times the distance $d(p,q)$ in $P$. We call a connection between two sites in the spanner a link.","In some settings, such as when $P$ is a simple polygon with $m$ vertices and a link is a shortest path in $P$, links can consist of $\\Theta (m)$ segments and thus have non-constant complexity.","The total spanner complexity is a recently-introduced measure of how compact a spanner is.","In this paper, we study what happens if we are allowed to introduce $k$ Steiner points to reduce the spanner complexity.","We study such Steiner spanners in simple polygons, polygonal domains, and edge-weighted trees.   ","Surprisingly, we show that Steiner points have only limited utility.","For a spanner that uses $k$ Steiner points, we provide an $\\Omega(nm/k)$ lower bound on the worst-case complexity of any $(3-\\varepsilon)$-spanner, and an $\\Omega(mn^{1/(t+1)}/k^{1/(t+1)})$ lower bound on the worst-case complexity of any $(t-\\varepsilon)$-spanner, for any constant $\\varepsilon\\in (0,1)$ and integer constant $t \\geq 2$.","These lower bounds hold in all settings.","Additionally, we show NP-hardness for the problem of deciding whether a set of sites in a polygonal domain admits a $3$-spanner with a given maximum complexity using $k$ Steiner points.   ","On the positive side, for trees we show how to build a $2t$-spanner that uses $k$ Steiner points and of complexity $O(mn^{1/t}/k^{1/t} + n \\log (n/k))$, for any integer $t \\geq 1$.","We generalize this result to forests, and apply it to obtain a $2\\sqrt{2}t$-spanner in a simple polygon or a $6t$-spanner in a polygonal domain, with total complexity $O(mn^{1/t}(\\log k)^{1+1/t}/k^{1/t} + n\\log^2 n)$."],"url":"http://arxiv.org/abs/2402.12110v1","category":"cs.CG"}
{"created":"2024-02-19 12:55:00","title":"The sign of linear periods","abstract":"Let $G$ be a group with subgroup $H$, and let $(\\pi,V)$ be a complex representation of $G$. The natural action of the normalizer $N$ of $H$ in $G$ on the space $\\mathrm{Hom}_H(\\pi,\\mathbb{C})$ of $H$-invariant linear forms on $V$, provides a representation $\\chi_{\\pi}$ of $N$ trivial on $H$, which is a character when $\\mathrm{Hom}_H(\\pi,\\mathbb{C})$ is one dimensional. If moreover $G$ is a reductive group over a $p$-adic field, and $\\pi$ is smooth irreducible, it is an interesting problem to express $\\chi_{\\pi}$ in terms of the possibly conjectural Langlands parameter $\\phi_\\pi$ of $\\pi$. In this paper we consider the following situation: $G=\\mathrm{GL}_m(D)$ for $D$ a central division algebra of dimension $d^2$ over a $p$-adic field $F$, $H$ is the centralizer of a non central element $\\delta\\in G$ such that $\\delta^2$ is in the center of $G$, and $\\pi$ has generic Jacquet-Langlands transfer to $\\mathrm{GL}_{md}(F)$. In this setting the space $\\mathrm{Hom}_H(\\pi,\\mathbb{C})$ is at most one dimensional. When $\\mathrm{Hom}_H(\\pi,\\mathbb{C})\\simeq \\mathbb{C}$ and $H\\neq N$, we prove that the value of the $\\chi_{\\pi}$ on the non trivial class of $\\frac{N}{H}$ is $(-1)^m\\epsilon(\\phi_\\pi)$ where $\\epsilon(\\phi_\\pi)$ is the root number of $\\phi_{\\pi}$. Along the way we extend many useful multiplicity one results for linear and Shalika models to the case of non split $G$, and we also classify standard modules with linear periods and Shalika models, which are new results even when $D=F$.","sentences":["Let $G$ be a group with subgroup $H$, and let $(\\pi,V)$ be a complex representation of $G$. The natural action of the normalizer $N$ of $H$ in $G$ on the space $\\mathrm{Hom}_H(\\pi,\\mathbb{C})$ of $H$-invariant linear forms on $V$, provides a representation $\\chi_{\\pi}$ of $N$ trivial on $H$, which is a character when $\\mathrm{Hom}_H(\\pi,\\mathbb{C})$ is one dimensional.","If moreover $G$ is a reductive group over a $p$-adic field, and $\\pi$ is smooth irreducible, it is an interesting problem to express $\\chi_{\\pi}$ in terms of the possibly conjectural Langlands parameter $\\phi_\\pi$ of $\\pi$. In this paper we consider the following situation: $G=\\mathrm{GL}_m(D)$ for $D$ a central division algebra of dimension $d^2$ over a $p$-adic field $F$, $H$ is the centralizer of a non central element $\\delta\\in G$ such that $\\delta^2$ is in the center of $G$, and $\\pi$ has generic Jacquet-Langlands transfer to $\\mathrm{GL}_{md}(F)$. In this setting the space $\\mathrm{Hom}_H(\\pi,\\mathbb{C})$ is at most one dimensional.","When $\\mathrm{Hom}_H(\\pi,\\mathbb{C})\\simeq \\mathbb{C}$ and $H\\neq N$, we prove that the value of the $\\chi_{\\pi}$ on the non trivial class of $\\frac{N}{H}$ is $(-1)^m\\epsilon(\\phi_\\pi)$ where $\\epsilon(\\phi_\\pi)$ is the root number of $\\phi_{\\pi}$. Along the way we extend many useful multiplicity one results for linear and Shalika models to the case of non split $G$, and we also classify standard modules with linear periods and Shalika models, which are new results even when $D=F$."],"url":"http://arxiv.org/abs/2402.12106v1","category":"math.RT"}
{"created":"2024-02-19 12:45:52","title":"Is It a Free Lunch for Removing Outliers during Pretraining?","abstract":"With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \\citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models.","sentences":["With the growing size of large language models, the role of quantization becomes increasingly significant.","However, outliers present in weights or activations notably influence the performance of quantized models.","Recently, \\citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization.","Interestingly, we observed that such an approach leads to performance degradation in full precision.","Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning.","Moreover, this improved method also facilitates successful pretraining of causal language models."],"url":"http://arxiv.org/abs/2402.12102v1","category":"cs.CL"}
{"created":"2024-02-19 12:31:56","title":"Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation","abstract":"With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney.","sentences":["With the prevalence of text-to-image generative models, their safety becomes a critical concern.","adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content.","However, existing solutions face several challenges, including low success rate and inefficiency.","We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models.","Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts.","Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney."],"url":"http://arxiv.org/abs/2402.12100v1","category":"cs.CL"}
{"created":"2024-02-19 12:27:39","title":"Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization","abstract":"Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving. While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge. This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers. Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point. This allows us to better understand how SS models make their predictions and identify potential areas for improvement. Relevant codes are available at https://github.com/geoai4cities/pGS-CAM.","sentences":["Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving.","While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge.","This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers.","Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet).","Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point.","This allows us to better understand how SS models make their predictions and identify potential areas for improvement.","Relevant codes are available at https://github.com/geoai4cities/pGS-CAM."],"url":"http://arxiv.org/abs/2402.12098v1","category":"cs.CV"}
{"created":"2024-02-19 12:12:35","title":"Do Large Language Models Understand Logic or Just Mimick Context?","abstract":"Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses. This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs.","sentences":["Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference.","A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting.","However, the reasons behind the success of such models using contextual reasoning have not been fully explored.","Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context?","This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts.","Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers.","If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses.","This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs."],"url":"http://arxiv.org/abs/2402.12091v1","category":"cs.CL"}
{"created":"2024-02-19 12:12:32","title":"Characterization of optimization problems that are solvable iteratively with linear convergence","abstract":"In this work, we state a general conjecture on the solvability of optimization problems via algorithms with linear convergence guarantees. We make a first step towards examining its correctness by fully characterizing the problems that are solvable via Riemannian gradient descent with linear convergence.","sentences":["In this work, we state a general conjecture on the solvability of optimization problems via algorithms with linear convergence guarantees.","We make a first step towards examining its correctness by fully characterizing the problems that are solvable via Riemannian gradient descent with linear convergence."],"url":"http://arxiv.org/abs/2402.12090v1","category":"math.OC"}
{"created":"2024-02-19 11:54:47","title":"Periodic Implicit Representation, Design and Optimization of Porous Structures Using Periodic B-splines","abstract":"Porous structures are intricate solid materials with numerous small pores, extensively used in fields like medicine, chemical engineering, and aerospace. However, the design of such structures using computer-aided tools is a time-consuming and tedious process.In this study, we propose a novel representation method and design approach for porous units that can be infinitely spliced to form a porous structure. We use periodic B-spline functions to represent periodic or symmetric porous units. Starting from a voxel representation of a porous sample, the discrete distance field is computed. To fit the discrete distance field with a periodic B-spline, we introduce the constrained least squares progressive-iterative approximation algorithm, which results in an implicit porous unit. This unit can be subject to optimization to enhance connectivity and utilized for topology optimization, thereby improving the model's stiffness while maintaining periodicity or symmetry. The experimental results demonstrate the potential of the designed complex porous units in enhancing the mechanical performance of the model. Consequently, this study has the potential to incorporate remarkable structures derived from artificial design or nature into the design of high-performing models, showing the promise for biomimetic applications.","sentences":["Porous structures are intricate solid materials with numerous small pores, extensively used in fields like medicine, chemical engineering, and aerospace.","However, the design of such structures using computer-aided tools is a time-consuming and tedious process.","In this study, we propose a novel representation method and design approach for porous units that can be infinitely spliced to form a porous structure.","We use periodic B-spline functions to represent periodic or symmetric porous units.","Starting from a voxel representation of a porous sample, the discrete distance field is computed.","To fit the discrete distance field with a periodic B-spline, we introduce the constrained least squares progressive-iterative approximation algorithm, which results in an implicit porous unit.","This unit can be subject to optimization to enhance connectivity and utilized for topology optimization, thereby improving the model's stiffness while maintaining periodicity or symmetry.","The experimental results demonstrate the potential of the designed complex porous units in enhancing the mechanical performance of the model.","Consequently, this study has the potential to incorporate remarkable structures derived from artificial design or nature into the design of high-performing models, showing the promise for biomimetic applications."],"url":"http://arxiv.org/abs/2402.12076v1","category":"cs.GR"}
{"created":"2024-02-19 11:50:30","title":"HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph","abstract":"In recent years, temporal knowledge graph (TKG) reasoning has received significant attention. Most existing methods assume that all timestamps and corresponding graphs are available during training, which makes it difficult to predict future events. To address this issue, recent works learn to infer future events based on historical information. However, these methods do not comprehensively consider the latent patterns behind temporal changes, to pass historical information selectively, update representations appropriately and predict events accurately. In this paper, we propose the Historical Information Passing (HIP) network to predict future events. HIP network passes information from temporal, structural and repetitive perspectives, which are used to model the temporal evolution of events, the interactions of events at the same time step, and the known events respectively. In particular, our method considers the updating of relation representations and adopts three scoring functions corresponding to the above dimensions. Experimental results on five benchmark datasets show the superiority of HIP network, and the significant improvements on Hits@1 prove that our method can more accurately predict what is going to happen.","sentences":["In recent years, temporal knowledge graph (TKG) reasoning has received significant attention.","Most existing methods assume that all timestamps and corresponding graphs are available during training, which makes it difficult to predict future events.","To address this issue, recent works learn to infer future events based on historical information.","However, these methods do not comprehensively consider the latent patterns behind temporal changes, to pass historical information selectively, update representations appropriately and predict events accurately.","In this paper, we propose the Historical Information Passing (HIP) network to predict future events.","HIP network passes information from temporal, structural and repetitive perspectives, which are used to model the temporal evolution of events, the interactions of events at the same time step, and the known events respectively.","In particular, our method considers the updating of relation representations and adopts three scoring functions corresponding to the above dimensions.","Experimental results on five benchmark datasets show the superiority of HIP network, and the significant improvements on Hits@1 prove that our method can more accurately predict what is going to happen."],"url":"http://arxiv.org/abs/2402.12074v1","category":"cs.AI"}
{"created":"2024-02-19 11:49:38","title":"On transasymptotic expansions of o-minimal germs","abstract":"Given an o-minimal expansion $\\mathbb{R}_{\\mathcal{A}}$ of the real ordered field, generated by a generalized quasianalytic class $\\mathcal{A}$, we construct an explicit truncation closed ordered differential field embedding of the Hardy field of the expansion $\\mathbb{R}_{\\mathcal{A},\\exp}$ of $\\mathbb{R}_{\\mathcal{A}}$ by the unrestricted exponential function, into the field $\\mathbb{T}$ of transseries. We use this to prove some non-definability results. In particular, we show that the restriction to the positive half-line of Euler's Gamma function is not definable in the structure $\\mathbb{R}_{\\text{an}^{*},\\exp}$, generated by all convergent generalized power series and the exponential function, thus establishing the non-interdefinability of the restrictions to a neighbourhood of $+\\infty$ of Euler's Gamma and of the Riemann Zeta function.","sentences":["Given an o-minimal expansion $\\mathbb{R}_{\\mathcal{A}}$ of the real ordered field, generated by a generalized quasianalytic class $\\mathcal{A}$, we construct an explicit truncation closed ordered differential field embedding of the Hardy field of the expansion $\\mathbb{R}_{\\mathcal{A},\\exp}$ of $\\mathbb{R}_{\\mathcal{A}}$ by the unrestricted exponential function, into the field $\\mathbb{T}$ of transseries.","We use this to prove some non-definability results.","In particular, we show that the restriction to the positive half-line of Euler's Gamma function is not definable in the structure $\\mathbb{R}_{\\text{an}^{*},\\exp}$, generated by all convergent generalized power series and the exponential function, thus establishing the non-interdefinability of the restrictions to a neighbourhood of $+\\infty$ of Euler's Gamma and of the Riemann Zeta function."],"url":"http://arxiv.org/abs/2402.12073v1","category":"math.LO"}
{"created":"2024-02-19 11:48:09","title":"EmoBench: Evaluating the Emotional Intelligence of Large Language Models","abstract":"Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data will be publicly available from https://github.com/Sahandfer/EmoBench.","sentences":["Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks.","Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited.","Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation.","We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application.","EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding.","Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research.","Our code and data will be publicly available from https://github.com/Sahandfer/EmoBench."],"url":"http://arxiv.org/abs/2402.12071v1","category":"cs.CL"}
{"created":"2024-02-19 11:41:46","title":"On the Computation of Equilibria in Discrete First-Price Auctions","abstract":"We study the computational complexity of computing Bayes-Nash equilibria in first-price auctions with discrete value distributions and discrete bidding space, under general subjective beliefs. It is known that such auctions do not always have pure equilibria. In this paper we prove that the problem of deciding their existence is NP-complete, even for approximate equilibria. On the other hand, it can be shown that mixed equilibria are guaranteed to exist; however, their computational complexity has not been studied before. We establish the PPAD-completeness of computing a mixed equilibrium and we complement this by an efficient algorithm for finding symmetric approximate equilibria in the special case of iid priors. En route to these results, we develop a computational equivalence framework between continuous and discrete first-price auctions, which can be of independent interest, and which allows us to transfer existing positive and negative results from one setting to the other. Finally, we show that correlated equilibria of the auction can be computed in polynomial time.","sentences":["We study the computational complexity of computing Bayes-Nash equilibria in first-price auctions with discrete value distributions and discrete bidding space, under general subjective beliefs.","It is known that such auctions do not always have pure equilibria.","In this paper we prove that the problem of deciding their existence is NP-complete, even for approximate equilibria.","On the other hand, it can be shown that mixed equilibria are guaranteed to exist; however, their computational complexity has not been studied before.","We establish the PPAD-completeness of computing a mixed equilibrium and we complement this by an efficient algorithm for finding symmetric approximate equilibria in the special case of iid priors.","En route to these results, we develop a computational equivalence framework between continuous and discrete first-price auctions, which can be of independent interest, and which allows us to transfer existing positive and negative results from one setting to the other.","Finally, we show that correlated equilibria of the auction can be computed in polynomial time."],"url":"http://arxiv.org/abs/2402.12068v1","category":"cs.GT"}
{"created":"2024-02-19 11:35:01","title":"Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks","abstract":"Visual navigation requires a whole range of capabilities. A crucial one of these is the ability of an agent to determine its own location and heading in an environment. Prior works commonly assume this information as given, or use methods which lack a suitable inductive bias and accumulate error over time. In this work, we show how the method of slow feature analysis (SFA), inspired by neuroscience research, overcomes both limitations by generating interpretable representations of visual data that encode location and heading of an agent. We employ SFA in a modern reinforcement learning context, analyse and compare representations and illustrate where hierarchical SFA can outperform other feature extractors on navigation tasks.","sentences":["Visual navigation requires a whole range of capabilities.","A crucial one of these is the ability of an agent to determine its own location and heading in an environment.","Prior works commonly assume this information as given, or use methods which lack a suitable inductive bias and accumulate error over time.","In this work, we show how the method of slow feature analysis (SFA), inspired by neuroscience research, overcomes both limitations by generating interpretable representations of visual data that encode location and heading of an agent.","We employ SFA in a modern reinforcement learning context, analyse and compare representations and illustrate where hierarchical SFA can outperform other feature extractors on navigation tasks."],"url":"http://arxiv.org/abs/2402.12067v1","category":"cs.LG"}
{"created":"2024-02-19 11:33:21","title":"WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More","abstract":"Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization.","sentences":["Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process.","This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers.","We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs.","To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs.","Specifically, we incorporates past-only quantization to improve the computation of attention.","Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization.","Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization."],"url":"http://arxiv.org/abs/2402.12065v1","category":"cs.LG"}
{"created":"2024-02-19 11:30:00","title":"Causal Equal Protection as Algorithmic Fairness","abstract":"Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classification parity, but also fails to model our moral intuitions in a number of common scenarios, for example, when the predictor is causally downstream relative to the protected characteristic. To address these difficulties, we defend a novel principle, causal equal protection, that models the fair allocation of the risks of erroneous classification through the lenses of causality.","sentences":["Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness.","One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics.","Despite its intuitive appeal, classification parity has come under attack.","Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated.","To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice.","Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications.","We show that equal protection avoids many of the counterexamples to classification parity, but also fails to model our moral intuitions in a number of common scenarios, for example, when the predictor is causally downstream relative to the protected characteristic.","To address these difficulties, we defend a novel principle, causal equal protection, that models the fair allocation of the risks of erroneous classification through the lenses of causality."],"url":"http://arxiv.org/abs/2402.12062v1","category":"cs.CY"}
{"created":"2024-02-19 11:28:20","title":"All Language Models Large and Small","abstract":"Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on LLM calls almost surely enabling it to solve various tasks while significantly lowering computational costs. We test LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs while reducing GPU usage by up to 30%.","sentences":["Many leading language models (LMs) use high-intensity computational resources both during training and execution.","This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others.","We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework.","LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else.","LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM.","We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage.","Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task.","We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on LLM calls almost surely enabling it to solve various tasks while significantly lowering computational costs.","We test LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs while reducing GPU usage by up to 30%."],"url":"http://arxiv.org/abs/2402.12061v1","category":"cs.LG"}
{"created":"2024-02-19 11:26:33","title":"Flipped structured matrix-sequences in image deblurring with general boundary conditions","abstract":"Motivated by a recent work on a preconditioned MINRES for flipped linear systems in imaging, in this note we extend the scope of that research for including more precise boundary conditions such as reflective and anti-reflective ones. We prove spectral results for the matrix-sequences associated to the original problem, which justify the use of the MINRES in the current setting. The theoretical spectral analysis is supported by a wide variety of numerical experiments, concerning the visualization of the spectra of the original matrices in various ways. We also report numerical tests regarding the convergence speed and regularization features of the associated GMRES and MINRES methods. Conclusions and open problems end the present study.","sentences":["Motivated by a recent work on a preconditioned MINRES for flipped linear systems in imaging, in this note we extend the scope of that research for including more precise boundary conditions such as reflective and anti-reflective ones.","We prove spectral results for the matrix-sequences associated to the original problem, which justify the use of the MINRES in the current setting.","The theoretical spectral analysis is supported by a wide variety of numerical experiments, concerning the visualization of the spectra of the original matrices in various ways.","We also report numerical tests regarding the convergence speed and regularization features of the associated GMRES and MINRES methods.","Conclusions and open problems end the present study."],"url":"http://arxiv.org/abs/2402.12059v1","category":"math.NA"}
{"created":"2024-02-19 11:23:53","title":"Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models","abstract":"State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the textual CoT prompting. Our code is released in https://github.com/leixy20/Scaffold.","sentences":["State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks.","Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information.","Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs.","In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination.","Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references.","Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the textual CoT prompting.","Our code is released in https://github.com/leixy20/Scaffold."],"url":"http://arxiv.org/abs/2402.12058v1","category":"cs.CV"}
{"created":"2024-02-19 11:22:18","title":"A Multilayer Eigen-Sensitivity Method Using Loop Gain Model for Oscillation Diagnosis of Converter-Based System","abstract":"Loop gain-based eigen-sensitivity (LGES) is a useful frequency-domain tool for oscillation diagnosis of converter-based system. However, the existing theory is still scant in two aspects: participation factor (PF) is bound up with the frequency-domain modal characteristic that does not necessarily point to the stability as that of the time-domain eigen-sensitivity (i.e., PF of oscillation mode); a systematic LGES analysis framework containing both component- and parameter- level sensitivity is missing. These two factors hinder the application of LGES method on the proper evaluation of stability effects, which are closely related with the time-domain oscillation mode. To address these issues, this paper proposes a multilayer LGES method directed to the oscillation mode, and a full set of indices like PF, component and parameter sensitivity are established. The link from the eigen-sensitivity of frequency domain to that of time domain is revealed, through which it is shown how the proposed LGES method can facilitate the control parameter tuning-guided oscillation suppression. The effectiveness of the proposed LGES method is validated via case studies conducted on a generic AC/DC converter-based system.","sentences":["Loop gain-based eigen-sensitivity (LGES) is a useful frequency-domain tool for oscillation diagnosis of converter-based system.","However, the existing theory is still scant in two aspects: participation factor (PF) is bound up with the frequency-domain modal characteristic that does not necessarily point to the stability as that of the time-domain eigen-sensitivity (i.e., PF of oscillation mode); a systematic LGES analysis framework containing both component- and parameter- level sensitivity is missing.","These two factors hinder the application of LGES method on the proper evaluation of stability effects, which are closely related with the time-domain oscillation mode.","To address these issues, this paper proposes a multilayer LGES method directed to the oscillation mode, and a full set of indices like PF, component and parameter sensitivity are established.","The link from the eigen-sensitivity of frequency domain to that of time domain is revealed, through which it is shown how the proposed LGES method can facilitate the control parameter tuning-guided oscillation suppression.","The effectiveness of the proposed LGES method is validated via case studies conducted on a generic AC/DC converter-based system."],"url":"http://arxiv.org/abs/2402.12057v1","category":"physics.app-ph"}
{"created":"2024-02-19 11:14:49","title":"The interplay between forming planets and photoevaporating discs II: Wind-driven gas redistribution","abstract":"Disc winds and planet-disc interactions are two crucial mechanisms that define the structure, evolution and dispersal of protoplanetary discs. While winds are capable of removing material from discs, eventually leading to their dispersal, massive planets can shape their disc by creating sub-structures such as gaps and spiral arms. We study the interplay between an X-ray photoevaporative disc wind and the substructures generated due to planet-disc interactions to determine how their mutual interactions affect the disc's and the planet's evolution. We perform three-dimensional hydrodynamic simulations of viscous ($\\alpha = 6.9\\cdot10^{-4}$) discs that host a Jupiter-like planet and undergo X-ray photoevaporation. We trace the gas flows within the disc and wind and measure the accretion rate onto the planet, as well as the gravitational torque that is acting on it. Our results show that the planetary gap takes away the wind's pressure support, allowing wind material to fall back into the gap. This opens new pathways for material from the inner disc (and part of the outer disc) to be redistributed through the wind towards the gap. Consequently, the gap becomes shallower, and the flow of mass across the gap in both directions is significantly increased, as well as the planet's mass-accretion rate (by factors $\\approx 5$ and $\\approx 2$, respectively). Moreover, the wind-driven redistribution results in a denser inner disc and less dense outer disc, which, combined with the recycling of a significant portion of the inner wind, leads to longer lifetimes of the inner disc, contrary to the expectation in a planet-induced photoevaporation (PIPE) scenario that has been proposed in the past.","sentences":["Disc winds and planet-disc interactions are two crucial mechanisms that define the structure, evolution and dispersal of protoplanetary discs.","While winds are capable of removing material from discs, eventually leading to their dispersal, massive planets can shape their disc by creating sub-structures such as gaps and spiral arms.","We study the interplay between an X-ray photoevaporative disc wind and the substructures generated due to planet-disc interactions to determine how their mutual interactions affect the disc's and the planet's evolution.","We perform three-dimensional hydrodynamic simulations of viscous ($\\alpha = 6.9\\cdot10^{-4}$) discs that host a Jupiter-like planet and undergo X-ray photoevaporation.","We trace the gas flows within the disc and wind and measure the accretion rate onto the planet, as well as the gravitational torque that is acting on it.","Our results show that the planetary gap takes away the wind's pressure support, allowing wind material to fall back into the gap.","This opens new pathways for material from the inner disc (and part of the outer disc) to be redistributed through the wind towards the gap.","Consequently, the gap becomes shallower, and the flow of mass across the gap in both directions is significantly increased, as well as the planet's mass-accretion rate (by factors $\\approx 5$ and $\\approx 2$, respectively).","Moreover, the wind-driven redistribution results in a denser inner disc and less dense outer disc, which, combined with the recycling of a significant portion of the inner wind, leads to longer lifetimes of the inner disc, contrary to the expectation in a planet-induced photoevaporation (PIPE) scenario that has been proposed in the past."],"url":"http://arxiv.org/abs/2402.12053v1","category":"astro-ph.EP"}
{"created":"2024-02-19 11:00:35","title":"Tones and upstream-travelling waves in ideally-expanded round impinging jets","abstract":"We study the generation of tones by ideally-expanded round jets impinging on a flat plate. Data from large-eddy simulations performed for different nozzle-to-plate distances is explored, and we consider closure of the aeroacoustic feedback loop responsible for the tones by guided jet modes. Allowable frequency ranges for resonance, underpinned by the existence of modes with upstream-directed group velocities, are computed using two different models: a cylindrical vortex-sheet model; and a locally-parallel stability model which considers a finite-thickness velocity profile. It is shown that inclusion of a finite-thickness velocity profile consistent with the mean flow in the vicinity of the plate improves the agreement between observed tones and model predictions. The frequency of the largest tones found in the data are found to fall within, or very close to, the frequency limits of the finite-thickness model, correcting discrepancies observed with the vortex-sheet model. The same trend is observed in comparisons with experimental and numerical data gathered from the literature. Pressure eigenfunctions of the stability model are in good agreement with upstream-travelling disturbances educed from the data at the tone frequencies. This provides further evidence for the involvement of guided jet modes in the resonance mechanism.","sentences":["We study the generation of tones by ideally-expanded round jets impinging on a flat plate.","Data from large-eddy simulations performed for different nozzle-to-plate distances is explored, and we consider closure of the aeroacoustic feedback loop responsible for the tones by guided jet modes.","Allowable frequency ranges for resonance, underpinned by the existence of modes with upstream-directed group velocities, are computed using two different models: a cylindrical vortex-sheet model; and a locally-parallel stability model which considers a finite-thickness velocity profile.","It is shown that inclusion of a finite-thickness velocity profile consistent with the mean flow in the vicinity of the plate improves the agreement between observed tones and model predictions.","The frequency of the largest tones found in the data are found to fall within, or very close to, the frequency limits of the finite-thickness model, correcting discrepancies observed with the vortex-sheet model.","The same trend is observed in comparisons with experimental and numerical data gathered from the literature.","Pressure eigenfunctions of the stability model are in good agreement with upstream-travelling disturbances educed from the data at the tone frequencies.","This provides further evidence for the involvement of guided jet modes in the resonance mechanism."],"url":"http://arxiv.org/abs/2402.12047v1","category":"physics.flu-dyn"}
{"created":"2024-02-19 10:56:58","title":"A Lightweight Parallel Framework for Blind Image Quality Assessment","abstract":"Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on convolutional neural networks (CNNs) or transformer. In addition, some BIQA methods enhance the performance of the model in a two-stage training manner. Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources. To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA. First, we extract the visual features using a pre-trained feature extraction network. Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information. To improve the robustness of the latent representations, we present two novel self-supervised subtasks, including a sample-level category prediction task and a batch-level quality comparison task. The sample-level category prediction task is presented to help the model with coarse-grained distortion perception. The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations. Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches. Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed.","sentences":["Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on convolutional neural networks (CNNs) or transformer.","In addition, some BIQA methods enhance the performance of the model in a two-stage training manner.","Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources.","To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA.","First, we extract the visual features using a pre-trained feature extraction network.","Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information.","To improve the robustness of the latent representations, we present two novel self-supervised subtasks, including a sample-level category prediction task and a batch-level quality comparison task.","The sample-level category prediction task is presented to help the model with coarse-grained distortion perception.","The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations.","Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores.","Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches.","Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed."],"url":"http://arxiv.org/abs/2402.12043v1","category":"cs.CV"}
{"created":"2024-02-19 10:56:47","title":"Linear bandits with polylogarithmic minimax regret","abstract":"We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\\lambda_{\\min} ( V_t ) = \\Omega (\\sqrt{\\lambda_{\\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.","sentences":["We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector.","We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms.","Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\\lambda_{\\min} ( V_t ) = \\Omega (\\sqrt{\\lambda_{\\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest.","This allows us to tightly control the expected regret in each time step to be of the order $O(\\frac1{t})$, leading to the logarithmic scaling of the cumulative regret."],"url":"http://arxiv.org/abs/2402.12042v1","category":"cs.LG"}
{"created":"2024-02-19 10:55:49","title":"Attack Tree Generation via Process Mining","abstract":"Attack Trees are a graphical model of security used to study threat scenarios. While visually appealing and supported by solid theories and effective tools, one of their main drawbacks remains the amount of effort required by security experts to design them from scratch. This work aims to remedy this by providing a method for the automatic generation of Attack Trees from attack logs. The main original feature of our approach w.r.t existing ones is the use of Process Mining algorithms to synthesize Attack Trees, which allow users to customize the way a set of logs are summarized as an Attack Tree, for example by discarding statistically irrelevant events. Our approach is supported by a prototype that, apart from the derivation and translation of the model, provides the user with an Attack Tree in the RisQFLan format, a tool used for quantitative risk modeling and analysis with Attack Trees. We illustrate our approach with the case study of attacks on a communication protocol, produced by a state-of-the-art protocol analyzer.","sentences":["Attack Trees are a graphical model of security used to study threat scenarios.","While visually appealing and supported by solid theories and effective tools, one of their main drawbacks remains the amount of effort required by security experts to design them from scratch.","This work aims to remedy this by providing a method for the automatic generation of Attack Trees from attack logs.","The main original feature of our approach w.r.t existing ones is the use of Process Mining algorithms to synthesize Attack Trees, which allow users to customize the way a set of logs are summarized as an Attack Tree, for example by discarding statistically irrelevant events.","Our approach is supported by a prototype that, apart from the derivation and translation of the model, provides the user with an Attack Tree in the RisQFLan format, a tool used for quantitative risk modeling and analysis with Attack Trees.","We illustrate our approach with the case study of attacks on a communication protocol, produced by a state-of-the-art protocol analyzer."],"url":"http://arxiv.org/abs/2402.12040v1","category":"cs.CR"}
{"created":"2024-02-19 10:47:09","title":"Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations","abstract":"Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a fully automated manner.","sentences":["Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance.","However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales.","In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance.","Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL.","Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors.","Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a fully automated manner."],"url":"http://arxiv.org/abs/2402.12038v1","category":"cs.LG"}
{"created":"2024-02-19 10:43:13","title":"Class-incremental Learning for Time Series: Benchmark and Evaluation","abstract":"Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the advanced methodologies. Further, based on standardized settings, we develop a unified experimental framework that supports the rapid development of new algorithms, easy integration of new datasets, and standardization of the evaluation process. Using this framework, we conduct a comprehensive evaluation of various generic and time-series-specific CIL methods in both standard and privacy-sensitive scenarios. Our extensive experiments not only provide a standard baseline to support future research but also shed light on the impact of various design factors such as normalization layers or memory budget thresholds. Codes are available at https://github.com/zqiao11/TSCIL.","sentences":["Real-world environments are inherently non-stationary, frequently introducing new classes over time.","This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition.","In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem.","However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied.","Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets.","To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the advanced methodologies.","Further, based on standardized settings, we develop a unified experimental framework that supports the rapid development of new algorithms, easy integration of new datasets, and standardization of the evaluation process.","Using this framework, we conduct a comprehensive evaluation of various generic and time-series-specific CIL methods in both standard and privacy-sensitive scenarios.","Our extensive experiments not only provide a standard baseline to support future research but also shed light on the impact of various design factors such as normalization layers or memory budget thresholds.","Codes are available at https://github.com/zqiao11/TSCIL."],"url":"http://arxiv.org/abs/2402.12035v1","category":"cs.LG"}
{"created":"2024-02-19 10:37:29","title":"Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs","abstract":"Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread use of distillation techniques.","sentences":["Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility.","Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones.","Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning.","However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families.","In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation.","Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread use of distillation techniques."],"url":"http://arxiv.org/abs/2402.12030v1","category":"cs.CL"}
{"created":"2024-02-19 10:34:48","title":"Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space","abstract":"Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are available at https://github.com/ZrW00/MuScleLoRA.","sentences":["Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks.","Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios.","In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.","Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping.","To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters.","Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.","Experimental results demonstrate that MuScleLoRA outperforms baselines significantly.","Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2.","The codes are available at https://github.com/ZrW00/MuScleLoRA."],"url":"http://arxiv.org/abs/2402.12026v1","category":"cs.CL"}
{"created":"2024-02-19 10:33:29","title":"Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought","abstract":"Smart contracts, as a key component of blockchain technology, play a crucial role in ensuring the automation of transactions and adherence to protocol rules. However, smart contracts are susceptible to security vulnerabilities, which, if exploited, can lead to significant asset losses. This study explores the potential of enhancing smart contract security audits using the GPT-4 model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate GPT-4's ability to identify seven common types of vulnerabilities. Moreover, we assessed GPT-4's performance in code parsing and vulnerability capture by simulating a professional auditor's auditing process using CoT(Chain of Thought) prompts based on the audit reports of eight groups of smart contracts. We also evaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through experimentation, we found that GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection. Meanwhile, it demonstrated good contract code parsing capabilities, with an average comprehensive score of 6.5, capable of identifying the background information and functional relationships of smart contracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4 has significant potential application in PoC writing. These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively, but its performance in contract code parsing and PoC writing demonstrates its significant potential as an auxiliary tool in enhancing the efficiency and effectiveness of smart contract security audits.","sentences":["Smart contracts, as a key component of blockchain technology, play a crucial role in ensuring the automation of transactions and adherence to protocol rules.","However, smart contracts are susceptible to security vulnerabilities, which, if exploited, can lead to significant asset losses.","This study explores the potential of enhancing smart contract security audits using the GPT-4 model.","We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate GPT-4's ability to identify seven common types of vulnerabilities.","Moreover, we assessed GPT-4's performance in code parsing and vulnerability capture by simulating a professional auditor's auditing process using CoT(Chain of Thought) prompts based on the audit reports of eight groups of smart contracts.","We also evaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs).","Through experimentation, we found that GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection.","Meanwhile, it demonstrated good contract code parsing capabilities, with an average comprehensive score of 6.5, capable of identifying the background information and functional relationships of smart contracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4 has significant potential application in PoC writing.","These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively, but its performance in contract code parsing and PoC writing demonstrates its significant potential as an auxiliary tool in enhancing the efficiency and effectiveness of smart contract security audits."],"url":"http://arxiv.org/abs/2402.12023v1","category":"cs.CR"}
{"created":"2024-02-19 10:31:53","title":"Distilling Large Language Models for Text-Attributed Graph Learning","abstract":"Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed framework.","sentences":["Text-Attributed Graphs (TAGs) are graphs of connected textual documents.","Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications.","Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues.","Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning.","To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale.","Extensive experiments validate the efficacy of our proposed framework."],"url":"http://arxiv.org/abs/2402.12022v1","category":"cs.CL"}
{"created":"2024-02-19 10:13:25","title":"An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking","abstract":"In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes. Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-armed bandit techniques. It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions. For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards. Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark.","sentences":["In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking.","Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge.","To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes.","Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked.","The discrete state is defined by the dynamic mode.","The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable.","We resort to heuristics through the famous restless multi-armed bandit techniques.","It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions.","For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards.","Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark."],"url":"http://arxiv.org/abs/2402.12015v1","category":"eess.SY"}
{"created":"2024-02-19 10:11:12","title":"The 3-dicritical semi-complete digraphs","abstract":"A digraph is $3$-dicritical if it cannot be vertex-partitioned into two sets inducing acyclic digraphs, but each of its proper subdigraphs can. We give a human-readable proof that the number of 3-dicritical semi-complete digraphs is finite. Further, we give a computer-assisted proof of a full characterization of 3-dicritical semi-complete digraphs. There are eight such digraphs, two of which are tournaments. We finally give a general upper bound on the maximum number of arcs in a $3$-dicritical digraph.","sentences":["A digraph is $3$-dicritical if it cannot be vertex-partitioned into two sets inducing acyclic digraphs, but each of its proper subdigraphs can.","We give a human-readable proof that the number of 3-dicritical semi-complete digraphs is finite.","Further, we give a computer-assisted proof of a full characterization of 3-dicritical semi-complete digraphs.","There are eight such digraphs, two of which are tournaments.","We finally give a general upper bound on the maximum number of arcs in a $3$-dicritical digraph."],"url":"http://arxiv.org/abs/2402.12014v1","category":"math.CO"}
{"created":"2024-02-19 10:08:24","title":"Degenerate conformal blocks for the $W_3$ algebra at c=2 and Specht polynomials","abstract":"We study a homogeneous system of $d+8$ linear partial differential equations (PDEs) in $d$ variables arising from two-dimensional Conformal Field Theories (CFTs) with a $W_3$-symmetry algebra. In the CFT context, $d$ PDEs are third order and correspond to the null-state equations, whereas the remaining 8 PDEs (five being second order and three being first order) correspond to the $W_3$ global Ward identities. In the case of central charge $c=2$, we construct a subspace of the space of all solutions which grow no faster than a power law. We call this subspace the space of $W_3$ conformal blocks, and we provide a basis expressed in terms of Specht polynomials associated with column-strict, rectangular Young tableaux with three columns. The dimension of this space is a Kotska number and it coincides with CFT predictions, hence we conjecture that it exhausts the space of all solutions having a power law bound. Moreover, we prove that the space of $W_3$ conformal blocks is an irreducible representation of a certain diagram algebra defined from $\\mathfrak{sl}_3$ webs that we call Kuperberg algebra. Finally, we formulate a precise conjecture relating the $W_3$ conformal blocks at $c=2$ to scaling limits of probabilities in the triple dimer model recently studied by Kenyon and Shi. We verify the conjecture for explicit examples up to $d=6$. For more general central charges, we expect that $W_3$ conformal blocks are related to scaling limits of probabilities in lattice models based on $\\mathfrak{sl}_3$ webs.","sentences":["We study a homogeneous system of $d+8$ linear partial differential equations (PDEs) in $d$ variables arising from two-dimensional Conformal Field Theories (CFTs) with a $W_3$-symmetry algebra.","In the CFT context, $d$ PDEs are third order and correspond to the null-state equations, whereas the remaining 8 PDEs (five being second order and three being first order) correspond to the $W_3$ global Ward identities.","In the case of central charge $c=2$, we construct a subspace of the space of all solutions which grow no faster than a power law.","We call this subspace the space of $W_3$ conformal blocks, and we provide a basis expressed in terms of Specht polynomials associated with column-strict, rectangular Young tableaux with three columns.","The dimension of this space is a Kotska number and it coincides with CFT predictions, hence we conjecture that it exhausts the space of all solutions having a power law bound.","Moreover, we prove that the space of $W_3$ conformal blocks is an irreducible representation of a certain diagram algebra defined from $\\mathfrak{sl}_3$ webs that we call Kuperberg algebra.","Finally, we formulate a precise conjecture relating the $W_3$ conformal blocks at $c=2$ to scaling limits of probabilities in the triple dimer model recently studied by Kenyon and Shi.","We verify the conjecture for explicit examples up to $d=6$. For more general central charges, we expect that $W_3$ conformal blocks are related to scaling limits of probabilities in lattice models based on $\\mathfrak{sl}_3$ webs."],"url":"http://arxiv.org/abs/2402.12013v1","category":"math-ph"}
{"created":"2024-02-19 10:03:46","title":"Training Green AI Models Using Elite Samples","abstract":"The substantial increase in AI model training has considerable environmental implications, mandating more energy-efficient and sustainable AI practices. On the one hand, data-centric approaches show great potential towards training energy-efficient AI models. On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation. Despite the growing interest in both topics, the impact of data-centric training set selection on energy efficiency remains to date unexplored. This paper presents an evolutionary-based sampling framework aimed at (i) identifying elite training samples tailored for datasets and model pairs, (ii) comparing model performance and energy efficiency gains against typical model training practice, and (iii) investigating the feasibility of this framework for fostering sustainable model training practices. To evaluate the proposed framework, we conducted an empirical experiment including 8 commonly used AI classification models and 25 publicly available datasets. The results showcase that by considering 10% elite training samples, the models' performance can show a 50% improvement and remarkable energy savings of 98% compared to the common training practice.","sentences":["The substantial increase in AI model training has considerable environmental implications, mandating more energy-efficient and sustainable AI practices.","On the one hand, data-centric approaches show great potential towards training energy-efficient AI models.","On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation.","Despite the growing interest in both topics, the impact of data-centric training set selection on energy efficiency remains to date unexplored.","This paper presents an evolutionary-based sampling framework aimed at (i) identifying elite training samples tailored for datasets and model pairs, (ii) comparing model performance and energy efficiency gains against typical model training practice, and (iii) investigating the feasibility of this framework for fostering sustainable model training practices.","To evaluate the proposed framework, we conducted an empirical experiment including 8 commonly used AI classification models and 25 publicly available datasets.","The results showcase that by considering 10% elite training samples, the models' performance can show a 50% improvement and remarkable energy savings of 98% compared to the common training practice."],"url":"http://arxiv.org/abs/2402.12010v1","category":"cs.LG"}
{"created":"2024-02-19 10:02:00","title":"Cluster Metric Sensitivity to Irrelevant Features","abstract":"Clustering algorithms are used extensively in data analysis for data exploration and discovery. Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. This provides great opportunities in data analytics as the data can be interrogated for many different purposes. This however leads challenges, such as identification of relevant features for a given task. In supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). In unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. In this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. We show how different types of irrelevant variables can impact the outcome of a clustering result from $k$-means in different ways. We observe a resilience to very high proportions of irrelevant features for adjusted rand index (ARI) and normalised mutual information (NMI) when the irrelevant features are Gaussian distributed. For Uniformly distributed irrelevant features, we notice the resilience of ARI and NMI is dependent on the dimensionality of the data and exhibits tipping points between high scores and near zero. Our results show that the Silhouette Coefficient and the Davies-Bouldin score are the most sensitive to irrelevant added features exhibiting large changes in score for comparably low proportions of irrelevant features regardless of underlying distribution or data scaling. As such the Silhouette Coefficient and the Davies-Bouldin score are good candidates for optimising feature selection in unsupervised clustering tasks.","sentences":["Clustering algorithms are used extensively in data analysis for data exploration and discovery.","Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity.","This provides great opportunities in data analytics as the data can be interrogated for many different purposes.","This however leads challenges, such as identification of relevant features for a given task.","In supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy).","In unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks.","In this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters.","We show how different types of irrelevant variables can impact the outcome of a clustering result from $k$-means in different ways.","We observe a resilience to very high proportions of irrelevant features for adjusted rand index (ARI) and normalised mutual information (NMI) when the irrelevant features are Gaussian distributed.","For Uniformly distributed irrelevant features, we notice the resilience of ARI and NMI is dependent on the dimensionality of the data and exhibits tipping points between high scores and near zero.","Our results show that the Silhouette Coefficient and the Davies-Bouldin score are the most sensitive to irrelevant added features exhibiting large changes in score for comparably low proportions of irrelevant features regardless of underlying distribution or data scaling.","As such the Silhouette Coefficient and the Davies-Bouldin score are good candidates for optimising feature selection in unsupervised clustering tasks."],"url":"http://arxiv.org/abs/2402.12008v1","category":"cs.LG"}
{"created":"2024-02-19 09:59:53","title":"Terahertz control in a transmission electron microscope","abstract":"Ultrafast electron microscopy provides a movie-like access to structural dynamics of materials in space and time, but fundamental atomic motions or electron dynamics are, so far, too quick to be resolved. Here we report the all-optical control, compression and characterization of electron pulses in a transmission electron microscope by the single optical cycles of laser-generated terahertz light. This concept provides isolated electron pulses and merges the spatial resolution of a transmission electron microscope with the temporal resolution that is offered by a single cycle of laser light. Central to these achievements is a perforated parallel-plate metallic waveguide in which transverse velocity mismatch and magnetic forces are mitigated by electrically constructive and magnetically destructive interferences of incoming and reflected terahertz half-cycles from a displaced waveguide termination. Measurements of spatial chirp via energy-filtered imaging reveal flat pulses with no transversal deflection or temporal aberrations at the specimen. We also report the all-optical control of multi-electron states and discover a substantial two-electron and three-electron anti-correlation in the time domain. These results open up the possibility to visualize atomic and electronic motions together with their quantum correlations on fundamental dimensions in space and time.","sentences":["Ultrafast electron microscopy provides a movie-like access to structural dynamics of materials in space and time, but fundamental atomic motions or electron dynamics are, so far, too quick to be resolved.","Here we report the all-optical control, compression and characterization of electron pulses in a transmission electron microscope by the single optical cycles of laser-generated terahertz light.","This concept provides isolated electron pulses and merges the spatial resolution of a transmission electron microscope with the temporal resolution that is offered by a single cycle of laser light.","Central to these achievements is a perforated parallel-plate metallic waveguide in which transverse velocity mismatch and magnetic forces are mitigated by electrically constructive and magnetically destructive interferences of incoming and reflected terahertz half-cycles from a displaced waveguide termination.","Measurements of spatial chirp via energy-filtered imaging reveal flat pulses with no transversal deflection or temporal aberrations at the specimen.","We also report the all-optical control of multi-electron states and discover a substantial two-electron and three-electron anti-correlation in the time domain.","These results open up the possibility to visualize atomic and electronic motions together with their quantum correlations on fundamental dimensions in space and time."],"url":"http://arxiv.org/abs/2402.12007v1","category":"physics.optics"}
{"created":"2024-02-19 09:52:41","title":"Direct Consistency Optimization for Compositional Text-to-Image Personalization","abstract":"Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \\emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( https://dco-t2i.github.io/).","sentences":["Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency.","However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models.","To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model.","We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency.","Our method, dubbed \\emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models.","Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity.","Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment.","We show the efficacy of the proposed method on the T2I personalization for subject, style, or both.","In particular, our method results in a superior Pareto frontier to the baselines.","Generated examples and codes are in our project page( https://dco-t2i.github.io/)."],"url":"http://arxiv.org/abs/2402.12004v1","category":"cs.CV"}
{"created":"2024-02-19 09:49:53","title":"A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions","abstract":"With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task. Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review.","sentences":["With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task.","Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks.","In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies.","Future directions are also laid out based on our extensive and comparative review."],"url":"http://arxiv.org/abs/2402.12001v1","category":"cs.AI"}
{"created":"2024-02-19 09:46:17","title":"CV@R penalized portfolio optimization with biased stochastic mirror descent","abstract":"This article studies and solves the problem of optimal portfolio allocation with CV@R penalty when dealing with imperfectly simulated financial assets. We use a Stochastic biased Mirror Descent to find optimal resource allocation for a portfolio whose underlying assets cannot be generated exactly and may only be approximated with a numerical scheme that satisfies suitable error bounds, under a risk management constraint. We establish almost sure asymptotic properties as well as the rate of convergence for the averaged algorithm. We then focus on the optimal tuning of the overall procedure to obtain an optimized numerical cost. Our results are then illustrated numerically on simulated as well as real data sets.","sentences":["This article studies and solves the problem of optimal portfolio allocation with CV@R penalty when dealing with imperfectly simulated financial assets.","We use a Stochastic biased Mirror Descent to find optimal resource allocation for a portfolio whose underlying assets cannot be generated exactly and may only be approximated with a numerical scheme that satisfies suitable error bounds, under a risk management constraint.","We establish almost sure asymptotic properties as well as the rate of convergence for the averaged algorithm.","We then focus on the optimal tuning of the overall procedure to obtain an optimized numerical cost.","Our results are then illustrated numerically on simulated as well as real data sets."],"url":"http://arxiv.org/abs/2402.11999v1","category":"math.OC"}
{"created":"2024-02-19 09:44:19","title":"On photons and matter inversion spheres from complex super-spinars accretion structures","abstract":"Our analysis focus on the dragging effects on the accretion flows and jet emission in Kerr super-spinars. These attractors are characterized by peculiar accretion structures as double tori, or special dragged tori in the ergoregion, produced by the balance of the hydrodynamic and centrifugal forces and also effects of super-spinars repulsive gravity. We investigate the accretion flows, constituted by particles and photons, from toroids orbiting a central Kerr super-spinar. As results of our analysis, in both accretion and jet flows, properties characterizing these geometries, that constitute possible strong observational signatures or these attractors, are highlighted. We found that the flow is characterized by closed surfaces, defining inversion coronas (spherical shell), with null the particles flow toroidal velocity ($u^\\phi=0$) embedding the central singularity. We proved that this region distinguishes proto-jets and accretion driven flows, co-rotating and counter-rotating flows. Therefore in both cases the flow carries information about the accretion structures around the central attractor, demonstrating that inversion points can constitute an observational aspect capable of distinguishing the super-spinars.","sentences":["Our analysis focus on the dragging effects on the accretion flows and jet emission in Kerr super-spinars.","These attractors are characterized by peculiar accretion structures as double tori, or special dragged tori in the ergoregion, produced by the balance of the hydrodynamic and centrifugal forces and also effects of super-spinars repulsive gravity.","We investigate the accretion flows, constituted by particles and photons, from toroids orbiting a central Kerr super-spinar.","As results of our analysis, in both accretion and jet flows, properties characterizing these geometries, that constitute possible strong observational signatures or these attractors, are highlighted.","We found that the flow is characterized by closed surfaces, defining inversion coronas (spherical shell), with null the particles flow toroidal velocity ($u^\\phi=0$) embedding the central singularity.","We proved that this region distinguishes proto-jets and accretion driven flows, co-rotating and counter-rotating flows.","Therefore in both cases the flow carries information about the accretion structures around the central attractor, demonstrating that inversion points can constitute an observational aspect capable of distinguishing the super-spinars."],"url":"http://arxiv.org/abs/2402.11998v1","category":"astro-ph.HE"}
{"created":"2024-02-19 09:43:03","title":"Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models","abstract":"Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \\textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).","sentences":["Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited.","This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial.","This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \\textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities.","Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses.","Further, exploring various fine-tuning approaches yielded no major performance improvements.","The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN)."],"url":"http://arxiv.org/abs/2402.11997v1","category":"cs.CL"}
{"created":"2024-02-19 09:41:57","title":"ISCUTE: Instance Segmentation of Cables Using Text Embedding","abstract":"In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.","sentences":["In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes.","This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification.","In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly.","Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM).","We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation."],"url":"http://arxiv.org/abs/2402.11996v1","category":"cs.CV"}
{"created":"2024-02-19 09:38:55","title":"Towards Energy Efficient RAN: From Industry Standards to Trending Practice","abstract":"As 5G deployments continue throughout the world, concerns regarding its energy consumption have gained significant traction. This article focuses on radio access networks (RANs) which account for a major portion of the network energy use. Firstly, we introduce the state-of-the-art 3GPP and O-RAN standardization work on enhancing RAN energy efficiency. Then we highlight three unique ways for enabling energy optimization in telecommunication networks, including full stack acceleration, network functions consolidation, and shared infrastructure between communication and artificial intelligence. These network design strategies not only allow for considerable overall reduction in the energy footprint, but also deliver several added benefits including improved throughput, reduced cost of ownership, and increased revenue opportunities for telcos.","sentences":["As 5G deployments continue throughout the world, concerns regarding its energy consumption have gained significant traction.","This article focuses on radio access networks (RANs) which account for a major portion of the network energy use.","Firstly, we introduce the state-of-the-art 3GPP and O-RAN standardization work on enhancing RAN energy efficiency.","Then we highlight three unique ways for enabling energy optimization in telecommunication networks, including full stack acceleration, network functions consolidation, and shared infrastructure between communication and artificial intelligence.","These network design strategies not only allow for considerable overall reduction in the energy footprint, but also deliver several added benefits including improved throughput, reduced cost of ownership, and increased revenue opportunities for telcos."],"url":"http://arxiv.org/abs/2402.11993v1","category":"cs.NI"}
{"created":"2024-02-19 09:32:48","title":"Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models","abstract":"Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.","sentences":["Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss.","However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage.","To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA).","PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain.","However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation.","To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization.","Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images.","Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA."],"url":"http://arxiv.org/abs/2402.11989v1","category":"cs.LG"}
{"created":"2024-02-19 09:31:54","title":"Photonic Chiplet Interconnection via 3D-Nanoprinted Interposer","abstract":"Photonic integrated circuits utilize various waveguide materials, each excelling in specific metrics like efficient light emission, low propagation loss, high electro-optic efficiency, and potential for mass production. Inherent shortcomings in each platform push exploration of hybrid and heterogeneous integration, which demands specialized designs and extra fabrication processes for each material combination. Our work introduces a novel hybrid integration scheme employing a 3D-nanoprinted interposer for a photonic chiplet interconnection system. This method represents a generic solution that can readily couple between chips of any material system, with each fabricated on its own technology platform with no change in the established process flow for the individual chips. Mode-size engineering is enhanced by the off-chip parabolic micro-reflectors. The 3D-nanoprinted chip-coupling frame and fiber-guiding funnel enable low-loss, fully passive assembly with a fast-printing process achieving sub-micron accuracy. Mode-field-dimension conversion ratio of 5:2 from fiber to chip is demonstrated with <0.5dB excess loss on top of the 1.7dB inherent coupling loss, marking the largest mode size conversion using non-waveguided components. Additionally, our system demonstrates a 2.5dB die-to-die coupling loss between silicon and InP chips over a 140nm wavelength range (1480nm to 1620nm), showcasing the potential for extensive cross-platform integration by bridging different waveguide materials.","sentences":["Photonic integrated circuits utilize various waveguide materials, each excelling in specific metrics like efficient light emission, low propagation loss, high electro-optic efficiency, and potential for mass production.","Inherent shortcomings in each platform push exploration of hybrid and heterogeneous integration, which demands specialized designs and extra fabrication processes for each material combination.","Our work introduces a novel hybrid integration scheme employing a 3D-nanoprinted interposer for a photonic chiplet interconnection system.","This method represents a generic solution that can readily couple between chips of any material system, with each fabricated on its own technology platform with no change in the established process flow for the individual chips.","Mode-size engineering is enhanced by the off-chip parabolic micro-reflectors.","The 3D-nanoprinted chip-coupling frame and fiber-guiding funnel enable low-loss, fully passive assembly with a fast-printing process achieving sub-micron accuracy.","Mode-field-dimension conversion ratio of 5:2 from fiber to chip is demonstrated with <0.5dB excess loss on top of the 1.7dB inherent coupling loss, marking the largest mode size conversion using non-waveguided components.","Additionally, our system demonstrates a 2.5dB die-to-die coupling loss between silicon and InP chips over a 140nm wavelength range (1480nm to 1620nm), showcasing the potential for extensive cross-platform integration by bridging different waveguide materials."],"url":"http://arxiv.org/abs/2402.11988v1","category":"physics.optics"}
{"created":"2024-02-19 09:30:05","title":"Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling","abstract":"Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn","sentences":["Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision.","The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies).","In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module.","WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision.","We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images.","Code: https://github.com/philip-mueller/wsrpn"],"url":"http://arxiv.org/abs/2402.11985v1","category":"cs.CV"}
{"created":"2024-02-19 09:29:37","title":"Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks","abstract":"Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems.","sentences":["Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications.","However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting.","How could neuronal operations solve this problem is an important question for AI and neuroscience.","Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations.","Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing.","In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks.","We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection.","This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems.","Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces.","Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings.","Our method can pave a solid path for building continual neuromorphic computing systems."],"url":"http://arxiv.org/abs/2402.11984v1","category":"cs.NE"}
{"created":"2024-02-19 09:27:47","title":"Universal Generalization Guarantees for Wasserstein Distributionally Robust Models","abstract":"Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that robust models built from Wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality. However, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice. In contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth. For instance, our result applies to deep learning, without requiring restrictive assumptions. We achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results. Our approach is general enough to extend to the recent versions of Wasserstein/Sinkhorn distributionally robust problems that involve (double) regularizations.","sentences":["Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts.","Recent statistical analyses have proved that robust models built from Wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality.","However, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice.","In contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth.","For instance, our result applies to deep learning, without requiring restrictive assumptions.","We achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results.","Our approach is general enough to extend to the recent versions of Wasserstein/Sinkhorn distributionally robust problems that involve (double) regularizations."],"url":"http://arxiv.org/abs/2402.11981v1","category":"math.OC"}
{"created":"2024-02-19 09:21:22","title":"Measurement of the thermal accommodation coefficient of helium on a crystalline silicon surface at low-temperatures","abstract":"Next-generation gravitational wave observatories are expected to use cryogenically cooled, pendulum-suspended 200 kg test mass mirrors from a crystalline material such as crystalline silicon. During operation of the observatories, these mirrors undergo heating due to the absorption of laser radiation of up to a watt. Low noise cooling techniques need to be developed. Low-pressure helium exchange gas at 5 K might contribute to the challenging task. Here, we report the measurement of the helium accommodation coefficient $\\alpha(11\\,\\mathrm{K}<T< 30\\,\\mathrm{K})$, which is the probability that a helium atom thermalises with a surface at a given temperature, when reflected from it. We find $\\alpha(T) > 0.7$ for temperatures < 20 K, which increases the cooling power compared to recently used assumptions. The idea of free molecular flow helium gas cooling is thus supported and might find application in some observatory concepts.","sentences":["Next-generation gravitational wave observatories are expected to use cryogenically cooled, pendulum-suspended 200 kg test mass mirrors from a crystalline material such as crystalline silicon.","During operation of the observatories, these mirrors undergo heating due to the absorption of laser radiation of up to a watt.","Low noise cooling techniques need to be developed.","Low-pressure helium exchange gas at 5 K might contribute to the challenging task.","Here, we report the measurement of the helium accommodation coefficient $\\alpha(11\\,\\mathrm{K}<T< 30\\,\\mathrm{K})$, which is the probability that a helium atom thermalises with a surface at a given temperature, when reflected from it.","We find $\\alpha(T) > 0.7$ for temperatures < 20 K, which increases the cooling power compared to recently used assumptions.","The idea of free molecular flow helium gas cooling is thus supported and might find application in some observatory concepts."],"url":"http://arxiv.org/abs/2402.11977v1","category":"physics.ins-det"}
{"created":"2024-02-19 09:20:29","title":"Chaotic fields behave universally out of equilibrium","abstract":"Chaotic dynamics is always characterized by swarms of unstable trajectories, unpredictable individually, and thus generally studied statistically. It is often the case that such phase-space densities relax exponentially fast to a limiting distribution, that rules the long-time average of every observable of interest. Before that asymptotic timescale, the statistics of chaos is generally believed to depend on both the initial conditions and the chosen observable. I show that this is not the case for a widely applicable class of models, that feature a phase-space (`field') distribution common to all pushed-forward or integrated observables, while the system is still relaxing towards statistical equilibrium or a steady state. This universal profile is determined by both leading and first subleading eigenfunctions of the transport operator (Koopman or Perron-Frobenius) that maps phase-space densities forward or backwards in time.","sentences":["Chaotic dynamics is always characterized by swarms of unstable trajectories, unpredictable individually, and thus generally studied statistically.","It is often the case that such phase-space densities relax exponentially fast to a limiting distribution, that rules the long-time average of every observable of interest.","Before that asymptotic timescale, the statistics of chaos is generally believed to depend on both the initial conditions and the chosen observable.","I show that this is not the case for a widely applicable class of models, that feature a phase-space (`field') distribution common to all pushed-forward or integrated observables, while the system is still relaxing towards statistical equilibrium or a steady state.","This universal profile is determined by both leading and first subleading eigenfunctions of the transport operator (Koopman or Perron-Frobenius) that maps phase-space densities forward or backwards in time."],"url":"http://arxiv.org/abs/2402.11976v1","category":"nlin.CD"}
{"created":"2024-02-19 09:19:50","title":"Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations","abstract":"Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a ''One-for-All'' approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY's superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences. Our codes are available at https://github.com/nuochenpku/COMEDY.","sentences":["Existing retrieval-based methods have made significant strides in maintaining long-term conversations.","However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions.","This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases.","Instead, COMEDY adopts a ''One-for-All'' approach, utilizing a single language model to manage memory generation, compression, and response generation.","Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format.","To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions.","Comparative evaluations demonstrate COMEDY's superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences.","Our codes are available at https://github.com/nuochenpku/COMEDY."],"url":"http://arxiv.org/abs/2402.11975v1","category":"cs.CL"}
{"created":"2024-02-19 09:18:54","title":"How curved is a random complex curve?","abstract":"In this paper, we study the curvature properties of random complex plane curves. We bound from below the probability that a uniform proportion of the area of a random complex degree $d$ plane curve has a curvature smaller than $-d/8$. Our lower bound is uniform, in the sense that it does not depend on $d$. We also provide uniform upper bounds for similar probabilities. These results extend to random complex curves of projective surfaces equipped with an ample line bundle. This paper can be viewed as a sequel of [1], where other metric statistics were given. On a larger time scale, it joins the general program initiated in [11] of understanding random complex hypersurfaces of projective manifolds.","sentences":["In this paper, we study the curvature properties of random complex plane curves.","We bound from below the probability that a uniform proportion of the area of a random complex degree $d$ plane curve has a curvature smaller than $-d/8$. Our lower bound is uniform, in the sense that it does not depend on $d$. We also provide uniform upper bounds for similar probabilities.","These results extend to random complex curves of projective surfaces equipped with an ample line bundle.","This paper can be viewed as a sequel of [1], where other metric statistics were given.","On a larger time scale, it joins the general program initiated in [11] of understanding random complex hypersurfaces of projective manifolds."],"url":"http://arxiv.org/abs/2402.11972v1","category":"math.AG"}
{"created":"2024-02-19 09:07:28","title":"Node-Opening Construction of Maxfaces","abstract":"The node-opening technique developed by Traizet has been very useful in constructing minimal surfaces. In this paper, we use the technique to construct families of maximal immersions in Lorentz space that are embedded outside a compact set. Each family depends on a real parameter $t$. The surfaces look like horizontal planes connected by small necks that shrink to singular points as $t \\to 0$. The limit positions of the necks must satisfy a balance condition, which turns out to be exactly the same for maxfaces and for minimal surfaces. By simply comparing notes, we obtain a rich variety of new maxfaces with high genus and arbitrarily many space-like ends. Among them are the Lorentzian Costa--Hofmann--Meeks surfaces. Non-planar complete maximal immersions must have singularities. We will analyse the singularity structure. For sufficiently small non-zero $t$, the singular set consists of curves in the waist of every neck. In generic and some symmetric cases, we are able to prove that all but finitely many singularities are cuspidal edges, and the non-cuspidal singularities are swallowtails.","sentences":["The node-opening technique developed by Traizet has been very useful in constructing minimal surfaces.","In this paper, we use the technique to construct families of maximal immersions in Lorentz space that are embedded outside a compact set.","Each family depends on a real parameter $t$. The surfaces look like horizontal planes connected by small necks that shrink to singular points as $t \\to 0$.","The limit positions of the necks must satisfy a balance condition, which turns out to be exactly the same for maxfaces and for minimal surfaces.","By simply comparing notes, we obtain a rich variety of new maxfaces with high genus and arbitrarily many space-like ends.","Among them are the Lorentzian Costa--Hofmann--Meeks surfaces.","Non-planar complete maximal immersions must have singularities.","We will analyse the singularity structure.","For sufficiently small non-zero $t$, the singular set consists of curves in the waist of every neck.","In generic and some symmetric cases, we are able to prove that all but finitely many singularities are cuspidal edges, and the non-cuspidal singularities are swallowtails."],"url":"http://arxiv.org/abs/2402.11965v1","category":"math.DG"}
{"created":"2024-02-19 09:06:26","title":"Imbalance in Regression Datasets","abstract":"For classification, the problem of class imbalance is well known and has been extensively studied. In this paper, we argue that imbalance in regression is an equally important problem which has so far been overlooked: Due to under- and over-representations in a data set's target distribution, regressors are prone to degenerate to naive models, systematically neglecting uncommon training data and over-representing targets seen often during training. We analyse this problem theoretically and use resulting insights to develop a first definition of imbalance in regression, which we show to be a generalisation of the commonly employed imbalance measure in classification. With this, we hope to turn the spotlight on the overlooked problem of imbalance in regression and to provide common ground for future research.","sentences":["For classification, the problem of class imbalance is well known and has been extensively studied.","In this paper, we argue that imbalance in regression is an equally important problem which has so far been overlooked: Due to under- and over-representations in a data set's target distribution, regressors are prone to degenerate to naive models, systematically neglecting uncommon training data and over-representing targets seen often during training.","We analyse this problem theoretically and use resulting insights to develop a first definition of imbalance in regression, which we show to be a generalisation of the commonly employed imbalance measure in classification.","With this, we hope to turn the spotlight on the overlooked problem of imbalance in regression and to provide common ground for future research."],"url":"http://arxiv.org/abs/2402.11963v1","category":"cs.LG"}
{"created":"2024-02-19 09:05:34","title":"On the derivations and automorphisms of the algebra $k\\langle x, y\\rangle/(yx-xy-x^N)$","abstract":"We consider the algebra $A_N=k\\langle x, y\\rangle/(yx-xy-x^N)$, with $k$ a field of characteristic zero and $N$ a positive integer. Our main result is a complete description of the first Hochschild cohomology $\\operatorname{HH}^1(A_N)$ of $A_N$ that consists both of explicit derivations of $A_N$ whose cohomology classes span it and a description of its Lie algebra structure. As we do this, we compute the automorphism group of the algebra, as well as certain characteristic subgroups thereof related to locally nilpotent derivations, classify the finite groups that act on $A_N$ and, finally, show that there are no inner-faithful actions of generalized Taft Hopf algebras on $A_N$. We establish this last result thanks to another calculation of Hochschild cohomology, now with twisted coefficients.","sentences":["We consider the algebra $A_N=k\\langle x, y\\rangle/(yx-xy-x^N)$, with $k$ a field of characteristic zero and $N$ a positive integer.","Our main result is a complete description of the first Hochschild cohomology $\\operatorname{HH}^1(A_N)$ of $A_N$ that consists both of explicit derivations of $A_N$ whose cohomology classes span it and a description of its Lie algebra structure.","As we do this, we compute the automorphism group of the algebra, as well as certain characteristic subgroups thereof related to locally nilpotent derivations, classify the finite groups that act on $A_N$ and, finally, show that there are no inner-faithful actions of generalized Taft Hopf algebras on $A_N$. We establish this last result thanks to another calculation of Hochschild cohomology, now with twisted coefficients."],"url":"http://arxiv.org/abs/2402.11962v1","category":"math.KT"}
{"created":"2024-02-19 09:04:30","title":"DB-LLM: Accurate Dual-Binarization for Efficient LLMs","abstract":"Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while retaining the inherent high sparsity of ultra-low bit quantization. For the macro-level, we find the distortion that exists in the prediction of LLM after quantization, which is specified as the deviations related to the ambiguity of samples. We propose the Deviation-Aware Distillation (DAD) method, enabling the model to focus differently on various samples. Comprehensive experiments show that our DB-LLM not only significantly surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization (eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional 20\\% reduction in computational consumption compared to the SOTA method under the same bit-width. Our code will be released soon.","sentences":["Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment.","Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs.","However, existing ultra-low-bit quantization always causes severe accuracy drops.","In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM.","For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB).","By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while retaining the inherent high sparsity of ultra-low bit quantization.","For the macro-level, we find the distortion that exists in the prediction of LLM after quantization, which is specified as the deviations related to the ambiguity of samples.","We propose the Deviation-Aware Distillation (DAD) method, enabling the model to focus differently on various samples.","Comprehensive experiments show that our DB-LLM not only significantly surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization (eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional 20\\% reduction in computational consumption compared to the SOTA method under the same bit-width.","Our code will be released soon."],"url":"http://arxiv.org/abs/2402.11960v1","category":"cs.LG"}
{"created":"2024-02-19 08:52:12","title":"Analysis of Multidomain Abstractive Summarization Using Salience Allocation","abstract":"This paper explores the realm of abstractive text summarization through the lens of the SEASON (Salience Allocation as Guidance for Abstractive SummarizatiON) technique, a model designed to enhance summarization by leveraging salience allocation techniques. The study evaluates SEASON's efficacy by comparing it with prominent models like BART, PEGASUS, and ProphetNet, all fine-tuned for various text summarization tasks. The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these models fine-tuned for generating abstractive summaries. The analysis of these metrics offers a thorough insight into the strengths and weaknesses demonstrated by each model in summarizing news dataset, dialogue dataset and financial text dataset. The results presented in this paper not only contribute to the evaluation of the SEASON model's effectiveness but also illuminate the intricacies of salience allocation techniques across various types of datasets.","sentences":["This paper explores the realm of abstractive text summarization through the lens of the SEASON (Salience Allocation as Guidance for Abstractive SummarizatiON) technique, a model designed to enhance summarization by leveraging salience allocation techniques.","The study evaluates SEASON's efficacy by comparing it with prominent models like BART, PEGASUS, and ProphetNet, all fine-tuned for various text summarization tasks.","The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06.","This paper employs various evaluation metrics such as ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these models fine-tuned for generating abstractive summaries.","The analysis of these metrics offers a thorough insight into the strengths and weaknesses demonstrated by each model in summarizing news dataset, dialogue dataset and financial text dataset.","The results presented in this paper not only contribute to the evaluation of the SEASON model's effectiveness but also illuminate the intricacies of salience allocation techniques across various types of datasets."],"url":"http://arxiv.org/abs/2402.11955v1","category":"cs.CL"}
{"created":"2024-02-19 08:47:06","title":"Orthosymplectic $Z_2\\times Z_2$-graded Lie superalgebras and parastatistics","abstract":"A $Z_2\\times Z_2$-graded Lie superalgebra $g$ is a $Z_2\\times Z_2$-graded algebra with a bracket $[.,.]$ that satisfies certain graded versions of the symmetry and Jacobi identity. In particular, despite the common terminology, $g$ is not a Lie superalgebra. We construct the most general orthosymplectic $Z_2\\times Z_2$-graded Lie superalgebra $osp(2m_1+1,2m_2|2n_1,2n_2)$ in terms of defining matrices. A special case of this algebra appeared already in work of Tolstoy in 2014. Our construction is based on the notion of graded supertranspose for a $Z_2\\times Z_2$-graded matrix. Since the orthosymplectic Lie superalgebra $osp(2m+1|2n)$ is closely related to the definition of parabosons, parafermions and mixed parastatistics, we investigate here the new parastatistics relations following from $osp(2m_1+1,2m_2|2n_1,2n_2)$. Some special cases are of particular interest, even when one is dealing with parabosons only.","sentences":["A $Z_2\\times Z_2$-graded Lie superalgebra $g$ is a $Z_2\\times Z_2$-graded algebra with a bracket $[.,.]$ that satisfies certain graded versions of the symmetry and Jacobi identity.","In particular, despite the common terminology, $g$ is not a Lie superalgebra.","We construct the most general orthosymplectic $Z_2\\times Z_2$-graded Lie superalgebra $osp(2m_1+1,2m_2|2n_1,2n_2)$ in terms of defining matrices.","A special case of this algebra appeared already in work of Tolstoy in 2014.","Our construction is based on the notion of graded supertranspose for a $Z_2\\times Z_2$-graded matrix.","Since the orthosymplectic Lie superalgebra $osp(2m+1|2n)$ is closely related to the definition of parabosons, parafermions and mixed parastatistics, we investigate here the new parastatistics relations following from $osp(2m_1+1,2m_2|2n_1,2n_2)$. Some special cases are of particular interest, even when one is dealing with parabosons only."],"url":"http://arxiv.org/abs/2402.11952v1","category":"math-ph"}
{"created":"2024-02-19 08:46:04","title":"A novel molecule generative model of VAE combined with Transformer","abstract":"Recently, molecule generation using deep learning has been actively investigated in drug discovery. In this field, Transformer and VAE are widely used as powerful models, but they are rarely used in combination due to structural and performance mismatch of them. This study proposes a model that combines these two models through structural and parameter optimization in handling diverse molecules. The proposed model shows comparable performance to existing models in generating molecules, and showed by far superior performance in generating molecules with unseen structures. In addition, the proposed model successfully predicted molecular properties using the latent representation of VAE. Ablation studies suggested the advantage of VAE over other generative models like language model in generating novel molecules, and that the molecules can be described by ~32 dimensional variables, much smaller than existing descriptors and models. This study is expected to provide a virtual chemical library containing a wide variety of compounds for virtual screening and to enable efficient screening.","sentences":["Recently, molecule generation using deep learning has been actively investigated in drug discovery.","In this field, Transformer and VAE are widely used as powerful models, but they are rarely used in combination due to structural and performance mismatch of them.","This study proposes a model that combines these two models through structural and parameter optimization in handling diverse molecules.","The proposed model shows comparable performance to existing models in generating molecules, and showed by far superior performance in generating molecules with unseen structures.","In addition, the proposed model successfully predicted molecular properties using the latent representation of VAE.","Ablation studies suggested the advantage of VAE over other generative models like language model in generating novel molecules, and that the molecules can be described by ~32 dimensional variables, much smaller than existing descriptors and models.","This study is expected to provide a virtual chemical library containing a wide variety of compounds for virtual screening and to enable efficient screening."],"url":"http://arxiv.org/abs/2402.11950v1","category":"q-bio.BM"}
{"created":"2024-02-19 08:43:00","title":"Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model","abstract":"Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model. It leverages the dominant diagonal blocks in the generalized Gauss-Newton matrix based on the analysis of the Hessian matrix of LFA model and serves as an intermediary strategy bridging the gap between first-order and second-order optimization methods. Experiment results indicate that, with Mini-Hes, the LFA model outperforms several state-of-the-art models in addressing missing data estimation task on multiple real HDI datasets from recommender system. (The source code of Mini-Hes is available at https://github.com/Goallow/Mini-Hes)","sentences":["Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks.","Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors.","Latent factor analysis (LFA) model has proven to be effective in representing HDI data.","The performance of an LFA model relies heavily on its training process, which is a non-convex optimization.","It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods.","However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges.","To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.","It leverages the dominant diagonal blocks in the generalized Gauss-Newton matrix based on the analysis of the Hessian matrix of LFA model and serves as an intermediary strategy bridging the gap between first-order and second-order optimization methods.","Experiment results indicate that, with Mini-Hes, the LFA model outperforms several state-of-the-art models in addressing missing data estimation task on multiple real HDI datasets from recommender system.","(The source code of Mini-Hes is available at https://github.com/Goallow/Mini-Hes)"],"url":"http://arxiv.org/abs/2402.11948v1","category":"cs.LG"}
{"created":"2024-02-19 08:42:34","title":"$H_0$ tension as a manifestation of the time evolution of gravity coupling","abstract":"Using the effective theory of dark energy (EFT) we show that the time evolution of non minimal gravity coupling can provide a natural explanation to the apparent Hubble tension. The non minimal coupling induces a modification of the Einstein frame Friedman equation, which can explain the difference between low and high red-shift estimations of $H_0$. Since the EFT predicts the non minimal coupling to be only a function of time, tests of the equivalence principle are insensitive to it, if experiments are performed in regions of space-time where the time scale of the coupling time evolution is much larger than the experiment time scale. The effects of a time varying non minimal gravity coupling only manifest on sufficiently long time scales, such as in cosmological observations at different red-shift, and if ignored lead to apparent tensions in the values of cosmological parameters estimated with observations from different epochs of the Universe history.","sentences":["Using the effective theory of dark energy (EFT) we show that the time evolution of non minimal gravity coupling can provide a natural explanation to the apparent Hubble tension.","The non minimal coupling induces a modification of the Einstein frame Friedman equation, which can explain the difference between low and high red-shift estimations of $H_0$. Since the EFT predicts the non minimal coupling to be only a function of time, tests of the equivalence principle are insensitive to it, if experiments are performed in regions of space-time where the time scale of the coupling time evolution is much larger than the experiment time scale.","The effects of a time varying non minimal gravity coupling only manifest on sufficiently long time scales, such as in cosmological observations at different red-shift, and if ignored lead to apparent tensions in the values of cosmological parameters estimated with observations from different epochs of the Universe history."],"url":"http://arxiv.org/abs/2402.11947v1","category":"gr-qc"}
{"created":"2024-02-19 08:38:08","title":"Affine root systems, stable tubes and a conjecture by Geiss-Leclerc-Schr\u00f6er","abstract":"Associated to a symmetrisable Cartan matrix $C$, Geiss-Lerclerc-Schr\\\"{o}er constructed and studied a class of Iwanaga-Gorenstein algebras $H$. They proved a generalised version of Gabriel's Theorem, that is, the rank vectors of $\\tau$-locally free $H$-modules are the positive roots of type $C$ when $C$ is of finite type, and conjectured that this is true for any $C$. In this paper, we look into this conjecture when $C$ is of affine type. We construct explicitly stable tubes, some of which have rigid mouth modules, while others not. We deduce that any positive root of type $C$ is the rank vector of some $\\tau$-locally free $H$-module. However, the converse is not true in general. Our construction shows that there are $\\tau$-locally free $H$-modules whose rank vectors are not roots, when $C$ is of type $\\widetilde{\\mathbb{B}}_n$, $\\widetilde{\\mathbb{CD}}_n$, $\\widetilde{\\mathbb{F}}_{41}$ and $\\widetilde{\\mathbb{G}}_{21}$, and so the conjecture fails in these four types.","sentences":["Associated to a symmetrisable Cartan matrix $C$, Geiss-Lerclerc-Schr\\\"{o}er constructed and studied a class of Iwanaga-Gorenstein algebras $H$.","They proved a generalised version of Gabriel's Theorem, that is, the rank vectors of $\\tau$-locally free $H$-modules are the positive roots of type $C$ when $C$ is of finite type, and conjectured that this is true for any $C$.","In this paper, we look into this conjecture when $C$ is of affine type.","We construct explicitly stable tubes, some of which have rigid mouth modules, while others not.","We deduce that any positive root of type $C$ is the rank vector of some $\\tau$-locally free $H$-module.","However, the converse is not true in general.","Our construction shows that there are $\\tau$-locally free $H$-modules whose rank vectors are not roots, when $C$ is of type $\\widetilde{\\mathbb{B}}_n$, $\\widetilde{\\mathbb{CD}}_n$, $\\widetilde{\\mathbb{F}}_{41}$ and $\\widetilde{\\mathbb{G}}_{21}$, and so the conjecture fails in these four types."],"url":"http://arxiv.org/abs/2402.11946v1","category":"math.RT"}
{"created":"2024-02-19 08:30:06","title":"The effect of Leaky ReLUs on the training and generalization of overparameterized networks","abstract":"We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\\alpha$. We show that $\\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.","sentences":["We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions.","More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\\alpha$. We show that $\\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound.","Furthermore, in special settings, it is also optimal for the generalization error bound.","Numerical experiments empirically support the practical choices guided by the theory."],"url":"http://arxiv.org/abs/2402.11942v1","category":"cs.LG"}
{"created":"2024-02-19 08:26:52","title":"Parallel Program Analysis on Path Ranges","abstract":"Symbolic execution is a software verification technique symbolically running programs and thereby checking for bugs. Ranged symbolic execution performs symbolic execution on program parts, so called path ranges, in parallel. Due to the parallelism, verification is accelerated and hence scales to larger programs.   In this paper, we discuss a generalization of ranged symbolic execution to arbitrary program analyses. More specifically, we present a verification approach that splits programs into path ranges and then runs arbitrary analyses on the ranges in parallel. Our approach in particular allows to run different analyses on different program parts. We have implemented this generalization on top of the tool CPAchecker and evaluated it on programs from the SV-COMP benchmark. Our evaluation shows that verification can benefit from the parallelisation of the verification task, but also needs a form of work stealing (between analyses) as to become efficient","sentences":["Symbolic execution is a software verification technique symbolically running programs and thereby checking for bugs.","Ranged symbolic execution performs symbolic execution on program parts, so called path ranges, in parallel.","Due to the parallelism, verification is accelerated and hence scales to larger programs.   ","In this paper, we discuss a generalization of ranged symbolic execution to arbitrary program analyses.","More specifically, we present a verification approach that splits programs into path ranges and then runs arbitrary analyses on the ranges in parallel.","Our approach in particular allows to run different analyses on different program parts.","We have implemented this generalization on top of the tool CPAchecker and evaluated it on programs from the SV-COMP benchmark.","Our evaluation shows that verification can benefit from the parallelisation of the verification task, but also needs a form of work stealing (between analyses) as to become efficient"],"url":"http://arxiv.org/abs/2402.11938v1","category":"cs.SE"}
{"created":"2024-02-19 08:23:19","title":"Computing epsilon multiplicities in graded algebras","abstract":"This article investigates the computational aspects of the $\\varepsilon$-multiplicity. Primarily, we show that the $\\varepsilon$-multiplicity of a homogeneous ideal $I$ in a two-dimensional standard graded domain of finite type over an algebraically closed field of arbitrary characteristic, is always a rational number. In this situation, we produce a formula for the $\\varepsilon$-multiplicity of $I$ in terms of certain mixed multiplicities associated to $I$. In any dimension, under the assumptions that the saturated Rees algebra of $I$ is finitely generated, we give a different expression of the $\\varepsilon$-multiplicity in terms of mixed multiplicities by using the Veronese degree. This enabled us to make various explicit computations of $\\varepsilon$-multiplicities. We further write a Macaulay2 algorithm to compute $\\varepsilon$-multiplicity (under the Noetherian hypotheses) even when the base ring is not necessarily standard graded.","sentences":["This article investigates the computational aspects of the $\\varepsilon$-multiplicity.","Primarily, we show that the $\\varepsilon$-multiplicity of a homogeneous ideal $I$ in a two-dimensional standard graded domain of finite type over an algebraically closed field of arbitrary characteristic, is always a rational number.","In this situation, we produce a formula for the $\\varepsilon$-multiplicity of $I$ in terms of certain mixed multiplicities associated to $I$. In any dimension, under the assumptions that the saturated Rees algebra of $I$ is finitely generated, we give a different expression of the $\\varepsilon$-multiplicity in terms of mixed multiplicities by using the Veronese degree.","This enabled us to make various explicit computations of $\\varepsilon$-multiplicities.","We further write a Macaulay2 algorithm to compute $\\varepsilon$-multiplicity (under the Noetherian hypotheses) even when the base ring is not necessarily standard graded."],"url":"http://arxiv.org/abs/2402.11935v1","category":"math.AC"}
{"created":"2024-02-19 08:22:51","title":"Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text","abstract":"This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST","sentences":["This paper presents the participation of team QUST in Task 8 SemEval 2024.","We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy.","In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods.","Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B.","The final model construction employed a stacking ensemble that combined fine-tuning with MPU.","Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A.","We release our system code at:https://github.com/warmth27/SemEval2024_QUST"],"url":"http://arxiv.org/abs/2402.11934v1","category":"cs.CL"}
{"created":"2024-02-19 08:19:26","title":"SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning","abstract":"To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision.","sentences":["To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed.","While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams.","In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels.","In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels.","SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time.","To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones.","Failure in these tasks for a node signals its deviation from the norm.","Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t.","the graph size) in response to each new edge in the input stream.","In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision."],"url":"http://arxiv.org/abs/2402.11933v1","category":"cs.LG"}
{"created":"2024-02-19 08:17:21","title":"DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation","abstract":"This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.","sentences":["This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation.","While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting.","Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups.","To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting.","However, the scene geometry needed to produce the radiance hints is unknown.","Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction.","Based on this observation, we introduce a three stage method for controlling the lighting during image generation.","In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting.","Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image.","To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet.","Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object.","We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions."],"url":"http://arxiv.org/abs/2402.11929v1","category":"cs.CV"}
{"created":"2024-02-19 08:12:47","title":"Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching","abstract":"The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices. To this end, IoT devices require offloading their data to an edge server in proximity. However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge. To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques. The first one is data deepening, where each data sample's features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA). Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of transmitted data. The criteria to offload data are derived for binary and multi-class classifiers, which are designed based on support vector machine (SVM) and deep neural network (DNN), respectively. The second one is data prefetching, where some features potentially required in the future are offloaded in advance, thus achieving high efficiency via precise prediction and parameter optimization. We evaluate the effectiveness of JD2P through experiments using the MNIST dataset, and the results demonstrate its significant reduction in expected energy consumption compared to several benchmarks without degrading learning accuracy.","sentences":["The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices.","To this end, IoT devices require offloading their data to an edge server in proximity.","However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge.","To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques.","The first one is data deepening, where each data sample's features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA).","Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of transmitted data.","The criteria to offload data are derived for binary and multi-class classifiers, which are designed based on support vector machine (SVM) and deep neural network (DNN), respectively.","The second one is data prefetching, where some features potentially required in the future are offloaded in advance, thus achieving high efficiency via precise prediction and parameter optimization.","We evaluate the effectiveness of JD2P through experiments using the MNIST dataset, and the results demonstrate its significant reduction in expected energy consumption compared to several benchmarks without degrading learning accuracy."],"url":"http://arxiv.org/abs/2402.11925v1","category":"cs.LG"}
{"created":"2024-02-19 08:11:26","title":"A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning","abstract":"Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/PLUTO-SCY/GPDiff.","sentences":["Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions.","To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning.","Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities.","We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics.","GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models.","By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.","The implementation of our approach is available: https://github.com/PLUTO-SCY/GPDiff."],"url":"http://arxiv.org/abs/2402.11922v1","category":"cs.LG"}
{"created":"2024-02-19 08:10:35","title":"A Feasible Method for Constrained Derivative-Free Optimization","abstract":"This paper explores a method for solving constrained optimization problems when the derivatives of the objective function are unavailable, while the derivatives of the constraints are known. We allow the objective and constraint function to be nonconvex. The method constructs a quadratic model of the objective function via interpolation and computes a step by minimizing this model subject to the original constraints in the problem and a trust region constraint. The step computation requires the solution of a general nonlinear program, which is economically feasible when the constraints and their derivatives are very inexpensive to compute compared to the objective function. The paper includes a summary of numerical results that highlight the method's promising potential.","sentences":["This paper explores a method for solving constrained optimization problems when the derivatives of the objective function are unavailable, while the derivatives of the constraints are known.","We allow the objective and constraint function to be nonconvex.","The method constructs a quadratic model of the objective function via interpolation and computes a step by minimizing this model subject to the original constraints in the problem and a trust region constraint.","The step computation requires the solution of a general nonlinear program, which is economically feasible when the constraints and their derivatives are very inexpensive to compute compared to the objective function.","The paper includes a summary of numerical results that highlight the method's promising potential."],"url":"http://arxiv.org/abs/2402.11920v1","category":"math.OC"}
{"created":"2024-02-19 07:59:37","title":"Optimal Pseudorandom Generators for Low-Degree Polynomials Over Moderately Large Fields","abstract":"We construct explicit pseudorandom generators that fool $n$-variate polynomials of degree at most $d$ over a finite field $\\mathbb{F}_q$. The seed length of our generators is $O(d \\log n + \\log q)$, over fields of size exponential in $d$ and characteristic at least $d(d-1)+1$. Previous constructions such as Bogdanov's (STOC 2005) and Derksen and Viola's (FOCS 2022) had either suboptimal seed length or required the field size to depend on $n$.   Our approach follows Bogdanov's paradigm while incorporating techniques from Lecerf's factorization algorithm (J. Symb. Comput. 2007) and insights from the construction of Derksen and Viola regarding the role of indecomposability of polynomials.","sentences":["We construct explicit pseudorandom generators that fool $n$-variate polynomials of degree at most $d$ over a finite field $\\mathbb{F}_q$. The seed length of our generators is $O(d \\log n + \\log q)$, over fields of size exponential in $d$ and characteristic at least $d(d-1)+1$. Previous constructions such as Bogdanov's (STOC 2005) and Derksen and Viola's (FOCS 2022) had either suboptimal seed length or required the field size to depend on $n$.   Our approach follows Bogdanov's paradigm while incorporating techniques from Lecerf's factorization algorithm (J. Symb.","Comput.","2007) and insights from the construction of Derksen and Viola regarding the role of indecomposability of polynomials."],"url":"http://arxiv.org/abs/2402.11915v1","category":"cs.CC"}
{"created":"2024-02-19 07:59:20","title":"Dynamics of spining particles in a homogeneous space with rotation as possible mechanism of the inertial mass formation and interpretation of quantum uncertainty principle in general relativity","abstract":"The dynamics of particles with intrinsic angular momentum (spin) described by the Dirac equation is considered in a homogeneous space with rotation in the presence of a homogeneous vortex gravitational field. The effects of the interaction between the spin of Dirac particles and the vortex gravitational field, as well as a possible mechanism for the appearance of an inertial mass, which is inextricably linked, as shown, with the spin of particles, are revealed. A geometric interpretation of the uncertainty principle is given.","sentences":["The dynamics of particles with intrinsic angular momentum (spin) described by the Dirac equation is considered in a homogeneous space with rotation in the presence of a homogeneous vortex gravitational field.","The effects of the interaction between the spin of Dirac particles and the vortex gravitational field, as well as a possible mechanism for the appearance of an inertial mass, which is inextricably linked, as shown, with the spin of particles, are revealed.","A geometric interpretation of the uncertainty principle is given."],"url":"http://arxiv.org/abs/2402.11914v1","category":"gr-qc"}
{"created":"2024-02-19 18:58:18","title":"HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools","abstract":"With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training. Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types. Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting. HunFlair2 showed the best performance on average, being closely followed by PubTator. Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust.","sentences":["With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications.","Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents.","However, tools for these two steps are rarely applied in the same context in which they were developed.","Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type.","This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications.","Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training.","Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types.","Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting.","HunFlair2 showed the best performance on average, being closely followed by PubTator.","Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust."],"url":"http://arxiv.org/abs/2402.12372v1","category":"cs.CL"}
{"created":"2024-02-19 18:56:35","title":"Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks","abstract":"The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent populations, one of short-period main sequence binaries and another of Delta Scuti stars. Our neural network model and related code is additionally provided as open-source code for public use and extension.","sentences":["The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets.","To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort.","We present a convolutional neural network that we train to identify short period variables.","To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods.","Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches.","We present a collection of 14156 short-period variables identified by our network.","The majority of our identified variables fall into two prominent populations, one of short-period main sequence binaries and another of Delta Scuti stars.","Our neural network model and related code is additionally provided as open-source code for public use and extension."],"url":"http://arxiv.org/abs/2402.12369v1","category":"astro-ph.SR"}
{"created":"2024-02-19 17:27:04","title":"UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking","abstract":"Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack","sentences":["Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods.","The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty.","This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT.","While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking.","We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors.","Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\\% and improves mMOTA by 2-3%.","The source code is available at https://github.com/TRAILab/UncertaintyTrack"],"url":"http://arxiv.org/abs/2402.12303v1","category":"cs.CV"}
{"created":"2024-02-19 17:05:29","title":"KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students","abstract":"Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based on 27 learners and 32 6-day study trajectories, KARL shows the ability to enhance medium-term educational learning, proving its efficacy for scheduling.","sentences":["Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions.","Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards.","Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies.","To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions.","To test KARL, we collect a new dataset of diverse study history on trivia questions.","KARL bests existing student models in AUC and calibration error.","Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online.","Based on 27 learners and 32 6-day study trajectories, KARL shows the ability to enhance medium-term educational learning, proving its efficacy for scheduling."],"url":"http://arxiv.org/abs/2402.12291v1","category":"cs.CL"}
{"created":"2024-02-19 14:09:58","title":"Angularly Sparse Channel Estimation in Dual-Wideband Tera-Hertz (THz) Hybrid MIMO Systems Relying on Bayesian Learning","abstract":"Bayesian learning aided massive antenna array based THz MIMO systems are designed for spatial-wideband and frequency-wideband scenarios, collectively termed as the dual-wideband channels. Essentially, numerous antenna modules of the THz system result in a significant delay in the transmission/ reception of signals in the time-domain across the antennas, which leads to spatial-selectivity. As a further phenomenon, the wide bandwidth of THz communication results in substantial variation of the effective angle of arrival/ departure (AoA/ AoD) with respect to the subcarrier frequency. This is termed as the beam squint effect, which renders the channel state information (CSI) estimation challenging in such systems. To address this problem, initially, a pilot-aided (PA) Bayesian learning (PA-BL) framework is derived for the estimation of the Terahertz (THz) MIMO channel that relies exclusively on the pilot beams transmitted. Since the framework designed can successfully operate in an ill-posed model, it can verifiably lead to reduced pilot transmissions in comparison to conventional methodologies. The above paradigm is subsequently extended to additionally incorporate data symbols to derive a Data-Aided (DA) BL approach that performs joint data detection and CSI estimation. We will demonstrate that it is capable of improving the dual-wideband channels estimate, despite further reducing the training overhead. The Bayesian Cramer-Rao bounds (BCRLBs) are also obtained for explicitly characterizing the lower bounds on the mean squared error (MSE) of the PA-BL and DA-BL frameworks. Our simulation results show the improved normalized MSE (NMSE) and bit-error rate (BER) performance of the proposed estimation schemes and confirm that they approach their respective BCRLB benchmarks.","sentences":["Bayesian learning aided massive antenna array based THz MIMO systems are designed for spatial-wideband and frequency-wideband scenarios, collectively termed as the dual-wideband channels.","Essentially, numerous antenna modules of the THz system result in a significant delay in the transmission/ reception of signals in the time-domain across the antennas, which leads to spatial-selectivity.","As a further phenomenon, the wide bandwidth of THz communication results in substantial variation of the effective angle of arrival/ departure (AoA/ AoD) with respect to the subcarrier frequency.","This is termed as the beam squint effect, which renders the channel state information (CSI) estimation challenging in such systems.","To address this problem, initially, a pilot-aided (PA) Bayesian learning (PA-BL) framework is derived for the estimation of the Terahertz (THz) MIMO channel that relies exclusively on the pilot beams transmitted.","Since the framework designed can successfully operate in an ill-posed model, it can verifiably lead to reduced pilot transmissions in comparison to conventional methodologies.","The above paradigm is subsequently extended to additionally incorporate data symbols to derive a Data-Aided (DA) BL approach that performs joint data detection and CSI estimation.","We will demonstrate that it is capable of improving the dual-wideband channels estimate, despite further reducing the training overhead.","The Bayesian Cramer-Rao bounds (BCRLBs) are also obtained for explicitly characterizing the lower bounds on the mean squared error (MSE) of the PA-BL and DA-BL frameworks.","Our simulation results show the improved normalized MSE (NMSE) and bit-error rate (BER) performance of the proposed estimation schemes and confirm that they approach their respective BCRLB benchmarks."],"url":"http://arxiv.org/abs/2402.12158v1","category":"eess.SP"}
{"created":"2024-02-19 14:04:14","title":"Cross talk of a large-scale depleted monolithic active pixel sensor (DMAPS) in 180 nm CMOS technology","abstract":"Monolithic pixel detectors combine readout electronics and sensor in a single entity of silicon, which simplifies the production procedure and lowers the material budget compared to conventional hybrid pixel detector concepts. Benefiting from the advances in commercial CMOS processes towards large biasing voltage capabilities and the increasing availability of high-resistivity substrates, depleted monolithic active pixel sensors (DMAPS) are able to cope with the high-rate and high-radiation environments faced in modern high-energy physics experiments. TJ-Monopix2 is the latest iteration of a DMAPS development line designed in 180 nm TowerSemicondutor technology, which features a large scale (2 x 2) cm$^2$ chip divided into (512 x 512) pixels with a pitch of (33 x 33) um$^2$. All in-pixel electronics are separated from its small collection electrode and process modifications are implemented to improve charge collection efficiency especially after irradiation. The latest laboratory measurements and investigations of a threshold variation observed for TJ-Monopix2 in typical operating conditions are presented.","sentences":["Monolithic pixel detectors combine readout electronics and sensor in a single entity of silicon, which simplifies the production procedure and lowers the material budget compared to conventional hybrid pixel detector concepts.","Benefiting from the advances in commercial CMOS processes towards large biasing voltage capabilities and the increasing availability of high-resistivity substrates, depleted monolithic active pixel sensors (DMAPS) are able to cope with the high-rate and high-radiation environments faced in modern high-energy physics experiments.","TJ-Monopix2 is the latest iteration of a DMAPS development line designed in 180 nm TowerSemicondutor technology, which features a large scale (2 x 2) cm$^2$ chip divided into (512 x 512) pixels with a pitch of (33 x 33)","um$^2$. All in-pixel electronics are separated from its small collection electrode and process modifications are implemented to improve charge collection efficiency especially after irradiation.","The latest laboratory measurements and investigations of a threshold variation observed for TJ-Monopix2 in typical operating conditions are presented."],"url":"http://arxiv.org/abs/2402.12153v1","category":"physics.ins-det"}
{"created":"2024-02-19 12:55:53","title":"Precision mass measurements at the JYFLTRAP double Penning trap pin down the mass surface across the neutron midshell at $N=66$","abstract":"Precision mass measurements of $^{104}$Y, $^{106}$Zr, $^{104,104m,109}$Nb, and $^{111,112}$Mo have been performed with the JYFLTRAP double Penning trap mass spectrometer at the Ion Guide Isotope Separator On-Line facility. The trend in two-neutron separation energies around the $N=66$ neutron midshell appeared to be steeper with respect to the Atomic Mass Evaluation 2020 extrapolations for the $_{39}$Y and $_{40}$Zr isotopic chains and less steep for the $_{41}$Nb chain, indicating a possible gap opening around $Z=40$. The experimental results were compared to the BSkG2 model calculations performed with and without vibrational and rotational corrections. All of them predict two low-lying minima for $^{106}$Zr. While the unaltered BSkG2 model fails to predict the trend in two-neutron separation energies, selecting the more deformed minima in calculations and removing the vibrational correction, the calculations are more in line with experimental data. The same is also true for the $2^+_1$ excitation energies and differences in charge radii in the Zr isotopes. The results stress the importance of improved treatment of collective corrections in large-scale models and further development of beyond-mean-field techniques.","sentences":["Precision mass measurements of $^{104}$Y, $^{106}$Zr, $^{104,104m,109}$Nb, and $^{111,112}$Mo have been performed with the JYFLTRAP double Penning trap mass spectrometer at the Ion Guide Isotope Separator On-Line facility.","The trend in two-neutron separation energies around the $N=66$ neutron midshell appeared to be steeper with respect to the Atomic Mass Evaluation 2020 extrapolations for the $_{39}$Y and $_{40}$Zr isotopic chains and less steep for the $_{41}$Nb chain, indicating a possible gap opening around $Z=40$.","The experimental results were compared to the BSkG2 model calculations performed with and without vibrational and rotational corrections.","All of them predict two low-lying minima for $^{106}$Zr.","While the unaltered BSkG2 model fails to predict the trend in two-neutron separation energies, selecting the more deformed minima in calculations and removing the vibrational correction, the calculations are more in line with experimental data.","The same is also true for the $2^+_1$ excitation energies and differences in charge radii in the Zr isotopes.","The results stress the importance of improved treatment of collective corrections in large-scale models and further development of beyond-mean-field techniques."],"url":"http://arxiv.org/abs/2402.12107v1","category":"nucl-ex"}
{"created":"2024-02-19 12:26:16","title":"Holographic dual effective field theory for an SYK model","abstract":"We derive an emergent holographic dual description for an SYK model, where the renormalization group (RG) flows of collective bi-local fields appear manifestly in the bulk effective action with an emergent extradimension. This holographic dual effective field theory reproduces $1/N$ quantum corrections given by the Schwarzian action when we take the UV limit in the bulk effective action. Going into the IR regime in the extradimension, we observe that the field theoretic $1/N$, $1/N^{2}$, ... quantum corrections are resummed in the all-loop order and reorganized to form a holographic dual effective field theory in a large $N$ fashion living on the one-higher dimensional spacetime. Taking the large $N$ limit in the holographic dual effective field theory, we obtain nonlinearly coupled second-order bulk differential equations of motion for the three bi-local order-parameter fields of fermion self-energy, Green's function, and polarization function. Here, both UV and IR boundary conditions are derived self-consistently from the boundary effective action. We solve these highly intertwined nonlinear differential equations based on the so called matching method. Our ansatz for the bi-local order-parameter fields coincide with the conformally invariant solution of the field theoretic large $N$ limit in the UV limit, but their overall coefficients RG-flow along the extradimensional space, respectively, reflecting effects of higher-order quantum corrections. As a result, we find an insulating behavior, where the self-energy diverges at IR. To confirm this insulating physics, we investigate thermodynamics. We obtain an effective free energy functional in terms of such bi-local dual order-parameter fields, which satisfy the Hamilton-Jacobi equation of the holographic dual effective field theory. ...","sentences":["We derive an emergent holographic dual description for an SYK model, where the renormalization group (RG) flows of collective bi-local fields appear manifestly in the bulk effective action with an emergent extradimension.","This holographic dual effective field theory reproduces $1/N$ quantum corrections given by the Schwarzian action when we take the UV limit in the bulk effective action.","Going into the IR regime in the extradimension, we observe that the field theoretic $1/N$, $1/N^{2}$, ...","quantum corrections are resummed in the all-loop order and reorganized to form a holographic dual effective field theory in a large $N$ fashion living on the one-higher dimensional spacetime.","Taking the large $N$ limit in the holographic dual effective field theory, we obtain nonlinearly coupled second-order bulk differential equations of motion for the three bi-local order-parameter fields of fermion self-energy, Green's function, and polarization function.","Here, both UV and IR boundary conditions are derived self-consistently from the boundary effective action.","We solve these highly intertwined nonlinear differential equations based on the so called matching method.","Our ansatz for the bi-local order-parameter fields coincide with the conformally invariant solution of the field theoretic large $N$ limit in the UV limit, but their overall coefficients RG-flow along the extradimensional space, respectively, reflecting effects of higher-order quantum corrections.","As a result, we find an insulating behavior, where the self-energy diverges at IR.","To confirm this insulating physics, we investigate thermodynamics.","We obtain an effective free energy functional in terms of such bi-local dual order-parameter fields, which satisfy the Hamilton-Jacobi equation of the holographic dual effective field theory. ..."],"url":"http://arxiv.org/abs/2402.12097v1","category":"hep-th"}
{"created":"2024-02-19 12:23:39","title":"Major TOM: Expandable Datasets for Earth Observation","abstract":"Deep learning models are increasingly data-hungry, requiring significant resources to collect and compile the datasets needed to train them, with Earth Observation (EO) models being no exception. However, the landscape of datasets in EO is relatively atomised, with interoperability made difficult by diverse formats and data structures. If ever larger datasets are to be built, and duplication of effort minimised, then a shared framework that allows users to combine and access multiple datasets is needed. Here, Major TOM (Terrestrial Observation Metaset) is proposed as this extensible framework. Primarily, it consists of a geographical indexing system based on a set of grid points and a metadata structure that allows multiple datasets with different sources to be merged. Besides the specification of Major TOM as a framework, this work also presents a large, open-access dataset, MajorTOM-Core, which covers the vast majority of the Earth's land surface. This dataset provides the community with both an immediately useful resource, as well as acting as a template for future additions to the Major TOM ecosystem. Access: https://huggingface.co/Major-TOM","sentences":["Deep learning models are increasingly data-hungry, requiring significant resources to collect and compile the datasets needed to train them, with Earth Observation (EO) models being no exception.","However, the landscape of datasets in EO is relatively atomised, with interoperability made difficult by diverse formats and data structures.","If ever larger datasets are to be built, and duplication of effort minimised, then a shared framework that allows users to combine and access multiple datasets is needed.","Here, Major TOM (Terrestrial Observation Metaset) is proposed as this extensible framework.","Primarily, it consists of a geographical indexing system based on a set of grid points and a metadata structure that allows multiple datasets with different sources to be merged.","Besides the specification of Major TOM as a framework, this work also presents a large, open-access dataset, MajorTOM-Core, which covers the vast majority of the Earth's land surface.","This dataset provides the community with both an immediately useful resource, as well as acting as a template for future additions to the Major TOM ecosystem.","Access: https://huggingface.co/Major-TOM"],"url":"http://arxiv.org/abs/2402.12095v1","category":"cs.CV"}
{"created":"2024-02-19 11:30:58","title":"To test $R_{NLRs}~-~L_{O3}$ relation for narrow emission line regions of AGN through low redshift Type-2 AGN in SDSS","abstract":"Sizes of narrow emission line regions (NLRs) of AGN could be estimated by [O~{\\sc iii}] line luminosity $L_{O3}$ through the known $R_{NLRs}-L_{O3}$ empirical relations. Unfortunately, it is not convenient to test the $R_{NLRs}-L_{O3}$ empirical relations through structure properties of spatially resolved NLRs of large samples of AGN. In this manuscript, a method is proposed to test the $R_{NLRs}-L_{O3}^{\\sim0.25}$ empirical relations for AGN NLRs through SDSS Type-2 AGN having few orientation effects on NLRs sizes expected by AGN unified model, after considering sizes $R_{fib}$ of SDSS fiber covered regions. Comparing $R_{fib}$ and $R_{NLRs}$ estimated by $L_{O3}$, Type-2 AGN with $R_{fib}>R_{NLRs}$ (Sample-II) and with $R_{fib}<R_{NLRs}$ (Sample-I) should have different physical properties of NLRs. Accepted electron density gradients in AGN NLRs, statistically higher electron densities (traced by lower flux ratio $R_{S2}$ of [S~{\\sc ii}]$\\lambda6717$\\AA~ to [S~{\\sc ii}]$\\lambda6731$\\AA) could be expected for the Type-2 AGN in the Sample-I. Then, through the collected 1062 SDSS Type-2 AGN in the Sample-I and 3658 SDSS Type-2 AGN in the Sample-II, statistically lower $R_{S2}$ for the Type-2 AGN in the Sample-I can be confirmed with confidence level higher than 5$\\sigma$, even after considering necessary effects. Therefore, the results in this manuscript can provide strong clues to support that the reported $R_{NLRs}~\\propto~L_{O3}^{0.25}$ empirical relation is preferred to estimate NLRs sizes of SDSS AGN through SDSS fiber spectroscopic results, and also to support the commonly expected electron density gradients in AGN NLRs.","sentences":["Sizes of narrow emission line regions (NLRs) of AGN could be estimated by [O~{\\sc iii}] line luminosity $L_{O3}$ through the known $R_{NLRs}-L_{O3}$ empirical relations.","Unfortunately, it is not convenient to test the $R_{NLRs}-L_{O3}$ empirical relations through structure properties of spatially resolved NLRs of large samples of AGN.","In this manuscript, a method is proposed to test the $R_{NLRs}-L_{O3}^{\\sim0.25}$ empirical relations for AGN NLRs through SDSS Type-2 AGN having few orientation effects on NLRs sizes expected by AGN unified model, after considering sizes $R_{fib}$ of SDSS fiber covered regions.","Comparing $R_{fib}$ and $R_{NLRs}$ estimated by $L_{O3}$, Type-2 AGN with $R_{fib}>R_{NLRs}$ (Sample-II) and with $R_{fib}<R_{NLRs}$ (Sample-I) should have different physical properties of NLRs.","Accepted electron density gradients in AGN NLRs, statistically higher electron densities (traced by lower flux ratio $R_{S2}$ of [S~{\\sc ii}]$\\lambda6717$\\AA~ to [S~{\\sc ii}]$\\lambda6731$\\AA) could be expected for the Type-2 AGN in","the Sample-I. Then, through the collected 1062 SDSS Type-2 AGN in the Sample-I and 3658 SDSS Type-2 AGN in the Sample-II, statistically lower $R_{S2}$ for the Type-2 AGN in the Sample-I can be confirmed with confidence level higher than 5$\\sigma$, even after considering necessary effects.","Therefore, the results in this manuscript can provide strong clues to support that the reported $R_{NLRs}~\\propto~L_{O3}^{0.25}$ empirical relation is preferred to estimate NLRs sizes of SDSS AGN through SDSS fiber spectroscopic results, and also to support the commonly expected electron density gradients in AGN NLRs."],"url":"http://arxiv.org/abs/2402.12063v1","category":"astro-ph.GA"}
{"created":"2024-02-19 09:48:22","title":"Thinking Outside the Black Box: Insights from a Digital Exhibition in the Humanities","abstract":"One of the main goals of Open Science is to make research more reproducible. There is no consensus, however, on what exactly \"reproducibility\" is, as opposed for example to \"replicability\", and how it applies to different research fields. After a short review of the literature on reproducibility/replicability with a focus on the humanities, we describe how the creation of the digital twin of the temporary exhibition \"The Other Renaissance\" has been documented throughout, with different methods, but with constant attention to research transparency, openness and accountability. A careful documentation of the study design, data collection and analysis techniques helps reflect and make all possible influencing factors explicit, and is a fundamental tool for reliability and rigour and for opening the \"black box\" of research.","sentences":["One of the main goals of Open Science is to make research more reproducible.","There is no consensus, however, on what exactly \"reproducibility\" is, as opposed for example to \"replicability\", and how it applies to different research fields.","After a short review of the literature on reproducibility/replicability with a focus on the humanities, we describe how the creation of the digital twin of the temporary exhibition \"The Other Renaissance\" has been documented throughout, with different methods, but with constant attention to research transparency, openness and accountability.","A careful documentation of the study design, data collection and analysis techniques helps reflect and make all possible influencing factors explicit, and is a fundamental tool for reliability and rigour and for opening the \"black box\" of research."],"url":"http://arxiv.org/abs/2402.12000v1","category":"cs.DL"}
{"created":"2024-02-19 09:00:10","title":"Automatic Evaluation for Mental Health Counseling using LLMs","abstract":"High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming.   To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of LLMs as supervisory tools for psychotherapists. By integrating LLMs into the evaluation process, our approach offers a cost-effective and dependable means of assessing counseling quality, enhancing overall effectiveness.","sentences":["High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness.","However, obtaining professional evaluation for each counseling session is expensive and challenging.","Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming.   ","To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations.","We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory.","Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts.","This highlights the potential of LLMs as supervisory tools for psychotherapists.","By integrating LLMs into the evaluation process, our approach offers a cost-effective and dependable means of assessing counseling quality, enhancing overall effectiveness."],"url":"http://arxiv.org/abs/2402.11958v1","category":"cs.CL"}
{"created":"2024-02-19 07:46:25","title":"Non-monotonous shear rate dependence of dielectric relaxation frequency of a nematic liquid crystal revealed by rheo-dielectric spectroscopy","abstract":"Dielectric relaxation of materials provides important information on the polarisation dynamics at different time scales. We study the dielectric relaxation of a nematic liquid crystal under steady rotational shear and simultaneously measure the viscosity. The dielectric anisotropy of the nematic is positive and the applied field is parallel to the velocity gradient direction with a magnitude larger than the Freedericksz threshold field. The complex dielectric constant as well as the effective viscosity decreases rapidly with increasing shear rate. The dielectric relaxation frequency exhibits a non-monotonous shear rate dependence, first decreasing but beyond a critical shear rate increasing. Our experiments suggest the emergence of collective dipolar relaxation under the influence of the competing effects of hydrodynamic and dielectric torques.","sentences":["Dielectric relaxation of materials provides important information on the polarisation dynamics at different time scales.","We study the dielectric relaxation of a nematic liquid crystal under steady rotational shear and simultaneously measure the viscosity.","The dielectric anisotropy of the nematic is positive and the applied field is parallel to the velocity gradient direction with a magnitude larger than the Freedericksz threshold field.","The complex dielectric constant as well as the effective viscosity decreases rapidly with increasing shear rate.","The dielectric relaxation frequency exhibits a non-monotonous shear rate dependence, first decreasing but beyond a critical shear rate increasing.","Our experiments suggest the emergence of collective dipolar relaxation under the influence of the competing effects of hydrodynamic and dielectric torques."],"url":"http://arxiv.org/abs/2402.11906v1","category":"cond-mat.soft"}
{"created":"2024-02-19 07:38:57","title":"SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning","abstract":"Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurate solutions within polynomial loops. We evaluate the performance of SoLA on various datasets and empirically demonstrate its consistent outperformance against existing symbolic solvers (including Z3 and Kissat) and tool-learning methods in terms of efficiency in large-scale problem-solving.","sentences":["Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning.","While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions.","In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability.","In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution.","Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability.","The backdoor theory ensures that SoLA can obtain accurate solutions within polynomial loops.","We evaluate the performance of SoLA on various datasets and empirically demonstrate its consistent outperformance against existing symbolic solvers (including Z3 and Kissat) and tool-learning methods in terms of efficiency in large-scale problem-solving."],"url":"http://arxiv.org/abs/2402.11903v1","category":"cs.CL"}
{"created":"2024-02-19 07:35:49","title":"Real-World Planning with PDDL+ and Beyond","abstract":"Real-world applications of AI Planning often require a highly expressive modeling language to accurately capture important intricacies of target systems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the standardized modeling language for capturing such systems as planning domains. PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics, exogenous activity, and many other interesting features exhibited in realistic scenarios. However, the uptake in usage of PDDL+ has been slow and apprehensive, largely due to a general shortage of PDDL+ planning software, and rigid limitations of the few existing planners. To overcome this chasm, we present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity, and, most importantly, adaptability. The planner is designed to be effortlessly customizable to expand its capabilities well beyond the scope of PDDL+. As a result, Nyx can be tailored to virtually any potential real-world application requiring some form of AI Planning, paving the way for wider adoption of planning methods for solving real-world problems.","sentences":["Real-world applications of AI Planning often require a highly expressive modeling language to accurately capture important intricacies of target systems.","Hybrid systems are ubiquitous in the real-world, and PDDL+ is the standardized modeling language for capturing such systems as planning domains.","PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics, exogenous activity, and many other interesting features exhibited in realistic scenarios.","However, the uptake in usage of PDDL+ has been slow and apprehensive, largely due to a general shortage of PDDL+ planning software, and rigid limitations of the few existing planners.","To overcome this chasm, we present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity, and, most importantly, adaptability.","The planner is designed to be effortlessly customizable to expand its capabilities well beyond the scope of PDDL+.","As a result, Nyx can be tailored to virtually any potential real-world application requiring some form of AI Planning, paving the way for wider adoption of planning methods for solving real-world problems."],"url":"http://arxiv.org/abs/2402.11901v1","category":"cs.AI"}
{"created":"2024-02-19 07:30:36","title":"Automatic Radio Map Adaptation for Robust Localization with Dynamic Adversarial Learning","abstract":"Wireless fingerprint-based localization has become one of the most promising technologies for ubiquitous location-aware computing and intelligent location-based services. However, due to RF vulnerability to environmental dynamics over time, continuous radio map updates are time-consuming and infeasible, resulting in severe accuracy degradation. To address this issue, we propose a novel approach of robust localization with dynamic adversarial learning, known as DadLoc which realizes automatic radio map adaptation by incorporating multiple robust factors underlying RF fingerprints to learn the evolving feature representation with the complicated environmental dynamics. DadLoc performs a finer-grained distribution adaptation with the developed dynamic adversarial adaptation network and quantifies the contributions of both global and local distribution adaptation in a dynamics-adaptive manner. Furthermore, we adopt the strategy of prediction uncertainty suppression to conduct source-supervised training, target-unsupervised training, and source-target dynamic adversarial adaptation which can trade off the environment adaptability and the location discriminability of the learned deep representation for safe and effective feature transfer across different environments. With extensive experimental results, the satisfactory accuracy over other comparative schemes demonstrates that the proposed DanLoc can facilitate fingerprint-based localization for wide deployments.","sentences":["Wireless fingerprint-based localization has become one of the most promising technologies for ubiquitous location-aware computing and intelligent location-based services.","However, due to RF vulnerability to environmental dynamics over time, continuous radio map updates are time-consuming and infeasible, resulting in severe accuracy degradation.","To address this issue, we propose a novel approach of robust localization with dynamic adversarial learning, known as DadLoc which realizes automatic radio map adaptation by incorporating multiple robust factors underlying RF fingerprints to learn the evolving feature representation with the complicated environmental dynamics.","DadLoc performs a finer-grained distribution adaptation with the developed dynamic adversarial adaptation network and quantifies the contributions of both global and local distribution adaptation in a dynamics-adaptive manner.","Furthermore, we adopt the strategy of prediction uncertainty suppression to conduct source-supervised training, target-unsupervised training, and source-target dynamic adversarial adaptation which can trade off the environment adaptability and the location discriminability of the learned deep representation for safe and effective feature transfer across different environments.","With extensive experimental results, the satisfactory accuracy over other comparative schemes demonstrates that the proposed DanLoc can facilitate fingerprint-based localization for wide deployments."],"url":"http://arxiv.org/abs/2402.11898v1","category":"eess.SP"}
{"created":"2024-02-19 07:10:30","title":"Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint","abstract":"Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.","sentences":["Large language models internalize enormous parametric knowledge during pre-training.","Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks.","This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts.","In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them.","It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets.","Code is available."],"url":"http://arxiv.org/abs/2402.11893v1","category":"cs.AI"}
{"created":"2024-02-19 07:07:44","title":"Evaluating Program Repair with Semantic-Preserving Transformations: A Naturalness Assessment","abstract":"In this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of NPR. To achieve this, we conduct a two-stage human study, including (1) interviews with senior software developers to establish the first concrete criteria for assessing the naturalness of code transformations and (2) a survey involving 10 developers to assess the naturalness of 1178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings reveal that nearly 60% and 20% of these transformations are considered natural and unnatural with substantially high agreement among human annotators. Furthermore, the unnatural code transformations introduce a 25.2% false alarm rate on robustness of five well-known NPR systems. Additionally, the performance of the NPR systems drops notably when evaluated using natural transformations, i.e., a drop of up to 22.9% and 23.6% in terms of the numbers of correct and plausible patches generated by these systems. These results highlight the importance of robustness testing by considering naturalness of code transformations, which unveils true effectiveness of NPR systems. Finally, we conduct an exploration study on automating the assessment of naturalness of code transformations by deriving a new naturalness metric based on Cross-Entropy. Based on our naturalness metric, we can effectively assess naturalness for code transformations automatically with an AUC of 0.7.","sentences":["In this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of NPR.","To achieve this, we conduct a two-stage human study, including (1) interviews with senior software developers to establish the first concrete criteria for assessing the naturalness of code transformations and (2) a survey involving 10 developers to assess the naturalness of 1178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs.","Our findings reveal that nearly 60% and 20% of these transformations are considered natural and unnatural with substantially high agreement among human annotators.","Furthermore, the unnatural code transformations introduce a 25.2% false alarm rate on robustness of five well-known NPR systems.","Additionally, the performance of the NPR systems drops notably when evaluated using natural transformations, i.e., a drop of up to 22.9% and 23.6% in terms of the numbers of correct and plausible patches generated by these systems.","These results highlight the importance of robustness testing by considering naturalness of code transformations, which unveils true effectiveness of NPR systems.","Finally, we conduct an exploration study on automating the assessment of naturalness of code transformations by deriving a new naturalness metric based on Cross-Entropy.","Based on our naturalness metric, we can effectively assess naturalness for code transformations automatically with an AUC of 0.7."],"url":"http://arxiv.org/abs/2402.11892v1","category":"cs.SE"}
{"created":"2024-02-19 07:06:52","title":"FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation","abstract":"Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \\beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines.","sentences":["Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent.","With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses.","However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges.","To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks.","This dataset, derived from 16 sub-collections of the widely used \\beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements.","Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search.","We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison.","Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines."],"url":"http://arxiv.org/abs/2402.11891v1","category":"cs.IR"}
{"created":"2024-02-19 06:54:55","title":"The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth","abstract":"Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.","sentences":["Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation.","Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information.","Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support.","However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT.","This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers.","To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content.","To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input.","We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences.","We find that LLM responses are supportive and inclusive, outscoring humans.","However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice.","We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses.","Our annotated dataset is available for further research."],"url":"http://arxiv.org/abs/2402.11886v1","category":"cs.CL"}
{"created":"2024-02-19 06:46:16","title":"InMD-X: Large Language Models for Internal Medicine Doctors","abstract":"In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field. These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation. InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research. Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support. This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information. We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios.","sentences":["In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD).","InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field.","These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation.","InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research.","Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support.","This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information.","We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios."],"url":"http://arxiv.org/abs/2402.11883v1","category":"cs.CV"}
{"created":"2024-02-19 06:43:25","title":"NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization","abstract":"The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge. Providing a summary of the patient's progress is crucial, as it significantly influences future care and planning. Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary. Therefore, we propose \"NOTE\", which stands for \"Notable generation Of patient Text summaries through an Efficient approach based on direct preference optimization\". NOTE is based on Medical Information Mart for Intensive Care- III dataset and summarizes a single hospitalization of a patient. Patient events are sequentially combined and used to generate a discharge summary for each hospitalization. In the present circumstances, large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions. Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital. Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior performance. To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software. In the future, we will aim to deploy the software available for actual use by clinicians in hospital. NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient's journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency.","sentences":["The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge.","Providing a summary of the patient's progress is crucial, as it significantly influences future care and planning.","Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary.","Therefore, we propose \"NOTE\", which stands for \"Notable generation Of patient Text summaries through an Efficient approach based on direct preference optimization\".","NOTE is based on Medical Information Mart for Intensive Care- III dataset and summarizes a single hospitalization of a patient.","Patient events are sequentially combined and used to generate a discharge summary for each hospitalization.","In the present circumstances, large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions.","Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital.","Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior performance.","To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software.","In the future, we will aim to deploy the software available for actual use by clinicians in hospital.","NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient's journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency."],"url":"http://arxiv.org/abs/2402.11882v1","category":"cs.CV"}
{"created":"2024-02-19 06:33:51","title":"Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model","abstract":"Reinforcement learning has witnessed significant advancements, particularly with the emergence of model-based approaches. Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings. However, the extension of $Q$-learning to a model-based framework remains relatively unexplored. In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach. Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart.","sentences":["Reinforcement learning has witnessed significant advancements, particularly with the emergence of model-based approaches.","Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings.","However, the extension of $Q$-learning to a model-based framework remains relatively unexplored.","In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach.","Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart."],"url":"http://arxiv.org/abs/2402.11877v1","category":"cs.LG"}
{"created":"2024-02-19 06:28:21","title":"From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data","abstract":"Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relational representations include but go beyond classical, intuitive notions of high-level actions; and that the learned models allow planning algorithms to scale to tasks that were previously beyond the scope of planning without hand-crafted abstractions.","sentences":["Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems.","However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting.","Removing this dependency on human intuition is a highly active research area.   ","This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories.","The learned representations constitute auto-invented PDDL-like domain models.","Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relational representations include but go beyond classical, intuitive notions of high-level actions; and that the learned models allow planning algorithms to scale to tasks that were previously beyond the scope of planning without hand-crafted abstractions."],"url":"http://arxiv.org/abs/2402.11871v1","category":"cs.RO"}
{"created":"2024-02-19 06:26:17","title":"Cooperative Backscatter Communications with Reconfigurable Intelligent Surfaces: An APSK Approach","abstract":"In this paper, a novel amplitude phase shift keying (APSK) modulation scheme for cooperative backscatter communications aided by a reconfigurable intelligent surface (RIS-CBC) is presented, according to which the RIS is configured to modulate backscatter information onto unmodulated or PSK-modulated signals impinging on its surface via APSK. We consider both passive and active RISs, with the latter including an amplification unit at each reflecting element. In the passive (resp. active) RIS-CBC-APSK, backscatter information is conveyed through the number of RIS reflecting elements being on the ON state (resp. active mode) and their phase shift values. By using the optimal APSK constellation to ensure that reflected signals from the RIS undergo APSK modulation, a bit-mapping mechanism is presented. Assuming maximum-likelihood detection, we also present closed-form upper bounds for the symbol error rate (SER) performance for both passive and active RIS-CBC-APSK schemes over Rician fading channels. In addition, we devise a low-complexity detector that can achieve flexible trade-offs between performance and complexity. Finally, we extend RIS-CBC-APSK to multiple-input single-output scenarios and present an alternating optimization approach for the joint design of transmit beamforming and RIS reflection. Our extensive simulation results on the SER performance corroborate our conducted performance analysis and showcase the superiority of the proposed RIS-CBC-APSK schemes over the state-of-the-art RIS-CBC benchmarks.","sentences":["In this paper, a novel amplitude phase shift keying (APSK) modulation scheme for cooperative backscatter communications aided by a reconfigurable intelligent surface (RIS-CBC) is presented, according to which the RIS is configured to modulate backscatter information onto unmodulated or PSK-modulated signals impinging on its surface via APSK.","We consider both passive and active RISs, with the latter including an amplification unit at each reflecting element.","In the passive (resp.","active) RIS-CBC-APSK, backscatter information is conveyed through the number of RIS reflecting elements being on the ON state (resp.","active mode) and their phase shift values.","By using the optimal APSK constellation to ensure that reflected signals from the RIS undergo APSK modulation, a bit-mapping mechanism is presented.","Assuming maximum-likelihood detection, we also present closed-form upper bounds for the symbol error rate (SER) performance for both passive and active RIS-CBC-APSK schemes over Rician fading channels.","In addition, we devise a low-complexity detector that can achieve flexible trade-offs between performance and complexity.","Finally, we extend RIS-CBC-APSK to multiple-input single-output scenarios and present an alternating optimization approach for the joint design of transmit beamforming and RIS reflection.","Our extensive simulation results on the SER performance corroborate our conducted performance analysis and showcase the superiority of the proposed RIS-CBC-APSK schemes over the state-of-the-art RIS-CBC benchmarks."],"url":"http://arxiv.org/abs/2402.11870v1","category":"cs.IT"}
{"created":"2024-02-19 06:14:46","title":"Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic","abstract":"Our aim of this paper is to develop new map matching algorithms and to improve upon previous work. We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching. AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the usual boolean logic. Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes. Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods.","sentences":["Our aim of this paper is to develop new map matching algorithms and to improve upon previous work.","We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching.","AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the usual boolean logic.","Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes.","Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods."],"url":"http://arxiv.org/abs/2402.11866v1","category":"cs.CG"}
{"created":"2024-02-19 05:13:39","title":"WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection","abstract":"The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure.","sentences":["The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs.","The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security.","Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities.","To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications.","WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles.","2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models.","These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios.","Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure."],"url":"http://arxiv.org/abs/2402.11843v1","category":"cs.CV"}
{"created":"2024-02-19 05:13:22","title":"CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking","abstract":"Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models.","sentences":["Transformer based code models have impressive performance in many software engineering tasks.","However, their effectiveness degrades when symbols are missing or not informative.","The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols.","We propose a new method to pre-train general code models when symbols are lacking.","We observe that in such cases, programs degenerate to something written in a very primitive language.","We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models).","We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences.","In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others.","To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduce a new pre-training algorithm.","We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool.","We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification.","Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively.","It also substantially outperforms other general pre-training techniques of code understanding models."],"url":"http://arxiv.org/abs/2402.11842v1","category":"cs.SE"}
{"created":"2024-02-19 04:58:40","title":"DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications","abstract":"The creation of accurate virtual models of real-world objects is imperative to robotic simulations and applications such as computer vision, artificial intelligence, and machine learning. This paper documents the different methods employed for generating a database of mesh models of real-world objects. These methods address the tedious and time-intensive process of manually generating the models using CAD software. Essentially, DSLR/phone cameras were employed to acquire images of target objects. These images were processed using a photogrammetry software known as Meshroom to generate a dense surface reconstruction of the scene. The result produced by Meshroom was edited and simplified using MeshLab, a mesh-editing software to produce the final model. Based on the obtained models, this process was effective in modelling the geometry and texture of real-world objects with high fidelity. An active 3D scanner was also utilized to accelerate the process for large objects. All generated models and captured images are made available on the website of the project.","sentences":["The creation of accurate virtual models of real-world objects is imperative to robotic simulations and applications such as computer vision, artificial intelligence, and machine learning.","This paper documents the different methods employed for generating a database of mesh models of real-world objects.","These methods address the tedious and time-intensive process of manually generating the models using CAD software.","Essentially, DSLR/phone cameras were employed to acquire images of target objects.","These images were processed using a photogrammetry software known as Meshroom to generate a dense surface reconstruction of the scene.","The result produced by Meshroom was edited and simplified using MeshLab, a mesh-editing software to produce the final model.","Based on the obtained models, this process was effective in modelling the geometry and texture of real-world objects with high fidelity.","An active 3D scanner was also utilized to accelerate the process for large objects.","All generated models and captured images are made available on the website of the project."],"url":"http://arxiv.org/abs/2402.11836v1","category":"cs.RO"}
{"created":"2024-02-19 04:44:24","title":"Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced Tracking and Security Features","abstract":"This study focuses on the implementation of modern and intelligent logistics vehicles equipped with advanced tracking and security features. In response to the evolving landscape of logistics management, the proposed system integrates cutting edge technologies to enhance efficiency and ensure the security of the entire logistics process. The core component of this implementation is the incorporation of state-of-the art tracking mechanisms, enabling real-time monitoring of vehicle locations and movements. Furthermore, the system addresses the paramount concern of security by introducing advanced security measures. Through the utilization of sophisticated tracking technologies and security protocols, the proposed logistics vehicles aim to safeguard both customer and provider data. The implementation includes the integration of QR code concepts, creating a binary image system that conceals sensitive information and ensures access only to authorized users. In addition to tracking and security, the study delves into the realm of information mining, employing techniques such as classification, clustering, and recommendation to extract meaningful patterns from vast datasets. Collaborative filtering techniques are incorporated to enhance customer experience by recommending services based on user preferences and historical data. This abstract encapsulates the comprehensive approach of deploying modern logistics vehicles, emphasizing their intelligence through advanced tracking, robust security measures, and data-driven insights. The proposed system aims to revolutionize logistics management, providing a seamless and secure experience for both customers and service providers in the dynamic logistics landscape.","sentences":["This study focuses on the implementation of modern and intelligent logistics vehicles equipped with advanced tracking and security features.","In response to the evolving landscape of logistics management, the proposed system integrates cutting edge technologies to enhance efficiency and ensure the security of the entire logistics process.","The core component of this implementation is the incorporation of state-of-the art tracking mechanisms, enabling real-time monitoring of vehicle locations and movements.","Furthermore, the system addresses the paramount concern of security by introducing advanced security measures.","Through the utilization of sophisticated tracking technologies and security protocols, the proposed logistics vehicles aim to safeguard both customer and provider data.","The implementation includes the integration of QR code concepts, creating a binary image system that conceals sensitive information and ensures access only to authorized users.","In addition to tracking and security, the study delves into the realm of information mining, employing techniques such as classification, clustering, and recommendation to extract meaningful patterns from vast datasets.","Collaborative filtering techniques are incorporated to enhance customer experience by recommending services based on user preferences and historical data.","This abstract encapsulates the comprehensive approach of deploying modern logistics vehicles, emphasizing their intelligence through advanced tracking, robust security measures, and data-driven insights.","The proposed system aims to revolutionize logistics management, providing a seamless and secure experience for both customers and service providers in the dynamic logistics landscape."],"url":"http://arxiv.org/abs/2402.11829v1","category":"cs.CR"}
{"created":"2024-02-19 04:41:31","title":"Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search","abstract":"Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers' preferences as feedback. The resulting model achieves state-of-the-art performance on two recent conversational search benchmarks, significantly outperforming existing baselines, including GPT-3.5.","sentences":["Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context.","The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results.","To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems.","The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences.","Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations.","Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers' preferences as feedback.","The resulting model achieves state-of-the-art performance on two recent conversational search benchmarks, significantly outperforming existing baselines, including GPT-3.5."],"url":"http://arxiv.org/abs/2402.11827v1","category":"cs.IR"}
{"created":"2024-02-19 04:17:21","title":"Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages","abstract":"Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot methods and achieves comparable performance with models fully fine-tuned using thousands of examples. The World Wide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in Nepal, significantly reducing their operational burden, and ensuring that AI tools for conservation actually reach the communities that need them the most. NewsSerow has also been deployed for countries with other languages like Colombia.","sentences":["Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact.","Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English.","However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably.","In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages.","NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs).","Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot methods and achieves comparable performance with models fully fine-tuned using thousands of examples.","The World Wide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in Nepal, significantly reducing their operational burden, and ensuring that AI tools for conservation actually reach the communities that need them the most.","NewsSerow has also been deployed for countries with other languages like Colombia."],"url":"http://arxiv.org/abs/2402.11818v1","category":"cs.CL"}
{"created":"2024-02-19 04:11:34","title":"HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?","abstract":"This paper describes our system developed for SemEval-2024 Task 8, \"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection.\" Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a single base model can have comparable performance with the help of data augmentation and contrastive learning.","sentences":["This paper describes our system developed for SemEval-2024 Task 8, \"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection.\"","Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials.","A lot of systems have been developed to detect machine-generated text.","Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation.","In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants).","Our key finding is that even without an ensemble of multiple models, a single base model can have comparable performance with the help of data augmentation and contrastive learning."],"url":"http://arxiv.org/abs/2402.11815v1","category":"cs.CL"}
{"created":"2024-02-19 04:08:44","title":"An Empirical Evaluation of LLMs for Solving Offensive Security Challenges","abstract":"Capture The Flag (CTF) challenges are puzzles related to computer security scenarios. With the advent of large language models (LLMs), more and more CTF participants are using LLMs to understand and solve the challenges. However, so far no work has evaluated the effectiveness of LLMs in solving CTF challenges with a fully automated workflow. We develop two CTF-solving workflows, human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability to solve a selected set of CTF challenges, prompted with information about the question. We collect human contestants' results on the same set of questions, and find that LLMs achieve higher success rate than an average human participant. This work provides a comprehensive evaluation of the capability of LLMs in solving real world CTF challenges, from real competition to fully automated workflow. Our results provide references for applying LLMs in cybersecurity education and pave the way for systematic evaluation of offensive cybersecurity capabilities in LLMs.","sentences":["Capture The Flag (CTF) challenges are puzzles related to computer security scenarios.","With the advent of large language models (LLMs), more and more CTF participants are using LLMs to understand and solve the challenges.","However, so far no work has evaluated the effectiveness of LLMs in solving CTF challenges with a fully automated workflow.","We develop two CTF-solving workflows, human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability to solve a selected set of CTF challenges, prompted with information about the question.","We collect human contestants' results on the same set of questions, and find that LLMs achieve higher success rate than an average human participant.","This work provides a comprehensive evaluation of the capability of LLMs in solving real world CTF challenges, from real competition to fully automated workflow.","Our results provide references for applying LLMs in cybersecurity education and pave the way for systematic evaluation of offensive cybersecurity capabilities in LLMs."],"url":"http://arxiv.org/abs/2402.11814v1","category":"cs.CR"}
{"created":"2024-02-19 04:02:40","title":"A novel framework for adaptive stress testing of autonomous vehicles in highways","abstract":"Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the highway. The proposed framework is further integrated with a new driving model enabling us to create more realistic traffic scenarios capturing both the longitudinal and lateral movements of vehicles on the highway. In our experiment, we calibrate our model using real-world crash statistics involving automated vehicles in California, and then we analyze the characteristics of the AV and the framework. Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms other existing AST schemes. The study can help discover crash scenarios of AV that are unknown or absent in human driving, thereby enhancing the safety and trustworthiness of AV technology.","sentences":["Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance.","It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario.","In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario.","The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases.","To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the highway.","The proposed framework is further integrated with a new driving model enabling us to create more realistic traffic scenarios capturing both the longitudinal and lateral movements of vehicles on the highway.","In our experiment, we calibrate our model using real-world crash statistics involving automated vehicles in California, and then we analyze the characteristics of the AV and the framework.","Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms other existing AST schemes.","The study can help discover crash scenarios of AV that are unknown or absent in human driving, thereby enhancing the safety and trustworthiness of AV technology."],"url":"http://arxiv.org/abs/2402.11813v1","category":"cs.RO"}
{"created":"2024-02-19 03:56:44","title":"FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema","abstract":"In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference dataset is meticulously constructed using the optimal and suboptimal LLMs, undergoing rigorous cross-verification by human experts and analytical models. Applying the insights from the data with Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO schema across five public benchmarks. Codes, data and scripts are here: https://github.com/LuJunru/FIPO_Project.","sentences":["In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user.","Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO).","This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema.","The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content.","This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts.","The FIPO preference dataset is meticulously constructed using the optimal and suboptimal LLMs, undergoing rigorous cross-verification by human experts and analytical models.","Applying the insights from the data with Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO schema across five public benchmarks.","Codes, data and scripts are here: https://github.com/LuJunru/FIPO_Project."],"url":"http://arxiv.org/abs/2402.11811v1","category":"cs.CL"}
{"created":"2024-02-19 03:45:23","title":"Sensor Integration and Performance Optimizations for Mineral Exploration using Large-scale Hybrid Multirotor UAVs","abstract":"In this paper, the focus is on improving the efficiency and precision of mineral data collection using UAVs by addressing key challenges associated with sensor integration. These challenges include mitigating electromagnetic interference, reducing vibration noise, and ensuring consistent sensor performance during flight. The paper demonstrates how innovative approaches to these issues can significantly transform UAV-assisted mineral data collection. Through meticulous design, testing, and evaluation, the study presents experimental evidence of the efficacy of these methods in collecting mineral data via UAVs. The advancements achieved in this research enable the UAV platform to remain airborne up to 6$\\times$ longer than standard battery-powered multirotors, while still gathering high-quality mineral data. This leads to increased operational efficiency and reduced costs in UAV-based mineral data-gathering processes","sentences":["In this paper, the focus is on improving the efficiency and precision of mineral data collection using UAVs by addressing key challenges associated with sensor integration.","These challenges include mitigating electromagnetic interference, reducing vibration noise, and ensuring consistent sensor performance during flight.","The paper demonstrates how innovative approaches to these issues can significantly transform UAV-assisted mineral data collection.","Through meticulous design, testing, and evaluation, the study presents experimental evidence of the efficacy of these methods in collecting mineral data via UAVs.","The advancements achieved in this research enable the UAV platform to remain airborne up to 6$\\times$ longer than standard battery-powered multirotors, while still gathering high-quality mineral data.","This leads to increased operational efficiency and reduced costs in UAV-based mineral data-gathering processes"],"url":"http://arxiv.org/abs/2402.11810v1","category":"cs.RO"}
{"created":"2024-02-19 03:39:10","title":"Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding","abstract":"This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel \\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.","sentences":["This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters.","We propose \\textbf{S}mart \\textbf{P}arallel \\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs.","By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification.","This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens.","Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation.","Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality."],"url":"http://arxiv.org/abs/2402.11809v1","category":"cs.CL"}
{"created":"2024-02-19 03:21:19","title":"LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs","abstract":"Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios.","sentences":["Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications.","One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects.","In this paper, we attempt to address this challenge with Large Language Models (LLMs).","Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice.","On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training.","On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively.","Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios."],"url":"http://arxiv.org/abs/2402.11804v1","category":"cs.AI"}
{"created":"2024-02-19 03:08:02","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling","abstract":"Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \\emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the mixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing uniform boundedness of the iterates. As such, our proof may be of independent interest. Next, to mitigate the impact of the maximum delay on the convergence rate, we provide the first finite-time analysis of a delay-adaptive SA scheme under Markovian sampling. In particular, we show that the exponent of convergence of this scheme gets scaled down by $\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here, $\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the adaptive scheme requires no prior knowledge of the delay sequence for step-size tuning. Our theoretical findings shed light on the finite-time effects of delays for a broad class of algorithms, including TD learning, Q-learning, and stochastic gradient descent under Markovian sampling.","sentences":["Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling.","While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood.","In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \\emph{last iterate} to a ball around the SA operator's fixed point.","Notably, our bound is \\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the mixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing uniform boundedness of the iterates.","As such, our proof may be of independent interest.","Next, to mitigate the impact of the maximum delay on the convergence rate, we provide the first finite-time analysis of a delay-adaptive SA scheme under Markovian sampling.","In particular, we show that the exponent of convergence of this scheme gets scaled down by $\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here, $\\tau_{avg}$ denotes the average delay across all iterations.","Moreover, the adaptive scheme requires no prior knowledge of the delay sequence for step-size tuning.","Our theoretical findings shed light on the finite-time effects of delays for a broad class of algorithms, including TD learning, Q-learning, and stochastic gradient descent under Markovian sampling."],"url":"http://arxiv.org/abs/2402.11800v1","category":"cs.LG"}
{"created":"2024-02-19 03:00:37","title":"Suspended Magnetometer Survey for Mineral Data Acquisition with Vertical Take-off and Landing Fixed-wing Aircraft","abstract":"Multirotor Unmanned Aerial Vehicles (UAV)s have recently become an important instrument for collecting mineral data, enabling more effective and accurate geological investigations. This paper explores the difficulties in mounting high-sensitivity sensors on a UAV platform, including electromagnetic interference, payload dynamics, and maintaining stable sensor performance while in flight. It is highlighted how the specific solutions provided to deal with these problems have the potential to alter the collection of mineral data assisted by UAVs. The work also shows experimental findings that demonstrate the creative potential of these solutions in UAV-based mineral data collecting, leading to improvements in effective mineral exploration through careful design, testing, and assessment of these systems. These innovations resulted in a platform that is quickly deployable in remote areas and able to operate more efficiently compared to traditional multirotor UAVs while still producing equal or higher quality mineral data. This allows for much higher efficiency and lower operating costs for high-production UAV-based mineral data acquisition.","sentences":["Multirotor Unmanned Aerial Vehicles (UAV)s have recently become an important instrument for collecting mineral data, enabling more effective and accurate geological investigations.","This paper explores the difficulties in mounting high-sensitivity sensors on a UAV platform, including electromagnetic interference, payload dynamics, and maintaining stable sensor performance while in flight.","It is highlighted how the specific solutions provided to deal with these problems have the potential to alter the collection of mineral data assisted by UAVs.","The work also shows experimental findings that demonstrate the creative potential of these solutions in UAV-based mineral data collecting, leading to improvements in effective mineral exploration through careful design, testing, and assessment of these systems.","These innovations resulted in a platform that is quickly deployable in remote areas and able to operate more efficiently compared to traditional multirotor UAVs while still producing equal or higher quality mineral data.","This allows for much higher efficiency and lower operating costs for high-production UAV-based mineral data acquisition."],"url":"http://arxiv.org/abs/2402.11797v1","category":"cs.RO"}
{"created":"2024-02-19 02:48:40","title":"Generative Kaleidoscopic Networks","abstract":"We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots f_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper the MLP, higher is the quality of samples recovered. Scope: We observed this phenomenon to various degrees for the other deep learning architectures like CNNs, Transformers & U-Nets and we are currently investigating them further.","sentences":["We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon.","That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process.","In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP.","We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'.","Briefly, if we learn a MLP to map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots f_\\mathcal{N}(z)\\cdots )$.","After a burn-in period duration, we start observing samples from the input distribution and we found that deeper the MLP, higher is the quality of samples recovered.","Scope:","We observed this phenomenon to various degrees for the other deep learning architectures like CNNs, Transformers & U-Nets and we are currently investigating them further."],"url":"http://arxiv.org/abs/2402.11793v1","category":"cs.LG"}
{"created":"2024-02-19 02:31:36","title":"MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion","abstract":"Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.","sentences":["Survival risk stratification is an important step in clinical decision making for breast cancer management.","We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data.","It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level.","A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy.","Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods.","This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes."],"url":"http://arxiv.org/abs/2402.11788v1","category":"cs.CV"}
{"created":"2024-02-19 02:12:07","title":"Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware","abstract":"With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures creating a Pareto optimal frontier of downstream task accuracy and execution metrics (e.g., latency). The proposed framework can comprehend the complex interplay between a sub-network's performance and the CiM hardware configuration choices including bandwidth, processing element size, and memory size. Exhaustive experiments on different model architectures from both CNN and Transformer families demonstrate the efficacy of the CiMNet in finding co-optimized sub-networks and CiM hardware configurations. Specifically, for similar ImageNet classification accuracy as baseline ViT-B, optimizing only the model architecture increases performance (or reduces workload execution time) by 1.7x while optimizing for both the model architecture and hardware configuration increases it by 3.1x.","sentences":["With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures.","However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems.","Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget.","In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures creating a Pareto optimal frontier of downstream task accuracy and execution metrics (e.g., latency).","The proposed framework can comprehend the complex interplay between a sub-network's performance and the CiM hardware configuration choices including bandwidth, processing element size, and memory size.","Exhaustive experiments on different model architectures from both CNN and Transformer families demonstrate the efficacy of the CiMNet in finding co-optimized sub-networks and CiM hardware configurations.","Specifically, for similar ImageNet classification accuracy as baseline ViT-B, optimizing only the model architecture increases performance (or reduces workload execution time) by 1.7x while optimizing for both the model architecture and hardware configuration increases it by 3.1x."],"url":"http://arxiv.org/abs/2402.11780v1","category":"cs.AR"}
{"created":"2024-02-19 02:08:09","title":"Towards Theoretical Understandings of Self-Consuming Generative Models","abstract":"This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declines beyond a threshold point. Finally, we specialize our general results to diffusion models, delivering nuanced insights such as the efficacy of optimal early stopping within the self-consuming loop.","sentences":["This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations.","We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models.","Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios.","Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough.","Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declines beyond a threshold point.","Finally, we specialize our general results to diffusion models, delivering nuanced insights such as the efficacy of optimal early stopping within the self-consuming loop."],"url":"http://arxiv.org/abs/2402.11778v1","category":"cs.LG"}
{"created":"2024-02-19 02:08:03","title":"Uncovering Latent Human Wellbeing in Language Model Embeddings","abstract":"Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.","sentences":["Do language models implicitly learn a concept of human wellbeing?","We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations.","Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy.","This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing.","Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters.","We find performance is nondecreasing with increased model size when using sufficient numbers of principal components."],"url":"http://arxiv.org/abs/2402.11777v1","category":"cs.CL"}
{"created":"2024-02-19 02:06:04","title":"Dynamic Multi-Network Mining of Tensor Time Series","abstract":"Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clusters with distinct networks from tensor time series according to the minimum description length (MDL). (c) Scalable: it scales linearly in terms of the input data size when solving a non-convex problem to optimize the number of segments and clusters, and thus it is applicable to long-range and high-dimensional tensors. Extensive experiments with synthetic datasets confirm that our method outperforms the state-of-the-art methods in terms of clustering accuracy. We then use real datasets to demonstrate that DMM is useful for providing interpretable insights from tensor time series.","sentences":["Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data.","Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights?","In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm.","Our method has the following properties.","(a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships.","(b) Accurate: it discovers the clusters with distinct networks from tensor time series according to the minimum description length (MDL).","(c) Scalable: it scales linearly in terms of the input data size when solving a non-convex problem to optimize the number of segments and clusters, and thus it is applicable to long-range and high-dimensional tensors.","Extensive experiments with synthetic datasets confirm that our method outperforms the state-of-the-art methods in terms of clustering accuracy.","We then use real datasets to demonstrate that DMM is useful for providing interpretable insights from tensor time series."],"url":"http://arxiv.org/abs/2402.11773v1","category":"cs.LG"}
{"created":"2024-02-19 01:55:55","title":"Evaluating the Effectiveness of Index-Based Treatment Allocation","abstract":"When resources are scarce, an allocation policy is needed to decide who receives a resource. This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods. This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. This enables us to effectively draw valid statistical conclusions, a critical gap in previous work. Our extensive experiments validate our methodology in practical settings, while also showcasing its statistical power. We conclude by proposing and empirically verifying extensions of our methodology that enable us to reevaluate a past randomized control trial to evaluate different ML allocation policies in the context of a mHealth program, drawing previously invisible conclusions.","sentences":["When resources are scarce, an allocation policy is needed to decide who receives a resource.","This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods.","This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial.","Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators.","Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals.","This enables us to effectively draw valid statistical conclusions, a critical gap in previous work.","Our extensive experiments validate our methodology in practical settings, while also showcasing its statistical power.","We conclude by proposing and empirically verifying extensions of our methodology that enable us to reevaluate a past randomized control trial to evaluate different ML allocation policies in the context of a mHealth program, drawing previously invisible conclusions."],"url":"http://arxiv.org/abs/2402.11771v1","category":"cs.LG"}
{"created":"2024-02-19 01:28:48","title":"ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs","abstract":"Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost.","sentences":["Large Language models (LLMs), while powerful, exhibit harmful social biases.","Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities.","This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs.","We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories.","We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets.","Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones.","These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost."],"url":"http://arxiv.org/abs/2402.11764v1","category":"cs.CL"}
{"created":"2024-02-19 01:11:44","title":"Large Language Models for Stemming: Promises, Pitfalls and Failures","abstract":"Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from each document entities that should not be stemmed, then use vocabulary stemming to stem the rest of the terms (entity-based contextual stemming). Through a series of empirical experiments, we compare the use of LLMs for stemming with that of traditional lexical stemmers such as Porter and Krovetz for English text. We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions.","sentences":["Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form.","The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25.","However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information.","Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding.","With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from each document entities that should not be stemmed, then use vocabulary stemming to stem the rest of the terms (entity-based contextual stemming).","Through a series of empirical experiments, we compare the use of LLMs for stemming with that of traditional lexical stemmers such as Porter and Krovetz for English text.","We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions."],"url":"http://arxiv.org/abs/2402.11757v1","category":"cs.IR"}
{"created":"2024-02-19 00:43:31","title":"ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs","abstract":"Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs.","sentences":["Safety is critical to the usage of large language models (LLMs).","Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety.","However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics.","This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs.","For example, users of forums often use ASCII art, a form of text-based art, to convey image information.","In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics.","We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art.","Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs.","ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack.","We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs."],"url":"http://arxiv.org/abs/2402.11753v1","category":"cs.CL"}
{"created":"2024-02-19 00:43:22","title":"Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing","abstract":"It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.","sentences":["It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models.","This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD).","We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased.","Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective.","Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance."],"url":"http://arxiv.org/abs/2402.11752v1","category":"cs.LG"}
{"created":"2024-02-19 00:18:09","title":"Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic","abstract":"Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full fine-tuning, respectively, while maintaining most of the model's performance on the task. We release the source codes at: https://github.com/declare-lab/resta.","sentences":["Aligned language models face a significant limitation as their fine-tuning often results in compromised safety.","To tackle this, we propose a simple method RESTA that performs LLM safety realignment.","RESTA stands for REstoring Safety through Task Arithmetic.","At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model.","We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math.","We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm.","Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full fine-tuning, respectively, while maintaining most of the model's performance on the task.","We release the source codes at: https://github.com/declare-lab/resta."],"url":"http://arxiv.org/abs/2402.11746v1","category":"cs.CL"}
{"created":"2024-02-18 23:41:38","title":"Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation","abstract":"In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method.","sentences":["In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks.","In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks.","The output discrepancy can quantitatively characterize the output difference produced by compression procedures.","Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network.","Then, we repair the compressed FNN by re-training based on the training set.","We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method."],"url":"http://arxiv.org/abs/2402.11737v1","category":"cs.LG"}
{"created":"2024-02-18 23:19:21","title":"Solving Data-centric Tasks using Large Language Models","abstract":"Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.","sentences":["Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users.","These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data.","But how do we decide how much data and which data to include in the prompt?","This paper makes two contributions towards answering this question.","First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts.","Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt.","Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline."],"url":"http://arxiv.org/abs/2402.11734v1","category":"cs.PL"}
{"created":"2024-02-18 23:14:40","title":"The Effectiveness of Random Forgetting for Robust Generalization","abstract":"Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \"Forget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.","sentences":["Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy.","Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks.","However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization.","Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \"Forget to Mitigate Overfitting (FOMO)\".","FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features.","Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness.","Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods.","Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios."],"url":"http://arxiv.org/abs/2402.11733v1","category":"cs.LG"}
{"created":"2024-02-18 23:01:28","title":"Prospector Heads: Generalized Feature Attribution for Large Models & Data","abstract":"Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on \"explaining\" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in the input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for machine learning models in complex domains.","sentences":["Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains.","Current methods for feature attribution, which rely on \"explaining\" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges.","We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality.","Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC.","We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in the input data.","Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for machine learning models in complex domains."],"url":"http://arxiv.org/abs/2402.11729v1","category":"cs.LG"}
{"created":"2024-02-18 21:13:05","title":"GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network","abstract":"Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process.","sentences":["Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them.","However, fine-tuning still remains crucial to further enhance their adaptability.","Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality.","We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach.","GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation.","GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN.","Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters.","We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency.","Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process."],"url":"http://arxiv.org/abs/2402.11709v1","category":"cs.CL"}
{"created":"2024-02-18 21:10:18","title":"Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable","abstract":"In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI). Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases. We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability. Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem. We argue how all these factors could reduce the reliability of search engines. Finally, we summarize some of the active research directions and open questions.","sentences":["In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI).","Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases.","We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability.","Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem.","We argue how all these factors could reduce the reliability of search engines.","Finally, we summarize some of the active research directions and open questions."],"url":"http://arxiv.org/abs/2402.11707v1","category":"cs.IR"}
{"created":"2024-02-18 20:48:09","title":"Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation","abstract":"Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.","sentences":["Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios.","However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world.","To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub).","Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code.","These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development."],"url":"http://arxiv.org/abs/2402.11702v1","category":"cs.SE"}
{"created":"2024-02-18 19:26:49","title":"ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \\url{https://allava.freedomai.cn}.","sentences":["Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices.","This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data.","To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images.","The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs.","This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs.","Our online demo is available at \\url{https://allava.freedomai.cn}."],"url":"http://arxiv.org/abs/2402.11684v1","category":"cs.CL"}
{"created":"2024-02-18 19:08:19","title":"3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods","abstract":"Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network. We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches that are based on generic octree point cloud compression or based on raw point cloud data compression, our approach achieves the best quantitative and visual performance. Source code and dataset are available at https://github.com/ika-rwth-aachen/Point-Cloud-Compression.","sentences":["Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM.","However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume.","Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms.","We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation.","To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network.","We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity.","Compared to approaches that are based on generic octree point cloud compression or based on raw point cloud data compression, our approach achieves the best quantitative and visual performance.","Source code and dataset are available at https://github.com/ika-rwth-aachen/Point-Cloud-Compression."],"url":"http://arxiv.org/abs/2402.11680v1","category":"cs.CV"}
{"created":"2024-02-18 18:56:13","title":"MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection","abstract":"Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of their resistance ability. Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy. We provide insights into which multi-modal design choices make such models robust against certain perturbations. The dataset generation code and benchmark are open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.","sentences":["Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes.","However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications.","Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras.","Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference.","To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions.","We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of their resistance ability.","Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy.","We provide insights into which multi-modal design choices make such models robust against certain perturbations.","The dataset generation code and benchmark are open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt."],"url":"http://arxiv.org/abs/2402.11677v1","category":"cs.CV"}
{"created":"2024-02-18 18:56:07","title":"A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models","abstract":"Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and outperform alternative metrics, indicating their potential as multi-aspect, reference-free and interpretable evaluators for counter narrative evaluation.","sentences":["Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy.","While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped.","Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria.","To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs.","We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and outperform alternative metrics, indicating their potential as multi-aspect, reference-free and interpretable evaluators for counter narrative evaluation."],"url":"http://arxiv.org/abs/2402.11676v1","category":"cs.CL"}
{"created":"2024-02-18 18:33:48","title":"A Fast Algorithm to Simulate Nonlinear Resistive Networks","abstract":"In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our simulation methodology significantly outperforms existing SPICE-based simulations, enabling the training of networks up to 325 times larger at speeds 150 times faster, resulting in a 50,000-fold improvement in the ratio of network size to epoch duration. Our approach, adaptable to other electrical components, can foster more rapid progress in the simulations of nonlinear electrical networks.","sentences":["In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks.","These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation.","Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE.","Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm.","Our simulation methodology significantly outperforms existing SPICE-based simulations, enabling the training of networks up to 325 times larger at speeds 150 times faster, resulting in a 50,000-fold improvement in the ratio of network size to epoch duration.","Our approach, adaptable to other electrical components, can foster more rapid progress in the simulations of nonlinear electrical networks."],"url":"http://arxiv.org/abs/2402.11674v1","category":"cs.ET"}
{"created":"2024-02-18 18:20:57","title":"Autocorrect for Estonian texts: final report from project EKTB25","abstract":"The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models. We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail.   There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-language support, has been created. We took into account the existence of the model when adjusting plans and in the report we present a comparison with the ability of GPT4 to improve the Estonian language text.   The final results show that the approach we have developed provides better scores than GPT4 and the result is usable but not entirely reliable yet. The report also contains ideas on how GPT4 and other major language models can be implemented in the future, focusing on open-source solutions.   All results of this project are open-data/open-source, with licenses that allow them to be used for purposes including commercial ones.","sentences":["The project was funded in 2021-2023 by the National Programme of Estonian Language Technology.","Its main aim was to develop spelling and grammar correction tools for the Estonian language.","The main challenge was the very small amount of available error correction data needed for such development.","To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models.","We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail.   ","There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-language support, has been created.","We took into account the existence of the model when adjusting plans and in the report we present a comparison with the ability of GPT4 to improve the Estonian language text.   ","The final results show that the approach we have developed provides better scores than GPT4 and the result is usable but not entirely reliable yet.","The report also contains ideas on how GPT4 and other major language models can be implemented in the future, focusing on open-source solutions.   ","All results of this project are open-data/open-source, with licenses that allow them to be used for purposes including commercial ones."],"url":"http://arxiv.org/abs/2402.11671v1","category":"cs.CL"}
{"created":"2024-02-18 18:09:38","title":"Disturbance Ratio for Optimal Multi-Event Classification in Power Distribution Networks","abstract":"This paper presents an effective approach to identify power quality events based on IEEE Std 1159-2009 caused by intermittent power sources like those of renewable energy. An efficient characterization of these disturbances is granted by the use of two useful wavelet based indices. For this purpose, a wavelet-based Global Disturbance Ratio index (GDR), defined through its instantaneous precursor (Instantaneous Transient Disturbance index ITD(t)), is used in power distribution networks (PDN) under steady-state and/or transient conditions. An intelligent disturbance classification is done using a Support Vector Machine (SVM) with a minimum input vector based on the GDR index. The effectiveness of the proposed technique is validated using a real-time experimental system with single events and multi-events signals.","sentences":["This paper presents an effective approach to identify power quality events based on IEEE Std 1159-2009 caused by intermittent power sources like those of renewable energy.","An efficient characterization of these disturbances is granted by the use of two useful wavelet based indices.","For this purpose, a wavelet-based Global Disturbance Ratio index (GDR), defined through its instantaneous precursor (Instantaneous Transient Disturbance index ITD(t)), is used in power distribution networks (PDN) under steady-state and/or transient conditions.","An intelligent disturbance classification is done using a Support Vector Machine (SVM) with a minimum input vector based on the GDR index.","The effectiveness of the proposed technique is validated using a real-time experimental system with single events and multi-events signals."],"url":"http://arxiv.org/abs/2402.11668v1","category":"eess.SP"}
{"created":"2024-02-18 17:32:53","title":"Dynamic planning in hierarchical active inference","abstract":"By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand and exploit affordances for object manipulation, and to learn the hierarchical interactions between the self and the environment, including other agents. We start from a simple unit and gradually describe more advanced structures, comparing recently proposed design choices and providing basic examples for each section. This study distances itself from traditional views centered on neural networks and reinforcement learning, and points toward a yet unexplored direction in active inference: hybrid representations in hierarchical models.","sentences":["By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions.","A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states.","Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence.","Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments.","Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand and exploit affordances for object manipulation, and to learn the hierarchical interactions between the self and the environment, including other agents.","We start from a simple unit and gradually describe more advanced structures, comparing recently proposed design choices and providing basic examples for each section.","This study distances itself from traditional views centered on neural networks and reinforcement learning, and points toward a yet unexplored direction in active inference: hybrid representations in hierarchical models."],"url":"http://arxiv.org/abs/2402.11658v1","category":"cs.AI"}
{"created":"2024-02-18 17:17:15","title":"Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing","abstract":"Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming. However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks. Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs. Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers. Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offloading strategy. Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server. Moreover, existing multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function. We proposed a novel combinatorial client-master MADRL (CCM\\_MADRL) algorithm for task offloading in MEC (CCM\\_MADRL\\_MEC) that enables UDs to decide their resource requirements and the server to make a combinatorial decision based on the requirements of the UDs. CCM\\_MADRL\\_MEC is the first MADRL in task offloading to consider server storage capacity in addition to the constraints in the UDs. By taking advantage of the combinatorial action selection, CCM\\_MADRL\\_MEC has shown superior convergence over existing MADDPG and heuristic algorithms.","sentences":["Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming.","However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks.","Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs.","Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers.","Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity.","However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offloading strategy.","Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server.","Moreover, existing multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function.","We proposed a novel combinatorial client-master MADRL (CCM\\_MADRL) algorithm for task offloading in MEC (CCM\\_MADRL\\_MEC) that enables UDs to decide their resource requirements and the server to make a combinatorial decision based on the requirements of the UDs.","CCM\\_MADRL\\_MEC is the first MADRL in task offloading to consider server storage capacity in addition to the constraints in the UDs.","By taking advantage of the combinatorial action selection, CCM\\_MADRL\\_MEC has shown superior convergence over existing MADDPG and heuristic algorithms."],"url":"http://arxiv.org/abs/2402.11653v1","category":"cs.AI"}
{"created":"2024-02-18 17:10:07","title":"Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents","abstract":"Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin.","sentences":["Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines.","However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents.","To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them.","As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning.","In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies.","We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic question answering tasks.","Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin."],"url":"http://arxiv.org/abs/2402.11651v1","category":"cs.CL"}
{"created":"2024-02-18 16:55:54","title":"Quantum Image Denoising with Machine Learning: A Novel Approach to Improve Quantum Image Processing Quality and Reliability","abstract":"Quantum Image Processing (QIP) is a field that aims to utilize the benefits of quantum computing for manipulating and analyzing images. However, QIP faces two challenges: the limitation of qubits and the presence of noise in a quantum machine. In this research we propose a novel approach to address the issue of noise in QIP. By training and employing a machine learning model that identifies and corrects the noise in quantum processed images, we can compensate for the noisiness caused by the machine and retrieve a processing result similar to that performed by a classical computer with higher efficiency. The model is trained by learning a dataset consisting of both existing processed images and quantum processed images from open access datasets. This model will be capable of providing us with the confidence level for each pixel and its potential original value. To assess the model's accuracy in compensating for loss and decoherence in QIP, we evaluate it using three metrics: Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Opinion Score (MOS). Additionally, we discuss the applicability of our model across domains well as its cost effectiveness compared to alternative methods.","sentences":["Quantum Image Processing (QIP) is a field that aims to utilize the benefits of quantum computing for manipulating and analyzing images.","However, QIP faces two challenges: the limitation of qubits and the presence of noise in a quantum machine.","In this research we propose a novel approach to address the issue of noise in QIP.","By training and employing a machine learning model that identifies and corrects the noise in quantum processed images, we can compensate for the noisiness caused by the machine and retrieve a processing result similar to that performed by a classical computer with higher efficiency.","The model is trained by learning a dataset consisting of both existing processed images and quantum processed images from open access datasets.","This model will be capable of providing us with the confidence level for each pixel and its potential original value.","To assess the model's accuracy in compensating for loss and decoherence in QIP, we evaluate it using three metrics: Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Opinion Score (MOS).","Additionally, we discuss the applicability of our model across domains well as its cost effectiveness compared to alternative methods."],"url":"http://arxiv.org/abs/2402.11645v1","category":"quant-ph"}
{"created":"2024-02-18 16:43:21","title":"Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models","abstract":"Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we align the abilities of LLMs with the requirements of each procedure. Finally, we point out the promising directions that could better leverage the strength of LLMs towards versatile graph learning methods.","sentences":["Graph-structured data are the commonly used and have wide application scenarios in the real world.","For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches.","Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence.","This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives.","From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving.","We then explore the application scenarios of LLMs in these procedures across a wider spectrum.","In the ``how'' perspective, we align the abilities of LLMs with the requirements of each procedure.","Finally, we point out the promising directions that could better leverage the strength of LLMs towards versatile graph learning methods."],"url":"http://arxiv.org/abs/2402.11641v1","category":"cs.LG"}
{"created":"2024-02-18 16:29:38","title":"Non-equilibrium pathways to emergent polar supertextures","abstract":"Ultrafast stimuli can stabilize metastable states of matter inaccessible by equilibrium means. Establishing the spatiotemporal link between ultrafast excitation and metastability is crucial to understanding these phenomena. Here, we use single-shot optical-pump, X-ray-probe measurements to provide snapshots of the emergence of a persistent polar vortex supercrystal in a heterostructure that hosts a fine balance between built-in electrostatic and elastic frustrations by design. By perturbing this balance with photoinduced charges, a starting heterogenous mixture of polar phases disorders within a few picoseconds, resulting in a soup state composed of disordered ferroelectric and suppressed vortex orders. On the pico-to-nanosecond timescales, transient labyrinthine fluctuations form in this soup along with a recovering vortex order. On longer timescales, these fluctuations are progressively quenched by dynamical strain modulations, which drive the collective emergence of a single supercrystal phase. Our results, corroborated by dynamical phase-field modeling, reveal how ultrafast excitation of designer systems generates pathways for persistent metastability.","sentences":["Ultrafast stimuli can stabilize metastable states of matter inaccessible by equilibrium means.","Establishing the spatiotemporal link between ultrafast excitation and metastability is crucial to understanding these phenomena.","Here, we use single-shot optical-pump, X-ray-probe measurements to provide snapshots of the emergence of a persistent polar vortex supercrystal in a heterostructure that hosts a fine balance between built-in electrostatic and elastic frustrations by design.","By perturbing this balance with photoinduced charges, a starting heterogenous mixture of polar phases disorders within a few picoseconds, resulting in a soup state composed of disordered ferroelectric and suppressed vortex orders.","On the pico-to-nanosecond timescales, transient labyrinthine fluctuations form in this soup along with a recovering vortex order.","On longer timescales, these fluctuations are progressively quenched by dynamical strain modulations, which drive the collective emergence of a single supercrystal phase.","Our results, corroborated by dynamical phase-field modeling, reveal how ultrafast excitation of designer systems generates pathways for persistent metastability."],"url":"http://arxiv.org/abs/2402.11634v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-18 16:16:16","title":"Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks","abstract":"This paper introduces the distributed and intelligent integrated sensing and communications (DISAC) concept, a transformative approach for 6G wireless networks that extends the emerging concept of integrated sensing and communications (ISAC). DISAC addresses the limitations of the existing ISAC models and, to overcome them, it introduces two novel foundational functionalities for both sensing and communications: a distributed architecture and a semantic and goal-oriented framework. The distributed architecture enables large-scale and energy-efficient tracking of connected users and objects, leveraging the fusion of heterogeneous sensors. The semantic and goal-oriented intelligent and parsimonious framework, enables the transition from classical data fusion to the composition of semantically selected information, offering new paradigms for the optimization of resource utilization and exceptional multi-modal sensing performance across various use cases. This paper details DISAC's principles, architecture, and potential applications.","sentences":["This paper introduces the distributed and intelligent integrated sensing and communications (DISAC) concept, a transformative approach for 6G wireless networks that extends the emerging concept of integrated sensing and communications (ISAC).","DISAC addresses the limitations of the existing ISAC models and, to overcome them, it introduces two novel foundational functionalities for both sensing and communications: a distributed architecture and a semantic and goal-oriented framework.","The distributed architecture enables large-scale and energy-efficient tracking of connected users and objects, leveraging the fusion of heterogeneous sensors.","The semantic and goal-oriented intelligent and parsimonious framework, enables the transition from classical data fusion to the composition of semantically selected information, offering new paradigms for the optimization of resource utilization and exceptional multi-modal sensing performance across various use cases.","This paper details DISAC's principles, architecture, and potential applications."],"url":"http://arxiv.org/abs/2402.11630v1","category":"eess.SP"}
{"created":"2024-02-18 15:28:39","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models","abstract":"Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.","sentences":["Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs).","Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image.","To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models.","However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations.","In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects.","Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT.","In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa.","Whether their responses can form a logical closed loop serves as an indicator of object hallucination.","As a plug-and-play method, it can be seamlessly applied to all existing LVLMs.","Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality."],"url":"http://arxiv.org/abs/2402.11622v1","category":"cs.CV"}
{"created":"2024-02-18 15:20:24","title":"On Network Design and Planning 2.0 for Optical-computing-enabled Networks","abstract":"In accommodating the continued explosive growth in Internet traffic, optical core networks have been evolving accordingly thanks to numerous technological and architectural innovations. From an architectural perspective, the adoption of optical-bypass networking in the last two decades has resulted in substantial cost savings, owning to the elimination of massive optical-electrical optical interfaces. In optical-bypass framework, the basic functions of optical nodes include adding (dropping) and cross-connecting transitional lightpaths. Moreover, in the process of cross-connecting transiting lightpaths through an intermediate node, these lightpaths must be separated from each other in either time, frequency or spatial domain, to avoid unwanted interference which deems to deteriorate the signal qualities. In light of recently enormous advances in photonic signal processing / computing technologies enabling the precisely controlled interference of optical channels for various computing functions, we propose a new architectural paradigm for future optical networks, namely, optical-computing-enabled networks. Our proposal is defined by the added capability of optical nodes permitting the superposition of transitional lightpaths for computing purposes to achieve greater capacity efficiency. Specifically, we present two illustrative examples highlighting the potential benefits of bringing about in-network optical computing functions which are relied on optical aggregation and optical XOR gate. The new optical computing capabilities armed at optical nodes therefore call for a radical change in formulating networking problems and designing accompanying algorithms, which are collectively referred to as optical network design and planning 2.0 so that the capital and operational efficiency could be fully unlocked.","sentences":["In accommodating the continued explosive growth in Internet traffic, optical core networks have been evolving accordingly thanks to numerous technological and architectural innovations.","From an architectural perspective, the adoption of optical-bypass networking in the last two decades has resulted in substantial cost savings, owning to the elimination of massive optical-electrical optical interfaces.","In optical-bypass framework, the basic functions of optical nodes include adding (dropping) and cross-connecting transitional lightpaths.","Moreover, in the process of cross-connecting transiting lightpaths through an intermediate node, these lightpaths must be separated from each other in either time, frequency or spatial domain, to avoid unwanted interference which deems to deteriorate the signal qualities.","In light of recently enormous advances in photonic signal processing / computing technologies enabling the precisely controlled interference of optical channels for various computing functions, we propose a new architectural paradigm for future optical networks, namely, optical-computing-enabled networks.","Our proposal is defined by the added capability of optical nodes permitting the superposition of transitional lightpaths for computing purposes to achieve greater capacity efficiency.","Specifically, we present two illustrative examples highlighting the potential benefits of bringing about in-network optical computing functions which are relied on optical aggregation and optical XOR gate.","The new optical computing capabilities armed at optical nodes therefore call for a radical change in formulating networking problems and designing accompanying algorithms, which are collectively referred to as optical network design and planning 2.0 so that the capital and operational efficiency could be fully unlocked."],"url":"http://arxiv.org/abs/2402.11618v1","category":"cs.NI"}
{"created":"2024-02-18 14:34:57","title":"AI-assisted inverse design of sequence-ordered high intrinsic thermal conductivity polymers","abstract":"Artificial intelligence (AI) promotes the polymer design paradigm from a traditional trial-and-error approach to a data-driven style. Achieving high thermal conductivity (TC) for intrinsic polymers is urgent because of their importance in the thermal management of many industrial applications such as microelectronic devices and integrated circuits. In this work, we have proposed a robust AI-assisted workflow for the inverse design of high TC polymers. By using 1144 polymers with known computational TCs, we construct a surrogate deep neural network model for TC prediction and extract a polymer-unit library with 32 sequences. Two state-of-the-art multi-objective optimization algorithms of unified non-dominated sorting genetic algorithm III (U-NSGA-III) and q-noisy expected hypervolume improvement (qNEHVI) are employed for sequence-ordered polymer design with both high TC and synthetic possibility. For triblock polymer design, the result indicates that qNHEVI is capable of exploring a diversity of optimal polymers at the Pareto front, but the uncertainty in Quasi-Monte Carlo sampling makes the trials costly. The performance of U-NSGA-III is affected by the initial random structures and usually falls into a locally optimal solution, but it takes fewer attempts with lower costs. 20 parallel U-NSGA-III runs are conducted to design the pentablock polymers with high TC, and half of the candidates among 1921 generated polymers achieve the targets (TC > 0.4 W/(mK) and SA < 3.0). Ultimately, we check the TC of 50 promising polymers through molecular dynamics simulations and reveal the intrinsic connections between microstructures and TCs. Our developed AI-assisted inverse design approach for polymers is flexible and universal, and can be extended to the design of polymers with other target properties.","sentences":["Artificial intelligence (AI) promotes the polymer design paradigm from a traditional trial-and-error approach to a data-driven style.","Achieving high thermal conductivity (TC) for intrinsic polymers is urgent because of their importance in the thermal management of many industrial applications such as microelectronic devices and integrated circuits.","In this work, we have proposed a robust AI-assisted workflow for the inverse design of high TC polymers.","By using 1144 polymers with known computational TCs, we construct a surrogate deep neural network model for TC prediction and extract a polymer-unit library with 32 sequences.","Two state-of-the-art multi-objective optimization algorithms of unified non-dominated sorting genetic algorithm III (U-NSGA-III) and q-noisy expected hypervolume improvement (qNEHVI) are employed for sequence-ordered polymer design with both high TC and synthetic possibility.","For triblock polymer design, the result indicates that qNHEVI is capable of exploring a diversity of optimal polymers at the Pareto front, but the uncertainty in Quasi-Monte Carlo sampling makes the trials costly.","The performance of U-NSGA-III is affected by the initial random structures and usually falls into a locally optimal solution, but it takes fewer attempts with lower costs.","20 parallel U-NSGA-III runs are conducted to design the pentablock polymers with high TC, and half of the candidates among 1921 generated polymers achieve the targets (TC > 0.4 W/(mK) and SA < 3.0).","Ultimately, we check the TC of 50 promising polymers through molecular dynamics simulations and reveal the intrinsic connections between microstructures and TCs.","Our developed AI-assisted inverse design approach for polymers is flexible and universal, and can be extended to the design of polymers with other target properties."],"url":"http://arxiv.org/abs/2402.11600v1","category":"cond-mat.soft"}
{"created":"2024-02-18 14:12:15","title":"Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI","abstract":"Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. OML is able to process data in a sequential manner, which is especially useful for data streams. The `river` package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, clustering, anomaly detection, and more. The `spotRiver` package provides a framework for hyperparameter tuning of OML models. The `spotRiverGUI` is a graphical user interface for the `spotRiver` package. The `spotRiverGUI` releases the user from the burden of manually searching for the optimal hyperparameter setting. After the data is provided, users can compare different OML algorithms from the powerful `river` package in a convenient way and tune the selected algorithms very efficiently.","sentences":["Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data.","This is especially true for available memory, handling drift in data streams, and processing new, unknown data.","Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML.","OML is able to process data in a sequential manner, which is especially useful for data streams.","The `river` package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, clustering, anomaly detection, and more.","The `spotRiver` package provides a framework for hyperparameter tuning of OML models.","The `spotRiverGUI` is a graphical user interface for the `spotRiver` package.","The `spotRiverGUI` releases the user from the burden of manually searching for the optimal hyperparameter setting.","After the data is provided, users can compare different OML algorithms from the powerful `river` package in a convenient way and tune the selected algorithms very efficiently."],"url":"http://arxiv.org/abs/2402.11594v1","category":"cs.LG"}
{"created":"2024-02-18 13:53:24","title":"Designing interactive data visualizations representing recovery progress for patients after stroke","abstract":"Stroke is one of the leading causes of disability worldwide. The efficacy of recovery is determined by a variety of factors, including patient adherence to rehabilitation programs. One way to increase patient adherence to their rehabilitation program is to show patients their progress that is visualized in a simple and intuitive way. We begin to gather preliminary information on Functional Capacity, Motor Function, and Mood/cognition from occupational Therapists at the Bruyere Hospital to gain a better understanding of how stroke recovery data is collected within in-patient stroke rehabilitation centers. The future aim is to design, develop, and evaluate a data visualization tool representing progress made by patients recovering from stroke.","sentences":["Stroke is one of the leading causes of disability worldwide.","The efficacy of recovery is determined by a variety of factors, including patient adherence to rehabilitation programs.","One way to increase patient adherence to their rehabilitation program is to show patients their progress that is visualized in a simple and intuitive way.","We begin to gather preliminary information on Functional Capacity, Motor Function, and Mood/cognition from occupational Therapists at the Bruyere Hospital to gain a better understanding of how stroke recovery data is collected within in-patient stroke rehabilitation centers.","The future aim is to design, develop, and evaluate a data visualization tool representing progress made by patients recovering from stroke."],"url":"http://arxiv.org/abs/2402.11590v1","category":"cs.HC"}
{"created":"2024-02-18 13:42:11","title":"SDiT: Spiking Diffusion Model with Transformer","abstract":"Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.","sentences":["Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing.","However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed.","In this paper, we explore a novel diffusion model architecture within spiking neural networks.","We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models.","It can generate higher quality images with relatively lower computational cost and shorter sampling time.","It aims to provide an empirical baseline for research of generative models based on SNNs.","Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models."],"url":"http://arxiv.org/abs/2402.11588v1","category":"cs.CV"}
{"created":"2024-02-18 13:22:50","title":"Planetesimal and planet formation in transient dust traps","abstract":"The ring-like structures in protoplanetary discs that are observed in the cold dust emission by ALMA, might be explained by dust aggregates trapped aerodynamically in pressure maxima. The effect of a transient pressure maximum is investigated that develops between two regimes with different turbulent levels. We study how such a pressure maximum collects dust aggregates and transforms them into large planetesimals and Moon-mass cores that can further grow to a few Earth-mass planets by pebble accretion, and eventually to giant planets, by considering the accretion of a gaseous envelope. A numerical model is developed, incorporating the evolution of gaseous disc, growth and transport of pebbles, N-body interactions of growing planetary cores and their backreaction to gas disc by opening a partial gap. Planetesimal formation by streaming instability is parametrized in our model. A transient pressure maximum efficiently accumulates dust particles that can grow larger than mm-size. If this happens, dust aggregates can be transformed by the streaming instability process into such large planetesimals, which can grow further by pebble accretion, according to our assumptions. As the gas evolves to its steady state, the pressure maximum vanishes, and the concentrated pebbles that are not transformed to planetesimals and accreted by the growing planet, drift inward. During this inward drift, if the conditions of the streaming instability are met, planetesimals are formed in a wide radial range of the disc. Conclusions. A transient pressure maximum is a favourable place for planetesimal and planet formation during its lifetime and the concentration of pebbles induces continuous formation of planetesimals even after its disappearance. Besides, the formation of a planet can trigger the formation of planetesimals over a wide area of the protoplanetary disc.","sentences":["The ring-like structures in protoplanetary discs that are observed in the cold dust emission by ALMA, might be explained by dust aggregates trapped aerodynamically in pressure maxima.","The effect of a transient pressure maximum is investigated that develops between two regimes with different turbulent levels.","We study how such a pressure maximum collects dust aggregates and transforms them into large planetesimals and Moon-mass cores that can further grow to a few Earth-mass planets by pebble accretion, and eventually to giant planets, by considering the accretion of a gaseous envelope.","A numerical model is developed, incorporating the evolution of gaseous disc, growth and transport of pebbles, N-body interactions of growing planetary cores and their backreaction to gas disc by opening a partial gap.","Planetesimal formation by streaming instability is parametrized in our model.","A transient pressure maximum efficiently accumulates dust particles that can grow larger than mm-size.","If this happens, dust aggregates can be transformed by the streaming instability process into such large planetesimals, which can grow further by pebble accretion, according to our assumptions.","As the gas evolves to its steady state, the pressure maximum vanishes, and the concentrated pebbles that are not transformed to planetesimals and accreted by the growing planet, drift inward.","During this inward drift, if the conditions of the streaming instability are met, planetesimals are formed in a wide radial range of the disc.","Conclusions.","A transient pressure maximum is a favourable place for planetesimal and planet formation during its lifetime and the concentration of pebbles induces continuous formation of planetesimals even after its disappearance.","Besides, the formation of a planet can trigger the formation of planetesimals over a wide area of the protoplanetary disc."],"url":"http://arxiv.org/abs/2402.11584v1","category":"astro-ph.EP"}
{"created":"2024-02-18 12:35:52","title":"Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru","abstract":"Social robots aim to establish long-term bonds with humans through engaging conversation. However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating large language models (LLMs) into social robots to achieve more dynamic and expressive conversations. We introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality. We incorporate robot behavior with two modalities: 1) a text-to-speech (TTS) engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions. A demo of our system is available here. To illuminate design and implementation issues, we conduct a pilot study where volunteers chat with a social robot using our proposed system, and we analyze their feedback, conducting a rigorous error analysis of chat transcripts. Feedback was overwhelmingly positive, with participants commenting on the robot's empathy, helpfulness, naturalness, and entertainment. Most negative feedback was due to automatic speech recognition (ASR) errors which had limited impact on conversations. However, we observed a small class of errors, such as the LLM repeating itself or hallucinating fictitious information and human responses, that have the potential to derail conversations, raising important issues for LLM application.","sentences":["Social robots aim to establish long-term bonds with humans through engaging conversation.","However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations.","This paper addresses this limitation by integrating large language models (LLMs) into social robots to achieve more dynamic and expressive conversations.","We introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality.","We incorporate robot behavior with two modalities: 1) a text-to-speech (TTS) engine capable of various delivery styles, and 2) a library of physical actions for the robot.","We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions.","A demo of our system is available here.","To illuminate design and implementation issues, we conduct a pilot study where volunteers chat with a social robot using our proposed system, and we analyze their feedback, conducting a rigorous error analysis of chat transcripts.","Feedback was overwhelmingly positive, with participants commenting on the robot's empathy, helpfulness, naturalness, and entertainment.","Most negative feedback was due to automatic speech recognition (ASR) errors which had limited impact on conversations.","However, we observed a small class of errors, such as the LLM repeating itself or hallucinating fictitious information and human responses, that have the potential to derail conversations, raising important issues for LLM application."],"url":"http://arxiv.org/abs/2402.11571v1","category":"cs.RO"}
{"created":"2024-02-18 12:33:54","title":"Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru","abstract":"This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality. Additionally, our study contributes to the broader understanding of dialogue design in social robotics, offering practical insights for future developments in the field.","sentences":["This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching.","We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change.","The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality.","Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening.","The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12).","The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality.","Additionally, our study contributes to the broader understanding of dialogue design in social robotics, offering practical insights for future developments in the field."],"url":"http://arxiv.org/abs/2402.11569v1","category":"cs.RO"}
{"created":"2024-02-18 12:30:04","title":"Saturability of the Quantum Cram\u00e9r-Rao Bound in Multiparameter Quantum Estimation at the Single-Copy Level","abstract":"The quantum Cram\\'{e}r-Rao bound (QCRB) as the ultimate lower bound for precision in quantum parameter estimation is only known to be saturable in the multiparameter setting in special cases and under conditions such as full or average commutavity of the symmetric logarithmic derivatives (SLDs) associated with the parameters. Moreover, for general mixed states, collective measurements over infinitely many identical copies of the quantum state are generally required to attain the QCRB. In the important and experimentally relevant single-copy scenario, a necessary condition for saturating the QCRB in the multiparameter setting for general mixed states is the so-called partial commutativity condition on the SLDs. However, it is not known if this condition is also sufficient. This paper derives new necessary conditions that imply partial commutativity and are almost sufficient. It is shown that together with another condition they become sufficient for saturability of the QCRB in the multiparameter single-copy case. Moreover, when the sufficient conditions are satisfied an optimal measurement saturating the QCRB can be chosen to be projective and explicitly characterized. An example is developed to illustrate the case of a multiparameter quantum state where the conditions derived herein are satisfied and can be explicitly verified.","sentences":["The quantum Cram\\'{e}r-Rao bound (QCRB) as the ultimate lower bound for precision in quantum parameter estimation is only known to be saturable in the multiparameter setting in special cases and under conditions such as full or average commutavity of the symmetric logarithmic derivatives (SLDs) associated with the parameters.","Moreover, for general mixed states, collective measurements over infinitely many identical copies of the quantum state are generally required to attain the QCRB.","In the important and experimentally relevant single-copy scenario, a necessary condition for saturating the QCRB in the multiparameter setting for general mixed states is the so-called partial commutativity condition on the SLDs.","However, it is not known if this condition is also sufficient.","This paper derives new necessary conditions that imply partial commutativity and are almost sufficient.","It is shown that together with another condition they become sufficient for saturability of the QCRB in the multiparameter single-copy case.","Moreover, when the sufficient conditions are satisfied an optimal measurement saturating the QCRB can be chosen to be projective and explicitly characterized.","An example is developed to illustrate the case of a multiparameter quantum state where the conditions derived herein are satisfied and can be explicitly verified."],"url":"http://arxiv.org/abs/2402.11567v1","category":"quant-ph"}
{"created":"2024-02-18 12:24:45","title":"Continual Learning on Graphs: Challenges, Solutions, and Opportunities","abstract":"Continual learning on graph data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged graph tasks. While there have been efforts to summarize progress on continual learning research over Euclidean data, e.g., images and texts, a systematic review of progress in continual learning on graphs, a.k.a, continual graph learning (CGL) or lifelong graph learning, is still demanding. Graph data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging. To bridge the gap, we provide a comprehensive review of existing continual graph learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics. We compare the CGL methods with traditional continual learning techniques and analyze the applicability of the traditional continual learning techniques to CGL tasks. Additionally, we review the benchmark works that are crucial to CGL research. Finally, we discuss the remaining challenges and propose several future directions. We will maintain an up-to-date GitHub repository featuring a comprehensive list of CGL algorithms, accessible at https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.","sentences":["Continual learning on graph data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged graph tasks.","While there have been efforts to summarize progress on continual learning research over Euclidean data, e.g., images and texts, a systematic review of progress in continual learning on graphs, a.k.a, continual graph learning (CGL) or lifelong graph learning, is still demanding.","Graph data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging.","To bridge the gap, we provide a comprehensive review of existing continual graph learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics.","We compare the CGL methods with traditional continual learning techniques and analyze the applicability of the traditional continual learning techniques to CGL tasks.","Additionally, we review the benchmark works that are crucial to CGL research.","Finally, we discuss the remaining challenges and propose several future directions.","We will maintain an up-to-date GitHub repository featuring a comprehensive list of CGL algorithms, accessible at https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs."],"url":"http://arxiv.org/abs/2402.11565v1","category":"cs.LG"}
{"created":"2024-02-18 12:18:16","title":"The Properties of Radially Excited Charmonia in The Light Front Quark Model","abstract":"Investigating the properties of excited charmonia is important to clarify its internal structure. In this paper, we present the mass spectra (MS) and decay constants (DC) for charmonia up to 3S states calculated by means of the light-front quark model based on a variational approach. In particular, we consider the QCD-motivated effective Hamiltonian, which includes both confinement (linear and screened) and Coulomb-like potentials. Furthermore, since the existence of the nature of heavy quark symmetry, we treat hyperfine interactions perturbatively. We developed the harmonic oscillator expansion method to approximate the wave function (WF) for excited states. We found that the results of our theoretical calculations, using screened potentials rather than linear ones, are in good agreement with experimental data. By looking at the mass and decay constant result, we found that our result on the {\\psi}(3S) state matched the properties of the {\\psi}(4040) resonance.","sentences":["Investigating the properties of excited charmonia is important to clarify its internal structure.","In this paper, we present the mass spectra (MS) and decay constants (DC) for charmonia up to 3S states calculated by means of the light-front quark model based on a variational approach.","In particular, we consider the QCD-motivated effective Hamiltonian, which includes both confinement (linear and screened) and Coulomb-like potentials.","Furthermore, since the existence of the nature of heavy quark symmetry, we treat hyperfine interactions perturbatively.","We developed the harmonic oscillator expansion method to approximate the wave function (WF) for excited states.","We found that the results of our theoretical calculations, using screened potentials rather than linear ones, are in good agreement with experimental data.","By looking at the mass and decay constant result, we found that our result on the {\\psi}(3S) state matched the properties of the {\\psi}(4040) resonance."],"url":"http://arxiv.org/abs/2402.11564v1","category":"hep-ph"}
{"created":"2024-02-18 11:59:04","title":"Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation","abstract":"Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare. However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors. Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data. Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions. In contrast, recent deep learning-based methods, leveraging graph and recurrent neural networks, have demonstrated enhanced efficacy. Nonetheless, these approaches are prone to error accumulation. Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future predictions. These models grapple with the challenge of producing unstable results, a particular issue in diffusion-based models. We aim to address these challenges by designing conditional features to guide the generative process and expedite training. Specifically, we introduce C$^2$TSD, a novel approach incorporating trend and seasonal information as conditional features and employing contrastive learning to improve model generalizability. The extensive experiments on three real-world datasets demonstrate the superior performance of C$^2$TSD over various state-of-the-art baselines.","sentences":["Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare.","However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors.","Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data.","Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions.","In contrast, recent deep learning-based methods, leveraging graph and recurrent neural networks, have demonstrated enhanced efficacy.","Nonetheless, these approaches are prone to error accumulation.","Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future predictions.","These models grapple with the challenge of producing unstable results, a particular issue in diffusion-based models.","We aim to address these challenges by designing conditional features to guide the generative process and expedite training.","Specifically, we introduce C$^2$TSD, a novel approach incorporating trend and seasonal information as conditional features and employing contrastive learning to improve model generalizability.","The extensive experiments on three real-world datasets demonstrate the superior performance of C$^2$TSD over various state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.11558v1","category":"cs.LG"}
{"created":"2024-02-18 11:46:52","title":"LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration","abstract":"Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \\textit{lost in the middle}. In this paper, we propose \\textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \\textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \\textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.","sentences":["Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks.","However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency.","Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \\textit{lost in the middle}.","In this paper, we propose \\textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4.","In \\textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents.","Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members.","To address this, we develop an \\textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing.","Our experimental results indicate that \\textsc{LongAgent} offers a promising alternative for long-text processing.","The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4."],"url":"http://arxiv.org/abs/2402.11550v1","category":"cs.CL"}
{"created":"2024-02-18 11:46:16","title":"Syntactic Language Change in English and German: Metrics, Parsers, and Convergences","abstract":"Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected by data 'noise' such as spelling changes and OCR errors in our historic data, we find that results of syntactic language change are sensitive to the parsers involved, which is a caution against using a single parser for evaluating syntactic language change as done in previous work. We also show that syntactic language change over the time period investigated is largely similar between English and German across the different metrics explored: only 4% of cases we examine yield opposite conclusions regarding upwards and downtrends of syntactic metrics across German and English. We also show that changes in syntactic measures seem to be more frequent at the tails of sentence length distributions. To our best knowledge, ours is the most comprehensive analysis of syntactic language using modern NLP technology in recent corpora of English and German.","sentences":["Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency.","Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load.","The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years.","We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives.","Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance.","Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected by data 'noise' such as spelling changes and OCR errors in our historic data, we find that results of syntactic language change are sensitive to the parsers involved, which is a caution against using a single parser for evaluating syntactic language change as done in previous work.","We also show that syntactic language change over the time period investigated is largely similar between English and German across the different metrics explored: only 4% of cases we examine yield opposite conclusions regarding upwards and downtrends of syntactic metrics across German and English.","We also show that changes in syntactic measures seem to be more frequent at the tails of sentence length distributions.","To our best knowledge, ours is the most comprehensive analysis of syntactic language using modern NLP technology in recent corpora of English and German."],"url":"http://arxiv.org/abs/2402.11549v1","category":"cs.CL"}
{"created":"2024-02-18 11:41:07","title":"KMMLU: Measuring Massive Multitask Language Understanding in Korean","abstract":"We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We make our dataset publicly available on the Hugging Face Hub and integrate the benchmark into EleutherAI's Language Model Evaluation Harness.","sentences":["We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM.","Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language.","We test 26 publically available and proprietary LLMs, identifying significant room for improvement.","The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%.","This model was primarily trained for English and Chinese, not Korean.","Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse.","Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively.","This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress.","We make our dataset publicly available on the Hugging Face Hub and integrate the benchmark into EleutherAI's Language Model Evaluation Harness."],"url":"http://arxiv.org/abs/2402.11548v1","category":"cs.CL"}
{"created":"2024-02-18 11:39:23","title":"Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis and Transmission Design","abstract":"The emerging reflecting intelligent surface (RIS) technology promises to enhance the capacity of wireless communication systems via passive reflect beamforming. However, the product path loss limits its performance gains. Fully-connected (FC) active RIS, which integrates reflect-type power amplifiers into the RIS elements, has been recently introduced in response to this issue. Also, sub-connected (SC) active RIS and hybrid FC-active/passive RIS variants, which employ a limited number of reflect-type power amplifiers, have been proposed to provide energy savings. Nevertheless, their flexibility in balancing diverse capacity requirements and power consumption constraints is limited. In this direction, this study introduces novel hybrid RIS structures, wherein at least one reflecting sub-surface (RS) adopts the SC-active RIS design. The asymptotic signal-to-noise-ratio of the FC-active/passive and the proposed hybrid RIS variants is analyzed in a single-user single-input single-output setup. Furthermore, the transmit and RIS beamforming weights are jointly optimized in each scenario to maximize the energy efficiency of a hybrid RIS-aided multi-user multiple-input single-output downlink system subject to the power consumption constraints of the base station and the active RSs. Numerical simulation and analytic results highlight the performance gains of the proposed RIS designs over benchmarks, unveil non-trivial trade-offs, and provide valuable insights.","sentences":["The emerging reflecting intelligent surface (RIS) technology promises to enhance the capacity of wireless communication systems via passive reflect beamforming.","However, the product path loss limits its performance gains.","Fully-connected (FC) active RIS, which integrates reflect-type power amplifiers into the RIS elements, has been recently introduced in response to this issue.","Also, sub-connected (SC) active RIS and hybrid FC-active/passive RIS variants, which employ a limited number of reflect-type power amplifiers, have been proposed to provide energy savings.","Nevertheless, their flexibility in balancing diverse capacity requirements and power consumption constraints is limited.","In this direction, this study introduces novel hybrid RIS structures, wherein at least one reflecting sub-surface (RS) adopts the SC-active RIS design.","The asymptotic signal-to-noise-ratio of the FC-active/passive and the proposed hybrid RIS variants is analyzed in a single-user single-input single-output setup.","Furthermore, the transmit and RIS beamforming weights are jointly optimized in each scenario to maximize the energy efficiency of a hybrid RIS-aided multi-user multiple-input single-output downlink system subject to the power consumption constraints of the base station and the active RSs.","Numerical simulation and analytic results highlight the performance gains of the proposed RIS designs over benchmarks, unveil non-trivial trade-offs, and provide valuable insights."],"url":"http://arxiv.org/abs/2402.11547v1","category":"cs.IT"}
{"created":"2024-02-18 10:44:48","title":"Question Answering Over Spatio-Temporal Knowledge Graph","abstract":"Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the question and retrieve accurate answers from the STKG. Through extensive experiments, we demonstrate the quality of our dataset and the effectiveness of our STKGQA method.","sentences":["Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information.","While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored.","Furthermore, a lack of comprehensive datasets also has hindered progress in this area.","To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA).","Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset.","In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx.","By extracting temporal and spatial information from a question, our QA model can better comprehend the question and retrieve accurate answers from the STKG.","Through extensive experiments, we demonstrate the quality of our dataset and the effectiveness of our STKGQA method."],"url":"http://arxiv.org/abs/2402.11542v1","category":"cs.CL"}
{"created":"2024-02-18 10:44:03","title":"Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought","abstract":"Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG. Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts. This counter-intuitive finding provides substantial insights for future research on LLMs' comprehension of structured knowledge.","sentences":["Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate.","Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge.","However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs.","In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge.","We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG.","Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts.","This counter-intuitive finding provides substantial insights for future research on LLMs' comprehension of structured knowledge."],"url":"http://arxiv.org/abs/2402.11541v1","category":"cs.CL"}
{"created":"2024-02-18 10:36:05","title":"Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning","abstract":"Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of LLMs.","sentences":["Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance.","However, the impact of each component of the pretraining corpus remains opaque.","As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal.","To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities.","Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships.","We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities.","These findings provide insights into the organization of data to support more efficient pretraining of LLMs."],"url":"http://arxiv.org/abs/2402.11537v1","category":"cs.CL"}
{"created":"2024-02-18 10:15:38","title":"PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability","abstract":"Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\\textbf{PreAct}$, an agent framework that integrates $\\textbf{pre}$diction with $\\textbf{rea}$soning and $\\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.","sentences":["Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction.","In this paper, we introduce $\\textbf{PreAct}$, an agent framework that integrates $\\textbf{pre}$diction with $\\textbf{rea}$soning and $\\textbf{act}$ion.","Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks.","Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods.","We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning.","The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct."],"url":"http://arxiv.org/abs/2402.11534v1","category":"cs.CL"}
{"created":"2024-02-18 10:10:40","title":"Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models","abstract":"Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual summarization, demonstrating the generalizability of CoI models on unseen composite downstream tasks.","sentences":["Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks.","However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a).","In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain.","Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached.","CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks.","CoI-tuned models also outperformed baseline models on multilingual summarization, demonstrating the generalizability of CoI models on unseen composite downstream tasks."],"url":"http://arxiv.org/abs/2402.11532v1","category":"cs.CL"}
{"created":"2024-02-18 09:51:49","title":"Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution","abstract":"Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \\textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \\textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions not trained with \\textit{RLHF}. Further analysis indicates that the model's language capabilities play a crucial role in preference learning. A reward model with strong language capabilities can more sensitively learn the subtle differences in translation quality and align better with real human translation preferences.","sentences":["Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation.","However, traditional metrics like \\textit{BLEU} do not strictly align with human preference of translation quality.","In this paper, we explore leveraging reinforcement learning with human feedback (\\textit{RLHF}) to improve translation quality.","It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages.","To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations.","In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation.","Experimental results demonstrate that \\textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions not trained with \\textit{RLHF}.","Further analysis indicates that the model's language capabilities play a crucial role in preference learning.","A reward model with strong language capabilities can more sensitively learn the subtle differences in translation quality and align better with real human translation preferences."],"url":"http://arxiv.org/abs/2402.11525v1","category":"cs.CL"}
{"created":"2024-02-18 09:46:51","title":"Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering","abstract":"While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings. These samples' impact depends on their similarities to the anchor node and the negative samples. Using the graph-based collaborative filtering model as our backbone and following the same data augmentation methods as the existing contrastive learning model SGL, we effectively enhance the performance of the recommendation model. Our proposed Neighborhood-Enhanced Supervised Contrastive Loss (NESCL) model substitutes the contrastive loss function in SGL with our novel loss function, showing marked performance improvement. On three real-world datasets, Yelp2018, Gowalla, and Amazon-Book, our model surpasses the original SGL by 10.09%, 7.09%, and 35.36% on NDCG@20, respectively.","sentences":["While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity.","Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this.","However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy.","In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function.","This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss.","We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings.","These samples' impact depends on their similarities to the anchor node and the negative samples.","Using the graph-based collaborative filtering model as our backbone and following the same data augmentation methods as the existing contrastive learning model SGL, we effectively enhance the performance of the recommendation model.","Our proposed Neighborhood-Enhanced Supervised Contrastive Loss (NESCL) model substitutes the contrastive loss function in SGL with our novel loss function, showing marked performance improvement.","On three real-world datasets, Yelp2018, Gowalla, and Amazon-Book, our model surpasses the original SGL by 10.09%, 7.09%, and 35.36% on NDCG@20, respectively."],"url":"http://arxiv.org/abs/2402.11523v1","category":"cs.IR"}
{"created":"2024-02-18 08:32:59","title":"Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources","abstract":"Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving up to a 3.1% average improvement in downstream NLP task performance. FlexLoRA's practicality is further underscored by its seamless integration with existing LoRA-based FL methods and theoretical analysis, offering a path toward scalable, privacy-preserving federated tuning for LLMs.","sentences":["Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs).","While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.","This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants.","FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge.","By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources.","Involving over 1,600 clients performing diverse NLP tasks, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving up to a 3.1% average improvement in downstream NLP task performance.","FlexLoRA's practicality is further underscored by its seamless integration with existing LoRA-based FL methods and theoretical analysis, offering a path toward scalable, privacy-preserving federated tuning for LLMs."],"url":"http://arxiv.org/abs/2402.11505v1","category":"cs.CL"}
{"created":"2024-02-18 08:13:25","title":"A Three-Party Repeated Coalition Formation Game for PLS in Wireless Communications with IRSs","abstract":"In this paper, a repeated coalition formation game (RCFG) with dynamic decision-making for physical layer security (PLS) in wireless communications with intelligent reflecting surfaces (IRSs) has been investigated. In the considered system, one central legitimate transmitter (LT) aims to transmit secret signals to a group of legitimate receivers (LRs) under the threat of a proactive eavesdropper (EV), while there exist a number of third-party IRSs (TIRSs) which can choose to form a coalition with either legitimate pairs (LPs) or the EV to improve their respective performances in exchange for potential benefits (e.g., payments). Unlike existing works that commonly restricted to friendly IRSs or malicious IRSs only, we study the complicated dynamic ally-adversary relationships among LPs, EV and TIRSs, under unpredictable wireless channel conditions, and introduce a RCFG to model their long-term strategic interactions. Particularly, we first analyze the existence of Nash equilibrium (NE) in the formulated RCFG, and then propose a switch operations-based coalition selection along with a deep reinforcement learning (DRL)-based algorithm for obtaining such equilibrium. Simulations examine the feasibility of the proposed algorithm and show its superiority over counterparts.","sentences":["In this paper, a repeated coalition formation game (RCFG) with dynamic decision-making for physical layer security (PLS) in wireless communications with intelligent reflecting surfaces (IRSs) has been investigated.","In the considered system, one central legitimate transmitter (LT) aims to transmit secret signals to a group of legitimate receivers (LRs) under the threat of a proactive eavesdropper (EV), while there exist a number of third-party IRSs (TIRSs) which can choose to form a coalition with either legitimate pairs (LPs) or the EV to improve their respective performances in exchange for potential benefits (e.g., payments).","Unlike existing works that commonly restricted to friendly IRSs or malicious IRSs only, we study the complicated dynamic ally-adversary relationships among LPs, EV and TIRSs, under unpredictable wireless channel conditions, and introduce a RCFG to model their long-term strategic interactions.","Particularly, we first analyze the existence of Nash equilibrium (NE) in the formulated RCFG, and then propose a switch operations-based coalition selection along with a deep reinforcement learning (DRL)-based algorithm for obtaining such equilibrium.","Simulations examine the feasibility of the proposed algorithm and show its superiority over counterparts."],"url":"http://arxiv.org/abs/2402.11500v1","category":"cs.GT"}
{"created":"2024-02-18 08:05:54","title":"Verifiably Following Complex Robot Instructions with Foundation Models","abstract":"Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that are correct-by-construction. We demonstrate LIMP in three real-world environments, across a set of 35 complex spatiotemporal instructions, showing the generality of our approach and the ease of deployment in novel unstructured domains. In our experiments, LIMP can spatially ground open-vocabulary referents and synthesize constraint-satisfying plans in 90% of object-goal navigation and 71% of mobile manipulation instructions. See supplementary videos at https://robotlimp.github.io","sentences":["Enabling robots to follow complex natural language instructions is an important yet challenging problem.","People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots.","Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world.","We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints.","In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that are correct-by-construction.","We demonstrate LIMP in three real-world environments, across a set of 35 complex spatiotemporal instructions, showing the generality of our approach and the ease of deployment in novel unstructured domains.","In our experiments, LIMP can spatially ground open-vocabulary referents and synthesize constraint-satisfying plans in 90% of object-goal navigation and 71% of mobile manipulation instructions.","See supplementary videos at https://robotlimp.github.io"],"url":"http://arxiv.org/abs/2402.11498v1","category":"cs.RO"}
{"created":"2024-02-18 07:56:29","title":"Thyroid ultrasound diagnosis improvement via multi-view self-supervised learning and two-stage pre-training","abstract":"Thyroid nodule classification and segmentation in ultrasound images are crucial for computer-aided diagnosis; however, they face limitations owing to insufficient labeled data. In this study, we proposed a multi-view contrastive self-supervised method to improve thyroid nodule classification and segmentation performance with limited manual labels. Our method aligns the transverse and longitudinal views of the same nodule, thereby enabling the model to focus more on the nodule area. We designed an adaptive loss function that eliminates the limitations of the paired data. Additionally, we adopted a two-stage pre-training to exploit the pre-training on ImageNet and thyroid ultrasound images. Extensive experiments were conducted on a large-scale dataset collected from multiple centers. The results showed that the proposed method significantly improves nodule classification and segmentation performance with limited manual labels and outperforms state-of-the-art self-supervised methods. The two-stage pre-training also significantly exceeded ImageNet pre-training.","sentences":["Thyroid nodule classification and segmentation in ultrasound images are crucial for computer-aided diagnosis; however, they face limitations owing to insufficient labeled data.","In this study, we proposed a multi-view contrastive self-supervised method to improve thyroid nodule classification and segmentation performance with limited manual labels.","Our method aligns the transverse and longitudinal views of the same nodule, thereby enabling the model to focus more on the nodule area.","We designed an adaptive loss function that eliminates the limitations of the paired data.","Additionally, we adopted a two-stage pre-training to exploit the pre-training on ImageNet and thyroid ultrasound images.","Extensive experiments were conducted on a large-scale dataset collected from multiple centers.","The results showed that the proposed method significantly improves nodule classification and segmentation performance with limited manual labels and outperforms state-of-the-art self-supervised methods.","The two-stage pre-training also significantly exceeded ImageNet pre-training."],"url":"http://arxiv.org/abs/2402.11497v1","category":"cs.CV"}
{"created":"2024-02-18 07:42:49","title":"What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs","abstract":"Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.","sentences":["Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment.","Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents.","In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning.","Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology.","Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup.","Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners."],"url":"http://arxiv.org/abs/2402.11489v1","category":"cs.CL"}
{"created":"2024-02-18 07:24:34","title":"LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation","abstract":"Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.","sentences":["Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer.","However, existing language adaptation methods often overlook the benefits of cross-lingual supervision.","In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages.","This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling.","We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages.","The source code is available at https://github.com/studio-ousia/leia."],"url":"http://arxiv.org/abs/2402.11485v1","category":"cs.CL"}
{"created":"2024-02-18 07:17:29","title":"A Fisher Information based Receding Horizon Control Method for Signal Strength Model Estimation","abstract":"This paper considers the problem of localizing a set of nodes in a wireless sensor network when both their positions and the parameters of the communication model are unknown. We assume that a single agent moves through the environment, taking measurements of the Received Signal Strength (RSS), and seek a controller that optimizes a performance metric based on the Fisher Information Matrix (FIM). We develop a receding horizon (RH) approach that alternates between estimating the parameter values (using a maximum likelihood estimator) and determining where to move so as to maximally inform the estimation problem. The receding horizon controller solves a multi-stage look ahead problem to determine the next control to be applied, executes the move, collects the next measurement, and then re-estimates the parameters before repeating the sequence. We consider both a Dynamic Programming (DP) approach to solving the optimal control problem at each step, and a simplified heuristic based on a pruning algorithm that significantly reduces the computational complexity. We also consider a modified cost function that seeks to balance the information acquired about each of the parameters to ensure the controller does not focus on a single value in its optimization. These approaches are compared against two baselines, one based on a purely random trajectory and one on a greedy control solution. The simulations indicate our RH schemes outperform the baselines, while the pruning algorithm produces significant reductions in computation time with little effect on overall performance.","sentences":["This paper considers the problem of localizing a set of nodes in a wireless sensor network when both their positions and the parameters of the communication model are unknown.","We assume that a single agent moves through the environment, taking measurements of the Received Signal Strength (RSS), and seek a controller that optimizes a performance metric based on the Fisher Information Matrix (FIM).","We develop a receding horizon (RH) approach that alternates between estimating the parameter values (using a maximum likelihood estimator) and determining where to move so as to maximally inform the estimation problem.","The receding horizon controller solves a multi-stage look ahead problem to determine the next control to be applied, executes the move, collects the next measurement, and then re-estimates the parameters before repeating the sequence.","We consider both a Dynamic Programming (DP) approach to solving the optimal control problem at each step, and a simplified heuristic based on a pruning algorithm that significantly reduces the computational complexity.","We also consider a modified cost function that seeks to balance the information acquired about each of the parameters to ensure the controller does not focus on a single value in its optimization.","These approaches are compared against two baselines, one based on a purely random trajectory and one on a greedy control solution.","The simulations indicate our RH schemes outperform the baselines, while the pruning algorithm produces significant reductions in computation time with little effect on overall performance."],"url":"http://arxiv.org/abs/2402.11483v1","category":"eess.SY"}
{"created":"2024-02-18 06:22:01","title":"DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning","abstract":"Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issues by leveraging the intrinsic knowledge from pre-trained models, which can be efficiently deployed with minimal downstream data. Specifically, to solve the first challenge, DDIPrompt employs augmented links between drugs, considering both structural and interactive proximity. It features a hierarchical pre-training strategy that comprehends intra-molecular structures and inter-molecular interactions, fostering a comprehensive and unbiased understanding of drug properties. For the second challenge, we implement a prototype-enhanced prompting mechanism during inference. This mechanism, refined by few-shot examples from each category, effectively harnesses the rich pre-training knowledge to enhance prediction accuracy, particularly for these rare but crucial interactions. Comprehensive evaluations on two benchmark datasets demonstrate the superiority of DDIPrompt, particularly in predicting rare DDI events.","sentences":["Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules.","However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented.","This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions.","(2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data.","In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting.","Our framework aims to address these issues by leveraging the intrinsic knowledge from pre-trained models, which can be efficiently deployed with minimal downstream data.","Specifically, to solve the first challenge, DDIPrompt employs augmented links between drugs, considering both structural and interactive proximity.","It features a hierarchical pre-training strategy that comprehends intra-molecular structures and inter-molecular interactions, fostering a comprehensive and unbiased understanding of drug properties.","For the second challenge, we implement a prototype-enhanced prompting mechanism during inference.","This mechanism, refined by few-shot examples from each category, effectively harnesses the rich pre-training knowledge to enhance prediction accuracy, particularly for these rare but crucial interactions.","Comprehensive evaluations on two benchmark datasets demonstrate the superiority of DDIPrompt, particularly in predicting rare DDI events."],"url":"http://arxiv.org/abs/2402.11472v1","category":"q-bio.BM"}
{"created":"2024-02-18 05:58:25","title":"A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models","abstract":"Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \\textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness. Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under adversarial training, and (d) robust to statistical randomness. Our code will be publicly available.","sentences":["Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations.","Traditional adversarial evaluation is often done \\textit{only after} fine-tuning the models and ignoring the training data.","In this paper, we want to prove that there is also a strong correlation between training data and model robustness.","To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models.","Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument.","First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness.","Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under adversarial training, and (d) robust to statistical randomness.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2402.11469v1","category":"cs.LG"}
{"created":"2024-02-18 05:35:01","title":"Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective","abstract":"In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \\textbf{\\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.","sentences":["In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities.","Recognizing the chaotic nature of real-world data, our model, \\textbf{\\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems.","Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy.","Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets."],"url":"http://arxiv.org/abs/2402.11463v1","category":"cs.LG"}
{"created":"2024-02-18 05:23:15","title":"FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network","abstract":"Geometry problem solving has always been a long-standing challenge in the fields of automated reasoning and artificial intelligence. This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive reasoning. The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational reasoning and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance. The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-Apply Cycle to ultimately achieve readable and traceable automatic solving of geometric problems. Experiments demonstrate the correctness and effectiveness of this neural-symbolic architecture. We achieved a step-wised accuracy of 87.65% and an overall accuracy of 85.53% on the formalgeo7k datasets. The code and data is available at https://github.com/BitSecret/HyperGNet.","sentences":["Geometry problem solving has always been a long-standing challenge in the fields of automated reasoning and artificial intelligence.","This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive reasoning.","The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational reasoning and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges.","The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance.","The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-Apply Cycle to ultimately achieve readable and traceable automatic solving of geometric problems.","Experiments demonstrate the correctness and effectiveness of this neural-symbolic architecture.","We achieved a step-wised accuracy of 87.65% and an overall accuracy of 85.53% on the formalgeo7k datasets.","The code and data is available at https://github.com/BitSecret/HyperGNet."],"url":"http://arxiv.org/abs/2402.11461v1","category":"cs.AI"}
{"created":"2024-02-18 05:04:50","title":"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge","abstract":"Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods.","sentences":["Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging.","While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions.","To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds.","Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process.","Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods."],"url":"http://arxiv.org/abs/2402.11459v1","category":"q-bio.BM"}
{"created":"2024-02-18 04:19:44","title":"SciAgent: Tool-augmented Language Models for Scientific Reasoning","abstract":"Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.","sentences":["Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs).","To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning.","This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user.","To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools.","Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving.","Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance.","Extensive experiments on SciToolBench confirm the effectiveness of SciAgent.","Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy.","Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT."],"url":"http://arxiv.org/abs/2402.11451v1","category":"cs.CL"}
{"created":"2024-02-18 03:50:34","title":"Gauging Public Acceptance of Conditionally Automated Cars in the United States","abstract":"In this work we look at an element of smart cities, conditionally automated cars (SAE Level 3), investigating the factors influencing public acceptance in the United States. We apply an adaptation of the UTUAT2 model. Taking an experimental approach study 358 participants in the US were presented with a vignette outlining the L3 technology followed by a series of questions to capture their perceptions of conditionally automated cars. PLS-SEM was used to analyze the collected data. The results reveal that the acceptance of the technology, in order of decreasing importance, was determined by social influence, performance expectancy, hedonic motivation, facilitating conditions, and effort expectancy. Furthermore, hedonic motivation, social influence, facilitating conditions and effort expectancy all have a positive influence on the perception of how useful the technology is; facilitating conditions, hedonic motivation, and social influence all have a positive influence on effort expectancy; social influence and facilitating conditions positively influence hedonic motivation; and social influence positively influences facilitating conditions. A moderating effect for gender was found, with the effect of hedonic motivation influencing intention to adopt is more prominent for men.","sentences":["In this work we look at an element of smart cities, conditionally automated cars (SAE Level 3), investigating the factors influencing public acceptance in the United States.","We apply an adaptation of the UTUAT2 model.","Taking an experimental approach study 358 participants in the US were presented with a vignette outlining the L3 technology followed by a series of questions to capture their perceptions of conditionally automated cars.","PLS-SEM was used to analyze the collected data.","The results reveal that the acceptance of the technology, in order of decreasing importance, was determined by social influence, performance expectancy, hedonic motivation, facilitating conditions, and effort expectancy.","Furthermore, hedonic motivation, social influence, facilitating conditions and effort expectancy all have a positive influence on the perception of how useful the technology is; facilitating conditions, hedonic motivation, and social influence all have a positive influence on effort expectancy; social influence and facilitating conditions positively influence hedonic motivation; and social influence positively influences facilitating conditions.","A moderating effect for gender was found, with the effect of hedonic motivation influencing intention to adopt is more prominent for men."],"url":"http://arxiv.org/abs/2402.11444v1","category":"cs.CY"}
{"created":"2024-02-18 03:36:26","title":"InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration","abstract":"Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs demonstrate that InfuserKI can effectively acquire new knowledge and outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge forgetting.","sentences":["Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks.","To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules.","However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning.","Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge.","Injecting new knowledge poses the risk of forgetting previously acquired knowledge.","To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting.","Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs demonstrate that InfuserKI can effectively acquire new knowledge and outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge forgetting."],"url":"http://arxiv.org/abs/2402.11441v1","category":"cs.CL"}
{"created":"2024-02-18 03:10:39","title":"Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models","abstract":"Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.","sentences":["Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks.","We discovered that such a contrary is due to LLM's bias towards their own output.","In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics.","We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks.","We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks.","Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.","To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks."],"url":"http://arxiv.org/abs/2402.11436v1","category":"cs.CL"}
{"created":"2024-02-18 02:55:19","title":"Improved Indoor Localization with Machine Learning Techniques for IoT applications","abstract":"The rise of the Internet of Things (IoT) and mobile internet applications has spurred interest in location-based services (LBS) for commercial, military, and social applications. While the global positioning system (GPS) dominates outdoor localization, its efficacy wanes indoors due to signal challenges. Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee, Bluetooth, UWB, selecting based on context. Received signal strength indicator (RSSI) technology, known for its accuracy and simplicity, is widely adopted. This study employs machine learning algorithms in three phases: supervised regressors, supervised classifiers, and ensemble methods for RSSI-based indoor localization. Additionally, it introduces a weighted least squares technique and pseudo-linear solution approach to address non-linear RSSI measurement equations by approximating them with linear equations. An experimental testbed, utilizing diverse wireless technologies and anchor nodes, is designed for data collection, employing IoT cloud architectures. Pre-processing involves investigating filters for data refinement before algorithm training. The study employs machine learning models like linear regression, polynomial regression, support vector regression, random forest regression, and decision tree regressor across various wireless technologies. These models estimate the geographical coordinates of a moving target node, and their performance is evaluated using metrics such as accuracy, root mean square errors, precision, recall, sensitivity, coefficient of determinant, and the f1-score. The experiment's outcomes provide insights into the effectiveness of different supervised machine learning techniques in terms of localization accuracy and robustness in indoor environments.","sentences":["The rise of the Internet of Things (IoT) and mobile internet applications has spurred interest in location-based services (LBS) for commercial, military, and social applications.","While the global positioning system (GPS) dominates outdoor localization, its efficacy wanes indoors due to signal challenges.","Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee, Bluetooth, UWB, selecting based on context.","Received signal strength indicator (RSSI) technology, known for its accuracy and simplicity, is widely adopted.","This study employs machine learning algorithms in three phases: supervised regressors, supervised classifiers, and ensemble methods for RSSI-based indoor localization.","Additionally, it introduces a weighted least squares technique and pseudo-linear solution approach to address non-linear RSSI measurement equations by approximating them with linear equations.","An experimental testbed, utilizing diverse wireless technologies and anchor nodes, is designed for data collection, employing IoT cloud architectures.","Pre-processing involves investigating filters for data refinement before algorithm training.","The study employs machine learning models like linear regression, polynomial regression, support vector regression, random forest regression, and decision tree regressor across various wireless technologies.","These models estimate the geographical coordinates of a moving target node, and their performance is evaluated using metrics such as accuracy, root mean square errors, precision, recall, sensitivity, coefficient of determinant, and the f1-score.","The experiment's outcomes provide insights into the effectiveness of different supervised machine learning techniques in terms of localization accuracy and robustness in indoor environments."],"url":"http://arxiv.org/abs/2402.11433v1","category":"cs.LG"}
{"created":"2024-02-18 02:52:54","title":"Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning","abstract":"Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive parts. This dataset can also be used to evaluate the complex reasoning capability of current large language models and serve as a reasoning benchmark for further research.","sentences":["Deception detection has attracted increasing attention due to its importance in many practical scenarios.","Currently, data scarcity harms the development of this field.","On the one hand, it is costly to hire participants to simulate deception scenarios.","On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet.","To address data scarcity, this paper proposes a new data collection pipeline.","Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer.","During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence.","Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size.","Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive parts.","This dataset can also be used to evaluate the complex reasoning capability of current large language models and serve as a reasoning benchmark for further research."],"url":"http://arxiv.org/abs/2402.11432v1","category":"cs.CL"}
{"created":"2024-02-18 02:38:58","title":"Deformable Object Manipulation With Constraints Using Path Set Planning and Tracking","abstract":"In robotic deformable object manipulation (DOM) applications, constraints arise commonly from environments and task-specific requirements. Enabling DOM with constraints is therefore crucial for its deployment in practice. However, dealing with constraints turns out to be challenging due to many inherent factors such as inaccessible deformation models of deformable objects (DOs) and varying environmental setups. This article presents a systematic manipulation framework for DOM subject to constraints by proposing a novel path set planning and tracking scheme. First, constrained DOM tasks are formulated into a versatile optimization formalism which enables dynamic constraint imposition. Because of the lack of the local optimization objective and high state dimensionality, the formulated problem is not analytically solvable. To address this, planning of the path set, which collects paths of DO feedback points, is proposed subsequently to offer feasible path and motion references for DO in constrained setups. Both theoretical analyses and computationally efficient algorithmic implementation of path set planning are discussed. Lastly, a control architecture combining path set tracking and constraint handling is designed for task execution. The effectiveness of our methods is validated in a variety of DOM tasks with constrained experimental settings.","sentences":["In robotic deformable object manipulation (DOM) applications, constraints arise commonly from environments and task-specific requirements.","Enabling DOM with constraints is therefore crucial for its deployment in practice.","However, dealing with constraints turns out to be challenging due to many inherent factors such as inaccessible deformation models of deformable objects (DOs) and varying environmental setups.","This article presents a systematic manipulation framework for DOM subject to constraints by proposing a novel path set planning and tracking scheme.","First, constrained DOM tasks are formulated into a versatile optimization formalism which enables dynamic constraint imposition.","Because of the lack of the local optimization objective and high state dimensionality, the formulated problem is not analytically solvable.","To address this, planning of the path set, which collects paths of DO feedback points, is proposed subsequently to offer feasible path and motion references for DO in constrained setups.","Both theoretical analyses and computationally efficient algorithmic implementation of path set planning are discussed.","Lastly, a control architecture combining path set tracking and constraint handling is designed for task execution.","The effectiveness of our methods is validated in a variety of DOM tasks with constrained experimental settings."],"url":"http://arxiv.org/abs/2402.11429v1","category":"cs.RO"}
{"created":"2024-02-18 02:19:02","title":"OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations","abstract":"First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming that estimation errors diminish to zero as historical gradients accumulate and that SGD-based OptEx enjoys an effective acceleration rate of $\\Omega(\\sqrt{N})$ over standard SGD given parallelism of N. We also use extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training across various datasets, to underscore the substantial efficiency improvements achieved by OptEx.","sentences":["First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising.","However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence.","In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck.","OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO.","We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming that estimation errors diminish to zero as historical gradients accumulate and that SGD-based OptEx enjoys an effective acceleration rate of $\\Omega(\\sqrt{N})$ over standard SGD given parallelism of N.","We also use extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training across various datasets, to underscore the substantial efficiency improvements achieved by OptEx."],"url":"http://arxiv.org/abs/2402.11427v1","category":"cs.LG"}
{"created":"2024-02-18 01:54:28","title":"Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition","abstract":"In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showcase that D$^3$GZSL elevates the performance of existing generative GZSL methods, underscoring its potential to refine zero-shot learning practices.The code is available at: https://github.com/PJBQ/D3GZSL.git","sentences":["In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data.","To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL.","This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model.","D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD).","ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence.","O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories.","Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks.","Extensive experiments consistently showcase that D$^3$GZSL elevates the performance of existing generative GZSL methods, underscoring its potential to refine zero-shot learning practices.","The code is available at: https://github.com/PJBQ/D3GZSL.git"],"url":"http://arxiv.org/abs/2402.11424v1","category":"cs.CV"}
{"created":"2024-02-18 01:43:39","title":"Analysis of Fatigue-Induced Compensatory Movements in Bicep Curls: Gaining Insights for the Deployment of Wearable Sensors","abstract":"A common challenge in Bicep Curls rehabilitation is muscle compensation, where patients adopt alternative movement patterns when the primary muscle group cannot act due to injury or fatigue, significantly decreasing the effectiveness of rehabilitation efforts. The problem is exacerbated by the growing trend toward transitioning from in-clinic to home-based rehabilitation, where constant monitoring and correction by physiotherapists are limited. To address this challenge, developing wearable sensors capable of detecting muscle compensation becomes crucial. This study aims to gain insights for the optimal deployment of wearable sensors through a comprehensive study of muscle compensation in Bicep Curls. We collect upper limb joint kinematics and surface electromyography signals (sEMG) from eight muscles in 12 healthy subjects during standard and fatigue stages. Two muscle synergies are derived from sEMG signals and are analyzed comprehensively along with joint kinematics. Our findings reveal a shift in the relative contribution of forearm muscles to shoulder muscles, accompanied by a significant increase in activation amplitude for both synergies. Additionally, more pronounced movement was observed at the shoulder joint during fatigue. These results suggest focusing on the should muscle activities and joint motions when deploying wearable sensors for effective detection of compensatory movements.","sentences":["A common challenge in Bicep Curls rehabilitation is muscle compensation, where patients adopt alternative movement patterns when the primary muscle group cannot act due to injury or fatigue, significantly decreasing the effectiveness of rehabilitation efforts.","The problem is exacerbated by the growing trend toward transitioning from in-clinic to home-based rehabilitation, where constant monitoring and correction by physiotherapists are limited.","To address this challenge, developing wearable sensors capable of detecting muscle compensation becomes crucial.","This study aims to gain insights for the optimal deployment of wearable sensors through a comprehensive study of muscle compensation in Bicep Curls.","We collect upper limb joint kinematics and surface electromyography signals (sEMG) from eight muscles in 12 healthy subjects during standard and fatigue stages.","Two muscle synergies are derived from sEMG signals and are analyzed comprehensively along with joint kinematics.","Our findings reveal a shift in the relative contribution of forearm muscles to shoulder muscles, accompanied by a significant increase in activation amplitude for both synergies.","Additionally, more pronounced movement was observed at the shoulder joint during fatigue.","These results suggest focusing on the should muscle activities and joint motions when deploying wearable sensors for effective detection of compensatory movements."],"url":"http://arxiv.org/abs/2402.11421v1","category":"cs.RO"}
{"created":"2024-02-18 01:26:45","title":"Capturing many-body correlation effects with quantum and classical computing","abstract":"Theoretical descriptions of excited states of molecular systems in high-energy regimes are crucial for supporting and driving many experimental efforts at light source facilities. However, capturing their complicated correlation effects requires formalisms that provide a hierarchical infrastructure of approximations. These approximations lead to an increased overhead in classical computing methods, and therefore, decisions regarding the ranking of approximations and the quality of results must be made on purely numerical grounds. The emergence of quantum computing methods has the potential to change this situation. In this study, we demonstrate the efficiency of Quantum Phase Estimator (QPE) in identifying core-level states relevant to x-ray photoelectron spectroscopy. We compare and validate the QPE predictions with exact diagonalization and real-time equation-of-motion coupled cluster formulations, which are some of the most accurate methods for states dominated by collective correlation effects.","sentences":["Theoretical descriptions of excited states of molecular systems in high-energy regimes are crucial for supporting and driving many experimental efforts at light source facilities.","However, capturing their complicated correlation effects requires formalisms that provide a hierarchical infrastructure of approximations.","These approximations lead to an increased overhead in classical computing methods, and therefore, decisions regarding the ranking of approximations and the quality of results must be made on purely numerical grounds.","The emergence of quantum computing methods has the potential to change this situation.","In this study, we demonstrate the efficiency of Quantum Phase Estimator (QPE) in identifying core-level states relevant to x-ray photoelectron spectroscopy.","We compare and validate the QPE predictions with exact diagonalization and real-time equation-of-motion coupled cluster formulations, which are some of the most accurate methods for states dominated by collective correlation effects."],"url":"http://arxiv.org/abs/2402.11418v1","category":"quant-ph"}
{"created":"2024-02-19 18:59:50","title":"Observation of a phase transition from a continuous to a discrete time crystal","abstract":"Discrete (DTCs) and continuous time crystals (CTCs) are novel dynamical many-body states, that are characterized by robust self-sustained oscillations, emerging via spontaneous breaking of discrete or continuous time translation symmetry. DTCs are periodically driven systems that oscillate with a subharmonic of the drive, while CTCs are driven continuously and oscillate with a system inherent frequency. Here, we explore a phase transition from a continuous time crystal to a discrete time crystal. A CTC with a characteristic oscillation frequency $\\omega_\\mathrm{CTC}$ is prepared in a continuously pumped atom-cavity system. Modulating the pump intensity of the CTC with a frequency $\\omega_{\\mathrm{dr}}$ close to $2\\,\\omega_\\mathrm{CTC}$ leads to robust locking of $\\omega_\\mathrm{CTC}$ to $\\omega_{\\mathrm{dr}}/2$, and hence a DTC arises. This phase transition in a quantum many-body system is related to subharmonic injection locking of non-linear mechanical and electronic oscillators or lasers.","sentences":["Discrete (DTCs) and continuous time crystals (CTCs) are novel dynamical many-body states, that are characterized by robust self-sustained oscillations, emerging via spontaneous breaking of discrete or continuous time translation symmetry.","DTCs are periodically driven systems that oscillate with a subharmonic of the drive, while CTCs are driven continuously and oscillate with a system inherent frequency.","Here, we explore a phase transition from a continuous time crystal to a discrete time crystal.","A CTC with a characteristic oscillation frequency $\\omega_\\mathrm{CTC}$ is prepared in a continuously pumped atom-cavity system.","Modulating the pump intensity of the CTC with a frequency $\\omega_{\\mathrm{dr}}$ close to $2\\,\\omega_\\mathrm{CTC}$ leads to robust locking of $\\omega_\\mathrm{CTC}$ to $\\omega_{\\mathrm{dr}}/2$, and hence a DTC arises.","This phase transition in a quantum many-body system is related to subharmonic injection locking of non-linear mechanical and electronic oscillators or lasers."],"url":"http://arxiv.org/abs/2402.12378v1","category":"quant-ph"}
{"created":"2024-02-19 18:49:31","title":"Critical crack-length during fracture","abstract":"Through controlled numerical simulations in a one dimensional fiber bundle model with local stress concentration, we established an inverse correlation between the strength of the material and the cracks which grow inside it - both the maximum crack and the one that set in instability within the system, defined to be the critical crack. Through Pearson correlation function as well as probabilistic study of individual configurations, we found that the maximum and the critical crack often differ from each other unless the disorder strength is extremely low. A phase diagram on the plane of disorder vs system size demarcates between the regions where the largest crack is the most vulnerable one and where they differ from each other but still shows moderate correlation.","sentences":["Through controlled numerical simulations in a one dimensional fiber bundle model with local stress concentration, we established an inverse correlation between the strength of the material and the cracks which grow inside it - both the maximum crack and the one that set in instability within the system, defined to be the critical crack.","Through Pearson correlation function as well as probabilistic study of individual configurations, we found that the maximum and the critical crack often differ from each other unless the disorder strength is extremely low.","A phase diagram on the plane of disorder vs system size demarcates between the regions where the largest crack is the most vulnerable one and where they differ from each other but still shows moderate correlation."],"url":"http://arxiv.org/abs/2402.12362v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 18:40:06","title":"Epitaxial rare-earth doped complex oxide thin films for infrared applications","abstract":"Rare earth dopants are one of the most extensively studied optical emission centers for a broad range of applications such as laser optoelectronics, sensing, lighting, and quantum information technologies due to their narrow optical linewidth and exceptional coherence properties. Epitaxial doped oxide thin films can serve as a promising and controlled host to investigate rare-earth dopants suitable for scalable quantum memories, on-chip lasers and amplifiers. Here, we report high-quality epitaxial thin films of Tm-doped CaZrO$_3$ grown by pulsed laser deposition for infrared optoelectronic and quantum memory applications. We perform extensive structural and chemical characterization to probe the crystallinity of the films and the doping behavior. Low temperature photoluminescence measurements show sharp radiative transitions in the short-wave infrared range of 1.75 - 2 \\mu m.","sentences":["Rare earth dopants are one of the most extensively studied optical emission centers for a broad range of applications such as laser optoelectronics, sensing, lighting, and quantum information technologies due to their narrow optical linewidth and exceptional coherence properties.","Epitaxial doped oxide thin films can serve as a promising and controlled host to investigate rare-earth dopants suitable for scalable quantum memories, on-chip lasers and amplifiers.","Here, we report high-quality epitaxial thin films of Tm-doped CaZrO$_3$ grown by pulsed laser deposition for infrared optoelectronic and quantum memory applications.","We perform extensive structural and chemical characterization to probe the crystallinity of the films and the doping behavior.","Low temperature photoluminescence measurements show sharp radiative transitions in the short-wave infrared range of 1.75 - 2 \\mu m."],"url":"http://arxiv.org/abs/2402.12358v1","category":"physics.optics"}
{"created":"2024-02-19 18:28:02","title":"Rational powers, invariant ideals, and the summation formula","abstract":"We study the Rees valuations and rational powers of several classes of invariant ideals, providing explicit descriptions in terms of certain polyhedra. This allows us to show that a version of Musta\\c{t}\\u{a}-Takagi's summation formula for rational powers holds for these ideals. Moreover, for arbitrary ideals over the complex numbers, we prove a weaker version of this formula that holds for high enough rational numbers. Our methods lead to a definition of rational powers of ideal sheaves in normal complex varieties inspired by work of Lejeune-Jalabert-Teissier and Lazarsfeld.","sentences":["We study the Rees valuations and rational powers of several classes of invariant ideals, providing explicit descriptions in terms of certain polyhedra.","This allows us to show that a version of Musta\\c{t}\\u{a}-Takagi's summation formula for rational powers holds for these ideals.","Moreover, for arbitrary ideals over the complex numbers, we prove a weaker version of this formula that holds for high enough rational numbers.","Our methods lead to a definition of rational powers of ideal sheaves in normal complex varieties inspired by work of Lejeune-Jalabert-Teissier and Lazarsfeld."],"url":"http://arxiv.org/abs/2402.12350v1","category":"math.AC"}
{"created":"2024-02-19 18:13:37","title":"Designed spin-texture-lattice to control anisotropic magnon transport in antiferromagnets","abstract":"Spin waves in magnetic materials are promising information carriers for future computing technologies due to their ultra-low energy dissipation and long coherence length. Antiferromagnets are strong candidate materials due, in part, to their stability to external fields and larger group velocities. Multiferroic aniferromagnets, such as BiFeO$_3$ (BFO), have an additional degree of freedom stemming from magnetoelectric coupling, allowing for control of the magnetic structure, and thus spin waves, with electric field. Unfortunately, spin-wave propagation in BFO is not well understood due to the complexity of the magnetic structure. In this work, we explore long-range spin transport within an epitaxially engineered, electrically tunable, one-dimensional (1D) magnonic crystal. We discover a striking anisotropy in the spin transport parallel and perpendicular to the 1D crystal axis. Multiscale theory and simulation suggests that this preferential magnon conduction emerges from a combination of a population imbalance in its dispersion, as well as anisotropic structural scattering. This work provides a pathway to electrically-reconfigurable magnonic crystals in antiferromagnets.","sentences":["Spin waves in magnetic materials are promising information carriers for future computing technologies due to their ultra-low energy dissipation and long coherence length.","Antiferromagnets are strong candidate materials due, in part, to their stability to external fields and larger group velocities.","Multiferroic aniferromagnets, such as BiFeO$_3$ (BFO), have an additional degree of freedom stemming from magnetoelectric coupling, allowing for control of the magnetic structure, and thus spin waves, with electric field.","Unfortunately, spin-wave propagation in BFO is not well understood due to the complexity of the magnetic structure.","In this work, we explore long-range spin transport within an epitaxially engineered, electrically tunable, one-dimensional (1D) magnonic crystal.","We discover a striking anisotropy in the spin transport parallel and perpendicular to the 1D crystal axis.","Multiscale theory and simulation suggests that this preferential magnon conduction emerges from a combination of a population imbalance in its dispersion, as well as anisotropic structural scattering.","This work provides a pathway to electrically-reconfigurable magnonic crystals in antiferromagnets."],"url":"http://arxiv.org/abs/2402.12341v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 18:13:09","title":"Simple Mechanisms for Utility Maximization: Approximating Welfare in the I.I.D. Unit-Demand Setting","abstract":"We investigate the objective of utility maximization from the perspective of Bayesian mechanism design, initiating this direction, and focus on the unit-demand setting where values are i.i.d. across both items and buyers. We take the approach of developing simple, approximately optimal mechanisms, targeting the simplest benchmark of optimal welfare. We give a $(1-1/e)$-approximation when there are more items than buyers, and an $O(\\log(n/m))$-approximation when there are more buyers than items, which is tight up to constant factors. We also characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself.","sentences":["We investigate the objective of utility maximization from the perspective of Bayesian mechanism design, initiating this direction, and focus on the unit-demand setting where values are i.i.d. across both items and buyers.","We take the approach of developing simple, approximately optimal mechanisms, targeting the simplest benchmark of optimal welfare.","We give a $(1-1/e)$-approximation when there are more items than buyers, and an $O(\\log(n/m))$-approximation when there are more buyers than items, which is tight up to constant factors.","We also characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself."],"url":"http://arxiv.org/abs/2402.12340v1","category":"cs.GT"}
{"created":"2024-02-19 17:28:38","title":"A Digital Silicon Photomultiplier","abstract":"Silicon Photomultipliers (SiPMs) are state-of-the-art photon detectors used in particle physics, medical imaging, and beyond. They are sensitive to individual photons in the optical wavelength regime and achieve time resolutions of a few tens of picoseconds, which makes them interesting candidates for timing detectors in tracking systems for particle physics experiments. The Geiger discharges triggered in the sensitive elements of a SiPM, Single-Photon Avalanche Diodes (SPADs), yield signal amplitudes independent of the energy deposited by a photon or ionizing particle. This intrinsically digital nature of the signal motivates its digitization already on SPAD level.   A digital SiPM (dSiPM) was designed at Deutsches Elektronen Synchrotron (DESY), combining a SPAD array with embedded CMOS circuitry for on-chip signal processing. A key feature of the DESY dSiPM is its capability to provide hit-position information on pixel level, and one hit time stamp per quadrant at a 3 MHz readout-frame rate. The pixels comprise four SPADs and have a pitch of about 70 um. The four time stamps are provided by 12 bit Time-to-Digital Converters (TDCs) with a resolution better than 100 ps.   The chip was characterized in the laboratory to determine dark count rate, breakdown voltage, and TDC characteristics. Test-beam measurements are analyzed to assess the DESY dSiPMs performance in the context of a 4D-tracking applications. The results demonstrate a spatial hit resolution on a pixel level, a minimum-ionizing particle detection efficiency of about 30 % and a time resolution in the order of 50 ps.","sentences":["Silicon Photomultipliers (SiPMs) are state-of-the-art photon detectors used in particle physics, medical imaging, and beyond.","They are sensitive to individual photons in the optical wavelength regime and achieve time resolutions of a few tens of picoseconds, which makes them interesting candidates for timing detectors in tracking systems for particle physics experiments.","The Geiger discharges triggered in the sensitive elements of a SiPM, Single-Photon Avalanche Diodes (SPADs), yield signal amplitudes independent of the energy deposited by a photon or ionizing particle.","This intrinsically digital nature of the signal motivates its digitization already on SPAD level.   ","A digital SiPM (dSiPM) was designed at Deutsches Elektronen Synchrotron (DESY), combining a SPAD array with embedded CMOS circuitry for on-chip signal processing.","A key feature of the DESY dSiPM is its capability to provide hit-position information on pixel level, and one hit time stamp per quadrant at a 3 MHz readout-frame rate.","The pixels comprise four SPADs and have a pitch of about 70 um.","The four time stamps are provided by 12 bit Time-to-Digital Converters (TDCs) with a resolution better than 100 ps.   ","The chip was characterized in the laboratory to determine dark count rate, breakdown voltage, and TDC characteristics.","Test-beam measurements are analyzed to assess the DESY dSiPMs performance in the context of a 4D-tracking applications.","The results demonstrate a spatial hit resolution on a pixel level, a minimum-ionizing particle detection efficiency of about 30 % and a time resolution in the order of 50 ps."],"url":"http://arxiv.org/abs/2402.12305v1","category":"physics.ins-det"}
{"created":"2024-02-19 17:24:23","title":"Time-periodic behaviour in one- and two-dimensional interacting particle systems","abstract":"We provide a class of examples of interacting particle systems on $\\mathbb{Z}^d$, for $d\\in\\{1,2\\}$, that admit a unique translation-invariant stationary measure, which is not the long-time limit of all translation-invariant starting measures, due to the existence of time-periodic orbits in the associated measure-valued dynamics. This is the first such example and shows that even in low dimensions, not every limit point of the measure-valued dynamics needs to be a time-stationary measure.","sentences":["We provide a class of examples of interacting particle systems on $\\mathbb{Z}^d$, for $d\\in\\{1,2\\}$, that admit a unique translation-invariant stationary measure, which is not the long-time limit of all translation-invariant starting measures, due to the existence of time-periodic orbits in the associated measure-valued dynamics.","This is the first such example and shows that even in low dimensions, not every limit point of the measure-valued dynamics needs to be a time-stationary measure."],"url":"http://arxiv.org/abs/2402.12300v1","category":"math.PR"}
{"created":"2024-02-19 17:23:46","title":"Search for Stellar Companions of Exoplanet Host Stars with AstraLux/CAHA 2.2 m","abstract":"Stellar multiplicity is a key aspect of exoplanet diversity, as the presence of more than one star in a planetary system can have both devastating and positive effects on its formation and evolution. In this paper, we present the results of a Lucky Imaging survey of 212 exoplanet host stars performed with AstraLux at CAHA 2.2 m. The survey includes data from seven observing epochs between August 2015 and September 2020, and data for individual targets from four earlier observing epochs. The targets of this survey are nearby, bright, solar-like stars with high proper motions. In total, we detected 46 co-moving companions of 43 exoplanet host stars. Accordingly, this survey shows that the minimum multiplicity rate of exoplanet host stars is 20 $\\pm$ 3 %. In total, 33 binary and ten hierarchical triple star systems with exoplanets have been identified. All companions were found to have a common proper motion with the observed exoplanet host stars, and with our astrometry we even find evidence of orbital motion for 28 companions. For all targets, we determined the detection limit and explore the detection space for possible additional companions of these stars. Based on the reached detection limit, additional co-moving companions beyond the detected ones can be excluded around all observed exoplanet host stars. The increasing number of exoplanets discovered in multiple stellar systems suggests that the formation of planets in such systems is by no means rare, but common. Therefore, our study highlights the need to consider stellar multiplicity in future studies of exoplanet habitability.","sentences":["Stellar multiplicity is a key aspect of exoplanet diversity, as the presence of more than one star in a planetary system can have both devastating and positive effects on its formation and evolution.","In this paper, we present the results of a Lucky Imaging survey of 212 exoplanet host stars performed with AstraLux at CAHA 2.2 m.","The survey includes data from seven observing epochs between August 2015 and September 2020, and data for individual targets from four earlier observing epochs.","The targets of this survey are nearby, bright, solar-like stars with high proper motions.","In total, we detected 46 co-moving companions of 43 exoplanet host stars.","Accordingly, this survey shows that the minimum multiplicity rate of exoplanet host stars is 20 $\\pm$ 3 %.","In total, 33 binary and ten hierarchical triple star systems with exoplanets have been identified.","All companions were found to have a common proper motion with the observed exoplanet host stars, and with our astrometry we even find evidence of orbital motion for 28 companions.","For all targets, we determined the detection limit and explore the detection space for possible additional companions of these stars.","Based on the reached detection limit, additional co-moving companions beyond the detected ones can be excluded around all observed exoplanet host stars.","The increasing number of exoplanets discovered in multiple stellar systems suggests that the formation of planets in such systems is by no means rare, but common.","Therefore, our study highlights the need to consider stellar multiplicity in future studies of exoplanet habitability."],"url":"http://arxiv.org/abs/2402.12299v1","category":"astro-ph.EP"}
{"created":"2024-02-19 17:04:04","title":"DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models","abstract":"A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of chain-of-thought (CoT) modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. DriveVLM-Dual achieves robust spatial understanding and real-time inference speed. Extensive experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing methods in complex and unpredictable driving conditions.","sentences":["A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors.","We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities.","DriveVLM integrates a unique combination of chain-of-thought (CoT) modules for scene description, scene analysis, and hierarchical planning.","Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline.","DriveVLM-Dual achieves robust spatial understanding and real-time inference speed.","Extensive experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing methods in complex and unpredictable driving conditions."],"url":"http://arxiv.org/abs/2402.12289v1","category":"cs.CV"}
{"created":"2024-02-19 16:48:42","title":"Challenges and Experiences of Iranian Developers with MLOps at Enterprise","abstract":"Data is becoming more complex, and so are the approaches designed to process it. Enterprises have access to more data than ever, but many still struggle to glean the full potential of insights from what they have. This research explores the challenges and experiences of Iranian developers in implementing the MLOps paradigm within enterprise settings. MLOps, or Machine Learning Operations, is a discipline focused on automating the continuous delivery of machine learning models. In this study, we review the most popular MLOps tools used by leading technology enterprises. Additionally, we present the results of a questionnaire answered by over 110 Iranian Machine Learning experts and Software Developers, shedding light on MLOps tools and the primary obstacles faced. The findings reveal that data quality problems, a lack of resources, and difficulties in model deployment are among the primary challenges faced by practitioners. Collaboration between ML, DevOps, Ops, and Science teams is seen as a pivotal challenge in implementing MLOps effectively.","sentences":["Data is becoming more complex, and so are the approaches designed to process it.","Enterprises have access to more data than ever, but many still struggle to glean the full potential of insights from what they have.","This research explores the challenges and experiences of Iranian developers in implementing the MLOps paradigm within enterprise settings.","MLOps, or Machine Learning Operations, is a discipline focused on automating the continuous delivery of machine learning models.","In this study, we review the most popular MLOps tools used by leading technology enterprises.","Additionally, we present the results of a questionnaire answered by over 110 Iranian Machine Learning experts and Software Developers, shedding light on MLOps tools and the primary obstacles faced.","The findings reveal that data quality problems, a lack of resources, and difficulties in model deployment are among the primary challenges faced by practitioners.","Collaboration between ML, DevOps, Ops, and Science teams is seen as a pivotal challenge in implementing MLOps effectively."],"url":"http://arxiv.org/abs/2402.12281v1","category":"cs.SE"}
{"created":"2024-02-19 16:40:38","title":"Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models","abstract":"The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes.   This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores. By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major document ranking datasets. Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers.","sentences":["The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability.","Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined.","Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes.   ","This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores.","By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major document ranking datasets.","Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers."],"url":"http://arxiv.org/abs/2402.12276v1","category":"cs.IR"}
{"created":"2024-02-19 16:15:03","title":"Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships","abstract":"Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.","sentences":["Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories.","We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data.","We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models.","This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context.","Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph.","Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships."],"url":"http://arxiv.org/abs/2402.12259v1","category":"cs.CV"}
{"created":"2024-02-19 16:14:35","title":"Lyapunov Densities For Markov Processes: An Application To Quantum Systems With Non-Demolition Measurements","abstract":"Stochastic convergence of discrete time Markov processes has been analysed based on a dual Lyapunov approach. Using some existing results on ergodic theory of Markov processes, it has been shown that existence of a properly subinvariant function (counterpart of the Lyapunov density in deterministic systems) implies sweeping of a Markov process out of the sets where this function is integrable. Such a function can be used as a certificate of convergence in probability of a stochastic system. We apply this technique to Markov processes induced by a quantum system with non-demolition measurement and propose dual Lyapunov certificates to certify sweeping.","sentences":["Stochastic convergence of discrete time Markov processes has been analysed based on a dual Lyapunov approach.","Using some existing results on ergodic theory of Markov processes, it has been shown that existence of a properly subinvariant function (counterpart of the Lyapunov density in deterministic systems) implies sweeping of a Markov process out of the sets where this function is integrable.","Such a function can be used as a certificate of convergence in probability of a stochastic system.","We apply this technique to Markov processes induced by a quantum system with non-demolition measurement and propose dual Lyapunov certificates to certify sweeping."],"url":"http://arxiv.org/abs/2402.12257v1","category":"math.DS"}
{"created":"2024-02-19 16:06:06","title":"Lax Additivity","abstract":"We introduce notions of lax semiadditive and lax additive $(\\infty,2)$-categories, categorifying the classical notions of semiadditive and additive 1-categories. To establish a well-behaved axiomatic framework, we develop a calculus of lax matrices and use it to prove that in locally cocomplete $(\\infty,2)$-categories lax limits and lax colimits agree and are absolute. In the lax additive setting, we categorify fundamental constructions from homological algebra such as mapping complexes and mapping cones and establish their basic properties.","sentences":["We introduce notions of lax semiadditive and lax additive $(\\infty,2)$-categories, categorifying the classical notions of semiadditive and additive 1-categories.","To establish a well-behaved axiomatic framework, we develop a calculus of lax matrices and use it to prove that in locally cocomplete $(\\infty,2)$-categories lax limits and lax colimits agree and are absolute.","In the lax additive setting, we categorify fundamental constructions from homological algebra such as mapping complexes and mapping cones and establish their basic properties."],"url":"http://arxiv.org/abs/2402.12251v1","category":"math.CT"}
{"created":"2024-02-19 15:58:30","title":"Constrained Boundary Labeling","abstract":"Boundary labeling is a technique used to label dense sets of feature points in an illustration. It involves placing labels along a rectangular boundary box and connecting each label with its corresponding feature using non-crossing leader lines. Although boundary labeling is well-studied, semantic constraints on the labels have not been investigated thoroughly. In this paper, we consider grouping and ordering constraints for boundary labeling: Grouping constraints enforce that all labels in a group are placed consecutively on the boundary, and ordering constraints enforce a partial order over the labels. We show that finding an admissible labeling for labels of uniform size that can be placed on fixed candidate positions on two opposite sides of the boundary is NP-complete. Furthermore, we show that it is also weakly NP-hard to find an admissible labeling for non-uniform labels that can slide along one side of the boundary. However, we obtain polynomial-time algorithms in the one-sided setting for either fixed candidate positions or uniform-height labels. Finally, we experimentally confirm that our approach has also practical relevance.","sentences":["Boundary labeling is a technique used to label dense sets of feature points in an illustration.","It involves placing labels along a rectangular boundary box and connecting each label with its corresponding feature using non-crossing leader lines.","Although boundary labeling is well-studied, semantic constraints on the labels have not been investigated thoroughly.","In this paper, we consider grouping and ordering constraints for boundary labeling: Grouping constraints enforce that all labels in a group are placed consecutively on the boundary, and ordering constraints enforce a partial order over the labels.","We show that finding an admissible labeling for labels of uniform size that can be placed on fixed candidate positions on two opposite sides of the boundary is NP-complete.","Furthermore, we show that it is also weakly NP-hard to find an admissible labeling for non-uniform labels that can slide along one side of the boundary.","However, we obtain polynomial-time algorithms in the one-sided setting for either fixed candidate positions or uniform-height labels.","Finally, we experimentally confirm that our approach has also practical relevance."],"url":"http://arxiv.org/abs/2402.12245v1","category":"cs.CG"}
{"created":"2024-02-19 15:58:21","title":"Symplectic billiards for pairs of polygons","abstract":"We introduce symplectic billiards for pairs of possibly non-convex polygons. After establishing basic properties, we give several criteria on pairs of polygons for the symplectic billiard map to be fully periodic, i.e. $\\textit{every}$ orbit is periodic. First fully periodic examples were discovered by Albers-Tabachnikov [AT18] and Albers-Banhatti-Sadlo-Schwartz-Tabachnikov in [ABS+19]. Our criteria allow us to construct a plethora of new examples. Moreover, we provide an example of a pair of polygons where the symplectic billiard map is fully periodic while having orbits of arbitrarily large period. After giving a class of examples which provably have isolated periodic orbits (and are thus not fully periodic) we exhibit the first example without any periodic orbits at all. It is open whether being fully periodic with unbounded period or having no periodic orbits at all is possible in the single polygon setting. Finally, we prove that if one replaces polygons by smooth strictly convex curves then there are always infinitely many periodic orbits.","sentences":["We introduce symplectic billiards for pairs of possibly non-convex polygons.","After establishing basic properties, we give several criteria on pairs of polygons for the symplectic billiard map to be fully periodic, i.e. $\\textit{every}$ orbit is periodic.","First fully periodic examples were discovered by Albers-Tabachnikov","[AT18] and Albers-Banhatti-Sadlo-Schwartz-Tabachnikov in [ABS+19].","Our criteria allow us to construct a plethora of new examples.","Moreover, we provide an example of a pair of polygons where the symplectic billiard map is fully periodic while having orbits of arbitrarily large period.","After giving a class of examples which provably have isolated periodic orbits (and are thus not fully periodic) we exhibit the first example without any periodic orbits at all.","It is open whether being fully periodic with unbounded period or having no periodic orbits at all is possible in the single polygon setting.","Finally, we prove that if one replaces polygons by smooth strictly convex curves then there are always infinitely many periodic orbits."],"url":"http://arxiv.org/abs/2402.12244v1","category":"math.DS"}
{"created":"2024-02-19 15:56:43","title":"Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis","abstract":"We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \\emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions. Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters.","sentences":["We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \\emph{without} massive overparameterization.","Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function.","Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions.","Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters."],"url":"http://arxiv.org/abs/2402.12241v1","category":"cs.LG"}
{"created":"2024-02-19 15:45:04","title":"Rotation curves in protoplanetary disks with thermal stratification","abstract":"In recent years the gas kinematics probed by molecular lines detected with ALMA has opened a new window to study protoplanetary disks. High spatial and spectral resolution observations have revealed the complexity of protoplanetary disk structure and correctly interpreting these data allow us to gain a better comprehension of the planet formation process. We investigate the impact of thermal stratification on the azimuthal velocity of protoplanetary disks. High resolution gas observations are showing velocity differences between CO isotopologues, which cannot be adequately explained with vertically isothermal models. The aim of this work is to determine whether a stratified model can explain this discrepancy. We analytically solve the hydrostatic equilibrium for a stratified disk and we derive the azimuthal velocity. We test the model with SPH numerical simulations and then we use it to fit for star mass, disk mass and scale radius of the sources in the MAPS sample. In particular, we use 12CO and 13CO datacubes.","sentences":["In recent years the gas kinematics probed by molecular lines detected with ALMA has opened a new window to study protoplanetary disks.","High spatial and spectral resolution observations have revealed the complexity of protoplanetary disk structure and correctly interpreting these data allow us to gain a better comprehension of the planet formation process.","We investigate the impact of thermal stratification on the azimuthal velocity of protoplanetary disks.","High resolution gas observations are showing velocity differences between CO isotopologues, which cannot be adequately explained with vertically isothermal models.","The aim of this work is to determine whether a stratified model can explain this discrepancy.","We analytically solve the hydrostatic equilibrium for a stratified disk and we derive the azimuthal velocity.","We test the model with SPH numerical simulations and then we use it to fit for star mass, disk mass and scale radius of the sources in the MAPS sample.","In particular, we use 12CO and 13CO datacubes."],"url":"http://arxiv.org/abs/2402.12236v1","category":"astro-ph.EP"}
{"created":"2024-02-19 15:43:35","title":"Task-Oriented Dialogue with In-Context Learning","abstract":"We describe a system for building task-oriented dialogue systems combining the in-context learning abilities of large language models (LLMs) with the deterministic execution of business logic. LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling task-oriented dialogue systems to a large number of tasks. We make our implementation available for use and further study.","sentences":["We describe a system for building task-oriented dialogue systems combining the in-context learning abilities of large language models (LLMs) with the deterministic execution of business logic.","LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic.","We compare our approach to the intent-based NLU approach predominantly used in industry today.","Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling task-oriented dialogue systems to a large number of tasks.","We make our implementation available for use and further study."],"url":"http://arxiv.org/abs/2402.12234v1","category":"cs.CL"}
{"created":"2024-02-19 15:36:36","title":"Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations","abstract":"Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.","sentences":["Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging.","In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions.","We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs.","By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters.","We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters."],"url":"http://arxiv.org/abs/2402.12231v1","category":"cs.LG"}
{"created":"2024-02-19 15:35:02","title":"Self-projective sets","abstract":"Self-projective sets are natural fractal sets which describe the action of a semigroup of matrices on projective space. In recent years there has been growing interest in studying the dimension theory of self-projective sets, as well as progress in the understanding of closely related objects such as Furstenberg measures. The aim of this survey is twofold: first to motivate the study of these objects from several different perspectives and second to make the study of these objects more accessible for readers with expertise in iterated function systems.","sentences":["Self-projective sets are natural fractal sets which describe the action of a semigroup of matrices on projective space.","In recent years there has been growing interest in studying the dimension theory of self-projective sets, as well as progress in the understanding of closely related objects such as Furstenberg measures.","The aim of this survey is twofold: first to motivate the study of these objects from several different perspectives and second to make the study of these objects more accessible for readers with expertise in iterated function systems."],"url":"http://arxiv.org/abs/2402.12229v1","category":"math.DS"}
{"created":"2024-02-19 15:19:10","title":"Forming Long-range Order of Semiconducting Polymers through Liquid-phase Directional Molecular Assemblies","abstract":"Intermolecular interactions are crucial in determining the morphology of solution-processed semiconducting polymer thin films. However, these random interactions often lead to disordered or short-range ordered structures. Achieving long-range order in these films has been a challenge due to limited control over microscopic interactions in current techniques. Here, we present a molecular-level methodology that leverages spatial matching of intermolecular dynamics among solutes, solvents, and substrates to induce directional molecular assembly in weakly bonded polymers. Within the optimized dynamic scale of 2.5 \\r{A} between polymer side chains and self-assembled monolayers (SAMs) on nanogrooved substrates, our approach transforms random aggregates into unidirectional fibers with a remarkable increase in the anisotropic stacking ratio from 1 to 11. The Flory-Huggins-based molecular stacking model accurately predicts the transitioning order on various SAMs, validated by morphologic and spectroscopic observations. The enhanced structural ordering spans over 3 orders of magnitude in length, raising from the smallest 7.3 nm random crystallites to >14 um unidirectional fibers on sub-millimeter areas. Overall, this study provides insights into the control of complex intermolecular interactions and offers enhanced molecular-level controllability in solution-based processes.","sentences":["Intermolecular interactions are crucial in determining the morphology of solution-processed semiconducting polymer thin films.","However, these random interactions often lead to disordered or short-range ordered structures.","Achieving long-range order in these films has been a challenge due to limited control over microscopic interactions in current techniques.","Here, we present a molecular-level methodology that leverages spatial matching of intermolecular dynamics among solutes, solvents, and substrates to induce directional molecular assembly in weakly bonded polymers.","Within the optimized dynamic scale of 2.5 \\r{A} between polymer side chains and self-assembled monolayers (SAMs) on nanogrooved substrates, our approach transforms random aggregates into unidirectional fibers with a remarkable increase in the anisotropic stacking ratio from 1 to 11.","The Flory-Huggins-based molecular stacking model accurately predicts the transitioning order on various SAMs, validated by morphologic and spectroscopic observations.","The enhanced structural ordering spans over 3 orders of magnitude in length, raising from the smallest 7.3 nm random crystallites to >14 um unidirectional fibers on sub-millimeter areas.","Overall, this study provides insights into the control of complex intermolecular interactions and offers enhanced molecular-level controllability in solution-based processes."],"url":"http://arxiv.org/abs/2402.12215v1","category":"cond-mat.soft"}
{"created":"2024-02-19 15:13:31","title":"Modeling the mechanisms of antibody mixtures in viral infections: the cases of sequential homologous and heterologous dengue infections","abstract":"Antibodies play an essential role in the immune response to viral infections, vaccination, or antibody therapy. Nevertheless, they can be either protective or harmful during the immune response. In addition, competition or cooperation between antibodies, when mixed, can enhance or reduce this protective or harmful effect. Using the laws of chemical reactions to model the binding of antibodies to antigens and their actions to neutralize or enhance infection, we propose a new approach to modeling the activity of the antigen-antibody complex. The resulting expression covers not only purely competitive or purely independent binding between antibodies but also synergistic binding which, depending on the type of antibody, can promote either neutralization or enhancement of the viral activity. We then integrate this expression of viral activity in a within-host model, involving both healthy and infected target cells, virus replication, and the production of two antibodies during sequential infections. We investigate the existence of steady-states (disease-free and endemic) and their local and global asymptotic stability. We complete our study with numerical simulations to illustrate different scenarios. In particular, the scenario where both antibodies are neutralizing (homologous sequential DENV infection) and the scenario where one antibody is neutralizing and the other enhancing (heterologous sequential DENV infection). Our model indicates that efficient viral neutralization is associated with purely independent antibody binding, whereas strong enhancement of viral activity is expected in the case of purely competitive antibody binding. The model developed here has several potential applications for a variety of viral infections involving different antibody molecules. It could also be useful for studying the efficacy of vaccines and antibody-based therapies, for example for HIV.","sentences":["Antibodies play an essential role in the immune response to viral infections, vaccination, or antibody therapy.","Nevertheless, they can be either protective or harmful during the immune response.","In addition, competition or cooperation between antibodies, when mixed, can enhance or reduce this protective or harmful effect.","Using the laws of chemical reactions to model the binding of antibodies to antigens and their actions to neutralize or enhance infection, we propose a new approach to modeling the activity of the antigen-antibody complex.","The resulting expression covers not only purely competitive or purely independent binding between antibodies but also synergistic binding which, depending on the type of antibody, can promote either neutralization or enhancement of the viral activity.","We then integrate this expression of viral activity in a within-host model, involving both healthy and infected target cells, virus replication, and the production of two antibodies during sequential infections.","We investigate the existence of steady-states (disease-free and endemic) and their local and global asymptotic stability.","We complete our study with numerical simulations to illustrate different scenarios.","In particular, the scenario where both antibodies are neutralizing (homologous sequential DENV infection) and the scenario where one antibody is neutralizing and the other enhancing (heterologous sequential DENV infection).","Our model indicates that efficient viral neutralization is associated with purely independent antibody binding, whereas strong enhancement of viral activity is expected in the case of purely competitive antibody binding.","The model developed here has several potential applications for a variety of viral infections involving different antibody molecules.","It could also be useful for studying the efficacy of vaccines and antibody-based therapies, for example for HIV."],"url":"http://arxiv.org/abs/2402.12210v1","category":"q-bio.QM"}
{"created":"2024-02-19 15:12:12","title":"Some Riemannian properties of $\\mathbf{SU_n}$ endowed with a bi-invariant metric","abstract":"We study some properties of $SU_n$ endowed with the Frobenius metric $\\phi$, which is, up to a positive constant multiple, the unique bi-invariant Riemannian metric on $SU_n$. In particular we express the distance between $P, Q \\in SU_n$ in terms of eigenvalues of $P^*Q$; we compute the diameter of $(SU_n, \\phi)$ and we determine its diametral pairs; we prove that the set of all minimizing geodesic segments with endpoints $P$, $Q$ can be parametrized by means of a compact connected submanifold of $\\mathfrak{su}_n$, diffeomorphic to a suitable complex Grassmannian depending on $P$ and $Q$.","sentences":["We study some properties of $SU_n$ endowed with the Frobenius metric $\\phi$, which is, up to a positive constant multiple, the unique bi-invariant Riemannian metric on $SU_n$. In particular we express the distance between $P, Q \\in SU_n$ in terms of eigenvalues of $P^*Q$; we compute the diameter of $(SU_n, \\phi)$ and we determine its diametral pairs; we prove that the set of all minimizing geodesic segments with endpoints $P$, $Q$ can be parametrized by means of a compact connected submanifold of $\\mathfrak{su}_n$, diffeomorphic to a suitable complex Grassmannian depending on $P$ and $Q$."],"url":"http://arxiv.org/abs/2402.12209v1","category":"math.DG"}
{"created":"2024-02-19 15:04:53","title":"Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT","abstract":"Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.","sentences":["Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations.","We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features?","We propose a circuit discovery framework alternative to activation patching.","Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity.","The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output.","Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors.","We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it."],"url":"http://arxiv.org/abs/2402.12201v1","category":"cs.LG"}
{"created":"2024-02-19 14:54:20","title":"Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data","abstract":"AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes. New technological platforms have facilitated the timely acquisition of multimodal data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making. In this work, we propose a modular machine learning framework designed for personalized counterfactual cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies. These specialized counterfactual experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision. The framework is tailored to address critical challenges inherent in data-driven cancer research, including the high-dimensional nature of the data, and the presence of treatment assignment bias in the retrospective observational data. The framework is showcased through comprehensive demonstrations using data from in-vitro and in-vivo treatment responses from a cohort of patients with ovarian cancer. Our method aims to empower clinicians with a reality-centric decision-support tool including probabilistic treatment suggestions with calibrated confidence and personalized explanations for tailoring treatment strategies to multi-omics characteristics of individual cancer patients.","sentences":["AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes.","New technological platforms have facilitated the timely acquisition of multimodal data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making.","In this work, we propose a modular machine learning framework designed for personalized counterfactual cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies.","These specialized counterfactual experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision.","The framework is tailored to address critical challenges inherent in data-driven cancer research, including the high-dimensional nature of the data, and the presence of treatment assignment bias in the retrospective observational data.","The framework is showcased through comprehensive demonstrations using data from in-vitro and in-vivo treatment responses from a cohort of patients with ovarian cancer.","Our method aims to empower clinicians with a reality-centric decision-support tool including probabilistic treatment suggestions with calibrated confidence and personalized explanations for tailoring treatment strategies to multi-omics characteristics of individual cancer patients."],"url":"http://arxiv.org/abs/2402.12190v1","category":"stat.ML"}
{"created":"2024-02-19 14:50:39","title":"Subspace methods for the simulation of molecular response properties on a quantum computer","abstract":"We explore Davidson methods for obtaining excitation energies and other linear response properties within quantum self-consistent linear response (q-sc-LR) theory. Davidson-type methods allow for obtaining only a few selected excitation energies without explicitly constructing the electronic Hessian since they only require the ability to perform Hessian-vector multiplications. We apply the Davidson method to calculate the excitation energies of hydrogen chains (up to H$_{10}$) and analyze aspects of statistical noise for computing excitation energies on quantum simulators. Additionally, we apply Davidson methods for computing linear response properties such as static polarizabilities for H$_2$, LiH, H$_2$O, OH$^-$, and NH$_3$, and show that unitary coupled cluster outperforms classical projected coupled cluster for molecular systems with strong correlation. Finally, we formulate the Davidson method for damped (complex) linear response, with application to the nitrogen K-edge X-ray absorption of ammonia, and the $C_6$ coefficients of H$_2$, LiH, H$_2$O, OH$^-$, and NH$_3$.","sentences":["We explore Davidson methods for obtaining excitation energies and other linear response properties within quantum self-consistent linear response (q-sc-LR) theory.","Davidson-type methods allow for obtaining only a few selected excitation energies without explicitly constructing the electronic Hessian since they only require the ability to perform Hessian-vector multiplications.","We apply the Davidson method to calculate the excitation energies of hydrogen chains (up to H$_{10}$) and analyze aspects of statistical noise for computing excitation energies on quantum simulators.","Additionally, we apply Davidson methods for computing linear response properties such as static polarizabilities for H$_2$, LiH, H$_2$O, OH$^-$, and NH$_3$, and show that unitary coupled cluster outperforms classical projected coupled cluster for molecular systems with strong correlation.","Finally, we formulate the Davidson method for damped (complex) linear response, with application to the nitrogen K-edge X-ray absorption of ammonia, and the $C_6$ coefficients of H$_2$, LiH, H$_2$O, OH$^-$, and NH$_3$."],"url":"http://arxiv.org/abs/2402.12186v1","category":"physics.chem-ph"}
{"created":"2024-02-19 14:41:20","title":"Anomalous Diffusion, Prethermalization, and Particle Binding in an Interacting Flat Band System","abstract":"We study the broadening of initially localized wave packets in a quasi one-dimensional diamond ladder with interacting, spinless fermions. The lattice possesses a flat band causing localization. We place special focus on the transition away from the flat band many-body localized case by adding very weak dispersion. By doing so, we allow propagation of the wave packet on significantly different timescales which causes anomalous diffusion. Due to the temporal separation of dynamic processes, an interaction-induced, prethermal equilibrium becomes apparent. A physical picture of light and heavy modes for this prethermal behavior can be obtained within Born-Oppenheimer approximation via basis transformation of the original Hamiltonian. This reveals a detachment between light, symmetric and heavy, anti-symmetric particle species. We show that the prethermal state is characterized by heavy particles binding together mediated by the light particles.","sentences":["We study the broadening of initially localized wave packets in a quasi one-dimensional diamond ladder with interacting, spinless fermions.","The lattice possesses a flat band causing localization.","We place special focus on the transition away from the flat band many-body localized case by adding very weak dispersion.","By doing so, we allow propagation of the wave packet on significantly different timescales which causes anomalous diffusion.","Due to the temporal separation of dynamic processes, an interaction-induced, prethermal equilibrium becomes apparent.","A physical picture of light and heavy modes for this prethermal behavior can be obtained within Born-Oppenheimer approximation via basis transformation of the original Hamiltonian.","This reveals a detachment between light, symmetric and heavy, anti-symmetric particle species.","We show that the prethermal state is characterized by heavy particles binding together mediated by the light particles."],"url":"http://arxiv.org/abs/2402.12180v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 14:22:48","title":"Electroweak three-body decays in the presence of two- and three-body bound states","abstract":"Recently, formalism has been derived for studying electroweak transition amplitudes for three-body systems both in infinite and finite volumes. The formalism provides exact relations that the infinite-volume amplitudes must satisfy, as well as a relationship between physical amplitudes and finite-volume matrix elements, which can be constrained from lattice QCD calculations. This formalism poses additional challenges when compared with the analogous well-studied two-body equivalent one, including the necessary step of solving integral equations of singular functions. In this work, we provide some non-trivial analytical and numerical tests on the aforementioned formalism. In particular, we consider a case where the three-particle system can have three-body bound states as well as bound states in the two-body subsystem. For kinematics below the three-body threshold, we demonstrate that the scattering amplitudes satisfy unitarity. We also check that for these kinematics the finite-volume matrix elements are accurately described by the formalism for two-body systems up to exponentially suppressed corrections. Finally, we verify that in the case of the three-body bound state, the finite-volume matrix element is equal to the infinite-volume coupling of the bound state, up to exponentially suppressed errors.","sentences":["Recently, formalism has been derived for studying electroweak transition amplitudes for three-body systems both in infinite and finite volumes.","The formalism provides exact relations that the infinite-volume amplitudes must satisfy, as well as a relationship between physical amplitudes and finite-volume matrix elements, which can be constrained from lattice QCD calculations.","This formalism poses additional challenges when compared with the analogous well-studied two-body equivalent one, including the necessary step of solving integral equations of singular functions.","In this work, we provide some non-trivial analytical and numerical tests on the aforementioned formalism.","In particular, we consider a case where the three-particle system can have three-body bound states as well as bound states in the two-body subsystem.","For kinematics below the three-body threshold, we demonstrate that the scattering amplitudes satisfy unitarity.","We also check that for these kinematics the finite-volume matrix elements are accurately described by the formalism for two-body systems up to exponentially suppressed corrections.","Finally, we verify that in the case of the three-body bound state, the finite-volume matrix element is equal to the infinite-volume coupling of the bound state, up to exponentially suppressed errors."],"url":"http://arxiv.org/abs/2402.12167v1","category":"hep-lat"}
{"created":"2024-02-19 14:20:18","title":"Integrating Dynamic Weighted Approach with Fictitious Play and Pure Counterfactual Regret Minimization for Equilibrium Finding","abstract":"Developing efficient algorithms to converge to Nash Equilibrium is a key focus in game theory. The use of dynamic weighting has been especially advantageous in normal-form games, enhancing the rate of convergence. For instance, the Greedy Regret Minimization (RM) algorithm has markedly outperformed earlier techniques. Nonetheless, its dependency on mixed strategies throughout the iterative process introduces complexity to dynamic weighting, which in turn restricts its use in extensive-form games.   In this study, we introduce two novel dynamic weighting algorithms: Dynamic Weighted Fictitious Play (DW-FP) and Dynamic Weighted Pure Counterfactual Regret Minimization (DW-PCFR). These algorithms, utilizing pure strategies in each iteration, offer key benefits: (i) Addressing the complexity of dynamic weight computation in Greedy RM, thereby facilitating application in extensive-form games; (ii) Incorporating the low-memory usage and ease-of-use features of FP and CFR; (iii) They guarantee a convergence lower bound of $\\mathcal{O}\\left(T^{-\\frac{1}{2}}\\right)$, with a tendency to achieve a convergence rate of $\\mathcal{O}(T^{-1})$ as runtime increases. This research not only theoretically affirms the convergence capabilities of these algorithms but also empirically demonstrates their superiority over existing leading algorithms across all our tests.","sentences":["Developing efficient algorithms to converge to Nash Equilibrium is a key focus in game theory.","The use of dynamic weighting has been especially advantageous in normal-form games, enhancing the rate of convergence.","For instance, the Greedy Regret Minimization (RM) algorithm has markedly outperformed earlier techniques.","Nonetheless, its dependency on mixed strategies throughout the iterative process introduces complexity to dynamic weighting, which in turn restricts its use in extensive-form games.   ","In this study, we introduce two novel dynamic weighting algorithms: Dynamic Weighted Fictitious Play (DW-FP) and Dynamic Weighted Pure Counterfactual Regret Minimization (DW-PCFR).","These algorithms, utilizing pure strategies in each iteration, offer key benefits: (i) Addressing the complexity of dynamic weight computation in Greedy RM, thereby facilitating application in extensive-form games; (ii) Incorporating the low-memory usage and ease-of-use features of FP and CFR; (iii) They guarantee a convergence lower bound of $\\mathcal{O}\\left(T^{-\\frac{1}{2}}\\right)$, with a tendency to achieve a convergence rate of $\\mathcal{O}(T^{-1})$ as runtime increases.","This research not only theoretically affirms the convergence capabilities of these algorithms but also empirically demonstrates their superiority over existing leading algorithms across all our tests."],"url":"http://arxiv.org/abs/2402.12164v1","category":"cs.GT"}
{"created":"2024-02-19 14:19:37","title":"Equivariant Hopf bifurcation arising in circular-distributed predator-prey interaction with taxis","abstract":"In this paper, we study the Rosenzweig-MacArthur predator-prey model with predator-taxis and time delay defined on a disk. Theoretically, we studied the equivariant Hopf bifurcation around the positive constant steady-state solution. Standing and rotating waves have been investigated through the theory of isotropic subgroups and Lyapunov-Schmidt reduction. The existence conditions, the formula for the periodic direction and the periodic variation of bifurcation periodic solutions are obtained. Numerically, we select appropriate parameters and conduct numerical simulations to illustrate the theoretical results and reveal quite complicated dynamics on the disk. Different types of rotating and standing waves, as well as more complex spatiotemporal patterns with random initial values, are new dynamic phenomena that do not occur in one-dimensional intervals.","sentences":["In this paper, we study the Rosenzweig-MacArthur predator-prey model with predator-taxis and time delay defined on a disk.","Theoretically, we studied the equivariant Hopf bifurcation around the positive constant steady-state solution.","Standing and rotating waves have been investigated through the theory of isotropic subgroups and Lyapunov-Schmidt reduction.","The existence conditions, the formula for the periodic direction and the periodic variation of bifurcation periodic solutions are obtained.","Numerically, we select appropriate parameters and conduct numerical simulations to illustrate the theoretical results and reveal quite complicated dynamics on the disk.","Different types of rotating and standing waves, as well as more complex spatiotemporal patterns with random initial values, are new dynamic phenomena that do not occur in one-dimensional intervals."],"url":"http://arxiv.org/abs/2402.12163v1","category":"math.DS"}
{"created":"2024-02-19 13:56:20","title":"Nonlocal to local convergence of phase field systems with inertial term","abstract":"This paper deals with a nonlocal model for a hyperbolic phase field system coupling the standard energy balance equation for temperature with a dynamic for the phase variable: the latter includes an inertial term and a nonlocal convolution-type operator where the family of kernels depends on a small parameter. We rigorously study the asymptotic convergence of the system as the approximating parameter tends to zero and we obtain at the limit the local system with the elliptic laplacian operator acting on the phase variable. Our analysis is based on some asymptotic properties on nonlocal-to-local convergence that have been recently and successfully applied to families of Cahn--Hilliard models.","sentences":["This paper deals with a nonlocal model for a hyperbolic phase field system coupling the standard energy balance equation for temperature with a dynamic for the phase variable: the latter includes an inertial term and a nonlocal convolution-type operator where the family of kernels depends on a small parameter.","We rigorously study the asymptotic convergence of the system as the approximating parameter tends to zero and we obtain at the limit the local system with the elliptic laplacian operator acting on the phase variable.","Our analysis is based on some asymptotic properties on nonlocal-to-local convergence that have been recently and successfully applied to families of Cahn--Hilliard models."],"url":"http://arxiv.org/abs/2402.12145v1","category":"math.AP"}
{"created":"2024-02-19 13:53:13","title":"Connectivity Labeling in Faulty Colored Graphs","abstract":"Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex graph $G=(V,E)$ and $f\\geq 1$, produce succinct yet informative labels for the elements of the graph. Given only the labels of two vertices $u,v$ and of the elements in a faulty-set $F$ with $|F|\\leq f$, one can determine if $u,v$ are connected in $G-F$, the surviving graph after removing $F$. For the edge or vertex faults models, i.e., $F\\subseteq E$ or $F\\subseteq V$, a sequence of recent work established schemes with $poly(f,\\log n)$-bit labels. This paper considers the color faults model, recently introduced in the context of spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known correlations between failures. Here, the edges (or vertices) of the input $G$ are arbitrarily colored, and the faulty elements in $F$ are colors; a failing color causes all edges (vertices) of that color to crash.   Our main contribution is settling the label length complexity for connectivity under one color fault ($f=1$). The existing implicit solution, by applying the state-of-the-art scheme for edge faults of [Dory and Parter, PODC'21], might yield labels of $\\Omega(n)$ bits. We provide a deterministic scheme with labels of $\\tilde{O}(\\sqrt{n})$ bits in the worst case, and a matching lower bound. Moreover, our scheme is universally optimal: even schemes tailored to handle only colorings of one specific graph topology cannot produce asymptotically smaller labels. We extend our labeling approach to yield a routing scheme avoiding a single forbidden color. We also consider the centralized setting, and show an $\\tilde{O}(n)$-space oracle, answering connectivity queries under one color fault in $\\tilde{O}(1)$ time. Turning to $f\\geq 2$ color faults, we give a randomized labeling scheme with $\\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of $\\Omega(n^{1-1/(f+1)})$ bits.","sentences":["Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex graph $G=(V,E)$ and $f\\geq 1$, produce succinct yet informative labels for the elements of the graph.","Given only the labels of two vertices $u,v$ and of the elements in a faulty-set $F$ with $|F|\\leq f$, one can determine if $u,v$ are connected in $G-F$, the surviving graph after removing $F$. For the edge or vertex faults models, i.e., $F\\subseteq E$ or $F\\subseteq V$, a sequence of recent work established schemes with $poly(f,\\log n)$-bit labels.","This paper considers the color faults model, recently introduced in the context of spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known correlations between failures.","Here, the edges (or vertices) of the input $G$ are arbitrarily colored, and the faulty elements in $F$ are colors; a failing color causes all edges (vertices) of that color to crash.   ","Our main contribution is settling the label length complexity for connectivity under one color fault ($f=1$).","The existing implicit solution, by applying the state-of-the-art scheme for edge faults of [Dory and Parter, PODC'21], might yield labels of $\\Omega(n)$ bits.","We provide a deterministic scheme with labels of $\\tilde{O}(\\sqrt{n})$ bits in the worst case, and a matching lower bound.","Moreover, our scheme is universally optimal: even schemes tailored to handle only colorings of one specific graph topology cannot produce asymptotically smaller labels.","We extend our labeling approach to yield a routing scheme avoiding a single forbidden color.","We also consider the centralized setting, and show an $\\tilde{O}(n)$-space oracle, answering connectivity queries under one color fault in $\\tilde{O}(1)$ time.","Turning to $f\\geq 2$ color faults, we give a randomized labeling scheme with $\\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of $\\Omega(n^{1-1/(f+1)})$ bits."],"url":"http://arxiv.org/abs/2402.12144v1","category":"cs.DS"}
{"created":"2024-02-19 13:52:37","title":"Federated Bayesian Network Ensembles","abstract":"Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location.   In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federated learning toolbox, especially when local populations are heavily biased, or there is a strong imbalance in population size across parties. We discuss the advantages and disadvantages of this approach in terms of time complexity, model accuracy, privacy protection, and model interpretability.","sentences":["Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns.","Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated.","Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location.   ","In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data.","Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages.","We show that FBNE is a potentially useful tool within the federated learning toolbox, especially when local populations are heavily biased, or there is a strong imbalance in population size across parties.","We discuss the advantages and disadvantages of this approach in terms of time complexity, model accuracy, privacy protection, and model interpretability."],"url":"http://arxiv.org/abs/2402.12142v1","category":"cs.LG"}
{"created":"2024-02-19 13:20:19","title":"Development of a cylindrical mirror analyzer electron spectrometer and associated data acquisition system to study inner shell electron emission following ion-atom collision","abstract":"In this paper we report on the development and performance of a cylindrical mirror analyser electron spectrometer for ion atom collision experiments. A low cost data acquisition system using Arduino microcontroller has also been developed and tested. We have measured the Auger emission spectra for various gaseous targets in collision with 1 MeV proton beams. Relative total Auger emission cross sections have also been measured for N2 molecular target as a function of proton energy.","sentences":["In this paper we report on the development and performance of a cylindrical mirror analyser electron spectrometer for ion atom collision experiments.","A low cost data acquisition system using Arduino microcontroller has also been developed and tested.","We have measured the Auger emission spectra for various gaseous targets in collision with 1 MeV proton beams.","Relative total Auger emission cross sections have also been measured for N2 molecular target as a function of proton energy."],"url":"http://arxiv.org/abs/2402.12126v1","category":"physics.atom-ph"}
{"created":"2024-02-19 13:16:59","title":"Brownian oscillator with time-dependent strength: a delta function protocol","abstract":"We consider a classical Brownian oscillator of mass $m$ driven from an arbitrary initial state by varying the stiffness $k(t)$ of the harmonic potential according to the protocol $k(t)=k_0+a\\,\\delta(t)$, involving the Dirac delta function. The microscopic work performed on the oscillator is shown to be $W=(a^2/2m)\\,q^2-a q v$, where $q$ and $v$ are the coordinate and velocity in the initial state. If the initial distribution of $q$ and $v$ is the equilibrium one with temperature $T$, the average work is $\\langle W \\rangle=a^2T/(2m\\,k_0)$ and the distribution $f(W)$ has the form of the product of exponential and modified Bessel functions. The distribution is asymmetric and diverges as $W\\to 0$. The system's response for $t>0$ is evaluated for specific models.","sentences":["We consider a classical Brownian oscillator of mass $m$ driven from an arbitrary initial state by varying the stiffness $k(t)$ of the harmonic potential according to the protocol $k(t)=k_0+a\\,\\delta(t)$, involving the Dirac delta function.","The microscopic work performed on the oscillator is shown to be $W=(a^2/2m)\\,q^2-a q v$, where $q$ and $v$ are the coordinate and velocity in the initial state.","If the initial distribution of $q$ and $v$ is the equilibrium one with temperature $T$, the average work is $\\langle W \\rangle=a^2T/(2m\\,k_0)$ and the distribution $f(W)$ has the form of the product of exponential and modified Bessel functions.","The distribution is asymmetric and diverges as $W\\to 0$.","The system's response for $t>0$ is evaluated for specific models."],"url":"http://arxiv.org/abs/2402.12124v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 13:13:04","title":"Still mystery after all these years -- Unconventional superconductivity of Sr2RuO4 --","abstract":"This review describes recent significant research developments made on the layered perovskite Sr2RuO4 and discusses current issues from both experimental and theoretical perspectives. Since the discovery of superconductivity in Sr2RuO4 in 1994, studies using high-quality single crystals quickly revealed it to be an archetypal unconventional superconductor among strongly correlated electron systems. In particular, it was thought that the spin-triplet chiral p-wave superconducting state, which breaks time-reversal symmetry, was a prominent possibility. In 2019, however, a new development overturned the past experimental results, and spin-singlet-like behavior became conclusive. Furthermore, innovation in uniaxial strain devices has stimulated researchers to explore changes in the superconducting state by controlling the symmetry and dimensionality of the Fermi surfaces and enhancing the superconducting transition temperature Tc from 1.5 K to 3.5 K. A spin-singlet chiral d-wave superconducting state is consistent with most of these recent experimental results. Nevertheless, there are still unnatural aspects that remain to be explained. The focus of this review is on unraveling this mystery. Unlike many other unconventional superconductors, the normal state of Sr2RuO4 exhibits typical Fermi liquid behavior. Nevertheless, to elucidate its superconducting state, it may be essential to go beyond the traditional framework of unconventional superconductivity and recast the theory explicitly considering the multi-orbital aspects of its electronic states. In this review, we describe the frontiers of superconductivity research in Sr2RuO4 and discuss how the remaining issues may be resolved.","sentences":["This review describes recent significant research developments made on the layered perovskite Sr2RuO4 and discusses current issues from both experimental and theoretical perspectives.","Since the discovery of superconductivity in Sr2RuO4 in 1994, studies using high-quality single crystals quickly revealed it to be an archetypal unconventional superconductor among strongly correlated electron systems.","In particular, it was thought that the spin-triplet chiral p-wave superconducting state, which breaks time-reversal symmetry, was a prominent possibility.","In 2019, however, a new development overturned the past experimental results, and spin-singlet-like behavior became conclusive.","Furthermore, innovation in uniaxial strain devices has stimulated researchers to explore changes in the superconducting state by controlling the symmetry and dimensionality of the Fermi surfaces and enhancing the superconducting transition temperature Tc from 1.5 K to 3.5 K. A spin-singlet chiral d-wave superconducting state is consistent with most of these recent experimental results.","Nevertheless, there are still unnatural aspects that remain to be explained.","The focus of this review is on unraveling this mystery.","Unlike many other unconventional superconductors, the normal state of Sr2RuO4 exhibits typical Fermi liquid behavior.","Nevertheless, to elucidate its superconducting state, it may be essential to go beyond the traditional framework of unconventional superconductivity and recast the theory explicitly considering the multi-orbital aspects of its electronic states.","In this review, we describe the frontiers of superconductivity research in Sr2RuO4 and discuss how the remaining issues may be resolved."],"url":"http://arxiv.org/abs/2402.12117v1","category":"cond-mat.supr-con"}
{"created":"2024-02-19 13:11:10","title":"Discrete Morse theory for open complexes","abstract":"We develop a discrete Morse theory for open simplicial complexes $K=X\\setminus T$ where $X$ is a simplicial complex and $T$ a subcomplex of $X$. A discrete Morse function $f$ on $K$ gives rise to a discrete Morse function on the order complex $S_K$ of $K$, and the topology change determined by $f$ on $K$ can be understood by analyzing the the topology change determined by the discrete Morse function on $S_K$. This topology change is given by a structure theorem on the level subcomplexes of $S_K$. Finally, we show that the Borel-Moore homology of $K$, a homology theory for locally compact spaces, is isomorphic to the homology induced by a gradient vector field on $K$ and deduce corresponding weak Morse inequalities.","sentences":["We develop a discrete Morse theory for open simplicial complexes $K=X\\setminus T$ where $X$ is a simplicial complex and $T$ a subcomplex of $X$. A discrete Morse function $f$ on $K$ gives rise to a discrete Morse function on the order complex $S_K$ of $K$, and the topology change determined by $f$ on $K$ can be understood by analyzing the the topology change determined by the discrete Morse function on $S_K$. This topology change is given by a structure theorem on the level subcomplexes of $S_K$. Finally, we show that the Borel-Moore homology of $K$, a homology theory for locally compact spaces, is isomorphic to the homology induced by a gradient vector field on $K$ and deduce corresponding weak Morse inequalities."],"url":"http://arxiv.org/abs/2402.12116v1","category":"math.AT"}
{"created":"2024-02-19 13:07:50","title":"Topological Phase Diagram of Optimally Shaken Honeycomb Lattices: A Dual Perspective from Stroboscopic and Non-Stroboscopic Floquet Hamiltonians","abstract":"We present a direct comparison between the stroboscopic and non-stroboscopic effective approaches for ultracold atoms in shaken honeycomb lattices, focusing specifically on the optimal driving introduced by A. Verdeny and F. Mintert [Phys. Rev. A 92, 063615 (2015)]. In the fast-driving regime, we compare the effective non-stroboscopic Hamiltonian derived through a perturbative expansion with a non-perturbative calculation of the stroboscopic Floquet Hamiltonian, obtained through a simple non-perturbative numerical approach. We show that while some of the tunneling parameters are inherently model-dependent, the topological properties of the system remains robust, as expected. Using the same numerical approach we compute the topological phase diagram, arguing that it is most effectively represented in terms of the physical parameters characterizing the driving and the bare Hamiltonian -- parameters directly accessible in experiments -- rather than the emergent tunneling parameters, that depend on the model representation.","sentences":["We present a direct comparison between the stroboscopic and non-stroboscopic effective approaches for ultracold atoms in shaken honeycomb lattices, focusing specifically on the optimal driving introduced by A. Verdeny and F. Mintert [Phys.","Rev. A 92, 063615 (2015)].","In the fast-driving regime, we compare the effective non-stroboscopic Hamiltonian derived through a perturbative expansion with a non-perturbative calculation of the stroboscopic Floquet Hamiltonian, obtained through a simple non-perturbative numerical approach.","We show that while some of the tunneling parameters are inherently model-dependent, the topological properties of the system remains robust, as expected.","Using the same numerical approach we compute the topological phase diagram, arguing that it is most effectively represented in terms of the physical parameters characterizing the driving and the bare Hamiltonian -- parameters directly accessible in experiments -- rather than the emergent tunneling parameters, that depend on the model representation."],"url":"http://arxiv.org/abs/2402.12113v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-19 13:04:16","title":"Magnetic anisotropy and GGG substrate stray field in YIG films down to millikelvin temperatures","abstract":"Quantum magnonics investigates the quantum-mechanical properties of magnons such as quantum coherence or entanglement for solid-state quantum information technologies at the nanoscale. The most promising material for quantum magnonics is the ferrimagnetic yttrium iron garnet (YIG), which hosts magnons with the longest lifetimes. YIG films of the highest quality are grown on a paramagnetic gadolinium gallium garnet (GGG) substrate. The literature has reported that ferromagnetic resonance (FMR) frequencies of YIG/GGG decrease at temperatures below 50 K despite the increase in YIG magnetization. We investigated a 97 nm-thick YIG film grown on 500 $\\mathrm{\\mu}$m-thick GGG substrate through a series of experiments conducted at temperatures as low as 30 mK, and using both analytical and numerical methods. Our findings suggest that the primary factor contributing to the FMR frequency shift is the stray magnetic field created by the partially magnetized GGG substrate. This stray field is antiparallel to the applied external field and is highly inhomogeneous, reaching up to 40 mT in the center of the sample. At temperatures below 500 mK, the GGG field exhibits a saturation that cannot be described by the standard Brillouin function for a paramagnet. Including the calculated GGG field in the analysis of the FMR frequency versus temperature dependence allowed the determination of the cubic and uniaxial anisotropies. We find that the total anisotropy increases more than three times with the decrease in temperature down to 2 K. Our findings enable accurate predictions of the YIG/GGG magnetic systems behavior at low and ultra-low millikelvin temperatures, crucial for developing quantum magnonic devices.","sentences":["Quantum magnonics investigates the quantum-mechanical properties of magnons such as quantum coherence or entanglement for solid-state quantum information technologies at the nanoscale.","The most promising material for quantum magnonics is the ferrimagnetic yttrium iron garnet (YIG), which hosts magnons with the longest lifetimes.","YIG films of the highest quality are grown on a paramagnetic gadolinium gallium garnet (GGG) substrate.","The literature has reported that ferromagnetic resonance (FMR) frequencies of YIG/GGG decrease at temperatures below 50 K despite the increase in YIG magnetization.","We investigated a 97 nm-thick YIG film grown on 500 $\\mathrm{\\mu}$m-thick GGG substrate through a series of experiments conducted at temperatures as low as 30 mK, and using both analytical and numerical methods.","Our findings suggest that the primary factor contributing to the FMR frequency shift is the stray magnetic field created by the partially magnetized GGG substrate.","This stray field is antiparallel to the applied external field and is highly inhomogeneous, reaching up to 40 mT in the center of the sample.","At temperatures below 500 mK, the GGG field exhibits a saturation that cannot be described by the standard Brillouin function for a paramagnet.","Including the calculated GGG field in the analysis of the FMR frequency versus temperature dependence allowed the determination of the cubic and uniaxial anisotropies.","We find that the total anisotropy increases more than three times with the decrease in temperature down to 2 K. Our findings enable accurate predictions of the YIG/GGG magnetic systems behavior at low and ultra-low millikelvin temperatures, crucial for developing quantum magnonic devices."],"url":"http://arxiv.org/abs/2402.12112v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 12:56:44","title":"Weak-Linear Types","abstract":"Computational interpretations of linear logic allow static control of memory resources: the data produced by the program are endowed through its type with attributes that determine its life cycle, and guarantee safe deallocation. The use of linear types encounters limitations in practice, since linear data, in the traditional sense, do not so often appear in actual programs. Several alternatives have been proposed in the attempt to relax the condition of linearity, adding coercions to the language to allow linear objects to be temporarily aliased. In this work we propose a new alternative, whose virtue is to preserve the simplicity and elegance of the original system.","sentences":["Computational interpretations of linear logic allow static control of memory resources: the data produced by the program are endowed through its type with attributes that determine its life cycle, and guarantee safe deallocation.","The use of linear types encounters limitations in practice, since linear data, in the traditional sense, do not so often appear in actual programs.","Several alternatives have been proposed in the attempt to relax the condition of linearity, adding coercions to the language to allow linear objects to be temporarily aliased.","In this work we propose a new alternative, whose virtue is to preserve the simplicity and elegance of the original system."],"url":"http://arxiv.org/abs/2402.12108v1","category":"cs.PL"}
{"created":"2024-02-19 12:53:52","title":"A joint optimization approach of parameterized quantum circuits with a tensor network","abstract":"Despite the advantage quantum computers are expected to deliver when performing simulations compared to their classical counterparts, the current noisy intermediate-scale quantum (NISQ) devices remain limited in their capabilities. The training of parameterized quantum circuits (PQCs) remains a significant practical challenge, exacerbated by the requirement of shallow circuit depth necessary for their hardware implementation. Hybrid methods employing classical computers alongside quantum devices, such as the Variational Quantum Eigensolver (VQE), have proven useful for analyzing the capabilities of NISQ devices to solve relevant optimization problems. Still, in the simulation of complex structures involving the many-body problem in quantum mechanics, major issues remain about the representation of the system and obtaining results which clearly outperform classical computational devices. In this research contribution we propose the use of parameterized Tensor Networks (TNs) to attempt an improved performance of the VQE algorithm. A joint approach is presented where the Hamiltonian of a system is encapsulated into a Matrix Product Operator (MPO) within a parameterized unitary TN hereby splitting up the optimization task between the TN and the VQE. We show that the hybrid TN-VQE implementation improves the convergence of the algorithm in comparison to optimizing randomly-initialized quantum circuits via VQE.","sentences":["Despite the advantage quantum computers are expected to deliver when performing simulations compared to their classical counterparts, the current noisy intermediate-scale quantum (NISQ) devices remain limited in their capabilities.","The training of parameterized quantum circuits (PQCs) remains a significant practical challenge, exacerbated by the requirement of shallow circuit depth necessary for their hardware implementation.","Hybrid methods employing classical computers alongside quantum devices, such as the Variational Quantum Eigensolver (VQE), have proven useful for analyzing the capabilities of NISQ devices to solve relevant optimization problems.","Still, in the simulation of complex structures involving the many-body problem in quantum mechanics, major issues remain about the representation of the system and obtaining results which clearly outperform classical computational devices.","In this research contribution we propose the use of parameterized Tensor Networks (TNs) to attempt an improved performance of the VQE algorithm.","A joint approach is presented where the Hamiltonian of a system is encapsulated into a Matrix Product Operator (MPO) within a parameterized unitary TN hereby splitting up the optimization task between the TN and the VQE.","We show that the hybrid TN-VQE implementation improves the convergence of the algorithm in comparison to optimizing randomly-initialized quantum circuits via VQE."],"url":"http://arxiv.org/abs/2402.12105v1","category":"quant-ph"}
{"created":"2024-02-19 12:47:24","title":"Interference Mitigation in LEO Constellations with Limited Radio Environment Information","abstract":"This research paper delves into interference mitigation within Low Earth Orbit (LEO) satellite constellations, particularly when operating under constraints of limited radio environment information. Leveraging cognitive capabilities facilitated by the Radio Environment Map (REM), we explore strategies to mitigate the impact of both intentional and unintentional interference using planar antenna array (PAA) beamforming techniques. We address the complexities encountered in the design of beamforming weights, a challenge exacerbated by the array size and the increasing number of directions of interest and avoidance. Furthermore, we conduct an extensive analysis of beamforming performance from various perspectives associated with limited REM information: static versus dynamic, partial versus full, and perfect versus imperfect. To substantiate our findings, we provide simulation results and offer conclusions based on the outcomes of our investigation.","sentences":["This research paper delves into interference mitigation within Low Earth Orbit (LEO) satellite constellations, particularly when operating under constraints of limited radio environment information.","Leveraging cognitive capabilities facilitated by the Radio Environment Map (REM), we explore strategies to mitigate the impact of both intentional and unintentional interference using planar antenna array (PAA) beamforming techniques.","We address the complexities encountered in the design of beamforming weights, a challenge exacerbated by the array size and the increasing number of directions of interest and avoidance.","Furthermore, we conduct an extensive analysis of beamforming performance from various perspectives associated with limited REM information: static versus dynamic, partial versus full, and perfect versus imperfect.","To substantiate our findings, we provide simulation results and offer conclusions based on the outcomes of our investigation."],"url":"http://arxiv.org/abs/2402.12103v1","category":"eess.SP"}
{"created":"2024-02-19 12:37:27","title":"Design and Performance of Enhanced Spread Spectrum Aloha for Unsourced Multiple Access","abstract":"We analyze the performance of enhanced spread spectrum Aloha (E-SSA) in the framework of unsourced multiple access (UMAC). The asynchronous, unframed transmission of E-SSA is modified to enable a direct comparison with framed UMAC schemes, as well as with the Polyanskiy's achievability bound. The design of E-SSA is tailored to the peculiarities of the UMAC setting, resorting to short polar codes and the use of a timing channel to improve the energy efficiency of the protocol. We assess the impact of the preamble length and of the spreading factor on the system efficiency. The resulting scheme exhibits simplicity at the transmitter and linear complexity with respect to the number of active users at the receiver, approaching the UMAC achievability bound in close competition with the best known UMAC schemes.","sentences":["We analyze the performance of enhanced spread spectrum Aloha (E-SSA) in the framework of unsourced multiple access (UMAC).","The asynchronous, unframed transmission of E-SSA is modified to enable a direct comparison with framed UMAC schemes, as well as with the Polyanskiy's achievability bound.","The design of E-SSA is tailored to the peculiarities of the UMAC setting, resorting to short polar codes and the use of a timing channel to improve the energy efficiency of the protocol.","We assess the impact of the preamble length and of the spreading factor on the system efficiency.","The resulting scheme exhibits simplicity at the transmitter and linear complexity with respect to the number of active users at the receiver, approaching the UMAC achievability bound in close competition with the best known UMAC schemes."],"url":"http://arxiv.org/abs/2402.12101v1","category":"cs.IT"}
{"created":"2024-02-19 12:26:12","title":"Broadband ferromagnetic resonance in Mn-doped Li ferrite nanoparticles","abstract":"Lithium ferrites are well known materials due to their numerous technological applications especially in microwave devices. Mn-doped lithium ferrite nanoparticles were prepared by sol-gel technique by means of Pechini method, and then annealed at different temperatures in 250 to 1000 {\\deg}C range. XRD confirms spinel formation with particle size in the 15 to 200 nm range, with increased size with annealing temperature. Microwave magnetoabsorption data of annealed lithium ferrite nanoparticles, obtained with a broadband system based on a network analyzer operating up to 8.5 GHz are presented. At fields up to 200 mT we can observe a broad absorption peak that shifts to higher frequencies with magnetic field according to ferromagnetic resonance theory. The amplitude of absorption, up to 85%, together with the frequency width of about 4.5 GHz makes this material suitable as wave absorber. Samples annealed at higher temperatures show a behaviour similar to polycrystalline samples, thus suggesting their multidomain character","sentences":["Lithium ferrites are well known materials due to their numerous technological applications especially in microwave devices.","Mn-doped lithium ferrite nanoparticles were prepared by sol-gel technique by means of Pechini method, and then annealed at different temperatures in 250 to 1000 {","\\deg}C range.","XRD confirms spinel formation with particle size in the 15 to 200 nm range, with increased size with annealing temperature.","Microwave magnetoabsorption data of annealed lithium ferrite nanoparticles, obtained with a broadband system based on a network analyzer operating up to 8.5 GHz are presented.","At fields up to 200 mT we can observe a broad absorption peak that shifts to higher frequencies with magnetic field according to ferromagnetic resonance theory.","The amplitude of absorption, up to 85%, together with the frequency width of about 4.5 GHz makes this material suitable as wave absorber.","Samples annealed at higher temperatures show a behaviour similar to polycrystalline samples, thus suggesting their multidomain character"],"url":"http://arxiv.org/abs/2402.12096v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 12:13:33","title":"Parameter Refinement of a Ballbot and Predictive Control for Reference Tracking with Linear Parameter-Varying Embedding","abstract":"In this study, we implement a control method for stabilizing a ballbot that simultaneously follows a reference. A ballbot is a robot balancing on a spherical wheel where the single point of contact with the ground makes it omnidirectional and highly maneuverable but with inherent instability. After introducing the scheduling parameters, we start the analysis by embedding the nonlinear dynamic model derived from first principles to a linear parameter-varying (LPV) formulation. Continuously, and as an extension of a past study, we refine the parameters of the nonlinear model that enhance significantly its accuracy. The crucial advantages of the LPV formulation are that it consists of a nonlinear predictor that can be used in model predictive control (MPC) by retaining the convexity of the quadratic optimization problem with linear constraints and further evades computational burdens that appear in other nonlinear MPC methods with only a slight loss in performance. The LPVMPC control method can be solved efficiently as a quadratic program (QP) that provides timing that supports real-time implementation. Finally, to illustrate the method, we test the control designs on a two-set point 1D non-smooth reference with sudden changes, to a 2D nonstationary smooth reference known as Lissajous curves, and to a single-set point 1D non-smooth reference where for this case theoretical guarantees such as stability and recursive feasibility are provided.","sentences":["In this study, we implement a control method for stabilizing a ballbot that simultaneously follows a reference.","A ballbot is a robot balancing on a spherical wheel where the single point of contact with the ground makes it omnidirectional and highly maneuverable but with inherent instability.","After introducing the scheduling parameters, we start the analysis by embedding the nonlinear dynamic model derived from first principles to a linear parameter-varying (LPV) formulation.","Continuously, and as an extension of a past study, we refine the parameters of the nonlinear model that enhance significantly its accuracy.","The crucial advantages of the LPV formulation are that it consists of a nonlinear predictor that can be used in model predictive control (MPC) by retaining the convexity of the quadratic optimization problem with linear constraints and further evades computational burdens that appear in other nonlinear MPC methods with only a slight loss in performance.","The LPVMPC control method can be solved efficiently as a quadratic program (QP) that provides timing that supports real-time implementation.","Finally, to illustrate the method, we test the control designs on a two-set point 1D non-smooth reference with sudden changes, to a 2D nonstationary smooth reference known as Lissajous curves, and to a single-set point 1D non-smooth reference where for this case theoretical guarantees such as stability and recursive feasibility are provided."],"url":"http://arxiv.org/abs/2402.12092v1","category":"math.OC"}
{"created":"2024-02-19 12:07:27","title":"Navigating simplicity and complexity of social-ecological systems through a dialog between dynamical systems and agent-based models","abstract":"Social-ecological systems (SES) research aims to understand the nature of social-ecological phenomena, to find effective ways to foster or manage conditions under which desirable phenomena, such as sustainable resource use, occur or to change conditions or reduce the negative consequences of undesirable phenomena, such as poverty traps. Challenges such as these are often addressed using dynamical systems models (DSM) or agent-based models (ABM). Both modeling approaches have strengths and weaknesses. DSM are praised for their analytical tractability and efficient exploration of asymptotic dynamics and bifurcation, which are enabled by reduced number and heterogeneity of system components. ABM allows representing heterogeneity, agency, learning and interactions of diverse agents within SES, but this also comes at a price such as inefficiency to explore asymptotic dynamics or bifurcations. In this paper we combine DSM and ABM to leverage strengths of each modeling technique and gain deeper insights into dynamics of a system. We start with an ABM and research questions that the ABM was not able to answer. Using results of the ABM analysis as inputs for DSM, we create a DSM. Stability and bifurcation analysis of the DSM gives partial answers to the research questions and direct attention to where additional details are needed. This informs further ABM analysis, prevents burdening the ABM with less important details and reveals new insights about system dynamics. The iterative process and dialogue between the ABM and DSM leads to more complete answers to research questions and surpasses insights provided by each of the models separately. We illustrate the procedure with the example of the emergence of poverty traps in an agricultural system with endogenously driven innovation.","sentences":["Social-ecological systems (SES) research aims to understand the nature of social-ecological phenomena, to find effective ways to foster or manage conditions under which desirable phenomena, such as sustainable resource use, occur or to change conditions or reduce the negative consequences of undesirable phenomena, such as poverty traps.","Challenges such as these are often addressed using dynamical systems models (DSM) or agent-based models (ABM).","Both modeling approaches have strengths and weaknesses.","DSM are praised for their analytical tractability and efficient exploration of asymptotic dynamics and bifurcation, which are enabled by reduced number and heterogeneity of system components.","ABM allows representing heterogeneity, agency, learning and interactions of diverse agents within SES, but this also comes at a price such as inefficiency to explore asymptotic dynamics or bifurcations.","In this paper we combine DSM and ABM to leverage strengths of each modeling technique and gain deeper insights into dynamics of a system.","We start with an ABM and research questions that the ABM was not able to answer.","Using results of the ABM analysis as inputs for DSM, we create a DSM.","Stability and bifurcation analysis of the DSM gives partial answers to the research questions and direct attention to where additional details are needed.","This informs further ABM analysis, prevents burdening the ABM with less important details and reveals new insights about system dynamics.","The iterative process and dialogue between the ABM and DSM leads to more complete answers to research questions and surpasses insights provided by each of the models separately.","We illustrate the procedure with the example of the emergence of poverty traps in an agricultural system with endogenously driven innovation."],"url":"http://arxiv.org/abs/2402.12086v1","category":"cs.MA"}
{"created":"2024-02-19 12:06:24","title":"X-ray multibeam ptychography at up to 20 keV: nano-lithography enhances X-ray nano-imaging","abstract":"Non-destructive nano-imaging of the internal structure of solid matter is only feasible using hard X-rays due to their high penetration. The highest resolution images are achieved at synchrotron radiation sources (SRF), offering superior spectral brightness and enabling methods such as X-ray ptychography delivering single-digit nm resolution. However the resolution or field of view is ultimately constrained by the available coherent flux. To address this, the beam's incoherent fraction can be exploited using multiple parallel beams in an approach known as X-ray multibeam ptychography (MBP). This expands the domain of X-ray ptychography to larger samples or more rapid measurements. Both qualities favor the study of complex composite or functional samples, such as catalysts, energy materials, or electronic devices. The challenges of performing ptychography at high energy and with many parallel beams must be overcome to extract the full advantages for extended samples while minimizing beam attenuation. Here, we report the application of MBP with up to 12 beams and at photon energies of 13 and 20 keV. We demonstrate performance for various samples: a Siemens star test pattern, a porous Ni/\\ce{Al2O3} catalyst, a microchip, and gold nano-crystal clusters, exceeding the measurement limits of conventional hard X-ray ptychography without compromising image quality.","sentences":["Non-destructive nano-imaging of the internal structure of solid matter is only feasible using hard X-rays due to their high penetration.","The highest resolution images are achieved at synchrotron radiation sources (SRF), offering superior spectral brightness and enabling methods such as X-ray ptychography delivering single-digit nm resolution.","However the resolution or field of view is ultimately constrained by the available coherent flux.","To address this, the beam's incoherent fraction can be exploited using multiple parallel beams in an approach known as X-ray multibeam ptychography (MBP).","This expands the domain of X-ray ptychography to larger samples or more rapid measurements.","Both qualities favor the study of complex composite or functional samples, such as catalysts, energy materials, or electronic devices.","The challenges of performing ptychography at high energy and with many parallel beams must be overcome to extract the full advantages for extended samples while minimizing beam attenuation.","Here, we report the application of MBP with up to 12 beams and at photon energies of 13 and 20 keV.","We demonstrate performance for various samples: a Siemens star test pattern, a porous Ni/\\ce{Al2O3} catalyst, a microchip, and gold nano-crystal clusters, exceeding the measurement limits of conventional hard X-ray ptychography without compromising image quality."],"url":"http://arxiv.org/abs/2402.12082v1","category":"physics.app-ph"}
{"created":"2024-02-19 12:04:25","title":"Can LLMs Compute with Reasons?","abstract":"Large language models (LLMs) often struggle with complex mathematical tasks, prone to \"hallucinating\" incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data. To address this challenge, we propose an \"Inductive Learning\" approach utilizing a distributed network of SLMs. This network leverages error-based learning and hint incorporation to refine the reasoning capabilities of SLMs. Our goal is to provide a framework that empowers SLMs to approach the level of logic-based applications achieved by high-parameter models, potentially benefiting any language model. Ultimately, this novel concept paves the way for bridging the logical gap between humans and LLMs across various fields.","sentences":["Large language models (LLMs) often struggle with complex mathematical tasks, prone to \"hallucinating\" incorrect answers due to their reliance on statistical patterns.","This limitation is further amplified in average Small LangSLMs with limited context and training data.","To address this challenge, we propose an \"Inductive Learning\" approach utilizing a distributed network of SLMs.","This network leverages error-based learning and hint incorporation to refine the reasoning capabilities of SLMs.","Our goal is to provide a framework that empowers SLMs to approach the level of logic-based applications achieved by high-parameter models, potentially benefiting any language model.","Ultimately, this novel concept paves the way for bridging the logical gap between humans and LLMs across various fields."],"url":"http://arxiv.org/abs/2402.12080v1","category":"cs.CL"}
{"created":"2024-02-19 11:55:17","title":"Single and Multi-Objective Real-Time Optimisation of an Industrial Injection Moulding Process via a Bayesian Adaptive Design of Experiment Approach","abstract":"Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM). Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space. Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design. In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time. A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling. This characterisation approach was verified by post-production metrology results.   A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed. The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach. ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II. Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II.","sentences":["Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM).","Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space.","Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design.","In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time.","A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling.","This characterisation approach was verified by post-production metrology results.   ","A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed.","The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach.","ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II.","Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II."],"url":"http://arxiv.org/abs/2402.12077v1","category":"eess.SY"}
{"created":"2024-02-19 11:53:42","title":"Order Estimation of Linear-Phase FIR Filters for DAC Equalization in Multiple Nyquist Bands","abstract":"This letter considers the design of linear-phase finite-length impulse response (FIR) filters for equalization of the frequency responses of digital-to-analog converters (DACs). The letter derives estimates for the filter orders required, as functions of the bandwidth and equalization accuracy, for four DAC pulses that are used in DACs in multiple Nyquist bands. The estimates are derived through a large set of minimax-optimal equalizers and the use of symbolic regression followed by minimax-optimal curve fitting for further enhancement. Design examples included demonstrate the accuracy of the proposed estimates. In addition, the letter discusses the appropriateness of the four types of linear-phase FIR filters, for the different equalizer cases, as well as the corresponding properties of the equalized systems.","sentences":["This letter considers the design of linear-phase finite-length impulse response (FIR) filters for equalization of the frequency responses of digital-to-analog converters (DACs).","The letter derives estimates for the filter orders required, as functions of the bandwidth and equalization accuracy, for four DAC pulses that are used in DACs in multiple Nyquist bands.","The estimates are derived through a large set of minimax-optimal equalizers and the use of symbolic regression followed by minimax-optimal curve fitting for further enhancement.","Design examples included demonstrate the accuracy of the proposed estimates.","In addition, the letter discusses the appropriateness of the four types of linear-phase FIR filters, for the different equalizer cases, as well as the corresponding properties of the equalized systems."],"url":"http://arxiv.org/abs/2402.12075v1","category":"eess.SP"}
{"created":"2024-02-19 11:33:38","title":"Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength","abstract":"In this letter, we investigate the performance of Max Minimum Fairness (MMF) for uplink Rate-Splitting Multiple Access (RSMA) in short-packet communications. Specifically, considering a Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we optimize the transmit power allocation between the splitting user messages to maximize the minimum rate among users with Finite Blocklength (FBL) constraints. To tackle this problem, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Numerical results show that RSMA outperforms conventional transmission schemes such as Non-orthogonal Multiple Access (NOMA) in terms of MMF.","sentences":["In this letter, we investigate the performance of Max Minimum Fairness (MMF) for uplink Rate-Splitting Multiple Access (RSMA) in short-packet communications.","Specifically, considering a Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we optimize the transmit power allocation between the splitting user messages to maximize the minimum rate among users with Finite Blocklength (FBL) constraints.","To tackle this problem, we propose a Successive Convex Approximation (SCA)-based approach.","Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver.","Numerical results show that RSMA outperforms conventional transmission schemes such as Non-orthogonal Multiple Access (NOMA) in terms of MMF."],"url":"http://arxiv.org/abs/2402.12066v1","category":"cs.IT"}
{"created":"2024-02-19 11:26:37","title":"Design and evaluation of a multi-finger skin-stretch tactile interface for hand rehabilitation robots","abstract":"Object properties perceived through the tactile sense, such as weight, friction, and slip, greatly influence motor control during manipulation tasks. However, the provision of tactile information during robotic training in neurorehabilitation has not been well explored. Therefore, we designed and evaluated a tactile interface based on a two-degrees-of-freedom moving platform mounted on a hand rehabilitation robot that provides skin stretch at four fingertips, from the index through the little finger. To accurately control the rendered forces, we included a custom magnetic-based force sensor to control the tactile interface in a closed loop. The technical evaluation showed that our custom force sensor achieved measurable shear forces of +-8N with accuracies of 95.2-98.4% influenced by hysteresis, viscoelastic creep, and torsional deformation. The tactile interface accurately rendered forces with a step response steady-state accuracy of 97.5-99.4% and a frequency response in the range of most activities of daily living. Our sensor showed the highest measurement-range-to-size ratio and comparable accuracy to sensors of its kind. These characteristics enabled the closed-loop force control of the tactile interface for precise rendering of multi-finger two-dimensional skin stretch. The proposed system is a first step towards more realistic and rich haptic feedback during robotic sensorimotor rehabilitation, potentially improving therapy outcomes.","sentences":["Object properties perceived through the tactile sense, such as weight, friction, and slip, greatly influence motor control during manipulation tasks.","However, the provision of tactile information during robotic training in neurorehabilitation has not been well explored.","Therefore, we designed and evaluated a tactile interface based on a two-degrees-of-freedom moving platform mounted on a hand rehabilitation robot that provides skin stretch at four fingertips, from the index through the little finger.","To accurately control the rendered forces, we included a custom magnetic-based force sensor to control the tactile interface in a closed loop.","The technical evaluation showed that our custom force sensor achieved measurable shear forces of +-8N with accuracies of 95.2-98.4% influenced by hysteresis, viscoelastic creep, and torsional deformation.","The tactile interface accurately rendered forces with a step response steady-state accuracy of 97.5-99.4% and a frequency response in the range of most activities of daily living.","Our sensor showed the highest measurement-range-to-size ratio and comparable accuracy to sensors of its kind.","These characteristics enabled the closed-loop force control of the tactile interface for precise rendering of multi-finger two-dimensional skin stretch.","The proposed system is a first step towards more realistic and rich haptic feedback during robotic sensorimotor rehabilitation, potentially improving therapy outcomes."],"url":"http://arxiv.org/abs/2402.12060v1","category":"cs.RO"}
{"created":"2024-02-19 11:19:02","title":"Are LLM-based Evaluators Confusing NLG Quality Criteria?","abstract":"Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.","sentences":["Some prior work has shown that LLMs perform well in NLG evaluation for different tasks.","However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability.","For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves.","So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved.","Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs.","We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations.","Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation."],"url":"http://arxiv.org/abs/2402.12055v1","category":"cs.CL"}
{"created":"2024-02-19 11:09:03","title":"From higher-order rewriting systems to higher-order categorial algebras and higher-order Curry-Howard isomorphisms","abstract":"This ongoing project aims to define and investigate, from the standpoint of category theory, order theory and universal algebra, the notions of higher-order many-sorted rewriting system and of higher-order many-sorted categorial algebra and their relationships, via the higher-order Curry-Howard isomorphisms.   The ultimate goal, to be developed in future versions of this work, is to define and investigate the category of towers, whose objects will consist of families, indexed by $\\mathbb{N}$, of higher-order many-sorted rewriting systems and of higher-order many-sorted categorial algebras, including higher-order Curry-Howard type results for the latter, together with an additional structure that intertwines such $\\mathbb{N}$-families; and whose morphism from a tower to another will be families, indexed by $\\mathbb{N}$, of morphisms between its higher-order many-sorted rewriting systems and of higher-order many-sorted categorial algebras compatible with their structures. All feedback is appreciated.","sentences":["This ongoing project aims to define and investigate, from the standpoint of category theory, order theory and universal algebra, the notions of higher-order many-sorted rewriting system and of higher-order many-sorted categorial algebra and their relationships, via the higher-order Curry-Howard isomorphisms.   ","The ultimate goal, to be developed in future versions of this work, is to define and investigate the category of towers, whose objects will consist of families, indexed by $\\mathbb{N}$, of higher-order many-sorted rewriting systems and of higher-order many-sorted categorial algebras, including higher-order Curry-Howard type results for the latter, together with an additional structure that intertwines such $\\mathbb{N}$-families; and whose morphism from a tower to another will be families, indexed by $\\mathbb{N}$, of morphisms between its higher-order many-sorted rewriting systems and of higher-order many-sorted categorial algebras compatible with their structures.","All feedback is appreciated."],"url":"http://arxiv.org/abs/2402.12051v1","category":"math.CT"}
{"created":"2024-02-19 10:56:28","title":"Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenge","abstract":"In this paper, we provide a survey on automotive surround-view fisheye optics, with an emphasis on the impact of optical artifacts on computer vision tasks in autonomous driving and ADAS. The automotive industry has advanced in applying state-of-the-art computer vision to enhance road safety and provide automated driving functionality. When using camera systems on vehicles, there is a particular need for a wide field of view to capture the entire vehicle's surroundings, in areas such as low-speed maneuvering, automated parking, and cocoon sensing. However, one crucial challenge in surround-view cameras is the strong optical aberrations of the fisheye camera, which is an area that has received little attention in the literature. Additionally, a comprehensive dataset is needed for testing safety-critical scenarios in vehicle automation. The industry has turned to simulation as a cost-effective strategy for creating synthetic datasets with surround-view camera imagery. We examine different simulation methods (such as model-driven and data-driven simulations) and discuss the simulators' ability (or lack thereof) to model real-world optical performance. Overall, this paper highlights the optical aberrations in automotive fisheye datasets, and the limitations of optical reality in simulated fisheye datasets, with a focus on computer vision in surround-view optical systems.","sentences":["In this paper, we provide a survey on automotive surround-view fisheye optics, with an emphasis on the impact of optical artifacts on computer vision tasks in autonomous driving and ADAS.","The automotive industry has advanced in applying state-of-the-art computer vision to enhance road safety and provide automated driving functionality.","When using camera systems on vehicles, there is a particular need for a wide field of view to capture the entire vehicle's surroundings, in areas such as low-speed maneuvering, automated parking, and cocoon sensing.","However, one crucial challenge in surround-view cameras is the strong optical aberrations of the fisheye camera, which is an area that has received little attention in the literature.","Additionally, a comprehensive dataset is needed for testing safety-critical scenarios in vehicle automation.","The industry has turned to simulation as a cost-effective strategy for creating synthetic datasets with surround-view camera imagery.","We examine different simulation methods (such as model-driven and data-driven simulations) and discuss the simulators' ability (or lack thereof) to model real-world optical performance.","Overall, this paper highlights the optical aberrations in automotive fisheye datasets, and the limitations of optical reality in simulated fisheye datasets, with a focus on computer vision in surround-view optical systems."],"url":"http://arxiv.org/abs/2402.12041v1","category":"cs.CV"}
{"created":"2024-02-19 10:52:28","title":"On Higher Topological T-duality Functors","abstract":"We propose a higher version of the Topological T-duality Functor $P$ of Bunke and co-workers \\cite{BunkeS1}. We begin with an introduction to T-duality in string theory and Topological T-duality. We then study morphisms in the category of pairs $\\C_X$ over $X$ defined in Ref.\\ \\cite{BunkeS1}. For every $X$ we naturally construct a simplicial complex $G(X)$ from $\\C_X$ and study its homotopy properties. We calculate all the homotopy groups of $G(X).$ We show that $P$ is naturally $\\pi_0(G(X)) \\simeq P_0(X)$ and define $P_i$ as the higher homotopy groups of $G(X).$ We argue that the $P_i$ are invariant under the action of Topological T-duality on $\\C_X.$ We argue that $G(X)$ is a way of labelling toroidally compactified vacua in string field theory with $X$ as the space of uncompactified directions. We also argue that $P_0, P_1$ may be described naturally in terms of string field theory. We study some properties of the functors $P_0,P_1.$ We show the connection between these functors and topological T-duality for triples studied in Ref.\\ \\cite{Pan2}. We also show that this approach to Topological T-duality gives us a natural definition of T-folds \\cite{HullT}.","sentences":["We propose a higher version of the Topological T-duality Functor $P$ of Bunke and co-workers \\cite{BunkeS1}.","We begin with an introduction to T-duality in string theory and Topological T-duality.","We then study morphisms in the category of pairs $\\C_X$ over $X$ defined in Ref.\\ \\cite{BunkeS1}.","For every $X$ we naturally construct a simplicial complex $G(X)$ from $\\C_X$ and study its homotopy properties.","We calculate all the homotopy groups of $G(X).$ We show that $P$ is naturally $\\pi_0(G(X))","\\simeq P_0(X)$ and define $P_i$ as the higher homotopy groups of $G(X).$ We argue that the $P_i$ are invariant under the action of Topological T-duality on $\\C_X.$ We argue that $G(X)$ is a way of labelling toroidally compactified vacua in string field theory with $X$ as the space of uncompactified directions.","We also argue that $P_0, P_1$ may be described naturally in terms of string field theory.","We study some properties of the functors $P_0,P_1.$ We show the connection between these functors and topological T-duality for triples studied in Ref.\\ \\cite{Pan2}.","We also show that this approach to Topological T-duality gives us a natural definition of T-folds \\cite{HullT}."],"url":"http://arxiv.org/abs/2402.12039v1","category":"math-ph"}
{"created":"2024-02-19 10:45:04","title":"Numerical simulations of Josephson Traveling Wave Parametric Amplifiers (JTWPAs): comparative study of open-source tools","abstract":"Josephson Traveling Wave Parametric Amplifiers (JTWPAs) are largely exploited in quantum technologies for their broadband and low noise performance in the microwave regime. When one or more microwave tones are applied at the input, such devices show a complex wave-mixing response due to their intrinsic nonlinear nature. Numerical simulations of the JTWPAs nonlinear behaviour provide useful insights not only for the design of such devices, but also for the interpretation and validation of the experimental results. Here we present and discuss a comparative analysis of different open-source tools which can be used for JTWPAs numerical simulations. We focus on two tools for transient simulations, WRSPICE and PSCAN2, and on one tool for direct simulation of the frequency domain behaviour, JosephsonCircuit.jl. We describe the working principle of these three tools and test them considering as a benchmark a JTWPA based on SNAILs (Superconducting Nonlinear Asymmetric Inductive eLement) with realistic experimental parameters. Our results can serve as a guide for numerical simulations of JTWPAs with open-source tools, highlighting advantages and disadvantages depending on the simulation tasks.","sentences":["Josephson Traveling Wave Parametric Amplifiers (JTWPAs) are largely exploited in quantum technologies for their broadband and low noise performance in the microwave regime.","When one or more microwave tones are applied at the input, such devices show a complex wave-mixing response due to their intrinsic nonlinear nature.","Numerical simulations of the JTWPAs nonlinear behaviour provide useful insights not only for the design of such devices, but also for the interpretation and validation of the experimental results.","Here we present and discuss a comparative analysis of different open-source tools which can be used for JTWPAs numerical simulations.","We focus on two tools for transient simulations, WRSPICE and PSCAN2, and on one tool for direct simulation of the frequency domain behaviour, JosephsonCircuit.jl.","We describe the working principle of these three tools and test them considering as a benchmark a JTWPA based on SNAILs (Superconducting Nonlinear Asymmetric Inductive eLement) with realistic experimental parameters.","Our results can serve as a guide for numerical simulations of JTWPAs with open-source tools, highlighting advantages and disadvantages depending on the simulation tasks."],"url":"http://arxiv.org/abs/2402.12037v1","category":"quant-ph"}
{"created":"2024-02-19 10:40:51","title":"Constraining the stellar populations of ultra-diffuse galaxies in the MATLAS survey using spectral energy distribution fitting","abstract":"We use spectral energy distribution (SED) fitting to place constraints on the stellar populations of 59 ultra-diffuse galaxies (UDGs) in the low-to-moderate density fields of the MATLAS survey. We use the routine PROSPECTOR, coupled with archival data in the optical from DECaLS, and near- and mid-infrared imaging from WISE, to recover the stellar masses, ages, metallicities and star formation timescales of the UDGs. We find that a subsample of the UDGs lies within the scatter of the mass-metallicity relation (MZR) for local classical dwarfs. However, another subsample is more metal-poor, being consistent with the evolving MZR at high-redshift. We investigate UDG positioning trends in the mass-metallicity plane as a function of surface brightness, effective radius, axis ratio, local volume density, mass-weighted age, star formation timescale, globular cluster (GC) counts and GC specific frequency. We find that our sample of UDGs can be separated into two main classes. Class A: Comprised of UDGs with lower stellar masses, prolonged star formation histories (SFHs), more elongated, inhabiting less dense environments, hosting fewer GCs, younger, consistent with the classical dwarf MZR, and fainter. Class B: UDGs with higher stellar masses, rapid SFHs, rounder, inhabiting the densest of our probed environments, hosting on average the most numerous GC systems, older, consistent with the high-redshift MZR (i.e., consistent with early-quenching), and brighter. The combination of these properties suggests that UDGs of Class A are consistent with a `puffed-up dwarf' formation scenario, while UDGs of Class B seem to be better explained by `failed galaxy' scenarios.","sentences":["We use spectral energy distribution (SED) fitting to place constraints on the stellar populations of 59 ultra-diffuse galaxies (UDGs) in the low-to-moderate density fields of the MATLAS survey.","We use the routine PROSPECTOR, coupled with archival data in the optical from DECaLS, and near- and mid-infrared imaging from WISE, to recover the stellar masses, ages, metallicities and star formation timescales of the UDGs.","We find that a subsample of the UDGs lies within the scatter of the mass-metallicity relation (MZR) for local classical dwarfs.","However, another subsample is more metal-poor, being consistent with the evolving MZR at high-redshift.","We investigate UDG positioning trends in the mass-metallicity plane as a function of surface brightness, effective radius, axis ratio, local volume density, mass-weighted age, star formation timescale, globular cluster (GC) counts and GC specific frequency.","We find that our sample of UDGs can be separated into two main classes.","Class A: Comprised of UDGs with lower stellar masses, prolonged star formation histories (SFHs), more elongated, inhabiting less dense environments, hosting fewer GCs, younger, consistent with the classical dwarf MZR, and fainter.","Class B: UDGs with higher stellar masses, rapid SFHs, rounder, inhabiting the densest of our probed environments, hosting on average the most numerous GC systems, older, consistent with the high-redshift MZR (i.e., consistent with early-quenching), and brighter.","The combination of these properties suggests that UDGs of Class A are consistent with a `puffed-up dwarf' formation scenario, while UDGs of Class B seem to be better explained by `failed galaxy' scenarios."],"url":"http://arxiv.org/abs/2402.12033v1","category":"astro-ph.GA"}
{"created":"2024-02-19 10:39:14","title":"Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets","abstract":"In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed. To this aim, a robust optimization algorithm is developed to account for the asymmetric nature of the uncertainties that characterize the market prices, as well as the energy production of the RVPP stochastic sources and flexible demand consumption. Simulation results show increased RVPP benefits compared to other existing solutions and demonstrate the potential of renewable sources to further increase their economic competitiveness. The simplicity of the implementation, the computational efficiency, and the flexible robustness are also verified.","sentences":["In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed.","To this aim, a robust optimization algorithm is developed to account for the asymmetric nature of the uncertainties that characterize the market prices, as well as the energy production of the RVPP stochastic sources and flexible demand consumption.","Simulation results show increased RVPP benefits compared to other existing solutions and demonstrate the potential of renewable sources to further increase their economic competitiveness.","The simplicity of the implementation, the computational efficiency, and the flexible robustness are also verified."],"url":"http://arxiv.org/abs/2402.12032v1","category":"eess.SY"}
{"created":"2024-02-19 10:35:55","title":"Semi-Markov multistate modeling approaches for multicohort event history data","abstract":"Two Cox-based multistate modeling approaches are compared for analyzing a complex multicohort event history process. The first approach incorporates cohort information as a fixed covariate, thereby providing a direct estimation of the cohort-specific effects. The second approach includes the cohort as stratum variable, thus giving an extra flexibility in estimating the transition probabilities. Additionally, both approaches may include possible interaction terms between the cohort and a given prognostic predictor. Furthermore, the Markov property conditional on observed prognostic covariates is assessed using a global score test. Whenever departures from the Markovian assumption are revealed for a given transition, the time of entry into the current state is incorporated as a fixed covariate, yielding a semi-Markov process. The two proposed methods are applied to a three-wave dataset of COVID-19-hospitalized adults in the southern Barcelona metropolitan area (Spain), and the corresponding performance is discussed. While both semi-Markovian approaches are shown to be useful, the preferred one will depend on the focus of the inference. To summarize, the cohort-covariate approach enables an insightful discussion on the the behavior of the cohort effects, whereas the stratum-cohort approach provides flexibility to estimate transition-specific underlying risks according with the different cohorts","sentences":["Two Cox-based multistate modeling approaches are compared for analyzing a complex multicohort event history process.","The first approach incorporates cohort information as a fixed covariate, thereby providing a direct estimation of the cohort-specific effects.","The second approach includes the cohort as stratum variable, thus giving an extra flexibility in estimating the transition probabilities.","Additionally, both approaches may include possible interaction terms between the cohort and a given prognostic predictor.","Furthermore, the Markov property conditional on observed prognostic covariates is assessed using a global score test.","Whenever departures from the Markovian assumption are revealed for a given transition, the time of entry into the current state is incorporated as a fixed covariate, yielding a semi-Markov process.","The two proposed methods are applied to a three-wave dataset of COVID-19-hospitalized adults in the southern Barcelona metropolitan area (Spain), and the corresponding performance is discussed.","While both semi-Markovian approaches are shown to be useful, the preferred one will depend on the focus of the inference.","To summarize, the cohort-covariate approach enables an insightful discussion on the the behavior of the cohort effects, whereas the stratum-cohort approach provides flexibility to estimate transition-specific underlying risks according with the different cohorts"],"url":"http://arxiv.org/abs/2402.12027v1","category":"stat.AP"}
{"created":"2024-02-19 10:29:03","title":"Light storage in wavy dielectric grating with Kerr nonlinearity","abstract":"Periodical corrugation in dielectric slab transfers the two waveguide modes at zero Bloch wave number into a leaky resonant mode and a symmetry protected bound states in the continuum (BIC) with small frequency detune. The leaky resonant mode can be directly excited by weak linearly polarized normally incident optical field. In the presence of Kerr nonlinearity, the BIC can be indirectly excited by an optical bistable response. Two types of bistable operations are considered. For the first type, the intensity of the incident field gradually increases to exceed a critical value, and then decreases to zero. For the second type, the intensity is fixed, while the linear polarization angle of the incident field gradually increases to exceed a critical value, and then decreases to 0$^{o}$. Theoretically, the indirectly excited BIC can store the optical energy without loss, even though the intensity of the incident field decreases to zero. Incidence of an optical field with double frequency or orthogonal linear polarization can erase the stored optical field by destroying the BIC. The proposed optical system could function as optical storage and switching device.","sentences":["Periodical corrugation in dielectric slab transfers the two waveguide modes at zero Bloch wave number into a leaky resonant mode and a symmetry protected bound states in the continuum (BIC) with small frequency detune.","The leaky resonant mode can be directly excited by weak linearly polarized normally incident optical field.","In the presence of Kerr nonlinearity, the BIC can be indirectly excited by an optical bistable response.","Two types of bistable operations are considered.","For the first type, the intensity of the incident field gradually increases to exceed a critical value, and then decreases to zero.","For the second type, the intensity is fixed, while the linear polarization angle of the incident field gradually increases to exceed a critical value, and then decreases to 0$^{o}$.","Theoretically, the indirectly excited BIC can store the optical energy without loss, even though the intensity of the incident field decreases to zero.","Incidence of an optical field with double frequency or orthogonal linear polarization can erase the stored optical field by destroying the BIC.","The proposed optical system could function as optical storage and switching device."],"url":"http://arxiv.org/abs/2402.12020v1","category":"physics.optics"}
{"created":"2024-02-19 10:23:37","title":"Even-Cycle Detection in the Randomized and Quantum CONGEST Model","abstract":"We show that, for every $k\\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the \\CONGEST{} model by a randomized Monte-Carlo distributed algorithm with one-sided error probability $1/3$. This matches the best round-complexities of previously known algorithms for $k\\in\\{2,3,4,5\\}$ by Drucker et al. [PODC'14] and Censor-Hillel et al. [DISC'20], but improves the complexities of the known algorithms for $k>5$ by Eden et al. [DISC'19], which were essentially of the form $\\tilde O(n^{1-2/k^2})$. Our algorithm uses colored BFS-explorations with threshold, but with an original \\emph{global} approach that enables to overcome a recent impossibility result by Fraigniaud et al. [SIROCCO'23] about using colored BFS-exploration with \\emph{local} threshold for detecting cycles.   We also show how to quantize our algorithm for achieving a round-complexity $\\tilde O(n^{\\frac{1}{2}-\\frac{1}{2k}})$ in the quantum setting for deciding $C_{2k}$ freeness. Furthermore, this allows us to improve the known quantum complexities of the simpler problem of detecting cycles of length \\emph{at most}~$2k$ by van Apeldoorn and de Vos [PODC'22]. Our quantization is in two steps. First, the congestion of our randomized algorithm is reduced, to the cost of reducing its success probability too. Second, the success probability is boosted using a new quantum framework derived from sequential algorithms, namely Monte-Carlo quantum amplification.","sentences":["We show that, for every $k\\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the \\CONGEST{} model by a randomized Monte-Carlo distributed algorithm with one-sided error probability $1/3$. This matches the best round-complexities of previously known algorithms for $k\\in\\{2,3,4,5\\}$ by Drucker et al.","[PODC'14] and Censor-Hillel et al.","[DISC'20], but improves the complexities of the known algorithms for $k>5$ by Eden et al.","[DISC'19], which were essentially of the form $\\tilde O(n^{1-2/k^2})$. Our algorithm uses colored BFS-explorations with threshold, but with an original \\emph{global} approach that enables to overcome a recent impossibility result by Fraigniaud et al.","[SIROCCO'23] about using colored BFS-exploration with \\emph{local} threshold for detecting cycles.   ","We also show how to quantize our algorithm for achieving a round-complexity $\\tilde O(n^{\\frac{1}{2}-\\frac{1}{2k}})$ in the quantum setting for deciding $C_{2k}$ freeness.","Furthermore, this allows us to improve the known quantum complexities of the simpler problem of detecting cycles of length \\emph{at most}~$2k$ by van Apeldoorn and de Vos [PODC'22].","Our quantization is in two steps.","First, the congestion of our randomized algorithm is reduced, to the cost of reducing its success probability too.","Second, the success probability is boosted using a new quantum framework derived from sequential algorithms, namely Monte-Carlo quantum amplification."],"url":"http://arxiv.org/abs/2402.12018v1","category":"cs.DC"}
{"created":"2024-02-19 10:02:10","title":"Moduli of Continuity in Metric Models and Extension of Liveability Indices","abstract":"Index spaces serve as valuable metric models for studying properties relevant to various applications, such as social science or economics. These properties are represented by real Lipschitz functions that describe the degree of association with each element within the underlying metric space. After determining the index value within a given sample subset, the classic McShane and Whitney formulas allow a Lipschitz regression procedure to be performed to extend the index values over the entire metric space. To improve the adaptability of the metric model to specific scenarios, this paper introduces the concept of a composition metric, which involves composing a metric with an increasing, positive and subadditive function $\\phi$. The results presented here extend well-established results for Lipschitz indices on metric spaces to composition metrics. In addition, we establish the corresponding approximation properties that facilitate the use of this functional structure. To illustrate the power and simplicity of this mathematical framework, we provide a concrete application involving the modelling of livability indices in North American cities.","sentences":["Index spaces serve as valuable metric models for studying properties relevant to various applications, such as social science or economics.","These properties are represented by real Lipschitz functions that describe the degree of association with each element within the underlying metric space.","After determining the index value within a given sample subset, the classic McShane and Whitney formulas allow a Lipschitz regression procedure to be performed to extend the index values over the entire metric space.","To improve the adaptability of the metric model to specific scenarios, this paper introduces the concept of a composition metric, which involves composing a metric with an increasing, positive and subadditive function $\\phi$. The results presented here extend well-established results for Lipschitz indices on metric spaces to composition metrics.","In addition, we establish the corresponding approximation properties that facilitate the use of this functional structure.","To illustrate the power and simplicity of this mathematical framework, we provide a concrete application involving the modelling of livability indices in North American cities."],"url":"http://arxiv.org/abs/2402.12009v1","category":"stat.ME"}
{"created":"2024-02-19 09:59:24","title":"Infinitely many solutions for a class of fractional Schrodinger equations coupled with neutral scalar field","abstract":"We study the fractional Schr\\\"{o}dinger equations coupled with a neutral scalar field   $$   (-\\Delta)^s u+V(x)u=K(x)\\phi u +g(x)|u|^{q-2}u, \\quad x\\in \\mathbb{R}^3,\\qquad   (I-\\Delta)^t \\phi=K(x)u^2, \\quad x\\in \\mathbb{R}^3,   $$ where $(-\\Delta)^s$ and $(I-\\Delta)^t$ denote the fractional Laplacian and Bessel operators with $\\frac{3}{4} <s<1$ and $0<t<1$, respectively. Under some suitable assumptions for the external potentials $V$, $K$ and $g$, given $q\\in(1,2)\\cup(2,2_s^*)$ with $2_s^*:= \\frac{6}{3-2s}$, with the help of an improved Fountain theorem dealing with a class of strongly indefinite variational problems approached by Gu-Zhou [Adv. Nonlinear Stud., {\\bf 17} (2017), 727--738], we show that the system admits infinitely many nontrivial solutions.","sentences":["We study the fractional Schr\\\"{o}dinger equations coupled with a neutral scalar field   $$   (-\\Delta)^s u+V(x)u=K(x)\\phi u +g(x)|u|^{q-2}u, \\quad x\\in \\mathbb{R}^3,\\qquad   (I-\\Delta)^t \\phi=K(x)u^2, \\quad x\\in \\mathbb{R}^3,   $$ where $(-\\Delta)^s$ and $(I-\\Delta)^t$ denote the fractional Laplacian and Bessel operators with $\\frac{3}{4} <s<1$ and $0<t<1$, respectively.","Under some suitable assumptions for the external potentials $V$, $K$ and $g$, given $q\\in(1,2)\\cup(2,2_s^*)$ with $2_s^*:= \\frac{6}{3-2s}$, with the help of an improved Fountain theorem dealing with a class of strongly indefinite variational problems approached by Gu-Zhou","[Adv.","Nonlinear Stud., {\\bf 17} (2017), 727--738], we show that the system admits infinitely many nontrivial solutions."],"url":"http://arxiv.org/abs/2402.12006v1","category":"math.AP"}
{"created":"2024-02-19 09:51:03","title":"Mixed-Reality-Guided Teleoperation of a Collaborative Robot for Surgical Procedures","abstract":"The development of advanced surgical systems embedding the Master-Slave control strategy introduced the possibility of remote interaction between the surgeon and the patient, also known as teleoperation. The present paper aims to integrate innovative technologies into the teleoperation process to enhance workflow during surgeries. The proposed system incorporates a collaborative robot, Kuka IIWA LBR, and Hololens 2 (an augmented reality device), allowing the user to control the robot in an expansive environment that integrates actual (real data) with additional digital information imported via Hololens 2. Experimental data demonstrate the user's ability to control the Kuka IIWA using various gestures to position it with respect to real or digital objects. Thus, this system offers a novel solution to manipulate robots used in surgeries in a more intuitive manner, contributing to the reduction of the learning curve for surgeons. Calibration and testing in multiple scenarios demonstrate the efficiency of the system in providing seamless movements.","sentences":["The development of advanced surgical systems embedding the Master-Slave control strategy introduced the possibility of remote interaction between the surgeon and the patient, also known as teleoperation.","The present paper aims to integrate innovative technologies into the teleoperation process to enhance workflow during surgeries.","The proposed system incorporates a collaborative robot, Kuka IIWA LBR, and Hololens 2 (an augmented reality device), allowing the user to control the robot in an expansive environment that integrates actual (real data) with additional digital information imported via Hololens 2.","Experimental data demonstrate the user's ability to control the Kuka IIWA using various gestures to position it with respect to real or digital objects.","Thus, this system offers a novel solution to manipulate robots used in surgeries in a more intuitive manner, contributing to the reduction of the learning curve for surgeons.","Calibration and testing in multiple scenarios demonstrate the efficiency of the system in providing seamless movements."],"url":"http://arxiv.org/abs/2402.12002v1","category":"cs.RO"}
{"created":"2024-02-19 09:35:18","title":"Kicking the Can Down the Road: Understanding the Effects of Delaying the Deployment of Stratospheric Aerosol Injection","abstract":"Climate change is a prevalent threat, and it is unlikely that current mitigation efforts will be enough to avoid unwanted impacts. One potential option to reduce climate change impacts is the use of stratospheric aerosol injection (SAI). Even if SAI is ultimately deployed, it might be initiated only after some temperature target is exceeded. The consequences of such a delay are assessed herein. This study compares two cases, with the same target global mean temperature of 1.5C above preindustrial, but start dates of 2035 or a delayed start in 2045. We make use of simulations in the Community Earth System Model version 2 with the Whole Atmosphere Coupled Chemistry Model version 6 (CESM2-WACCM6), using SAI under the SSP2-4.5 emissions pathway. We find that delaying the start of deployment (relative to the target temperature) necessitates lower net radiative forcing (-30%) and thus larger sulfur dioxide injection rates (+20%), even after surface temperatures converge, to compensate for the extra energy absorbed by the Earth system. However, many of the surface climate differences between the 2035 and 2045 start simulations appear to be small during the 10-25 years following the delayed SAI start, although longer simulations would be needed to assess any longer-term impacts in this model. In addition, irreversibilities and tipping points that might be triggered during the period of increased warming may not be adequately represented in the model but could change this conclusion in the real world.","sentences":["Climate change is a prevalent threat, and it is unlikely that current mitigation efforts will be enough to avoid unwanted impacts.","One potential option to reduce climate change impacts is the use of stratospheric aerosol injection (SAI).","Even if SAI is ultimately deployed, it might be initiated only after some temperature target is exceeded.","The consequences of such a delay are assessed herein.","This study compares two cases, with the same target global mean temperature of 1.5C above preindustrial, but start dates of 2035 or a delayed start in 2045.","We make use of simulations in the Community Earth System Model version 2 with the Whole Atmosphere Coupled Chemistry Model version 6 (CESM2-WACCM6), using SAI under the SSP2-4.5 emissions pathway.","We find that delaying the start of deployment (relative to the target temperature) necessitates lower net radiative forcing (-30%) and thus larger sulfur dioxide injection rates (+20%), even after surface temperatures converge, to compensate for the extra energy absorbed by the Earth system.","However, many of the surface climate differences between the 2035 and 2045 start simulations appear to be small during the 10-25 years following the delayed SAI start, although longer simulations would be needed to assess any longer-term impacts in this model.","In addition, irreversibilities and tipping points that might be triggered during the period of increased warming may not be adequately represented in the model but could change this conclusion in the real world."],"url":"http://arxiv.org/abs/2402.11992v1","category":"physics.ao-ph"}
{"created":"2024-02-19 09:29:18","title":"Antenna Array Design for Mono-Static ISAC","abstract":"Mono-static sensing operations in Integrated Sensing and Communications (ISAC) require joint beamforming operations between transmitter and receiver, according to all the considerations already done in the radar literature about coarray theory. In contrast to pure radar systems, ISAC requires to fulfill communications tasks and to retain the corresponding design constraints for at least one half-duplex array. This shifts the available degrees of freedom to the design of the second half-duplex array, that completes the mono-static sensing setup of the ISAC system. Therefore, it is necessary to translate the analysis from the radar literature for the design of sparse arrays to the new ISAC paradigm in order to provision such systems.   Accordingly, we propose a model to evaluate the angular capabilities of an ISAC setup, constrained to the shape of the communications array and its topology requirements. Our analysis is validated by simulation experiments, confirming the value of our model in providing system designers with a tool to drastically improve the trade-off between angular capabilities for sensing and the cost of the deployed hardware. Finally, we discuss possible enhancements to the cellular standards to fully leverage the angular capabilities of such mono-static ISAC systems.","sentences":["Mono-static sensing operations in Integrated Sensing and Communications (ISAC) require joint beamforming operations between transmitter and receiver, according to all the considerations already done in the radar literature about coarray theory.","In contrast to pure radar systems, ISAC requires to fulfill communications tasks and to retain the corresponding design constraints for at least one half-duplex array.","This shifts the available degrees of freedom to the design of the second half-duplex array, that completes the mono-static sensing setup of the ISAC system.","Therefore, it is necessary to translate the analysis from the radar literature for the design of sparse arrays to the new ISAC paradigm in order to provision such systems.   ","Accordingly, we propose a model to evaluate the angular capabilities of an ISAC setup, constrained to the shape of the communications array and its topology requirements.","Our analysis is validated by simulation experiments, confirming the value of our model in providing system designers with a tool to drastically improve the trade-off between angular capabilities for sensing and the cost of the deployed hardware.","Finally, we discuss possible enhancements to the cellular standards to fully leverage the angular capabilities of such mono-static ISAC systems."],"url":"http://arxiv.org/abs/2402.11983v1","category":"eess.SP"}
{"created":"2024-02-19 09:26:22","title":"Buffered Streaming Edge Partitioning","abstract":"Addressing the challenges of processing massive graphs, which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale graphs. HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input graph. Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the graph size in both time and memory complexity with no dependence on the number of blocks of partition. Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices. Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks.","sentences":["Addressing the challenges of processing massive graphs, which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale graphs.","HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input graph.","Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the graph size in both time and memory complexity with no dependence on the number of blocks of partition.","Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices.","Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks."],"url":"http://arxiv.org/abs/2402.11980v1","category":"cs.DS"}
{"created":"2024-02-19 09:19:07","title":"Global stability and optimal control in a single-strain dengue model with fractional-order transmission and recovery process","abstract":"In this manuscript, we develop a novel single-strain dengue model with fractional-order transmission and recovery from a stochastic process. A fractional derivative that appeared in the disease transmission and recovery processes is known as a tempered fractional (TF) derivative. We showed that if a function satisfying certain conditions, then the TF derivative of that function is proportional to the function itself. By utilizing this result, we investigated the local and global stability of various equilibrium solutions (disease-free and endemic) related to the novel fractional-order dengue model in terms of the basic reproduction number R0. Additionally, we developed an optimal control problem for the extended fractional-order dengue system with control to study the effect of three different interventions: reduction of mosquito recruitment rate, adult vector control, and individual protection. Furthermore, we derived a sufficient condition for the existence of a solution to the optimal control problem. Finally, numerical experiments suggest that policymakers may focus on fractional-order parameters that represent the mechanisms of disease transmission and recovery in addition to the two vector controls to reduce dengue spreading in a location.","sentences":["In this manuscript, we develop a novel single-strain dengue model with fractional-order transmission and recovery from a stochastic process.","A fractional derivative that appeared in the disease transmission and recovery processes is known as a tempered fractional (TF) derivative.","We showed that if a function satisfying certain conditions, then the TF derivative of that function is proportional to the function itself.","By utilizing this result, we investigated the local and global stability of various equilibrium solutions (disease-free and endemic) related to the novel fractional-order dengue model in terms of the basic reproduction number R0.","Additionally, we developed an optimal control problem for the extended fractional-order dengue system with control to study the effect of three different interventions: reduction of mosquito recruitment rate, adult vector control, and individual protection.","Furthermore, we derived a sufficient condition for the existence of a solution to the optimal control problem.","Finally, numerical experiments suggest that policymakers may focus on fractional-order parameters that represent the mechanisms of disease transmission and recovery in addition to the two vector controls to reduce dengue spreading in a location."],"url":"http://arxiv.org/abs/2402.11974v1","category":"math.DS"}
{"created":"2024-02-19 09:16:52","title":"Port-Hamiltonian modeling and control of a curling HASEL actuator","abstract":"This paper is concerned with the modeling and control of a curling Hydraulically Amplified Self-healing Electrostatic (HASEL) actuator using the port-Hamiltonian (PH) approach. For that purpose, we use a modular approach and consider the HASEL actuator as an interconnection of elementary subsystems. Each subsystem is modeled by an electrical component consisting of a capacitor in parallel with an inductor connected through the conservation of volume of the moving liquid to a mechanical structure based on inertia, linear, and torsional springs. The parameters are then identified, and the model is validated on the experimental setup. Position control is achieved by using Interconnection and Damping Assignment-Passivity Based Control (IDA-PBC) with integral action (IA) for disturbance rejection. Simulation results show the efficiency of the proposed controller.","sentences":["This paper is concerned with the modeling and control of a curling Hydraulically Amplified Self-healing Electrostatic (HASEL) actuator using the port-Hamiltonian (PH) approach.","For that purpose, we use a modular approach and consider the HASEL actuator as an interconnection of elementary subsystems.","Each subsystem is modeled by an electrical component consisting of a capacitor in parallel with an inductor connected through the conservation of volume of the moving liquid to a mechanical structure based on inertia, linear, and torsional springs.","The parameters are then identified, and the model is validated on the experimental setup.","Position control is achieved by using Interconnection and Damping Assignment-Passivity Based Control (IDA-PBC) with integral action (IA) for disturbance rejection.","Simulation results show the efficiency of the proposed controller."],"url":"http://arxiv.org/abs/2402.11970v1","category":"math.DS"}
{"created":"2024-02-19 09:14:23","title":"New asymptotics for strong solutions of the strongly stratified Boussinesq system without rotation and for large ill-prepared initial data","abstract":"In our previous work dedicated to the strongly stratified Boussinesq system, we obtained for the first time a limit system (when the froude number $\\epsilon$ goes to zero) that depends on the thermal diffusivity $\\nu$ ' (other works obtained a limit system only depending on the visosity $\\nu$). To reach those richer asymptotics we had to consider an unusual initial data which is the sum of a function depending on the full space variable and a function only depending on the vertical coordinate, and we studied the convergence of the weak Leray-type solutions. In the present article we extend these results to the strong Fujita-Kato-type solutions. We obtain far better convergence rates (in $\\epsilon$) for ill-prepared initial data with very large oscillating part of size some negative power of the small parameter $\\epsilon$. The main difficulties come from the anisotropy induced by the presence of x3-depending functions.","sentences":["In our previous work dedicated to the strongly stratified Boussinesq system, we obtained for the first time a limit system (when the froude number $\\epsilon$ goes to zero) that depends on the thermal diffusivity $\\nu$ ' (other works obtained a limit system only depending on the visosity $\\nu$).","To reach those richer asymptotics we had to consider an unusual initial data which is the sum of a function depending on the full space variable and a function only depending on the vertical coordinate, and we studied the convergence of the weak Leray-type solutions.","In the present article we extend these results to the strong Fujita-Kato-type solutions.","We obtain far better convergence rates (in $\\epsilon$) for ill-prepared initial data with very large oscillating part of size some negative power of the small parameter $\\epsilon$. The main difficulties come from the anisotropy induced by the presence of x3-depending functions."],"url":"http://arxiv.org/abs/2402.11967v1","category":"math.AP"}
{"created":"2024-02-19 09:13:51","title":"Passive viscous flow selection via fluid-induced buckling","abstract":"We study the buckling of a clamped beam immersed in a creeping flow within a rectangular channel. Via a combination of precision experiments, simulations, and theoretical modeling, we show how the instability depends on a pressure feedback mechanism and rationalize it in terms of dimensionless parameters. As the beam can bend until touching the wall above a critical flow rate, we finally demonstrate how the system can be used as a tunable passive flow selector, effectively redirecting the flow within a designed hydraulic circuit.","sentences":["We study the buckling of a clamped beam immersed in a creeping flow within a rectangular channel.","Via a combination of precision experiments, simulations, and theoretical modeling, we show how the instability depends on a pressure feedback mechanism and rationalize it in terms of dimensionless parameters.","As the beam can bend until touching the wall above a critical flow rate, we finally demonstrate how the system can be used as a tunable passive flow selector, effectively redirecting the flow within a designed hydraulic circuit."],"url":"http://arxiv.org/abs/2402.11966v1","category":"cond-mat.soft"}
{"created":"2024-02-19 09:06:31","title":"Tagged particle behavior in a harmonic chain of direction reversing active Brownian particles","abstract":"We study the tagged particle dynamics in a harmonic chain of direction reversing active Brownian particles, with spring constant $k$, rotation diffusion coefficient $D_{\\text{r}}$, and directional reversal rate $\\gamma$. We exactly compute the tagged particle position variance for quenched and annealed initial orientations of the particles. For well-separated time scales, $k^{-1}$, $D_{\\text{r}}^{-1}$ and $\\gamma^{-1}$, the strength of spring constant $k$ relative to $D_{\\text{r}}$ and $\\gamma$ gives rise to different coupling limits and for each coupling limit there are short, intermediate, and long time regimes. In the thermodynamic limit, we show that, to the leading order, the tagged particle variance exhibits an algebraic growth $t^{\\nu}$, where the value of the exponent $\\nu$ depends on the specific regime. For a quenched initial orientation, the exponent $\\nu$ crosses over from $3$ to $1/2$, via intermediate values $5/2$ or $1$, depending on the specific coupling limits. On the other hand, for the annealed initial orientation, $\\nu$ crosses over from $2$ to $1/2$ via an intermediate value $3/2$ or $1$ for strong coupling limit and weak coupling limit respectively. An additional time scale $t_N=N^2/k$ emerges for a system with a finite number of oscillators $N$. We show that the behavior of the tagged particle variance across $t_N$ can be expressed in terms of a crossover scaling function, which we find exactly. Finally, we characterize the stationary state behavior of the separation between two consecutive particles by calculating the corresponding spatio-temporal correlation function.","sentences":["We study the tagged particle dynamics in a harmonic chain of direction reversing active Brownian particles, with spring constant $k$, rotation diffusion coefficient $D_{\\text{r}}$, and directional reversal rate $\\gamma$.","We exactly compute the tagged particle position variance for quenched and annealed initial orientations of the particles.","For well-separated time scales, $k^{-1}$, $D_{\\text{r}}^{-1}$ and $\\gamma^{-1}$, the strength of spring constant $k$ relative to $D_{\\text{r}}$ and $\\gamma$ gives rise to different coupling limits and for each coupling limit there are short, intermediate, and long time regimes.","In the thermodynamic limit, we show that, to the leading order, the tagged particle variance exhibits an algebraic growth $t^{\\nu}$, where the value of the exponent $\\nu$ depends on the specific regime.","For a quenched initial orientation, the exponent $\\nu$ crosses over from $3$ to $1/2$, via intermediate values $5/2$ or $1$, depending on the specific coupling limits.","On the other hand, for the annealed initial orientation, $\\nu$ crosses over from $2$ to $1/2$ via an intermediate value $3/2$ or $1$ for strong coupling limit and weak coupling limit respectively.","An additional time scale $t_N=N^2/k$ emerges for a system with a finite number of oscillators $N$. We show that the behavior of the tagged particle variance across $t_N$ can be expressed in terms of a crossover scaling function, which we find exactly.","Finally, we characterize the stationary state behavior of the separation between two consecutive particles by calculating the corresponding spatio-temporal correlation function."],"url":"http://arxiv.org/abs/2402.11964v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-19 08:59:58","title":"Event-Based Motion Magnification","abstract":"Detecting and magnifying imperceptible high-frequency motions in real-world scenarios has substantial implications for industrial and medical applications. These motions are characterized by small amplitudes and high frequencies. Traditional motion magnification methods rely on costly high-speed cameras or active light sources, which limit the scope of their applications. In this work, we propose a dual-camera system consisting of an event camera and a conventional RGB camera for video motion magnification, containing temporally-dense information from the event stream and spatially-dense data from the RGB images. This innovative combination enables a broad and cost-effective amplification of high-frequency motions. By revisiting the physical camera model, we observe that estimating motion direction and magnitude necessitates the integration of event streams with additional image features. On this basis, we propose a novel deep network for event-based video motion magnification that addresses two primary challenges: firstly, the high frequency of motion induces a large number of interpolated frames (up to 80), which our network mitigates with a Second-order Recurrent Propagation module for better handling of long-term frame interpolations; and secondly, magnifying subtle motions is sensitive to noise, which we address by utilizing a temporal filter to amplify motion at specific frequencies and reduce noise impact. We demonstrate the effectiveness and accuracy of our dual-camera system and network through extensive experiments in magnifying small-amplitude, high-frequency motions, offering a cost-effective and flexible solution for motion detection and magnification.","sentences":["Detecting and magnifying imperceptible high-frequency motions in real-world scenarios has substantial implications for industrial and medical applications.","These motions are characterized by small amplitudes and high frequencies.","Traditional motion magnification methods rely on costly high-speed cameras or active light sources, which limit the scope of their applications.","In this work, we propose a dual-camera system consisting of an event camera and a conventional RGB camera for video motion magnification, containing temporally-dense information from the event stream and spatially-dense data from the RGB images.","This innovative combination enables a broad and cost-effective amplification of high-frequency motions.","By revisiting the physical camera model, we observe that estimating motion direction and magnitude necessitates the integration of event streams with additional image features.","On this basis, we propose a novel deep network for event-based video motion magnification that addresses two primary challenges: firstly, the high frequency of motion induces a large number of interpolated frames (up to 80), which our network mitigates with a Second-order Recurrent Propagation module for better handling of long-term frame interpolations; and secondly, magnifying subtle motions is sensitive to noise, which we address by utilizing a temporal filter to amplify motion at specific frequencies and reduce noise impact.","We demonstrate the effectiveness and accuracy of our dual-camera system and network through extensive experiments in magnifying small-amplitude, high-frequency motions, offering a cost-effective and flexible solution for motion detection and magnification."],"url":"http://arxiv.org/abs/2402.11957v1","category":"cs.CV"}
{"created":"2024-02-19 08:53:11","title":"Hyperbolic phonon polaritons and wave vector direction dependent dielectric tensors in anisotropic crystals","abstract":"Hyperbolic phonon polariton is important in precisely controlling photons at the nanoscale. It was common practice to calculate the dielectric function of the phonon polariton system with the Drude-Lorenz model. We considered the impact of LO-TO splitting while applying the Drude-Lorenz model. Then the dielectric functions become wave vector direction dependent besides electric polarization direction dependent. Our results show that considering LO-TO splitting can more accurately predict dielectric functions. Additionally, we discovered that, besides hexagonal BN, hexagonal AlN exhibits a wide hyperbolic frequency band range, while the other four materials display it scarcely. Furthermore, we found that the phonon frequency, lifetime, and the difference of infrared active transverse optical phonon frequencies with different wave vector directions are critical factors in determining the width of the hyperbolic frequency band range. We also found some dumbbell-shaped and butterfly-shaped isofrequency curves in h-AlN, h-GaP, and especially h-GaN. Our study provides a fresh perspective on understanding the dielectric properties of these materials and lays a theoretical foundation for further exploration and development of new hyperbolic phonon polariton materials.","sentences":["Hyperbolic phonon polariton is important in precisely controlling photons at the nanoscale.","It was common practice to calculate the dielectric function of the phonon polariton system with the Drude-Lorenz model.","We considered the impact of LO-TO splitting while applying the Drude-Lorenz model.","Then the dielectric functions become wave vector direction dependent besides electric polarization direction dependent.","Our results show that considering LO-TO splitting can more accurately predict dielectric functions.","Additionally, we discovered that, besides hexagonal BN, hexagonal AlN exhibits a wide hyperbolic frequency band range, while the other four materials display it scarcely.","Furthermore, we found that the phonon frequency, lifetime, and the difference of infrared active transverse optical phonon frequencies with different wave vector directions are critical factors in determining the width of the hyperbolic frequency band range.","We also found some dumbbell-shaped and butterfly-shaped isofrequency curves in h-AlN, h-GaP, and especially h-GaN.","Our study provides a fresh perspective on understanding the dielectric properties of these materials and lays a theoretical foundation for further exploration and development of new hyperbolic phonon polariton materials."],"url":"http://arxiv.org/abs/2402.11956v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 08:49:09","title":"Multimodal Emotion Recognition from Raw Audio with Sinc-convolution","abstract":"Speech Emotion Recognition (SER) is still a complex task for computers with average recall rates usually about 70% on the most realistic datasets. Most SER systems use hand-crafted features extracted from audio signal such as energy, zero crossing rate, spectral information, prosodic, mel frequency cepstral coefficient (MFCC), and so on. More recently, using raw waveform for training neural network is becoming an emerging trend. This approach is advantageous as it eliminates the feature extraction pipeline. Learning from time-domain signal has shown good results for tasks such as speech recognition, speaker verification etc. In this paper, we utilize Sinc-convolution layer, which is an efficient architecture for preprocessing raw speech waveform for emotion recognition, to extract acoustic features from raw audio signals followed by a long short-term memory (LSTM). We also incorporate linguistic features and append a dialogical emotion decoding (DED) strategy. Our approach achieves a weighted accuracy of 85.1\\% in four class emotion on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset.","sentences":["Speech Emotion Recognition (SER) is still a complex task for computers with average recall rates usually about 70% on the most realistic datasets.","Most SER systems use hand-crafted features extracted from audio signal such as energy, zero crossing rate, spectral information, prosodic, mel frequency cepstral coefficient (MFCC), and so on.","More recently, using raw waveform for training neural network is becoming an emerging trend.","This approach is advantageous as it eliminates the feature extraction pipeline.","Learning from time-domain signal has shown good results for tasks such as speech recognition, speaker verification etc.","In this paper, we utilize Sinc-convolution layer, which is an efficient architecture for preprocessing raw speech waveform for emotion recognition, to extract acoustic features from raw audio signals followed by a long short-term memory (LSTM).","We also incorporate linguistic features and append a dialogical emotion decoding (DED) strategy.","Our approach achieves a weighted accuracy of 85.1\\% in four class emotion on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset."],"url":"http://arxiv.org/abs/2402.11954v1","category":"cs.SD"}
{"created":"2024-02-19 08:47:20","title":"Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels","abstract":"Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20.","sentences":["Machine learning, with its myriad applications, has become an integral component of numerous technological systems.","A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks.","As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities.","In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models.","Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method.","Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures.","We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required.","We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20."],"url":"http://arxiv.org/abs/2402.11953v1","category":"cs.CR"}
{"created":"2024-02-19 08:46:28","title":"Inexact and Implementable Accelerated Newton Proximal Extragradient Method for Convex Optimization","abstract":"In this paper, we investigate the convergence behavior of the Accelerated Newton Proximal Extragradient (A-NPE) method when employing inexact Hessian information. The exact A-NPE method was the pioneer near-optimal second-order approach, exhibiting an oracle complexity of $\\Tilde{O}(\\epsilon^{-2/7})$ for convex optimization. Despite its theoretical optimality, there has been insufficient attention given to the study of its inexact version and efficient implementation. We introduce the inexact A-NPE method (IA-NPE), which is shown to maintain the near-optimal oracle complexity. In particular, we design a dynamic approach to balance the computational cost of constructing the Hessian matrix and the progress of the convergence. Moreover, we show the robustness of the line-search procedure, which is a subroutine in IA-NPE, in the face of the inexactness of the Hessian. These nice properties enable the implementation of highly effective machine learning techniques like sub-sampling and various heuristics in the method. Extensive numerical results illustrate that IA-NPE compares favorably with state-of-the-art second-order methods, including Newton's method with cubic regularization and Trust-Region methods.","sentences":["In this paper, we investigate the convergence behavior of the Accelerated Newton Proximal Extragradient (A-NPE) method when employing inexact Hessian information.","The exact A-NPE method was the pioneer near-optimal second-order approach, exhibiting an oracle complexity of $\\Tilde{O}(\\epsilon^{-2/7})$ for convex optimization.","Despite its theoretical optimality, there has been insufficient attention given to the study of its inexact version and efficient implementation.","We introduce the inexact A-NPE method (IA-NPE), which is shown to maintain the near-optimal oracle complexity.","In particular, we design a dynamic approach to balance the computational cost of constructing the Hessian matrix and the progress of the convergence.","Moreover, we show the robustness of the line-search procedure, which is a subroutine in IA-NPE, in the face of the inexactness of the Hessian.","These nice properties enable the implementation of highly effective machine learning techniques like sub-sampling and various heuristics in the method.","Extensive numerical results illustrate that IA-NPE compares favorably with state-of-the-art second-order methods, including Newton's method with cubic regularization and Trust-Region methods."],"url":"http://arxiv.org/abs/2402.11951v1","category":"math.OC"}
{"created":"2024-02-19 08:32:28","title":"Description of ultrastrong light-matter interaction through coupled harmonic oscillator models and their connection with cavity-QED Hamiltonians","abstract":"Classical coupled harmonic oscillator models have been proven capable to describe successfully the spectra of many nanophotonic systems where an optical mode couples to a molecular or matter excitation. Although models with distinct coupling terms have been proposed, they are used interchangeably due to their similar results in the weak and strong coupling regimes. However, in the ultrastrong coupling regime, each oscillator model leads to very different predictions. Further, it is important to determine which physical magnitude is associated to each harmonic oscillator of these models in order to reproduce appropriately experimentally measurable quantities in each system. To clarify which classical model must be used for a given experiment, we establish a connection with the quantum description of these systems based on cavity quantum electrodynamics. We show that the proper choice of the classical coupling term depends on the presence or absence of the diamagnetic term in the quantum models and on whether the electromagnetic modes involved in the coupling are transverse or longitudinal. The comparison with quantum models further enables to make the correspondence between quantum operators and classical variables in the oscillator models, in order to extract measurable information of the hybrid modes of the system.","sentences":["Classical coupled harmonic oscillator models have been proven capable to describe successfully the spectra of many nanophotonic systems where an optical mode couples to a molecular or matter excitation.","Although models with distinct coupling terms have been proposed, they are used interchangeably due to their similar results in the weak and strong coupling regimes.","However, in the ultrastrong coupling regime, each oscillator model leads to very different predictions.","Further, it is important to determine which physical magnitude is associated to each harmonic oscillator of these models in order to reproduce appropriately experimentally measurable quantities in each system.","To clarify which classical model must be used for a given experiment, we establish a connection with the quantum description of these systems based on cavity quantum electrodynamics.","We show that the proper choice of the classical coupling term depends on the presence or absence of the diamagnetic term in the quantum models and on whether the electromagnetic modes involved in the coupling are transverse or longitudinal.","The comparison with quantum models further enables to make the correspondence between quantum operators and classical variables in the oscillator models, in order to extract measurable information of the hybrid modes of the system."],"url":"http://arxiv.org/abs/2402.11944v1","category":"quant-ph"}
{"created":"2024-02-19 08:32:27","title":"LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation","abstract":"The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively.","sentences":["The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies.","Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification.","The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem.","Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills.","In this paper, we first investigate the potential of LVLM on multimodal misinformation detection.","We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence.","Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation.","LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection.","Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively."],"url":"http://arxiv.org/abs/2402.11943v1","category":"cs.CL"}
{"created":"2024-02-19 08:26:34","title":"Exploring Astrophysically-Relevant Superadiabaticity Driven by Non-Resonant Heating in a Laboratory Magnetized Plasma","abstract":"The polytropic index of electrons in a magnetized plasma is experimentally investigated in the presence of external heating and anisotropic work done on the system by the magnetic field. The measurements clearly demonstrate localized regions of superadiabatic electrons due to particle acceleration and energization through non-resonant betatron and Fermi heating processes. In regions where wave-induced heating through electron cyclotron resonance dominates, the plasma electrons behave adiabatically. The realization of superadiabaticity is universal where heating dominates the work done and has broader implications for energy exchange processes in astrophysically-relevant plasmas.","sentences":["The polytropic index of electrons in a magnetized plasma is experimentally investigated in the presence of external heating and anisotropic work done on the system by the magnetic field.","The measurements clearly demonstrate localized regions of superadiabatic electrons due to particle acceleration and energization through non-resonant betatron and Fermi heating processes.","In regions where wave-induced heating through electron cyclotron resonance dominates, the plasma electrons behave adiabatically.","The realization of superadiabaticity is universal where heating dominates the work done and has broader implications for energy exchange processes in astrophysically-relevant plasmas."],"url":"http://arxiv.org/abs/2402.11937v1","category":"physics.plasm-ph"}
{"created":"2024-02-19 08:18:52","title":"Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease Detection","abstract":"Alzheimer's disease is a common cognitive disorder in the elderly. Early and accurate diagnosis of Alzheimer's disease (AD) has a major impact on the progress of research on dementia. At present, researchers have used machine learning methods to detect Alzheimer's disease from the speech of participants. However, the recognition accuracy of current methods is unsatisfactory, and most of them focus on using low-dimensional handcrafted features to extract relevant information from audios. This paper proposes an Alzheimer's disease detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In addition, by replacing the loss function with the Soft-Weighted CrossEntropy loss function, we achieved 85.45\\% recognition accuracy on the same test dataset.","sentences":["Alzheimer's disease is a common cognitive disorder in the elderly.","Early and accurate diagnosis of Alzheimer's disease (AD) has a major impact on the progress of research on dementia.","At present, researchers have used machine learning methods to detect Alzheimer's disease from the speech of participants.","However, the recognition accuracy of current methods is unsatisfactory, and most of them focus on using low-dimensional handcrafted features to extract relevant information from audios.","This paper proposes an Alzheimer's disease detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2).","In addition, by replacing the loss function with the Soft-Weighted CrossEntropy loss function, we achieved 85.45\\% recognition accuracy on the same test dataset."],"url":"http://arxiv.org/abs/2402.11931v1","category":"cs.SD"}
{"created":"2024-02-19 08:11:50","title":"Unusual Multiple Magnetic Transitions and Anomalous Hall Effect Observed in Antiferromagnetic Weyl Semimetal, Mn$_{2.94}$Ge (Ge-rich)","abstract":"We report on the magnetic and Hall effect measurements of the magnetic Weyl semimetal, Mn$_{2.94}$Ge (Ge-rich) single crystal. From the magnetic properties study, we identify unusual multiple magnetic transitions below the N$\\acute{e}$el temperature of 353 K, such as the spin-reorientation ($T_{SR}$) and ferromagnetic-like transitions. Consistent with the magnetic properties, the Hall effect study shows unusual behavior around the spin-reorientation transition. Specifically, the anomalous Hall conductivity (AHC) increases with increasing temperature, reaching a maximum at $T_{SR}$, which then gradually decreases with increasing temperature. This observation is quite in contrast to the Mn$_{3+\\delta}$Ge (Mn-rich) system, though both compositions share the same hexagonal crystal symmetry. This study unravels the sensitivity of magnetic and topological properties on the Mn concentration.","sentences":["We report on the magnetic and Hall effect measurements of the magnetic Weyl semimetal, Mn$_{2.94}$Ge (Ge-rich) single crystal.","From the magnetic properties study, we identify unusual multiple magnetic transitions below the N$\\acute{e}$el temperature of 353 K, such as the spin-reorientation ($T_{SR}$) and ferromagnetic-like transitions.","Consistent with the magnetic properties, the Hall effect study shows unusual behavior around the spin-reorientation transition.","Specifically, the anomalous Hall conductivity (AHC) increases with increasing temperature, reaching a maximum at $T_{SR}$, which then gradually decreases with increasing temperature.","This observation is quite in contrast to the Mn$_{3+\\delta}$Ge (Mn-rich) system, though both compositions share the same hexagonal crystal symmetry.","This study unravels the sensitivity of magnetic and topological properties on the Mn concentration."],"url":"http://arxiv.org/abs/2402.11923v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 08:10:59","title":"Hydrodynamics for asymmetric simple exclusion on a finite segment with Glauber-type relaxation","abstract":"We consider an open interacting particle system on a finite lattice. The particles perform asymmetric simple exclusion and are randomly created or destroyed at all sites, with rates that grow rapidly near the boundaries. We study the hydrodynamic limit for the particle density at the hyperbolic space-time scale and obtain the entropy solution to a boundary-driven quasilinear conservation law with a relaxation term. Different from the usual boundary conditions introduced in [Bardos, Roux, and Nedelec, (1979), Comm. Part. Diff. Equ], discontinuity (boundary layer) does not formulate at the boundaries due to the strong relaxation scheme.","sentences":["We consider an open interacting particle system on a finite lattice.","The particles perform asymmetric simple exclusion and are randomly created or destroyed at all sites, with rates that grow rapidly near the boundaries.","We study the hydrodynamic limit for the particle density at the hyperbolic space-time scale and obtain the entropy solution to a boundary-driven quasilinear conservation law with a relaxation term.","Different from the usual boundary conditions introduced in [Bardos, Roux, and Nedelec, (1979), Comm.","Part.","Diff.","Equ], discontinuity (boundary layer) does not formulate at the boundaries due to the strong relaxation scheme."],"url":"http://arxiv.org/abs/2402.11921v1","category":"math.PR"}
{"created":"2024-02-19 08:07:01","title":"Unraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts","abstract":"Underwater acoustic target recognition is a difficult task owing to the intricate nature of underwater acoustic signals. The complex underwater environments, unpredictable transmission channels, and dynamic motion states greatly impact the real-world underwater acoustic signals, and may even obscure the intrinsic characteristics related to targets. Consequently, the data distribution of underwater acoustic signals exhibits high intra-class diversity, thereby compromising the accuracy and robustness of recognition systems.To address these issues, this work proposes a convolution-based mixture of experts (CMoE) that recognizes underwater targets in a fine-grained manner. The proposed technique introduces multiple expert layers as independent learners, along with a routing layer that determines the assignment of experts according to the characteristics of inputs. This design allows the model to utilize independent parameter spaces, facilitating the learning of complex underwater signals with high intra-class diversity. Furthermore, this work optimizes the CMoE structure by balancing regularization and an optional residual module. To validate the efficacy of our proposed techniques, we conducted detailed experiments and visualization analyses on three underwater acoustic databases across several acoustic features. The experimental results demonstrate that our CMoE consistently achieves significant performance improvements, delivering superior recognition accuracy when compared to existing advanced methods.","sentences":["Underwater acoustic target recognition is a difficult task owing to the intricate nature of underwater acoustic signals.","The complex underwater environments, unpredictable transmission channels, and dynamic motion states greatly impact the real-world underwater acoustic signals, and may even obscure the intrinsic characteristics related to targets.","Consequently, the data distribution of underwater acoustic signals exhibits high intra-class diversity, thereby compromising the accuracy and robustness of recognition systems.","To address these issues, this work proposes a convolution-based mixture of experts (CMoE) that recognizes underwater targets in a fine-grained manner.","The proposed technique introduces multiple expert layers as independent learners, along with a routing layer that determines the assignment of experts according to the characteristics of inputs.","This design allows the model to utilize independent parameter spaces, facilitating the learning of complex underwater signals with high intra-class diversity.","Furthermore, this work optimizes the CMoE structure by balancing regularization and an optional residual module.","To validate the efficacy of our proposed techniques, we conducted detailed experiments and visualization analyses on three underwater acoustic databases across several acoustic features.","The experimental results demonstrate that our CMoE consistently achieves significant performance improvements, delivering superior recognition accuracy when compared to existing advanced methods."],"url":"http://arxiv.org/abs/2402.11919v1","category":"cs.SD"}
{"created":"2024-02-19 08:04:25","title":"A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task","abstract":"Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models.","sentences":["Transformers demonstrate impressive performance on a range of reasoning benchmarks.","To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies.","However, these studies do not provide insights into the internal mechanisms driving the observed capabilities.","To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task.","We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence.","Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions.","We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models."],"url":"http://arxiv.org/abs/2402.11917v1","category":"cs.LG"}
{"created":"2024-02-19 08:00:37","title":"The kagome Hubbard model from a functional renormalization group perspective","abstract":"The recent discovery of a variety of intricate electronic order in kagome metals has sprouted significant theoretical and experimental interest. From an electronic perspective on the potential microscopic origin of these phases, the most basic model is given by a Hubbard model on the kagome lattice. We employ functional renormalization group (FRG) to analyze the kagome Hubbard model. Through our methodological refinement of FRG both within its N-patch and truncated unity formulation, we resolve previous discrepancies of different FRG approaches (Wang et al., 2013 vs. Kiesel et al., 2013), and analyze both the pure ($p$-type) and mixed ($m$-type) van Hove fillings of the kagome lattice. We further study the RG flow into symmetry broken phases to identify the energetically preferred linear combination of the respective order parameter without any need for additional mean field analysis. Our findings suggest some consistency with recent experiments, and underline the richness of electronic phases already found in the kagome Hubbard model. We also provide a no-go theorem for a complex charge bond ordered phase in the single orbital kagome Hubbard model, suggesting that this model cannot capture aspects of orbital current phases.","sentences":["The recent discovery of a variety of intricate electronic order in kagome metals has sprouted significant theoretical and experimental interest.","From an electronic perspective on the potential microscopic origin of these phases, the most basic model is given by a Hubbard model on the kagome lattice.","We employ functional renormalization group (FRG) to analyze the kagome Hubbard model.","Through our methodological refinement of FRG both within its N-patch and truncated unity formulation, we resolve previous discrepancies of different FRG approaches (Wang et al., 2013 vs. Kiesel et al., 2013), and analyze both the pure ($p$-type) and mixed ($m$-type) van Hove fillings of the kagome lattice.","We further study the RG flow into symmetry broken phases to identify the energetically preferred linear combination of the respective order parameter without any need for additional mean field analysis.","Our findings suggest some consistency with recent experiments, and underline the richness of electronic phases already found in the kagome Hubbard model.","We also provide a no-go theorem for a complex charge bond ordered phase in the single orbital kagome Hubbard model, suggesting that this model cannot capture aspects of orbital current phases."],"url":"http://arxiv.org/abs/2402.11916v1","category":"cond-mat.str-el"}
{"created":"2024-02-19 07:51:00","title":"Analyzing the Impact of Design Factors on Solar Module Thermomechanical Durability Using Interpretable Machine Learning Techniques","abstract":"Solar modules in utility-scale PV systems are expected to maintain decades of lifetime to effectively rival the cost of conventional energy sources. However, the long-term performance of these modules is often degraded by cyclic thermomechanical loading, emphasizing the need for a proper module design to counteract the detrimental effects of thermal expansion mismatch between module materials. Given the complex composition of solar modules, isolating the impact of individual components on overall durability remains a challenging task. In this work, we analyze a comprehensive data set capturing bill-of-materials and post-thermal-cycling power loss from over 250 distinct module designs. Using the data set, we develop a machine learning model to correlate the design factors with the degradation and apply the Shapley additive explanation to provide interpretative insights into the model's decision-making. Our analysis reveals that the type of silicon solar cell, whether monocrystalline or polycrystalline, predominantly influences the degradation, and monocrystalline cells present better durability. This finding is further substantiated by statistical testing on our raw data set. We also demonstrate that the thickness of the encapsulant, particularly the front side one, remains another important factor. While thicker encapsulants lead to reduced power loss, further increasing their thickness does not yield additional benefits. The study moreover provides a blueprint for utilizing explainable machine learning techniques in a complex material system and can potentially steer future research on optimizing solar module design.","sentences":["Solar modules in utility-scale PV systems are expected to maintain decades of lifetime to effectively rival the cost of conventional energy sources.","However, the long-term performance of these modules is often degraded by cyclic thermomechanical loading, emphasizing the need for a proper module design to counteract the detrimental effects of thermal expansion mismatch between module materials.","Given the complex composition of solar modules, isolating the impact of individual components on overall durability remains a challenging task.","In this work, we analyze a comprehensive data set capturing bill-of-materials and post-thermal-cycling power loss from over 250 distinct module designs.","Using the data set, we develop a machine learning model to correlate the design factors with the degradation and apply the Shapley additive explanation to provide interpretative insights into the model's decision-making.","Our analysis reveals that the type of silicon solar cell, whether monocrystalline or polycrystalline, predominantly influences the degradation, and monocrystalline cells present better durability.","This finding is further substantiated by statistical testing on our raw data set.","We also demonstrate that the thickness of the encapsulant, particularly the front side one, remains another important factor.","While thicker encapsulants lead to reduced power loss, further increasing their thickness does not yield additional benefits.","The study moreover provides a blueprint for utilizing explainable machine learning techniques in a complex material system and can potentially steer future research on optimizing solar module design."],"url":"http://arxiv.org/abs/2402.11911v1","category":"physics.app-ph"}
{"created":"2024-02-19 07:48:25","title":"Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric","abstract":"Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data. These multimodal data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes. The success of these models relies on the ability to extract and consolidate semantic information from clinical text. This paper addresses the need for more robust methods to evaluate the semantic content of medical reports. Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and machine translation, often failing to capture the complex semantic meanings inherent in medical content. In this study, we introduce a novel approach designed specifically for assessing the semantic similarity between generated medical reports and the ground truth. Our approach is validated, demonstrating its efficiency in assessing domain-specific semantic similarity within medical contexts. By applying our metric to state-of-the-art Chest X-ray report generation models, we obtain results that not only align with conventional metrics but also provide more contextually meaningful scores in the considered medical domain.","sentences":["Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data.","These multimodal data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes.","The success of these models relies on the ability to extract and consolidate semantic information from clinical text.","This paper addresses the need for more robust methods to evaluate the semantic content of medical reports.","Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and machine translation, often failing to capture the complex semantic meanings inherent in medical content.","In this study, we introduce a novel approach designed specifically for assessing the semantic similarity between generated medical reports and the ground truth.","Our approach is validated, demonstrating its efficiency in assessing domain-specific semantic similarity within medical contexts.","By applying our metric to state-of-the-art Chest X-ray report generation models, we obtain results that not only align with conventional metrics but also provide more contextually meaningful scores in the considered medical domain."],"url":"http://arxiv.org/abs/2402.11908v1","category":"cs.CL"}
{"created":"2024-02-19 07:26:44","title":"Enhancing Power Prediction of Photovoltaic Systems: Leveraging Dynamic Physical Model for Irradiance-to-Power Conversion","abstract":"Power prediction is crucial to the efficiency and reliability of Photovoltaic (PV) systems. For the model-chain-based (also named indirect or physical) power prediction, the conversion of ground environmental data (plane-of-array irradiance and module temperature) to the output power is a fundamental step, commonly accomplished through physical modeling. The core of the physical model lies in the parameters. However, traditional parameter estimation either relies on datasheet information that cannot reflect the system's current health status or necessitates additional I-V characterization of the entire array, which is not commonly available. To address this, our paper introduces PVPro, a dynamic physical modeling method for irradiance-to-power conversion. It extracts model parameters from the recent production data without requiring I-V curve measurements. This dynamic model, periodically-updated (as short as daily), can closely capture the actual health status, enabling precise power estimation. To evaluate the performance, PVPro is compared with the smart persistence, nominal physical, and various machine learning models for day-ahead power prediction. The results indicate that PVPro achieves an outstanding power estimation performance with the average nMAE =1.4% across four field PV systems, reducing the error by 17.6% compared to the best of other techniques. Furthermore, PVPro demonstrates robustness across different seasons and weather conditions. More importantly, PVPro can also perform well with a limited amount of historical production data (3 days), rendering it applicable for new PV systems. The tool is available as a Python package at: https://github.com/DuraMAT/pvpro.","sentences":["Power prediction is crucial to the efficiency and reliability of Photovoltaic (PV) systems.","For the model-chain-based (also named indirect or physical) power prediction, the conversion of ground environmental data (plane-of-array irradiance and module temperature) to the output power is a fundamental step, commonly accomplished through physical modeling.","The core of the physical model lies in the parameters.","However, traditional parameter estimation either relies on datasheet information that cannot reflect the system's current health status or necessitates additional I-V characterization of the entire array, which is not commonly available.","To address this, our paper introduces PVPro, a dynamic physical modeling method for irradiance-to-power conversion.","It extracts model parameters from the recent production data without requiring I-V curve measurements.","This dynamic model, periodically-updated (as short as daily), can closely capture the actual health status, enabling precise power estimation.","To evaluate the performance, PVPro is compared with the smart persistence, nominal physical, and various machine learning models for day-ahead power prediction.","The results indicate that PVPro achieves an outstanding power estimation performance with the average nMAE","=1.4% across four field PV systems, reducing the error by 17.6% compared to the best of other techniques.","Furthermore, PVPro demonstrates robustness across different seasons and weather conditions.","More importantly, PVPro can also perform well with a limited amount of historical production data (3 days), rendering it applicable for new PV systems.","The tool is available as a Python package at: https://github.com/DuraMAT/pvpro."],"url":"http://arxiv.org/abs/2402.11897v1","category":"eess.SY"}
{"created":"2024-02-19 06:56:34","title":"The Volterra lattice, Abel's equation of the first kind, and the SIR epidemic models","abstract":"The Volterra lattice, when imposing non-zero constant boundary values, admits the structure of a completely integrable Hamiltonian system if the system size is sufficiently small. Such a Volterra lattice can be regarded as an epidemic model known as the SIR model with vaccination, which extends the celebrated SIR model to account for vaccination. Upon the introduction of an appropriate variable transformation, the SIR model with vaccination reduces to an Abel equation of the first kind, which corresponds to an exact differential equation. The equipotential curve of the exact differential equation is the Lambert curve. Thus, the general solution to the initial value problem of the SIR model with vaccination, or the Volterra lattice with constant boundary values, is implicitly provided by using the Lambert W function.","sentences":["The Volterra lattice, when imposing non-zero constant boundary values, admits the structure of a completely integrable Hamiltonian system if the system size is sufficiently small.","Such a Volterra lattice can be regarded as an epidemic model known as the SIR model with vaccination, which extends the celebrated SIR model to account for vaccination.","Upon the introduction of an appropriate variable transformation, the SIR model with vaccination reduces to an Abel equation of the first kind, which corresponds to an exact differential equation.","The equipotential curve of the exact differential equation is the Lambert curve.","Thus, the general solution to the initial value problem of the SIR model with vaccination, or the Volterra lattice with constant boundary values, is implicitly provided by using the Lambert W function."],"url":"http://arxiv.org/abs/2402.11888v1","category":"nlin.SI"}
{"created":"2024-02-19 06:36:45","title":"Incipient Slip Detection by Vibration Injection into Soft Sensor","abstract":"In robotic manipulation, preventing objects from slipping and establishing a secure grip on them is critical. Successful manipulation requires tactile sensors that detect the microscopic incipient slip phenomenon at the contact surface. Unfortunately, the tiny signals generated by incipient slip are quickly buried by environmental noise, and precise stress-distribution measurement requires an extensive optical system and integrated circuits. In this study, we focus on the macroscopic deformation of the entire fingertip's soft structure instead of directly observing the contact surface and its role as a vibration medium for sensing. The proposed method compresses the stick ratio's information into a one-dimensional pressure signal using the change in the propagation characteristics by vibration injection into the soft structure, which magnifies the microscopic incipient slip phenomena into the entire deformation. This mechanism allows a tactile sensor to use just a single vibration sensor. In the implemented system, a biomimetic tactile sensor is vibrated using a white signal from a PZT motor and utilizes frequency spectrum change of the propagated vibration as features. We investigated the proposed method's effectiveness on stick-ratio estimation and \\red{stick-ratio stabilization} control during incipient slip. Our estimation error and the control performance results significantly outperformed the conventional methods.","sentences":["In robotic manipulation, preventing objects from slipping and establishing a secure grip on them is critical.","Successful manipulation requires tactile sensors that detect the microscopic incipient slip phenomenon at the contact surface.","Unfortunately, the tiny signals generated by incipient slip are quickly buried by environmental noise, and precise stress-distribution measurement requires an extensive optical system and integrated circuits.","In this study, we focus on the macroscopic deformation of the entire fingertip's soft structure instead of directly observing the contact surface and its role as a vibration medium for sensing.","The proposed method compresses the stick ratio's information into a one-dimensional pressure signal using the change in the propagation characteristics by vibration injection into the soft structure, which magnifies the microscopic incipient slip phenomena into the entire deformation.","This mechanism allows a tactile sensor to use just a single vibration sensor.","In the implemented system, a biomimetic tactile sensor is vibrated using a white signal from a PZT motor and utilizes frequency spectrum change of the propagated vibration as features.","We investigated the proposed method's effectiveness on stick-ratio estimation and \\red{stick-ratio stabilization} control during incipient slip.","Our estimation error and the control performance results significantly outperformed the conventional methods."],"url":"http://arxiv.org/abs/2402.11879v1","category":"cs.RO"}
{"created":"2024-02-19 06:34:01","title":"Simulator Demonstration of Large Scale Variational Quantum Algorithm on HPC Cluster","abstract":"Advances in quantum simulator technology is increasingly required because research on quantum algorithms is becoming more sophisticated and complex. State vector simulation utilizes CPU and memory resources in computing nodes exponentially with respect to the number of qubits; furthermore, in a variational quantum algorithm, the large number of repeated runs by classical optimization is also a heavy load. This problem has been addressed by preparing numerous computing nodes or simulation frameworks that work effectively. This study aimed to accelerate quantum simulation using two newly proposed methods: to efficiently utilize limited computational resources by adjusting the ratio of the MPI and distributed processing parallelism corresponding to the target problem settings and to slim down the Hamiltonian by considering the effect of accuracy on the calculation result. Ground-state energy calculations of fermionic model were performed using variational quantum eigensolver (VQE) on an HPC cluster with up to 1024 FUJITSU Processor A64FX connected to each other by InfiniBand; the processor is also used on supercomputer Fugaku. We achieved 200 times higher speed over VQE simulations and demonstrated 32 qubits ground-state energy calculations in acceptable time. This result indicates that > 30 qubit state vector simulations can be realistically utilized to further research on variational quantum algorithms.","sentences":["Advances in quantum simulator technology is increasingly required because research on quantum algorithms is becoming more sophisticated and complex.","State vector simulation utilizes CPU and memory resources in computing nodes exponentially with respect to the number of qubits; furthermore, in a variational quantum algorithm, the large number of repeated runs by classical optimization is also a heavy load.","This problem has been addressed by preparing numerous computing nodes or simulation frameworks that work effectively.","This study aimed to accelerate quantum simulation using two newly proposed methods: to efficiently utilize limited computational resources by adjusting the ratio of the MPI and distributed processing parallelism corresponding to the target problem settings and to slim down the Hamiltonian by considering the effect of accuracy on the calculation result.","Ground-state energy calculations of fermionic model were performed using variational quantum eigensolver (VQE) on an HPC cluster with up to 1024 FUJITSU Processor A64FX connected to each other by InfiniBand; the processor is also used on supercomputer Fugaku.","We achieved 200 times higher speed over VQE simulations and demonstrated 32 qubits ground-state energy calculations in acceptable time.","This result indicates that > 30 qubit state vector simulations can be realistically utilized to further research on variational quantum algorithms."],"url":"http://arxiv.org/abs/2402.11878v1","category":"quant-ph"}
{"created":"2024-02-19 06:32:39","title":"M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation","abstract":"Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge. However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens. In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the counterfactual effect for more accurate anchor token detection. The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reducing hallucinations.","sentences":["Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge.","However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice.","Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens.","In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens.","Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction.","Furthermore, we introduce the counterfactual effect for more accurate anchor token detection.","The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reducing hallucinations."],"url":"http://arxiv.org/abs/2402.11875v1","category":"cs.CL"}
{"created":"2024-02-19 06:24:54","title":"Orbital-rotated Fermi-Hubbard model as a benchmarking problem for quantum chemistry with the exact solution","abstract":"Evaluating the relative performance of different quantum algorithms for quantum computers is of great significance in the research of quantum algorithms. In this study, we consider the problem of quantum chemistry, which is considered one of the important applications of quantum algorithms. While evaluating these algorithms in systems with a large number of qubits is essential to see the scalability of the algorithms, the solvable models usually used for such evaluations typically have a small number of terms compared to the molecular Hamiltonians used in quantum chemistry. The large number of terms in molecular Hamiltonians is a major bottleneck when applying quantum algorithms to quantum chemistry. Various methods are being considered to address this problem, highlighting its importance in developing quantum algorithms for quantum chemistry. Based on these points, a solvable model with a number of terms comparable to the molecular Hamiltonian is essential to evaluate the performance of such algorithms. In this paper, we propose a set of exactly solvable Hamiltonians that has a comparable order of terms with molecular Hamiltonians by applying a spin-involving orbital rotation to the one-dimensional Fermi-Hubbard Hamiltonian. We verify its similarity to the molecular Hamiltonian from some prospectives and investigate whether the difficulty of calculating the ground-state energy changes before and after orbital rotation by applying the density matrix renormalization group up to 24 sites corresponding to 48 qubits. This proposal would enable proper evaluation of the performance of quantum algorithms for quantum chemistry, serving as a guiding framework for algorithm development.","sentences":["Evaluating the relative performance of different quantum algorithms for quantum computers is of great significance in the research of quantum algorithms.","In this study, we consider the problem of quantum chemistry, which is considered one of the important applications of quantum algorithms.","While evaluating these algorithms in systems with a large number of qubits is essential to see the scalability of the algorithms, the solvable models usually used for such evaluations typically have a small number of terms compared to the molecular Hamiltonians used in quantum chemistry.","The large number of terms in molecular Hamiltonians is a major bottleneck when applying quantum algorithms to quantum chemistry.","Various methods are being considered to address this problem, highlighting its importance in developing quantum algorithms for quantum chemistry.","Based on these points, a solvable model with a number of terms comparable to the molecular Hamiltonian is essential to evaluate the performance of such algorithms.","In this paper, we propose a set of exactly solvable Hamiltonians that has a comparable order of terms with molecular Hamiltonians by applying a spin-involving orbital rotation to the one-dimensional Fermi-Hubbard Hamiltonian.","We verify its similarity to the molecular Hamiltonian from some prospectives and investigate whether the difficulty of calculating the ground-state energy changes before and after orbital rotation by applying the density matrix renormalization group up to 24 sites corresponding to 48 qubits.","This proposal would enable proper evaluation of the performance of quantum algorithms for quantum chemistry, serving as a guiding framework for algorithm development."],"url":"http://arxiv.org/abs/2402.11869v1","category":"quant-ph"}
{"created":"2024-02-19 06:13:05","title":"Entanglement Measure Based on Optimal Entanglement Witness","abstract":"We introduce a new entanglement measure based on optimal entanglement witness. First of all, we show that the entanglement measure satisfies some necessary properties, including zero entanglements for all separable states, convexity, continuity, invariance under local unitary operations and non-increase under local operations and classical communication(LOCC). More than that, we give a specific mathematical expression for the lower bound of this entanglement measure for any bipartite mixed states. We further improve the lower bound for 2$ \\otimes $2 systems. Finally, we numerically simulate the lower bound of several types of specific quantum states.","sentences":["We introduce a new entanglement measure based on optimal entanglement witness.","First of all, we show that the entanglement measure satisfies some necessary properties, including zero entanglements for all separable states, convexity, continuity, invariance under local unitary operations and non-increase under local operations and classical communication(LOCC).","More than that, we give a specific mathematical expression for the lower bound of this entanglement measure for any bipartite mixed states.","We further improve the lower bound for 2$ \\otimes $2 systems.","Finally, we numerically simulate the lower bound of several types of specific quantum states."],"url":"http://arxiv.org/abs/2402.11865v1","category":"quant-ph"}
{"created":"2024-02-19 06:06:00","title":"The Effects of Group Discussion and Role-playing Training on Self-efficacy, Support-seeking, and Reporting Phishing Emails: Evidence from a Mixed-design Experiment","abstract":"Organizations rely on phishing interventions to enhance employees' vigilance and safe responses to phishing emails that bypass technical solutions. While various resources are available to counteract phishing, studies emphasize the need for interactive and practical training approaches. To investigate the effectiveness of such an approach, we developed and delivered two anti-phishing trainings, group discussion and role-playing, at a European university. We conducted a pre-registered experiment (N = 105), incorporating repeated measures at three time points, a control group, and three in-situ phishing tests. Both trainings enhanced employees' anti-phishing self-efficacy and support-seeking intention in within-group analyses. Only the role-playing training significantly improved support-seeking intention when compared to the control group. Participants in both trainings reported more phishing tests and demonstrated heightened vigilance to phishing attacks compared to the control group. We discuss practical implications for evaluating and improving phishing interventions and promoting safe responses to phishing threats within organizations.","sentences":["Organizations rely on phishing interventions to enhance employees' vigilance and safe responses to phishing emails that bypass technical solutions.","While various resources are available to counteract phishing, studies emphasize the need for interactive and practical training approaches.","To investigate the effectiveness of such an approach, we developed and delivered two anti-phishing trainings, group discussion and role-playing, at a European university.","We conducted a pre-registered experiment (N = 105), incorporating repeated measures at three time points, a control group, and three in-situ phishing tests.","Both trainings enhanced employees' anti-phishing self-efficacy and support-seeking intention in within-group analyses.","Only the role-playing training significantly improved support-seeking intention when compared to the control group.","Participants in both trainings reported more phishing tests and demonstrated heightened vigilance to phishing attacks compared to the control group.","We discuss practical implications for evaluating and improving phishing interventions and promoting safe responses to phishing threats within organizations."],"url":"http://arxiv.org/abs/2402.11862v1","category":"cs.HC"}
{"created":"2024-02-19 05:57:21","title":"Exponential attractors for a nonlocal delayed reaction-diffusion equation on an unbounded domain","abstract":"The main objective of this paper is to investigate exponential attractors for a nonlocal delayed reaction-diffusion equation on an unbounded domain. We first obtain the existence of a globally attractive absorbing set for the dynamical system generated by the equation under the assumption that the nonlinear term is bounded. Then, we construct exponential attractors of the equation directly in its natural phase space, i.e., a Banach space with explicit fractal dimension by combining squeezing properties of the system as well as a covering lemma of finite subspace of Banach spaces. Our result generalizes the methods established in Hilbert spaces and weighted spaces, and the fractal dimension of the obtained exponential attractor does not depend on the entropy number but only depends on some inner characteristic of the studied equation.","sentences":["The main objective of this paper is to investigate exponential attractors for a nonlocal delayed reaction-diffusion equation on an unbounded domain.","We first obtain the existence of a globally attractive absorbing set for the dynamical system generated by the equation under the assumption that the nonlinear term is bounded.","Then, we construct exponential attractors of the equation directly in its natural phase space, i.e., a Banach space with explicit fractal dimension by combining squeezing properties of the system as well as a covering lemma of finite subspace of Banach spaces.","Our result generalizes the methods established in Hilbert spaces and weighted spaces, and the fractal dimension of the obtained exponential attractor does not depend on the entropy number but only depends on some inner characteristic of the studied equation."],"url":"http://arxiv.org/abs/2402.11856v1","category":"math.AP"}
{"created":"2024-02-19 05:38:19","title":"Exponential contractivity and propagation of chaos for Langevin dynamics of McKean-Vlasov type with L\u00e9vy noises","abstract":"By the probabilistic coupling approach which combines a new refined basic coupling with the synchronous coupling for L\\'evy processes, we obtain explicit exponential contraction rates in terms of the standard $L^1$-Wasserstein distance for the following Langevin dynamic $(X_t,Y_t)_{t\\ge0}$ of McKean-Vlasov type on $\\mathbb{R}^{2d}$: \\begin{equation*}\\left\\{\\begin{array}{l} dX_t=Y_tdt,\\\\ dY_t=\\left(b(X_t)+\\displaystyle\\int_{\\mathbb{R}^d}\\tilde{b}(X_t,z)\\mu^X_t(dz)-\\gamma Y_t\\right)dt+dL_t,\\quad \\mu^X_t={\\rm Law}(X_t),\\end{array}\\right. \\end{equation*} where $\\gamma>0$, $b:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$ and $\\tilde{b}:\\mathbb{R}^{2d}\\rightarrow\\mathbb{R}^d$ are two globally Lipschitz continuous functions, and $(L_t)_{t\\ge0}$ is an $\\mathbb{R}^d$-valued pure jump L\\'evy process. The proof is also based on a novel distance function, which is designed according to the distance of the marginals associated with the constructed coupling process. Furthermore, by applying the coupling technique above with some modifications, we also provide the propagation of chaos uniformly in time for the corresponding mean-field interacting particle systems with L\\'evy noises in the standard $L^1$-Wasserstein distance as well as with explicit bounds.","sentences":["By the probabilistic coupling approach which combines a new refined basic coupling with the synchronous coupling for L\\'evy processes, we obtain explicit exponential contraction rates in terms of the standard $L^1$-Wasserstein distance for the following Langevin dynamic $(X_t,Y_t)_{t\\ge0}$ of McKean-Vlasov type on $\\mathbb{R}^{2d}$: \\begin{equation*}\\left\\{\\begin{array}{l} dX_t=Y_tdt,\\\\ dY_t=\\left(b(X_t)+\\displaystyle\\int_{\\mathbb{R}^d}\\tilde{b}(X_t,z)\\mu^X_t(dz)-\\gamma Y_t\\right)dt+dL_t,\\quad \\mu^X_t={\\rm Law}(X_t),\\end{array}\\right.","\\end{equation*} where $\\gamma>0$, $b:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$ and $\\tilde{b}:\\mathbb{R}^{2d}\\rightarrow\\mathbb{R}^d$ are two globally Lipschitz continuous functions, and $(L_t)_{t\\ge0}$ is an $\\mathbb{R}^d$-valued pure jump L\\'evy process.","The proof is also based on a novel distance function, which is designed according to the distance of the marginals associated with the constructed coupling process.","Furthermore, by applying the coupling technique above with some modifications, we also provide the propagation of chaos uniformly in time for the corresponding mean-field interacting particle systems with L\\'evy noises in the standard $L^1$-Wasserstein distance as well as with explicit bounds."],"url":"http://arxiv.org/abs/2402.11851v1","category":"math.PR"}
{"created":"2024-02-19 05:08:44","title":"ASGNet: Adaptive Semantic Gate Networks for Log-Based Anomaly Diagnosis","abstract":"Logs are widely used in the development and maintenance of software systems. Logs can help engineers understand the runtime behavior of systems and diagnose system failures. For anomaly diagnosis, existing methods generally use log event data extracted from historical logs to build diagnostic models. However, we find that existing methods do not make full use of two types of features, (1) statistical features: some inherent statistical features in log data, such as word frequency and abnormal label distribution, are not well exploited. Compared with log raw data, statistical features are deterministic and naturally compatible with corresponding tasks. (2) semantic features: Logs contain the execution logic behind software systems, thus log statements share deep semantic relationships. How to effectively combine statistical features and semantic features in log data to improve the performance of log anomaly diagnosis is the key point of this paper. In this paper, we propose an adaptive semantic gate networks (ASGNet) that combines statistical features and semantic features to selectively use statistical features to consolidate log text semantic representation. Specifically, ASGNet encodes statistical features via a variational encoding module and fuses useful information through a well-designed adaptive semantic threshold mechanism. The threshold mechanism introduces the information flow into the classifier based on the confidence of the semantic features in the decision, which is conducive to training a robust classifier and can solve the overfitting problem caused by the use of statistical features. The experimental results on the real data set show that our method proposed is superior to all baseline methods in terms of various performance indicators.","sentences":["Logs are widely used in the development and maintenance of software systems.","Logs can help engineers understand the runtime behavior of systems and diagnose system failures.","For anomaly diagnosis, existing methods generally use log event data extracted from historical logs to build diagnostic models.","However, we find that existing methods do not make full use of two types of features, (1) statistical features: some inherent statistical features in log data, such as word frequency and abnormal label distribution, are not well exploited.","Compared with log raw data, statistical features are deterministic and naturally compatible with corresponding tasks.","(2) semantic features:","Logs contain the execution logic behind software systems, thus log statements share deep semantic relationships.","How to effectively combine statistical features and semantic features in log data to improve the performance of log anomaly diagnosis is the key point of this paper.","In this paper, we propose an adaptive semantic gate networks (ASGNet) that combines statistical features and semantic features to selectively use statistical features to consolidate log text semantic representation.","Specifically, ASGNet encodes statistical features via a variational encoding module and fuses useful information through a well-designed adaptive semantic threshold mechanism.","The threshold mechanism introduces the information flow into the classifier based on the confidence of the semantic features in the decision, which is conducive to training a robust classifier and can solve the overfitting problem caused by the use of statistical features.","The experimental results on the real data set show that our method proposed is superior to all baseline methods in terms of various performance indicators."],"url":"http://arxiv.org/abs/2402.11841v1","category":"cs.SE"}
{"created":"2024-02-19 05:06:52","title":"An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models","abstract":"Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTs offer patient-specific insights of complex anatomy, enabling real-time intraoperative navigation to complement endoscopy imaging. However, surgery elicits anatomical changes not represented in the preoperative model, generating an inaccurate basis for navigation during surgery progression.   Methods: We propose a first vision-based approach to update the preoperative 3D anatomical model leveraging intraoperative endoscopic video for navigated sinus surgery where relative camera poses are known. We rely on comparisons of intraoperative monocular depth estimates and preoperative depth renders to identify modified regions. The new depths are integrated in these regions through volumetric fusion in a truncated signed distance function representation to generate an intraoperative 3D model that reflects tissue manipulation.   Results: We quantitatively evaluate our approach by sequentially updating models for a five-step surgical progression in an ex vivo specimen. We compute the error between correspondences from the updated model and ground-truth intraoperative CT in the region of anatomical modification. The resulting models show a decrease in error during surgical progression as opposed to increasing when no update is employed.   Conclusion: Our findings suggest that preoperative 3D anatomical models can be updated using intraoperative endoscopy video in navigated sinus surgery. Future work will investigate improvements to monocular depth estimation as well as removing the need for external navigation systems. The resulting ability to continuously update the patient model may provide surgeons with a more precise understanding of the current anatomical state and paves the way toward a digital twin paradigm for sinus surgery.","sentences":["Purpose:","Preoperative imaging plays a pivotal role in sinus surgery where CTs offer patient-specific insights of complex anatomy, enabling real-time intraoperative navigation to complement endoscopy imaging.","However, surgery elicits anatomical changes not represented in the preoperative model, generating an inaccurate basis for navigation during surgery progression.   ","Methods: We propose a first vision-based approach to update the preoperative 3D anatomical model leveraging intraoperative endoscopic video for navigated sinus surgery where relative camera poses are known.","We rely on comparisons of intraoperative monocular depth estimates and preoperative depth renders to identify modified regions.","The new depths are integrated in these regions through volumetric fusion in a truncated signed distance function representation to generate an intraoperative 3D model that reflects tissue manipulation.   ","Results: We quantitatively evaluate our approach by sequentially updating models for a five-step surgical progression in an ex vivo specimen.","We compute the error between correspondences from the updated model and ground-truth intraoperative CT in the region of anatomical modification.","The resulting models show a decrease in error during surgical progression as opposed to increasing when no update is employed.   ","Conclusion: Our findings suggest that preoperative 3D anatomical models can be updated using intraoperative endoscopy video in navigated sinus surgery.","Future work will investigate improvements to monocular depth estimation as well as removing the need for external navigation systems.","The resulting ability to continuously update the patient model may provide surgeons with a more precise understanding of the current anatomical state and paves the way toward a digital twin paradigm for sinus surgery."],"url":"http://arxiv.org/abs/2402.11840v1","category":"cs.CV"}
{"created":"2024-02-19 05:04:11","title":"UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction","abstract":"Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts that align and leverage intrinsic and shared knowledge across scenarios. These designs together unlock the potential of a one-for-all model for spatio-temporal prediction with powerful generalization capability. Extensive experiments on 15 cities and 6 domains demonstrate the universality of UniST in advancing state-of-the-art prediction performance, especially in few-shot and zero-shot scenarios.","sentences":["Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning.","Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind.","Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data.","In this work, we propose a universal model, UniST, for urban spatio-temporal prediction.","Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts that align and leverage intrinsic and shared knowledge across scenarios.","These designs together unlock the potential of a one-for-all model for spatio-temporal prediction with powerful generalization capability.","Extensive experiments on 15 cities and 6 domains demonstrate the universality of UniST in advancing state-of-the-art prediction performance, especially in few-shot and zero-shot scenarios."],"url":"http://arxiv.org/abs/2402.11838v1","category":"cs.LG"}
{"created":"2024-02-19 04:58:39","title":"Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization","abstract":"We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds all prior methods in environments which are neither fully stationary nor fully nonstationary.","sentences":["We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains.","ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics.","In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment.","In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees.","Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds all prior methods in environments which are neither fully stationary nor fully nonstationary."],"url":"http://arxiv.org/abs/2402.11835v1","category":"cs.LG"}
{"created":"2024-02-19 04:57:12","title":"Terahertz User-Centric Clustering in the Presence of Beam Misalignment","abstract":"Beam misalignment is one of the main challenges for the design of reliable wireless systems in terahertz (THz) bands. This paper investigates how to apply user-centric base station (BS) clustering as a valuable add-on in THz networks. In particular, to reduce the impact of beam misalignment, a user-centric BS clustering design that provides multi-connectivity via BS cooperation is investigated. The coverage probability is derived by leveraging an accurate approximation of the aggregate interference distribution that captures the effect of beam misalignment and THz fading. The numerical results reveal the impact of beam misalignment with respect to crucial link parameters, such as the transmitter's beam width and the serving cluster size, demonstrating that user-centric BS clustering is a promising enabler of THz networks.","sentences":["Beam misalignment is one of the main challenges for the design of reliable wireless systems in terahertz (THz) bands.","This paper investigates how to apply user-centric base station (BS) clustering as a valuable add-on in THz networks.","In particular, to reduce the impact of beam misalignment, a user-centric BS clustering design that provides multi-connectivity via BS cooperation is investigated.","The coverage probability is derived by leveraging an accurate approximation of the aggregate interference distribution that captures the effect of beam misalignment and THz fading.","The numerical results reveal the impact of beam misalignment with respect to crucial link parameters, such as the transmitter's beam width and the serving cluster size, demonstrating that user-centric BS clustering is a promising enabler of THz networks."],"url":"http://arxiv.org/abs/2402.11834v1","category":"eess.SY"}
{"created":"2024-02-19 04:55:51","title":"Approximation of plurisubharmonic functions by logarithms of Gaussian analytic functions","abstract":"Let $\\Omega$ be a bounded psudoconvex domain in $\\mathbb{C}^N$. Given continuous plurisubharmonic function $u$ on $\\Omega$, we construct a sequence of Gaussian analytic functions $f_n$ on $\\Omega$ associated with $u$ such that $\\frac{1}{n}\\log|f_n|$ converges to $u$ in $L^1_{loc}(\\Omega)$ almost surely, as $n\\rightarrow\\infty$. Gaussian analytic function $f_n$ is constructed by its covariance, or equivalently by its reproducing kernel Hilbert space, which is given by weighed Bergman space with weight $e^{-2nu}$ with respect to the Lebesgue measure.","sentences":["Let $\\Omega$ be a bounded psudoconvex domain in $\\mathbb{C}^N$. Given continuous plurisubharmonic function $u$ on $\\Omega$, we construct a sequence of Gaussian analytic functions $f_n$ on $\\Omega$ associated with $u$ such that $\\frac{1}{n}\\log|f_n|$ converges to $u$ in $L^1_{loc}(\\Omega)$ almost surely, as $n\\rightarrow\\infty$. Gaussian analytic function $f_n$ is constructed by its covariance, or equivalently by its reproducing kernel Hilbert space, which is given by weighed Bergman space with weight $e^{-2nu}$ with respect to the Lebesgue measure."],"url":"http://arxiv.org/abs/2402.11833v1","category":"math.CV"}
{"created":"2024-02-19 04:32:33","title":"Moir\u00e9 optical phonons dancing with heavy electrons in magic-angle twisted bilayer graphene","abstract":"Electron-phonon coupling in magic-angle twisted bilayer graphene is an important but difficult topic. We propose a scheme to simplify and understand this problem. Weighted by the coupling strength with the low-energy heavy electrons ($f$ orbitals), several moir\\'{e} optical phonons are singled out which strongly couple to the flat bands. These modes have localized envelopes in the moir\\'{e} scale, while in the atomic scale they inherit the monolayer oscillations like the Kekul\\'{e} pattern. They flip the flavor of $f$ orbitals, helping stabilize some symmetry-breaking orders. Such electron-phonon couplings are incorporated into an effective extended Holstein model, where both phonons and electrons are written as moir\\'{e} scale basis. We hope this model will inspire some insights guiding further studies about the superconductivity and other correlated effects in this system.","sentences":["Electron-phonon coupling in magic-angle twisted bilayer graphene is an important but difficult topic.","We propose a scheme to simplify and understand this problem.","Weighted by the coupling strength with the low-energy heavy electrons ($f$ orbitals), several moir\\'{e} optical phonons are singled out which strongly couple to the flat bands.","These modes have localized envelopes in the moir\\'{e} scale, while in the atomic scale they inherit the monolayer oscillations like the Kekul\\'{e} pattern.","They flip the flavor of $f$ orbitals, helping stabilize some symmetry-breaking orders.","Such electron-phonon couplings are incorporated into an effective extended Holstein model, where both phonons and electrons are written as moir\\'{e} scale basis.","We hope this model will inspire some insights guiding further studies about the superconductivity and other correlated effects in this system."],"url":"http://arxiv.org/abs/2402.11824v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 04:25:35","title":"A critical analysis of cognitive load measurement methods for evaluating the usability of different types of interfaces: guidelines and framework for Human-Computer Interaction","abstract":"Usability testing is an essential part of product design, particularly for user interfaces. To enhance the reliability of usability evaluations, employing cognitive load measurement methods can be highly effective in assessing the mental effort required to complete tasks during user testing. This review aims to provide an overview of the most suitable cognitive load measurement methods for evaluating various types of user interfaces, serving as a valuable resource for guiding usability assessments. To bridge the existing gap in the literature, a systematic review was conducted, analyzing 76 articles with experimental study designs that met the eligibility criteria. The review encompasses different methods of measuring cognitive load applicable to assessing the usability of diverse user interfaces, including computer software, information systems, video games, web and mobile applications, robotics, and virtual reality applications. The results highlight the most widely utilized cognitive load measurement methods in software usability, their respective usage percentages, and their application in evaluating the usability of each user interface type. Additionally, the advantages and disadvantages of each method are discussed. Furthermore, the review proposes a framework to assist usability testers in selecting an appropriate cognitive load measurement method for conducting accurate usability evaluations.","sentences":["Usability testing is an essential part of product design, particularly for user interfaces.","To enhance the reliability of usability evaluations, employing cognitive load measurement methods can be highly effective in assessing the mental effort required to complete tasks during user testing.","This review aims to provide an overview of the most suitable cognitive load measurement methods for evaluating various types of user interfaces, serving as a valuable resource for guiding usability assessments.","To bridge the existing gap in the literature, a systematic review was conducted, analyzing 76 articles with experimental study designs that met the eligibility criteria.","The review encompasses different methods of measuring cognitive load applicable to assessing the usability of diverse user interfaces, including computer software, information systems, video games, web and mobile applications, robotics, and virtual reality applications.","The results highlight the most widely utilized cognitive load measurement methods in software usability, their respective usage percentages, and their application in evaluating the usability of each user interface type.","Additionally, the advantages and disadvantages of each method are discussed.","Furthermore, the review proposes a framework to assist usability testers in selecting an appropriate cognitive load measurement method for conducting accurate usability evaluations."],"url":"http://arxiv.org/abs/2402.11820v1","category":"cs.HC"}
{"created":"2024-02-19 04:15:06","title":"EMU/GAMA: A Technique for Detecting Active Galactic Nuclei in Low Mass Systems","abstract":"We propose a new method for identifying active galactic nuclei (AGN) in low mass ($\\rm M_*\\leq10^{10}M_\\odot$) galaxies. This method relies on spectral energy distribution (SED) fitting to identify galaxies whose radio flux density has an excess over that expected from star formation alone. Combining data in the Galaxy and Mass Assembly (GAMA) G23 region from GAMA, Evolutionary Map of the Universe (EMU) early science observations, and Wide-field Infrared Survey Explorer (WISE), we compare this technique with a selection of different AGN diagnostics to explore the similarities and differences in AGN classification. We find that diagnostics based on optical and near-infrared criteria (the standard BPT diagram, the WISE colour criterion, and the mass-excitation, or MEx diagram) tend to favour detection of AGN in high mass, high luminosity systems, while the ``ProSpect'' SED fitting tool can identify AGN efficiently in low mass systems. We investigate an explanation for this result in the context of proportionally lower mass black holes in lower mass galaxies compared to higher mass galaxies and differing proportions of emission from AGN and star formation dominating the light at optical and infrared wavelengths as a function of galaxy stellar mass. We conclude that SED-derived AGN classification is an efficient approach to identify low mass hosts with low radio luminosity AGN.","sentences":["We propose a new method for identifying active galactic nuclei (AGN) in low mass ($\\rm M_*\\leq10^{10}M_\\odot$) galaxies.","This method relies on spectral energy distribution (SED) fitting to identify galaxies whose radio flux density has an excess over that expected from star formation alone.","Combining data in the Galaxy and Mass Assembly (GAMA) G23 region from GAMA, Evolutionary Map of the Universe (EMU) early science observations, and Wide-field Infrared Survey Explorer (WISE), we compare this technique with a selection of different AGN diagnostics to explore the similarities and differences in AGN classification.","We find that diagnostics based on optical and near-infrared criteria (the standard BPT diagram, the WISE colour criterion, and the mass-excitation, or MEx diagram) tend to favour detection of AGN in high mass, high luminosity systems, while the ``ProSpect'' SED fitting tool can identify AGN efficiently in low mass systems.","We investigate an explanation for this result in the context of proportionally lower mass black holes in lower mass galaxies compared to higher mass galaxies and differing proportions of emission from AGN and star formation dominating the light at optical and infrared wavelengths as a function of galaxy stellar mass.","We conclude that SED-derived AGN classification is an efficient approach to identify low mass hosts with low radio luminosity AGN."],"url":"http://arxiv.org/abs/2402.11817v1","category":"astro-ph.GA"}
{"created":"2024-02-19 03:29:34","title":"Bohr inequalities via proper combinations for a certain class of close-to-convex harmonic mappings","abstract":"Let $ \\mathcal{H}(\\Omega) $ be the class of complex-valued functions harmonic in $ \\Omega\\subset\\mathbb{C} $ and each $f=h+\\overline{g}\\in \\mathcal{H}(\\Omega)$, where $ h $ and $ g $ are analytic. In the study of Bohr phenomenon for certain class of harmonic mappings, it is to find a constant $ r_f\\in (0, 1) $ such that the inequality   \\begin{align*}   M_f(r):=r+\\sum_{n=2}^{\\infty}\\left(|a_n|+|b_n|\\right)r^n\\leq d\\left(f(0), \\partial\\Omega\\right) \\;\\mbox{for}\\;|z|=r\\leq r_f,   \\end{align*}   where $ d\\left(f(0), \\partial\\Omega\\right) $ is the Euclidean distance between $ f(0) $ and the boundary of $ \\Omega:=f(\\mathbb{D}) $. The largest such radius $ r_f $ is called the Bohr radius and the inequality $ M_f(r)\\leq d\\left(f(0), \\partial\\Omega\\right) $ is called the Bohr inequality for the class $ \\mathcal{H}(\\Omega) $. In this paper, we study Bohr phenomenon for the class of close-to-convex harmonic mappings establishing several inequalities. All the results are proved to be sharp.","sentences":["Let $ \\mathcal{H}(\\Omega) $ be the class of complex-valued functions harmonic in $ \\Omega\\subset\\mathbb{C} $ and each $f=h+\\overline{g}\\in \\mathcal{H}(\\Omega)$, where $ h $ and $ g $ are analytic.","In the study of Bohr phenomenon for certain class of harmonic mappings, it is to find a constant $ r_f\\in (0, 1) $ such that the inequality   \\begin{align*}   M_f(r):=r+\\sum_{n=2}^{\\infty}\\left(|a_n|+|b_n|\\right)r^n\\leq d\\left(f(0), \\partial\\Omega\\right) \\;\\mbox{for}\\;|z|=r\\leq r_f,   \\end{align*}   where $ d\\left(f(0), \\partial\\Omega\\right) $ is the Euclidean distance between $ f(0) $ and the boundary of $ \\Omega:=f(\\mathbb{D}) $.","The largest such radius $ r_f $ is called the Bohr radius and the inequality $ M_f(r)\\leq d\\left(f(0), \\partial\\Omega\\right) $ is called the Bohr inequality for the class $ \\mathcal{H}(\\Omega) $.","In this paper, we study Bohr phenomenon for the class of close-to-convex harmonic mappings establishing several inequalities.","All the results are proved to be sharp."],"url":"http://arxiv.org/abs/2402.11808v1","category":"math.CV"}
{"created":"2024-02-19 03:06:43","title":"Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning","abstract":"Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments.","sentences":["Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years.","However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions.","To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows.","We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy.","A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments."],"url":"http://arxiv.org/abs/2402.11799v1","category":"cs.RO"}
{"created":"2024-02-19 02:54:54","title":"The Maximum Singularity Degree for Linear and Semidefinite Programming","abstract":"The singularity degree plays a crucial role in understanding linear and semidefinite programming, providing a theoretical framework for analyzing these problems. It is defined as the minimum number of facial reduction (FR) steps needed to reach strict feasibility for a convex set. On the other hand, the maximum singularity degree (MSD) is the maximum number of steps required. Recent progress in the applications of MSD has motivated us to explore its fundamental properties in this paper.   For semidefinite programming, we establish a necessary condition for an FR sequence to be the longest. Additionally, we propose an upper bound for MSD, which can be computed more easily. By leveraging these findings, we prove that computing MSD is NP-hard. This complexity result complements the existing algorithms for computing the singularity degree found in the literature. For linear programming, we provide a characterization for the longest FR sequences, which also serves as a polynomial-time algorithm for constructing such a sequence. In addition, we introduce two operations that ensure the longest FR sequences remain the longest. Lastly, we prove that MSD is equivalent to a novel parameter called the implicit problem singularity.","sentences":["The singularity degree plays a crucial role in understanding linear and semidefinite programming, providing a theoretical framework for analyzing these problems.","It is defined as the minimum number of facial reduction (FR) steps needed to reach strict feasibility for a convex set.","On the other hand, the maximum singularity degree (MSD) is the maximum number of steps required.","Recent progress in the applications of MSD has motivated us to explore its fundamental properties in this paper.   ","For semidefinite programming, we establish a necessary condition for an FR sequence to be the longest.","Additionally, we propose an upper bound for MSD, which can be computed more easily.","By leveraging these findings, we prove that computing MSD is NP-hard.","This complexity result complements the existing algorithms for computing the singularity degree found in the literature.","For linear programming, we provide a characterization for the longest FR sequences, which also serves as a polynomial-time algorithm for constructing such a sequence.","In addition, we introduce two operations that ensure the longest FR sequences remain the longest.","Lastly, we prove that MSD is equivalent to a novel parameter called the implicit problem singularity."],"url":"http://arxiv.org/abs/2402.11795v1","category":"math.OC"}
{"created":"2024-02-19 02:47:27","title":"SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction","abstract":"Linguistic ambiguity is ubiquitous in our daily lives. Previous works adopted interaction between robots and humans for language disambiguation. Nevertheless, when interactive robots are deployed in daily environments, there are significant challenges for natural human-robot interaction, stemming from complex and unpredictable visual inputs, open-ended interaction, and diverse user demands. In this paper, we present SInViG, which is a self-evolving interactive visual agent for human-robot interaction based on natural languages, aiming to resolve language ambiguity, if any, through multi-turn visual-language dialogues. It continuously and automatically learns from unlabeled images and large language models, without human intervention, to be more robust against visual and linguistic complexity. Benefiting from self-evolving, it sets new state-of-the-art on several interactive visual grounding benchmarks. Moreover, our human-robot interaction experiments show that the evolved models consistently acquire more and more preferences from human users. Besides, we also deployed our model on a Franka robot for interactive manipulation tasks. Results demonstrate that our model can follow diverse user instructions and interact naturally with humans in natural language, despite the complexity and disturbance of the environment.","sentences":["Linguistic ambiguity is ubiquitous in our daily lives.","Previous works adopted interaction between robots and humans for language disambiguation.","Nevertheless, when interactive robots are deployed in daily environments, there are significant challenges for natural human-robot interaction, stemming from complex and unpredictable visual inputs, open-ended interaction, and diverse user demands.","In this paper, we present SInViG, which is a self-evolving interactive visual agent for human-robot interaction based on natural languages, aiming to resolve language ambiguity, if any, through multi-turn visual-language dialogues.","It continuously and automatically learns from unlabeled images and large language models, without human intervention, to be more robust against visual and linguistic complexity.","Benefiting from self-evolving, it sets new state-of-the-art on several interactive visual grounding benchmarks.","Moreover, our human-robot interaction experiments show that the evolved models consistently acquire more and more preferences from human users.","Besides, we also deployed our model on a Franka robot for interactive manipulation tasks.","Results demonstrate that our model can follow diverse user instructions and interact naturally with humans in natural language, despite the complexity and disturbance of the environment."],"url":"http://arxiv.org/abs/2402.11792v1","category":"cs.RO"}
{"created":"2024-02-19 02:41:37","title":"SDGE: Stereo Guided Depth Estimation for 360\u00b0 Camera Sets","abstract":"Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360{\\deg} perception. These 360{\\deg} camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360{\\deg} cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D object detection and occupancy prediction.","sentences":["Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360{\\deg} perception.","These 360{\\deg} camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image.","Alternatively, monocular methods may not produce consistent cross-view predictions.","To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap.","We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360{\\deg} cameras.","For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap.","These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region.","This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency.","The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes.","Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D object detection and occupancy prediction."],"url":"http://arxiv.org/abs/2402.11791v1","category":"cs.CV"}
{"created":"2024-02-19 02:33:41","title":"CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms","abstract":"Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge. In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment. The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by leveraging shared data and refining joint pose graph optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation. Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates. Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks. In order to contribute to the research community, we will make our code open-source and accessible at \\url{https://github.com/PengYu-team/Co-LRIO}.","sentences":["Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge.","In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment.","The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations.","The server back-end establishes a global reference by leveraging shared data and refining joint pose graph optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation.","Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates.","Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks.","In order to contribute to the research community, we will make our code open-source and accessible at \\url{https://github.com/PengYu-team/Co-LRIO}."],"url":"http://arxiv.org/abs/2402.11790v1","category":"cs.RO"}
{"created":"2024-02-19 02:11:06","title":"Affine manifolds: The differential geometry of the multi-dimensionally consistent TED equation","abstract":"It is shown that a canonical geometric setting of the integrable TED equation is a Kahlerian tangent bundle of an affine manifold. The remarkable multi-dimensional consistency of this 4+4-dimensional dispersionless partial differential equation arises naturally in this context. In a particular 4-dimensional reduction, the affine manifolds turn out to be self-dual Einstein spaces of neutral signature governed by Plebanski's first heavenly equation. In another reduction, the affine manifolds are Hessian, governed by compatible general heavenly equations. The Legendre invariance of the latter gives rise to a (dual) Hessian structure. Foliations of affine manifolds in terms of self-dual Einstein spaces are also shown to arise in connection with a natural 5-dimensional reduction.","sentences":["It is shown that a canonical geometric setting of the integrable TED equation is a Kahlerian tangent bundle of an affine manifold.","The remarkable multi-dimensional consistency of this 4+4-dimensional dispersionless partial differential equation arises naturally in this context.","In a particular 4-dimensional reduction, the affine manifolds turn out to be self-dual Einstein spaces of neutral signature governed by Plebanski's first heavenly equation.","In another reduction, the affine manifolds are Hessian, governed by compatible general heavenly equations.","The Legendre invariance of the latter gives rise to a (dual) Hessian structure.","Foliations of affine manifolds in terms of self-dual Einstein spaces are also shown to arise in connection with a natural 5-dimensional reduction."],"url":"http://arxiv.org/abs/2402.11779v1","category":"nlin.SI"}
{"created":"2024-02-19 02:07:37","title":"Early feasibility of an embedded bi-directional brain-computer interface for ambulation","abstract":"Current treatments for paraplegia induced by spinal cord injury (SCI) are often limited by the severity of the injury. The accompanying loss of sensory and motor functions often results in reliance on wheelchairs, which in turn causes reduced quality of life and increased risk of co-morbidities. While brain-computer interfaces (BCIs) for ambulation have shown promise in restoring or replacing lower extremity motor functions, none so far have simultaneously implemented sensory feedback functions. Additionally, many existing BCIs for ambulation rely on bulky external hardware that make them ill-suited for non-research settings. Here, we present an embedded bi-directional BCI (BDBCI), that restores motor function by enabling neural control over a robotic gait exoskeleton (RGE) and delivers sensory feedback via direct cortical electrical stimulation (DCES) in response to RGE leg swing. A first demonstration with this system was performed with a single subject implanted with electrocorticography electrodes, achieving an average lag-optimized cross-correlation of 0.80$\\pm$0.08 between cues and decoded states over 5 runs.","sentences":["Current treatments for paraplegia induced by spinal cord injury (SCI) are often limited by the severity of the injury.","The accompanying loss of sensory and motor functions often results in reliance on wheelchairs, which in turn causes reduced quality of life and increased risk of co-morbidities.","While brain-computer interfaces (BCIs) for ambulation have shown promise in restoring or replacing lower extremity motor functions, none so far have simultaneously implemented sensory feedback functions.","Additionally, many existing BCIs for ambulation rely on bulky external hardware that make them ill-suited for non-research settings.","Here, we present an embedded bi-directional BCI (BDBCI), that restores motor function by enabling neural control over a robotic gait exoskeleton (RGE) and delivers sensory feedback via direct cortical electrical stimulation (DCES) in response to RGE leg swing.","A first demonstration with this system was performed with a single subject implanted with electrocorticography electrodes, achieving an average lag-optimized cross-correlation of 0.80$\\pm$0.08 between cues and decoded states over 5 runs."],"url":"http://arxiv.org/abs/2402.11776v1","category":"q-bio.QM"}
{"created":"2024-02-19 18:19:17","title":"Quantum Sabidussi's Theorem","abstract":"Sabidussi's theorem [Duke Math. J. 28, 1961] gives if and only if conditions under which the automorphism group of a lexicographic product of two graphs is a wreath product of the respective automorphism groups. We prove a quantum version of Sabidussi's theorem for finite graphs, with the automorphism groups replaced by quantum automorphism groups and the wreath product replaced by the free wreath product of quantum groups. This extends the result of Chassaniol [J. Algebra 456, 2016], who proved it for regular graphs. Moreover, we apply our result to lexicographic products of quantum vertex transitive graphs, determining their quantum automorphism groups even when Sabidussi's conditions do not apply.","sentences":["Sabidussi's theorem","[Duke Math.","J. 28, 1961] gives if and only if conditions under which the automorphism group of a lexicographic product of two graphs is a wreath product of the respective automorphism groups.","We prove a quantum version of Sabidussi's theorem for finite graphs, with the automorphism groups replaced by quantum automorphism groups and the wreath product replaced by the free wreath product of quantum groups.","This extends the result of Chassaniol [J. Algebra 456, 2016], who proved it for regular graphs.","Moreover, we apply our result to lexicographic products of quantum vertex transitive graphs, determining their quantum automorphism groups even when Sabidussi's conditions do not apply."],"url":"http://arxiv.org/abs/2402.12344v1","category":"math.QA"}
{"created":"2024-02-19 18:16:51","title":"Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!","abstract":"Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.","sentences":["Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans.","However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation.","This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training.","Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin.","Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment."],"url":"http://arxiv.org/abs/2402.12343v1","category":"cs.CL"}
{"created":"2024-02-19 18:01:56","title":"On the Ocean Conditions of Hycean Worlds","abstract":"Recent studies have suggested the possibility of Hycean worlds, characterised by deep liquid water oceans beneath H$_2$-rich atmospheres. These planets significantly widen the range of planetary properties over which habitable conditions could exist. We conduct internal structure modelling of Hycean worlds to investigate the range of interior compositions, ocean depths and atmospheric mass fractions possible. Our investigation explicitly considers habitable oceans, where the surface conditions are limited to those that can support potential life. The ocean depths depend on the surface gravity and temperature, confirming previous studies, and span 10s to $\\sim$1000 km for Hycean conditions, reaching ocean base pressures up to $\\sim$6$\\times$10$^4$ bar before transitioning to high-pressure ice. We explore in detail test cases of five Hycean candidates, placing constraints on their possible ocean depths and interior compositions based on their bulk properties. We report limits on their atmospheric mass fractions admissible for Hycean conditions, as well as those allowed for other possible interior compositions. For the Hycean conditions considered, across these candidates we find the admissible mass fractions of the H/He envelopes to be $\\lesssim$10$^{-3}$. At the other extreme, the maximum H/He mass fractions allowed for these planets can be up to $\\sim$4-8$\\%$, representing purely rocky interiors with no H$_2$O layer. These results highlight the diverse conditions possible among these planets and demonstrate their potential to host habitable conditions under vastly different circumstances to the Earth. Upcoming JWST observations of candidate Hycean worlds will allow for improved constraints on the nature of their atmospheres and interiors.","sentences":["Recent studies have suggested the possibility of Hycean worlds, characterised by deep liquid water oceans beneath H$_2$-rich atmospheres.","These planets significantly widen the range of planetary properties over which habitable conditions could exist.","We conduct internal structure modelling of Hycean worlds to investigate the range of interior compositions, ocean depths and atmospheric mass fractions possible.","Our investigation explicitly considers habitable oceans, where the surface conditions are limited to those that can support potential life.","The ocean depths depend on the surface gravity and temperature, confirming previous studies, and span 10s to $\\sim$1000 km for Hycean conditions, reaching ocean base pressures up to $\\sim$6$\\times$10$^4$ bar before transitioning to high-pressure ice.","We explore in detail test cases of five Hycean candidates, placing constraints on their possible ocean depths and interior compositions based on their bulk properties.","We report limits on their atmospheric mass fractions admissible for Hycean conditions, as well as those allowed for other possible interior compositions.","For the Hycean conditions considered, across these candidates we find the admissible mass fractions of the H/He envelopes to be $\\lesssim$10$^{-3}$. At the other extreme, the maximum H/He mass fractions allowed for these planets can be up to $\\sim$4-8$\\%$, representing purely rocky interiors with no H$_2$O layer.","These results highlight the diverse conditions possible among these planets and demonstrate their potential to host habitable conditions under vastly different circumstances to the Earth.","Upcoming JWST observations of candidate Hycean worlds will allow for improved constraints on the nature of their atmospheres and interiors."],"url":"http://arxiv.org/abs/2402.12330v1","category":"astro-ph.EP"}
{"created":"2024-02-19 17:56:43","title":"Expressing and visualizing model uncertainty in Bayesian variable selection using Cartesian credible sets","abstract":"Modern regression applications can involve hundreds or thousands of variables which motivates the use of variable selection methods. Bayesian variable selection defines a posterior distribution on the possible subsets of the variables (which are usually termed models) to express uncertainty about which variables are strongly linked to the response. This can be used to provide Bayesian model averaged predictions or inference, and to understand the relative importance of different variables. However, there has been little work on meaningful representations of this uncertainty beyond first order summaries. We introduce Cartesian credible sets to address this gap. The elements of these sets are formed by concatenating sub-models defined on each block of a partition of the variables. Investigating these sub-models allow us to understand whether the models in the Cartesian credible set always/never/sometimes include a particular variable or group of variables and provide a useful summary of model uncertainty. We introduce methods to find these sets that emphasize ease of understanding. The potential of the method is illustrated on regression problems with both small and large numbers of variables.","sentences":["Modern regression applications can involve hundreds or thousands of variables which motivates the use of variable selection methods.","Bayesian variable selection defines a posterior distribution on the possible subsets of the variables (which are usually termed models) to express uncertainty about which variables are strongly linked to the response.","This can be used to provide Bayesian model averaged predictions or inference, and to understand the relative importance of different variables.","However, there has been little work on meaningful representations of this uncertainty beyond first order summaries.","We introduce Cartesian credible sets to address this gap.","The elements of these sets are formed by concatenating sub-models defined on each block of a partition of the variables.","Investigating these sub-models allow us to understand whether the models in the Cartesian credible set always/never/sometimes include a particular variable or group of variables and provide a useful summary of model uncertainty.","We introduce methods to find these sets that emphasize ease of understanding.","The potential of the method is illustrated on regression problems with both small and large numbers of variables."],"url":"http://arxiv.org/abs/2402.12323v1","category":"stat.ME"}
{"created":"2024-02-19 17:36:39","title":"Prescribed $L_p$ quotient curvature problem and related eigenvalue problem","abstract":"In this paper, we investigate the existence of admissible (and strictly convex) smooth solutions to the prescribed $L_p$ quotient type curvature problem with $p>1$. For cases where $p=k-l+1$ and $p> k-l+1$, we obtain an admissible solution without any additional conditions, which is strictly spherically convex under a convexity condition. Under the same convexity condition, we establish the existence of a strictly spherically convex solution for the case $p<k-l+1$, provided that the prescribed function is even, a condition known to be necessary.","sentences":["In this paper, we investigate the existence of admissible (and strictly convex) smooth solutions to the prescribed $L_p$ quotient type curvature problem with $p>1$. For cases where $p=k-l+1$ and $p> k-l+1$, we obtain an admissible solution without any additional conditions, which is strictly spherically convex under a convexity condition.","Under the same convexity condition, we establish the existence of a strictly spherically convex solution for the case $p<k-l+1$, provided that the prescribed function is even, a condition known to be necessary."],"url":"http://arxiv.org/abs/2402.12314v1","category":"math.AP"}
{"created":"2024-02-19 17:24:35","title":"Simple model for the kinetics of stimuli-responsive gels with porous structures","abstract":"Stimuli-responsive gels are the gels that vary the volume depending on environmental conditions. It has been reported that the stimuli-responsive gel with an inhomogeneous structure exhibits faster volume change than the gel without it. It is understood as a difference in the transfer dynamics of the solvent, though, there are few models for discussing the effect of inhomogeneity explicitly. In this paper, we propose a simple model for the kinetics of volume change by introducing inhomogeneity as the probability distribution for a random variable that characterizes the structure. Under this framework, we demonstrate that inhomogeneity actually increases the rate of volume change. Furthermore, through analysis of a simplified example, we show that some features that are often seen in experiments can be reproduced by the model. This model provides a simple equation for the time series of volume change that can be used to characterize the experimental data based on a simple physical picture of the phenomenon.","sentences":["Stimuli-responsive gels are the gels that vary the volume depending on environmental conditions.","It has been reported that the stimuli-responsive gel with an inhomogeneous structure exhibits faster volume change than the gel without it.","It is understood as a difference in the transfer dynamics of the solvent, though, there are few models for discussing the effect of inhomogeneity explicitly.","In this paper, we propose a simple model for the kinetics of volume change by introducing inhomogeneity as the probability distribution for a random variable that characterizes the structure.","Under this framework, we demonstrate that inhomogeneity actually increases the rate of volume change.","Furthermore, through analysis of a simplified example, we show that some features that are often seen in experiments can be reproduced by the model.","This model provides a simple equation for the time series of volume change that can be used to characterize the experimental data based on a simple physical picture of the phenomenon."],"url":"http://arxiv.org/abs/2402.12301v1","category":"cond-mat.soft"}
{"created":"2024-02-19 16:14:09","title":"JWST MIRI MRS Images Disk Winds, Water, and CO in an Edge-On Protoplanetary Disk","abstract":"We present JWST MIRI MRS observations of the edge-on protoplanetary disk around the young sub-solar mass star Tau 042021, acquired as part of the Cycle 1 GO program \"Mapping Inclined Disk Astrochemical Signatures (MIDAS).\" These data resolve the mid-IR spatial distributions of H$_2$, revealing X-shaped emission extending to ~200 au above the disk midplane with a semi-opening angle of $35 \\pm 5$ degrees. We do not velocity-resolve the gas in the spectral images, but the measured semi-opening angle of the H$_2$ is consistent with an MHD wind origin. A collimated, bipolar jet is seen in forbidden emission lines from [Ne II], [Ne III], [Ni II], [Fe II], [Ar II], and [S III]. Extended H$_2$O and CO emission lines are also detected, reaching diameters between ~90 and 190 au, respectively. Hot molecular emission is not expected at such radii, and we interpret its extended spatial distribution as scattering of inner disk molecular emission by dust grains in the outer disk surface. H I recombination lines, characteristic of inner disk accretion shocks, are similarly extended, and are likely also scattered light from the innermost star-disk interface. Finally, we detect extended PAH emission at 11.3 microns co-spatial with the scattered light continuum, making this the first low-mass T Tauri star around which extended PAHs have been confirmed, to our knowledge. MIRI MRS line images of edge-on disks provide an unprecedented window into the outflow, accretion, and scattering processes within protoplanetary disks, allowing us to constrain the disk lifetimes and accretion and mass loss mechanisms.","sentences":["We present JWST MIRI MRS observations of the edge-on protoplanetary disk around the young sub-solar mass star Tau 042021, acquired as part of the Cycle 1 GO program \"Mapping Inclined Disk Astrochemical Signatures (MIDAS).\"","These data resolve the mid-IR spatial distributions of H$_2$, revealing X-shaped emission extending to ~200 au above the disk midplane with a semi-opening angle of $35 \\pm 5$ degrees.","We do not velocity-resolve the gas in the spectral images, but the measured semi-opening angle of the H$_2$ is consistent with an MHD wind origin.","A collimated, bipolar jet is seen in forbidden emission lines from [Ne II], [Ne III], [Ni II], [Fe II], [Ar II], and [S III].","Extended H$_2$O and CO emission lines are also detected, reaching diameters between ~90 and 190 au, respectively.","Hot molecular emission is not expected at such radii, and we interpret its extended spatial distribution as scattering of inner disk molecular emission by dust grains in the outer disk surface.","H I recombination lines, characteristic of inner disk accretion shocks, are similarly extended, and are likely also scattered light from the innermost star-disk interface.","Finally, we detect extended PAH emission at 11.3 microns co-spatial with the scattered light continuum, making this the first low-mass T Tauri star around which extended PAHs have been confirmed, to our knowledge.","MIRI MRS line images of edge-on disks provide an unprecedented window into the outflow, accretion, and scattering processes within protoplanetary disks, allowing us to constrain the disk lifetimes and accretion and mass loss mechanisms."],"url":"http://arxiv.org/abs/2402.12256v1","category":"astro-ph.SR"}
{"created":"2024-02-19 15:03:04","title":"Zero shot VLMs for hate meme detection: Are we there yet?","abstract":"Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we observe that large VLMs are still vulnerable for zero-shot hate meme detection.","sentences":["Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form.","Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes.","Extensive research has been conducted to address this issue by developing hate meme detection models.","However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification.","Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks.","In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection.","We use various prompt settings to focus on zero-shot classification of hateful/harmful memes.","Through our analysis, we observe that large VLMs are still vulnerable for zero-shot hate meme detection."],"url":"http://arxiv.org/abs/2402.12198v1","category":"cs.CL"}
{"created":"2024-02-19 14:48:23","title":"ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning","abstract":"Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/UniModal4Reasoning/ChartVLM","sentences":["Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously.","However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored.","In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data.","Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images.","We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set.","Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models.","Both ChartX and ChartVLM are available at: https://github.com/UniModal4Reasoning/ChartVLM"],"url":"http://arxiv.org/abs/2402.12185v1","category":"cs.CV"}
{"created":"2024-02-19 14:29:43","title":"Fine grinding localized updates via gauge equivariant flows in the 2D Schwinger model","abstract":"State-of-the-art simulations of discrete gauge theories are based on Markov chains with local changes in the field space, which however at very fine lattice spacings are notoriously difficult due to separated topological sectors of the gauge field. Hybrid Monte Carlo (HMC) algorithms, which are very efficient at coarser lattice spacings, suffer from increasing autocorrelation times. This makes simulation of lattice QCD close to the continuum infeasible even with exa-scale computing.   An approach, which can overcome long autocorrelation times, is based on trivializing maps, where a new gauge proposal is given by mapping a configuration from a trivial space to the target one, distributed via the associated Boltzmann factor. Using gauge equivariant coupling layers, the map can be approximated via machine learning techniques. However the deviations scale with the volume in case of local theories and extensive distributions, rendering a global update unfeasible for realistic box sizes.   In this proceeding, we will discuss the potential of localized updates in case of the 2D Schwinger Model. Using gauge-equivariant flow maps, a local update can be fine grained towards finer lattice spacing. Based on this we will present results on simulating the 2D Schwinger Model with dynamical Nf=2 Wilson fermions at fine lattice spacings using scalable global correction steps and compare the performance to the HMC.","sentences":["State-of-the-art simulations of discrete gauge theories are based on Markov chains with local changes in the field space, which however at very fine lattice spacings are notoriously difficult due to separated topological sectors of the gauge field.","Hybrid Monte Carlo (HMC) algorithms, which are very efficient at coarser lattice spacings, suffer from increasing autocorrelation times.","This makes simulation of lattice QCD close to the continuum infeasible even with exa-scale computing.   ","An approach, which can overcome long autocorrelation times, is based on trivializing maps, where a new gauge proposal is given by mapping a configuration from a trivial space to the target one, distributed via the associated Boltzmann factor.","Using gauge equivariant coupling layers, the map can be approximated via machine learning techniques.","However the deviations scale with the volume in case of local theories and extensive distributions, rendering a global update unfeasible for realistic box sizes.   ","In this proceeding, we will discuss the potential of localized updates in case of the 2D Schwinger Model.","Using gauge-equivariant flow maps, a local update can be fine grained towards finer lattice spacing.","Based on this we will present results on simulating the 2D Schwinger Model with dynamical Nf=2 Wilson fermions at fine lattice spacings using scalable global correction steps and compare the performance to the HMC."],"url":"http://arxiv.org/abs/2402.12176v1","category":"hep-lat"}
{"created":"2024-02-19 14:25:09","title":"A frequentist test of proportional colocalization after selecting relevant genetic variants","abstract":"Colocalization analyses assess whether two traits are affected by the same or distinct causal genetic variants in a single gene region. A class of Bayesian colocalization tests are now routinely used in practice; for example, for genetic analyses in drug development pipelines. In this work, we consider an alternative frequentist approach to colocalization testing that examines the proportionality of genetic associations with each trait. The proportional colocalization approach uses markedly different assumptions to Bayesian colocalization tests, and therefore can provide valuable complementary evidence in cases where Bayesian colocalization results are inconclusive or sensitive to priors. We propose a novel conditional test of proportional colocalization, prop-coloc-cond, that aims to account for the uncertainty in variant selection, in order to recover accurate type I error control. The test can be implemented straightforwardly, requiring only summary data on genetic associations. Simulation evidence and an empirical investigation into GLP1R gene expression demonstrates how tests of proportional colocalization can offer important insights in conjunction with Bayesian colocalization tests.","sentences":["Colocalization analyses assess whether two traits are affected by the same or distinct causal genetic variants in a single gene region.","A class of Bayesian colocalization tests are now routinely used in practice; for example, for genetic analyses in drug development pipelines.","In this work, we consider an alternative frequentist approach to colocalization testing that examines the proportionality of genetic associations with each trait.","The proportional colocalization approach uses markedly different assumptions to Bayesian colocalization tests, and therefore can provide valuable complementary evidence in cases where Bayesian colocalization results are inconclusive or sensitive to priors.","We propose a novel conditional test of proportional colocalization, prop-coloc-cond, that aims to account for the uncertainty in variant selection, in order to recover accurate type I error control.","The test can be implemented straightforwardly, requiring only summary data on genetic associations.","Simulation evidence and an empirical investigation into GLP1R gene expression demonstrates how tests of proportional colocalization can offer important insights in conjunction with Bayesian colocalization tests."],"url":"http://arxiv.org/abs/2402.12171v1","category":"stat.ME"}
{"created":"2024-02-19 14:18:08","title":"SCARF: Securing Chips with a Robust Framework against Fabrication-time Hardware Trojans","abstract":"The globalization of the semiconductor industry has introduced security challenges to Integrated Circuits (ICs), particularly those related to the threat of Hardware Trojans (HTs) - malicious logic that can be introduced during IC fabrication. While significant efforts are directed towards verifying the correctness and reliability of ICs, their security is often overlooked. In this paper, we propose a comprehensive approach to enhance IC security from the front-end to back-end stages of design. Initially, we outline a systematic method to transform existing verification assets into potent security checkers by repurposing verification assertions. To further improve security, we introduce an innovative technique for integrating online monitors during physical synthesis - a back-end insertion providing an additional layer of defense. Experimental results demonstrate a significant increase in security, measured by our introduced metric, Security Coverage (SC), with a marginal rise in area and power consumption, typically under 20\\%. The insertion of online monitors during physical synthesis enhances security metrics by up to 33.5\\%. This holistic approach offers a comprehensive and resilient defense mechanism across the entire spectrum of IC design.","sentences":["The globalization of the semiconductor industry has introduced security challenges to Integrated Circuits (ICs), particularly those related to the threat of Hardware Trojans (HTs) - malicious logic that can be introduced during IC fabrication.","While significant efforts are directed towards verifying the correctness and reliability of ICs, their security is often overlooked.","In this paper, we propose a comprehensive approach to enhance IC security from the front-end to back-end stages of design.","Initially, we outline a systematic method to transform existing verification assets into potent security checkers by repurposing verification assertions.","To further improve security, we introduce an innovative technique for integrating online monitors during physical synthesis - a back-end insertion providing an additional layer of defense.","Experimental results demonstrate a significant increase in security, measured by our introduced metric, Security Coverage (SC), with a marginal rise in area and power consumption, typically under 20\\%.","The insertion of online monitors during physical synthesis enhances security metrics by up to 33.5\\%.","This holistic approach offers a comprehensive and resilient defense mechanism across the entire spectrum of IC design."],"url":"http://arxiv.org/abs/2402.12162v1","category":"cs.CR"}
{"created":"2024-02-19 13:16:10","title":"Almost sure convergence rates of adaptive increasingly rare Markov chain Monte Carlo","abstract":"We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC), which is an adaptive MCMC method, where the adaptation concerning the past happens less and less frequently over time. Under a contraction assumption for a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is close to the one that appears in a law of the iterated logarithm. We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity. All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case. In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed.","sentences":["We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC), which is an adaptive MCMC method, where the adaptation concerning the past happens less and less frequently over time.","Under a contraction assumption for a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is close to the one that appears in a law of the iterated logarithm.","We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity.","All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case.","In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed."],"url":"http://arxiv.org/abs/2402.12122v1","category":"math.NA"}
{"created":"2024-02-19 12:07:23","title":"Principle of multi-critical-points in the ALP-Higgs model and the corresponding phase transition","abstract":"The principle of multi-critical-points (PMCP) may be a convincing approach to determine the emerging parameter values in different kinds of beyond-standard-model (BSM) models. This could certainly be applied to solve the problem of undetermined new parameters in the ALP-Higgs interaction models. In this paper, we apply this principle to such model and investigate whether there are suitable solutions. Then, using the 1-loop effective potential, we study the phase transition property of this model under the PMCP requirement. It is gratifying to find that under the requirement of PMCP, the phase transition can be not only first-order, but also strong enough to serve as a solution for electroweak baryongenesis (EWBG). Finally, we show the parameter space of ALP and provide the parameter range that leads to the first-order phase transition.","sentences":["The principle of multi-critical-points (PMCP) may be a convincing approach to determine the emerging parameter values in different kinds of beyond-standard-model (BSM) models.","This could certainly be applied to solve the problem of undetermined new parameters in the ALP-Higgs interaction models.","In this paper, we apply this principle to such model and investigate whether there are suitable solutions.","Then, using the 1-loop effective potential, we study the phase transition property of this model under the PMCP requirement.","It is gratifying to find that under the requirement of PMCP, the phase transition can be not only first-order, but also strong enough to serve as a solution for electroweak baryongenesis (EWBG).","Finally, we show the parameter space of ALP and provide the parameter range that leads to the first-order phase transition."],"url":"http://arxiv.org/abs/2402.12085v1","category":"hep-ph"}
{"created":"2024-02-19 10:57:37","title":"Broadband parametric amplification in DARTWARS","abstract":"Superconducting parametric amplifiers offer the capability to amplify feeble signals with extremely low levels of added noise, potentially reaching quantum-limited amplification. This characteristic makes them essential components in the realm of high-fidelity quantum computing and serves to propel advancements in the field of quantum sensing. In particular, Traveling-Wave Parametric Amplifiers (TWPAs) may be especially suitable for practical applications due to their multi-Gigahertz amplification bandwidth, a feature lacking in Josephson Parametric Amplifiers (JPAs), despite the latter being a more established technology. This paper presents recent developments of the DARTWARS (Detector Array Readout with Traveling Wave AmplifieRS) project, focusing on the latest prototypes of Kinetic Inductance TWPAs (KITWPAs). The project aims to develop a KITWPA capable of achieving $20\\,$ dB of amplification. To enhance the production yield, the first prototypes were fabricated with half the length and expected gain of the final device. In this paper, we present the results of the characterization of one of the half-length prototypes. The measurements revealed an average amplification of approximately $9\\,$dB across a $2\\,$GHz bandwidth for a KITWPA spanning $17\\,$mm in length.","sentences":["Superconducting parametric amplifiers offer the capability to amplify feeble signals with extremely low levels of added noise, potentially reaching quantum-limited amplification.","This characteristic makes them essential components in the realm of high-fidelity quantum computing and serves to propel advancements in the field of quantum sensing.","In particular, Traveling-Wave Parametric Amplifiers (TWPAs) may be especially suitable for practical applications due to their multi-Gigahertz amplification bandwidth, a feature lacking in Josephson Parametric Amplifiers (JPAs), despite the latter being a more established technology.","This paper presents recent developments of the DARTWARS (Detector Array Readout with Traveling Wave AmplifieRS) project, focusing on the latest prototypes of Kinetic Inductance TWPAs (KITWPAs).","The project aims to develop a KITWPA capable of achieving $20\\,$ dB of amplification.","To enhance the production yield, the first prototypes were fabricated with half the length and expected gain of the final device.","In this paper, we present the results of the characterization of one of the half-length prototypes.","The measurements revealed an average amplification of approximately $9\\,$dB across a $2\\,$GHz bandwidth for a KITWPA spanning $17\\,$mm in length."],"url":"http://arxiv.org/abs/2402.12045v1","category":"quant-ph"}
{"created":"2024-02-19 10:33:41","title":"Lightweight Syntactic API Usage Analysis with UCov","abstract":"Designing an effective API is essential for library developers as it is the lens through which clients will judge its usability and benefits, as well as the main friction point when the library evolves. Despite its importance, defining the boundaries of an API is a challenging task, mainly due to the diverse mechanisms provided by programming languages that have non-trivial interplays. In this paper, we present a novel conceptual framework designed to assist library maintainers in understanding the interactions allowed by their APIs via the use of syntactic usage models. These customizable models enable library maintainers to improve their design ahead of release, reducing friction during evolution. The complementary syntactic usage footprints and coverage scores, inferred from client code using the API (e.g., documentation samples, tests, third-party clients), enable developers to understand in-the-wild uses of their APIs and to reflect on the adequacy of their tests and documentation. We implement these models for Java libraries in a new tool UCov and demonstrate its capabilities on three libraries exhibiting diverse styles of interaction: jsoup, commons-cli, and spark. Our exploratory case study shows that UCov provides valuable information regarding API design and fine-grained analysis of client code to identify under-tested and under-documented library code.","sentences":["Designing an effective API is essential for library developers as it is the lens through which clients will judge its usability and benefits, as well as the main friction point when the library evolves.","Despite its importance, defining the boundaries of an API is a challenging task, mainly due to the diverse mechanisms provided by programming languages that have non-trivial interplays.","In this paper, we present a novel conceptual framework designed to assist library maintainers in understanding the interactions allowed by their APIs via the use of syntactic usage models.","These customizable models enable library maintainers to improve their design ahead of release, reducing friction during evolution.","The complementary syntactic usage footprints and coverage scores, inferred from client code using the API (e.g., documentation samples, tests, third-party clients), enable developers to understand in-the-wild uses of their APIs and to reflect on the adequacy of their tests and documentation.","We implement these models for Java libraries in a new tool UCov and demonstrate its capabilities on three libraries exhibiting diverse styles of interaction: jsoup, commons-cli, and spark.","Our exploratory case study shows that UCov provides valuable information regarding API design and fine-grained analysis of client code to identify under-tested and under-documented library code."],"url":"http://arxiv.org/abs/2402.12024v1","category":"cs.SE"}
{"created":"2024-02-19 10:07:20","title":"Combinatorial correlation functions in three-dimensional eight-vertex models","abstract":"A new version of the self-similarity spin transform on three-dimensional cubic lattices is proposed that makes possible calculation of nontrivial spin correlations in a \"combinatorial\" model, in which all permitted spin configurations have equal probabilities.","sentences":["A new version of the self-similarity spin transform on three-dimensional cubic lattices is proposed that makes possible calculation of nontrivial spin correlations in a \"combinatorial\" model, in which all permitted spin configurations have equal probabilities."],"url":"http://arxiv.org/abs/2402.12012v1","category":"math.QA"}
{"created":"2024-02-19 10:04:59","title":"A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change","abstract":"Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather than solely focusing on the extent of semantic change.","sentences":["Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC).","Current evaluations typically focus on a specific task known as Graded Change Detection (GCD).","However, performance comparison across work are often misleading due to their reliance on diverse settings.","In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions.","We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels.","Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather than solely focusing on the extent of semantic change."],"url":"http://arxiv.org/abs/2402.12011v1","category":"cs.CL"}
{"created":"2024-02-19 09:39:54","title":"Network Inversion of Binarised Neural Nets","abstract":"While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model's learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous \"garbage\" to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network's structure, allowing for both inference and inversion.","sentences":["While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge.","Network inversion, a technique that aims to reconstruct the input space from the model's learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks.","In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous \"garbage\" to ensure the trustworthiness of the network.","Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments.","This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network's structure, allowing for both inference and inversion."],"url":"http://arxiv.org/abs/2402.11995v1","category":"cs.LG"}
{"created":"2024-02-19 09:34:44","title":"Measuring the magnetic dipole moment and magnetospheric fluctuations of SXP 18.3 with a Kalman filter","abstract":"The magnetic dipole moment $\\mu$ of an accretion-powered pulsar in magnetocentrifugal equilibrium cannot be inferred uniquely from time-averaged pulse period and aperiodic X-ray flux data, because the radiative efficiency $\\eta_0$ of the accretion is unknown, as are the mass, radius, and distance of the star. The degeneracy associated with the radiative efficiency is circumvented, if fluctuations of the pulse period and aperiodic X-ray flux are tracked with a Kalman filter, whereupon $\\mu$ can be measured uniquely up to the uncertainties in the mass, radius, and distance. Here the Kalman filter analysis is demonstrated successfully in practice for the first time on Rossi X-ray Timing Explorer observations of the X-ray transient SXP 18.3 in the Small Magellanic Cloud, which is monitored regularly. The analysis yields $\\mu = 8.0^{+1.3}_{-1.2} \\, \\times \\, 10^{30} \\, {\\rm G \\, cm^3}$ and $\\eta_0 = 0.04^{+0.02}_{-0.01}$, compared to $\\mu = 5.0^{+1.0}_{-1.0} \\times 10^{30} \\, {\\rm G \\, cm^3}$ as inferred traditionally from time-averaged data assuming $\\eta_0=1$. The analysis also yields time-resolved estimates of two hidden state variables, the mass accretion rate and the Maxwell stress at the disk-magnetosphere boundary. The success of the demonstration confirms that the Kalman filter analysis can be applied in the future to study the magnetic moments and disk-magnetosphere physics of accretion-powered pulsar populations in the Small Magellanic Cloud and elsewhere.","sentences":["The magnetic dipole moment $\\mu$ of an accretion-powered pulsar in magnetocentrifugal equilibrium cannot be inferred uniquely from time-averaged pulse period and aperiodic X-ray flux data, because the radiative efficiency $\\eta_0$ of the accretion is unknown, as are the mass, radius, and distance of the star.","The degeneracy associated with the radiative efficiency is circumvented, if fluctuations of the pulse period and aperiodic X-ray flux are tracked with a Kalman filter, whereupon $\\mu$ can be measured uniquely up to the uncertainties in the mass, radius, and distance.","Here the Kalman filter analysis is demonstrated successfully in practice for the first time on Rossi X-ray Timing Explorer observations of the X-ray transient SXP 18.3 in the Small Magellanic Cloud, which is monitored regularly.","The analysis yields $\\mu = 8.0^{+1.3}_{-1.2} \\, \\times \\, 10^{30} \\, {\\rm G \\, cm^3}$ and $\\eta_0 = 0.04^{+0.02}_{-0.01}$, compared to $\\mu = 5.0^{+1.0}_{-1.0} \\times 10^{30} \\, {\\rm G \\, cm^3}$ as inferred traditionally from time-averaged data assuming $\\eta_0=1$. The analysis also yields time-resolved estimates of two hidden state variables, the mass accretion rate and the Maxwell stress at the disk-magnetosphere boundary.","The success of the demonstration confirms that the Kalman filter analysis can be applied in the future to study the magnetic moments and disk-magnetosphere physics of accretion-powered pulsar populations in the Small Magellanic Cloud and elsewhere."],"url":"http://arxiv.org/abs/2402.11991v1","category":"astro-ph.HE"}
{"created":"2024-02-19 09:04:50","title":"Optimal Design of Climate Disclosure Policies: Transparency versus Externality","abstract":"Does a more transparent climate disclosure policy induce lower emissions? This paper analyzes the welfare consequences of transparency in corporate disclosure regulation in an environment in which regulatory disclosure constitutes the sole avenue for the verification of a firm's emissions. On the one hand, a potential trade-off between disclosure transparency and externality suggests a non-monotonic relationship between them. On the other hand, increased transparency never makes the firm worse off. Consequently, mandating full disclosure is no different from maximizing the firm's private benefit while disregarding the ensuing externality. When the regulator is symmetrically informed about the firm's energy efficiency level, transparency beyond binary disclosure does not lead to welfare improvements. In the presence of information asymmetry, the welfare-maximizing disclosure takes a threshold form: all emissions above the threshold are pooled together, whereas all emissions below are fully disclosed.","sentences":["Does a more transparent climate disclosure policy induce lower emissions?","This paper analyzes the welfare consequences of transparency in corporate disclosure regulation in an environment in which regulatory disclosure constitutes the sole avenue for the verification of a firm's emissions.","On the one hand, a potential trade-off between disclosure transparency and externality suggests a non-monotonic relationship between them.","On the other hand, increased transparency never makes the firm worse off.","Consequently, mandating full disclosure is no different from maximizing the firm's private benefit while disregarding the ensuing externality.","When the regulator is symmetrically informed about the firm's energy efficiency level, transparency beyond binary disclosure does not lead to welfare improvements.","In the presence of information asymmetry, the welfare-maximizing disclosure takes a threshold form: all emissions above the threshold are pooled together, whereas all emissions below are fully disclosed."],"url":"http://arxiv.org/abs/2402.11961v1","category":"econ.TH"}
{"created":"2024-02-19 08:34:24","title":"Asymptotic behavior for twisted traces of self-dual and conjugate self-dual representations of $\\mathrm{GL}_n$","abstract":"In this paper, we study the asymptotic behavior of the sum of twisted traces of self-dual or conjugate self-dual discrete automorphic representations of $\\mathrm{GL}_n$ for the level aspect of principal congruence subgroups under some conditions. Our asymptotic formula is derived from the Arthur twisted trace formula, and it is regarded as a twisted version of limit multiplicity formula on Lie groups. We determine the main terms for the asymptotic behavior under different conditions, and also obtain explicit forms of their Fourier transforms, which correspond to endoscopic lifts from classical groups. Its main application is the self-dual (resp. conjugate self-dual) globalization of local self-dual (resp. conjugate self-dual) representations of $\\mathrm{GL}_n$. We further derive an automorphic density theorem for conjugate self-dual representations of $\\mathrm{GL}_n$.","sentences":["In this paper, we study the asymptotic behavior of the sum of twisted traces of self-dual or conjugate self-dual discrete automorphic representations of $\\mathrm{GL}_n$ for the level aspect of principal congruence subgroups under some conditions.","Our asymptotic formula is derived from the Arthur twisted trace formula, and it is regarded as a twisted version of limit multiplicity formula on Lie groups.","We determine the main terms for the asymptotic behavior under different conditions, and also obtain explicit forms of their Fourier transforms, which correspond to endoscopic lifts from classical groups.","Its main application is the self-dual (resp.","conjugate self-dual) globalization of local self-dual (resp.","conjugate self-dual) representations of $\\mathrm{GL}_n$. We further derive an automorphic density theorem for conjugate self-dual representations of $\\mathrm{GL}_n$."],"url":"http://arxiv.org/abs/2402.11945v1","category":"math.NT"}
{"created":"2024-02-19 08:27:14","title":"CRAP Part II: Clutter Removal with Continuous Acquisitions Under Phase Noise","abstract":"The mitigation of clutter is an important research branch in Integrated Sensing and Communication (ISAC), one of the emerging technologies of future cellular networks. In this work, we extend our previously introduced method Clutter Removal with Acquisitions Under Phase Noise (CRAP) by means to track clutter over time. This is necessary in scenarios that require high reliability but can change dynamically, like safety applications in factory floors. To that end, exponential smoothing is leveraged to process new measurements and previous clutter information in a unique matrix using the singular value decomposition, allowing adaptation to changing environments in an efficient way.We further propose a singular value threshold based on the Marchenko-Pastur distribution to select the meaningful clutter components. Results from both simulations and measurements show that continuously updating the clutter components with new acquisitions according to our proposed algorithm Smoothed CRAP (SCRAP) enables coping with dynamic clutter environments and facilitates the detection of sensing targets.","sentences":["The mitigation of clutter is an important research branch in Integrated Sensing and Communication (ISAC), one of the emerging technologies of future cellular networks.","In this work, we extend our previously introduced method Clutter Removal with Acquisitions Under Phase Noise (CRAP) by means to track clutter over time.","This is necessary in scenarios that require high reliability but can change dynamically, like safety applications in factory floors.","To that end, exponential smoothing is leveraged to process new measurements and previous clutter information in a unique matrix using the singular value decomposition, allowing adaptation to changing environments in an efficient way.","We further propose a singular value threshold based on the Marchenko-Pastur distribution to select the meaningful clutter components.","Results from both simulations and measurements show that continuously updating the clutter components with new acquisitions according to our proposed algorithm Smoothed CRAP (SCRAP) enables coping with dynamic clutter environments and facilitates the detection of sensing targets."],"url":"http://arxiv.org/abs/2402.11939v1","category":"eess.SP"}
{"created":"2024-02-19 08:19:10","title":"Nonlocality enhanced precision in quantum polarimetry via entangled photons","abstract":"We present a nonlocal quantum approach to polarimetry, leveraging the phenomenon of entanglement in photon pairs to enhance the precision in sample property determination. By employing two distinct channels, one containing the sample of interest and the other serving as a reference, we explore the conditions under which the inherent correlation between entangled photons can increase measurement sensitivity. Specifically, we calculate the quantum Fisher information (QFI) and compare the accuracy and sensitivity for the cases of single sample channel versus two channel quantum state tomography measurements. The theoretical results are verified by experimental analysis. Our theoretical and experimental framework demonstrates that the nonlocal strategy enables enhanced precision and accuracy in extracting information about sample characteristics more than the local measurements. Depending on the chosen estimators and noise channels present, theoretical and experimental results show that noise-induced bias decreases the precision for the estimated parameter. Such a quantum-enhanced nonlocal polarimetry holds promise for advancing diverse fields including material science, biomedical imaging, and remote sensing, via high-precision measurements through quantum entanglement.","sentences":["We present a nonlocal quantum approach to polarimetry, leveraging the phenomenon of entanglement in photon pairs to enhance the precision in sample property determination.","By employing two distinct channels, one containing the sample of interest and the other serving as a reference, we explore the conditions under which the inherent correlation between entangled photons can increase measurement sensitivity.","Specifically, we calculate the quantum Fisher information (QFI) and compare the accuracy and sensitivity for the cases of single sample channel versus two channel quantum state tomography measurements.","The theoretical results are verified by experimental analysis.","Our theoretical and experimental framework demonstrates that the nonlocal strategy enables enhanced precision and accuracy in extracting information about sample characteristics more than the local measurements.","Depending on the chosen estimators and noise channels present, theoretical and experimental results show that noise-induced bias decreases the precision for the estimated parameter.","Such a quantum-enhanced nonlocal polarimetry holds promise for advancing diverse fields including material science, biomedical imaging, and remote sensing, via high-precision measurements through quantum entanglement."],"url":"http://arxiv.org/abs/2402.11932v1","category":"quant-ph"}
{"created":"2024-02-19 08:17:13","title":"Separating common from salient patterns with Contrastive Representation Learning","abstract":"Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset. Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations. On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.). In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis. We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize. We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning. Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions. We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis. Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr.","sentences":["Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset.","Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations.","On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.).","In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis.","We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize.","We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning.","Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions.","We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis.","Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr."],"url":"http://arxiv.org/abs/2402.11928v1","category":"cs.CV"}
{"created":"2024-02-19 08:15:19","title":"Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with error based time stepping for hyperbolic conservation laws","abstract":"Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws. This work extends the LWFR scheme to solve conservation laws on curvilinear meshes with adaptive mesh refinement (AMR). The scheme uses a subcell based blending limiter to perform shock capturing and exploits the same subcell structure to obtain admissibility preservation on curvilinear meshes. It is proven that the proposed extension of LWFR scheme to curvilinear grids preserves constant solution (free stream preservation) under the standard metric identities. For curvilinear meshes, linear Fourier stability analysis cannot be used to obtain an optimal CFL number. Thus, an embedded-error based time step computation method is proposed for LWFR method which reduces fine-tuning process required to select a stable CFL number using the wave speed based time step computation. The developments are tested on compressible Euler's equations, validating the blending limiter, admissibility preservation, AMR algorithm, curvilinear meshes and error based time stepping.","sentences":["Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws.","This work extends the LWFR scheme to solve conservation laws on curvilinear meshes with adaptive mesh refinement (AMR).","The scheme uses a subcell based blending limiter to perform shock capturing and exploits the same subcell structure to obtain admissibility preservation on curvilinear meshes.","It is proven that the proposed extension of LWFR scheme to curvilinear grids preserves constant solution (free stream preservation) under the standard metric identities.","For curvilinear meshes, linear Fourier stability analysis cannot be used to obtain an optimal CFL number.","Thus, an embedded-error based time step computation method is proposed for LWFR method which reduces fine-tuning process required to select a stable CFL number using the wave speed based time step computation.","The developments are tested on compressible Euler's equations, validating the blending limiter, admissibility preservation, AMR algorithm, curvilinear meshes and error based time stepping."],"url":"http://arxiv.org/abs/2402.11926v1","category":"math.NA"}
{"created":"2024-02-19 07:46:40","title":"Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation","abstract":"Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \\texttt{RLHF} method without relying on human-annotated preference data.","sentences":["Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem.","In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF.","Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA).","First, we use contrastive prompt pairs to automatically generate preference data.","Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score.","Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score.","In the experimental stage, our DLMA method could surpass the \\texttt{RLHF} method without relying on human-annotated preference data."],"url":"http://arxiv.org/abs/2402.11907v1","category":"cs.CL"}
{"created":"2024-02-19 07:45:17","title":"Learning to Edit: Aligning LLMs with Knowledge Editing","abstract":"Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of \"Teach a man to fish.\" LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE's superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are available at https://github.com/YJiangcm/LTE.","sentences":["Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention.","However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions.","To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of \"Teach a man to fish.\"","LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing.","By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE's superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds.","The data and code are available at https://github.com/YJiangcm/LTE."],"url":"http://arxiv.org/abs/2402.11905v1","category":"cs.CL"}
{"created":"2024-02-19 07:15:59","title":"Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation","abstract":"Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poorly, we design an extending strategy that adjusts the difficulty of the generated samples according to varying cognitive levels. This not only makes our evaluation more systematic, but also, with a balanced difficulty, even discern model capabilities better at fine-grained levels.","sentences":["Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges.","On one hand, the data leakage issue cause over-estimation on existing benchmarks.","On the other hand, periodically curating datasets manually is costly.","In this paper, we propose to automate dataset updates for reliable and timely evaluation.","The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues.","In specific, we propose two strategies with systematically verification.","First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset.","Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases.","Second, for the cases that mimicking dataset works poorly, we design an extending strategy that adjusts the difficulty of the generated samples according to varying cognitive levels.","This not only makes our evaluation more systematic, but also, with a balanced difficulty, even discern model capabilities better at fine-grained levels."],"url":"http://arxiv.org/abs/2402.11894v1","category":"cs.CL"}
{"created":"2024-02-19 07:01:10","title":"Revisiting Knowledge Distillation for Autoregressive Language Models","abstract":"Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.","sentences":["Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model.","However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student.","In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation.","Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD.","The core of ATKD is to reduce rote learning and make teaching more diverse and flexible.","Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes.","More encouragingly, ATKD can improve the student model generalization effectively."],"url":"http://arxiv.org/abs/2402.11890v1","category":"cs.CL"}
{"created":"2024-02-19 06:37:54","title":"Revision of the GeV $\u03b3$-ray Emission in the Region of HESS J1813-178 with Fermi-LAT","abstract":"HESS J1813-178 is one of the brightest and most compact TeV $\\gamma$-ray sources, and whether its $\\gamma$-ray emission is associated with supernova remnant (SNR), pulsar wind nebula (PWN) or young stellar cluster (YSC) is still under debate. By analysing the GeV $\\gamma$-ray data in the field of HESS J1813-178 using 14 years of PASS 8 data recorded by the Fermi Large Area Telescope (Fermi-LAT), we report the discovery of three different sources with different spectra in this region. The hard source with a power law spectral index of 2.11 $\\pm$ 0.08 has a small size extension, which is spatially and spectrally coincident with the TeV $\\gamma$-ray emission from HESS J1813-178. CO observations display the dense molecular clouds surrounding HESS J1813-178 in the velocity range of 45-60 km s$^{\\rm -1}$. The possible origins of the $\\gamma$-ray emission from HESS J1813-178 are discussed, including SNR G12.82-0.02, the PWN driven by the energetic X-ray pulsar PSR J1813-1749, and YSC Cl 1813-178. However, none of them can be ruled out clearly. Note that the maximum energy of protons in the hadronic model should exceed a few hundred TeV, which makes HESS J1813-178 to be a promising PeVatron. The detailed LHAASO data analysis about the morphology and spectrum would be helpful to investigate the origin of the $\\gamma$-ray emission in this region and test its PeVatron nature.","sentences":["HESS J1813-178 is one of the brightest and most compact TeV $\\gamma$-ray sources, and whether its $\\gamma$-ray emission is associated with supernova remnant (SNR), pulsar wind nebula (PWN) or young stellar cluster (YSC) is still under debate.","By analysing the GeV $\\gamma$-ray data in the field of HESS J1813-178 using 14 years of PASS 8 data recorded by the Fermi Large Area Telescope (Fermi-LAT), we report the discovery of three different sources with different spectra in this region.","The hard source with a power law spectral index of 2.11 $\\pm$ 0.08 has a small size extension, which is spatially and spectrally coincident with the TeV $\\gamma$-ray emission from HESS J1813-178.","CO observations display the dense molecular clouds surrounding HESS J1813-178 in the velocity range of 45-60 km s$^{\\rm -1}$.","The possible origins of the $\\gamma$-ray emission from HESS J1813-178 are discussed, including SNR G12.82-0.02, the PWN driven by the energetic X-ray pulsar PSR J1813-1749, and YSC Cl 1813-178.","However, none of them can be ruled out clearly.","Note that the maximum energy of protons in the hadronic model should exceed a few hundred TeV, which makes HESS J1813-178 to be a promising PeVatron.","The detailed LHAASO data analysis about the morphology and spectrum would be helpful to investigate the origin of the $\\gamma$-ray emission in this region and test its PeVatron nature."],"url":"http://arxiv.org/abs/2402.11880v1","category":"astro-ph.HE"}
{"created":"2024-02-19 06:24:25","title":"Recent Extensions of the ZKCM Library for Parallel and Accurate MPS Simulation of Quantum Circuits","abstract":"A C++ library ZKCM and its extension library ZKCM_QC have been developed since 2011 for multiple-precision matrix computation and accurate matrix-product-state (MPS) quantum circuit simulation, respectively. In this report, a recent progress in the extensions of these libraries is described, which are mainly for parallel processing with the OpenMP and CUDA frameworks.","sentences":["A C++ library ZKCM and its extension library ZKCM_QC have been developed since 2011 for multiple-precision matrix computation and accurate matrix-product-state (MPS) quantum circuit simulation, respectively.","In this report, a recent progress in the extensions of these libraries is described, which are mainly for parallel processing with the OpenMP and CUDA frameworks."],"url":"http://arxiv.org/abs/2402.11868v1","category":"physics.comp-ph"}
{"created":"2024-02-19 06:11:28","title":"How Interpretable are Reasoning Explanations from Prompting Large Language Models?","abstract":"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/wj210/CoT_interpretability","sentences":["Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks.","Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience.","Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness.","We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks.","Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation.","In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability.","Code is available at https://github.com/wj210/CoT_interpretability"],"url":"http://arxiv.org/abs/2402.11863v1","category":"cs.CL"}
{"created":"2024-02-19 06:03:09","title":"The weighted projective superspace with weights $+1, -1$ and an analog of the Fubini--Study form","abstract":"As a by-product of our work on super Pl\\\"{u}cker embedding, we came to the notion of a weighted projective superspace $P_{+1,-1}(V\\oplus W)$ with weights $+1,-1$. The construction is not in itself super and makes sense in ordinary (purely even) framework. Unlike the familiar weighted projective spaces with positive weights, the (super)space $P_{+1,-1}(V\\oplus W)$ is a smooth (super)manifold. We describe its structure and show that it possesses an analog of the Fubini--Study form.","sentences":["As a by-product of our work on super Pl\\\"{u}cker embedding, we came to the notion of a weighted projective superspace $P_{+1,-1}(V\\oplus W)$ with weights $+1,-1$.","The construction is not in itself super and makes sense in ordinary (purely even) framework.","Unlike the familiar weighted projective spaces with positive weights, the (super)space $P_{+1,-1}(V\\oplus W)$ is a smooth (super)manifold.","We describe its structure and show that it possesses an analog of the Fubini--Study form."],"url":"http://arxiv.org/abs/2402.11860v1","category":"math.DG"}
{"created":"2024-02-19 06:00:35","title":"Stochastic Hessian Fitting on Lie Group","abstract":"This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely on fast, robust and accurate Hessian estimations.","sentences":["This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products.","A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis.","Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups.","The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group.","To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic.","These findings are useful for stochastic second order optimizations that rely on fast, robust and accurate Hessian estimations."],"url":"http://arxiv.org/abs/2402.11858v1","category":"stat.ML"}
{"created":"2024-02-19 05:00:07","title":"Self-Guided Robust Graph Structure Refinement","abstract":"Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels. Our code is available at https://github.com/yeonjun-in/torch-SG-GSR.","sentences":["Recent studies have revealed that GNNs are vulnerable to adversarial attacks.","To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information.","However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios.","In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself.","Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution.","Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels.","Our code is available at https://github.com/yeonjun-in/torch-SG-GSR."],"url":"http://arxiv.org/abs/2402.11837v1","category":"cs.LG"}
{"created":"2024-02-19 04:44:33","title":"Maximum Likelihood Quantum Error Mitigation for Algorithms with a Single Correct Output","abstract":"Quantum error mitigation is an important technique to reduce the impact of noise in quantum computers. With more and more qubits being supported on quantum computers, there are two emerging fundamental challenges. First, the number of shots required for quantum algorithms with large numbers of qubits needs to increase in order to obtain a meaningful distribution or expected value of an observable. Second, although steady progress has been made in improving the fidelity of each qubit, circuits with a large number of qubits are likely to produce erroneous results. This low-shot, high-noise regime calls for highly scalable error mitigation techniques. In this paper, we propose a simple and effective mitigation scheme, qubit-wise majority vote, for quantum algorithms with a single correct output. We show that our scheme produces the maximum likelihood (ML) estimate under certain assumptions, and bound the number of shots required. Our experimental results on real quantum devices confirm that our proposed approach requires fewer shots than existing ones, and can sometimes recover the correct answers even when they are not observed from the measurement results.","sentences":["Quantum error mitigation is an important technique to reduce the impact of noise in quantum computers.","With more and more qubits being supported on quantum computers, there are two emerging fundamental challenges.","First, the number of shots required for quantum algorithms with large numbers of qubits needs to increase in order to obtain a meaningful distribution or expected value of an observable.","Second, although steady progress has been made in improving the fidelity of each qubit, circuits with a large number of qubits are likely to produce erroneous results.","This low-shot, high-noise regime calls for highly scalable error mitigation techniques.","In this paper, we propose a simple and effective mitigation scheme, qubit-wise majority vote, for quantum algorithms with a single correct output.","We show that our scheme produces the maximum likelihood (ML) estimate under certain assumptions, and bound the number of shots required.","Our experimental results on real quantum devices confirm that our proposed approach requires fewer shots than existing ones, and can sometimes recover the correct answers even when they are not observed from the measurement results."],"url":"http://arxiv.org/abs/2402.11830v1","category":"quant-ph"}
{"created":"2024-02-19 04:41:43","title":"Scaling Limit of Asymptotically-free Self-interacting Random Walks to Brownian Motion Perturbed at Extrema","abstract":"We show convergence of a family of one-dimensional self-interacting random walks to Brownian motion perturbed at extrema under the diffusive scaling. This completes the functional limit theorem in [KMP23] for the asymptotically free case when $0<p \\leq \\frac{1}{2}$. The approach is to approximate the total drift experienced by the walker via analyzing directed edge local times, described by the branching-like processes. The analysis depends on the diffusion approximation of the branching-like processes obtained in the Ray-Knight type framework.","sentences":["We show convergence of a family of one-dimensional self-interacting random walks to Brownian motion perturbed at extrema under the diffusive scaling.","This completes the functional limit theorem in [KMP23] for the asymptotically free case when $0<p \\leq \\frac{1}{2}$.","The approach is to approximate the total drift experienced by the walker via analyzing directed edge local times, described by the branching-like processes.","The analysis depends on the diffusion approximation of the branching-like processes obtained in the Ray-Knight type framework."],"url":"http://arxiv.org/abs/2402.11828v1","category":"math.PR"}
{"created":"2024-02-19 03:27:06","title":"Density estimation for elliptic PDE with random input by preintegration and quasi-Monte Carlo methods","abstract":"In this paper, we apply quasi-Monte Carlo (QMC) methods with an initial preintegration step to estimate cumulative distribution functions and probability density functions in uncertainty quantification (UQ). The distribution and density functions correspond to a quantity of interest involving the solution to an elliptic partial differential equation (PDE) with a lognormally distributed coefficient and a normally distributed source term. There is extensive previous work on using QMC to compute expected values in UQ, which have proven very successful in tackling a range of different PDE problems. However, the use of QMC for density estimation applied to UQ problems will be explored here for the first time. Density estimation presents a more difficult challenge compared to computing the expected value due to discontinuities present in the integral formulations of both the distribution and density. Our strategy is to use preintegration to eliminate the discontinuity by integrating out a carefully selected random parameter, so that QMC can be used to approximate the remaining integral. First, we establish regularity results for the PDE quantity of interest that are required for smoothing by preintegration to be effective. We then show that an $N$-point lattice rule can be constructed for the integrands corresponding to the distribution and density, such that after preintegration the QMC error is of order $\\mathcal{O}(N^{-1+\\epsilon})$ for arbitrarily small $\\epsilon>0$. This is the same rate achieved for computing the expected value of the quantity of interest. Numerical results are presented to reaffirm our theory.","sentences":["In this paper, we apply quasi-Monte Carlo (QMC) methods with an initial preintegration step to estimate cumulative distribution functions and probability density functions in uncertainty quantification (UQ).","The distribution and density functions correspond to a quantity of interest involving the solution to an elliptic partial differential equation (PDE) with a lognormally distributed coefficient and a normally distributed source term.","There is extensive previous work on using QMC to compute expected values in UQ, which have proven very successful in tackling a range of different PDE problems.","However, the use of QMC for density estimation applied to UQ problems will be explored here for the first time.","Density estimation presents a more difficult challenge compared to computing the expected value due to discontinuities present in the integral formulations of both the distribution and density.","Our strategy is to use preintegration to eliminate the discontinuity by integrating out a carefully selected random parameter, so that QMC can be used to approximate the remaining integral.","First, we establish regularity results for the PDE quantity of interest that are required for smoothing by preintegration to be effective.","We then show that an $N$-point lattice rule can be constructed for the integrands corresponding to the distribution and density, such that after preintegration the QMC error is of order $\\mathcal{O}(N^{-1+\\epsilon})$ for arbitrarily small $\\epsilon>0$. This is the same rate achieved for computing the expected value of the quantity of interest.","Numerical results are presented to reaffirm our theory."],"url":"http://arxiv.org/abs/2402.11807v1","category":"math.NA"}
{"created":"2024-02-19 02:32:45","title":"Statistical Test for Generated Hypotheses by Diffusion Models","abstract":"The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. We show the theoretical validity of the proposed statistical test and its effectiveness through numerical experiments on synthetic and brain image datasets.","sentences":["The enhanced performance of AI has accelerated its integration into scientific research.","In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields.","However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial.","In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability.","The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model.","Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate.","We show the theoretical validity of the proposed statistical test and its effectiveness through numerical experiments on synthetic and brain image datasets."],"url":"http://arxiv.org/abs/2402.11789v1","category":"stat.ML"}
{"created":"2024-02-19 01:45:32","title":"Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots","abstract":"Path planning for multiple non-holonomic robots in continuous domains constitutes a difficult robotics challenge with many applications. Despite significant recent progress on the topic, computationally efficient and high-quality solutions are lacking, especially in lifelong settings where robots must continuously take on new tasks. In this work, we make it possible to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot planning in discrete domains to the motion planning of multiple Ackerman (car-like) robots in lifelong settings, yielding high-performance centralized and decentralized planners. Our planners compute trajectories that allow the robots to reach precise $SE(2)$ goal poses. The effectiveness of our methods is thoroughly evaluated and confirmed using both simulation and real-world experiments.","sentences":["Path planning for multiple non-holonomic robots in continuous domains constitutes a difficult robotics challenge with many applications.","Despite significant recent progress on the topic, computationally efficient and high-quality solutions are lacking, especially in lifelong settings where robots must continuously take on new tasks.","In this work, we make it possible to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot planning in discrete domains to the motion planning of multiple Ackerman (car-like) robots in lifelong settings, yielding high-performance centralized and decentralized planners.","Our planners compute trajectories that allow the robots to reach precise $SE(2)$ goal poses.","The effectiveness of our methods is thoroughly evaluated and confirmed using both simulation and real-world experiments."],"url":"http://arxiv.org/abs/2402.11767v1","category":"cs.RO"}
{"created":"2024-02-19 01:33:57","title":"Guide to Numerical Experiments on Elections in Computational Social Choice","abstract":"We analyze how numerical experiments regarding elections were conducted within the computational social choice literature (focusing on papers published in the IJCAI, AAAI, and AAMAS conferences). We analyze the sizes of the studied elections and the methods used for generating preference data, thereby making previously hidden standards and practices explicit. In particular, we survey a number of statistical cultures for generating elections and their commonly used parameters.","sentences":["We analyze how numerical experiments regarding elections were conducted within the computational social choice literature (focusing on papers published in the IJCAI, AAAI, and AAMAS conferences).","We analyze the sizes of the studied elections and the methods used for generating preference data, thereby making previously hidden standards and practices explicit.","In particular, we survey a number of statistical cultures for generating elections and their commonly used parameters."],"url":"http://arxiv.org/abs/2402.11765v1","category":"cs.GT"}
{"created":"2024-02-19 01:17:52","title":"Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation","abstract":"Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at https://github.com/scailab/paser .","sentences":["Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation.","This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets.","Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference.","While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings.","In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved.","However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations.","To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures.","Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models.","Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance.","On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines.","We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models.","Code and data are available at https://github.com/scailab/paser ."],"url":"http://arxiv.org/abs/2402.11760v1","category":"cs.LG"}
{"created":"2024-02-19 01:04:22","title":"MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs","abstract":"Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found https://anonymous.4open.science/r/LLM_Uncertainity-309B.","sentences":["Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks.","However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments.","Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability.","Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring.","In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods.","MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question.","We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance.","We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs.","Lastly, we validate the efficacy of MARS on a Medical QA dataset.","Code can be found https://anonymous.4open.science/r/LLM_Uncertainity-309B."],"url":"http://arxiv.org/abs/2402.11756v1","category":"cs.CL"}
{"created":"2024-02-19 00:21:07","title":"Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation","abstract":"Foundation models have shown superior performance for speech emotion recognition (SER). However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting. This paper investigates parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes. The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters. Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions. Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in enhancing the performance on both the source and target domains.","sentences":["Foundation models have shown superior performance for speech emotion recognition (SER).","However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting.","This paper investigates parameter-efficient finetuning (PEFT) for SER.","Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes.","The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters.","Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions.","Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in enhancing the performance on both the source and target domains."],"url":"http://arxiv.org/abs/2402.11747v1","category":"eess.AS"}
{"created":"2024-02-19 00:14:37","title":"From Large to Small $\\mathcal{N}=(4,4)$ Superconformal Surface Defects in Holographic 6d SCFTs","abstract":"Two-dimensional (2d) $\\mathcal{N}=(4,4)$ Lie superalgebras can be either ''small'' or ''large'', meaning their R-symmetry is either $\\mathfrak{so}(4)$ or $\\mathfrak{so}(4) \\oplus \\mathfrak{so}(4)$, respectively. Both cases admit a superconformal extension and fit into the one-parameter family $\\mathfrak{d}\\left(2,1;\\gamma\\right)\\oplus \\mathfrak{d}\\left(2,1;\\gamma\\right)$, with parameter $\\gamma \\in (-\\infty,\\infty)$. The large algebra corresponds to generic values of $\\gamma$, while the small case corresponds to a degeneration limit with $\\gamma \\to -\\infty$. In 11d supergravity, we study known solutions with superisometry algebra $\\mathfrak{d}\\left(2,1;\\gamma\\right)\\oplus \\mathfrak{d}\\left(2,1;\\gamma\\right)$ that are asymptotically locally AdS$_7 \\times S^4$. These solutions are holographically dual to the 6d maximally superconformal field theory with 2d superconformal defects invariant under $\\mathfrak{d}\\left(2,1;\\gamma\\right)\\oplus \\mathfrak{d}\\left(2,1;\\gamma\\right)$. We show that a limit of these solutions, in which $\\gamma \\to -\\infty$, reproduces another known class of solutions, holographically dual to \\textit{small} $\\mathcal{N}=(4,4)$ superconformal defects. We then use this limit to generate new small $\\mathcal{N}=(4,4)$ solutions with finite Ricci scalar, in contrast to the known small $\\mathcal{N}=(4,4)$ solutions. We then use holography to compute the entanglement entropy of a spherical region centered on these small $\\mathcal{N}=(4,4)$ defects, which provides a linear combination of defect Weyl anomaly coefficients that characterizes the number of defect-localized degrees of freedom. We also comment on the generalization of our results to include $\\mathcal{N}=(0,4)$ surface defects through orbifolding.","sentences":["Two-dimensional (2d) $\\mathcal{N}=(4,4)$ Lie superalgebras can be either ''small'' or ''large'', meaning their R-symmetry is either $\\mathfrak{so}(4)$ or $\\mathfrak{so}(4) \\oplus \\mathfrak{so}(4)$, respectively.","Both cases admit a superconformal extension and fit into the one-parameter family $\\mathfrak{d}\\left(2,1;\\gamma\\right)\\oplus \\mathfrak{d}\\left(2,1;\\gamma\\right)$, with parameter $\\gamma \\in (-\\infty,\\infty)$.","The large algebra corresponds to generic values of $\\gamma$, while the small case corresponds to a degeneration limit with $\\gamma \\to -\\infty$.","In 11d supergravity, we study known solutions with superisometry algebra $\\mathfrak{d}\\left(2,1;\\gamma\\right)\\oplus \\mathfrak{d}\\left(2,1;\\gamma\\right)$ that are asymptotically locally AdS$_7 \\times S^4$. These solutions are holographically dual to the 6d maximally superconformal field theory with 2d superconformal defects invariant under $\\mathfrak{d}\\left(2,1;\\gamma\\right)\\oplus \\mathfrak{d}\\left(2,1;\\gamma\\right)$.","We show that a limit of these solutions, in which $\\gamma \\to -\\infty$, reproduces another known class of solutions, holographically dual to \\textit{small} $\\mathcal{N}=(4,4)$ superconformal defects.","We then use this limit to generate new small $\\mathcal{N}=(4,4)$ solutions with finite Ricci scalar, in contrast to the known small $\\mathcal{N}=(4,4)$ solutions.","We then use holography to compute the entanglement entropy of a spherical region centered on these small $\\mathcal{N}=(4,4)$ defects, which provides a linear combination of defect Weyl anomaly coefficients that characterizes the number of defect-localized degrees of freedom.","We also comment on the generalization of our results to include $\\mathcal{N}=(0,4)$ surface defects through orbifolding."],"url":"http://arxiv.org/abs/2402.11745v1","category":"hep-th"}
{"created":"2024-02-19 00:03:42","title":"Hybrid Online-Offline Learning for Task Offloading in Mobile Edge Computing Systems","abstract":"We consider a multi-user multi-server mobile edge computing (MEC) system, in which users arrive on a network randomly over time and generate computation tasks, which will be computed either locally on their own computing devices or be offloaded to one of the MEC servers. Under such a dynamic network environment, we propose a novel task offloading policy based on hybrid online-offline learning, which can efficiently reduce the overall computation delay and energy consumption only with information available at nearest MEC servers from each user. We provide a practical signaling and learning framework that can train deep neural networks for both online and offline learning and can adjust its offloading policy based on the queuing status of each MEC server and network dynamics. Numerical results demonstrate that the proposed scheme significantly reduces the average computation delay for a broad class of network environments compared to the conventional offloading methods. It is further shown that the proposed hybrid online-offline learning framework can be extended to a general cost function reflecting both delay and energy-dependent metrics.","sentences":["We consider a multi-user multi-server mobile edge computing (MEC) system, in which users arrive on a network randomly over time and generate computation tasks, which will be computed either locally on their own computing devices or be offloaded to one of the MEC servers.","Under such a dynamic network environment, we propose a novel task offloading policy based on hybrid online-offline learning, which can efficiently reduce the overall computation delay and energy consumption only with information available at nearest MEC servers from each user.","We provide a practical signaling and learning framework that can train deep neural networks for both online and offline learning and can adjust its offloading policy based on the queuing status of each MEC server and network dynamics.","Numerical results demonstrate that the proposed scheme significantly reduces the average computation delay for a broad class of network environments compared to the conventional offloading methods.","It is further shown that the proposed hybrid online-offline learning framework can be extended to a general cost function reflecting both delay and energy-dependent metrics."],"url":"http://arxiv.org/abs/2402.11743v1","category":"eess.SP"}
{"created":"2024-02-18 23:49:18","title":"A Transition System Abstraction Framework for Neural Network Dynamical System Models","abstract":"This paper proposes a transition system abstraction framework for neural network dynamical system models to enhance the model interpretability, with applications to complex dynamical systems such as human behavior learning and verification. To begin with, the localized working zone will be segmented into multiple localized partitions under the data-driven Maximum Entropy (ME) partitioning method. Then, the transition matrix will be obtained based on the set-valued reachability analysis of neural networks. Finally, applications to human handwriting dynamics learning and verification are given to validate our proposed abstraction framework, which demonstrates the advantages of enhancing the interpretability of the black-box model, i.e., our proposed framework is able to abstract a data-driven neural network model into a transition system, making the neural network model interpretable through verifying specifications described in Computational Tree Logic (CTL) languages.","sentences":["This paper proposes a transition system abstraction framework for neural network dynamical system models to enhance the model interpretability, with applications to complex dynamical systems such as human behavior learning and verification.","To begin with, the localized working zone will be segmented into multiple localized partitions under the data-driven Maximum Entropy (ME) partitioning method.","Then, the transition matrix will be obtained based on the set-valued reachability analysis of neural networks.","Finally, applications to human handwriting dynamics learning and verification are given to validate our proposed abstraction framework, which demonstrates the advantages of enhancing the interpretability of the black-box model, i.e., our proposed framework is able to abstract a data-driven neural network model into a transition system, making the neural network model interpretable through verifying specifications described in Computational Tree Logic (CTL) languages."],"url":"http://arxiv.org/abs/2402.11739v1","category":"eess.SY"}
{"created":"2024-02-18 22:55:26","title":"Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis","abstract":"In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism\". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.","sentences":["In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies.","To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain.","We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches.","Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism\".","Furthermore, we observed the dependence of earnings surprise and return on our optimism measure.","Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face."],"url":"http://arxiv.org/abs/2402.11728v1","category":"cs.CL"}
{"created":"2024-02-18 22:27:42","title":"Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models","abstract":"Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted. Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms.","sentences":["Advances in language modeling have paved the way for novel human-AI co-writing experiences.","This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process.","Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding).","Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time).","While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users.","No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted.","Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms."],"url":"http://arxiv.org/abs/2402.11723v1","category":"cs.HC"}
{"created":"2024-02-18 22:11:55","title":"Double-$Q$ and quadruple-$Q$ instabilities at low-symmetric ordering wave vectors under tetragonal symmetry","abstract":"Multiple-$Q$ states are expressed as a superposition of spin density waves at multiple ordering wave vectors, which results in unconventional complicated spin textures, such as skyrmion, hedgehog, and vortex. We investigate the multiple-$Q$ instability by focusing on the low-symmetric ordering wave vectors in momentum space. By systematically performing the simulated annealing for effective spin models with various ordering wave vectors on a two-dimensional square lattice, we classify the magnetic phase diagram into four types according to the position of the ordering wave vectors. Three out of four cases lead to a plethora of isotropic multiple-$Q$ instabilities yielding collinear, coplanar, and noncoplanar double-$Q$ and quadruple-$Q$ magnetic phases, while the remaining case leads to an anisotropic double-$Q$ instability when the multiple-spin interaction is introduced. Our results indicate that exotic multiple-$Q$ phases distinct from the skyrmion crystal phase are expected when the ordering wave vectors lie on the low-symmetric positions in the Brillouin zone.","sentences":["Multiple-$Q$ states are expressed as a superposition of spin density waves at multiple ordering wave vectors, which results in unconventional complicated spin textures, such as skyrmion, hedgehog, and vortex.","We investigate the multiple-$Q$ instability by focusing on the low-symmetric ordering wave vectors in momentum space.","By systematically performing the simulated annealing for effective spin models with various ordering wave vectors on a two-dimensional square lattice, we classify the magnetic phase diagram into four types according to the position of the ordering wave vectors.","Three out of four cases lead to a plethora of isotropic multiple-$Q$ instabilities yielding collinear, coplanar, and noncoplanar double-$Q$ and quadruple-$Q$ magnetic phases, while the remaining case leads to an anisotropic double-$Q$ instability when the multiple-spin interaction is introduced.","Our results indicate that exotic multiple-$Q$ phases distinct from the skyrmion crystal phase are expected when the ordering wave vectors lie on the low-symmetric positions in the Brillouin zone."],"url":"http://arxiv.org/abs/2402.11721v1","category":"cond-mat.str-el"}
{"created":"2024-02-18 21:29:56","title":"General characterisation of Hamiltonians generating velocity-independent forces","abstract":"Dynamics generated from Hamiltonians enjoy potential pathways to quantisation, but standard Hamiltonians are only capable of generating conservative forces. Classes of Hamiltonians have been proposed in Berry et al. capable of generating non-conservative velocity-independent forces. Such Hamiltonians have been classified in the past, under the strict assumption that they are polynomial in momentum. This assumption is relaxed here to analyticity. In doing so, broader classes of Hamiltonians are discovered.   By considering the Hamiltonian as a function of state space without introducing the Lagrangian and constructing a metric-like tensor, we develop strong general constraints on Hamiltonians generating velocity-independent forces and exhibit a surprising dichotomy between classes of such Hamiltonians. These results are applicable to any spatial domain of any dimension admitting well-defined Hamiltonian dynamics. As an example application, we apply these constraints to classify all Hamiltonian velocity-independent forces in two spatial dimensions, as well as all such Hamiltonians which do not generate an isotropic simple harmonic motion. The case of one spatial dimension is also discussed for the sake of completeness.","sentences":["Dynamics generated from Hamiltonians enjoy potential pathways to quantisation, but standard Hamiltonians are only capable of generating conservative forces.","Classes of Hamiltonians have been proposed in Berry et al. capable of generating non-conservative velocity-independent forces.","Such Hamiltonians have been classified in the past, under the strict assumption that they are polynomial in momentum.","This assumption is relaxed here to analyticity.","In doing so, broader classes of Hamiltonians are discovered.   ","By considering the Hamiltonian as a function of state space without introducing the Lagrangian and constructing a metric-like tensor, we develop strong general constraints on Hamiltonians generating velocity-independent forces and exhibit a surprising dichotomy between classes of such Hamiltonians.","These results are applicable to any spatial domain of any dimension admitting well-defined Hamiltonian dynamics.","As an example application, we apply these constraints to classify all Hamiltonian velocity-independent forces in two spatial dimensions, as well as all such Hamiltonians which do not generate an isotropic simple harmonic motion.","The case of one spatial dimension is also discussed for the sake of completeness."],"url":"http://arxiv.org/abs/2402.11714v1","category":"math-ph"}
{"created":"2024-02-18 21:29:50","title":"Existence of an optimal shape for the first eigenvalue of polyharmonic operators","abstract":"We prove the existence of an open set minimizing the first eigenvalue of the Dirichlet polylaplacian of order $m\\geq1$ under volume constraint. Moreover, the corresponding eigenfunction is shown to enjoy $C^{m-1,\\alpha}$ H\\\"older regularity. This is performed for dimension $2\\leq d\\leq 4m$. In particular, our analysis answers the question of the existence of an optimal shape for the clamped plate up to dimension $8$.","sentences":["We prove the existence of an open set minimizing the first eigenvalue of the Dirichlet polylaplacian of order $m\\geq1$ under volume constraint.","Moreover, the corresponding eigenfunction is shown to enjoy $C^{m-1,\\alpha}$ H\\\"older regularity.","This is performed for dimension $2\\leq d\\leq 4m$. In particular, our analysis answers the question of the existence of an optimal shape for the clamped plate up to dimension $8$."],"url":"http://arxiv.org/abs/2402.11713v1","category":"math.AP"}
{"created":"2024-02-18 21:28:06","title":"Modelling Political Coalition Negotiations Using LLM-based Agents","abstract":"Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.","sentences":["Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties.","Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data.","In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents.","We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries.","This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation.","Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes.","We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling."],"url":"http://arxiv.org/abs/2402.11712v1","category":"cs.CL"}
{"created":"2024-02-18 21:20:33","title":"A Note on Bias to Complete","abstract":"Minimizing social bias strengthens societal bonds, promoting shared understanding and better decision-making. We revisit the definition of bias by discovering new bias types (e.g., societal status) in dynamic environments and describe them relative to context, such as culture, region, time, and personal background. Our framework includes eight hypotheses about bias and a minimizing bias strategy for each assumption as well as five methods as proposed solutions in LLM. The realization of the framework is yet to be completed.","sentences":["Minimizing social bias strengthens societal bonds, promoting shared understanding and better decision-making.","We revisit the definition of bias by discovering new bias types (e.g., societal status) in dynamic environments and describe them relative to context, such as culture, region, time, and personal background.","Our framework includes eight hypotheses about bias and a minimizing bias strategy for each assumption as well as five methods as proposed solutions in LLM.","The realization of the framework is yet to be completed."],"url":"http://arxiv.org/abs/2402.11710v1","category":"cs.CL"}
{"created":"2024-02-18 20:01:55","title":"5G Cellular -- An Energy Efficiency Perspective","abstract":"While the 5G technology of cellular communications promises great capacity and coverage to access information anywhere and anytime, it is feared to have huge power consumption. Significant research been has been directed towards solving this problem which exists both on the subscribers side as well as the operators side. There have been efforts like predicting traffic, modifying the physical layer etc. towards making the 5G technology more energy efficient. The aim of this study is to see the technology enablers for 5G from an energy efficiency perspective. Efforts will be made to point out specific areas in 5G cellular where improvements or modifications could make 5G cellular more energy efficient.","sentences":["While the 5G technology of cellular communications promises great capacity and coverage to access information anywhere and anytime, it is feared to have huge power consumption.","Significant research been has been directed towards solving this problem which exists both on the subscribers side as well as the operators side.","There have been efforts like predicting traffic, modifying the physical layer etc. towards making the 5G technology more energy efficient.","The aim of this study is to see the technology enablers for 5G from an energy efficiency perspective.","Efforts will be made to point out specific areas in 5G cellular where improvements or modifications could make 5G cellular more energy efficient."],"url":"http://arxiv.org/abs/2402.11698v1","category":"cs.NI"}
{"created":"2024-02-18 19:59:59","title":"Apollonian carpets and the boundary of the Kahler cone of a hyperkahler manifold","abstract":"The ample cone of a compact Kahler $n$-manifold $M$ is the intersection of its Kahler cone and the real subspace generated by integer (1,1)-classes. Its isotropic boundary is the set of all points $\\eta$ on its boundary such that $\\int_M \\eta^n=0$. We are interested in the relation between the shape of the isotropic boundary of the ample cone of a hyperkahler manifold and the dynamics of its holomorphic automorphism group $G$. In this case, the projectivization of the ample cone is realized as an open, locally polyhedral subset in a hyperbolic space $H$. The isotropic boundary $S$ is realized as a subset of the hyperbolic boundary (the absolute) $A$ of $H$, which is naturally identified with a Euclidean sphere. It is clear that the isotropic boundary $S$ contains the limit set of $G$ acting on its ample cone. We prove that, conversely, all irrational points on $S$ belong to the limit set. Using a result of N. Shah about limiting distributions of curves under geodesic flow on hyperbolic manifolds, we prove that every real analytic curve in $S$ is contained in a geodesic sphere in $S$,and in presence of such curves the limit set is the closure of the union of these geodesic spheres. We study the geometry of such fractal sets, called Apollonian carpets, and establish the link between the Apollonian carpet and the structure of the automorphism group.","sentences":["The ample cone of a compact Kahler $n$-manifold $M$ is the intersection of its Kahler cone and the real subspace generated by integer (1,1)-classes.","Its isotropic boundary is the set of all points $\\eta$ on its boundary such that $\\int_M \\eta^n=0$.","We are interested in the relation between the shape of the isotropic boundary of the ample cone of a hyperkahler manifold and the dynamics of its holomorphic automorphism group $G$.","In this case, the projectivization of the ample cone is realized as an open, locally polyhedral subset in a hyperbolic space $H$.","The isotropic boundary $S$ is realized as a subset of the hyperbolic boundary (the absolute) $A$ of $H$, which is naturally identified with a Euclidean sphere.","It is clear that the isotropic boundary $S$ contains the limit set of $G$ acting on its ample cone.","We prove that, conversely, all irrational points on $S$ belong to the limit set.","Using a result of N. Shah about limiting distributions of curves under geodesic flow on hyperbolic manifolds, we prove that every real analytic curve in $S$ is contained in a geodesic sphere in $S$,and in presence of such curves the limit set is the closure of the union of these geodesic spheres.","We study the geometry of such fractal sets, called Apollonian carpets, and establish the link between the Apollonian carpet and the structure of the automorphism group."],"url":"http://arxiv.org/abs/2402.11697v1","category":"math.AG"}
{"created":"2024-02-18 19:44:31","title":"Identifying circular orders for blobs in phylogenetic networks","abstract":"Interest in the inference of evolutionary networks relating species or populations has grown with the increasing recognition of the importance of hybridization, gene flow and admixture, and the availability of large-scale genomic data. However, what network features may be validly inferred from various data types under different models remains poorly understood. Previous work has largely focused on level-1 networks, in which reticulation events are well separated, and on a general network's tree of blobs, the tree obtained by contracting every blob to a node. An open question is the identifiability of the topology of a blob of unknown level. We consider the identifiability of the circular order in which subnetworks attach to a blob, first proving that this order is well-defined for outer-labeled planar blobs. For this class of blobs, we show that the circular order information from 4-taxon subnetworks identifies the full circular order of the blob. Similarly, the circular order from 3-taxon rooted subnetworks identifies the full circular order of a rooted blob. We then show that subnetwork circular information is identifiable from certain data types and evolutionary models. This provides a general positive result for high-level networks, on the identifiability of the ordering in which taxon blocks attach to blobs in outer-labeled planar networks. Finally, we give examples of blobs with different internal structures which cannot be distinguished under many models and data types.","sentences":["Interest in the inference of evolutionary networks relating species or populations has grown with the increasing recognition of the importance of hybridization, gene flow and admixture, and the availability of large-scale genomic data.","However, what network features may be validly inferred from various data types under different models remains poorly understood.","Previous work has largely focused on level-1 networks, in which reticulation events are well separated, and on a general network's tree of blobs, the tree obtained by contracting every blob to a node.","An open question is the identifiability of the topology of a blob of unknown level.","We consider the identifiability of the circular order in which subnetworks attach to a blob, first proving that this order is well-defined for outer-labeled planar blobs.","For this class of blobs, we show that the circular order information from 4-taxon subnetworks identifies the full circular order of the blob.","Similarly, the circular order from 3-taxon rooted subnetworks identifies the full circular order of a rooted blob.","We then show that subnetwork circular information is identifiable from certain data types and evolutionary models.","This provides a general positive result for high-level networks, on the identifiability of the ordering in which taxon blocks attach to blobs in outer-labeled planar networks.","Finally, we give examples of blobs with different internal structures which cannot be distinguished under many models and data types."],"url":"http://arxiv.org/abs/2402.11693v1","category":"q-bio.PE"}
{"created":"2024-02-18 19:44:06","title":"On properties of the sets of positively curved Riemannian metrics on generalized Wallach spaces","abstract":"Sets related to positively curved invariant Riemannian metrics on generalized Wallach spaces are considered. The problem arises in studying of the evolution of such metrics under the normalized Ricci flow equation. For Riemannian metrics of the Wallach spaces $\\operatorname{SU}(3)/T_{\\max}$, $\\operatorname{Sp(3)}/ \\left(\\operatorname{Sp(1)}\\right)^3$ and $F_4/\\operatorname{Spin(8)}$ which admit positive sectional curvature and belong to a given invariant surface $\\Sigma$ of the normalized Ricci flow we established that they form a set $\\Sigma S$ bounded by three connected and pairwise disjoint regular space curves such that each of them approaches two others asymptotically at infinity. Analogously, we proved that for all generalized Wallach spaces the set of Riemannian metrics $\\Sigma R$ which belong to $\\Sigma$ and admit positive Ricci curvature is bounded by three curves each consisting of two connected components as regular curves. Intersections and asymptotical behaviors of these components were studied as well.","sentences":["Sets related to positively curved invariant Riemannian metrics on generalized Wallach spaces are considered.","The problem arises in studying of the evolution of such metrics under the normalized Ricci flow equation.","For Riemannian metrics of the Wallach spaces $\\operatorname{SU}(3)/T_{\\max}$, $\\operatorname{Sp(3)}/ \\left(\\operatorname{Sp(1)}\\right)^3$ and $F_4/\\operatorname{Spin(8)}$ which admit positive sectional curvature and belong to a given invariant surface $\\Sigma$ of the normalized Ricci flow we established that they form a set $\\Sigma S$ bounded by three connected and pairwise disjoint regular space curves such that each of them approaches two others asymptotically at infinity.","Analogously, we proved that for all generalized Wallach spaces the set of Riemannian metrics $\\Sigma R$ which belong to $\\Sigma$ and admit positive Ricci curvature is bounded by three curves each consisting of two connected components as regular curves.","Intersections and asymptotical behaviors of these components were studied as well."],"url":"http://arxiv.org/abs/2402.11692v1","category":"math.DG"}
{"created":"2024-02-18 19:35:30","title":"Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks","abstract":"Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. We conducted comprehensive experiments on various datasets with multiple QML model architectures. Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\\times$ and $0.99\\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\\_classes). To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process. In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Although noise and architectural variability can provide up to $\\sim16\\%$ output obfuscation, our comprehensive analysis revealed that models cloned under noisy conditions tend to be resilient, suffering little to no performance degradation due to such obfuscations. Despite limited success with our defense techniques, this outcome has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks.","sentences":["Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack.","In this study, we assess the efficacy of such attacks in the realm of quantum computing.","We conducted comprehensive experiments on various datasets with multiple QML model architectures.","Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\\times$ and $0.99\\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\\_classes).","To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process.","In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP).","Although noise and architectural variability can provide up to $\\sim16\\%$ output obfuscation, our comprehensive analysis revealed that models cloned under noisy conditions tend to be resilient, suffering little to no performance degradation due to such obfuscations.","Despite limited success with our defense techniques, this outcome has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks."],"url":"http://arxiv.org/abs/2402.11687v1","category":"quant-ph"}
{"created":"2024-02-18 19:31:26","title":"Learning the Topology and Behavior of Discrete Dynamical Systems","abstract":"Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system. We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to some classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the topology and behavior are unknown, using the well-known formalism of the Natarajan dimension. Our results provide a theoretical foundation for learning both the behavior and topology of discrete dynamical systems.","sentences":["Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks.","Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known.","In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system.","We show that, in general, this learning problem is computationally intractable.","On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to some classes.","Further, we examine a relaxed setting where the topology of an unknown system is partially observed.","For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity.","Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the topology and behavior are unknown, using the well-known formalism of the Natarajan dimension.","Our results provide a theoretical foundation for learning both the behavior and topology of discrete dynamical systems."],"url":"http://arxiv.org/abs/2402.11686v1","category":"cs.LG"}
{"created":"2024-02-18 19:29:01","title":"Variability-Aware Noise-Induced Dynamic Instability of Ultra-Low-Voltage SRAM Bitcells","abstract":"Stability of ultra-low-voltage SRAM bitcells in retention mode is threatened by two types of uncertainty: process variability and intrinsic noise. While variability dominates the failure probability, noise-induced bit flips in weakened bitcells lead to dynamic instability. We study both effects jointly in a unified SPICE simulation framework. Starting from a synthetic representation of process variations introduced in a previous work, we identify the cases of poor noise immunity that require thorough noise analyses. Relying on a rigorous and systematic methodology, we simulate them in the time domain so as to emulate a true data retention operation. Short times to failure, unacceptable for a practical ultra-low-power memory system application, are recorded. The transient bit-flip mechanism is analysed and a dynamic failure criterion involving the unstable point is established. We conclude that, beyond static variability, the dynamic noise inflates defectiveness among SRAM bitcells. We also discuss the limits of existing analytical formulas from the literature, which rely on a linear near-equilibrium approximation of the SRAM dynamics to, inaccurately, predict the mean time to failure.","sentences":["Stability of ultra-low-voltage SRAM bitcells in retention mode is threatened by two types of uncertainty: process variability and intrinsic noise.","While variability dominates the failure probability, noise-induced bit flips in weakened bitcells lead to dynamic instability.","We study both effects jointly in a unified SPICE simulation framework.","Starting from a synthetic representation of process variations introduced in a previous work, we identify the cases of poor noise immunity that require thorough noise analyses.","Relying on a rigorous and systematic methodology, we simulate them in the time domain so as to emulate a true data retention operation.","Short times to failure, unacceptable for a practical ultra-low-power memory system application, are recorded.","The transient bit-flip mechanism is analysed and a dynamic failure criterion involving the unstable point is established.","We conclude that, beyond static variability, the dynamic noise inflates defectiveness among SRAM bitcells.","We also discuss the limits of existing analytical formulas from the literature, which rely on a linear near-equilibrium approximation of the SRAM dynamics to, inaccurately, predict the mean time to failure."],"url":"http://arxiv.org/abs/2402.11685v1","category":"cs.AR"}
{"created":"2024-02-18 19:12:18","title":"Learning Conditional Invariances through Non-Commutativity","abstract":"Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\\Phi^*_\\tau$. We prove that non-commutativity steers the optimization towards $\\Phi^*_\\tau$ instead of $\\varphi^*$, bringing the $\\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source domain samples to meet the sample complexity needs of learning $\\Phi^*_\\tau$, surpassing SOTA invariance learning algorithms for domain adaptation, at times by over $2\\%$, approaching the performance of an oracle. Implementation is available at https://github.com/abhrac/nci.","sentences":["Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation.","We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain.","Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\\Phi^*_\\tau$. We prove that non-commutativity steers the optimization towards $\\Phi^*_\\tau$ instead of $\\varphi^*$, bringing the $\\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk.","Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source domain samples to meet the sample complexity needs of learning $\\Phi^*_\\tau$, surpassing SOTA invariance learning algorithms for domain adaptation, at times by over $2\\%$, approaching the performance of an oracle.","Implementation is available at https://github.com/abhrac/nci."],"url":"http://arxiv.org/abs/2402.11682v1","category":"cs.LG"}
{"created":"2024-02-18 18:43:05","title":"Secure quantum imaging with decoy state heralded single photons","abstract":"Weak coherent source (WCS) and spontaneous parametric down converted heralded single photon pairs have found applications in quantum key distribution (QKD) and quantum imaging (QI) experiments. Decoy state methods have also been used to enhance the security for QKD and QI. We study quantum secured imaging with the decoy state heralded single photon source (HSPS). The HSPSs superior performance in low photon number regimes makes it an ideal candidate for integrating quantum key distribution protocols to reduce measurement uncertainty and ensure secure QI. Furthermore, our results also infer the influence of the decoy state WCS, due to its higher operating speed than decoy state HSPS, would be effective in conditions that allow higher mean photon numbers for quantum secured imaging.","sentences":["Weak coherent source (WCS) and spontaneous parametric down converted heralded single photon pairs have found applications in quantum key distribution (QKD) and quantum imaging (QI) experiments.","Decoy state methods have also been used to enhance the security for QKD and QI.","We study quantum secured imaging with the decoy state heralded single photon source (HSPS).","The HSPSs superior performance in low photon number regimes makes it an ideal candidate for integrating quantum key distribution protocols to reduce measurement uncertainty and ensure secure QI.","Furthermore, our results also infer the influence of the decoy state WCS, due to its higher operating speed than decoy state HSPS, would be effective in conditions that allow higher mean photon numbers for quantum secured imaging."],"url":"http://arxiv.org/abs/2402.11675v1","category":"quant-ph"}
{"created":"2024-02-18 18:16:43","title":"Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry","abstract":"In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification. The opaque nature of these models, often considered 'black boxes', is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps. Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations. Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts. These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks. Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a better understanding of neural networks in these application areas.","sentences":["In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification.","The opaque nature of these models, often considered 'black boxes', is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps.","Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations.","Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts.","These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks.","Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a better understanding of neural networks in these application areas."],"url":"http://arxiv.org/abs/2402.11670v1","category":"cs.CV"}
{"created":"2024-02-18 18:02:27","title":"Specifying and Analyzing Networked and Layered Control Systems Operating on Multiple Clocks","abstract":"We consider the problem of reasoning about networked and layered control systems using assume-guarantee specifications. As these systems are formed by the interconnection of components that operate under various clocks, we introduce a new logic, Multiclock Logic (MCL), to be able to express the requirements of components form the point of view of their local clocks. Specifying components locally promotes independent design and component reuse. We carry out a contract-based analysis of a control system implemented via two control algorithms (model predictive control and feedback linearization) running on their own processors and clocks. Then we implement each of the contracts to build a system. The system performs as desired when the requirements derived from our system-level analysis are respected. Violating the constraints required by the contract-based analysis of the system leads to error.","sentences":["We consider the problem of reasoning about networked and layered control systems using assume-guarantee specifications.","As these systems are formed by the interconnection of components that operate under various clocks, we introduce a new logic, Multiclock Logic (MCL), to be able to express the requirements of components form the point of view of their local clocks.","Specifying components locally promotes independent design and component reuse.","We carry out a contract-based analysis of a control system implemented via two control algorithms (model predictive control and feedback linearization) running on their own processors and clocks.","Then we implement each of the contracts to build a system.","The system performs as desired when the requirements derived from our system-level analysis are respected.","Violating the constraints required by the contract-based analysis of the system leads to error."],"url":"http://arxiv.org/abs/2402.11666v1","category":"eess.SY"}
{"created":"2024-02-18 17:42:19","title":"TDE-3: An improved prior for optical flow computation in spiking neural networks","abstract":"Motion detection is a primary task required for robotic systems to perceive and navigate in their environment. Proposed in the literature bioinspired neuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and processors with spiking neural networks to provide real-time and energy-efficient motion detection through extracting temporal correlations between two points in space. However, on the algorithmic level, this design leads to loss of direction-selectivity of individual TDEs in textured environments. Here we propose an augmented 3-point TDE (TDE-3) with additional inhibitory input that makes TDE-3 direction-selectivity robust in textured environments. We developed a procedure to train the new TDE-3 using backpropagation through time and surrogate gradients to linearly map input velocities into an output spike count or an Inter-Spike Interval (ISI). Our work is the first instance of training a spiking neuron to have a specific ISI. Using synthetic data we compared training and inference with spike count and ISI with respect to changes in stimuli dynamic range, spatial frequency, and level of noise. ISI turns out to be more robust towards variation in spatial frequency, whereas the spike count is a more reliable training signal in the presence of noise. We performed the first in-depth quantitative investigation of optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of energy-efficiency and coding precision. Results show that on the network level both detectors show similar precision (20 degree angular error, 88% correlation with ground truth). Yet, due to the more robust direction-selectivity of individual TDEs, TDE-3 based network spike less and hence is more energy-efficient. Reported precision is on par with model-based methods but the spike-based processing of the TDEs provides allows more energy-efficient inference with neuromorphic hardware.","sentences":["Motion detection is a primary task required for robotic systems to perceive and navigate in their environment.","Proposed in the literature bioinspired neuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and processors with spiking neural networks to provide real-time and energy-efficient motion detection through extracting temporal correlations between two points in space.","However, on the algorithmic level, this design leads to loss of direction-selectivity of individual TDEs in textured environments.","Here we propose an augmented 3-point TDE (TDE-3) with additional inhibitory input that makes TDE-3 direction-selectivity robust in textured environments.","We developed a procedure to train the new TDE-3 using backpropagation through time and surrogate gradients to linearly map input velocities into an output spike count or an Inter-Spike Interval (ISI).","Our work is the first instance of training a spiking neuron to have a specific ISI.","Using synthetic data we compared training and inference with spike count and ISI with respect to changes in stimuli dynamic range, spatial frequency, and level of noise.","ISI turns out to be more robust towards variation in spatial frequency, whereas the spike count is a more reliable training signal in the presence of noise.","We performed the first in-depth quantitative investigation of optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of energy-efficiency and coding precision.","Results show that on the network level both detectors show similar precision (20 degree angular error, 88% correlation with ground truth).","Yet, due to the more robust direction-selectivity of individual TDEs, TDE-3 based network spike less and hence is more energy-efficient.","Reported precision is on par with model-based methods but the spike-based processing of the TDEs provides allows more energy-efficient inference with neuromorphic hardware."],"url":"http://arxiv.org/abs/2402.11662v1","category":"cs.NE"}
{"created":"2024-02-18 17:28:15","title":"On the importance of assessing topological convergence in Bayesian phylogenetic inference","abstract":"Modern phylogenetics research is often performed within a Bayesian framework, using sampling algorithms such as Markov chain Monte Carlo (MCMC) to approximate the posterior distribution. These algorithms require careful evaluation of the quality of the generated samples. Within the field of phylogenetics, one frequently adopted diagnostic approach is to evaluate the effective sample size (ESS) and to investigate trace graphs of the sampled parameters. A major limitation of these approaches is that they are developed for continuous parameters and therefore incompatible with a crucial parameter in these inferences: the tree topology. Several recent advancements have aimed at extending these diagnostics to topological space. In this short reflection paper, we present a case study illustrating how these topological diagnostics can contain information not found in standard diagnostics, and how decisions regarding which of these diagnostics to compute can impact inferences regarding MCMC convergence and mixing. Given the major importance of detecting convergence and mixing issues in Bayesian phylogenetic analyses, the lack of a unified approach to this problem warrants further action, especially now that additional tools are becoming available to researchers.","sentences":["Modern phylogenetics research is often performed within a Bayesian framework, using sampling algorithms such as Markov chain Monte Carlo (MCMC) to approximate the posterior distribution.","These algorithms require careful evaluation of the quality of the generated samples.","Within the field of phylogenetics, one frequently adopted diagnostic approach is to evaluate the effective sample size (ESS) and to investigate trace graphs of the sampled parameters.","A major limitation of these approaches is that they are developed for continuous parameters and therefore incompatible with a crucial parameter in these inferences: the tree topology.","Several recent advancements have aimed at extending these diagnostics to topological space.","In this short reflection paper, we present a case study illustrating how these topological diagnostics can contain information not found in standard diagnostics, and how decisions regarding which of these diagnostics to compute can impact inferences regarding MCMC convergence and mixing.","Given the major importance of detecting convergence and mixing issues in Bayesian phylogenetic analyses, the lack of a unified approach to this problem warrants further action, especially now that additional tools are becoming available to researchers."],"url":"http://arxiv.org/abs/2402.11657v1","category":"q-bio.PE"}
{"created":"2024-02-18 17:27:51","title":"Integrating Pre-Trained Language Model with Physical Layer Communications","abstract":"The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework achieves a 50% reduction in transmission size while demonstrating substantial generalization ability and noise robustness under standardized 3GPP channel models.","sentences":["The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks.","However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges.","In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator.","Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities.","Simulations, across various communication scenarios, reveal that our framework achieves a 50% reduction in transmission size while demonstrating substantial generalization ability and noise robustness under standardized 3GPP channel models."],"url":"http://arxiv.org/abs/2402.11656v1","category":"cs.IT"}
{"created":"2024-02-18 17:17:17","title":"Model-Free $\u03bc$-Synthesis: A Nonsmooth Optimization Perspective","abstract":"In this paper, we revisit model-free policy search on an important robust control benchmark, namely $\\mu$-synthesis. In the general output-feedback setting, there do not exist convex formulations for this problem, and hence global optimality guarantees are not expected. Apkarian (2011) presented a nonconvex nonsmooth policy optimization approach for this problem, and achieved state-of-the-art design results via using subgradient-based policy search algorithms which generate update directions in a model-based manner. Despite the lack of convexity and global optimality guarantees, these subgradient-based policy search methods have led to impressive numerical results in practice. Built upon such a policy optimization persepctive, our paper extends these subgradient-based search methods to a model-free setting. Specifically, we examine the effectiveness of two model-free policy optimization strategies: the model-free non-derivative sampling method and the zeroth-order policy search with uniform smoothing. We performed an extensive numerical study to demonstrate that both methods consistently replicate the design outcomes achieved by their model-based counterparts. Additionally, we provide some theoretical justifications showing that convergence guarantees to stationary points can be established for our model-free $\\mu$-synthesis under some assumptions related to the coerciveness of the cost function. Overall, our results demonstrate that derivative-free policy optimization offers a competitive and viable approach for solving general output-feedback $\\mu$-synthesis problems in the model-free setting.","sentences":["In this paper, we revisit model-free policy search on an important robust control benchmark, namely $\\mu$-synthesis.","In the general output-feedback setting, there do not exist convex formulations for this problem, and hence global optimality guarantees are not expected.","Apkarian (2011) presented a nonconvex nonsmooth policy optimization approach for this problem, and achieved state-of-the-art design results via using subgradient-based policy search algorithms which generate update directions in a model-based manner.","Despite the lack of convexity and global optimality guarantees, these subgradient-based policy search methods have led to impressive numerical results in practice.","Built upon such a policy optimization persepctive, our paper extends these subgradient-based search methods to a model-free setting.","Specifically, we examine the effectiveness of two model-free policy optimization strategies: the model-free non-derivative sampling method and the zeroth-order policy search with uniform smoothing.","We performed an extensive numerical study to demonstrate that both methods consistently replicate the design outcomes achieved by their model-based counterparts.","Additionally, we provide some theoretical justifications showing that convergence guarantees to stationary points can be established for our model-free $\\mu$-synthesis under some assumptions related to the coerciveness of the cost function.","Overall, our results demonstrate that derivative-free policy optimization offers a competitive and viable approach for solving general output-feedback $\\mu$-synthesis problems in the model-free setting."],"url":"http://arxiv.org/abs/2402.11654v1","category":"math.OC"}
{"created":"2024-02-18 17:13:46","title":"Doubly Robust Inference in Causal Latent Factor Models","abstract":"This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.","sentences":["This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes.","The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion.","We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate.","Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article."],"url":"http://arxiv.org/abs/2402.11652v1","category":"econ.EM"}
{"created":"2024-02-18 16:37:32","title":"In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness","abstract":"A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.","sentences":["A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context.","As such that learner must adapt to the context without additional training.","We explore the role of softmax attention in an ICL setting where each context encodes a regression task.","We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks.","Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks.","We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference.","Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses."],"url":"http://arxiv.org/abs/2402.11639v1","category":"cs.LG"}
{"created":"2024-02-18 16:36:00","title":"Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks","abstract":"The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.","sentences":["The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse.","The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios.","We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating.","Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels.","Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes.","Averaging all detectors, the performance drops by 35% across all attacks.","Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness."],"url":"http://arxiv.org/abs/2402.11638v1","category":"cs.CL"}
{"created":"2024-02-18 16:20:17","title":"Reliable long timescale decision-directed channel estimation for OFDM system","abstract":"Decision-directed channel estimation (DDCE) is one kind of blind channel estimation method that tracks the channel blindly by an iterative algorithm without relying on the pilots, which can increase the utilization of wireless resource. However, one major problem of DDCE is the performance degradation caused by error accumulation during the tracking process. In this paper, we propose an reliable DDCE (RDDCE) scheme for an OFDM-based communication system in the time-varying deep fading environment. By combining the conventional DDCE and discrete Fourier transform (DFT) channel estimation method, the proposed RDDCE scheme selects the reliable estimated channels on the subcarriers which are less affected by deep fading, and then estimates the channel based on the selected subcarriers by an extended DFT channel estimation where the indices of selected subcarriers are not distributed evenly. Simulation results show that RRDCE can alleviate the performance degradation effectively, track the channel with high accuracy on a long time scale, and has good performance under time-varying and noisy channel conditions.","sentences":["Decision-directed channel estimation (DDCE) is one kind of blind channel estimation method that tracks the channel blindly by an iterative algorithm without relying on the pilots, which can increase the utilization of wireless resource.","However, one major problem of DDCE is the performance degradation caused by error accumulation during the tracking process.","In this paper, we propose an reliable DDCE (RDDCE) scheme for an OFDM-based communication system in the time-varying deep fading environment.","By combining the conventional DDCE and discrete Fourier transform (DFT) channel estimation method, the proposed RDDCE scheme selects the reliable estimated channels on the subcarriers which are less affected by deep fading, and then estimates the channel based on the selected subcarriers by an extended DFT channel estimation where the indices of selected subcarriers are not distributed evenly.","Simulation results show that RRDCE can alleviate the performance degradation effectively, track the channel with high accuracy on a long time scale, and has good performance under time-varying and noisy channel conditions."],"url":"http://arxiv.org/abs/2402.11632v1","category":"eess.SP"}
{"created":"2024-02-18 16:01:28","title":"Interactive Garment Recommendation with User in the Loop","abstract":"Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training.","sentences":["Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases.","In this paper, we work under the assumption that no prior knowledge is given about a user.","We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit.","We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction.","To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop.","We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences.","Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training."],"url":"http://arxiv.org/abs/2402.11627v1","category":"cs.CV"}
{"created":"2024-02-18 15:33:24","title":"SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models","abstract":"In the digital era, the widespread use of APIs is evident. However, scalable utilization of APIs poses a challenge due to structure divergence observed in online API documentation. This underscores the need for automatic tools to facilitate API consumption. A viable approach involves the conversion of documentation into an API Specification format. While previous attempts have been made using rule-based methods, these approaches encountered difficulties in generalizing across diverse documentation. In this paper we introduce SpeCrawler, a comprehensive system that utilizes large language models (LLMs) to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline. By creating a standardized format for numerous APIs, SpeCrawler aids in streamlining integration processes within API orchestrating systems and facilitating the incorporation of tools into LLMs. The paper explores SpeCrawler's methodology, supported by empirical evidence and case studies, demonstrating its efficacy through LLM capabilities.","sentences":["In the digital era, the widespread use of APIs is evident.","However, scalable utilization of APIs poses a challenge due to structure divergence observed in online API documentation.","This underscores the need for automatic tools to facilitate API consumption.","A viable approach involves the conversion of documentation into an API Specification format.","While previous attempts have been made using rule-based methods, these approaches encountered difficulties in generalizing across diverse documentation.","In this paper we introduce SpeCrawler, a comprehensive system that utilizes large language models (LLMs) to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline.","By creating a standardized format for numerous APIs, SpeCrawler aids in streamlining integration processes within API orchestrating systems and facilitating the incorporation of tools into LLMs.","The paper explores SpeCrawler's methodology, supported by empirical evidence and case studies, demonstrating its efficacy through LLM capabilities."],"url":"http://arxiv.org/abs/2402.11625v1","category":"cs.CL"}
{"created":"2024-02-18 15:06:28","title":"Impact of the La2NiO4+\u03b4 oxygen content on the synaptic properties of the TiN/La2NiO4+\u03b4/Pt memristive devices","abstract":"The rapid development of brain-inspired computing requires new artificial components and architectures for its hardware implementation. In this regard, memristive devices emerged as potential candidates for artificial synapses because of their ability to emulate the plasticity of the biological synapses. In this work, the synaptic behavior of the TiN/La2NiO4+{\\delta}/Pt memristive devices based on thermally annealed La2NiO4+{\\delta} films is thoroughly investigated. Using electron energy loss spectroscopy, we show that annealing using reducing (Ar) or oxidizing (O2) atmospheres affects the interstitial oxygen content ({\\delta}) in the La2NiO4+{\\delta} films. Electrical characterization shows that both devices exhibit long-term potentiation/depression and spike-timing-dependent plasticity, which makes them suitable for neuromorphic applications. At the same time, the Ar annealed TiN/La2NiO4+{\\delta}/Pt device demonstrates non-volatile properties with low energy consumption during the learning process. On the other hand, in the O2 annealed TiN/La2NiO4+{\\delta}/Pt device the resistive switching behavior is more volatile and requires more energy for synaptic learning. Finally, the simulation tools show that spiking neural network architectures with unsupervised learning rules based on the experimental data achieve high inference accuracy in the digit recognition task, which proves the potential of TiN/La2NiO4+{\\delta}/Pt devices for artificial synapse applications.","sentences":["The rapid development of brain-inspired computing requires new artificial components and architectures for its hardware implementation.","In this regard, memristive devices emerged as potential candidates for artificial synapses because of their ability to emulate the plasticity of the biological synapses.","In this work, the synaptic behavior of the TiN/La2NiO4+{\\delta}/Pt memristive devices based on thermally annealed La2NiO4+{\\delta} films is thoroughly investigated.","Using electron energy loss spectroscopy, we show that annealing using reducing (Ar) or oxidizing (O2) atmospheres affects the interstitial oxygen content ({\\delta}) in the La2NiO4+{\\delta} films.","Electrical characterization shows that both devices exhibit long-term potentiation/depression and spike-timing-dependent plasticity, which makes them suitable for neuromorphic applications.","At the same time, the Ar annealed TiN/La2NiO4+{\\delta}/Pt device demonstrates non-volatile properties with low energy consumption during the learning process.","On the other hand, in the O2 annealed TiN/La2NiO4+{\\delta}/Pt device the resistive switching behavior is more volatile and requires more energy for synaptic learning.","Finally, the simulation tools show that spiking neural network architectures with unsupervised learning rules based on the experimental data achieve high inference accuracy in the digit recognition task, which proves the potential of TiN/La2NiO4+{\\delta}/Pt devices for artificial synapse applications."],"url":"http://arxiv.org/abs/2402.11612v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-18 14:59:07","title":"Risk-aware product decisions in A/B tests with multiple metrics","abstract":"In the past decade, AB tests have become the standard method for making product decisions in tech companies. They offer a scientific approach to product development, using statistical hypothesis testing to control the risks of incorrect decisions. Typically, multiple metrics are used in AB tests to serve different purposes, such as establishing evidence of success, guarding against regressions, or verifying test validity. To mitigate risks in AB tests with multiple outcomes, it's crucial to adapt the design and analysis to the varied roles of these outcomes. This paper introduces the theoretical framework for decision rules guiding the evaluation of experiments at Spotify. First, we show that if guardrail metrics with non-inferiority tests are used, the significance level does not need to be multiplicity-adjusted for those tests. Second, if the decision rule includes non-inferiority tests, deterioration tests, or tests for quality, the type II error rate must be corrected to guarantee the desired power level for the decision. We propose a decision rule encompassing success, guardrail, deterioration, and quality metrics, employing diverse tests. This is accompanied by a design and analysis plan that mitigates risks across any data-generating process. The theoretical results are demonstrated using Monte Carlo simulations.","sentences":["In the past decade, AB tests have become the standard method for making product decisions in tech companies.","They offer a scientific approach to product development, using statistical hypothesis testing to control the risks of incorrect decisions.","Typically, multiple metrics are used in AB tests to serve different purposes, such as establishing evidence of success, guarding against regressions, or verifying test validity.","To mitigate risks in AB tests with multiple outcomes, it's crucial to adapt the design and analysis to the varied roles of these outcomes.","This paper introduces the theoretical framework for decision rules guiding the evaluation of experiments at Spotify.","First, we show that if guardrail metrics with non-inferiority tests are used, the significance level does not need to be multiplicity-adjusted for those tests.","Second, if the decision rule includes non-inferiority tests, deterioration tests, or tests for quality, the type II error rate must be corrected to guarantee the desired power level for the decision.","We propose a decision rule encompassing success, guardrail, deterioration, and quality metrics, employing diverse tests.","This is accompanied by a design and analysis plan that mitigates risks across any data-generating process.","The theoretical results are demonstrated using Monte Carlo simulations."],"url":"http://arxiv.org/abs/2402.11609v1","category":"stat.ME"}
{"created":"2024-02-18 14:42:47","title":"Self-evolving Autoencoder Embedded Q-Network","abstract":"In the realm of sequential decision-making tasks, the exploration capability of a reinforcement learning (RL) agent is paramount for achieving high rewards through interactions with the environment. To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder architecture adapts and evolves as the agent explores the environment. This evolution enables the autoencoder to capture a diverse range of raw observations and represent them effectively in its latent space. By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards. During the evolution of the autoencoder architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent. This strategy involves two key components: (i) fostering the growth of nodes to retain previously acquired knowledge, ensuring a rich representation of the environment, and (ii) pruning the least contributing nodes to maintain a more manageable and tractable latent space. Extensive experimental evaluations conducted on three distinct benchmark environments and a real-world molecular environment demonstrate that the proposed SAQN significantly outperforms state-of-the-art counterparts. The results highlight the effectiveness of the self-evolving autoencoder and its collaboration with the Q-Network in tackling sequential decision-making tasks.","sentences":["In the realm of sequential decision-making tasks, the exploration capability of a reinforcement learning (RL) agent is paramount for achieving high rewards through interactions with the environment.","To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is embedded with a Q-Network (QN).","In SAQN, the self-evolving autoencoder architecture adapts and evolves as the agent explores the environment.","This evolution enables the autoencoder to capture a diverse range of raw observations and represent them effectively in its latent space.","By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards.","During the evolution of the autoencoder architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent.","This strategy involves two key components: (i) fostering the growth of nodes to retain previously acquired knowledge, ensuring a rich representation of the environment, and (ii) pruning the least contributing nodes to maintain a more manageable and tractable latent space.","Extensive experimental evaluations conducted on three distinct benchmark environments and a real-world molecular environment demonstrate that the proposed SAQN significantly outperforms state-of-the-art counterparts.","The results highlight the effectiveness of the self-evolving autoencoder and its collaboration with the Q-Network in tackling sequential decision-making tasks."],"url":"http://arxiv.org/abs/2402.11604v1","category":"cs.LG"}
{"created":"2024-02-18 14:17:30","title":"Faster algorithms on linear delta-matroids","abstract":"We show new algorithms and constructions over linear delta-matroids. We observe an alternative representation for linear delta-matroids, as a contraction representation over a skew-symmetric matrix. This is equivalent to the more standard \"twist representation\" up to $O(n^\\omega)$-time transformations, but is much more convenient for algorithmic tasks. For instance, the problem of finding a max-weight feasible set now reduces directly to the problem of finding a max-weight basis in a linear matroid. Supported by this representation, we provide new algorithms and constructions over linear delta-matroids. We show that the union and delta-sum of linear delta-matroids define linear delta-matroids, and a representation for the resulting delta-matroid can be constructed in randomized time $O(n^\\omega)$. Previously, it was only known that these operations define delta-matroids. We also note that every projected linear delta-matroid can be represented as an elementary projection. This implies that several optimization problems over (projected) linear delta-matroids, including the coverage, delta-coverage, and parity problems, reduce (in their decision versions) to a single $O(n^{\\omega})$-time matrix rank computation. Using the methods of Harvey, previously used by Cheung, Lao and Leung for linear matroid parity, we furthermore show how to solve the search versions in the same time. This improves on the $O(n^4)$-time augmenting path algorithm of Geelen, Iwata and Murota. Finally, we consider the maximum-cardinality delta-matroid intersection problem. Using Storjohann's algorithms for symbolic determinants, we show that such a solution can be found in $O(n^{\\omega+1})$ time. This is the first polynomial-time algorithm for the problem, solving an open question of Kakimura and Takamatsu.","sentences":["We show new algorithms and constructions over linear delta-matroids.","We observe an alternative representation for linear delta-matroids, as a contraction representation over a skew-symmetric matrix.","This is equivalent to the more standard \"twist representation\" up to $O(n^\\omega)$-time transformations, but is much more convenient for algorithmic tasks.","For instance, the problem of finding a max-weight feasible set now reduces directly to the problem of finding a max-weight basis in a linear matroid.","Supported by this representation, we provide new algorithms and constructions over linear delta-matroids.","We show that the union and delta-sum of linear delta-matroids define linear delta-matroids, and a representation for the resulting delta-matroid can be constructed in randomized time $O(n^\\omega)$. Previously, it was only known that these operations define delta-matroids.","We also note that every projected linear delta-matroid can be represented as an elementary projection.","This implies that several optimization problems over (projected) linear delta-matroids, including the coverage, delta-coverage, and parity problems, reduce (in their decision versions) to a single $O(n^{\\omega})$-time matrix rank computation.","Using the methods of Harvey, previously used by Cheung, Lao and Leung for linear matroid parity, we furthermore show how to solve the search versions in the same time.","This improves on the $O(n^4)$-time augmenting path algorithm of Geelen, Iwata and Murota.","Finally, we consider the maximum-cardinality delta-matroid intersection problem.","Using Storjohann's algorithms for symbolic determinants, we show that such a solution can be found in $O(n^{\\omega+1})$ time.","This is the first polynomial-time algorithm for the problem, solving an open question of Kakimura and Takamatsu."],"url":"http://arxiv.org/abs/2402.11596v1","category":"cs.DS"}
{"created":"2024-02-18 13:47:26","title":"CowScape: Quantitative reconstruction of the conformational landscape of biological macromolecules from cryo-EM data","abstract":"Cryo-EM data processing typically focuses on the structure of the main conformational state under investigation and discards images that belong to other states. This approach can reach atomic resolution, but ignores vast amounts of valuable information about the underlying conformational ensemble and its dynamics. CowScape analyzes an entire cryo-EM dataset and thereby obtains a quantitative description of structural variability of macromolecular complexes that represents the biochemically relevant conformational space. By combining extensive image classification with principal component analysis (PCA) of the classified 3D volumes and kernel density estimation, CowScape can be used as a quantitative tool to analyze this variability. PCA projects all 3D structures along the major modes spanning a low-dimensional space that captures a large portion of structural variability. The number of particle images in a given state can be used to calculate an energy landscape based on kernel density estimation and Boltzmann inversion. By revealing allosteric interactions in macromolecular complexes, CowScape allows us to distinguish and interpret dynamic changes in macromolecular complexes during function and regulation.","sentences":["Cryo-EM data processing typically focuses on the structure of the main conformational state under investigation and discards images that belong to other states.","This approach can reach atomic resolution, but ignores vast amounts of valuable information about the underlying conformational ensemble and its dynamics.","CowScape analyzes an entire cryo-EM dataset and thereby obtains a quantitative description of structural variability of macromolecular complexes that represents the biochemically relevant conformational space.","By combining extensive image classification with principal component analysis (PCA) of the classified 3D volumes and kernel density estimation, CowScape can be used as a quantitative tool to analyze this variability.","PCA projects all 3D structures along the major modes spanning a low-dimensional space that captures a large portion of structural variability.","The number of particle images in a given state can be used to calculate an energy landscape based on kernel density estimation and Boltzmann inversion.","By revealing allosteric interactions in macromolecular complexes, CowScape allows us to distinguish and interpret dynamic changes in macromolecular complexes during function and regulation."],"url":"http://arxiv.org/abs/2402.11589v1","category":"q-bio.BM"}
{"created":"2024-02-18 13:29:09","title":"Asymptotic behavior of 3-D evolutionary model of Magnetoelasticity for small data","abstract":"In this article, we consider the evolutionary model for magnetoelasticity with vanishing external magnetic field and vanishing viscosity/damping, which is a nonlinear dispersive system. The global regularity and scattering of the evolutionary model for magnetoelasticity under small size of initial data is proved. Our proof relies on the idea of vector-field method due to the quasilinearity and the presence of convective term. A key observation is that we construct a suitable energy functional including the mass quantity, which enable us provide a good decay estimates for Schr\\\"odinger flow. In particular, we establish the asymptotic behavior in both mass and energy spaces for Schr\\\"odinger map, not only for gauged equation.","sentences":["In this article, we consider the evolutionary model for magnetoelasticity with vanishing external magnetic field and vanishing viscosity/damping, which is a nonlinear dispersive system.","The global regularity and scattering of the evolutionary model for magnetoelasticity under small size of initial data is proved.","Our proof relies on the idea of vector-field method due to the quasilinearity and the presence of convective term.","A key observation is that we construct a suitable energy functional including the mass quantity, which enable us provide a good decay estimates for Schr\\\"odinger flow.","In particular, we establish the asymptotic behavior in both mass and energy spaces for Schr\\\"odinger map, not only for gauged equation."],"url":"http://arxiv.org/abs/2402.11587v1","category":"math.AP"}
{"created":"2024-02-18 13:24:48","title":"PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM","abstract":"Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios, along with videos containing challenging artefacts like fast motion and occlusion.","sentences":["Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps.","To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices.","PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead.","Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features.","Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models.","The evaluation of the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios, along with videos containing challenging artefacts like fast motion and occlusion."],"url":"http://arxiv.org/abs/2402.11585v1","category":"cs.CV"}
{"created":"2024-02-18 13:11:48","title":"Publicly auditable privacy-preserving electoral rolls","abstract":"While existing literature on electronic voting has extensively addressed verifiability of voting protocols, the vulnerability of electoral rolls in large public elections remains a critical concern. To ensure integrity of electoral rolls, the current practice is to either make electoral rolls public or share them with the political parties. However, this enables construction of detailed voter profiles and selective targeting and manipulation of voters, thereby undermining the fundamental principle of free and fair elections. In this paper, we study the problem of designing publicly auditable yet privacy-preserving electoral rolls. We first formulate a threat model and provide formal security definitions. We then present a protocol for creation and maintenance of electoral rolls that mitigates the threats. Eligible voters can verify their inclusion, whereas political parties and auditors can statistically audit the electoral roll. The entire electoral roll is never revealed, which prevents any large-scale systematic voter targeting and manipulation.","sentences":["While existing literature on electronic voting has extensively addressed verifiability of voting protocols, the vulnerability of electoral rolls in large public elections remains a critical concern.","To ensure integrity of electoral rolls, the current practice is to either make electoral rolls public or share them with the political parties.","However, this enables construction of detailed voter profiles and selective targeting and manipulation of voters, thereby undermining the fundamental principle of free and fair elections.","In this paper, we study the problem of designing publicly auditable yet privacy-preserving electoral rolls.","We first formulate a threat model and provide formal security definitions.","We then present a protocol for creation and maintenance of electoral rolls that mitigates the threats.","Eligible voters can verify their inclusion, whereas political parties and auditors can statistically audit the electoral roll.","The entire electoral roll is never revealed, which prevents any large-scale systematic voter targeting and manipulation."],"url":"http://arxiv.org/abs/2402.11582v1","category":"cs.CR"}
{"created":"2024-02-18 13:00:49","title":"Stackelberg reinsurance and premium decisions with MV criterion and irreversibility","abstract":"We study a reinsurance Stackelberg game in which both the insurer and the reinsurer adopt the mean-variance (abbr. MV) criterion in their decision-making and the reinsurance is irreversible. We apply a unified singular control framework where irreversible reinsurance contracts can be signed in both discrete and continuous times. The results theoretically illustrate that, rather than continuous-time contracts or a bunch of discrete-time contracts, a single once-for-all reinsurance contract is preferred. Moreover, the Stackelberg game turns out to be centering on the signing time of the single contract. The insurer signs the contract if the premium rate is lower than a time-dependent threshold and the reinsurer designs a premium that triggers the signing of the contract at his preferred time. Further, we find that reinsurance preference, discount and reversion have a decreasing dominance in the reinsurer's decision-making, which is not seen for the insurer.","sentences":["We study a reinsurance Stackelberg game in which both the insurer and the reinsurer adopt the mean-variance (abbr.","MV) criterion in their decision-making and the reinsurance is irreversible.","We apply a unified singular control framework where irreversible reinsurance contracts can be signed in both discrete and continuous times.","The results theoretically illustrate that, rather than continuous-time contracts or a bunch of discrete-time contracts, a single once-for-all reinsurance contract is preferred.","Moreover, the Stackelberg game turns out to be centering on the signing time of the single contract.","The insurer signs the contract if the premium rate is lower than a time-dependent threshold and the reinsurer designs a premium that triggers the signing of the contract at his preferred time.","Further, we find that reinsurance preference, discount and reversion have a decreasing dominance in the reinsurer's decision-making, which is not seen for the insurer."],"url":"http://arxiv.org/abs/2402.11580v1","category":"q-fin.MF"}
{"created":"2024-02-18 12:53:17","title":"Assessment of low-carbon tourism development from multi-aspect analysis: A case study of the Yellow River Basin, China","abstract":"Climate change has become an unavoidable problem in achieving sustainable development. As one of the major industries worldwide, tourism can make a significant contribution to mitigating climate change. The main objective of the paper is to assess the development level of low-carbon tourism from multi-aspect, using the Yellow River Basin as an example. Firstly, this study quantified tourism carbon dioxide emissions and tourism economy, and analyzed their evolution characteristics. The interaction and coordination degree between tourism carbon dioxide emissions and tourism economy were then analyzed using the improved coupling coordination degree model. Finally, this study analyzed the change in total factor productivity of low-carbon tourism by calculating the Malmquist-Luenberger productivity index. The results showed that: (1) The tourism industry in the Yellow River Basin has the characteristics of the initial environmental Kuznets curve. (2) There was a strong interaction between tourism carbon dioxide emissions and tourism economy, which was manifested as mutual promotion. (3) The total factor productivity of low-carbon tourism was increasing. Based on the above results, it could be concluded that the development level of low-carbon tourism in the Yellow River Basin has been continuously improved from 2000 to 2019, but it is still in the early development stage with the continuous growth of carbon dioxide emissions.","sentences":["Climate change has become an unavoidable problem in achieving sustainable development.","As one of the major industries worldwide, tourism can make a significant contribution to mitigating climate change.","The main objective of the paper is to assess the development level of low-carbon tourism from multi-aspect, using the Yellow River Basin as an example.","Firstly, this study quantified tourism carbon dioxide emissions and tourism economy, and analyzed their evolution characteristics.","The interaction and coordination degree between tourism carbon dioxide emissions and tourism economy were then analyzed using the improved coupling coordination degree model.","Finally, this study analyzed the change in total factor productivity of low-carbon tourism by calculating the Malmquist-Luenberger productivity index.","The results showed that: (1) The tourism industry in the Yellow River Basin has the characteristics of the initial environmental Kuznets curve.","(2) There was a strong interaction between tourism carbon dioxide emissions and tourism economy, which was manifested as mutual promotion.","(3) The total factor productivity of low-carbon tourism was increasing.","Based on the above results, it could be concluded that the development level of low-carbon tourism in the Yellow River Basin has been continuously improved from 2000 to 2019, but it is still in the early development stage with the continuous growth of carbon dioxide emissions."],"url":"http://arxiv.org/abs/2402.11579v1","category":"econ.GN"}
{"created":"2024-02-18 11:50:12","title":"On the Limits of Information Spread by Memory-less Agents","abstract":"We address the self-stabilizing bit-dissemination problem, designed to capture the challenges of spreading information and reaching consensus among entities with minimal cognitive and communication capacities. Specifically, a group of $n$ agents is required to adopt the correct opinion, initially held by a single informed individual, choosing from two possible opinions. In order to make decisions, agents are restricted to observing the opinions of a few randomly sampled agents, and lack the ability to communicate further and to identify the informed individual. Additionally, agents cannot retain any information from one round to the next.   According to a recent publication in SODA (2024), a logarithmic convergence time without memory is achievable in the parallel setting (where agents are updated simultaneously), as long as the number of samples is at least $\\Omega(\\sqrt{n \\log n})$. However, determining the minimal sample size for an efficient protocol to exist remains a challenging open question. As a preliminary step towards an answer, we establish the first lower bound for this problem in the parallel setting. Specifically, we demonstrate that any protocol with constant sample size requires asymptotically an almost-linear number of rounds to converge, with high probability. This lower bound holds even when agents are aware of both the exact value of $n$ and their own opinion, and encompasses various simple existing dynamics designed to achieve consensus.   Beyond the bit-dissemination problem, our result sheds light on the convergence time of the \"minority\" dynamics, the counterpart of the well-known majority rule, whose chaotic behavior is yet to be fully understood despite the apparent simplicity of the algorithm.","sentences":["We address the self-stabilizing bit-dissemination problem, designed to capture the challenges of spreading information and reaching consensus among entities with minimal cognitive and communication capacities.","Specifically, a group of $n$ agents is required to adopt the correct opinion, initially held by a single informed individual, choosing from two possible opinions.","In order to make decisions, agents are restricted to observing the opinions of a few randomly sampled agents, and lack the ability to communicate further and to identify the informed individual.","Additionally, agents cannot retain any information from one round to the next.   ","According to a recent publication in SODA (2024), a logarithmic convergence time without memory is achievable in the parallel setting (where agents are updated simultaneously), as long as the number of samples is at least $\\Omega(\\sqrt{n \\log n})$. However, determining the minimal sample size for an efficient protocol to exist remains a challenging open question.","As a preliminary step towards an answer, we establish the first lower bound for this problem in the parallel setting.","Specifically, we demonstrate that any protocol with constant sample size requires asymptotically an almost-linear number of rounds to converge, with high probability.","This lower bound holds even when agents are aware of both the exact value of $n$ and their own opinion, and encompasses various simple existing dynamics designed to achieve consensus.   ","Beyond the bit-dissemination problem, our result sheds light on the convergence time of the \"minority\" dynamics, the counterpart of the well-known majority rule, whose chaotic behavior is yet to be fully understood despite the apparent simplicity of the algorithm."],"url":"http://arxiv.org/abs/2402.11553v1","category":"cs.MA"}
{"created":"2024-02-18 11:46:59","title":"Pattern Recognition Facilities of Extended Kalman Filtering in Stochastic Neural Fields","abstract":"In mathematical neuroscience, a special interest is paid to a working memory mechanism in the neural tissue modeled by the Dynamic Neural Field (DNF) in the presence of model uncertainties. The working memory facility implies that the neurons' activity remains self-sustained after the external stimulus removal due to the recurrent interactions in the networks and allows the system to cope with missing sensors' information. In our previous works, we have developed two reconstruction methods of the neural membrane potential from {\\it incomplete} data available from the sensors. The methods are derived within the Extended Kalman filtering approach by using the Euler-Maruyama method and the It\\^{o}-Taylor expansion of order 1.5. It was shown that the It\\^{o}-Taylor EKF-based restoration process is more accurate than the Euler-Maruyama-based alternative. It improves the membrane potential reconstruction quality in case of incomplete sensors information. The aim of this paper is to investigate their pattern recognition facilities, i.e. the quality of the pattern formation reconstruction in case of model uncertainties and incomplete information. The numerical experiments are provided for an example of the stochastic DNF with multiple active zones arisen in a neural tissue.","sentences":["In mathematical neuroscience, a special interest is paid to a working memory mechanism in the neural tissue modeled by the Dynamic Neural Field (DNF) in the presence of model uncertainties.","The working memory facility implies that the neurons' activity remains self-sustained after the external stimulus removal due to the recurrent interactions in the networks and allows the system to cope with missing sensors' information.","In our previous works, we have developed two reconstruction methods of the neural membrane potential from {\\it incomplete} data available from the sensors.","The methods are derived within the Extended Kalman filtering approach by using the Euler-Maruyama method and the It\\^{o}-Taylor expansion of order 1.5.","It was shown that the It\\^{o}-Taylor EKF-based restoration process is more accurate than the Euler-Maruyama-based alternative.","It improves the membrane potential reconstruction quality in case of incomplete sensors information.","The aim of this paper is to investigate their pattern recognition facilities, i.e. the quality of the pattern formation reconstruction in case of model uncertainties and incomplete information.","The numerical experiments are provided for an example of the stochastic DNF with multiple active zones arisen in a neural tissue."],"url":"http://arxiv.org/abs/2402.11551v1","category":"math.OC"}
{"created":"2024-02-19 18:58:32","title":"Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding","abstract":"As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.84\\times$, and $2.37\\times$, and Llama2-70B offloading by up to $10.33\\times$ on L40.","sentences":["As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important.","While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware.","This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding.","To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens.","To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures.","Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform.","Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.84\\times$, and $2.37\\times$, and Llama2-70B offloading by up to $10.33\\times$ on L40."],"url":"http://arxiv.org/abs/2402.12374v1","category":"cs.CL"}
{"created":"2024-02-19 17:37:11","title":"Cosserat Rod Modeling and Validation for a Soft Continuum Robot with Self-Controllable Variable Curvature","abstract":"This paper introduces a Cosserat rod based mathematical model for modeling a self-controllable variable curvature soft continuum robot. This soft continuum robot has a hollow inner channel and was developed with the ability to perform variable curvature utilizing a growing spine. The growing spine is able to grow and retract while modifies its stiffness through milli-size particle (glass bubble) granular jamming. This soft continuum robot can then perform continuous curvature variation, unlike previous approaches whose curvature variation is discrete and depends on the number of locking mechanisms or manual configurations. The robot poses an emergent modeling problem due to the variable stiffness growing spine which is addressed in this paper. We investigate the property of growing spine stiffness and incorporate it into the Cosserat rod model by implementing a combined stiffness approach. We conduct experiments with the soft continuum robot in various configurations and compared the results with our developed mathematical model. The results show that the mathematical model based on the adapted Cosserat rod matches the experimental results with only a 3.3\\% error with respect to the length of the soft continuum robot.","sentences":["This paper introduces a Cosserat rod based mathematical model for modeling a self-controllable variable curvature soft continuum robot.","This soft continuum robot has a hollow inner channel and was developed with the ability to perform variable curvature utilizing a growing spine.","The growing spine is able to grow and retract while modifies its stiffness through milli-size particle (glass bubble) granular jamming.","This soft continuum robot can then perform continuous curvature variation, unlike previous approaches whose curvature variation is discrete and depends on the number of locking mechanisms or manual configurations.","The robot poses an emergent modeling problem due to the variable stiffness growing spine which is addressed in this paper.","We investigate the property of growing spine stiffness and incorporate it into the Cosserat rod model by implementing a combined stiffness approach.","We conduct experiments with the soft continuum robot in various configurations and compared the results with our developed mathematical model.","The results show that the mathematical model based on the adapted Cosserat rod matches the experimental results with only a 3.3\\% error with respect to the length of the soft continuum robot."],"url":"http://arxiv.org/abs/2402.12315v1","category":"cs.RO"}
{"created":"2024-02-19 16:30:35","title":"End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching","abstract":"We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.","sentences":["We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP).","We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices.","PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors.","Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data.","In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors."],"url":"http://arxiv.org/abs/2402.12269v1","category":"cs.LG"}
{"created":"2024-02-19 15:00:51","title":"Nanomechanical crystalline AlN resonators with high quality factors for quantum optoelectromechanics","abstract":"High-$Q_m$ mechanical resonators are crucial for applications where low noise and long coherence time are required, as mirror suspensions, quantum cavity optomechanical devices, or nanomechanical sensors. Tensile strain in the material enables the use of dissipation dilution and strain engineering techniques, which increase the mechanical quality factor. These techniques have been employed for high-$Q_m$ mechanical resonators made from amorphous materials and, recently, from crystalline materials such as InGaP, SiC, and Si. A strained crystalline film exhibiting substantial piezoelectricity expands the capability of high-$Q_m$ nanomechanical resonators to directly utilize electronic degrees of freedom. In this work we realize nanomechanical resonators with $Q_m$ up to $2.9\\times 10^{7}$ made from tensile-strained 290 nm-thick AlN, which is an epitaxially-grown crystalline material offering strong piezoelectricity. We demonstrate nanomechanical resonators that exploit dissipation dilution and strain engineering to reach a $Q_m \\times f_m$-product approaching $10^{13}$ Hz at room temperature. We realize a novel resonator geometry, triangline, whose shape follows the Al-N bonds and offers a central pad that we pattern with a photonic crystal. This allows us to reach an optical reflectivity above 80% for efficient coupling to out-of-plane light. The presented results pave the way for quantum optoelectromechanical devices at room temperature based on tensile-strained AlN.","sentences":["High-$Q_m$ mechanical resonators are crucial for applications where low noise and long coherence time are required, as mirror suspensions, quantum cavity optomechanical devices, or nanomechanical sensors.","Tensile strain in the material enables the use of dissipation dilution and strain engineering techniques, which increase the mechanical quality factor.","These techniques have been employed for high-$Q_m$ mechanical resonators made from amorphous materials and, recently, from crystalline materials such as InGaP, SiC, and Si.","A strained crystalline film exhibiting substantial piezoelectricity expands the capability of high-$Q_m$ nanomechanical resonators to directly utilize electronic degrees of freedom.","In this work we realize nanomechanical resonators with $Q_m$ up to $2.9\\times 10^{7}$ made from tensile-strained 290 nm-thick AlN, which is an epitaxially-grown crystalline material offering strong piezoelectricity.","We demonstrate nanomechanical resonators that exploit dissipation dilution and strain engineering to reach a $Q_m \\times f_m$-product approaching $10^{13}$ Hz at room temperature.","We realize a novel resonator geometry, triangline, whose shape follows the Al-N bonds and offers a central pad that we pattern with a photonic crystal.","This allows us to reach an optical reflectivity above 80% for efficient coupling to out-of-plane light.","The presented results pave the way for quantum optoelectromechanical devices at room temperature based on tensile-strained AlN."],"url":"http://arxiv.org/abs/2402.12196v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 14:42:29","title":"A Riemannian rank-adaptive method for higher-order tensor completion in the tensor-train format","abstract":"In this paper a new Riemannian rank adaptive method (RRAM) is proposed for the low-rank tensor completion problem (LRTCP) formulated as a least-squares optimization problem on the algebraic variety of tensors of bounded tensor-train (TT) rank. The method iteratively optimizes over fixed-rank smooth manifolds using a Riemannian conjugate gradient algorithm from Steinlechner (2016) and gradually increases the rank by computing a descent direction in the tangent cone to the variety. Additionally, a numerical method to estimate the amount of rank increase is proposed based on a theoretical result for the stationary points of the low-rank tensor approximation problem and a definition of an estimated TT-rank. Furthermore, when the iterate comes close to a lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm from Oseledets (2011) and a definition of a numerical rank. We prove that the TT-rounding algorithm can be considered as an approximate projection onto the lower-rank set which satisfies a certain angle condition to ensure that the image is sufficiently close to that of an exact projection. Several numerical experiments are given to illustrate the use of the RRAM and its subroutines in {\\Matlab}. Furthermore, in all experiments the proposed RRAM outperforms the state-of-the-art RRAM for tensor completion in the TT format from Steinlechner (2016) in terms of computation time.","sentences":["In this paper a new Riemannian rank adaptive method (RRAM) is proposed for the low-rank tensor completion problem (LRTCP) formulated as a least-squares optimization problem on the algebraic variety of tensors of bounded tensor-train (TT) rank.","The method iteratively optimizes over fixed-rank smooth manifolds using a Riemannian conjugate gradient algorithm from Steinlechner (2016) and gradually increases the rank by computing a descent direction in the tangent cone to the variety.","Additionally, a numerical method to estimate the amount of rank increase is proposed based on a theoretical result for the stationary points of the low-rank tensor approximation problem and a definition of an estimated TT-rank.","Furthermore, when the iterate comes close to a lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm from Oseledets (2011) and a definition of a numerical rank.","We prove that the TT-rounding algorithm can be considered as an approximate projection onto the lower-rank set which satisfies a certain angle condition to ensure that the image is sufficiently close to that of an exact projection.","Several numerical experiments are given to illustrate the use of the RRAM and its subroutines in {\\Matlab}.","Furthermore, in all experiments the proposed RRAM outperforms the state-of-the-art RRAM for tensor completion in the TT format from Steinlechner (2016) in terms of computation time."],"url":"http://arxiv.org/abs/2402.12182v1","category":"math.OC"}
{"created":"2024-02-19 14:26:58","title":"Second-order flows for approaching stationary points of a class of non-convex energies via convex-splitting schemes","abstract":"The use of accelerated gradient flows is an emerging field in optimization, scientific computing and beyond. This paper contributes to the theoretical underpinnings of a recently-introduced computational paradigm known as second-order flows, which demonstrate significant performance particularly for the minimization of non-convex energy functionals defined on Sobolev spaces, and are characterized by novel dissipative hyperbolic partial differential equations. Our approach hinges upon convex-splitting schemes, a tool which is not only pivotal for clarifying the well-posedness of second-order flows, but also yields a versatile array of robust numerical schemes through temporal and spatial discretization. We prove the convergence to stationary points of such schemes in the semi-discrete setting. Further, we establish their convergence to time-continuous solutions as the time-step tends to zero, and perform a comprehensive error analysis in the fully discrete case. Finally, these algorithms undergo thorough testing and validation in approaching stationary points of non-convex variational models in applied sciences, such as the Ginzburg-Landau energy in phase-field modeling and a specific case of the Landau-de Gennes energy of the Q-tensor model for liquid crystals.","sentences":["The use of accelerated gradient flows is an emerging field in optimization, scientific computing and beyond.","This paper contributes to the theoretical underpinnings of a recently-introduced computational paradigm known as second-order flows, which demonstrate significant performance particularly for the minimization of non-convex energy functionals defined on Sobolev spaces, and are characterized by novel dissipative hyperbolic partial differential equations.","Our approach hinges upon convex-splitting schemes, a tool which is not only pivotal for clarifying the well-posedness of second-order flows, but also yields a versatile array of robust numerical schemes through temporal and spatial discretization.","We prove the convergence to stationary points of such schemes in the semi-discrete setting.","Further, we establish their convergence to time-continuous solutions as the time-step tends to zero, and perform a comprehensive error analysis in the fully discrete case.","Finally, these algorithms undergo thorough testing and validation in approaching stationary points of non-convex variational models in applied sciences, such as the Ginzburg-Landau energy in phase-field modeling and a specific case of the Landau-de Gennes energy of the Q-tensor model for liquid crystals."],"url":"http://arxiv.org/abs/2402.12173v1","category":"math.NA"}
{"created":"2024-02-19 14:26:41","title":"Large $N$ Schur index of $\\mathcal N=4$ SYM from semiclassical D3 brane","abstract":"We consider the refined Schur superconformal index of 4d $\\mathcal N=4$ $U(N)$ SYM and the first term of its giant-graviton expansion, first predicted in arXiv:2001.11667 using indirect superconformal algebra considerations and analytic continuation of fugacities. This correction is the leading non-perturbative correction to the index at large $N$ and we reproduce it from the semiclassical partition function of quantum D3 brane wrapped on $S^{1}\\times S^{3}$ in a twisted modification of the $AdS_{5}\\times S^{5}$ string background, depending on the index R-symmetry fugacity. Our calculation does not exploit directly supersymmetry. It is based on the determination of the partition function of the various bosonic and fermionic fluctuations on the wrapped brane whose action is conformal with specific constant holonomies along thermal cycle. We show how those partition functions may be obtained by adapting the operator counting method of Cardy to the twisted background.","sentences":["We consider the refined Schur superconformal index of 4d $\\mathcal N=4$ $U(N)$ SYM and the first term of its giant-graviton expansion, first predicted in arXiv:2001.11667 using indirect superconformal algebra considerations and analytic continuation of fugacities.","This correction is the leading non-perturbative correction to the index at large $N$ and we reproduce it from the semiclassical partition function of quantum D3 brane wrapped on $S^{1}\\times S^{3}$ in a twisted modification of the $AdS_{5}\\times S^{5}$ string background, depending on the index R-symmetry fugacity.","Our calculation does not exploit directly supersymmetry.","It is based on the determination of the partition function of the various bosonic and fermionic fluctuations on the wrapped brane whose action is conformal with specific constant holonomies along thermal cycle.","We show how those partition functions may be obtained by adapting the operator counting method of Cardy to the twisted background."],"url":"http://arxiv.org/abs/2402.12172v1","category":"hep-th"}
{"created":"2024-02-19 13:16:09","title":"TASTE V. A new ground-based investigation of orbital decay in the ultra-hot Jupiter WASP-12b","abstract":"The discovery of the first transiting hot Jupiters (HJs; giant planets on orbital periods shorter than $P\\sim10$ days) was announced more than twenty years ago. As both ground- and space-based follow-up observations are piling up, we are approaching the temporal baseline required to detect secular variations in their orbital parameters. In particular, several recent studies focused on constraining the efficiency of the tidal decay mechanism to better understand the evolutionary time scales of HJ migration and engulfment. This can be achieved by measuring a monotonic decrease of orbital period $\\mathrm{d}P/\\mathrm{d}t<0$ due to mechanical energy being dissipated by tidal friction. WASP-12b was the first HJ for which a tidal decay scenario appeared convincing, even though alternative explanations have been hypothesized. Here we present a new analysis based on 28 unpublished high-precision transit light curves gathered over a twelve-year baseline and combined with all the available archival data, and an updated set of stellar parameters from HARPS-N high-resolution spectra, which are consistent with a main sequence scenario, close to the hydrogen exhaustion in the core. Our values of $\\mathrm{d}P/\\mathrm{d}t$ = $-30.72 \\pm 2.67$ and $Q_{\\ast}^{'}$ = $(2.13 \\pm 0.18) \\times 10^{5}$ are statistically consistent with previous studies, and indicate that WASP-12b is undergoing fast tidal dissipation. We additionally report the presence of an excess scatter in the timing data and discuss its possible origin.","sentences":["The discovery of the first transiting hot Jupiters (HJs; giant planets on orbital periods shorter than $P\\sim10$ days) was announced more than twenty years ago.","As both ground- and space-based follow-up observations are piling up, we are approaching the temporal baseline required to detect secular variations in their orbital parameters.","In particular, several recent studies focused on constraining the efficiency of the tidal decay mechanism to better understand the evolutionary time scales of HJ migration and engulfment.","This can be achieved by measuring a monotonic decrease of orbital period $\\mathrm{d}P/\\mathrm{d}t<0$ due to mechanical energy being dissipated by tidal friction.","WASP-12b was the first HJ for which a tidal decay scenario appeared convincing, even though alternative explanations have been hypothesized.","Here we present a new analysis based on 28 unpublished high-precision transit light curves gathered over a twelve-year baseline and combined with all the available archival data, and an updated set of stellar parameters from HARPS-N high-resolution spectra, which are consistent with a main sequence scenario, close to the hydrogen exhaustion in the core.","Our values of $\\mathrm{d}P/\\mathrm{d}t$ = $-30.72 \\pm 2.67$ and $Q_{\\ast}^{'}$ = $(2.13 \\pm 0.18)","\\times 10^{5}$ are statistically consistent with previous studies, and indicate that WASP-12b is undergoing fast tidal dissipation.","We additionally report the presence of an excess scatter in the timing data and discuss its possible origin."],"url":"http://arxiv.org/abs/2402.12120v1","category":"astro-ph.EP"}
{"created":"2024-02-19 12:11:20","title":"Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement","abstract":"This letter studies the adaptive Decentralized Congestion Control (DCC) algorithm defined in the ETSI TS 102 687 V1.2.1 specification. We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance. We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations. Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and fairness.","sentences":["This letter studies the adaptive Decentralized Congestion Control (DCC)","algorithm defined in the ETSI TS 102 687","V1.2.1 specification.","We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance.","We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations.","Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and fairness."],"url":"http://arxiv.org/abs/2402.12089v1","category":"cs.NI"}
{"created":"2024-02-19 11:02:05","title":"Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models","abstract":"Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the \"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to \"decorate the patch\", enhancing the model's performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.","sentences":["Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks.","This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor.","Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning.","Specifically, we derive a sparse mask to identify the \"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis.","Subsequently, a compensation mechanism is introduced to \"decorate the patch\", enhancing the model's performance on both target and original tasks.","Additionally, our method is adaptable to multi-task scenarios.","Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities."],"url":"http://arxiv.org/abs/2402.12048v1","category":"cs.CL"}
{"created":"2024-02-19 10:43:27","title":"Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics","abstract":"Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.","sentences":["Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks.","Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT.","However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes.","In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains.","Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure.","Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language.","Pre-trained language models and code are freely available for use."],"url":"http://arxiv.org/abs/2402.12036v1","category":"cs.CL"}
{"created":"2024-02-19 09:18:43","title":"Going Beyond Perfect Absorption: Reconfigurable Super-directive Absorbers","abstract":"In the context of electromagnetic absorption, it is obvious that for an infinite planar periodic structure illuminated by a plane wave, the maximum attainable absorptance, i.e., perfect absorption, is theoretically limited to 100% of the incident power. Here we show that an intriguing possibility of overcoming this limit arises in finite-size resonant absorbing arrays. We present a comprehensive analysis of a simple two-dimensional strip array over an infinite perfectly conducting plane, where the strips are loaded by reconfigurable impedance loads. The absorptance is defined as the ratio of the dissipated power per unit length of the strips to the incident power on the unit length of the array width. The results show that even regular arrays of impedance strips can slightly overcome the limit of 100% absorptance, while using aperiodic arrays with optimized loads, absorptance can be significantly increased as compared with the scenario where the strips are identical. In principle, by tuning the reconfigurable loads, high super-unity absorptance can be realized for all angles of illumination.","sentences":["In the context of electromagnetic absorption, it is obvious that for an infinite planar periodic structure illuminated by a plane wave, the maximum attainable absorptance, i.e., perfect absorption, is theoretically limited to 100% of the incident power.","Here we show that an intriguing possibility of overcoming this limit arises in finite-size resonant absorbing arrays.","We present a comprehensive analysis of a simple two-dimensional strip array over an infinite perfectly conducting plane, where the strips are loaded by reconfigurable impedance loads.","The absorptance is defined as the ratio of the dissipated power per unit length of the strips to the incident power on the unit length of the array width.","The results show that even regular arrays of impedance strips can slightly overcome the limit of 100% absorptance, while using aperiodic arrays with optimized loads, absorptance can be significantly increased as compared with the scenario where the strips are identical.","In principle, by tuning the reconfigurable loads, high super-unity absorptance can be realized for all angles of illumination."],"url":"http://arxiv.org/abs/2402.11971v1","category":"physics.app-ph"}
{"created":"2024-02-19 07:48:29","title":"One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation","abstract":"Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.","sentences":["Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability.","This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user.","We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images.","Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs.","We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation.","Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation."],"url":"http://arxiv.org/abs/2402.11909v1","category":"cs.CV"}
{"created":"2024-02-19 07:22:29","title":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning","abstract":"Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively.","sentences":["Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time.","Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs.","Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks.","In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual.","SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance.","Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively."],"url":"http://arxiv.org/abs/2402.11896v1","category":"cs.CL"}
{"created":"2024-02-19 06:55:50","title":"Generative Semi-supervised Graph Anomaly Detection","abstract":"This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and as a result, they fail to take account of the graph structure information. Our approach tackles this problem by generating graph structure-aware outlier nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space. Comprehensive experiments on four real-world datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes. Code will be made available at https://github.com/mala-lab/GGAD.","sentences":["This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph.","As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting.","However, their utilization of these normal nodes is limited.","In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes.","The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier.","There have been many generative anomaly detection approaches, but they are designed for non-graph data, and as a result, they fail to take account of the graph structure information.","Our approach tackles this problem by generating graph structure-aware outlier nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space.","Comprehensive experiments on four real-world datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes.","Code will be made available at https://github.com/mala-lab/GGAD."],"url":"http://arxiv.org/abs/2402.11887v1","category":"cs.LG"}
{"created":"2024-02-19 06:22:09","title":"LoRA Training in the NTK Regime has No Spurious Local Minima","abstract":"Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.","sentences":["Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited.","In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well."],"url":"http://arxiv.org/abs/2402.11867v1","category":"cs.LG"}
{"created":"2024-02-19 05:15:13","title":"Modularized Networks for Few-shot Hateful Meme Detection","abstract":"In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.","sentences":["In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available.","Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique.","We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules.","These modules are capable of essential reasoning skills for hateful meme detection.","We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance.","The model's learnable parameters are directly proportional to the number of LoRA modules.","This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection.","Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context.","The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.","We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance.","The model's learnable parameters are directly proportional to the number of LoRA modules.","This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection.","Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context.","The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference."],"url":"http://arxiv.org/abs/2402.11845v1","category":"cs.CL"}
{"created":"2024-02-19 04:13:33","title":"Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before","abstract":"Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features. Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework. In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures.","sentences":["Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data.","However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information.","With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks.","To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework.","Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features.","Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework.","In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures."],"url":"http://arxiv.org/abs/2402.11816v1","category":"cs.CV"}
{"created":"2024-02-19 01:26:22","title":"Autonomous Hyperspectral Characterisation Station: Robotically Assisted Characterisation of Polymer Degradation","abstract":"This paper addresses the gap between the capabilities and utilisation of robotics and automation in laboratory settings and builds upon the concept of Self Driving Labs (SDL). %to significantly impact laboratory operations. We introduce an innovative approach to the temporal characterisation of materials. The article discusses the challenges posed by manual methods involving established laboratory equipment and presents an automated hyperspectral characterisation station. This station integrates robot-aided hyperspectral imaging, complex material characterisation modeling, and automated data analysis, offering a non-destructive and comprehensive approach. This work explains how the proposed assembly can automatically measure the half-life of biodegradable polymers with higher throughput and accuracy than manual methods. The investigation explores the effect of pH, number of average molecular weight (Mn), end groups, and blends on the degradation rate of polylactic acid (PLA). The contributions of the paper lie in introducing an adaptable classification station for novel characterisation methods and presenting an innovative methodology for polymer degradation rate measurement. The proposed system has the potential to accelerate the development of high-throughput screening and characterisation methods in material and chemistry laboratories.","sentences":["This paper addresses the gap between the capabilities and utilisation of robotics and automation in laboratory settings and builds upon the concept of Self Driving Labs (SDL).","%to significantly impact laboratory operations.","We introduce an innovative approach to the temporal characterisation of materials.","The article discusses the challenges posed by manual methods involving established laboratory equipment and presents an automated hyperspectral characterisation station.","This station integrates robot-aided hyperspectral imaging, complex material characterisation modeling, and automated data analysis, offering a non-destructive and comprehensive approach.","This work explains how the proposed assembly can automatically measure the half-life of biodegradable polymers with higher throughput and accuracy than manual methods.","The investigation explores the effect of pH, number of average molecular weight (Mn), end groups, and blends on the degradation rate of polylactic acid (PLA).","The contributions of the paper lie in introducing an adaptable classification station for novel characterisation methods and presenting an innovative methodology for polymer degradation rate measurement.","The proposed system has the potential to accelerate the development of high-throughput screening and characterisation methods in material and chemistry laboratories."],"url":"http://arxiv.org/abs/2402.11763v1","category":"eess.SY"}
{"created":"2024-02-18 23:29:28","title":"LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection","abstract":"We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods.","sentences":["We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors.","To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network.","We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods."],"url":"http://arxiv.org/abs/2402.11735v1","category":"cs.RO"}
{"created":"2024-02-18 21:25:09","title":"MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization","abstract":"RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions.","sentences":["RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions.","However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks.","Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature.","In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously.","We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions.","Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions."],"url":"http://arxiv.org/abs/2402.11711v1","category":"cs.CL"}
{"created":"2024-02-18 19:01:02","title":"Adaptively Learning Memory Incorporating PSO","abstract":"Selection of perefect parameters for low-pass filters can sometimes be an expensive problem with no analytical solution or differentiability of cost function. In this paper, we introduce a new PSO-inspired algorithm, that incorporates the positive experiences of the swarm to learn the geometry of the search space,thus obtaining the ability to consistently reach global optimum and is especially suitable for nonsmooth semiconvex functions optimization. We compare it to a set of other algorithms on test functions of choice to prove it's suitability to a certain range of problems, and then apply it to the problem of finding perfect parameters for exponential smoothing algorithm.","sentences":["Selection of perefect parameters for low-pass filters can sometimes be an expensive problem with no analytical solution or differentiability of cost function.","In this paper, we introduce a new PSO-inspired algorithm, that incorporates the positive experiences of the swarm to learn the geometry of the search space,thus obtaining the ability to consistently reach global optimum and is especially suitable for nonsmooth semiconvex functions optimization.","We compare it to a set of other algorithms on test functions of choice to prove it's suitability to a certain range of problems, and then apply it to the problem of finding perfect parameters for exponential smoothing algorithm."],"url":"http://arxiv.org/abs/2402.11679v1","category":"math.OC"}
{"created":"2024-02-18 18:10:44","title":"Hybrid Kerr-electro-optic frequency combs on thin-film lithium niobate","abstract":"Optical frequency combs are indispensable links between the optical and microwave domains, enabling a wide range of applications including precision spectroscopy, ultrastable frequency generation, and timekeeping. Chip-scale integration miniaturizes bulk implementations onto photonic chips, offering highly compact, stable, and power-efficient frequency comb sources. State of the art integrated frequency comb sources are based on resonantly-enhanced Kerr effect and, more recently, on electro-optic effect. While the former can routinely reach octave-spanning bandwidths and the latter feature microwave-rate spacings, achieving both in the same material platform has been challenging. Here, we leverage both strong Kerr nonlinearity and efficient electro-optic phase modulation available in the ultralow-loss thin-film lithium niobate photonic platform, to demonstrate a hybrid Kerr-electro-optic frequency comb with stabilized spacing. In our approach, a dissipative Kerr soliton is first generated, and then electro-optic division is used to realize a frequency comb with 2,589 comb lines spaced by 29.308 GHz and spanning 75.9 THz (588 nm) end-to-end. Further, we demonstrate electronic stabilization and control of the soliton spacing, naturally facilitated by our approach. The broadband, microwave-rate comb in this work overcomes the spacing-span tradeoff that exists in all integrated frequency comb sources, and paves the way towards chip-scale solutions for complex tasks such as laser spectroscopy covering multiple bands, micro- and millimeter-wave generation, and massively parallel optical communications.","sentences":["Optical frequency combs are indispensable links between the optical and microwave domains, enabling a wide range of applications including precision spectroscopy, ultrastable frequency generation, and timekeeping.","Chip-scale integration miniaturizes bulk implementations onto photonic chips, offering highly compact, stable, and power-efficient frequency comb sources.","State of the art integrated frequency comb sources are based on resonantly-enhanced Kerr effect and, more recently, on electro-optic effect.","While the former can routinely reach octave-spanning bandwidths and the latter feature microwave-rate spacings, achieving both in the same material platform has been challenging.","Here, we leverage both strong Kerr nonlinearity and efficient electro-optic phase modulation available in the ultralow-loss thin-film lithium niobate photonic platform, to demonstrate a hybrid Kerr-electro-optic frequency comb with stabilized spacing.","In our approach, a dissipative Kerr soliton is first generated, and then electro-optic division is used to realize a frequency comb with 2,589 comb lines spaced by 29.308 GHz and spanning 75.9 THz (588 nm) end-to-end.","Further, we demonstrate electronic stabilization and control of the soliton spacing, naturally facilitated by our approach.","The broadband, microwave-rate comb in this work overcomes the spacing-span tradeoff that exists in all integrated frequency comb sources, and paves the way towards chip-scale solutions for complex tasks such as laser spectroscopy covering multiple bands, micro- and millimeter-wave generation, and massively parallel optical communications."],"url":"http://arxiv.org/abs/2402.11669v1","category":"physics.optics"}
{"created":"2024-02-18 18:05:36","title":"Fast-forwarding molecular ground state preparation with optimal control on analog quantum simulators","abstract":"We show that optimal control of the electron dynamics is able to prepare molecular ground states, within chemical accuracy, with evolution times approaching the bounds imposed by quantum mechanics. We propose a specific parameterization of the molecular evolution only in terms of interaction already present in the molecular Hamiltonian. Thus, the proposed method solely utilizes quantum simulation routines, retaining their favourable scalings. Due to the intimate relationships between variational quantum algorithms and optimal control we compare, when possible, our results with state-of-the-art methods in literature. We found that the number of parameters needed to reach chemical accuracy and algorithmic scaling are in line with compact adaptive strategies to build variational ansatze. The algorithm, which is also suitable for quantum simulators, is implemented emulating a digital quantum processor (up to 16 qubits) and tested on different molecules and geometries spanning different degrees of electron correlation.","sentences":["We show that optimal control of the electron dynamics is able to prepare molecular ground states, within chemical accuracy, with evolution times approaching the bounds imposed by quantum mechanics.","We propose a specific parameterization of the molecular evolution only in terms of interaction already present in the molecular Hamiltonian.","Thus, the proposed method solely utilizes quantum simulation routines, retaining their favourable scalings.","Due to the intimate relationships between variational quantum algorithms and optimal control we compare, when possible, our results with state-of-the-art methods in literature.","We found that the number of parameters needed to reach chemical accuracy and algorithmic scaling are in line with compact adaptive strategies to build variational ansatze.","The algorithm, which is also suitable for quantum simulators, is implemented emulating a digital quantum processor (up to 16 qubits) and tested on different molecules and geometries spanning different degrees of electron correlation."],"url":"http://arxiv.org/abs/2402.11667v1","category":"quant-ph"}
{"created":"2024-02-18 16:20:43","title":"Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs","abstract":"Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances. Furthermore, we propose SOLID-RL, which is further trained to generate a dialog in one step on the data generated by SOLID. We propose a length-based quality estimation mechanism to assign varying weights to SOLID-generated dialogs based on their quality during the training process of SOLID-RL. We use SOLID and SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size of existing datasets. Experiments show that IP methods trained on dialogs generated by SOLID and SOLID-RL achieve better IP quality than ones trained on human-generated dialogs.","sentences":["Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs.","Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training.","However, manually annotating intents is resource-intensive.","While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs.","In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs.","We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes.","The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances.","Furthermore, we propose SOLID-RL, which is further trained to generate a dialog in one step on the data generated by SOLID.","We propose a length-based quality estimation mechanism to assign varying weights to SOLID-generated dialogs based on their quality during the training process of SOLID-RL.","We use SOLID and SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size of existing datasets.","Experiments show that IP methods trained on dialogs generated by SOLID and SOLID-RL achieve better IP quality than ones trained on human-generated dialogs."],"url":"http://arxiv.org/abs/2402.11633v1","category":"cs.CL"}
{"created":"2024-02-18 14:49:48","title":"Finite-frequency normal and superfluid drag effects in two-component atomic Bose-Einstein condensates","abstract":"Two-component systems consisting of mutually interacting particles can demonstrate both intracomponent transport effects and intercomponent entrainment (or drag) effects. In the presence of superfluidity, the intracomponent transport is characterized by dissipative conductivity and superfluid weight in the framework of two-fluid model, and intercomponent entrainment gives rise to normal and nondissipative drag effects. We present unified treatment of all these effects for spatially homogeneous two-component atomic Bose-Einstein condensates based on the Bogoliubov theory, focusing specifically on the drag effects. Calculating finite-frequency intra- and intercomponent conductivities with taking into account quasiparticle damping, we derive and numerically check analytical Drude-like approximations applicable at low frequencies, and Lorentz-like approximations applicable at higher frequencies in vicinity of the resonant energy of spin-to-density Bogoliubov quasiparticle conversion. As possible physical realizations of two-component atomic systems, we consider three-dimensional Bose-Bose mixtures and closely spaced two-layered systems of magnetic dipolar atoms.","sentences":["Two-component systems consisting of mutually interacting particles can demonstrate both intracomponent transport effects and intercomponent entrainment (or drag) effects.","In the presence of superfluidity, the intracomponent transport is characterized by dissipative conductivity and superfluid weight in the framework of two-fluid model, and intercomponent entrainment gives rise to normal and nondissipative drag effects.","We present unified treatment of all these effects for spatially homogeneous two-component atomic Bose-Einstein condensates based on the Bogoliubov theory, focusing specifically on the drag effects.","Calculating finite-frequency intra- and intercomponent conductivities with taking into account quasiparticle damping, we derive and numerically check analytical Drude-like approximations applicable at low frequencies, and Lorentz-like approximations applicable at higher frequencies in vicinity of the resonant energy of spin-to-density Bogoliubov quasiparticle conversion.","As possible physical realizations of two-component atomic systems, we consider three-dimensional Bose-Bose mixtures and closely spaced two-layered systems of magnetic dipolar atoms."],"url":"http://arxiv.org/abs/2402.11606v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-18 13:03:11","title":"Using rule engine in self-healing systems and MAPE model","abstract":"Software malfunction presents a significant hurdle within the computing domain, carrying substantial risks for systems, enterprises, and users universally. To produce software with high reliability and quality, effective debugging is essential. Program debugging is an activity to reduce software maintenance costs. In this study, a failure repair method that uses a rule engine is presented. The simulation on mRUBIS showed that the proposed method could be efficient in the operational environment. Through a thorough grasp of software failure and the adoption of efficient mitigation strategies, stakeholders can bolster the dependability, security, and adaptability of software systems. This, in turn, reduces the repercussions of failures and cultivates increased confidence in digital technologies.","sentences":["Software malfunction presents a significant hurdle within the computing domain, carrying substantial risks for systems, enterprises, and users universally.","To produce software with high reliability and quality, effective debugging is essential.","Program debugging is an activity to reduce software maintenance costs.","In this study, a failure repair method that uses a rule engine is presented.","The simulation on mRUBIS showed that the proposed method could be efficient in the operational environment.","Through a thorough grasp of software failure and the adoption of efficient mitigation strategies, stakeholders can bolster the dependability, security, and adaptability of software systems.","This, in turn, reduces the repercussions of failures and cultivates increased confidence in digital technologies."],"url":"http://arxiv.org/abs/2402.11581v1","category":"cs.SE"}
{"created":"2024-02-18 12:31:29","title":"A novel Fourier neural operator framework for classification of multi-sized images: Application to 3D digital porous media","abstract":"Fourier neural operators (FNOs) are invariant with respect to the size of input images, and thus images with any size can be fed into FNO-based frameworks without any modification of network architectures, in contrast to traditional convolutional neural networks (CNNs). Leveraging the advantage of FNOs, we propose a novel deep-learning framework for classifying images with varying sizes. Particularly, we simultaneously train the proposed network on multi-sized images. As a practical application, we consider the problem of predicting the label (e.g., permeability) of three-dimensional digital porous media. To construct the framework, an intuitive approach is to connect FNO layers to a classifier using adaptive max pooling. First, we show that this approach is only effective for porous media with fixed sizes, whereas it fails for porous media of varying sizes. To overcome this limitation, we introduce our approach: instead of using adaptive max pooling, we use static max pooling with the size of channel width of FNO layers. Since the channel width of the FNO layers is independent of input image size, the introduced framework can handle multi-sized images during training. We show the effectiveness of the introduced framework and compare its performance with the intuitive approach through the example of the classification of three-dimensional digital porous media of varying sizes.","sentences":["Fourier neural operators (FNOs) are invariant with respect to the size of input images, and thus images with any size can be fed into FNO-based frameworks without any modification of network architectures, in contrast to traditional convolutional neural networks (CNNs).","Leveraging the advantage of FNOs, we propose a novel deep-learning framework for classifying images with varying sizes.","Particularly, we simultaneously train the proposed network on multi-sized images.","As a practical application, we consider the problem of predicting the label (e.g., permeability) of three-dimensional digital porous media.","To construct the framework, an intuitive approach is to connect FNO layers to a classifier using adaptive max pooling.","First, we show that this approach is only effective for porous media with fixed sizes, whereas it fails for porous media of varying sizes.","To overcome this limitation, we introduce our approach: instead of using adaptive max pooling, we use static max pooling with the size of channel width of FNO layers.","Since the channel width of the FNO layers is independent of input image size, the introduced framework can handle multi-sized images during training.","We show the effectiveness of the introduced framework and compare its performance with the intuitive approach through the example of the classification of three-dimensional digital porous media of varying sizes."],"url":"http://arxiv.org/abs/2402.11568v1","category":"cs.CV"}
{"created":"2024-02-18 12:02:25","title":"UD-based pairwise and MIMO Kalman-like filtering for estimation of econometric model structures","abstract":"One of the modern research lines in econometrics studies focuses on translating a wide variety of structural econometric models into their state-space form, which allows for efficient unknown dynamic system state and parameter estimations by the Kalman filtering scheme. The mentioned trend yields advanced state-space model structures, which demand innovative estimation techniques driven by application requirements to be devised. This paper explores both the linear time-invariant multiple-input, multiple-output system (LTI MIMO) and the pairwise Markov model (PMM) with the related pairwise Kalman filter (PKF). In particular, we design robust gradient-based adaptive Kalman-like filtering methods for the simultaneous state and parameter estimation in the outlined model structures. Our methods are fast and accurate because their analytically computed gradient is utilized in the calculation instead of its numerical approximation. Also, these employ the numerically robust $UDU^\\top$-factorization-based Kalman filter implementation, which is reliable in practice. Our novel techniques are examined on numerical examples and used for treating one stochastic model in econometrics.","sentences":["One of the modern research lines in econometrics studies focuses on translating a wide variety of structural econometric models into their state-space form, which allows for efficient unknown dynamic system state and parameter estimations by the Kalman filtering scheme.","The mentioned trend yields advanced state-space model structures, which demand innovative estimation techniques driven by application requirements to be devised.","This paper explores both the linear time-invariant multiple-input, multiple-output system (LTI MIMO) and the pairwise Markov model (PMM) with the related pairwise Kalman filter (PKF).","In particular, we design robust gradient-based adaptive Kalman-like filtering methods for the simultaneous state and parameter estimation in the outlined model structures.","Our methods are fast and accurate because their analytically computed gradient is utilized in the calculation instead of its numerical approximation.","Also, these employ the numerically robust $UDU^\\top$-factorization-based Kalman filter implementation, which is reliable in practice.","Our novel techniques are examined on numerical examples and used for treating one stochastic model in econometrics."],"url":"http://arxiv.org/abs/2402.11560v1","category":"math.OC"}
{"created":"2024-02-18 07:10:02","title":"DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics","abstract":"Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens. We carry out experiments using various LLM models on a comprehensive real-world medical laboratory report dataset for automatic diagnosis generation, our findings illustrate that DictLLM significantly outperforms established baseline methods and few-shot GPT-4 implementations in terms of both Rouge-L and Knowledge F1 scores. Furthermore, our evaluation of the framework's scalability and robustness, through a series of experiments, underscores its exceptional capability in accurately modeling the complex key-value data structure of medical dictionary data.","sentences":["Structured data offers a sophisticated mechanism for the organization of information.","Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data.","These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes.","In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses.","DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens.","We carry out experiments using various LLM models on a comprehensive real-world medical laboratory report dataset for automatic diagnosis generation, our findings illustrate that DictLLM significantly outperforms established baseline methods and few-shot GPT-4 implementations in terms of both Rouge-L and Knowledge F1 scores.","Furthermore, our evaluation of the framework's scalability and robustness, through a series of experiments, underscores its exceptional capability in accurately modeling the complex key-value data structure of medical dictionary data."],"url":"http://arxiv.org/abs/2402.11481v1","category":"cs.CL"}
{"created":"2024-02-18 05:50:19","title":"Adaptive Decision-Making for Autonomous Vehicles: A Learning-Enhanced Game-Theoretic Approach in Interactive Environments","abstract":"This paper proposes an adaptive behavioral decision-making method for autonomous vehicles (AVs) focusing on complex merging scenarios. Leveraging principles from non-cooperative game theory, we develop a vehicle interaction behavior model that defines key traffic elements and integrates a multifactorial reward function. Maximum entropy inverse reinforcement learning (IRL) is employed for behavior model parameter optimization. Optimal matching parameters can be obtained using the interaction behavior feature vector and the behavior probabilities output by the vehicle interaction model. Further, a behavioral decision-making method adapted to dynamic environments is proposed. By establishing a mapping model between multiple environmental variables and model parameters, it enables parameters online learning and recognition, and achieves to output interactive behavior probabilities of AVs. Quantitative analysis employing naturalistic driving datasets (highD and exiD) and real-vehicle test data validates the model's high consistency with human decision-making. In 188 tested interaction scenarios, the average human-like similarity rate is 81.73%, with a notable 83.12% in the highD dataset. Furthermore, in 145 dynamic interactions, the method matches human decisions at 77.12%, with 6913 consistence instances. Moreover, in real-vehicle tests, a 72.73% similarity with 0% safety violations are obtained. Results demonstrate the effectiveness of our proposed method in enabling AVs to make informed adaptive behavior decisions in interactive environments.","sentences":["This paper proposes an adaptive behavioral decision-making method for autonomous vehicles (AVs) focusing on complex merging scenarios.","Leveraging principles from non-cooperative game theory, we develop a vehicle interaction behavior model that defines key traffic elements and integrates a multifactorial reward function.","Maximum entropy inverse reinforcement learning (IRL) is employed for behavior model parameter optimization.","Optimal matching parameters can be obtained using the interaction behavior feature vector and the behavior probabilities output by the vehicle interaction model.","Further, a behavioral decision-making method adapted to dynamic environments is proposed.","By establishing a mapping model between multiple environmental variables and model parameters, it enables parameters online learning and recognition, and achieves to output interactive behavior probabilities of AVs.","Quantitative analysis employing naturalistic driving datasets (highD and exiD) and real-vehicle test data validates the model's high consistency with human decision-making.","In 188 tested interaction scenarios, the average human-like similarity rate is 81.73%, with a notable 83.12% in the highD dataset.","Furthermore, in 145 dynamic interactions, the method matches human decisions at 77.12%, with 6913 consistence instances.","Moreover, in real-vehicle tests, a 72.73% similarity with 0% safety violations are obtained.","Results demonstrate the effectiveness of our proposed method in enabling AVs to make informed adaptive behavior decisions in interactive environments."],"url":"http://arxiv.org/abs/2402.11467v1","category":"cs.MA"}
{"created":"2024-02-18 04:16:24","title":"Learning to Learn Faster from Human Feedback with Language Model Predictive Control","abstract":"Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: https://robot-teaching.github.io/.","sentences":["Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks.","However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions.","In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful).","Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success.","This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9.","Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%.","See videos, code, and demos at: https://robot-teaching.github.io/."],"url":"http://arxiv.org/abs/2402.11450v1","category":"cs.RO"}
{"created":"2024-02-18 03:13:20","title":"The Assignment Game: New Mechanisms for Equitable Core Imputations","abstract":"The set of core imputations of the assignment game forms a (non-finite) distributive lattice. So far, efficient algorithms were known for computing only its two extreme imputations; however, each of them maximally favors one side and disfavors the other side of the bipartition, leading to inequitable profit sharing. Another issue is that a sub-coalition consisting of one player (or a set of players from the same side of the bipartition) can make zero profit, therefore a core imputation is not obliged to give them any profit. Hence core imputations make no fairness guarantee at the level of individual agents. This raises the question of computing {\\em more equitable core imputations}.   In this paper, we give combinatorial (i.e., the mechanism does not invoke an LP-solver) polynomial time mechanisms for computing the leximin and leximax core imputations for the assignment game. These imputations achieve ``fairness'' in different ways: whereas leximin tries to make poor agents more rich, leximax tries to make rich agents less rich. In general, the two imputations are different.   Our mechanisms were derived by a suitable adaptation of the classical primal-dual paradigm from combinatorial optimization. The ``engine'' driving them involves recent insights, obtained via complementarity, into core imputations \\cite{Va.New-characterizations} and the pristine combinatorial structure of matching. We have identified other natural games which could benefit from our approach.","sentences":["The set of core imputations of the assignment game forms a (non-finite) distributive lattice.","So far, efficient algorithms were known for computing only its two extreme imputations; however, each of them maximally favors one side and disfavors the other side of the bipartition, leading to inequitable profit sharing.","Another issue is that a sub-coalition consisting of one player (or a set of players from the same side of the bipartition) can make zero profit, therefore a core imputation is not obliged to give them any profit.","Hence core imputations make no fairness guarantee at the level of individual agents.","This raises the question of computing {\\em more equitable core imputations}.   ","In this paper, we give combinatorial (i.e., the mechanism does not invoke an LP-solver) polynomial time mechanisms for computing the leximin and leximax core imputations for the assignment game.","These imputations achieve ``fairness'' in different ways: whereas leximin tries to make poor agents more rich, leximax tries to make rich agents less rich.","In general, the two imputations are different.   ","Our mechanisms were derived by a suitable adaptation of the classical primal-dual paradigm from combinatorial optimization.","The ``engine'' driving them involves recent insights, obtained via complementarity, into core imputations \\cite{Va.","New-characterizations} and the pristine combinatorial structure of matching.","We have identified other natural games which could benefit from our approach."],"url":"http://arxiv.org/abs/2402.11437v1","category":"cs.GT"}
{"created":"2024-02-18 01:46:46","title":"Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework","abstract":"Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences. Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable. In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired knowledge upon learning new domain-specific knowledge (i.e., catastrophic forgetting). To address this, we propose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework, which utilizes a continuously evolving teacher model for knowledge transfer in each domain, rather than focusing solely on new domain knowledge. It deserves to be mentioned that we are the first to apply continual learning methods to the multi-domain CSC task. Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for improving the model performance.","sentences":["Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences.","Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable.","In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired knowledge upon learning new domain-specific knowledge (i.e., catastrophic forgetting).","To address this, we propose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework, which utilizes a continuously evolving teacher model for knowledge transfer in each domain, rather than focusing solely on new domain knowledge.","It deserves to be mentioned that we are the first to apply continual learning methods to the multi-domain CSC task.","Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for improving the model performance."],"url":"http://arxiv.org/abs/2402.11422v1","category":"cs.CL"}
{"created":"2024-02-18 01:40:34","title":"Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction","abstract":"Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences. Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by the subjectivity of the CGEC task. In particular, our work is also an active exploration of how LLMs and small models better collaborate in downstream tasks. Extensive experiments and detailed analyses on widely used datasets verify the effectiveness of our thinking intuition and the proposed methods.","sentences":["Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks.","As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences.","Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus.","To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC.","Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance.","We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by the subjectivity of the CGEC task.","In particular, our work is also an active exploration of how LLMs and small models better collaborate in downstream tasks.","Extensive experiments and detailed analyses on widely used datasets verify the effectiveness of our thinking intuition and the proposed methods."],"url":"http://arxiv.org/abs/2402.11420v1","category":"cs.CL"}
{"created":"2024-02-18 01:20:00","title":"LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models","abstract":"Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical results demonstrate that the proposed method effectively improves training efficiency, enjoys better multi-task learning performance, and enhances the anti-overfitting capability. Plug-and-play codes built upon the Huggingface framework and PEFT library will be released.","sentences":["Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance.","However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs).","To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition.","Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs.","The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors.","LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\\times$ fewer parameters on the LLaMA-2-7B models.","Furthermore, empirical results demonstrate that the proposed method effectively improves training efficiency, enjoys better multi-task learning performance, and enhances the anti-overfitting capability.","Plug-and-play codes built upon the Huggingface framework and PEFT library will be released."],"url":"http://arxiv.org/abs/2402.11417v1","category":"cs.CL"}
{"created":"2024-02-17 23:08:32","title":"GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation","abstract":"Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight distribution and put more weightage on non-text nodes. We encode the complete graph as a knowledge representation and transfer it from the teacher to the student through the proposed distillation loss by effectively capturing both local and global information concurrently. Extensive experimentation on competitive benchmarks demonstrates that the proposed framework outperforms the current state-of-the-art approaches. The code will be available at: https://github.com/ayanban011/GraphKD.","sentences":["Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements.","Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices.","Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts.","Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image.","Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions.","Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight distribution and put more weightage on non-text nodes.","We encode the complete graph as a knowledge representation and transfer it from the teacher to the student through the proposed distillation loss by effectively capturing both local and global information concurrently.","Extensive experimentation on competitive benchmarks demonstrates that the proposed framework outperforms the current state-of-the-art approaches.","The code will be available at: https://github.com/ayanban011/GraphKD."],"url":"http://arxiv.org/abs/2402.11401v1","category":"cs.CV"}
{"created":"2024-02-17 21:58:28","title":"A soluble model for synchronized rhythmic activity in ant colonies","abstract":"Synchronization is one of the most striking instances of collective behavior, occurring in many natural phenomena. For example, in some ant species, ants are inactive within the nest most of the time, but their bursts of activity are highly synchronized and involve the entire nest population. Here we revisit a simulation model that generates this synchronized rhythmic activity through autocatalytic behavior, i.e., active ants can activate inactive ants, followed by a period of rest. We derive a set of delay differential equations that accurately describe the simulations for large ant colonies. Analysis of the fixed-point solutions, complemented by numerical integration of the equations, indicates the existence of stable limit-cycle solutions when the rest period is greater than a threshold and the event of spontaneous activation of inactive ants is very unlikely, so that most of the arousal of ants is done by active ants. Furthermore, we argue that the synchronized rhythmic activity observed in the simulations for small (but realistic) colony sizes is a finite-size effect.","sentences":["Synchronization is one of the most striking instances of collective behavior, occurring in many natural phenomena.","For example, in some ant species, ants are inactive within the nest most of the time, but their bursts of activity are highly synchronized and involve the entire nest population.","Here we revisit a simulation model that generates this synchronized rhythmic activity through autocatalytic behavior, i.e., active ants can activate inactive ants, followed by a period of rest.","We derive a set of delay differential equations that accurately describe the simulations for large ant colonies.","Analysis of the fixed-point solutions, complemented by numerical integration of the equations, indicates the existence of stable limit-cycle solutions when the rest period is greater than a threshold and the event of spontaneous activation of inactive ants is very unlikely, so that most of the arousal of ants is done by active ants.","Furthermore, we argue that the synchronized rhythmic activity observed in the simulations for small (but realistic) colony sizes is a finite-size effect."],"url":"http://arxiv.org/abs/2402.11391v1","category":"nlin.AO"}
{"created":"2024-02-17 21:35:13","title":"Reinforcement learning to maximise wind turbine energy generation","abstract":"We propose a reinforcement learning strategy to control wind turbine energy generation by actively changing the rotor speed, the rotor yaw angle and the blade pitch angle. A double deep Q-learning with a prioritized experience replay agent is coupled with a blade element momentum model and is trained to allow control for changing winds. The agent is trained to decide the best control (speed, yaw, pitch) for simple steady winds and is subsequently challenged with real dynamic turbulent winds, showing good performance. The double deep Q- learning is compared with a classic value iteration reinforcement learning control and both strategies outperform a classic PID control in all environments. Furthermore, the reinforcement learning approach is well suited to changing environments including turbulent/gusty winds, showing great adaptability. Finally, we compare all control strategies with real winds and compute the annual energy production. In this case, the double deep Q-learning algorithm also outperforms classic methodologies.","sentences":["We propose a reinforcement learning strategy to control wind turbine energy generation by actively changing the rotor speed, the rotor yaw angle and the blade pitch angle.","A double deep Q-learning with a prioritized experience replay agent is coupled with a blade element momentum model and is trained to allow control for changing winds.","The agent is trained to decide the best control (speed, yaw, pitch) for simple steady winds and is subsequently challenged with real dynamic turbulent winds, showing good performance.","The double deep Q- learning is compared with a classic value iteration reinforcement learning control and both strategies outperform a classic PID control in all environments.","Furthermore, the reinforcement learning approach is well suited to changing environments including turbulent/gusty winds, showing great adaptability.","Finally, we compare all control strategies with real winds and compute the annual energy production.","In this case, the double deep Q-learning algorithm also outperforms classic methodologies."],"url":"http://arxiv.org/abs/2402.11384v1","category":"cs.LG"}
{"created":"2024-02-17 20:53:41","title":"A bivariational, stable and convergent hierarchy for time-dependent coupled cluster with adaptive basis sets","abstract":"We propose a new formulation of time-dependent coupled cluster with adaptive basis functions and division of the one-particle space into active and secondary subspaces. The formalism is fully bivariational in the sense of a real-valued time-dependent bivariational principle and converges to the complete-active-space solution, a property that is obtained by the use of biorthogonal basis functions. A key and distinguishing feature of the theory is that the active bra and ket functions span the same space by construction. This ensures numerical stability and is achieved by employing a split unitary/non-unitary basis set transformation: The unitary part changes the active space itself, while the non-unitary part transforms the active basis. The formulation covers vibrational as well as electron dynamics. Detailed equations of motion are derived and implemented in the context of vibrational dynamics, and the numerical behavior is studied and compared to related methods.","sentences":["We propose a new formulation of time-dependent coupled cluster with adaptive basis functions and division of the one-particle space into active and secondary subspaces.","The formalism is fully bivariational in the sense of a real-valued time-dependent bivariational principle and converges to the complete-active-space solution, a property that is obtained by the use of biorthogonal basis functions.","A key and distinguishing feature of the theory is that the active bra and ket functions span the same space by construction.","This ensures numerical stability and is achieved by employing a split unitary/non-unitary basis set transformation: The unitary part changes the active space itself, while the non-unitary part transforms the active basis.","The formulation covers vibrational as well as electron dynamics.","Detailed equations of motion are derived and implemented in the context of vibrational dynamics, and the numerical behavior is studied and compared to related methods."],"url":"http://arxiv.org/abs/2402.11378v1","category":"physics.chem-ph"}
{"created":"2024-02-17 20:28:29","title":"Performance analysis of photodetectors based on 2D materials and heterostructures","abstract":"The unprecedented demand for sophisticated, self-powered, compact, ultrafast, cost-effective, and broadband light sensors for a myriad of applications has spurred a lot of research, precipitating in a slew of studies over the last decade. Apart from the photosensing ability of an active element in the light sensor, the device architecture is crucial in terms of photoinduced charge carrier generation and separation. Since the inception of graphene and the subsequent research growth in the atomically thin 2D materials, researchers have developed and adapted different families of 2D materials and device architectures, including single element 2D, 0D/2D, 2D/2D, 1D/ 2D stacked structures, and so on. This review discusses the recent reports on the light-sensing properties of various 2D materials, their heterostructures, and characteristics applicable to the ultraviolet-near infrared (UV-NIR), short-wave IR (SWIR), mid-wave IR (MWIR), long-wave IR (LWIR), and terahertz (THz) spectral ranges. It highlights the novelty of the burgeoning field, the heightened activity at the boundaries of engineering and materials science, particularly in the generation of charge carriers, their separation, and extraction, and the increased understanding of the underpinning science through modern experimental approaches. Devices based on the simultaneous effects of the pyro-phototronic effect (PPE) and the localized surface plasmon resonance (LSPR) effect, the photothermoelectric effect (PTE)-assisted photodetectors (PDs), waveguide-integrated silicon-2D PDs, metal-2D-metal PDs, and organic material PDs are examined rigorously. Theoretical treatment utilizing various computational approaches to investigate 2D materials and heterostructures for photodetection applications is also briefly discussed.","sentences":["The unprecedented demand for sophisticated, self-powered, compact, ultrafast, cost-effective, and broadband light sensors for a myriad of applications has spurred a lot of research, precipitating in a slew of studies over the last decade.","Apart from the photosensing ability of an active element in the light sensor, the device architecture is crucial in terms of photoinduced charge carrier generation and separation.","Since the inception of graphene and the subsequent research growth in the atomically thin 2D materials, researchers have developed and adapted different families of 2D materials and device architectures, including single element 2D, 0D/2D, 2D/2D, 1D/ 2D stacked structures, and so on.","This review discusses the recent reports on the light-sensing properties of various 2D materials, their heterostructures, and characteristics applicable to the ultraviolet-near infrared (UV-NIR), short-wave IR (SWIR), mid-wave IR (MWIR), long-wave IR (LWIR), and terahertz (THz) spectral ranges.","It highlights the novelty of the burgeoning field, the heightened activity at the boundaries of engineering and materials science, particularly in the generation of charge carriers, their separation, and extraction, and the increased understanding of the underpinning science through modern experimental approaches.","Devices based on the simultaneous effects of the pyro-phototronic effect (PPE) and the localized surface plasmon resonance (LSPR) effect, the photothermoelectric effect (PTE)-assisted photodetectors (PDs), waveguide-integrated silicon-2D PDs, metal-2D-metal PDs, and organic material PDs are examined rigorously.","Theoretical treatment utilizing various computational approaches to investigate 2D materials and heterostructures for photodetection applications is also briefly discussed."],"url":"http://arxiv.org/abs/2402.11375v1","category":"physics.app-ph"}
{"created":"2024-02-17 18:31:21","title":"Training Language Model Agents without Modifying Language Models","abstract":"Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.","sentences":["Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions.","To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications.","Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights.","By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process.","With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks.","We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability."],"url":"http://arxiv.org/abs/2402.11359v1","category":"cs.AI"}
{"created":"2024-02-17 18:03:47","title":"Unified Capacity Results for Free-Space Optical Communication Systems Over Gamma-Gamma Atmospheric Turbulence Channels","abstract":"In terrestrial free-space optical (FSO) communication systems, adaptive power control at the optical laser transmitters is crucial not only to prolong the life span of the laser sources, but more importantly to maintain robust and spectrally efficient communication through atmospheric turbulence. However, a comprehensive study of dynamic power adaptation in existing FSO systems is lacking in the literature. In this paper, we investigate FSO communication systems capable of adaptive laser power control with heterodyne detection (HD) and direct detection (DD) based receivers operating under shot-noise-limited conditions. Under these FSO systems considerations, we derive unified exact and asymptotic formulas for the capacities of Gamma-Gamma atmospheric turbulence channels with and without pointing errors; these novel closed-form capacity expressions are much simpler and provide new insights into the impact of varying turbulence conditions and pointing errors. Finally, the numerical results highlight the intricate relations of atmospheric fading, pointing error, and large-scale channel parameters in a typical terrestrial FSO channel setting, followed up by an accurate assessment of the key parameters determining the capacity performances of the aforementioned FSO systems revealing several interesting characteristics.","sentences":["In terrestrial free-space optical (FSO) communication systems, adaptive power control at the optical laser transmitters is crucial not only to prolong the life span of the laser sources, but more importantly to maintain robust and spectrally efficient communication through atmospheric turbulence.","However, a comprehensive study of dynamic power adaptation in existing FSO systems is lacking in the literature.","In this paper, we investigate FSO communication systems capable of adaptive laser power control with heterodyne detection (HD) and direct detection (DD) based receivers operating under shot-noise-limited conditions.","Under these FSO systems considerations, we derive unified exact and asymptotic formulas for the capacities of Gamma-Gamma atmospheric turbulence channels with and without pointing errors; these novel closed-form capacity expressions are much simpler and provide new insights into the impact of varying turbulence conditions and pointing errors.","Finally, the numerical results highlight the intricate relations of atmospheric fading, pointing error, and large-scale channel parameters in a typical terrestrial FSO channel setting, followed up by an accurate assessment of the key parameters determining the capacity performances of the aforementioned FSO systems revealing several interesting characteristics."],"url":"http://arxiv.org/abs/2402.11352v1","category":"cs.IT"}
{"created":"2024-02-17 17:43:54","title":"Odor Perceptual Shift Keying (OPSK) for Odor-Based Molecular Communication","abstract":"Molecular communication (MC) has promising potential and a wide range of applications. However, odor-based communication which is common in nature, has not been sufficiently examined within the context of MC, yet. In this paper, we introduce a novel approach for implementing odor-based MC systems. We propose a new modulation scheme called Odor Perceptual Shift Keying (OPSK), which encodes information by shifting the perceptual values of odor molecules in pleasantness, intensity and edibility dimensions. We construct a system which transmits OPSK modulated signals between a transmitter and receiver. We conduct analyses on the system parameters to simulate performance metrics such as symbol error rate (SER) and symbol rate (SR). Our analyses indicate that OPSK has a potential for realizing odor-based MC systems. We find that under certain conditions, reliable odor-based MC systems can be implemented using OPSK across a variety of distance ranges from millimeters up to kilometers. Additionally, we introduce adaptive symbol transmission to our system for input symbol sequences featuring symbols that occur with unequal probabilities. We further demonstrate that the proposed algorithm at the transmitter side can achieve extended operation times.","sentences":["Molecular communication (MC) has promising potential and a wide range of applications.","However, odor-based communication which is common in nature, has not been sufficiently examined within the context of MC, yet.","In this paper, we introduce a novel approach for implementing odor-based MC systems.","We propose a new modulation scheme called Odor Perceptual Shift Keying (OPSK), which encodes information by shifting the perceptual values of odor molecules in pleasantness, intensity and edibility dimensions.","We construct a system which transmits OPSK modulated signals between a transmitter and receiver.","We conduct analyses on the system parameters to simulate performance metrics such as symbol error rate (SER) and symbol rate (SR).","Our analyses indicate that OPSK has a potential for realizing odor-based MC systems.","We find that under certain conditions, reliable odor-based MC systems can be implemented using OPSK across a variety of distance ranges from millimeters up to kilometers.","Additionally, we introduce adaptive symbol transmission to our system for input symbol sequences featuring symbols that occur with unequal probabilities.","We further demonstrate that the proposed algorithm at the transmitter side can achieve extended operation times."],"url":"http://arxiv.org/abs/2402.11346v1","category":"cs.ET"}
{"created":"2024-02-17 17:37:53","title":"Variational Entropy Search for Adjusting Expected Improvement","abstract":"Bayesian optimization is a widely used technique for optimizing black-box functions, with Expected Improvement (EI) being the most commonly utilized acquisition function in this domain. While EI is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (ES) and max-value entropy search (MES), our work reveals that EI can be considered a special case of MES when approached through variational inference (VI). In this context, we have developed the Variational Entropy Search (VES) methodology and the VES-Gamma algorithm, which adapts EI by incorporating principles from information-theoretic concepts. The efficacy of VES-Gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in Bayesian optimization scenarios.","sentences":["Bayesian optimization is a widely used technique for optimizing black-box functions, with Expected Improvement (EI) being the most commonly utilized acquisition function in this domain.","While EI is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (ES) and max-value entropy search (MES), our work reveals that EI can be considered a special case of MES when approached through variational inference (VI).","In this context, we have developed the Variational Entropy Search (VES) methodology and the VES-Gamma algorithm, which adapts EI by incorporating principles from information-theoretic concepts.","The efficacy of VES-Gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in Bayesian optimization scenarios."],"url":"http://arxiv.org/abs/2402.11345v1","category":"stat.ML"}
{"created":"2024-02-17 16:03:35","title":"Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics","abstract":"Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle. Our experimental evaluation across six benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance.","sentences":["Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications.","However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges.","A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations.","To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA).","DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy.","We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle.","Our experimental evaluation across six benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance."],"url":"http://arxiv.org/abs/2402.11317v1","category":"cs.LG"}
{"created":"2024-02-17 15:00:19","title":"Knowledge Graph-based Session Recommendation with Adaptive Propagation","abstract":"Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing graphs and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction. To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation. Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items' neighbor information considering user intention within the learned session. Experimental results demonstrate that equipping our constructed knowledge graph and session-adaptive propagation enhances session recommendation backbones by 10%-20%. Moreover, we provide an industrial case study showing our proposed framework achieves 2% performance boost over an existing well-deployed model at The Home Depot e-platform.","sentences":["Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities.","While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties.","Previous works capture this cross-session information by constructing graphs and incorporating neighbor information.","However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction.","To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation.","Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions.","Then, we adaptively aggregate items' neighbor information considering user intention within the learned session.","Experimental results demonstrate that equipping our constructed knowledge graph and session-adaptive propagation enhances session recommendation backbones by 10%-20%.","Moreover, we provide an industrial case study showing our proposed framework achieves 2% performance boost over an existing well-deployed model at The Home Depot e-platform."],"url":"http://arxiv.org/abs/2402.11302v1","category":"cs.IR"}
{"created":"2024-02-17 14:34:31","title":"Dissecting Human and LLM Preferences","abstract":"As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. Interactive Demo: https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset: https://huggingface.co/datasets/GAIR/preference-dissection Code: https://github.com/GAIR-NLP/Preference-Dissection","sentences":["As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation.","Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks.","In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis.","We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits.","On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more.","Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs.","Finally, we show that preference-based evaluation can be intentionally manipulated.","In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them.","This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation.","Interactive Demo: https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset: https://huggingface.co/datasets/GAIR/preference-dissection Code: https://github.com/GAIR-NLP/Preference-Dissection"],"url":"http://arxiv.org/abs/2402.11296v1","category":"cs.CL"}
{"created":"2024-02-17 13:44:02","title":"Deep adaptive sampling for surrogate modeling without labeled data","abstract":"Surrogate modeling is of great practical significance for parametric differential equation systems. In contrast to classical numerical methods, using physics-informed deep learning methods to construct simulators for such systems is a promising direction due to its potential to handle high dimensionality, which requires minimizing a loss over a training set of random samples. However, the random samples introduce statistical errors, which may become the dominant errors for the approximation of low-regularity and high-dimensional problems. In this work, we present a deep adaptive sampling method for surrogate modeling ($\\text{DAS}^2$), where we generalize the deep adaptive sampling (DAS) method [62] [Tang, Wan and Yang, 2023] to build surrogate models for low-regularity parametric differential equations. In the parametric setting, the residual loss function can be regarded as an unnormalized probability density function (PDF) of the spatial and parametric variables. This PDF is approximated by a deep generative model, from which new samples are generated and added to the training set. Since the new samples match the residual-induced distribution, the refined training set can further reduce the statistical error in the current approximate solution. We demonstrate the effectiveness of $\\text{DAS}^2$ with a series of numerical experiments, including the parametric lid-driven 2D cavity flow problem with a continuous range of Reynolds numbers from 100 to 1000.","sentences":["Surrogate modeling is of great practical significance for parametric differential equation systems.","In contrast to classical numerical methods, using physics-informed deep learning methods to construct simulators for such systems is a promising direction due to its potential to handle high dimensionality, which requires minimizing a loss over a training set of random samples.","However, the random samples introduce statistical errors, which may become the dominant errors for the approximation of low-regularity and high-dimensional problems.","In this work, we present a deep adaptive sampling method for surrogate modeling ($\\text{DAS}^2$), where we generalize the deep adaptive sampling (DAS) method [62] [Tang, Wan and Yang, 2023] to build surrogate models for low-regularity parametric differential equations.","In the parametric setting, the residual loss function can be regarded as an unnormalized probability density function (PDF) of the spatial and parametric variables.","This PDF is approximated by a deep generative model, from which new samples are generated and added to the training set.","Since the new samples match the residual-induced distribution, the refined training set can further reduce the statistical error in the current approximate solution.","We demonstrate the effectiveness of $\\text{DAS}^2$ with a series of numerical experiments, including the parametric lid-driven 2D cavity flow problem with a continuous range of Reynolds numbers from 100 to 1000."],"url":"http://arxiv.org/abs/2402.11283v1","category":"math.NA"}
{"created":"2024-02-17 13:09:00","title":"TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method","abstract":"Recently, diffusion models have gained significant attention as a novel set of deep learning-based generative methods. These models attempt to sample data from a Gaussian distribution that adheres to a target distribution, and have been successfully adapted to the reconstruction of MRI data. However, as an unconditional generative model, the diffusion model typically disrupts image coordination because of the consistent projection of data introduced by conditional bootstrap. This often results in image fragmentation and incoherence. Furthermore, the inherent limitations of the diffusion model often lead to excessive smoothing of the generated images. In the same vein, some deep learning-based models often suffer from poor generalization performance, meaning their effectiveness is greatly affected by different acceleration factors. To address these challenges, we propose a novel diffusion model-based MRI reconstruction method, named TC-DiffRecon, which does not rely on a specific acceleration factor for training. We also suggest the incorporation of the MF-UNet module, designed to enhance the quality of MRI images generated by the model while mitigating the over-smoothing issue to a certain extent. During the image generation sampling process, we employ a novel TCKG module and a Coarse-to-Fine sampling scheme. These additions aim to harmonize image texture, expedite the sampling process, while achieving data consistency. Our source code is available at https://github.com/JustlfC03/TC-DiffRecon.","sentences":["Recently, diffusion models have gained significant attention as a novel set of deep learning-based generative methods.","These models attempt to sample data from a Gaussian distribution that adheres to a target distribution, and have been successfully adapted to the reconstruction of MRI data.","However, as an unconditional generative model, the diffusion model typically disrupts image coordination because of the consistent projection of data introduced by conditional bootstrap.","This often results in image fragmentation and incoherence.","Furthermore, the inherent limitations of the diffusion model often lead to excessive smoothing of the generated images.","In the same vein, some deep learning-based models often suffer from poor generalization performance, meaning their effectiveness is greatly affected by different acceleration factors.","To address these challenges, we propose a novel diffusion model-based MRI reconstruction method, named TC-DiffRecon, which does not rely on a specific acceleration factor for training.","We also suggest the incorporation of the MF-UNet module, designed to enhance the quality of MRI images generated by the model while mitigating the over-smoothing issue to a certain extent.","During the image generation sampling process, we employ a novel TCKG module and a Coarse-to-Fine sampling scheme.","These additions aim to harmonize image texture, expedite the sampling process, while achieving data consistency.","Our source code is available at https://github.com/JustlfC03/TC-DiffRecon."],"url":"http://arxiv.org/abs/2402.11274v1","category":"eess.IV"}
{"created":"2024-02-17 12:25:31","title":"MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning","abstract":"Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 30.15% improvement in \"RA\" for Phi-2-2.7B compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.","sentences":["Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge.","In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning.","MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs.","In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning.","Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings.","Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 30.15% improvement in \"RA\" for Phi-2-2.7B compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines."],"url":"http://arxiv.org/abs/2402.11260v1","category":"cs.CL"}
{"created":"2024-02-17 09:10:40","title":"Adaptive Split Balancing for Optimal Random Forest","abstract":"While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\\\"older class $\\mathcal{H}^{q,\\beta}$ for any $q\\in\\mathbb{N}$ and $\\beta\\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionally, we establish uniform upper bounds and explore the application of random forests in average treatment effect estimation problems. Through simulation studies and real-data applications, we demonstrate the superior empirical performance of the proposed methods over existing random forests.","sentences":["While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios.","In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class.","To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\\\"older class $\\mathcal{H}^{q,\\beta}$ for any $q\\in\\mathbb{N}$ and $\\beta\\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches.","Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results.","Conversely, a less random, more balanced approach demonstrates optimality.","Additionally, we establish uniform upper bounds and explore the application of random forests in average treatment effect estimation problems.","Through simulation studies and real-data applications, we demonstrate the superior empirical performance of the proposed methods over existing random forests."],"url":"http://arxiv.org/abs/2402.11228v1","category":"stat.ML"}
{"created":"2024-02-19 17:29:41","title":"Sommerfeld Radiation Condition for Helmholtz Equations with long-range Potentials","abstract":"We study the electric Helmholtz equation $\\Delta u + Vu + \\lambda u =f$ and show that, for certain potentials, the solution $u$ given by the limited absorption principle obeys a Sommerfeld radiation condition. We use a non-spherical approach based on the solution $K$ of the eikonal equation $|\\nabla K|^2=1 + \\frac{p}{\\lambda}$ to improve previous results in that area and extend them to long-range potentials which decay like $|x|^{-2-\\alpha}$ at infinity, with $\\alpha > 0$.","sentences":["We study the electric Helmholtz equation $\\Delta u + Vu + \\lambda u =f$ and show that, for certain potentials, the solution $u$ given by the limited absorption principle obeys a Sommerfeld radiation condition.","We use a non-spherical approach based on the solution $K$ of the eikonal equation $|\\nabla K|^2=1 + \\frac{p}{\\lambda}$ to improve previous results in that area and extend them to long-range potentials which decay like $|x|^{-2-\\alpha}$ at infinity, with $\\alpha > 0$."],"url":"http://arxiv.org/abs/2402.12306v1","category":"math.AP"}
{"created":"2024-02-19 17:28:15","title":"Analysis of the Picard-Newton iteration for the Navier-Stokes equations: global stability and quadratic convergence","abstract":"We analyze and test a simple-to-implement two-step iteration for the incompressible Navier-Stokes equations that consists of first applying the Picard iteration and then applying the Newton iteration to the Picard output. We prove that this composition of Picard and Newton converges quadratically, and our analysis (which covers both the unique solution and non-unique solution cases) also suggests that this solver has a larger convergence basin than usual Newton because of the improved stability properties of Picard-Newton over Newton. Numerical tests show that Picard-Newton dramatically outperforms both the Picard and Newton iterations, especially as the Reynolds number increases. We also consider enhancing the Picard step with Anderson acceleration (AA), and find that the AAPicard-Newton iteration has even better convergence properties on several benchmark test problems.","sentences":["We analyze and test a simple-to-implement two-step iteration for the incompressible Navier-Stokes equations that consists of first applying the Picard iteration and then applying the Newton iteration to the Picard output.","We prove that this composition of Picard and Newton converges quadratically, and our analysis (which covers both the unique solution and non-unique solution cases) also suggests that this solver has a larger convergence basin than usual Newton because of the improved stability properties of Picard-Newton over Newton.","Numerical tests show that Picard-Newton dramatically outperforms both the Picard and Newton iterations, especially as the Reynolds number increases.","We also consider enhancing the Picard step with Anderson acceleration (AA), and find that the AAPicard-Newton iteration has even better convergence properties on several benchmark test problems."],"url":"http://arxiv.org/abs/2402.12304v1","category":"math.NA"}
{"created":"2024-02-19 17:16:00","title":"The multigraded BGG correspondence in Macaulay2","abstract":"We give an overview of a Macaulay2 package for computing with the multigraded BGG correspondence. This software builds on the package BGG due to Abo-Decker-Eisenbud-Schreyer-Smith-Stillman, which concerns the standard graded BGG correspondence. In addition to implementing the multigraded BGG functors, this package includes an implementation of differential modules and their minimal free resolutions, and it contains a method for computing strongly linear strands of multigraded free resolutions.","sentences":["We give an overview of a Macaulay2 package for computing with the multigraded BGG correspondence.","This software builds on the package BGG due to Abo-Decker-Eisenbud-Schreyer-Smith-Stillman, which concerns the standard graded BGG correspondence.","In addition to implementing the multigraded BGG functors, this package includes an implementation of differential modules and their minimal free resolutions, and it contains a method for computing strongly linear strands of multigraded free resolutions."],"url":"http://arxiv.org/abs/2402.12293v1","category":"math.AC"}
{"created":"2024-02-19 16:50:58","title":"Ontology Enhanced Claim Detection","abstract":"We propose an ontology enhanced model for sentence based claim detection. We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models. The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods. In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus.","sentences":["We propose an ontology enhanced model for sentence based claim detection.","We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets.","Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models.","The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods.","In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus."],"url":"http://arxiv.org/abs/2402.12282v1","category":"cs.CL"}
{"created":"2024-02-19 16:24:20","title":"Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms","abstract":"Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging. In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization. In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with the 8-bit homogeneous equivalent.","sentences":["Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging.","In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization.","In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently.","We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy.","We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency.","In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with the 8-bit homogeneous equivalent."],"url":"http://arxiv.org/abs/2402.12263v1","category":"cs.LG"}
{"created":"2024-02-19 15:42:54","title":"Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers","abstract":"The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further. Code is available at $\\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\\,repo}$.","sentences":["The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge.","In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer).","We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further.","Code is available at $\\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\\,repo}$."],"url":"http://arxiv.org/abs/2402.12233v1","category":"cs.CL"}
{"created":"2024-02-19 15:36:04","title":"Half Space Property in RCD(0,N) spaces","abstract":"The goal of this note is to prove the Half Space Property for $RCD(0,N)$ spaces, namely that if $(X,d,m)$ is a parabolic $RCD(0,N)$ space and $ C \\subset X \\times \\mathbb{R}$ is locally the boundary of a locally perimeter minimizing set and it is contained in a half space, then $C$ is a locally finite union of horizontal slices.   As a consequence, we obtain oscillation estimates and a Half Space Theorem for minimal hypersurfaces in products $M \\times \\mathbb{R}$, where $M$ is a parabolic smooth manifold (possibly weighted and with boundary) with non-negative Ricci curvature.   On the way of proving the main results, we also obtain some properties of Green's functions on $RCD(K,N)$ spaces that are of independent interest.","sentences":["The goal of this note is to prove the Half Space Property for $RCD(0,N)$ spaces, namely that if $(X,d,m)$ is a parabolic $RCD(0,N)$ space and $ C \\subset X \\times \\mathbb{R}$ is locally the boundary of a locally perimeter minimizing set and it is contained in a half space, then $C$ is a locally finite union of horizontal slices.   ","As a consequence, we obtain oscillation estimates and a Half Space Theorem for minimal hypersurfaces in products $M \\times \\mathbb{R}$, where $M$ is a parabolic smooth manifold (possibly weighted and with boundary) with non-negative Ricci curvature.   ","On the way of proving the main results, we also obtain some properties of Green's functions on $RCD(K,N)$ spaces that are of independent interest."],"url":"http://arxiv.org/abs/2402.12230v1","category":"math.DG"}
{"created":"2024-02-19 15:32:33","title":"Holographic transport in anisotropic plasmas","abstract":"We study energy-momentum and charge transport in strongly interacting holographic quantum field theories in an anisotropic thermal state by contrasting three different holographic methods to compute transport coefficients: standard holographic calculation of retarded Greens functions, a method based on the null-focusing equation near horizon and the novel method based on background variations. Employing these methods we compute anisotropic shear and bulk viscosities and conductivities with anisotropy induced externally, for example by an external magnetic field. We show that all three methods yield consistent results. The novel method allows us to read off the transport coefficients from the horizon data and express them in analytic form from which we derive universal relations among them. Furthermore we extend the method based on the null-focusing equation to Gauss-Bonnet theory to compute higher derivative corrections to the aforementioned transport coefficients.","sentences":["We study energy-momentum and charge transport in strongly interacting holographic quantum field theories in an anisotropic thermal state by contrasting three different holographic methods to compute transport coefficients: standard holographic calculation of retarded Greens functions, a method based on the null-focusing equation near horizon and the novel method based on background variations.","Employing these methods we compute anisotropic shear and bulk viscosities and conductivities with anisotropy induced externally, for example by an external magnetic field.","We show that all three methods yield consistent results.","The novel method allows us to read off the transport coefficients from the horizon data and express them in analytic form from which we derive universal relations among them.","Furthermore we extend the method based on the null-focusing equation to Gauss-Bonnet theory to compute higher derivative corrections to the aforementioned transport coefficients."],"url":"http://arxiv.org/abs/2402.12224v1","category":"hep-th"}
{"created":"2024-02-19 15:32:25","title":"Second Order Meanfield Approximation for calculating Dynamics in Au-Nanoparticle Networks","abstract":"Exploiting physical processes for fast and energy-efficient computation bears great potential in the advancement of modern hardware components. This paper explores non-linear charge tunneling in nanoparticle networks, controlled by external voltages. The dynamics are described by a master equation, which describes the development of a distribution function over the set of charge occupation numbers. The driving force behind this evolution are charge tunneling events among nanoparticles and their associated rates. In this paper, we introduce two meanfield approximations to this master equation. By parametrization of the distribution function using its first- and second-order statistical moments, and a subsequent projection of the dynamics onto the resulting moment manifold, one can deterministically calculate expected charges and currents. Unlike a kinetic Monte Carlo approach, which extracts samples from the distribution function, this meanfield approach avoids any random elements. A comparison of results between the meanfield approximation and an already available kinetic Monte Carlo simulation demonstrates great accuracy. Our analysis also reveals that transitioning from a first-order to a second-order approximation significantly enhances the accuracy. Furthermore, we demonstrate the applicability of our approach to time-dependent simulations, using eulerian time-integration schemes.","sentences":["Exploiting physical processes for fast and energy-efficient computation bears great potential in the advancement of modern hardware components.","This paper explores non-linear charge tunneling in nanoparticle networks, controlled by external voltages.","The dynamics are described by a master equation, which describes the development of a distribution function over the set of charge occupation numbers.","The driving force behind this evolution are charge tunneling events among nanoparticles and their associated rates.","In this paper, we introduce two meanfield approximations to this master equation.","By parametrization of the distribution function using its first- and second-order statistical moments, and a subsequent projection of the dynamics onto the resulting moment manifold, one can deterministically calculate expected charges and currents.","Unlike a kinetic Monte Carlo approach, which extracts samples from the distribution function, this meanfield approach avoids any random elements.","A comparison of results between the meanfield approximation and an already available kinetic Monte Carlo simulation demonstrates great accuracy.","Our analysis also reveals that transitioning from a first-order to a second-order approximation significantly enhances the accuracy.","Furthermore, we demonstrate the applicability of our approach to time-dependent simulations, using eulerian time-integration schemes."],"url":"http://arxiv.org/abs/2402.12223v1","category":"physics.comp-ph"}
{"created":"2024-02-19 15:15:48","title":"Representation formulas and far-field behavior of time-periodic incompressible viscous flow around a translating rigid body","abstract":"This paper is concerned with integral representations and asymptotic expansions of solutions to the time-periodic incompressible Navier-Stokes equations for fluid flow in the exterior of a rigid body that moves with constant velocity. Using the time-periodic Oseen fundamental solution, we derive representation formulas for solutions with suitable regularity. From these formulas, the decomposition of the velocity component of the fundamental solution into steady-state and purely periodic parts and their detailed decay rate in space, we deduce complete information on the asymptotic structure of the velocity and pressure fields.","sentences":["This paper is concerned with integral representations and asymptotic expansions of solutions to the time-periodic incompressible Navier-Stokes equations for fluid flow in the exterior of a rigid body that moves with constant velocity.","Using the time-periodic Oseen fundamental solution, we derive representation formulas for solutions with suitable regularity.","From these formulas, the decomposition of the velocity component of the fundamental solution into steady-state and purely periodic parts and their detailed decay rate in space, we deduce complete information on the asymptotic structure of the velocity and pressure fields."],"url":"http://arxiv.org/abs/2402.12213v1","category":"math.AP"}
{"created":"2024-02-19 15:10:21","title":"On Li-Lin's open problem","abstract":"In this paper, we give a first partial negative answer to a question proposed by Li and Lin (Arch Ration Mech Anal 203(3):943-968, 2012). Meanwhile we also give a second partial positive answer to the Li-Lin's open problem. The first partial positive answer was given by G. Cerami, X. Zhong and W. Zou (Calc. Var. Partial Differential Equations, 54(2):1793-1829, 2015).","sentences":["In this paper, we give a first partial negative answer to a question proposed by Li and Lin (Arch Ration Mech Anal 203(3):943-968, 2012).","Meanwhile we also give a second partial positive answer to the Li-Lin's open problem.","The first partial positive answer was given by G. Cerami, X. Zhong and W. Zou (Calc.","Var.","Partial Differential Equations, 54(2):1793-1829, 2015)."],"url":"http://arxiv.org/abs/2402.12206v1","category":"math.AP"}
{"created":"2024-02-19 15:07:56","title":"Self-organized clustering, prediction, and superposition of long-term cognitive decline from short-term individual cognitive test scores in Alzheimer's disease","abstract":"Progressive cognitive decline spanning across decades is characteristic of Alzheimer's disease (AD). Various predictive models have been designed to realize its early onset and study the long-term trajectories of cognitive test scores across populations of interest. Research efforts have been geared towards superimposing patients' cognitive test scores with the long-term trajectory denoting gradual cognitive decline, while considering the heterogeneity of AD. Multiple trajectories representing cognitive assessment for the long-term have been developed based on various parameters, highlighting the importance of classifying several groups based on disease progression patterns. In this study, a novel method capable of self-organized prediction, classification, and the overlay of long-term cognitive trajectories based on short-term individual data was developed, based on statistical and differential equation modeling. We validated the predictive accuracy of the proposed method for the long-term trajectory of cognitive test score results on two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and the Japanese ADNI study. We also presented two practical illustrations of the simultaneous evaluation of risk factor associated with both the onset and the longitudinal progression of AD, and an innovative randomized controlled trial design for AD that standardizes the heterogeneity of patients enrolled in a clinical trial. These resources would improve the power of statistical hypothesis testing and help evaluate the therapeutic effect. The application of predicting the trajectory of longitudinal disease progression goes beyond AD, and is especially relevant for progressive and neurodegenerative disorders.","sentences":["Progressive cognitive decline spanning across decades is characteristic of Alzheimer's disease (AD).","Various predictive models have been designed to realize its early onset and study the long-term trajectories of cognitive test scores across populations of interest.","Research efforts have been geared towards superimposing patients' cognitive test scores with the long-term trajectory denoting gradual cognitive decline, while considering the heterogeneity of AD.","Multiple trajectories representing cognitive assessment for the long-term have been developed based on various parameters, highlighting the importance of classifying several groups based on disease progression patterns.","In this study, a novel method capable of self-organized prediction, classification, and the overlay of long-term cognitive trajectories based on short-term individual data was developed, based on statistical and differential equation modeling.","We validated the predictive accuracy of the proposed method for the long-term trajectory of cognitive test score results on two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and the Japanese ADNI study.","We also presented two practical illustrations of the simultaneous evaluation of risk factor associated with both the onset and the longitudinal progression of AD, and an innovative randomized controlled trial design for AD that standardizes the heterogeneity of patients enrolled in a clinical trial.","These resources would improve the power of statistical hypothesis testing and help evaluate the therapeutic effect.","The application of predicting the trajectory of longitudinal disease progression goes beyond AD, and is especially relevant for progressive and neurodegenerative disorders."],"url":"http://arxiv.org/abs/2402.12205v1","category":"q-bio.QM"}
{"created":"2024-02-19 14:47:23","title":"Colorizing Monochromatic Radiance Fields","abstract":"Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: https://liquidammonia.github.io/color-nerf.","sentences":["Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided.","Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial.","To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space.","By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module.","We then reproduce a colorful implicit model through the representation of luminance, density, and color.","Extensive experiments have been conducted to validate the effectiveness of our approaches.","Our project page: https://liquidammonia.github.io/color-nerf."],"url":"http://arxiv.org/abs/2402.12184v1","category":"cs.CV"}
{"created":"2024-02-19 14:22:18","title":"Classifications of cusps appearing on plane curves","abstract":"In this paper, we deal with plane curves with cusps. It is well known that there are various types of cusps. Among them, we investigate criteria for $(n, n+1)$ cusps with respect to several differential conditions and relations between these singularities and evolutes of fronts. We give complete classifications with respect to $(4, 5)$-cusps.","sentences":["In this paper, we deal with plane curves with cusps.","It is well known that there are various types of cusps.","Among them, we investigate criteria for $(n, n+1)$ cusps with respect to several differential conditions and relations between these singularities and evolutes of fronts.","We give complete classifications with respect to $(4, 5)$-cusps."],"url":"http://arxiv.org/abs/2402.12166v1","category":"math.DG"}
{"created":"2024-02-19 13:51:50","title":"Fast deep learning based reconstruction for limited angle tomography","abstract":"A major challenge in computed tomography is reconstructing objects from incomplete data. An increasingly popular solution for these problems is to incorporate deep learning models into reconstruction algorithms. This study introduces a novel approach by integrating a Fourier neural operator (FNO) into the Filtered Backprojection (FBP) reconstruction method, yielding the FNO back projection (FNO-BP) network. We employ moment conditions for sinogram extrapolation to assist the model in mitigating artefacts from limited data. Notably, our deep learning architecture maintains a runtime comparable to classical filtered back projection (FBP) reconstructions, ensuring swift performance during both inference and training. We assess our reconstruction method in the context of the Helsinki Tomography Challenge 2022 and also compare it against regular FBP methods.","sentences":["A major challenge in computed tomography is reconstructing objects from incomplete data.","An increasingly popular solution for these problems is to incorporate deep learning models into reconstruction algorithms.","This study introduces a novel approach by integrating a Fourier neural operator (FNO) into the Filtered Backprojection (FBP) reconstruction method, yielding the FNO back projection (FNO-BP) network.","We employ moment conditions for sinogram extrapolation to assist the model in mitigating artefacts from limited data.","Notably, our deep learning architecture maintains a runtime comparable to classical filtered back projection (FBP) reconstructions, ensuring swift performance during both inference and training.","We assess our reconstruction method in the context of the Helsinki Tomography Challenge 2022 and also compare it against regular FBP methods."],"url":"http://arxiv.org/abs/2402.12141v1","category":"math.NA"}
{"created":"2024-02-19 12:16:15","title":"P\u00f3lya's conjecture for thin products","abstract":"Let $\\Omega \\subset \\mathbb R^d$ be a bounded Euclidean domain. According to the famous Weyl law, both its Dirichlet eigenvalue $\\lambda_k(\\Omega)$ and its Neumann eigenvalue $\\mu_k(\\Omega)$ have the same leading asymptotics $w_k(\\Omega)=C(d,\\Omega)k^{2/d}$ as $k \\to \\infty$. G. P\\'olya conjectured in 1954 that each Dirichlet eigenvalue $\\lambda_k(\\Omega)$ is greater than $w_k(\\Omega)$, while each Neumann eigenvalue $\\mu_k(\\Omega)$ is no more than $w_k(\\Omega)$. In this paper we prove P\\'olya's conjecture for thin products, i.e. domains of the form $(a\\Omega_1) \\times \\Omega_2$, where $\\Omega_1, \\Omega_2$ are Euclidean domains, and $a$ is small enough. We also prove that the same inequalities hold if $\\Omega_2$ is replaced by a Riemannian manifold, and thus get P\\'olya's conjecture for a class of ``thin\" Riemannian manifolds with boundary.","sentences":["Let $\\Omega \\subset \\mathbb R^d$ be a bounded Euclidean domain.","According to the famous Weyl law, both its Dirichlet eigenvalue $\\lambda_k(\\Omega)$ and its Neumann eigenvalue $\\mu_k(\\Omega)$ have the same leading asymptotics $w_k(\\Omega)=C(d,\\Omega)k^{2/d}$ as $k \\to \\infty$. G. P\\'olya conjectured in 1954 that each Dirichlet eigenvalue $\\lambda_k(\\Omega)$ is greater than $w_k(\\Omega)$, while each Neumann eigenvalue $\\mu_k(\\Omega)$ is no more than $w_k(\\Omega)$.","In this paper we prove P\\'olya's conjecture for thin products, i.e. domains of the form $(a\\Omega_1) \\times \\Omega_2$, where $\\Omega_1, \\Omega_2$ are Euclidean domains, and $a$ is small enough.","We also prove that the same inequalities hold if $\\Omega_2$ is replaced by a Riemannian manifold, and thus get P\\'olya's conjecture for a class of ``thin\" Riemannian manifolds with boundary."],"url":"http://arxiv.org/abs/2402.12093v1","category":"math.SP"}
{"created":"2024-02-19 12:08:35","title":"Uniqueness, stability and algorithm for an inverse wave-number-dependent source problems","abstract":"This paper is concerned with an inverse wave-number-dependent/frequency-dependent source problem for the Helmholtz equation. In d-dimensions (d = 2,3), the unknown source term is supposed to be compactly supported in spatial variables but independent on x_d. The dependance of the source function on k is supposed to be unknown. Based on the Dirichlet-Laplacian method and the Fourier-Transform method, we develop two effcient non-iterative numerical algorithms to recover the wave-number-dependent source. Uniqueness and increasing stability analysis are proved. Numerical experiments are conducted to illustrate the effctiveness and effciency of the proposed method.","sentences":["This paper is concerned with an inverse wave-number-dependent/frequency-dependent source problem for the Helmholtz equation.","In d-dimensions (d = 2,3), the unknown source term is supposed to be compactly supported in spatial variables but independent on x_d.","The dependance of the source function on k is supposed to be unknown.","Based on the Dirichlet-Laplacian method and the Fourier-Transform method, we develop two effcient non-iterative numerical algorithms to recover the wave-number-dependent source.","Uniqueness and increasing stability analysis are proved.","Numerical experiments are conducted to illustrate the effctiveness and effciency of the proposed method."],"url":"http://arxiv.org/abs/2402.12088v1","category":"math.NA"}
{"created":"2024-02-19 11:19:17","title":"Malliavin Calculus for rough stochastic differential equations","abstract":"In this work we show that rough stochastic differential equations (RSDEs), as introduced by Friz, Hocquet, and L\\^e (2021), are Malliavin differentiable. We use this to prove existence of a density when the diffusion coefficients satisfies standard ellipticity assumptions. Moreover, when the coefficients are smooth and the diffusion coefficients satisfies a H\\\"ormander condition, the density is shown to be smooth. The key ingredient is to develop a comprehensive theory of linear rough stochastic differential equations, which could be of independent interest.","sentences":["In this work we show that rough stochastic differential equations (RSDEs), as introduced by Friz, Hocquet, and L\\^e (2021), are Malliavin differentiable.","We use this to prove existence of a density when the diffusion coefficients satisfies standard ellipticity assumptions.","Moreover, when the coefficients are smooth and the diffusion coefficients satisfies a H\\\"ormander condition, the density is shown to be smooth.","The key ingredient is to develop a comprehensive theory of linear rough stochastic differential equations, which could be of independent interest."],"url":"http://arxiv.org/abs/2402.12056v1","category":"math.PR"}
{"created":"2024-02-19 11:06:36","title":"Reinforcement Learning for Optimal Execution when Liquidity is Time-Varying","abstract":"Optimal execution is an important problem faced by any trader. Most solutions are based on the assumption of constant market impact, while liquidity is known to be dynamic. Moreover, models with time-varying liquidity typically assume that it is observable, despite the fact that, in reality, it is latent and hard to measure in real time. In this paper we show that the use of Double Deep Q-learning, a form of Reinforcement Learning based on neural networks, is able to learn optimal trading policies when liquidity is time-varying. Specifically, we consider an Almgren-Chriss framework with temporary and permanent impact parameters following several deterministic and stochastic dynamics. Using extensive numerical experiments, we show that the trained algorithm learns the optimal policy when the analytical solution is available, and overcomes benchmarks and approximated solutions when the solution is not available.","sentences":["Optimal execution is an important problem faced by any trader.","Most solutions are based on the assumption of constant market impact, while liquidity is known to be dynamic.","Moreover, models with time-varying liquidity typically assume that it is observable, despite the fact that, in reality, it is latent and hard to measure in real time.","In this paper we show that the use of Double Deep Q-learning, a form of Reinforcement Learning based on neural networks, is able to learn optimal trading policies when liquidity is time-varying.","Specifically, we consider an Almgren-Chriss framework with temporary and permanent impact parameters following several deterministic and stochastic dynamics.","Using extensive numerical experiments, we show that the trained algorithm learns the optimal policy when the analytical solution is available, and overcomes benchmarks and approximated solutions when the solution is not available."],"url":"http://arxiv.org/abs/2402.12049v1","category":"q-fin.TR"}
{"created":"2024-02-19 10:36:19","title":"Exact solutions to the Weighted Region Problem","abstract":"In this paper, we consider the Weighted Region Problem. In the Weighted Region Problem, the length of a path is defined as the sum of the weights of the subpaths within each region, where the weight of a subpath is its Euclidean length multiplied by a weight $ \\alpha \\geq 0 $ depending on the region. We study a restricted version of the problem of determining shortest paths through a single weighted rectangular region. We prove that even this very restricted version of the problem is unsolvable within the Algebraic Computation Model over the Rational Numbers (ACMQ). On the positive side, we provide the equations for the shortest paths that are computable within the ACMQ. Additionally, we provide equations for the bisectors between regions of the Shortest Path Map for a source point on the boundary of (or inside) the rectangular region.","sentences":["In this paper, we consider the Weighted Region Problem.","In the Weighted Region Problem, the length of a path is defined as the sum of the weights of the subpaths within each region, where the weight of a subpath is its Euclidean length multiplied by a weight $ \\alpha \\geq 0 $ depending on the region.","We study a restricted version of the problem of determining shortest paths through a single weighted rectangular region.","We prove that even this very restricted version of the problem is unsolvable within the Algebraic Computation Model over the Rational Numbers (ACMQ).","On the positive side, we provide the equations for the shortest paths that are computable within the ACMQ.","Additionally, we provide equations for the bisectors between regions of the Shortest Path Map for a source point on the boundary of (or inside) the rectangular region."],"url":"http://arxiv.org/abs/2402.12028v1","category":"cs.CG"}
{"created":"2024-02-19 10:17:04","title":"A Distinct Radial Acceleration Relation across Brightest Cluster Galaxies and Galaxy Clusters","abstract":"Recent studies reveal a radial acceleration relation (RAR) in galaxies, which illustrates a tight empirical correlation connecting the observational acceleration and the baryonic acceleration with a characteristic acceleration scale. However, a distinct RAR has been revealed on BCG-cluster scales with a seventeen times larger acceleration scale by the gravitational lensing effect. In this work, we systematically explored the acceleration and mass correlations between dynamical and baryonic components in 50 Brightest Cluster Galaxies (BCGs). To investigate the dynamical RAR in BCGs, we derived their dynamical accelerations from the stellar kinematics using the Jeans equation through Abel inversion and adopted the baryonic mass from the SDSS photometry. We explored the spatially resolved kinematic profiles with the largest integral field spectroscopy (IFS) data mounted by the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey. Our results demonstrate that the dynamical RAR in BCGs is consistent with the lensing RAR on BCG-cluster scales as well as a larger acceleration scale. This finding may imply that BCGs and galaxy clusters have fundamental differences from field galaxies. We also find a mass correlation, but it is less tight than the acceleration correlation.","sentences":["Recent studies reveal a radial acceleration relation (RAR) in galaxies, which illustrates a tight empirical correlation connecting the observational acceleration and the baryonic acceleration with a characteristic acceleration scale.","However, a distinct RAR has been revealed on BCG-cluster scales with a seventeen times larger acceleration scale by the gravitational lensing effect.","In this work, we systematically explored the acceleration and mass correlations between dynamical and baryonic components in 50 Brightest Cluster Galaxies (BCGs).","To investigate the dynamical RAR in BCGs, we derived their dynamical accelerations from the stellar kinematics using the Jeans equation through Abel inversion and adopted the baryonic mass from the SDSS photometry.","We explored the spatially resolved kinematic profiles with the largest integral field spectroscopy (IFS) data mounted by the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey.","Our results demonstrate that the dynamical RAR in BCGs is consistent with the lensing RAR on BCG-cluster scales as well as a larger acceleration scale.","This finding may imply that BCGs and galaxy clusters have fundamental differences from field galaxies.","We also find a mass correlation, but it is less tight than the acceleration correlation."],"url":"http://arxiv.org/abs/2402.12016v1","category":"astro-ph.GA"}
{"created":"2024-02-19 09:31:15","title":"Type Isomorphisms for Multiplicative-Additive Linear Logic","abstract":"We characterize type isomorphisms in the multiplicative-additive fragment of linear logic (MALL), and thus in *-autonomous categories with finite products, extending a result for the multiplicative fragment by Balat and Di Cosmo. This yields a much richer equational theory involving distributivity and cancellation laws. The unit-free case is obtained by relying on the proof-net syntax introduced by Hughes and Van Glabbeek. We use the sequent calculus to extend our results to full MALL, including all units, thanks to a study of cut-elimination and rule commutations.","sentences":["We characterize type isomorphisms in the multiplicative-additive fragment of linear logic (MALL), and thus in *-autonomous categories with finite products, extending a result for the multiplicative fragment by Balat and Di Cosmo.","This yields a much richer equational theory involving distributivity and cancellation laws.","The unit-free case is obtained by relying on the proof-net syntax introduced by Hughes and Van Glabbeek.","We use the sequent calculus to extend our results to full MALL, including all units, thanks to a study of cut-elimination and rule commutations."],"url":"http://arxiv.org/abs/2402.11987v1","category":"cs.LO"}
{"created":"2024-02-19 09:30:29","title":"Nonlocal weak Harnack estimates","abstract":"Various weak Harnack estimates for super-solutions to nonlocal, linear parabolic equations are studied. The nonlocal parabolic Harnack estimate for solutions results from them. Our new approach is measure theoretical in the spirit of DeGiorgi classes. It yields novel nonlocal weak Harnack estimates in the elliptic case as well.","sentences":["Various weak Harnack estimates for super-solutions to nonlocal, linear parabolic equations are studied.","The nonlocal parabolic Harnack estimate for solutions results from them.","Our new approach is measure theoretical in the spirit of DeGiorgi classes.","It yields novel nonlocal weak Harnack estimates in the elliptic case as well."],"url":"http://arxiv.org/abs/2402.11986v1","category":"math.AP"}
{"created":"2024-02-19 08:27:23","title":"AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization","abstract":"Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets with multiple victim models. The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output.","sentences":["Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP).","At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied.","In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images.","Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information.","We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values.","We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets with multiple victim models.","The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output."],"url":"http://arxiv.org/abs/2402.11940v1","category":"cs.CV"}
{"created":"2024-02-19 07:45:04","title":"Scalable Virtual Valuations Combinatorial Auction Design by Combining Zeroth-Order and First-Order Optimization Method","abstract":"Automated auction design seeks to discover empirically high-revenue and incentive-compatible mechanisms using machine learning. Ensuring dominant strategy incentive compatibility (DSIC) is crucial, and the most effective approach is to confine the mechanism to Affine Maximizer Auctions (AMAs). Nevertheless, existing AMA-based approaches encounter challenges such as scalability issues (arising from combinatorial candidate allocations) and the non-differentiability of revenue. In this paper, to achieve a scalable AMA-based method, we further restrict the auction mechanism to Virtual Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly fewer parameters. Initially, we employ a parallelizable dynamic programming algorithm to compute the winning allocation of a VVCA. Subsequently, we propose a novel optimization method that combines both zeroth-order and first-order techniques to optimize the VVCA parameters. Extensive experiments demonstrate the efficacy and scalability of our proposed approach, termed Zeroth-order and First-order Optimization of VVCAs (ZFO-VVCA), particularly when applied to large-scale auctions.","sentences":["Automated auction design seeks to discover empirically high-revenue and incentive-compatible mechanisms using machine learning.","Ensuring dominant strategy incentive compatibility (DSIC) is crucial, and the most effective approach is to confine the mechanism to Affine Maximizer Auctions (AMAs).","Nevertheless, existing AMA-based approaches encounter challenges such as scalability issues (arising from combinatorial candidate allocations) and the non-differentiability of revenue.","In this paper, to achieve a scalable AMA-based method, we further restrict the auction mechanism to Virtual Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly fewer parameters.","Initially, we employ a parallelizable dynamic programming algorithm to compute the winning allocation of a VVCA.","Subsequently, we propose a novel optimization method that combines both zeroth-order and first-order techniques to optimize the VVCA parameters.","Extensive experiments demonstrate the efficacy and scalability of our proposed approach, termed Zeroth-order and First-order Optimization of VVCAs (ZFO-VVCA), particularly when applied to large-scale auctions."],"url":"http://arxiv.org/abs/2402.11904v1","category":"cs.GT"}
{"created":"2024-02-19 07:21:09","title":"Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization","abstract":"While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is weaker for individuals at the extreme who already exhibit high conformity to their group. In contrast, during communal events, intergroup interactions can increase group conformity. Finally, we decompose the differential effects across religious groups in terms of emotions and topics of discussion. The results show that the dynamics of religious polarization are sensitive to the context and have important implications for understanding the role of intergroup interactions.","sentences":["While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial.","Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks.","We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020.","We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups.","We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events.","We find that for political and social events, intergroup interactions reduce polarization.","This decline is weaker for individuals at the extreme who already exhibit high conformity to their group.","In contrast, during communal events, intergroup interactions can increase group conformity.","Finally, we decompose the differential effects across religious groups in terms of emotions and topics of discussion.","The results show that the dynamics of religious polarization are sensitive to the context and have important implications for understanding the role of intergroup interactions."],"url":"http://arxiv.org/abs/2402.11895v1","category":"cs.SI"}
{"created":"2024-02-19 06:51:06","title":"Error estimates of the cubic interpolated pseudo-particle scheme for one-dimensional advection equations","abstract":"Error estimates of cubic interpolated pseudo-particle scheme (CIP scheme) for the one-dimensional advection equation with periodic boundary conditions are presented. The CIP scheme is a semi-Lagrangian method involving the piecewise cubic Hermite interpolation. Although it is numerically known that the space-time accuracy of the scheme is third order, its rigorous proof remains an open problem. In this paper, denoting the spatial and temporal mesh sizes by $ h $ and $ \\Delta t $ respectively, we prove an error estimate $ O(\\Delta t^3 + \\frac{h^4}{\\Delta t}) $ in $ L^2 $ norm theoretically, which justifies the above-mentioned prediction if $ h = O(\\Delta t) $. The proof is based on properties of the interpolation operator; the most important one is that it behaves as the $ L^2 $ projection for the second-order derivatives. We remark that the same strategy perfectly works as well to address an error estimate for the semi-Lagrangian method with the cubic spline interpolation.","sentences":["Error estimates of cubic interpolated pseudo-particle scheme (CIP scheme) for the one-dimensional advection equation with periodic boundary conditions are presented.","The CIP scheme is a semi-Lagrangian method involving the piecewise cubic Hermite interpolation.","Although it is numerically known that the space-time accuracy of the scheme is third order, its rigorous proof remains an open problem.","In this paper, denoting the spatial and temporal mesh sizes by $ h $ and $ \\Delta t $ respectively, we prove an error estimate $ O(\\Delta t^3 + \\frac{h^4}{\\Delta t}) $ in $ L^2 $ norm theoretically, which justifies the above-mentioned prediction if $ h = O(\\Delta t) $.","The proof is based on properties of the interpolation operator; the most important one is that it behaves as the $ L^2 $ projection for the second-order derivatives.","We remark that the same strategy perfectly works as well to address an error estimate for the semi-Lagrangian method with the cubic spline interpolation."],"url":"http://arxiv.org/abs/2402.11885v1","category":"math.NA"}
{"created":"2024-02-19 06:33:00","title":"Hausdorff dimension of random attractors for a stochastic delayed parabolic equation in Banach spaces","abstract":"The main purpose of this paper is to give an upper bound of Hausdorff dimension of random attractors for a stochastic delayed parabolic equation in Banach spaces. The estimation of dimensions of random attractors are obtained by combining the squeezing property and a covering lemma of finite subspace of Banach spaces, which generalizes the method established in Hilbert spaces. Unlike the existing works, where orthogonal projectors with finite ranks applied for proving the squeezing property of stochastic partial differential equations in Hilbert spaces, we adopt the state decomposition of phase space based on the exponential dichotomy of the the linear deterministic part of the studied SDPE to obtain similar squeezing property due to the lack of smooth inner product geometry structure. The obtained dimension of the random attractors depend only on the spectrum of the linear part and the random Lipschitz constant of the nonlinear term, while not relating to the compact embedding of the phase space to another Banach space as the existing works did.","sentences":["The main purpose of this paper is to give an upper bound of Hausdorff dimension of random attractors for a stochastic delayed parabolic equation in Banach spaces.","The estimation of dimensions of random attractors are obtained by combining the squeezing property and a covering lemma of finite subspace of Banach spaces, which generalizes the method established in Hilbert spaces.","Unlike the existing works, where orthogonal projectors with finite ranks applied for proving the squeezing property of stochastic partial differential equations in Hilbert spaces, we adopt the state decomposition of phase space based on the exponential dichotomy of the the linear deterministic part of the studied SDPE to obtain similar squeezing property due to the lack of smooth inner product geometry structure.","The obtained dimension of the random attractors depend only on the spectrum of the linear part and the random Lipschitz constant of the nonlinear term, while not relating to the compact embedding of the phase space to another Banach space as the existing works did."],"url":"http://arxiv.org/abs/2402.11876v1","category":"math.AP"}
{"created":"2024-02-19 05:59:09","title":"Communication-Efficient Distributed Learning with Local Immediate Error Compensation","abstract":"Gradient compression with error compensation has attracted significant attention with the target of reducing the heavy communication overhead in distributed learning. However, existing compression methods either perform only unidirectional compression in one iteration with higher communication cost, or bidirectional compression with slower convergence rate. In this work, we propose the Local Immediate Error Compensated SGD (LIEC-SGD) optimization algorithm to break the above bottlenecks based on bidirectional compression and carefully designed compensation approaches. Specifically, the bidirectional compression technique is to reduce the communication cost, and the compensation technique compensates the local compression error to the model update immediately while only maintaining the global error variable on the server throughout the iterations to boost its efficacy. Theoretically, we prove that LIEC-SGD is superior to previous works in either the convergence rate or the communication cost, which indicates that LIEC-SGD could inherit the dual advantages from unidirectional compression and bidirectional compression. Finally, experiments of training deep neural networks validate the effectiveness of the proposed LIEC-SGD algorithm.","sentences":["Gradient compression with error compensation has attracted significant attention with the target of reducing the heavy communication overhead in distributed learning.","However, existing compression methods either perform only unidirectional compression in one iteration with higher communication cost, or bidirectional compression with slower convergence rate.","In this work, we propose the Local Immediate Error Compensated SGD (LIEC-SGD) optimization algorithm to break the above bottlenecks based on bidirectional compression and carefully designed compensation approaches.","Specifically, the bidirectional compression technique is to reduce the communication cost, and the compensation technique compensates the local compression error to the model update immediately while only maintaining the global error variable on the server throughout the iterations to boost its efficacy.","Theoretically, we prove that LIEC-SGD is superior to previous works in either the convergence rate or the communication cost, which indicates that LIEC-SGD could inherit the dual advantages from unidirectional compression and bidirectional compression.","Finally, experiments of training deep neural networks validate the effectiveness of the proposed LIEC-SGD algorithm."],"url":"http://arxiv.org/abs/2402.11857v1","category":"cs.LG"}
{"created":"2024-02-19 05:06:10","title":"An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering","abstract":"Text document clustering can play a vital role in organizing and handling the everincreasing number of text documents. Uninformative and redundant features included in large text documents reduce the effectiveness of the clustering algorithm. Feature selection (FS) is a well-known technique for removing these features. Since FS can be formulated as an optimization problem, various meta-heuristic algorithms have been employed to solve it. Teaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm that benefits from the low number of parameters and fast convergence. A hybrid method can simultaneously benefit from the advantages of TLBO and tackle the possible entrapment in the local optimum. By proposing a hybrid of TLBO, Grey Wolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests a filter-based FS algorithm (TLBO-GWO). Six benchmark datasets are selected, and TLBO-GWO is compared with three recently proposed FS algorithms with similar approaches, the main TLBO and GWO. The comparison is conducted based on clustering evaluation measures, convergence behavior, and dimension reduction, and is validated using statistical tests. The results reveal that TLBO-GWO can significantly enhance the effectiveness of the text clustering technique (K-means).","sentences":["Text document clustering can play a vital role in organizing and handling the everincreasing number of text documents.","Uninformative and redundant features included in large text documents reduce the effectiveness of the clustering algorithm.","Feature selection (FS) is a well-known technique for removing these features.","Since FS can be formulated as an optimization problem, various meta-heuristic algorithms have been employed to solve it.","Teaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm that benefits from the low number of parameters and fast convergence.","A hybrid method can simultaneously benefit from the advantages of TLBO and tackle the possible entrapment in the local optimum.","By proposing a hybrid of TLBO, Grey Wolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests a filter-based FS algorithm (TLBO-GWO).","Six benchmark datasets are selected, and TLBO-GWO is compared with three recently proposed FS algorithms with similar approaches, the main TLBO and GWO.","The comparison is conducted based on clustering evaluation measures, convergence behavior, and dimension reduction, and is validated using statistical tests.","The results reveal that TLBO-GWO can significantly enhance the effectiveness of the text clustering technique (K-means)."],"url":"http://arxiv.org/abs/2402.11839v1","category":"cs.NE"}
{"created":"2024-02-19 04:45:15","title":"Rock Classification Based on Residual Networks","abstract":"Rock Classification is an essential geological problem since it provides important formation information. However, exploration on this problem using convolutional neural networks is not sufficient. To tackle this problem, we propose two approaches using residual neural networks. We first adopt data augmentation methods to enlarge our dataset. By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model. This boosts the model's performance, achieving an accuracy of 73.7% on the test dataset. We also explore how the number of bottleneck transformer blocks may influence model performance. We discover that models with more than one bottleneck transformer block may not further improve performance. Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures.","sentences":["Rock Classification is an essential geological problem since it provides important formation information.","However, exploration on this problem using convolutional neural networks is not sufficient.","To tackle this problem, we propose two approaches using residual neural networks.","We first adopt data augmentation methods to enlarge our dataset.","By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34.","Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model.","This boosts the model's performance, achieving an accuracy of 73.7% on the test dataset.","We also explore how the number of bottleneck transformer blocks may influence model performance.","We discover that models with more than one bottleneck transformer block may not further improve performance.","Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures."],"url":"http://arxiv.org/abs/2402.11831v1","category":"cs.CV"}
{"created":"2024-02-19 04:34:40","title":"Photoelectron Polarization Vortexes in Strong-Field Ionization","abstract":"The spin polarization of photoelectrons induced by an intense linearly polarized laser field is investigated using numerical solutions of the time-dependent Schr\\\"odinger equation in companion with our analytic treatment via the spin-resolved strong-field approximation and classical trajectory Monte Carlo simulations. We demonstrate that, even though the total polarization vanishes upon averaging over the photoelectron momentum, momentum-resolved spin polarization is significant, typically exhibiting a vortex structure relative to the laser polarization axis. The polarization arises from the transfer of spin-orbital coupling in the bound state to the spin-correlated quantum orbits in the continuum. The rescattering of photoelectrons at the atomic core plays an important role in forming the polarization vortex structure, while there is no significant effect of the spin-orbit coupling during the continuum dynamics. Furthermore, spin-polarized electron holography is demonstrated, feasible for extracting fine structural information about the atom.","sentences":["The spin polarization of photoelectrons induced by an intense linearly polarized laser field is investigated using numerical solutions of the time-dependent Schr\\\"odinger equation in companion with our analytic treatment via the spin-resolved strong-field approximation and classical trajectory Monte Carlo simulations.","We demonstrate that, even though the total polarization vanishes upon averaging over the photoelectron momentum, momentum-resolved spin polarization is significant, typically exhibiting a vortex structure relative to the laser polarization axis.","The polarization arises from the transfer of spin-orbital coupling in the bound state to the spin-correlated quantum orbits in the continuum.","The rescattering of photoelectrons at the atomic core plays an important role in forming the polarization vortex structure, while there is no significant effect of the spin-orbit coupling during the continuum dynamics.","Furthermore, spin-polarized electron holography is demonstrated, feasible for extracting fine structural information about the atom."],"url":"http://arxiv.org/abs/2402.11825v1","category":"physics.atom-ph"}
{"created":"2024-02-19 03:59:32","title":"Interpretable Embedding for Ad-hoc Video Search","abstract":"Answering query with semantic concepts has long been the mainstream approach for video search. Until recently, its performance is surpassed by concept-free approach, which embeds queries in a joint space as videos. Nevertheless, the embedded features as well as search results are not interpretable, hindering subsequent steps in video browsing and query reformulation. This paper integrates feature embedding and concept interpretation into a neural network for unified dual-task learning. In this way, an embedding is associated with a list of semantic concepts as an interpretation of video content. This paper empirically demonstrates that, by using either the embedding features or concepts, considerable search improvement is attainable on TRECVid benchmarked datasets. Concepts are not only effective in pruning false positive videos, but also highly complementary to concept-free search, leading to large margin of improvement compared to state-of-the-art approaches.","sentences":["Answering query with semantic concepts has long been the mainstream approach for video search.","Until recently, its performance is surpassed by concept-free approach, which embeds queries in a joint space as videos.","Nevertheless, the embedded features as well as search results are not interpretable, hindering subsequent steps in video browsing and query reformulation.","This paper integrates feature embedding and concept interpretation into a neural network for unified dual-task learning.","In this way, an embedding is associated with a list of semantic concepts as an interpretation of video content.","This paper empirically demonstrates that, by using either the embedding features or concepts, considerable search improvement is attainable on TRECVid benchmarked datasets.","Concepts are not only effective in pruning false positive videos, but also highly complementary to concept-free search, leading to large margin of improvement compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.11812v1","category":"cs.CV"}
{"created":"2024-02-19 03:24:38","title":"Theory of the spectral function of Fermi polarons at finite temperature","abstract":"We develop a general theory of Fermi polarons at nonzero temperature, including particle-hole excitations of the Fermi sea shake-up to arbitrarily high orders. The exact set of equations of the spectral function is derived by using both Chevy ansatz and diagrammatic approach, and their equivalence is clarified to hold in free space only, with an unregularized infinitesimal interaction strength. The correction to the polaron spectral function arising from two-particle-hole excitations is explicitly examined, for an exemplary case of Fermi polarons in one-dimensional optical lattices. We find quantitative improvements at low temperatures with the inclusion of two-particle-hole excitations, in both polaron energies and decay rates. Our exact theory of Fermi polarons with arbitrary orders of particle-hole excitations might be used to better understand the intriguing polaron dynamical responses in two or three dimensions, whether in free space or within lattices.","sentences":["We develop a general theory of Fermi polarons at nonzero temperature, including particle-hole excitations of the Fermi sea shake-up to arbitrarily high orders.","The exact set of equations of the spectral function is derived by using both Chevy ansatz and diagrammatic approach, and their equivalence is clarified to hold in free space only, with an unregularized infinitesimal interaction strength.","The correction to the polaron spectral function arising from two-particle-hole excitations is explicitly examined, for an exemplary case of Fermi polarons in one-dimensional optical lattices.","We find quantitative improvements at low temperatures with the inclusion of two-particle-hole excitations, in both polaron energies and decay rates.","Our exact theory of Fermi polarons with arbitrary orders of particle-hole excitations might be used to better understand the intriguing polaron dynamical responses in two or three dimensions, whether in free space or within lattices."],"url":"http://arxiv.org/abs/2402.11805v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-19 03:20:12","title":"Eigenvalue estimates on shrinkers","abstract":"We prove an eigenvalue estimate which holds on every properly embedded self-similar shrinker for mean curvature flow.","sentences":["We prove an eigenvalue estimate which holds on every properly embedded self-similar shrinker for mean curvature flow."],"url":"http://arxiv.org/abs/2402.11803v1","category":"math.DG"}
{"created":"2024-02-19 01:25:33","title":"Effective Kinetics of Chemical Reaction Networks","abstract":"Chemical reaction network theory is a powerful framework to describe and analyze chemical systems. While much about the concentration profile in an equilibrium state can be determined in terms of the graph structure, the overall reaction's time evolution depends on the network's kinetic rate function. In this article, we consider the problem of the effective kinetics of a chemical reaction network regarded as a conversion system from the feeding species to products. We define the notion of effective kinetics as a partial solution of a system of non-autonomous ordinary differential equations determined from a chemical reaction network. Examples of actual calculations include the Michaelis-Menten mechanism, for which it is confirmed that our notion of effective kinetics yields the classical formula. Further, we introduce the notion of straight-line solutions of non-autonomous ordinary differential equations to formalize the situation where a well-defined reaction rate exists and consider its relation with the quasi-stationary state approximation used in microkinetics. Our considerations here give a unified framework to formulate the reaction rate of chemical reaction networks.","sentences":["Chemical reaction network theory is a powerful framework to describe and analyze chemical systems.","While much about the concentration profile in an equilibrium state can be determined in terms of the graph structure, the overall reaction's time evolution depends on the network's kinetic rate function.","In this article, we consider the problem of the effective kinetics of a chemical reaction network regarded as a conversion system from the feeding species to products.","We define the notion of effective kinetics as a partial solution of a system of non-autonomous ordinary differential equations determined from a chemical reaction network.","Examples of actual calculations include the Michaelis-Menten mechanism, for which it is confirmed that our notion of effective kinetics yields the classical formula.","Further, we introduce the notion of straight-line solutions of non-autonomous ordinary differential equations to formalize the situation where a well-defined reaction rate exists and consider its relation with the quasi-stationary state approximation used in microkinetics.","Our considerations here give a unified framework to formulate the reaction rate of chemical reaction networks."],"url":"http://arxiv.org/abs/2402.11762v1","category":"q-bio.MN"}
{"created":"2024-02-19 00:21:13","title":"Low-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme","abstract":"Sound source localisation is used in many consumer electronics devices, to help isolate audio from individual speakers and to reject noise. Localization is frequently accomplished by \"beamforming\" algorithms, which combine microphone audio streams to improve received signal power from particular incident source directions. Beamforming algorithms generally use knowledge of the frequency components of the audio source, along with the known microphone array geometry, to analytically phase-shift microphone streams before combining them. A dense set of band-pass filters is often used to obtain known-frequency \"narrowband\" components from wide-band audio streams. These approaches achieve high accuracy, but state of the art narrowband beamforming algorithms are computationally demanding, and are therefore difficult to integrate into low-power IoT devices. We demonstrate a novel method for sound source localisation in arbitrary microphone arrays, designed for efficient implementation in ultra-low-power spiking neural networks (SNNs). We use a novel short-time Hilbert transform (STHT) to remove the need for demanding band-pass filtering of audio, and introduce a new accompanying method for audio encoding with spiking events. Our beamforming and localisation approach achieves state-of-the-art accuracy for SNN methods, and comparable with traditional non-SNN super-resolution approaches. We deploy our method to low-power SNN audio inference hardware, and achieve much lower power consumption compared with super-resolution methods. We demonstrate that signal processing approaches can be co-designed with spiking neural network implementations to achieve high levels of power efficiency. Our new Hilbert-transform-based method for beamforming promises to also improve the efficiency of traditional DSP-based signal processing.","sentences":["Sound source localisation is used in many consumer electronics devices, to help isolate audio from individual speakers and to reject noise.","Localization is frequently accomplished by \"beamforming\" algorithms, which combine microphone audio streams to improve received signal power from particular incident source directions.","Beamforming algorithms generally use knowledge of the frequency components of the audio source, along with the known microphone array geometry, to analytically phase-shift microphone streams before combining them.","A dense set of band-pass filters is often used to obtain known-frequency \"narrowband\" components from wide-band audio streams.","These approaches achieve high accuracy, but state of the art narrowband beamforming algorithms are computationally demanding, and are therefore difficult to integrate into low-power IoT devices.","We demonstrate a novel method for sound source localisation in arbitrary microphone arrays, designed for efficient implementation in ultra-low-power spiking neural networks (SNNs).","We use a novel short-time Hilbert transform (STHT) to remove the need for demanding band-pass filtering of audio, and introduce a new accompanying method for audio encoding with spiking events.","Our beamforming and localisation approach achieves state-of-the-art accuracy for SNN methods, and comparable with traditional non-SNN super-resolution approaches.","We deploy our method to low-power SNN audio inference hardware, and achieve much lower power consumption compared with super-resolution methods.","We demonstrate that signal processing approaches can be co-designed with spiking neural network implementations to achieve high levels of power efficiency.","Our new Hilbert-transform-based method for beamforming promises to also improve the efficiency of traditional DSP-based signal processing."],"url":"http://arxiv.org/abs/2402.11748v1","category":"cs.SD"}
{"created":"2024-02-18 23:54:35","title":"Extraction of nonlinearity in neural networks and model compression with Koopman operator","abstract":"Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.","sentences":["Nonlinearity plays a crucial role in deep neural networks.","In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential.","For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format.","The results imply that restricted nonlinearity is enough for the classification of handwritten numbers.","Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments.","Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks.","We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task."],"url":"http://arxiv.org/abs/2402.11740v1","category":"cs.LG"}
{"created":"2024-02-18 22:16:43","title":"Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems","abstract":"Fourier Neural Operator (FNO) is a popular operator learning method, which has demonstrated state-of-the-art performance across many tasks. However, FNO is mainly used in forward prediction, yet a large family of applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) that tackles both the forward and inverse problems. We designed a series of invertible Fourier blocks in the latent channel space to share the model parameters, efficiently exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to overcome challenges of illposedness, data shortage, noises, etc. We developed a three-step process for pre-training and fine tuning for efficient training. The evaluations on five benchmark problems have demonstrated the effectiveness of our approach.","sentences":["Fourier Neural Operator (FNO) is a popular operator learning method, which has demonstrated state-of-the-art performance across many tasks.","However, FNO is mainly used in forward prediction, yet a large family of applications rely on solving inverse problems.","In this paper, we propose an invertible Fourier Neural Operator (iFNO) that tackles both the forward and inverse problems.","We designed a series of invertible Fourier blocks in the latent channel space to share the model parameters, efficiently exchange the information, and mutually regularize the learning for the bi-directional tasks.","We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to overcome challenges of illposedness, data shortage, noises, etc.","We developed a three-step process for pre-training and fine tuning for efficient training.","The evaluations on five benchmark problems have demonstrated the effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.11722v1","category":"cs.LG"}
{"created":"2024-02-18 21:58:24","title":"Mixed material point method formulation, stabilization, and validation for a unified analysis of free-surface and seepage flow","abstract":"This paper presents a novel stabilized mixed material point method (MPM) designed for the unified modeling of free-surface and seepage flow. The unified formulation integrates the Navier-Stokes equation with the Darcy-Brinkman-Forchheimer equation, effectively capturing flows in both non-porous and porous domains. In contrast to the conventional Eulerian computational fluid dynamics (CFD) solver, which solves the velocity and pressure fields as unknown variables, the proposed method employs a monolithic displacement-pressure formulation adopted from the mixed-form updated-Lagrangian finite element method (FEM). To satisfy the discrete inf-sup stability condition, a stabilization strategy based on the variational multiscale method (VMS) is derived and integrated into the proposed formulation. Another distinctive feature is the implementation of blurred interfaces, which facilitate a seamless and stable transition of flows between free and porous domains, as well as across two distinct porous media. The efficacy of the proposed formulation is verified and validated through several benchmark cases in 1D, 2D, and 3D scenarios. Conducted numerical examples demonstrate enhanced accuracy and stability compared to analytical, experimental, and other numerical solutions.","sentences":["This paper presents a novel stabilized mixed material point method (MPM) designed for the unified modeling of free-surface and seepage flow.","The unified formulation integrates the Navier-Stokes equation with the Darcy-Brinkman-Forchheimer equation, effectively capturing flows in both non-porous and porous domains.","In contrast to the conventional Eulerian computational fluid dynamics (CFD) solver, which solves the velocity and pressure fields as unknown variables, the proposed method employs a monolithic displacement-pressure formulation adopted from the mixed-form updated-Lagrangian finite element method (FEM).","To satisfy the discrete inf-sup stability condition, a stabilization strategy based on the variational multiscale method (VMS) is derived and integrated into the proposed formulation.","Another distinctive feature is the implementation of blurred interfaces, which facilitate a seamless and stable transition of flows between free and porous domains, as well as across two distinct porous media.","The efficacy of the proposed formulation is verified and validated through several benchmark cases in 1D, 2D, and 3D scenarios.","Conducted numerical examples demonstrate enhanced accuracy and stability compared to analytical, experimental, and other numerical solutions."],"url":"http://arxiv.org/abs/2402.11719v1","category":"math.NA"}
{"created":"2024-02-18 21:51:35","title":"A symmetric function approach to polynomial regression","abstract":"We give an explicit solution formula for the polynomial regression problem in terms of Schur polynomials and Vandermonde determinants. We thereby generalize the work of Chang, Deng, and Floater to the case of model functions of the form $\\sum _{i=1}^{n} a_{i} x^{d_{i}}$ for some integer exponents $d_{1} >d_{2} >\\dotsc >d_{n} \\geq 0$ and phrase the results using Schur polynomials. Even though the solution circumvents the well-known problems with the forward stability of the normal equation, it is only of practical value if $n$ is small because the number of terms in the formula grows rapidly with the number $m$ of data points. The formula can be evaluated essentially without rounding.","sentences":["We give an explicit solution formula for the polynomial regression problem in terms of Schur polynomials and Vandermonde determinants.","We thereby generalize the work of Chang, Deng, and Floater to the case of model functions of the form $\\sum _{i=1}^{n} a_{i} x^{d_{i}}$ for some integer exponents $d_{1} >d_{2} >\\dotsc >d_{n} \\geq 0$ and phrase the results using Schur polynomials.","Even though the solution circumvents the well-known problems with the forward stability of the normal equation, it is only of practical value if $n$ is small because the number of terms in the formula grows rapidly with the number $m$ of data points.","The formula can be evaluated essentially without rounding."],"url":"http://arxiv.org/abs/2402.11717v1","category":"math.RA"}
{"created":"2024-02-18 21:01:49","title":"Learning Memory Kernels in Generalized Langevin Equations","abstract":"We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.","sentences":["We introduce a novel approach for learning memory kernels in Generalized Langevin Equations.","This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization.","Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions.","We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections.","Additionally, we provide examples that include the application of force and drift terms in the equation."],"url":"http://arxiv.org/abs/2402.11705v1","category":"stat.ML"}
{"created":"2024-02-18 20:47:33","title":"Explaining the Machine Learning Solution of the Ising Model","abstract":"As powerful as machine learning (ML) techniques are in solving problems involving data with large dimensionality, explaining the results from the fitted parameters remains a challenging task of utmost importance, especially in physics applications. Here it is shown how this can be accomplished for the ferromagnetic Ising model, the target of many ML studies in the last years. By using a neural network (NN) without any hidden layers and the symmetry of the Hamiltonian to find the critical temperature for the continuous phase transition of the model, an explanation of its strategy is found. This allows the prediction of the minimal extension of the NN to solve the problem when the symmetry is not known, which is also explainable.","sentences":["As powerful as machine learning (ML) techniques are in solving problems involving data with large dimensionality, explaining the results from the fitted parameters remains a challenging task of utmost importance, especially in physics applications.","Here it is shown how this can be accomplished for the ferromagnetic Ising model, the target of many ML studies in the last years.","By using a neural network (NN) without any hidden layers and the symmetry of the Hamiltonian to find the critical temperature for the continuous phase transition of the model, an explanation of its strategy is found.","This allows the prediction of the minimal extension of the NN to solve the problem when the symmetry is not known, which is also explainable."],"url":"http://arxiv.org/abs/2402.11701v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-18 17:55:59","title":"Interpretable Short-Term Load Forecasting via Multi-Scale Temporal Decomposition","abstract":"Rapid progress in machine learning and deep learning has enabled a wide range of applications in the electricity load forecasting of power systems, for instance, univariate and multivariate short-term load forecasting. Though the strong capabilities of learning the non-linearity of the load patterns and the high prediction accuracy have been achieved, the interpretability of typical deep learning models for electricity load forecasting is less studied. This paper proposes an interpretable deep learning method, which learns a linear combination of neural networks that each attends to an input time feature. We also proposed a multi-scale time series decomposition method to deal with the complex time patterns. Case studies have been carried out on the Belgium central grid load dataset and the proposed model demonstrated better accuracy compared to the frequently applied baseline model. Specifically, the proposed multi-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52, 0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed method displays generalization capability. On the other hand, it can demonstrate not only the feature but also the temporal interpretability compared to other baseline methods. Besides, the global time feature interpretabilities are also obtained. Obtaining global feature interpretabilities allows us to catch the overall patterns, trends, and cyclicality in load data while also revealing the significance of various time-related features in forming the final outputs.","sentences":["Rapid progress in machine learning and deep learning has enabled a wide range of applications in the electricity load forecasting of power systems, for instance, univariate and multivariate short-term load forecasting.","Though the strong capabilities of learning the non-linearity of the load patterns and the high prediction accuracy have been achieved, the interpretability of typical deep learning models for electricity load forecasting is less studied.","This paper proposes an interpretable deep learning method, which learns a linear combination of neural networks that each attends to an input time feature.","We also proposed a multi-scale time series decomposition method to deal with the complex time patterns.","Case studies have been carried out on the Belgium central grid load dataset and the proposed model demonstrated better accuracy compared to the frequently applied baseline model.","Specifically, the proposed multi-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52, 0.57 and 0.72 respectively.","As for interpretability, on one hand, the proposed method displays generalization capability.","On the other hand, it can demonstrate not only the feature but also the temporal interpretability compared to other baseline methods.","Besides, the global time feature interpretabilities are also obtained.","Obtaining global feature interpretabilities allows us to catch the overall patterns, trends, and cyclicality in load data while also revealing the significance of various time-related features in forming the final outputs."],"url":"http://arxiv.org/abs/2402.11664v1","category":"cs.LG"}
{"created":"2024-02-18 16:59:38","title":"Iterative Linear Quadratic Regulator With Variational Equation-Based Discretization","abstract":"This paper discusses discretization methods for implementing nonlinear model predictive controllers using Iterative Linear Quadratic Regulator (ILQR). Finite-difference approximations are mostly used to derive a discrete-time state equation from the original continuous-time model. However, the timestep of the discretization is sometimes restricted to be small to suppress the approximation error. In this paper, we propose to use the variational equation for deriving linearizations of the discretized system required in ILQR algorithms, which allows accurate computation regardless of the timestep. Numerical simulations of the swing-up control of an inverted pendulum demonstrate the effectiveness of this method. By the relaxing stringent requirement for the size of the timestep, the use of the variational equation can improve control performance by increasing the number of ILQR iterations possible at each timestep in the realtime computation.","sentences":["This paper discusses discretization methods for implementing nonlinear model predictive controllers using Iterative Linear Quadratic Regulator (ILQR).","Finite-difference approximations are mostly used to derive a discrete-time state equation from the original continuous-time model.","However, the timestep of the discretization is sometimes restricted to be small to suppress the approximation error.","In this paper, we propose to use the variational equation for deriving linearizations of the discretized system required in ILQR algorithms, which allows accurate computation regardless of the timestep.","Numerical simulations of the swing-up control of an inverted pendulum demonstrate the effectiveness of this method.","By the relaxing stringent requirement for the size of the timestep, the use of the variational equation can improve control performance by increasing the number of ILQR iterations possible at each timestep in the realtime computation."],"url":"http://arxiv.org/abs/2402.11648v1","category":"eess.SY"}
{"created":"2024-02-18 16:03:04","title":"Discrete Neural Algorithmic Reasoning","abstract":"Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.","sentences":["Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms.","While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data.","On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states.","In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states.","Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm.","To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks.","Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data."],"url":"http://arxiv.org/abs/2402.11628v1","category":"cs.LG"}
{"created":"2024-02-18 15:19:28","title":"A high order, block finite difference, error inhibiting scheme for the transport equation","abstract":"We propose a block finite difference, error inhibiting scheme that is fourth-order accurate for short to moderate times and has a six-order convergence rate for long times. This scheme outperforms the standard fourth-order Finite Difference scheme. We also demonstrate that the proposed scheme is a particular type of nodal-based Discontinuous Galerkin method with $p=1$.","sentences":["We propose a block finite difference, error inhibiting scheme that is fourth-order accurate for short to moderate times and has a six-order convergence rate for long times.","This scheme outperforms the standard fourth-order Finite Difference scheme.","We also demonstrate that the proposed scheme is a particular type of nodal-based Discontinuous Galerkin method with $p=1$."],"url":"http://arxiv.org/abs/2402.11617v1","category":"math.NA"}
{"created":"2024-02-18 15:08:06","title":"Modified Gravity Model $f(Q,T)$ and Wormhole Solution","abstract":"We investigate wormhole solutions using the modified gravity model $f(Q,T)$ with viscosity and aim to find a solution for the existence of wormholes mathematically without violating the energy conditions. We show that there is no need to define a wormhole from exotic matter and analyze the equations with numerical analysis to establish weak energy conditions. In the numerical analysis, we found that the appropriate values of the parameters can maintain the weak energy conditions without the need for exotic matter. Adjusting the parameters of the model can increase or decrease the rate of positive energy density or radial and tangential pressures. According to the numerical analysis conducted in this paper, the weak energy conditions are established in the whole space if $\\alpha< 0$, $12.56 < \\beta < 25.12$ or $\\alpha > 0$, $\\beta > 25.12$. The analysis also showed that the supporting matter of the wormhole is near normal matter, indicating that the generalized $f(Q,T)$ model with viscosity has an acceptable parameter space to describe a wormhole without the need for exotic matter.","sentences":["We investigate wormhole solutions using the modified gravity model $f(Q,T)$ with viscosity and aim to find a solution for the existence of wormholes mathematically without violating the energy conditions.","We show that there is no need to define a wormhole from exotic matter and analyze the equations with numerical analysis to establish weak energy conditions.","In the numerical analysis, we found that the appropriate values of the parameters can maintain the weak energy conditions without the need for exotic matter.","Adjusting the parameters of the model can increase or decrease the rate of positive energy density or radial and tangential pressures.","According to the numerical analysis conducted in this paper, the weak energy conditions are established in the whole space if $\\alpha< 0$, $12.56 <","\\beta < 25.12$ or $\\alpha > 0$, $\\beta >","25.12$.","The analysis also showed that the supporting matter of the wormhole is near normal matter, indicating that the generalized $f(Q,T)$ model with viscosity has an acceptable parameter space to describe a wormhole without the need for exotic matter."],"url":"http://arxiv.org/abs/2402.11614v1","category":"gr-qc"}
{"created":"2024-02-18 14:57:53","title":"Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations","abstract":"We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univariate encoding methods, in being able to predict both local and distributed representations. Together, this demonstrates the utility of Metric-Learning Encoding Methods for studying how linguistic features are neurally encoded in language models and the advantage of MLEMs over traditional methods. MLEMs can be extended to other domains (e.g. vision) and to other neural systems, such as the human brain.","sentences":["We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process.","As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding).","We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features.","Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univariate encoding methods, in being able to predict both local and distributed representations.","Together, this demonstrates the utility of Metric-Learning Encoding Methods for studying how linguistic features are neurally encoded in language models and the advantage of MLEMs over traditional methods.","MLEMs can be extended to other domains (e.g. vision) and to other neural systems, such as the human brain."],"url":"http://arxiv.org/abs/2402.11608v1","category":"cs.CL"}
{"created":"2024-02-18 14:45:49","title":"Gradient-enhanced crystal plasticity coupled with phase-field fracture modeling","abstract":"This study addresses ductile fracture of single grains in metals by modeling of the formation and propagation of transgranular cracks. A proposed model integrates gradient extended hardening, phase-field modeling for fracture, and crystal plasticity. It is presented in a thermodynamical framework in large deformation kinematics and accounts for damage irreversibility. A micromorphic approach for variationally and thermodynamically consistent damage irreversibility is adopted. The main objective of this work is to analyze the capability of the proposed model to predict transgranular crack propagation. Further, the micromorphic approach for damage irreversibility is evaluated in the context of the presented ductile phase-field model. This is done by analyzing the impact of gradient-enhanced hardening considering micro-free and micro-hard boundary conditions, studying the effect of the micromorphic regularization parameter, evaluating the performance of the model in ratcheting loading and and testing its capability to predict three-dimensional crack propagation. In order to solve the fully coupled global and local equation systems, a staggered solution scheme that extends to the local level is presented.","sentences":["This study addresses ductile fracture of single grains in metals by modeling of the formation and propagation of transgranular cracks.","A proposed model integrates gradient extended hardening, phase-field modeling for fracture, and crystal plasticity.","It is presented in a thermodynamical framework in large deformation kinematics and accounts for damage irreversibility.","A micromorphic approach for variationally and thermodynamically consistent damage irreversibility is adopted.","The main objective of this work is to analyze the capability of the proposed model to predict transgranular crack propagation.","Further, the micromorphic approach for damage irreversibility is evaluated in the context of the presented ductile phase-field model.","This is done by analyzing the impact of gradient-enhanced hardening considering micro-free and micro-hard boundary conditions, studying the effect of the micromorphic regularization parameter, evaluating the performance of the model in ratcheting loading and and testing its capability to predict three-dimensional crack propagation.","In order to solve the fully coupled global and local equation systems, a staggered solution scheme that extends to the local level is presented."],"url":"http://arxiv.org/abs/2402.11605v1","category":"math.NA"}
{"created":"2024-02-18 14:30:34","title":"Brill-Noether loci and strata of differentials","abstract":"We prove that the projectivized strata of differentials are not contained in pointed Brill-Noether divisors, with only a few exceptions. For a generic element in a stratum of differentials, we show that many of the associated pointed Brill-Noether loci are of expected dimension. We use our results to study the Auel-Haburcak Conjecture: We obtain new non-containments between maximal Brill-Noether loci in $\\mathcal{M}_g$. Our results regarding quadratic differentials imply that the quadratic strata in genus $6$ are uniruled.","sentences":["We prove that the projectivized strata of differentials are not contained in pointed Brill-Noether divisors, with only a few exceptions.","For a generic element in a stratum of differentials, we show that many of the associated pointed Brill-Noether loci are of expected dimension.","We use our results to study the Auel-Haburcak Conjecture:","We obtain new non-containments between maximal Brill-Noether loci in $\\mathcal{M}_g$. Our results regarding quadratic differentials imply that the quadratic strata in genus $6$ are uniruled."],"url":"http://arxiv.org/abs/2402.11599v1","category":"math.AG"}
{"created":"2024-02-18 14:27:48","title":"Modified Massive Abelian $p$-Form ($p = 1, 2, 3$) Gauge Theories: Existence of the Pseudo-Scalar field and Its Implications","abstract":"We demonstrate the existence of a single pseudo-scalar (PS) field in the St${\\ddot u}$ckelberg-modified versions of (i) the two (1 + 1)-dimensional (2D) massive Abelian 1-form gauge theory, (ii) the three (2 + 1)-dimensional (3D) massive Abelian 2-form gauge theory, and (iii) the four (3 + 1)-dimensional (4D) massive Abelian 3-form gauge theory. This PS field, in all the above modified versions of the massive gauge theories, turns-up with the negative kinetic term. However, this specific field is found to be endowed with a well-defined rest mass because it satisfies the usual Klein-Gordon (KG) equation of motion (which is true for a real massive scalar relativistic particle). In these models, the usual pure scalar field (having the positive kinetic term and obeying the KG equation) is also present. The KG equation for the above PS and pure scalar fields are derived from the properly gauge-fixed Lagrangian densities. The PS field (with negative kinetic term and a well-defined mass) is a possible candidate for dark matter and it plays a crucial role in the cyclic, bouncing and self-accelerated cosmological models of the Universe where such kinds of fields have been christened as the ``phantom'' and/or ``ghost'' fields.","sentences":["We demonstrate the existence of a single pseudo-scalar (PS) field in the St${\\ddot u}$ckelberg-modified versions of (i) the two (1 + 1)-dimensional (2D) massive Abelian 1-form gauge theory, (ii) the three (2 + 1)-dimensional (3D) massive Abelian 2-form gauge theory, and (iii) the four (3 + 1)-dimensional (4D) massive Abelian 3-form gauge theory.","This PS field, in all the above modified versions of the massive gauge theories, turns-up with the negative kinetic term.","However, this specific field is found to be endowed with a well-defined rest mass because it satisfies the usual Klein-Gordon (KG) equation of motion (which is true for a real massive scalar relativistic particle).","In these models, the usual pure scalar field (having the positive kinetic term and obeying the KG equation) is also present.","The KG equation for the above PS and pure scalar fields are derived from the properly gauge-fixed Lagrangian densities.","The PS field (with negative kinetic term and a well-defined mass) is a possible candidate for dark matter and it plays a crucial role in the cyclic, bouncing and self-accelerated cosmological models of the Universe where such kinds of fields have been christened as the ``phantom'' and/or ``ghost'' fields."],"url":"http://arxiv.org/abs/2402.11598v1","category":"hep-th"}
{"created":"2024-02-18 13:27:54","title":"Holographic RG flows and boundary conditions in a 3D gauged supergravity","abstract":"In this work we focus on the study of RG flows of conformal field theories that are holographically dual to Poincar\\'e domain wall solutions in $D=3$, $\\mathcal{N}=(2,0)$ gauged supergravity coupled to a sigma model with target space $\\mathrm{SU}(1, 1)/\\mathrm{U}(1) = \\mathbb{H}^2$. This theory is truncated to a subsector where the vector field and phase of the scalar field vanish and we consider different boundary conditions for the remaining real scalar field. The RG flows, which are mostly non-superysymmetric, are analyzed by treating the supergravity field equations as a dynamical system for the scalar field and its derivative with respect to the scale factor. Phase diagrams are constructed for different values of the parameter $a^2$, which is related to the curvature of the scalar manifold. The behavior of solutions near the boundary is used to determine their type based on the expansion of the corresponding fake superpotential. By incorporating information on the boundary conditions, the obtained RG flows are interpreted using the holographic dictionary. Numerical solutions and plots of the fake superpotential are also provided.","sentences":["In this work we focus on the study of RG flows of conformal field theories that are holographically dual to Poincar\\'e domain wall solutions in $D=3$, $\\mathcal{N}=(2,0)$ gauged supergravity coupled to a sigma model with target space $\\mathrm{SU}(1, 1)/\\mathrm{U}(1) = \\mathbb{H}^2$.","This theory is truncated to a subsector where the vector field and phase of the scalar field vanish and we consider different boundary conditions for the remaining real scalar field.","The RG flows, which are mostly non-superysymmetric, are analyzed by treating the supergravity field equations as a dynamical system for the scalar field and its derivative with respect to the scale factor.","Phase diagrams are constructed for different values of the parameter $a^2$, which is related to the curvature of the scalar manifold.","The behavior of solutions near the boundary is used to determine their type based on the expansion of the corresponding fake superpotential.","By incorporating information on the boundary conditions, the obtained RG flows are interpreted using the holographic dictionary.","Numerical solutions and plots of the fake superpotential are also provided."],"url":"http://arxiv.org/abs/2402.11586v1","category":"hep-th"}
{"created":"2024-02-18 12:34:23","title":"Imitation Learning-Based Online Time-Optimal Control with Multiple-Waypoint Constraints for Quadrotors","abstract":"Over the past decade, there has been a remarkable surge in utilizing quadrotors for various purposes due to their simple structure and aggressive maneuverability, such as search and rescue, delivery and autonomous drone racing, etc. One of the key challenges preventing quadrotors from being widely used in these scenarios is online waypoint-constrained time-optimal trajectory generation and control technique. This letter proposes an imitation learning-based online solution to efficiently navigate the quadrotor through multiple waypoints with time-optimal performance. The neural networks (WN&CNets) are trained to learn the control law from the dataset generated by the time-consuming CPC algorithm and then deployed to generate the optimal control commands online to guide the quadrotors. To address the challenge of limited training data and the hover maneuver at the final waypoint, we propose a transition phase strategy that utilizes polynomials to help the quadrotor 'jump over' the stop-and-go maneuver when switching waypoints. Our method is demonstrated in both simulation and real-world experiments, achieving a maximum speed of 7 m/s while navigating through 7 waypoints in a confined space of 6.0 m * 4.0 m * 2.0 m. The results show that with a slight loss in optimality, the WN&CNets significantly reduce the processing time and enable online optimal control for multiple-waypoint-constrained flight tasks.","sentences":["Over the past decade, there has been a remarkable surge in utilizing quadrotors for various purposes due to their simple structure and aggressive maneuverability, such as search and rescue, delivery and autonomous drone racing, etc.","One of the key challenges preventing quadrotors from being widely used in these scenarios is online waypoint-constrained time-optimal trajectory generation and control technique.","This letter proposes an imitation learning-based online solution to efficiently navigate the quadrotor through multiple waypoints with time-optimal performance.","The neural networks (WN&CNets) are trained to learn the control law from the dataset generated by the time-consuming CPC algorithm and then deployed to generate the optimal control commands online to guide the quadrotors.","To address the challenge of limited training data and the hover maneuver at the final waypoint, we propose a transition phase strategy that utilizes polynomials to help the quadrotor 'jump over' the stop-and-go maneuver when switching waypoints.","Our method is demonstrated in both simulation and real-world experiments, achieving a maximum speed of 7 m/s while navigating through 7 waypoints in a confined space of 6.0 m * 4.0 m * 2.0 m.","The results show that with a slight loss in optimality, the WN&CNets significantly reduce the processing time and enable online optimal control for multiple-waypoint-constrained flight tasks."],"url":"http://arxiv.org/abs/2402.11570v1","category":"cs.RO"}
{"created":"2024-02-18 12:13:05","title":"Origin and Customization of Bandgap in Chiral Phononic Crystals","abstract":"The wave equation governing the wave propagation in chiral phononic crystals, established through force equilibrium law, conceals the underlying physical information. This has led to a controversy over the bandgap mechanism. In this letter, we theoretically unveil the reason of this controversy, and put forward an alternative approach from wave behavior to formulate the wave equation, offering a new pathway to articulate the bandgap physics directly. We identify the obstacles in coupled acoustic and optic branches to widen and lower the bandgap, and introduce an approach based on spherical hinges to decrease the barriers, for customizing the bandgap frequency and width. Finally, we validate our proposal through numerical simulation and experimental demonstration.","sentences":["The wave equation governing the wave propagation in chiral phononic crystals, established through force equilibrium law, conceals the underlying physical information.","This has led to a controversy over the bandgap mechanism.","In this letter, we theoretically unveil the reason of this controversy, and put forward an alternative approach from wave behavior to formulate the wave equation, offering a new pathway to articulate the bandgap physics directly.","We identify the obstacles in coupled acoustic and optic branches to widen and lower the bandgap, and introduce an approach based on spherical hinges to decrease the barriers, for customizing the bandgap frequency and width.","Finally, we validate our proposal through numerical simulation and experimental demonstration."],"url":"http://arxiv.org/abs/2402.11562v1","category":"physics.app-ph"}
{"created":"2024-02-18 11:52:37","title":"SVD-based factored-form Cubature Kalman Filtering for continuous-time stochastic systems with discrete measurements","abstract":"In this paper, a singular value decomposition (SVD) approach is developed for implementing the cubature Kalman filter. The discussed estimator is one of the most popular and widely used method for solving nonlinear Bayesian filtering problem in practice. To improve its numerical stability (with respect to roundoff errors) and practical reliability of computations, the SVD-based methodology recently proposed for the classical Kalman filter is generalized on the nonlinear filtering problem. More precisely, we suggest the SVD-based solution for the continuous-discrete cubature Kalman filter and design two estimators: (i) the filter based on the traditionally used Euler-Maruyama discretization scheme; (ii) the estimator based on advanced It\\^{o}-Taylor expansion for discretizing the underlying stochastic differential equations. Both estimators are formulated in terms of SVD factors of the filter error covariance matrix and belong to the class of stable factored-form (square-root) algorithms. The new methods are tested on a radar tracking problem.","sentences":["In this paper, a singular value decomposition (SVD) approach is developed for implementing the cubature Kalman filter.","The discussed estimator is one of the most popular and widely used method for solving nonlinear Bayesian filtering problem in practice.","To improve its numerical stability (with respect to roundoff errors) and practical reliability of computations, the SVD-based methodology recently proposed for the classical Kalman filter is generalized on the nonlinear filtering problem.","More precisely, we suggest the SVD-based solution for the continuous-discrete cubature Kalman filter and design two estimators: (i) the filter based on the traditionally used Euler-Maruyama discretization scheme; (ii) the estimator based on advanced It\\^{o}-Taylor expansion for discretizing the underlying stochastic differential equations.","Both estimators are formulated in terms of SVD factors of the filter error covariance matrix and belong to the class of stable factored-form (square-root) algorithms.","The new methods are tested on a radar tracking problem."],"url":"http://arxiv.org/abs/2402.11555v1","category":"math.OC"}
{"created":"2024-02-18 11:32:57","title":"Standing wave solutions and instability for the Logarithmic Klein-Gordon equation","abstract":"In this paper, we study the standing wave solutions of Klein--Gordon equation with logarithmic nonlinearity.   The existence of the standing wave solution related to the ground state $\\phi_0(x)$ is obtained. Further, we prove the instability of solutions around $\\phi_0(x)$.","sentences":["In this paper, we study the standing wave solutions of Klein--Gordon equation with logarithmic nonlinearity.   ","The existence of the standing wave solution related to the ground state $\\phi_0(x)$ is obtained.","Further, we prove the instability of solutions around $\\phi_0(x)$."],"url":"http://arxiv.org/abs/2402.11546v1","category":"math.AP"}
{"created":"2024-02-18 11:07:39","title":"High-order QMC nonconforming FEMs for nearly incompressible planar stochastic elasticity equations","abstract":"In a recent work (Dick et al, arXiv:2310.06187), we considered a linear stochastic elasticity equation with random Lam\\'e parameters which are parameterized by a countably infinite number of terms in separate expansions. We estimated the expected values over the infinite dimensional parametric space of linear functionals ${\\mathcal L}$ acting on the continuous solution $\\vu$ of the elasticity equation. This was achieved by truncating the expansions of the random parameters, then using a high-order quasi-Monte Carlo (QMC) method to approximate the high dimensional integral combined with the conforming Galerkin finite element method (FEM) to approximate the displacement over the physical domain $\\Omega.$ In this work, as a further development of aforementioned article, we focus on the case of a nearly incompressible linear stochastic elasticity equation. To serve this purpose, in the presence of stochastic inhomogeneous (variable Lam\\'e parameters) nearly compressible material, we develop a new locking-free symmetric nonconforming Galerkin FEM that handles the inhomogeneity. In the case of nearly incompressible material, one known important advantage of nonconforming approximations is that they yield optimal order convergence rates that are uniform in the Poisson coefficient. Proving the convergence of the nonconforming FEM leads to another challenge that is summed up in showing the needed regularity properties of $\\vu$. For the error estimates from the high-order QMC method, which is needed to estimate the expected value over the infinite dimensional parametric space of ${\\mathcal L}\\vu,$ we %rely on (Dick et al. 2022). We are required here to show certain regularity properties of $\\vu$ with respect to the random coefficients. Some numerical results are delivered at the end.","sentences":["In a recent work (Dick et al, arXiv:2310.06187), we considered a linear stochastic elasticity equation with random Lam\\'e parameters which are parameterized by a countably infinite number of terms in separate expansions.","We estimated the expected values over the infinite dimensional parametric space of linear functionals ${\\mathcal L}$ acting on the continuous solution $\\vu$ of the elasticity equation.","This was achieved by truncating the expansions of the random parameters, then using a high-order quasi-Monte Carlo (QMC) method to approximate the high dimensional integral combined with the conforming Galerkin finite element method (FEM) to approximate the displacement over the physical domain $\\Omega.$ In this work, as a further development of aforementioned article, we focus on the case of a nearly incompressible linear stochastic elasticity equation.","To serve this purpose, in the presence of stochastic inhomogeneous (variable Lam\\'e parameters) nearly compressible material, we develop a new locking-free symmetric nonconforming Galerkin FEM that handles the inhomogeneity.","In the case of nearly incompressible material, one known important advantage of nonconforming approximations is that they yield optimal order convergence rates that are uniform in the Poisson coefficient.","Proving the convergence of the nonconforming FEM leads to another challenge that is summed up in showing the needed regularity properties of $\\vu$. For the error estimates from the high-order QMC method, which is needed to estimate the expected value over the infinite dimensional parametric space of ${\\mathcal L}\\vu,$ we %rely on (Dick et al. 2022).","We are required here to show certain regularity properties of $\\vu$ with respect to the random coefficients.","Some numerical results are delivered at the end."],"url":"http://arxiv.org/abs/2402.11545v1","category":"math.NA"}
{"created":"2024-02-18 10:30:41","title":"Hausdorff measure estimates for the degenerate quenching problem","abstract":"We study analytical and geometric properties of minimizers of non-differentiable functionals epitomizing the degenerate quenching problem. Our main finding unveils finite $(n-1)-$Hausdorff measure estimates for the pertaining free boundaries. The approach hinges upon deriving optimal gradient decay estimates, coupled with a fine analysis of an intrinsic auxiliary equation stripped of the singularity.","sentences":["We study analytical and geometric properties of minimizers of non-differentiable functionals epitomizing the degenerate quenching problem.","Our main finding unveils finite $(n-1)-$Hausdorff measure estimates for the pertaining free boundaries.","The approach hinges upon deriving optimal gradient decay estimates, coupled with a fine analysis of an intrinsic auxiliary equation stripped of the singularity."],"url":"http://arxiv.org/abs/2402.11536v1","category":"math.AP"}
{"created":"2024-02-18 09:53:14","title":"Measuring Privacy Loss in Distributed Spatio-Temporal Data","abstract":"Statistics about traffic flow and people's movement gathered from multiple geographical locations in a distributed manner are the driving force powering many applications, such as traffic prediction, demand prediction, and restaurant occupancy reports. However, these statistics are often based on sensitive location data of people, and hence privacy has to be preserved while releasing them. The standard way to do this is via differential privacy, which guarantees a form of rigorous, worst-case, person-level privacy. In this work, motivated by several counter-intuitive features of differential privacy in distributed location applications, we propose an alternative privacy loss against location reconstruction attacks by an informed adversary. Our experiments on real and synthetic data demonstrate that our privacy loss better reflects our intuitions on individual privacy violation in the distributed spatio-temporal setting.","sentences":["Statistics about traffic flow and people's movement gathered from multiple geographical locations in a distributed manner are the driving force powering many applications, such as traffic prediction, demand prediction, and restaurant occupancy reports.","However, these statistics are often based on sensitive location data of people, and hence privacy has to be preserved while releasing them.","The standard way to do this is via differential privacy, which guarantees a form of rigorous, worst-case, person-level privacy.","In this work, motivated by several counter-intuitive features of differential privacy in distributed location applications, we propose an alternative privacy loss against location reconstruction attacks by an informed adversary.","Our experiments on real and synthetic data demonstrate that our privacy loss better reflects our intuitions on individual privacy violation in the distributed spatio-temporal setting."],"url":"http://arxiv.org/abs/2402.11526v1","category":"cs.CR"}
{"created":"2024-02-18 09:47:27","title":"Fokker-Planck equations on homogeneous Lie groups and probabilistic counterparts","abstract":"The well-posedness of Fokker-Planck equations defined on homogeneous Lie groups and the properties of diffusion processes associated with such equations are addressed. The main difficulty arises from the polynomial growth of the coefficients, which is related to the growth of the family of vector fields generating the first layer of the associated Lie algebra. We prove that the Fokker-Planck equation has a unique energy solution, which we can represent as the transition density of the underlying subelliptic diffusion process. Moreover, we show its Holder continuity in time, where the Holder seminorm depends on the degree of homogeneity of the vector fields. Finally, we provide a probabilistic proof of the Feyman-Kac formula, also as a consequence of the uniform boundedness in finite time intervals of all moments.","sentences":["The well-posedness of Fokker-Planck equations defined on homogeneous Lie groups and the properties of diffusion processes associated with such equations are addressed.","The main difficulty arises from the polynomial growth of the coefficients, which is related to the growth of the family of vector fields generating the first layer of the associated Lie algebra.","We prove that the Fokker-Planck equation has a unique energy solution, which we can represent as the transition density of the underlying subelliptic diffusion process.","Moreover, we show its Holder continuity in time, where the Holder seminorm depends on the degree of homogeneity of the vector fields.","Finally, we provide a probabilistic proof of the Feyman-Kac formula, also as a consequence of the uniform boundedness in finite time intervals of all moments."],"url":"http://arxiv.org/abs/2402.11524v1","category":"math.AP"}
{"created":"2024-02-18 09:28:04","title":"Rate of convergence for first-order singular perturbation problems: Hamilton-Jacobi-Isaacs equations and mean field games of acceleration","abstract":"This work focuses on the rate of convergence for singular perturbation problems for first-order Hamilton-Jacobi equations. As an application we derive the rate of convergence for singularly perturbed two-players zero-sum deterministic differential games (i.e., leading to Hamilton-Jacobi-Isaacs equations) and, subsequently, in case of singularly perturbed mean field games of acceleration. Namely, we show that in both the models the rate of convergence is $\\varepsilon$.","sentences":["This work focuses on the rate of convergence for singular perturbation problems for first-order Hamilton-Jacobi equations.","As an application we derive the rate of convergence for singularly perturbed two-players zero-sum deterministic differential games (i.e., leading to Hamilton-Jacobi-Isaacs equations) and, subsequently, in case of singularly perturbed mean field games of acceleration.","Namely, we show that in both the models the rate of convergence is $\\varepsilon$."],"url":"http://arxiv.org/abs/2402.11521v1","category":"math.AP"}
{"created":"2024-02-18 09:21:12","title":"Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network","abstract":"Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose a novel REasoning meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate semantically feasible meta-structures. ReStruct also employs performance-oriented evolutionary operations. These two competing forces jointly optimize for semantic explainability and empirical performance of meta-structures. We also design a differential LLM explainer that can produce natural language explanations for the discovered meta-structures, and refine the explanation by reasoning through the search history. Experiments on five datasets demonstrate ReStruct achieve SOTA performance in node classification and link recommendation tasks. Additionally, a survey study involving 73 graduate students shows that the meta-structures and natural language explanations generated by ReStruct are substantially more comprehensible.","sentences":["Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types.","Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations.","However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms.","Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability.","Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans.","To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs).","We propose a novel REasoning meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into the evolutionary procedure.","ReStruct uses a grammar translator to encode meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate semantically feasible meta-structures.","ReStruct also employs performance-oriented evolutionary operations.","These two competing forces jointly optimize for semantic explainability and empirical performance of meta-structures.","We also design a differential LLM explainer that can produce natural language explanations for the discovered meta-structures, and refine the explanation by reasoning through the search history.","Experiments on five datasets demonstrate ReStruct achieve SOTA performance in node classification and link recommendation tasks.","Additionally, a survey study involving 73 graduate students shows that the meta-structures and natural language explanations generated by ReStruct are substantially more comprehensible."],"url":"http://arxiv.org/abs/2402.11518v1","category":"cs.LG"}
{"created":"2024-02-18 09:07:35","title":"Sharp lifespan estimate for the compressible Euler system with critical time-dependent damping in $\\R^2$","abstract":"This paper concerns the long time existence to the smooth solutions of the compressible Euler system with critical time dependent damping in $\\R^2$. We establish the sharp lifespan estimate from below, with respect to the small parameter of the initial perturbation. For this end, the vector fields $\\widehat{Z}$ (defined below) are used instead of the usual one $Z$, to get better decay for the linear error terms. This idea may also apply to the long time behavior study of nonlinear wave equations with time-dependent damping.","sentences":["This paper concerns the long time existence to the smooth solutions of the compressible Euler system with critical time dependent damping in $\\R^2$. We establish the sharp lifespan estimate from below, with respect to the small parameter of the initial perturbation.","For this end, the vector fields $\\widehat{Z}$ (defined below) are used instead of the usual one $Z$, to get better decay for the linear error terms.","This idea may also apply to the long time behavior study of nonlinear wave equations with time-dependent damping."],"url":"http://arxiv.org/abs/2402.11516v1","category":"math.AP"}
{"created":"2024-02-18 08:56:39","title":"Exactly solvable model for transmission line with artificial dispersion","abstract":"The problem of the emergence of wave dispersion due to the heterogeneity of a transmission line (TL) is considered. An exactly solvable model helps to better understand the physical process of a signal passing through a non-uniform section of the line and to compare the exact solution and solutions obtained using various approximate methods. Based on the transition to new variables, the developed approach made it possible to construct exact analytical solutions of telegraph equations with a continuous distribution of parameters, which depend on the coordinate. The flexibility of the discussed model is due to the presence of a number of free parameters, including two geometric factors characterizing the lengths of inhomogeneities in values of the inductance L and of the capacitance C. In the new variables, the spatiotemporal structure of the solutions is described using sine waves and elementary functions, and the dispersion is determined by the formulas of the waveguide type. The dispersive waveguide-like structure characterized by refractive index N and cut-off frequency $\\Omega$. The exact expressions for the complex reflection and transmission coefficients are derived. These expressions describe phase shifts for reflected and transmitted waves. The following interesting cases are analyzed: the passage of waves without phase change, the reflectionless passage of waves, and the passage of signals through a sequence of non-uniform sections. The developed mathematical formalism can be useful for the analysis of a wider range of problems.","sentences":["The problem of the emergence of wave dispersion due to the heterogeneity of a transmission line (TL) is considered.","An exactly solvable model helps to better understand the physical process of a signal passing through a non-uniform section of the line and to compare the exact solution and solutions obtained using various approximate methods.","Based on the transition to new variables, the developed approach made it possible to construct exact analytical solutions of telegraph equations with a continuous distribution of parameters, which depend on the coordinate.","The flexibility of the discussed model is due to the presence of a number of free parameters, including two geometric factors characterizing the lengths of inhomogeneities in values of the inductance L and of the capacitance C.","In the new variables, the spatiotemporal structure of the solutions is described using sine waves and elementary functions, and the dispersion is determined by the formulas of the waveguide type.","The dispersive waveguide-like structure characterized by refractive index N and cut-off frequency $\\Omega$. The exact expressions for the complex reflection and transmission coefficients are derived.","These expressions describe phase shifts for reflected and transmitted waves.","The following interesting cases are analyzed: the passage of waves without phase change, the reflectionless passage of waves, and the passage of signals through a sequence of non-uniform sections.","The developed mathematical formalism can be useful for the analysis of a wider range of problems."],"url":"http://arxiv.org/abs/2402.11513v1","category":"physics.app-ph"}
{"created":"2024-02-18 08:53:41","title":"From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings","abstract":"Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.","sentences":["Embeddings play a pivotal role in the efficacy of Large Language Models.","They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language.","Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias.","In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'.","We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks.","We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion."],"url":"http://arxiv.org/abs/2402.11512v1","category":"cs.CL"}
{"created":"2024-02-18 08:51:55","title":"Keller-Segel type approximation for nonlocal Fokker-Planck equations in one-dimensional bounded domain","abstract":"Numerous evolution equations with nonlocal convolution-type interactions have been proposed. In some cases, a convolution was imposed as the velocity in the advection term. Motivated by analyzing these equations, we approximate advective nonlocal interactions as local ones, thereby converting the effect of nonlocality. In this study, we investigate whether the solution to the nonlocal Fokker-Planck equation can be approximated using the Keller-Segel system. By singular limit analysis, we show that this approximation is feasible for the Fokker-Planck equation with any potential and that the convergence rate is specified. Moreover, we provide an explicit formula for determining the coefficient of the Lagrange interpolation polynomial with Chebyshev nodes. Using this formula, the Keller-Segel system parameters for the approximation are explicitly specified by the shape of the potential in the Fokker-Planck equation. Consequently, we demonstrate the relationship between advective nonlocal interactions and a local dynamical system.","sentences":["Numerous evolution equations with nonlocal convolution-type interactions have been proposed.","In some cases, a convolution was imposed as the velocity in the advection term.","Motivated by analyzing these equations, we approximate advective nonlocal interactions as local ones, thereby converting the effect of nonlocality.","In this study, we investigate whether the solution to the nonlocal Fokker-Planck equation can be approximated using the Keller-Segel system.","By singular limit analysis, we show that this approximation is feasible for the Fokker-Planck equation with any potential and that the convergence rate is specified.","Moreover, we provide an explicit formula for determining the coefficient of the Lagrange interpolation polynomial with Chebyshev nodes.","Using this formula, the Keller-Segel system parameters for the approximation are explicitly specified by the shape of the potential in the Fokker-Planck equation.","Consequently, we demonstrate the relationship between advective nonlocal interactions and a local dynamical system."],"url":"http://arxiv.org/abs/2402.11511v1","category":"math.AP"}
{"created":"2024-02-18 07:49:22","title":"Graph Out-of-Distribution Generalization via Causal Intervention","abstract":"Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4\\% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet.","sentences":["Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts.","The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data.","In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment.","The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes.","Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels.","Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor.","The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations.","Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4\\% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks.","Source codes are available at https://github.com/fannie1208/CaNet."],"url":"http://arxiv.org/abs/2402.11494v1","category":"cs.LG"}
{"created":"2024-02-18 07:48:03","title":"Electromagnetically induced gratings created by extremely short non-overlapping pulses of light in a three-level resonant mediu","abstract":"In a fixed spectral range, single- and half-cycle electromagnetic pulses have the shortest duration. Half-cycle pulses are promising tools for ultrafast control of quantum systems. Previously, the possibility of using a sequence of single- and half-cycle attosecond pulses to generate and ultrafast control light-induced population difference gratings has been demonstrated. However, such studies have been carried out using different approximations. For example, when the medium is modelled in the two-level approximation. In this paper, based on the numerical solution of the system of Maxwell-Bloch equations, it is shown that it is possible to generate and control population gratings in a three-level medium without using the approximations used in previous studies. It is shown that taking into account the additional level of the medium does not lead to a violation of the effect of generating such gratings. This extends the applicability of previous results.","sentences":["In a fixed spectral range, single- and half-cycle electromagnetic pulses have the shortest duration.","Half-cycle pulses are promising tools for ultrafast control of quantum systems.","Previously, the possibility of using a sequence of single- and half-cycle attosecond pulses to generate and ultrafast control light-induced population difference gratings has been demonstrated.","However, such studies have been carried out using different approximations.","For example, when the medium is modelled in the two-level approximation.","In this paper, based on the numerical solution of the system of Maxwell-Bloch equations, it is shown that it is possible to generate and control population gratings in a three-level medium without using the approximations used in previous studies.","It is shown that taking into account the additional level of the medium does not lead to a violation of the effect of generating such gratings.","This extends the applicability of previous results."],"url":"http://arxiv.org/abs/2402.11491v1","category":"physics.optics"}
{"created":"2024-02-18 07:13:31","title":"Martingale Suitable Weak Solutions of $3$-D Stochastic Navier-Stokes Equations with Vorticity Bounds","abstract":"In this paper, we construct suitable weak martingale solutions for $3$-dimensional incompressible stochastic Navier-Stokes equations with linear multiplicative noise. In deterministic setting, as widely known, ``suitable weak solutions'' are Leray-Hopf weak solutions enjoying two different types of local energy inequalities. In stochastic setting, we are able to show corresponding stochastic versions of the two local energy inequalities, where a semi-martingale term occurs as the effect of the noise. Note that the exponential formulas, which are widely used to transform the stochastic PDEs system into a random one, DO NOT show up in our formulations of local energy inequality. This is different to \\cite{FR02,Rom10} where the additive noise case is dealt. Also, a path-wise result in terms of ``a.e. super-martingale'' is derived from the stochastic local energy inequalities. Further more, if the initial vorticity is a finite Radon measure, we are able to bound the $L^1\\big(\\Omega;L^\\infty([0,T];L^1)\\big)$ norm of the vorticity and $L^{\\frac{4}{3+\\delta}}\\big(\\Omega\\times[0,T]\\times\\mathbb T^3\\big)$ norm of the gradient of the vorticity.","sentences":["In this paper, we construct suitable weak martingale solutions for $3$-dimensional incompressible stochastic Navier-Stokes equations with linear multiplicative noise.","In deterministic setting, as widely known, ``suitable weak solutions'' are Leray-Hopf weak solutions enjoying two different types of local energy inequalities.","In stochastic setting, we are able to show corresponding stochastic versions of the two local energy inequalities, where a semi-martingale term occurs as the effect of the noise.","Note that the exponential formulas, which are widely used to transform the stochastic PDEs system into a random one, DO NOT show up in our formulations of local energy inequality.","This is different to \\cite{FR02,Rom10} where the additive noise case is dealt.","Also, a path-wise result in terms of ``a.e. super-martingale'' is derived from the stochastic local energy inequalities.","Further more, if the initial vorticity is a finite Radon measure, we are able to bound the $L^1\\big(\\Omega;L^\\infty([0,T];L^1)\\big)$ norm of the vorticity and $L^{\\frac{4}{3+\\delta}}\\big(\\Omega\\times[0,T]\\times\\mathbb T^3\\big)$ norm of the gradient of the vorticity."],"url":"http://arxiv.org/abs/2402.11482v1","category":"math.PR"}
{"created":"2024-02-18 06:56:56","title":"Studying Differential Mental Health Expressions in India","abstract":"Psychosocial stressors and the symptomatology of mental disorders are known to vary with socio-cultural environment. Mental health expressions on social media, however, are primarily informed by studies in the WEIRD (Western, Educated, Industrial, Rich, and Democratic) contexts. In this paper, we analyze mental health posts on Reddit made by individuals in India, to identify variations in online depression language specific to the Indian context compared to users from the Rest of the World (ROW). Unlike in Western samples, mental health discussions in India additionally express sadness, use negation, are present-focused, and are related to work and achievement. {Illness} is exclusively correlated to India, reaffirming the link between somatic symptoms and mental disorders in Indian patients. Two clinical psychologists validated the findings from social media posts and found 95\\% of the top-20 topics associated with mental health discussions as {prevalent} in Indians. Significant linguistic variations in online mental health-related language in India compared to ROW, highlight the need for precision culturally-aware mental health models. These findings have important implications for designing culturally appropriate interventions to reduce the growing diagnosis and treatment gap for mental disorders in India.","sentences":["Psychosocial stressors and the symptomatology of mental disorders are known to vary with socio-cultural environment.","Mental health expressions on social media, however, are primarily informed by studies in the WEIRD (Western, Educated, Industrial, Rich, and Democratic) contexts.","In this paper, we analyze mental health posts on Reddit made by individuals in India, to identify variations in online depression language specific to the Indian context compared to users from the Rest of the World (ROW).","Unlike in Western samples, mental health discussions in India additionally express sadness, use negation, are present-focused, and are related to work and achievement.","{Illness} is exclusively correlated to India, reaffirming the link between somatic symptoms and mental disorders in Indian patients.","Two clinical psychologists validated the findings from social media posts and found 95\\% of the top-20 topics associated with mental health discussions as {prevalent} in Indians.","Significant linguistic variations in online mental health-related language in India compared to ROW, highlight the need for precision culturally-aware mental health models.","These findings have important implications for designing culturally appropriate interventions to reduce the growing diagnosis and treatment gap for mental disorders in India."],"url":"http://arxiv.org/abs/2402.11477v1","category":"cs.CY"}
{"created":"2024-02-18 05:55:29","title":"Online Physical Enhanced Residual Learning for Connected Autonomous Vehicles Platoon Centralized Control","abstract":"This paper introduces an online physical enhanced residual learning (PERL) framework for Connected Autonomous Vehicles (CAVs) platoon, aimed at addressing the challenges posed by the dynamic and unpredictable nature of traffic environments. The proposed framework synergistically combines a physical model, represented by Model Predictive Control (MPC), with data-driven online Q-learning. The MPC controller, enhanced for centralized CAV platoons, employs vehicle velocity as a control input and focuses on multi-objective cooperative optimization. The learning-based residual controller enriches the MPC with prior knowledge and corrects residuals caused by traffic disturbances. The PERL framework not only retains the interpretability and transparency of physics-based models but also significantly improves computational efficiency and control accuracy in real-world scenarios. The experimental results present that the online Q-learning PERL controller, in comparison to the MPC controller and PERL controller with a neural network, exhibits significantly reduced position and velocity errors. Specifically, the PERL's cumulative absolute position and velocity errors are, on average, 86.73% and 55.28% lower than the MPC's, and 12.82% and 18.83% lower than the neural network-based PERL's, in four tests with different reference trajectories and errors. The results demonstrate our advanced framework's superior accuracy and quick convergence capabilities, proving its effectiveness in maintaining platoon stability under diverse conditions.","sentences":["This paper introduces an online physical enhanced residual learning (PERL) framework for Connected Autonomous Vehicles (CAVs) platoon, aimed at addressing the challenges posed by the dynamic and unpredictable nature of traffic environments.","The proposed framework synergistically combines a physical model, represented by Model Predictive Control (MPC), with data-driven online Q-learning.","The MPC controller, enhanced for centralized CAV platoons, employs vehicle velocity as a control input and focuses on multi-objective cooperative optimization.","The learning-based residual controller enriches the MPC with prior knowledge and corrects residuals caused by traffic disturbances.","The PERL framework not only retains the interpretability and transparency of physics-based models but also significantly improves computational efficiency and control accuracy in real-world scenarios.","The experimental results present that the online Q-learning PERL controller, in comparison to the MPC controller and PERL controller with a neural network, exhibits significantly reduced position and velocity errors.","Specifically, the PERL's cumulative absolute position and velocity errors are, on average, 86.73% and 55.28% lower than the MPC's, and 12.82% and 18.83% lower than the neural network-based PERL's, in four tests with different reference trajectories and errors.","The results demonstrate our advanced framework's superior accuracy and quick convergence capabilities, proving its effectiveness in maintaining platoon stability under diverse conditions."],"url":"http://arxiv.org/abs/2402.11468v1","category":"cs.MA"}
{"created":"2024-02-18 04:13:53","title":"Nonlinear harmonic spectra in bilayer van der Waals antiferromagnets CrX$_{3}$","abstract":"Bilayer antiferromagnets CrX$_{3}$ (X $=$ Cl, Br, and I) are promising materials for spintronics and optoelectronics that are rooted in their peculiar electronic structures. However, their bands are often hybridized from the interlayer antiferromagnetic ordering, which are difficult to disentangle by traditional methods. In this work, we theoretically show that nonlinear harmonic spectra can differentiate subtle differences in their electronic states. In contrast to prior nonlinear optical studies which often use one or two photon energies, we systematically study the wavelength-dependent nonlinear harmonic spectra realized by hundreds of individual dynamical simulations under changed photon energies. Through turning on and off some excitation channels, we can pinpoint every dipole-allowed transition that largely contributes to the second and third harmonics. With the help of momentum matrix elements, highly entangled resonance peaks at a higher energy above the band edge can be assigned to specific transitions between the valence bands and three separate regions of conduction bands. Our findings demonstrate a feasible means to detect very complex electronic structures in an important family of two-dimensional antiferromagnets.","sentences":["Bilayer antiferromagnets CrX$_{3}$ (X $=$ Cl, Br, and I) are promising materials for spintronics and optoelectronics that are rooted in their peculiar electronic structures.","However, their bands are often hybridized from the interlayer antiferromagnetic ordering, which are difficult to disentangle by traditional methods.","In this work, we theoretically show that nonlinear harmonic spectra can differentiate subtle differences in their electronic states.","In contrast to prior nonlinear optical studies which often use one or two photon energies, we systematically study the wavelength-dependent nonlinear harmonic spectra realized by hundreds of individual dynamical simulations under changed photon energies.","Through turning on and off some excitation channels, we can pinpoint every dipole-allowed transition that largely contributes to the second and third harmonics.","With the help of momentum matrix elements, highly entangled resonance peaks at a higher energy above the band edge can be assigned to specific transitions between the valence bands and three separate regions of conduction bands.","Our findings demonstrate a feasible means to detect very complex electronic structures in an important family of two-dimensional antiferromagnets."],"url":"http://arxiv.org/abs/2402.11449v1","category":"physics.optics"}
{"created":"2024-02-18 03:54:21","title":"Balanced Truncation of Linear Systems with Quadratic Outputs in Limited Time and Frequency Intervals","abstract":"Model order reduction involves constructing a reduced-order approximation of a high-order model while retaining its essential characteristics. This reduced-order model serves as a substitute for the original one in various applications such as simulation, analysis, and design. Often, there's a need to maintain high accuracy within a specific time or frequency interval, while errors beyond this limit can be tolerated. This paper addresses time-limited and frequency-limited model order reduction scenarios for linear systems with quadratic outputs, by generalizing the recently introduced structure-preserving balanced truncation algorithm. To that end, limited interval system Gramians are defined, and the corresponding generalized Lyapunov equations governing their computation are derived. Additionally, low-rank solutions for these equations are investigated. Next, balanced truncation algorithms are proposed for time-limited and frequency-limited scenarios, each utilizing its corresponding limited-interval system Gramians. The proposed algorithms ensure accurate results within specified time and frequency intervals while preserving the quadratic-output structure. Two benchmark numerical examples are presented to demonstrate the effectiveness of the algorithms, showcasing their ability to achieve superior accuracy within the desired time or frequency interval.","sentences":["Model order reduction involves constructing a reduced-order approximation of a high-order model while retaining its essential characteristics.","This reduced-order model serves as a substitute for the original one in various applications such as simulation, analysis, and design.","Often, there's a need to maintain high accuracy within a specific time or frequency interval, while errors beyond this limit can be tolerated.","This paper addresses time-limited and frequency-limited model order reduction scenarios for linear systems with quadratic outputs, by generalizing the recently introduced structure-preserving balanced truncation algorithm.","To that end, limited interval system Gramians are defined, and the corresponding generalized Lyapunov equations governing their computation are derived.","Additionally, low-rank solutions for these equations are investigated.","Next, balanced truncation algorithms are proposed for time-limited and frequency-limited scenarios, each utilizing its corresponding limited-interval system Gramians.","The proposed algorithms ensure accurate results within specified time and frequency intervals while preserving the quadratic-output structure.","Two benchmark numerical examples are presented to demonstrate the effectiveness of the algorithms, showcasing their ability to achieve superior accuracy within the desired time or frequency interval."],"url":"http://arxiv.org/abs/2402.11445v1","category":"eess.SY"}
{"created":"2024-02-18 02:59:19","title":"Deep learning methods for Hamiltonian parameter estimation and magnetic domain image generation in twisted van der Waals magnets","abstract":"The application of twist engineering in van der Waals magnets has opened new frontiers in the field of two-dimensional magnetism, yielding distinctive magnetic domain structures. Despite the introduction of numerous theoretical methods, limitations persist in terms of accuracy or efficiency due to the complex nature of the magnetic Hamiltonians pertinent to these systems. In this study, we introduce a deep-learning approach to tackle these challenges. Utilizing customized, fully connected networks, we develop two deep-neural-network kernels that facilitate efficient and reliable analysis of twisted van der Waals magnets. Our regression model is adept at estimating the magnetic Hamiltonian parameters of twisted bilayer CrI3 from its magnetic domain images generated through atomistic spin simulations. The generative model excels in producing precise magnetic domain images from the provided magnetic parameters. The trained networks for these models undergo thorough validation, including statistical error analysis and assessment of robustness against noisy injections. These advancements not only extend the applicability of deep-learning methods to twisted van der Waals magnets but also streamline future investigations into these captivating yet poorly understood systems.","sentences":["The application of twist engineering in van der Waals magnets has opened new frontiers in the field of two-dimensional magnetism, yielding distinctive magnetic domain structures.","Despite the introduction of numerous theoretical methods, limitations persist in terms of accuracy or efficiency due to the complex nature of the magnetic Hamiltonians pertinent to these systems.","In this study, we introduce a deep-learning approach to tackle these challenges.","Utilizing customized, fully connected networks, we develop two deep-neural-network kernels that facilitate efficient and reliable analysis of twisted van der Waals magnets.","Our regression model is adept at estimating the magnetic Hamiltonian parameters of twisted bilayer CrI3 from its magnetic domain images generated through atomistic spin simulations.","The generative model excels in producing precise magnetic domain images from the provided magnetic parameters.","The trained networks for these models undergo thorough validation, including statistical error analysis and assessment of robustness against noisy injections.","These advancements not only extend the applicability of deep-learning methods to twisted van der Waals magnets but also streamline future investigations into these captivating yet poorly understood systems."],"url":"http://arxiv.org/abs/2402.11434v1","category":"cond-mat.str-el"}
{"created":"2024-02-18 00:59:08","title":"Predicting Maximum Permitted Process Forces for Object Grasping and Manipulation Using a Deep Learning Regression Model","abstract":"During the execution of handling processes in manufacturing, it is difficult to measure the process forces with state-of-the-art gripper systems since they usually lack integrated sensors. Thus, the exact state of the gripped object and the actuating process forces during manipulation and handling are unknown. This paper proposes a deep learning regression model to construct a continuous stability metric to predict the maximum process forces on the gripped objects using high-resolution optical tactile sensors. A pull experiment was developed to obtain a valid dataset for training. Continuously force-based labeled pairs of tactile images for varying grip positions of industrial gearbox parts were acquired to train a novel neural network inspired by encoder-decoder architectures. A ResNet-18 model was used for comparison. Both models can predict the maximum process force for each object with a precision of less than 1 N. During validation, the generalization potential of the proposed methodology with respect to previously unknown objects was demonstrated with an accuracy of 0.4-2.1 N and precision of 1.7-3.4 N, respectively.","sentences":["During the execution of handling processes in manufacturing, it is difficult to measure the process forces with state-of-the-art gripper systems since they usually lack integrated sensors.","Thus, the exact state of the gripped object and the actuating process forces during manipulation and handling are unknown.","This paper proposes a deep learning regression model to construct a continuous stability metric to predict the maximum process forces on the gripped objects using high-resolution optical tactile sensors.","A pull experiment was developed to obtain a valid dataset for training.","Continuously force-based labeled pairs of tactile images for varying grip positions of industrial gearbox parts were acquired to train a novel neural network inspired by encoder-decoder architectures.","A ResNet-18 model was used for comparison.","Both models can predict the maximum process force for each object with a precision of less than 1 N. During validation, the generalization potential of the proposed methodology with respect to previously unknown objects was demonstrated with an accuracy of 0.4-2.1 N and precision of 1.7-3.4 N, respectively."],"url":"http://arxiv.org/abs/2402.11412v1","category":"cs.RO"}
{"created":"2024-02-17 23:34:50","title":"An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection","abstract":"Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural models mapping sensor embeddings to atomic events (AEs) before CE detection, and (iii) a neuro-symbolic approach using a symbolic finite-state machine for CE detection from AEs. Empirically, the neuro-symbolic architecture significantly surpasses purely neural models, demonstrating superior performance in CE recognition, even with extensive training data and ample temporal context for neural approaches.","sentences":["Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively.","Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities.","Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data.","This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning.","We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns.","Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural models mapping sensor embeddings to atomic events (AEs) before CE detection, and (iii) a neuro-symbolic approach using a symbolic finite-state machine for CE detection from AEs.","Empirically, the neuro-symbolic architecture significantly surpasses purely neural models, demonstrating superior performance in CE recognition, even with extensive training data and ample temporal context for neural approaches."],"url":"http://arxiv.org/abs/2402.11403v1","category":"cs.AI"}
{"created":"2024-02-17 22:40:22","title":"Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications","abstract":"We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation.","sentences":["We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs).","RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency.","We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function.","For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems.","Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation."],"url":"http://arxiv.org/abs/2402.11397v1","category":"cs.LG"}
{"created":"2024-02-17 22:26:33","title":"Effect of photon propagation on a zero refractive index medium","abstract":"We present a model describing the transmission of light through atomic media with a vanishing index of refraction. Zero index materials are of particular interest as the infinite phase velocity of light within the material offers the potential to manipulate electromagnetic waves to mediate dipole-dipole interactions over extended distances. We focus on the preparation of zero-index conditions based on atomic coherence using two distinct atomic media as exemplary of generic zero-index materials. We establish a model based on the Maxwell-Bloch equations to describe the propagation of a light pulse through these media. To investigate the sustainability of the zero index under minimal light conditions, we assume single-photon intensity of the propagating pulse. Specifically, we examine whether the spatial phase change of the photon remains zero as it traverses the medium. We employ a finite-element numerical approach to solve the coupled Maxwell-Bloch equations describing the photon propagation. Our results indicate that the presence of a photon within the medium will disrupt the zero-index state, thus disallowing the establishment of enhanced dipole-dipole interactions over large distances.","sentences":["We present a model describing the transmission of light through atomic media with a vanishing index of refraction.","Zero index materials are of particular interest as the infinite phase velocity of light within the material offers the potential to manipulate electromagnetic waves to mediate dipole-dipole interactions over extended distances.","We focus on the preparation of zero-index conditions based on atomic coherence using two distinct atomic media as exemplary of generic zero-index materials.","We establish a model based on the Maxwell-Bloch equations to describe the propagation of a light pulse through these media.","To investigate the sustainability of the zero index under minimal light conditions, we assume single-photon intensity of the propagating pulse.","Specifically, we examine whether the spatial phase change of the photon remains zero as it traverses the medium.","We employ a finite-element numerical approach to solve the coupled Maxwell-Bloch equations describing the photon propagation.","Our results indicate that the presence of a photon within the medium will disrupt the zero-index state, thus disallowing the establishment of enhanced dipole-dipole interactions over large distances."],"url":"http://arxiv.org/abs/2402.11395v1","category":"physics.optics"}
{"created":"2024-02-17 21:35:49","title":"Comments on \"Can quantum statistics help distinguish Dirac from Majorana neutrinos?\" (arXiv:2402.05172 [hep-ph])","abstract":"In a recent article arXiv:2402.05172 [hep-ph], the authors discuss the question \"whether quantum statistics can help distinguish between Dirac and Majorana neutrinos.\" The paper aims to provide a critique of the results derived in our papers arXiv:2106.11785 [hep-ph] and arXiv:2307.05654 [hep-ph]. One of the criticisms is related to our expression for differential decay rate for the back-to-back neutrino-antineutrino configuration in the decay $B^0 \\to \\mu^- \\, \\mu^+ \\, \\nu_\\mu \\, \\overline{\\nu}_\\mu$. We show that the claim is wrong and point out how the correct result was obtained. The second criticism is related to the implementation of the anti-symmetrization as dictated by quantum statistics for Majorana neutrinos and antineutrinos (identical to neutrinos in this case). They have missed the point that our procedure holds when the neutrino and antineutrino remain undetected by the detector. In the back-to-back kinematic configuration, one can infer the neutrino energies without directly detecting their identities. This smartly ensures that the quantum statistical effects are not erased. Their overriding assertion that our papers arXiv:2106.11785 [hep-ph] and arXiv:2307.05654 [hep-ph] are incorrect fails to recognize that in both arXiv:2106.11785 [hep-ph] and arXiv:2307.05654 [hep-ph] we also point out generic conditions under which the practical Dirac-Majorana confusion theorem holds. \"Clearly there is no confusion over confusion theorem.\"","sentences":["In a recent article arXiv:2402.05172 [hep-ph], the authors discuss the question \"whether quantum statistics can help distinguish between Dirac and Majorana neutrinos.\"","The paper aims to provide a critique of the results derived in our papers arXiv:2106.11785","[hep-ph] and arXiv:2307.05654","[hep-ph].","One of the criticisms is related to our expression for differential decay rate for the back-to-back neutrino-antineutrino configuration in the decay $B^0 \\to \\mu^- \\, \\mu^+ \\, \\nu_\\mu \\, \\overline{\\nu}_\\mu$. We show that the claim is wrong and point out how the correct result was obtained.","The second criticism is related to the implementation of the anti-symmetrization as dictated by quantum statistics for Majorana neutrinos and antineutrinos (identical to neutrinos in this case).","They have missed the point that our procedure holds when the neutrino and antineutrino remain undetected by the detector.","In the back-to-back kinematic configuration, one can infer the neutrino energies without directly detecting their identities.","This smartly ensures that the quantum statistical effects are not erased.","Their overriding assertion that our papers arXiv:2106.11785 [hep-ph] and","arXiv:2307.05654","[hep-ph] are incorrect fails to recognize that in both arXiv:2106.11785 [hep-ph] and arXiv:2307.05654","[hep-ph] we also point out generic conditions under which the practical Dirac-Majorana confusion theorem holds.","\"Clearly there is no confusion over confusion theorem.\""],"url":"http://arxiv.org/abs/2402.11386v1","category":"hep-ph"}
{"created":"2024-02-17 20:51:17","title":"Reducibility of Klein-Gordon equations with maximal order perturbations","abstract":"We prove that all the solutions of a quasi-periodically forced linear Klein-Gordon equation $\\psi_{tt}-\\psi_{xx}+\\mathtt{m}\\psi+Q(\\omega t)\\psi=0 $ where $ Q(\\omega t) := a^{(2)}(\\omega t, x) \\partial_{xx} + a^{(1)}(\\omega t, x)\\partial_x + a^{(0)}(\\omega t, x) $ is a differential operator of order $ 2 $, parity preserving and reversible, are almost periodic in time and uniformly bounded for all times, provided that the coefficients $ a^{(2) }, a^{(1) }, a^{(0) } $ are small enough and the forcing frequency $\\omega\\in {\\mathbb R}^{\\nu}$ belongs to a Borel set of asymptotically full measure. This result is obtained by reducing the Klein-Gordon equation to a diagonal constant coefficient system with purely imaginary eigenvalues. The main difficulty is the presence in the perturbation $ Q (\\omega t) $ of the second order differential operator $ a^{(2)}(\\omega t, x)\\partial_{xx} $. In suitable coordinates the Klein-Gordon equation is the composition of two backward/forward quasi-periodic in time perturbed transport equations with non-constant coefficients, up to lower order pseudo-differential remainders. A key idea is to straighten this first order pseudo-differential operator with bi-characteristics through a novel quantitative Egorov analysis.","sentences":["We prove that all the solutions of a quasi-periodically forced linear Klein-Gordon equation $\\psi_{tt}-\\psi_{xx}+\\mathtt{m}\\psi+Q(\\omega t)\\psi=0 $ where $ Q(\\omega t) := a^{(2)}(\\omega t, x) \\partial_{xx} + a^{(1)}(\\omega t, x)\\partial_x + a^{(0)}(\\omega t, x) $ is a differential operator of order $ 2 $, parity preserving and reversible, are almost periodic in time and uniformly bounded for all times, provided that the coefficients $ a^{(2) }, a^{(1) }, a^{(0) } $ are small enough and the forcing frequency $\\omega\\in {\\mathbb R}^{\\nu}$ belongs to a Borel set of asymptotically full measure.","This result is obtained by reducing the Klein-Gordon equation to a diagonal constant coefficient system with purely imaginary eigenvalues.","The main difficulty is the presence in the perturbation $ Q (\\omega t) $ of the second order differential operator $ a^{(2)}(\\omega t, x)\\partial_{xx} $.","In suitable coordinates the Klein-Gordon equation is the composition of two backward/forward quasi-periodic in time perturbed transport equations with non-constant coefficients, up to lower order pseudo-differential remainders.","A key idea is to straighten this first order pseudo-differential operator with bi-characteristics through a novel quantitative Egorov analysis."],"url":"http://arxiv.org/abs/2402.11377v1","category":"math.AP"}
{"created":"2024-02-17 19:31:05","title":"Scattering and localized states for defocusing nonlinear Schr\u00f6dinger equations with potential","abstract":"We study the large time behavior of global energy class solutions of the one dimensional nonlinear Schr\\\"odinger equation with a general localized potential term and a defocusing nonlinear term. By using a new type of interaction Morawetz estimate localized to an exterior region, we prove that these solutions decompose into a free wave and a weakly localized part which is asymptotically orthogonal to any fixed free wave. We further show that the $L^2$ norm of this weakly localized part is concentrated in the region $|x| \\leq t^{1/2+}$, and that the energy ($\\dot{H}^1$) norm is concentrated in $|x| \\leq t^{1/3+}$.","sentences":["We study the large time behavior of global energy class solutions of the one dimensional nonlinear Schr\\\"odinger equation with a general localized potential term and a defocusing nonlinear term.","By using a new type of interaction Morawetz estimate localized to an exterior region, we prove that these solutions decompose into a free wave and a weakly localized part which is asymptotically orthogonal to any fixed free wave.","We further show that the $L^2$ norm of this weakly localized part is concentrated in the region $|x| \\leq t^{1/2+}$, and that the energy ($\\dot{H}^1$) norm is concentrated in $|x| \\leq t^{1/3+}$."],"url":"http://arxiv.org/abs/2402.11366v1","category":"math.AP"}
{"created":"2024-02-17 19:30:33","title":"Data-Driven Stochastic AC-OPF using Gaussian Processes","abstract":"The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases. To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning Gaussian process regression (GPR) model. The full Gaussian process (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs. The proposed approach uses various approximations for GP-uncertainty propagation. The full GP CC-OPF approach exhibits highly competitive and promising results, outperforming the state-of-the-art sample-based chance constraint approaches. To further improve the robustness and complexity/accuracy trade-off of the full GP CC-OPF, a fast data-driven setup is proposed. This setup relies on the sparse and hybrid Gaussian processes (GP) framework to model the power flow equations with input uncertainty.","sentences":["The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem.","Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact.","The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases.","To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning Gaussian process regression (GPR) model.","The full Gaussian process (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs.","The proposed approach uses various approximations for GP-uncertainty propagation.","The full GP CC-OPF approach exhibits highly competitive and promising results, outperforming the state-of-the-art sample-based chance constraint approaches.","To further improve the robustness and complexity/accuracy trade-off of the full GP CC-OPF, a fast data-driven setup is proposed.","This setup relies on the sparse and hybrid Gaussian processes (GP) framework to model the power flow equations with input uncertainty."],"url":"http://arxiv.org/abs/2402.11365v1","category":"cs.LG"}
{"created":"2024-02-17 18:51:21","title":"Exploiting T-norms for Deep Learning in Autonomous Driving","abstract":"Deep learning has been at the core of the autonomous driving field development, due to the neural networks' success in finding patterns in raw data and turning them into accurate predictions. Moreover, recent neuro-symbolic works have shown that incorporating the available background knowledge about the problem at hand in the loss function via t-norms can further improve the deep learning models' performance. However, t-norm-based losses may have very high memory requirements and, thus, they may be impossible to apply in complex application domains like autonomous driving. In this paper, we show how it is possible to define memory-efficient t-norm-based losses, allowing for exploiting t-norms for the task of event detection in autonomous driving. We conduct an extensive experimental analysis on the ROAD-R dataset and show (i) that our proposal can be implemented and run on GPUs with less than 25 GiB of available memory, while standard t-norm-based losses are estimated to require more than 100 GiB, far exceeding the amount of memory normally available, (ii) that t-norm-based losses improve performance, especially when limited labelled data are available, and (iii) that t-norm-based losses can further improve performance when exploited on both labelled and unlabelled data.","sentences":["Deep learning has been at the core of the autonomous driving field development, due to the neural networks' success in finding patterns in raw data and turning them into accurate predictions.","Moreover, recent neuro-symbolic works have shown that incorporating the available background knowledge about the problem at hand in the loss function via t-norms can further improve the deep learning models' performance.","However, t-norm-based losses may have very high memory requirements and, thus, they may be impossible to apply in complex application domains like autonomous driving.","In this paper, we show how it is possible to define memory-efficient t-norm-based losses, allowing for exploiting t-norms for the task of event detection in autonomous driving.","We conduct an extensive experimental analysis on the ROAD-R dataset and show (i) that our proposal can be implemented and run on GPUs with less than 25 GiB of available memory, while standard t-norm-based losses are estimated to require more than 100 GiB, far exceeding the amount of memory normally available, (ii) that t-norm-based losses improve performance, especially when limited labelled data are available, and (iii) that t-norm-based losses can further improve performance when exploited on both labelled and unlabelled data."],"url":"http://arxiv.org/abs/2402.11362v1","category":"cs.LG"}
{"created":"2024-02-17 18:48:41","title":"The matrix-free macro-element hybridized Discontinuous Galerkin method for steady and unsteady compressible flows","abstract":"The macro-element variant of the hybridized discontinuous Galerkin (HDG) method combines advantages of continuous and discontinuous finite element discretization. In this paper, we investigate the performance of the macro-element HDG method for the analysis of compressible flow problems at moderate Reynolds numbers. To efficiently handle the corresponding large systems of equations, we explore several strategies at the solver level. On the one hand, we devise a second-layer static condensation approach that reduces the size of the local system matrix in each macro-element and hence the factorization time of the local solver. On the other hand, we employ a multi-level preconditioner based on the FGMRES solver for the global system that integrates well within a matrix-free implementation. In addition, we integrate a standard diagonally implicit Runge-Kutta scheme for time integration. We test the matrix-free macro-element HDG method for compressible flow benchmarks, including Couette flow, flow past a sphere, and the Taylor-Green vortex. Our results show that unlike standard HDG, the macro-element HDG method can operate efficiently for moderate polynomial degrees, as the local computational load can be flexibly increased via mesh refinement within a macro-element. Our results also show that due to the balance of local and global operations, the reduction in degrees of freedom, and the reduction of the global problem size and the number of iterations for its solution, the macro-element HDG method can be a competitive option for the analysis of compressible flow problems.","sentences":["The macro-element variant of the hybridized discontinuous Galerkin (HDG) method combines advantages of continuous and discontinuous finite element discretization.","In this paper, we investigate the performance of the macro-element HDG method for the analysis of compressible flow problems at moderate Reynolds numbers.","To efficiently handle the corresponding large systems of equations, we explore several strategies at the solver level.","On the one hand, we devise a second-layer static condensation approach that reduces the size of the local system matrix in each macro-element and hence the factorization time of the local solver.","On the other hand, we employ a multi-level preconditioner based on the FGMRES solver for the global system that integrates well within a matrix-free implementation.","In addition, we integrate a standard diagonally implicit Runge-Kutta scheme for time integration.","We test the matrix-free macro-element HDG method for compressible flow benchmarks, including Couette flow, flow past a sphere, and the Taylor-Green vortex.","Our results show that unlike standard HDG, the macro-element HDG method can operate efficiently for moderate polynomial degrees, as the local computational load can be flexibly increased via mesh refinement within a macro-element.","Our results also show that due to the balance of local and global operations, the reduction in degrees of freedom, and the reduction of the global problem size and the number of iterations for its solution, the macro-element HDG method can be a competitive option for the analysis of compressible flow problems."],"url":"http://arxiv.org/abs/2402.11361v1","category":"cs.CE"}
{"created":"2024-02-17 17:51:03","title":"Wormholes in f(R,T) gravity with Casimir stress energy","abstract":"The Casimir stress energy is known to result in the violation of the null energy condition on the energy momentum tensor. This phenomenon makes such an equation of state an ideal candidate to generate wormhole solutions. We generate classes of wormhole solutions with this equation of state in $f(R,T)$ gravity theory, where $R$ represents the Ricci scalar and $T$ is the trace of the stress-energy tensor. For the background geometry we choose a static and spherically symmetric metric, and derive the field equations for exact wormhole solutions. For the specific choice of the Casimir equation of state (EoS) relating the energy-momentum tensor components [ Kar and Sahdev: Phys. Rev D {\\bf 52} 2030 (1995)] and different choices for redshift functions, we determine the wormhole geometry. The obtained wormhole solutions exhibit the violation of null energy conditions and all qualitative constraints demanded of physically realisable wormholes are satisfied. This is demonstrated via graphical plots for a suitably chosen range of values of the f(R,T) coupling parameter.","sentences":["The Casimir stress energy is known to result in the violation of the null energy condition on the energy momentum tensor.","This phenomenon makes such an equation of state an ideal candidate to generate wormhole solutions.","We generate classes of wormhole solutions with this equation of state in $f(R,T)$ gravity theory, where $R$ represents the Ricci scalar and $T$ is the trace of the stress-energy tensor.","For the background geometry we choose a static and spherically symmetric metric, and derive the field equations for exact wormhole solutions.","For the specific choice of the Casimir equation of state (EoS) relating the energy-momentum tensor components [ Kar and Sahdev: Phys.","Rev D {\\bf 52} 2030 (1995)] and different choices for redshift functions, we determine the wormhole geometry.","The obtained wormhole solutions exhibit the violation of null energy conditions and all qualitative constraints demanded of physically realisable wormholes are satisfied.","This is demonstrated via graphical plots for a suitably chosen range of values of the f(R,T) coupling parameter."],"url":"http://arxiv.org/abs/2402.11348v1","category":"gr-qc"}
{"created":"2024-02-17 17:35:06","title":"Solving the Regge-Wheeler and Teukolsky equations: supervised vs. unsupervised physics-informed neural networks","abstract":"Expanding on the research on physics-informed neural networks (PINNs) to solve the eigenvalue problems in black hole (BH) perturbation theory, the supervised learning approach was investigated to solve the Regge-Wheeler and Teukolsky equations governing gravitational perturbations of Schwarzschild and Kerr BHs. Previous works have applied unsupervised PINNs to compute quasinormal mode frequencies, i.e. the discrete spectra of eigenvalues that satisfy the BH perturbation equations; however, that was limited to only the zeroth and first overtone (i.e. n = 0, 1). In this paper, supervised learning is used to compute higher overtones with approximation errors accentuated primarily by the input data rather than the PINNs themselves. Our results show the ability of PINNs to approximate overtones up to the limits set by the input data, which concurs with the universal approximation theory of neural networks.","sentences":["Expanding on the research on physics-informed neural networks (PINNs) to solve the eigenvalue problems in black hole (BH) perturbation theory, the supervised learning approach was investigated to solve the Regge-Wheeler and Teukolsky equations governing gravitational perturbations of Schwarzschild and Kerr BHs.","Previous works have applied unsupervised PINNs to compute quasinormal mode frequencies, i.e. the discrete spectra of eigenvalues that satisfy the BH perturbation equations; however, that was limited to only the zeroth and first overtone (i.e. n = 0, 1).","In this paper, supervised learning is used to compute higher overtones with approximation errors accentuated primarily by the input data rather than the PINNs themselves.","Our results show the ability of PINNs to approximate overtones up to the limits set by the input data, which concurs with the universal approximation theory of neural networks."],"url":"http://arxiv.org/abs/2402.11343v1","category":"gr-qc"}
{"created":"2024-02-17 17:13:42","title":"Graph Neural Networks for Predicting Solubility in Diverse Solvents using MolMerger incorporating Solute-solvent Interactions","abstract":"Prediction of solubility has been a complex and challenging physiochemical problem that has tremendous implications in the chemical and pharmaceutical industry. Recent advancements in machine learning methods have provided great scope for predicting the reliable solubility of a large number of molecular systems. However, most of these methods rely on using physical properties obtained from experiments and or expensive quantum chemical calculations. Here, we developed a method that utilizes a graphical representation of solute-solvent interactions using `MolMerger', which captures the strongest polar interactions between molecules using Gasteiger charges and creates a graph incorporating the true nature of the system. Using these graphs as input, a neural network learns the correlation between the structural properties of a molecule in the form of node embedding and its physiochemical properties as output. This approach has been used to calculate molecular solubility by predicting the Log solubility values of various organic molecules and pharmaceuticals in diverse sets of solvents.","sentences":["Prediction of solubility has been a complex and challenging physiochemical problem that has tremendous implications in the chemical and pharmaceutical industry.","Recent advancements in machine learning methods have provided great scope for predicting the reliable solubility of a large number of molecular systems.","However, most of these methods rely on using physical properties obtained from experiments and or expensive quantum chemical calculations.","Here, we developed a method that utilizes a graphical representation of solute-solvent interactions using `MolMerger', which captures the strongest polar interactions between molecules using Gasteiger charges and creates a graph incorporating the true nature of the system.","Using these graphs as input, a neural network learns the correlation between the structural properties of a molecule in the form of node embedding and its physiochemical properties as output.","This approach has been used to calculate molecular solubility by predicting the Log solubility values of various organic molecules and pharmaceuticals in diverse sets of solvents."],"url":"http://arxiv.org/abs/2402.11340v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-17 16:57:25","title":"On a recent extension of a family of biprojective APN functions","abstract":"APN functions play a big role as primitives in symmetric cryptography as building blocks that yield optimal resistance to differential attacks. In this note, we consider a recent extension of a biprojective APN family by G\\\"olo\\u{g}lu defined on $\\mathbb{F}_{2^{2m}}$. We show that this generalization yields functions equivalent to G\\\"olo\\u{g}lu's original family if $3\\nmid m$. If $3|m$ we show exactly how many inequivalent APN functions this new family contains. We also show that the family has the minimal image set size for an APN function and determine its Walsh spectrum, hereby settling some open problems. In our proofs, we leverage a group theoretic technique recently developed by G\\\"olo\\u{g}lu and the author in conjunction with a group action on the set of projective polynomials.","sentences":["APN functions play a big role as primitives in symmetric cryptography as building blocks that yield optimal resistance to differential attacks.","In this note, we consider a recent extension of a biprojective APN family by G\\\"olo\\u{g}lu defined on $\\mathbb{F}_{2^{2m}}$. We show that this generalization yields functions equivalent to G\\\"olo\\u{g}lu's original family if $3\\nmid m$. If $3|m$ we show exactly how many inequivalent APN functions this new family contains.","We also show that the family has the minimal image set size for an APN function and determine its Walsh spectrum, hereby settling some open problems.","In our proofs, we leverage a group theoretic technique recently developed by G\\\"olo\\u{g}lu and the author in conjunction with a group action on the set of projective polynomials."],"url":"http://arxiv.org/abs/2402.11329v1","category":"math.CO"}
{"created":"2024-02-17 16:46:37","title":"Semi-Classical Electrodynamics and the Casimir Effect","abstract":"From the late 1960s and onwards the groups of Barry Ninham and Adrian Parsegian, and their many collaborators, made a number of important contributions to theory and experiment of intermolecular forces. In particular, they explored the semi-classical theory: Maxwell's equations and Planck quantization of light $\\rightarrow$ Lifshitz and Casimir interactions. We discuss some selected thought-provoking results from Ninham and his group. Some of the results have been conceived as controversial but, we dare, say never uninteresting.","sentences":["From the late 1960s and onwards the groups of Barry Ninham and Adrian Parsegian, and their many collaborators, made a number of important contributions to theory and experiment of intermolecular forces.","In particular, they explored the semi-classical theory: Maxwell's equations and Planck quantization of light $\\rightarrow$ Lifshitz and Casimir interactions.","We discuss some selected thought-provoking results from Ninham and his group.","Some of the results have been conceived as controversial but, we dare, say never uninteresting."],"url":"http://arxiv.org/abs/2402.11326v1","category":"quant-ph"}
{"created":"2024-02-17 16:33:54","title":"SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems","abstract":"Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental results show that our SpikeNAS improves the searching time and maintains high accuracy as compared to state-of-the-art while meeting the given memory budgets (e.g., 4.4x faster search with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000 Ada GPU machine), thereby quickly providing the appropriate SNN architecture for memory-constrained SNN-based systems.","sentences":["Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks.","Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware.","These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency.","Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets.","To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm.","The experimental results show that our SpikeNAS improves the searching time and maintains high accuracy as compared to state-of-the-art while meeting the given memory budgets (e.g., 4.4x faster search with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000 Ada GPU machine), thereby quickly providing the appropriate SNN architecture for memory-constrained SNN-based systems."],"url":"http://arxiv.org/abs/2402.11322v1","category":"cs.NE"}
{"created":"2024-02-17 16:20:59","title":"Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network","abstract":"Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to compensate for hysteresis. Tracking tests in task space using unseen trajectories show that the best controller reduces the mean position and orientation error by 61.39% (from 13.7 mm to 5.29 mm) and 64.04% (from 31.17{\\deg} to 11.21{\\deg}), respectively.","sentences":["Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths.","However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling.","These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator.","This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation.","We design customized fiducial markers to collect physical joint configurations as a dataset.","Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability.","Leveraging trained TCNs, we build a control algorithm to compensate for hysteresis.","Tracking tests in task space using unseen trajectories show that the best controller reduces the mean position and orientation error by 61.39% (from 13.7 mm to 5.29 mm) and 64.04% (from 31.17{\\deg} to 11.21{\\deg}), respectively."],"url":"http://arxiv.org/abs/2402.11319v1","category":"cs.RO"}
{"created":"2024-02-17 16:16:24","title":"BiasBuster: a Neural Approach for Accurate Estimation of Population Statistics using Biased Location Data","abstract":"While extremely useful (e.g., for COVID-19 forecasting and policy-making, urban mobility analysis and marketing, and obtaining business insights), location data collected from mobile devices often contain data from a biased population subset, with some communities over or underrepresented in the collected datasets. As a result, aggregate statistics calculated from such datasets (as is done by various companies including Safegraph, Google, and Facebook), while ignoring the bias, leads to an inaccurate representation of population statistics. Such statistics will not only be generally inaccurate, but the error will disproportionately impact different population subgroups (e.g., because they ignore the underrepresented communities). This has dire consequences, as these datasets are used for sensitive decision-making such as COVID-19 policymaking. This paper tackles the problem of providing accurate population statistics using such biased datasets. We show that statistical debiasing, although in some cases useful, often fails to improve accuracy. We then propose BiasBuster, a neural network approach that utilizes the correlations between population statistics and location characteristics to provide accurate estimates of population statistics. Extensive experiments on real-world data show that BiasBuster improves accuracy by up to 2 times in general and up to 3 times for underrepresented populations.","sentences":["While extremely useful (e.g., for COVID-19 forecasting and policy-making, urban mobility analysis and marketing, and obtaining business insights), location data collected from mobile devices often contain data from a biased population subset, with some communities over or underrepresented in the collected datasets.","As a result, aggregate statistics calculated from such datasets (as is done by various companies including Safegraph, Google, and Facebook), while ignoring the bias, leads to an inaccurate representation of population statistics.","Such statistics will not only be generally inaccurate, but the error will disproportionately impact different population subgroups (e.g., because they ignore the underrepresented communities).","This has dire consequences, as these datasets are used for sensitive decision-making such as COVID-19 policymaking.","This paper tackles the problem of providing accurate population statistics using such biased datasets.","We show that statistical debiasing, although in some cases useful, often fails to improve accuracy.","We then propose BiasBuster, a neural network approach that utilizes the correlations between population statistics and location characteristics to provide accurate estimates of population statistics.","Extensive experiments on real-world data show that BiasBuster improves accuracy by up to 2 times in general and up to 3 times for underrepresented populations."],"url":"http://arxiv.org/abs/2402.11318v1","category":"cs.LG"}
{"created":"2024-02-17 16:01:52","title":"Bottomonium evolution with in-medium heavy quark potential from lattice QCD","abstract":"The static properties and dynamic evolution of bottomonium states in a hot QCD medium are investigated through the Schr\\\"odinger equation with complex heavy quark potentials, which are presented recently in lattice QCD study and with three different extractions. This approach builds a direct connection between the in-medium heavy quark potentials from the lattice QCD to the experimental observables. The yields and nuclear modification factors $R_{AA}$ of bottomonium in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}=5.02~\\rm TeV$ are calculated in this work. Our results show a large suppression of the bottomonium yield in heavy ion collisions due to the large imaginary potential. To understand bottomonium $R_{AA}$ based on lattice QCD potentials, we propose a fomration time for bottomonium states and find that experimental data can be well explained with the heavy quark potential extracted by the Pad\\'e fit, which shows no color screening in the real part potential.","sentences":["The static properties and dynamic evolution of bottomonium states in a hot QCD medium are investigated through the Schr\\\"odinger equation with complex heavy quark potentials, which are presented recently in lattice QCD study and with three different extractions.","This approach builds a direct connection between the in-medium heavy quark potentials from the lattice QCD to the experimental observables.","The yields and nuclear modification factors $R_{AA}$ of bottomonium in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}=5.02~\\rm TeV$ are calculated in this work.","Our results show a large suppression of the bottomonium yield in heavy ion collisions due to the large imaginary potential.","To understand bottomonium $R_{AA}$ based on lattice QCD potentials, we propose a fomration time for bottomonium states and find that experimental data can be well explained with the heavy quark potential extracted by the Pad\\'e fit, which shows no color screening in the real part potential."],"url":"http://arxiv.org/abs/2402.11316v1","category":"nucl-th"}
{"created":"2024-02-17 16:00:21","title":"Rigidity aspects of Penrose's singularity theorem","abstract":"In this paper, we study rigidity aspects of Penrose's singularity theorem. Specifically, we aim to answer the following question: if a spacetime satisfies the hypotheses of Penrose's singularity theorem except with weakly trapped surfaces instead of trapped surfaces, then what can be said about the global spacetime structure if the spacetime is null geodesically complete? In this setting, we show that we obtain a foliation of MOTS which generate totally geodesic null hypersurfaces. Depending on our starting assumptions, we obtain either local or global rigidity results. We apply our arguments to cosmological spacetimes (i.e., spacetimes with compact Cauchy surfaces) and scenarios involving topological censorship.","sentences":["In this paper, we study rigidity aspects of Penrose's singularity theorem.","Specifically, we aim to answer the following question: if a spacetime satisfies the hypotheses of Penrose's singularity theorem except with weakly trapped surfaces instead of trapped surfaces, then what can be said about the global spacetime structure if the spacetime is null geodesically complete?","In this setting, we show that we obtain a foliation of MOTS which generate totally geodesic null hypersurfaces.","Depending on our starting assumptions, we obtain either local or global rigidity results.","We apply our arguments to cosmological spacetimes (i.e., spacetimes with compact Cauchy surfaces) and scenarios involving topological censorship."],"url":"http://arxiv.org/abs/2402.11315v1","category":"gr-qc"}
{"created":"2024-02-17 15:50:19","title":"Anisotropic induced polarization modeling with neural networks and effective medium theory","abstract":"Accurately interpreting induced polarization (IP) data that reflects the inherent anisotropy of the Earth's crust requires anisotropic IP models. The Generalized Effective Medium Theory of Induced Polarization (GEMTIP) model effectively simulates the IP signatures of rocks containing polarizable minerals. A pivotal element of the GEMTIP model is calculating the depolarization tensor elements, an intensive task for anisotropic rocks because one must numerically solve six parametric integrals for each mineral inclusion. This study aims to streamline anisotropic IP simulations by extending the GEMTIP framework and introducing a machine learning approach to estimate the depolarization tensors. The theoretical contributions of this research are two-fold: (1) we augment the GEMTIP model to encompass anisotropic background conductivity and triaxial ellipsoidal inclusions, and (2) we reformulate the depolarization integrals to normalize their input and output variables, facilitating their estimation by neural networks. Validation against analytical solutions for spherical and spheroidal inclusions corroborates the accuracy of the neural network. Analyzing the neural network model, we find that the relationship between chargeability and polarizable inclusion content is increasingly uncertain for increasingly anisotropic rocks. A similar observation applies to the relationship between critical frequency and host rock conductivity. Moreover, the depolarization tensors are, on average, 56 % sensitive to inclusion anisotropy and 44 % sensitive to host rock conductivity anisotropy. Remarkably, our neural network drastically accelerates GEMTIP simulations--up to 100,000 times faster than numerical integration--without substantially sacrificing accuracy. This advancement is promising for efficient rock-scale IP modeling in complex and anisotropic geological settings.","sentences":["Accurately interpreting induced polarization (IP) data that reflects the inherent anisotropy of the Earth's crust requires anisotropic IP models.","The Generalized Effective Medium Theory of Induced Polarization (GEMTIP) model effectively simulates the IP signatures of rocks containing polarizable minerals.","A pivotal element of the GEMTIP model is calculating the depolarization tensor elements, an intensive task for anisotropic rocks because one must numerically solve six parametric integrals for each mineral inclusion.","This study aims to streamline anisotropic IP simulations by extending the GEMTIP framework and introducing a machine learning approach to estimate the depolarization tensors.","The theoretical contributions of this research are two-fold: (1) we augment the GEMTIP model to encompass anisotropic background conductivity and triaxial ellipsoidal inclusions, and (2) we reformulate the depolarization integrals to normalize their input and output variables, facilitating their estimation by neural networks.","Validation against analytical solutions for spherical and spheroidal inclusions corroborates the accuracy of the neural network.","Analyzing the neural network model, we find that the relationship between chargeability and polarizable inclusion content is increasingly uncertain for increasingly anisotropic rocks.","A similar observation applies to the relationship between critical frequency and host rock conductivity.","Moreover, the depolarization tensors are, on average, 56 % sensitive to inclusion anisotropy and 44 % sensitive to host rock conductivity anisotropy.","Remarkably, our neural network drastically accelerates GEMTIP simulations--up to 100,000 times faster than numerical integration--without substantially sacrificing accuracy.","This advancement is promising for efficient rock-scale IP modeling in complex and anisotropic geological settings."],"url":"http://arxiv.org/abs/2402.11313v1","category":"physics.geo-ph"}
{"created":"2024-02-17 15:47:22","title":"Holomorphic foliations with no transversely projective structure","abstract":"We prove that on the product of two elliptic curves a generic nonsingular turbulent foliation does not admit any transversely projective structure.","sentences":["We prove that on the product of two elliptic curves a generic nonsingular turbulent foliation does not admit any transversely projective structure."],"url":"http://arxiv.org/abs/2402.11310v1","category":"math.DG"}
{"created":"2024-02-17 15:46:34","title":"On derivative-free extended Kalman filtering and its Matlab-oriented square-root implementations for state estimation in continuous-discrete nonlinear stochastic systems","abstract":"Recent research in nonlinear filtering and signal processing has suggested an efficient derivative-free Extended Kalman filter (EKF) designed for discrete-time stochastic systems. Such approach, however, has failed to address the estimation problem for continuous-discrete models. In this paper, we develop a novel continuous-discrete derivative-free EKF methodology by deriving the related moment differential equations (MDEs) and sample point differential equations (SPDEs). Additionally, we derive their Cholesky-based square-root MDEs and SPDEs and obtain several numerically stable derivative-free EKF methods. Finally, we propose the MATLAB-oriented implementations for all continuous-discrete derivative-free EKF algorithms derived. They are easy to implement because of the built-in fashion of the MATLAB numerical integrators utilized for solving either the MDEs or SPDEs in use, which are the ordinary differential equations (ODEs). More importantly, these are accurate derivative-free EKF implementations because any built-in MATLAB ODE solver includes the discretization error control that bounds the discretization error arisen and makes the implementation methods accurate. Besides, this is done in automatic way and no extra coding is required from users. The new filters are particularly effective for working with stochastic systems with highly nonlinear and/or nondifferentiable drift and observation functions, i.e. when the calculation of Jacobian matrices are either problematical or questionable. The performance of the novel filtering methods is demonstrated on several numerical tests.","sentences":["Recent research in nonlinear filtering and signal processing has suggested an efficient derivative-free Extended Kalman filter (EKF) designed for discrete-time stochastic systems.","Such approach, however, has failed to address the estimation problem for continuous-discrete models.","In this paper, we develop a novel continuous-discrete derivative-free EKF methodology by deriving the related moment differential equations (MDEs) and sample point differential equations (SPDEs).","Additionally, we derive their Cholesky-based square-root MDEs and SPDEs and obtain several numerically stable derivative-free EKF methods.","Finally, we propose the MATLAB-oriented implementations for all continuous-discrete derivative-free EKF algorithms derived.","They are easy to implement because of the built-in fashion of the MATLAB numerical integrators utilized for solving either the MDEs or SPDEs in use, which are the ordinary differential equations (ODEs).","More importantly, these are accurate derivative-free EKF implementations because any built-in MATLAB ODE solver includes the discretization error control that bounds the discretization error arisen and makes the implementation methods accurate.","Besides, this is done in automatic way and no extra coding is required from users.","The new filters are particularly effective for working with stochastic systems with highly nonlinear and/or nondifferentiable drift and observation functions, i.e. when the calculation of Jacobian matrices are either problematical or questionable.","The performance of the novel filtering methods is demonstrated on several numerical tests."],"url":"http://arxiv.org/abs/2402.11309v1","category":"math.OC"}
{"created":"2024-02-17 15:41:28","title":"Non-constant functions with zero nonlocal gradient and their role in nonlocal Neumann-type problems","abstract":"This work revolves around properties and applications of functions whose nonlocal gradient, or more precisely, finite-horizon fractional gradient, vanishes. Surprisingly, in contrast to the classical local theory, we show that this class forms an infinite-dimensional vector space. Our main result characterizes the functions with zero nonlocal gradient in terms of two simple features, namely, their values in a layer around the boundary and their average. The proof exploits recent progress in the solution theory of boundary-value problems with pseudo-differential operators. We complement these findings with a discussion of the regularity properties of such functions and give illustrative examples. Regarding applications, we provide several useful technical tools for working with nonlocal Sobolev spaces when the common complementary-value conditions are dropped. Among these, are new nonlocal Poincar\\'e inequalities and compactness statements, which are obtained after factoring out functions with vanishing nonlocal gradient. Following a variational approach, we exploit the previous findings to study a class of nonlocal partial differential equations subject to natural boundary conditions, in particular, nonlocal Neumann-type problems. Our analysis includes a proof of well-posedness and a rigorous link with their classical local counterparts via $\\Gamma$-convergence as the fractional parameter tends to 1.","sentences":["This work revolves around properties and applications of functions whose nonlocal gradient, or more precisely, finite-horizon fractional gradient, vanishes.","Surprisingly, in contrast to the classical local theory, we show that this class forms an infinite-dimensional vector space.","Our main result characterizes the functions with zero nonlocal gradient in terms of two simple features, namely, their values in a layer around the boundary and their average.","The proof exploits recent progress in the solution theory of boundary-value problems with pseudo-differential operators.","We complement these findings with a discussion of the regularity properties of such functions and give illustrative examples.","Regarding applications, we provide several useful technical tools for working with nonlocal Sobolev spaces when the common complementary-value conditions are dropped.","Among these, are new nonlocal Poincar\\'e inequalities and compactness statements, which are obtained after factoring out functions with vanishing nonlocal gradient.","Following a variational approach, we exploit the previous findings to study a class of nonlocal partial differential equations subject to natural boundary conditions, in particular, nonlocal Neumann-type problems.","Our analysis includes a proof of well-posedness and a rigorous link with their classical local counterparts via $\\Gamma$-convergence as the fractional parameter tends to 1."],"url":"http://arxiv.org/abs/2402.11308v1","category":"math.AP"}
{"created":"2024-02-17 14:41:22","title":"Dynamical system analysis of DBI scalar field cosmology in coincident $f(Q)$ gravity","abstract":"In this article, we offer the dynamical system analysis of the DBI (Dirac-Born-Infeld) scalar field in a modified $f(Q)$ gravity context. We have taken a polynomial form of modified gravity and used two different kinds of scalar potential, i.e., polynomial and exponential, and found a closed autonomous dynamical system of equations. We have analyzed the fixed points of such a system and commented on the conditions under which deceleration to late-time acceleration happens in this model. We have noted the similarity of the two models and have also shown that our result is indeed consistent with the previous work done on Einstein's gravity. We have also investigated the phenomenological implications of our models by plotting the EoS ($\\omega$), Energy density ($\\Omega$), and deceleration parameter ($q$) w.r.t. to e-fold time and comparing with the present value. Finally, we conclude the paper by observing how the dynamical system analysis differs in modified $f(Q)$ gravity, and we also provide some of the future scope of our work.","sentences":["In this article, we offer the dynamical system analysis of the DBI (Dirac-Born-Infeld) scalar field in a modified $f(Q)$ gravity context.","We have taken a polynomial form of modified gravity and used two different kinds of scalar potential, i.e., polynomial and exponential, and found a closed autonomous dynamical system of equations.","We have analyzed the fixed points of such a system and commented on the conditions under which deceleration to late-time acceleration happens in this model.","We have noted the similarity of the two models and have also shown that our result is indeed consistent with the previous work done on Einstein's gravity.","We have also investigated the phenomenological implications of our models by plotting the EoS ($\\omega$), Energy density ($\\Omega$), and deceleration parameter ($q$) w.r.t.","to e-fold time and comparing with the present value.","Finally, we conclude the paper by observing how the dynamical system analysis differs in modified $f(Q)$ gravity, and we also provide some of the future scope of our work."],"url":"http://arxiv.org/abs/2402.11300v1","category":"gr-qc"}
{"created":"2024-02-17 14:39:02","title":"Quasi-optimal complexity $hp$-FEM for Poisson on a rectangle","abstract":"We show, in one dimension, that an $hp$-Finite Element Method ($hp$-FEM) discretisation can be solved in optimal complexity because the discretisation has a special sparsity structure that ensures that the \\emph{reverse Cholesky factorisation} -- Cholesky starting from the bottom right instead of the top left -- remains sparse. Moreover, computing and inverting the factorisation almost entirely trivially parallelises across the different elements. By incorporating this approach into an Alternating Direction Implicit (ADI) method \\`a la Fortunato and Townsend (2020) we can solve, within a prescribed tolerance, an $hp$-FEM discretisation of the (screened) Poisson equation on a rectangle, in parallel, with quasi-optimal complexity: $O(N^2 \\log N)$ operations where $N$ is the maximal total degrees of freedom in each dimension. When combined with fast Legendre transforms we can also solve nonlinear time-evolution partial differential equations in a quasi-optimal complexity of $O(N^2 \\log^2 N)$ operations, which we demonstrate on the (viscid) Burgers' equation.","sentences":["We show, in one dimension, that an $hp$-Finite Element Method ($hp$-FEM) discretisation can be solved in optimal complexity because the discretisation has a special sparsity structure that ensures that the \\emph{reverse Cholesky factorisation} -- Cholesky starting from the bottom right instead of the top left -- remains sparse.","Moreover, computing and inverting the factorisation almost entirely trivially parallelises across the different elements.","By incorporating this approach into an Alternating Direction Implicit (ADI) method \\`a la Fortunato and Townsend (2020) we can solve, within a prescribed tolerance, an $hp$-FEM discretisation of the (screened) Poisson equation on a rectangle, in parallel, with quasi-optimal complexity: $O(N^2 \\log N)$ operations where $N$ is the maximal total degrees of freedom in each dimension.","When combined with fast Legendre transforms we can also solve nonlinear time-evolution partial differential equations in a quasi-optimal complexity of $O(N^2 \\log^2 N)$ operations, which we demonstrate on the (viscid) Burgers' equation."],"url":"http://arxiv.org/abs/2402.11299v1","category":"math.NA"}
{"created":"2024-02-17 14:17:06","title":"Predicting Breast Cancer Phenotypes from Single-cell RNA-seq Data Using CloudPred","abstract":"Numerous tools have been recently developed to predict disease phenotypes using single-cell RNA sequencing (RNA-seq) data. CloudPred is an end-to-end differentiable learning algorithm coupled with a biologically informed mixture model, originally tested on lupus data. This study extends CloudPred's applications to breast cancer disease phenotype prediction to test its robustness and applicability on untested and unrelated biological data. When applying a breast cancer single-cell RNA seq dataset, CloudPred achieved an area under the ROC curve (AUC) of 1 in predicting cancer status and performed better than a linear and Deepset models.","sentences":["Numerous tools have been recently developed to predict disease phenotypes using single-cell RNA sequencing (RNA-seq) data.","CloudPred is an end-to-end differentiable learning algorithm coupled with a biologically informed mixture model, originally tested on lupus data.","This study extends CloudPred's applications to breast cancer disease phenotype prediction to test its robustness and applicability on untested and unrelated biological data.","When applying a breast cancer single-cell RNA seq dataset, CloudPred achieved an area under the ROC curve (AUC) of 1 in predicting cancer status and performed better than a linear and Deepset models."],"url":"http://arxiv.org/abs/2402.11289v1","category":"q-bio.GN"}
{"created":"2024-02-17 13:59:09","title":"Prospects for joint reconstruction of imaging air Cherenkov Telescope array and extensive air shower array","abstract":"In this paper we proposed a joint reconstruction of \\gray events using both extensive air array (EAS) and Imaging air Cherenkov Telescope array (IACT). We considered eight Cherenkov telescopes to be built on the LHAASO (Large High Altitude Air Shower Observatory) site and investigate the improvement in differential sensitivity when combining the information from both IACT and Moun detectors of LHAASO-KM2A. We found that due to the higher cosmic ray background rejection power and higher gamma ray retention ratio provided by muon detectors of LHAASO, such a joint reconstruction can significantly improve the sensitivity of IACTs, especially for extended sources and long exposure time. In this article, we have shown the performance of an eight-telescopes mini array, and our results indicate that above $10~\\rm TeV$, the sensitivity can be improved by muon detector from $25\\% - 60\\%$ in different energy ranges.","sentences":["In this paper we proposed a joint reconstruction of \\gray events using both extensive air array (EAS) and Imaging air Cherenkov Telescope array (IACT).","We considered eight Cherenkov telescopes to be built on the LHAASO (Large High Altitude Air Shower Observatory) site and investigate the improvement in differential sensitivity when combining the information from both IACT and Moun detectors of LHAASO-KM2A. We found that due to the higher cosmic ray background rejection power and higher gamma ray retention ratio provided by muon detectors of LHAASO, such a joint reconstruction can significantly improve the sensitivity of IACTs, especially for extended sources and long exposure time.","In this article, we have shown the performance of an eight-telescopes mini array, and our results indicate that above $10~\\rm TeV$, the sensitivity can be improved by muon detector from $25\\% - 60\\%$ in different energy ranges."],"url":"http://arxiv.org/abs/2402.11286v1","category":"astro-ph.HE"}
{"created":"2024-02-19 17:12:16","title":"Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling","abstract":"This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.","sentences":["This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm.","It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA).","The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step.","The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments.","These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework."],"url":"http://arxiv.org/abs/2402.12292v1","category":"stat.ML"}
{"created":"2024-02-19 16:34:59","title":"Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2","abstract":"Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers.","sentences":["Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models.","In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services.","We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers."],"url":"http://arxiv.org/abs/2402.12271v1","category":"cs.DC"}
{"created":"2024-02-19 15:44:54","title":"The Fundamental Limits of Least-Privilege Learning","abstract":"The promise of least-privilege learning -- to find feature representations that are useful for a learning task but prevent inference of any sensitive information unrelated to this task -- is highly appealing. However, so far this concept has only been stated informally. It thus remains an open question whether and how we can achieve this goal. In this work, we provide the first formalisation of the least-privilege principle for machine learning and characterise its feasibility. We prove that there is a fundamental trade-off between a representation's utility for a given task and its leakage beyond the intended task: it is not possible to learn representations that have high utility for the intended task but, at the same time prevent inference of any attribute other than the task label itself. This trade-off holds regardless of the technique used to learn the feature mappings that produce these representations. We empirically validate this result for a wide range of learning techniques, model architectures, and datasets.","sentences":["The promise of least-privilege learning -- to find feature representations that are useful for a learning task but prevent inference of any sensitive information unrelated to this task -- is highly appealing.","However, so far this concept has only been stated informally.","It thus remains an open question whether and how we can achieve this goal.","In this work, we provide the first formalisation of the least-privilege principle for machine learning and characterise its feasibility.","We prove that there is a fundamental trade-off between a representation's utility for a given task and its leakage beyond the intended task: it is not possible to learn representations that have high utility for the intended task but, at the same time prevent inference of any attribute other than the task label itself.","This trade-off holds regardless of the technique used to learn the feature mappings that produce these representations.","We empirically validate this result for a wide range of learning techniques, model architectures, and datasets."],"url":"http://arxiv.org/abs/2402.12235v1","category":"cs.LG"}
{"created":"2024-02-19 14:51:20","title":"Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training","abstract":"Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness.","sentences":["Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples.","Adversarial training is used to mitigate this problem by increasing robustness against these attacks.","However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples.","The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified.","This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems.","Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial.","AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment.","Through our evaluations, we demonstrate the superior performance of AFA.","The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy.","We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness."],"url":"http://arxiv.org/abs/2402.12187v1","category":"cs.CV"}
{"created":"2024-02-19 14:28:31","title":"BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence","abstract":"Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.","sentences":["Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy.","However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality.","This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment.","We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning.","Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods.","The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering."],"url":"http://arxiv.org/abs/2402.12174v1","category":"cs.CL"}
{"created":"2024-02-19 14:02:13","title":"MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports","abstract":"Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum. This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches. Based on the tennis Grand Slam men's singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas. For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players. Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established. For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different methods were used to visualize the momentum. For further analysis of the momentum fluctuation, it is based on the popular CUMSUM algorithm in the industry as well as the RUN Test, and the result shows the momentum is not random and the trend might be random. At last, the robustness of the fusion model is analyzed by Monte Carlo simulation.","sentences":["Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum.","This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches.","Based on the tennis Grand Slam men's singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas.","For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players.","Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established.","For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different methods were used to visualize the momentum.","For further analysis of the momentum fluctuation, it is based on the popular CUMSUM algorithm in the industry as well as the RUN Test, and the result shows the momentum is not random and the trend might be random.","At last, the robustness of the fusion model is analyzed by Monte Carlo simulation."],"url":"http://arxiv.org/abs/2402.12149v1","category":"cs.LG"}
{"created":"2024-02-19 11:48:11","title":"Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview","abstract":"This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.","sentences":["This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning.","A special focus lies on point estimators and their robustness against adversarial perturbations.","In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees.","Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties."],"url":"http://arxiv.org/abs/2402.12072v1","category":"eess.IV"}
{"created":"2024-02-19 10:42:34","title":"When Do Off-Policy and On-Policy Policy Gradient Methods Align?","abstract":"Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.","sentences":["Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces.","These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available.","A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling.","A well-established off-policy objective is the excursion objective.","This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap.","We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met."],"url":"http://arxiv.org/abs/2402.12034v1","category":"stat.ML"}
{"created":"2024-02-19 10:34:13","title":"Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?","abstract":"The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.","sentences":["The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP.","This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks.","Among such tasks, this paper focuses on speech-to-text translation (ST).","By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them.","Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice.","Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST."],"url":"http://arxiv.org/abs/2402.12025v1","category":"cs.CL"}
{"created":"2024-02-19 09:19:01","title":"Bayesian Active Learning for Censored Regression","abstract":"Bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. This is commonly done by maximising the Bayesian Active Learning by Disagreement (BALD) acquisitions function. However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed. To address this, we derive the entropy and the mutual information for censored distributions and derive the BALD objective for active learning in censored regression ($\\mathcal{C}$-BALD). We propose a novel modelling approach to estimate the $\\mathcal{C}$-BALD objective and use it for active learning in the censored setting. Across a wide range of datasets and models, we demonstrate that $\\mathcal{C}$-BALD outperforms other Bayesian active learning methods in censored regression.","sentences":["Bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters.","This is commonly done by maximising the Bayesian Active Learning by Disagreement (BALD) acquisitions function.","However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed.","To address this, we derive the entropy and the mutual information for censored distributions and derive the BALD objective for active learning in censored regression ($\\mathcal{C}$-BALD).","We propose a novel modelling approach to estimate the $\\mathcal{C}$-BALD objective and use it for active learning in the censored setting.","Across a wide range of datasets and models, we demonstrate that $\\mathcal{C}$-BALD outperforms other Bayesian active learning methods in censored regression."],"url":"http://arxiv.org/abs/2402.11973v1","category":"cs.LG"}
{"created":"2024-02-19 07:59:16","title":"PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training","abstract":"Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos. Most recent approaches utilize convolutional networks with limited temporal modeling capability or ignore long temporal context. Supervised rPPG methods are also severely limited by scarce data availability. In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG transformer network and a self-supervised pre-training strategy that exploits unlabeled data to improve our model. Our strategy leverages traditional methods and image masking to provide pseudo-labels for self-supervised pre-training. Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in supervised training. Furthermore, we demonstrate that our self-supervised pre-training strategy further improves our model's performance by leveraging representations learned from unlabeled data.","sentences":["Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos.","Most recent approaches utilize convolutional networks with limited temporal modeling capability or ignore long temporal context.","Supervised rPPG methods are also severely limited by scarce data availability.","In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG transformer network and a self-supervised pre-training strategy that exploits unlabeled data to improve our model.","Our strategy leverages traditional methods and image masking to provide pseudo-labels for self-supervised pre-training.","Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in supervised training.","Furthermore, we demonstrate that our self-supervised pre-training strategy further improves our model's performance by leveraging representations learned from unlabeled data."],"url":"http://arxiv.org/abs/2402.11913v1","category":"cs.CV"}
{"created":"2024-02-19 06:32:23","title":"Language-guided Image Reflection Separation","abstract":"This paper studies the problem of language-guided reflection separation, which aims at addressing the ill-posed reflection separation problem by introducing language descriptions to provide layer content. We propose a unified framework to solve this problem, which leverages the cross-attention mechanism with contrastive learning strategies to construct the correspondence between language descriptions and image layers. A gated network design and a randomized training strategy are employed to tackle the recognizable layer ambiguity. The effectiveness of the proposed method is validated by the significant performance advantage over existing reflection separation methods on both quantitative and qualitative comparisons.","sentences":["This paper studies the problem of language-guided reflection separation, which aims at addressing the ill-posed reflection separation problem by introducing language descriptions to provide layer content.","We propose a unified framework to solve this problem, which leverages the cross-attention mechanism with contrastive learning strategies to construct the correspondence between language descriptions and image layers.","A gated network design and a randomized training strategy are employed to tackle the recognizable layer ambiguity.","The effectiveness of the proposed method is validated by the significant performance advantage over existing reflection separation methods on both quantitative and qualitative comparisons."],"url":"http://arxiv.org/abs/2402.11874v1","category":"cs.CV"}
{"created":"2024-02-19 04:39:16","title":"Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios","abstract":"Monocular depth estimation from RGB images plays a pivotal role in 3D vision. However, its accuracy can deteriorate in challenging environments such as nighttime or adverse weather conditions. While long-wave infrared cameras offer stable imaging in such challenging conditions, they are inherently low-resolution, lacking rich texture and semantics as delivered by the RGB image. Current methods focus solely on a single modality due to the difficulties to identify and integrate faithful depth cues from both sources. To address these issues, this paper presents a novel approach that identifies and integrates dominant cross-modality depth features with a learning-based framework. Concretely, we independently compute the coarse depth maps with separate networks by fully utilizing the individual depth cues from each modality. As the advantageous depth spreads across both modalities, we propose a novel confidence loss steering a confidence predictor network to yield a confidence map specifying latent potential depth areas. With the resulting confidence map, we propose a multi-modal fusion network that fuses the final depth in an end-to-end manner. Harnessing the proposed pipeline, our method demonstrates the ability of robust depth estimation in a variety of difficult scenarios. Experimental results on the challenging MS$^2$ and ViViD++ datasets demonstrate the effectiveness and robustness of our method.","sentences":["Monocular depth estimation from RGB images plays a pivotal role in 3D vision.","However, its accuracy can deteriorate in challenging environments such as nighttime or adverse weather conditions.","While long-wave infrared cameras offer stable imaging in such challenging conditions, they are inherently low-resolution, lacking rich texture and semantics as delivered by the RGB image.","Current methods focus solely on a single modality due to the difficulties to identify and integrate faithful depth cues from both sources.","To address these issues, this paper presents a novel approach that identifies and integrates dominant cross-modality depth features with a learning-based framework.","Concretely, we independently compute the coarse depth maps with separate networks by fully utilizing the individual depth cues from each modality.","As the advantageous depth spreads across both modalities, we propose a novel confidence loss steering a confidence predictor network to yield a confidence map specifying latent potential depth areas.","With the resulting confidence map, we propose a multi-modal fusion network that fuses the final depth in an end-to-end manner.","Harnessing the proposed pipeline, our method demonstrates the ability of robust depth estimation in a variety of difficult scenarios.","Experimental results on the challenging MS$^2$ and ViViD++ datasets demonstrate the effectiveness and robustness of our method."],"url":"http://arxiv.org/abs/2402.11826v1","category":"cs.CV"}
{"created":"2024-02-19 04:29:45","title":"Microstructures and Accuracy of Graph Recall by Large Language Models","abstract":"Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.","sentences":["Graphs data is crucial for many applications, and much of it exists in the relations described in textual format.","As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information.","Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships.","To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks?","In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall.","We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths.","Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain."],"url":"http://arxiv.org/abs/2402.11821v1","category":"cs.LG"}
{"created":"2024-02-19 02:48:44","title":"Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation","abstract":"Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.","sentences":["Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers.","An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs.","Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training.","In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models.","We further propose indicators for optimizing models' training methods and avoiding ineffective training."],"url":"http://arxiv.org/abs/2402.11794v1","category":"cs.CL"}
{"created":"2024-02-19 02:15:34","title":"What Evidence Do Language Models Find Convincing?","abstract":"Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as \"is aspartame linked to cancer\". To resolve these ambiguous queries, one must search through a large range of websites and consider \"which, if any, of this evidence do I find convincing?\". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.","sentences":["Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as \"is aspartame linked to cancer\".","To resolve these ambiguous queries, one must search through a large range of websites and consider \"which, if any, of this evidence do I find convincing?\".","In this work, we study how LLMs answer this question.","In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No).","We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions.","Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone.","Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements."],"url":"http://arxiv.org/abs/2402.11782v1","category":"cs.CL"}
{"created":"2024-02-19 02:07:15","title":"FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model","abstract":"Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions. An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI). However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available. In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge. Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a transformer-based deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD). Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required directions on initial acquisition. Evaluations of the reconstructed FOD with Angular Correlation Coefficient and qualitative visualizations reveal superior performance than the state-of-the-art in HCP testing data. Open source code for reproducibility is available at https://github.com/MICLab-Unicamp/FOD-Swin-Net.","sentences":["Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions.","An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI).","However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available.","In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge.","Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a transformer-based deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD).","Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required directions on initial acquisition.","Evaluations of the reconstructed FOD with Angular Correlation Coefficient and qualitative visualizations reveal superior performance than the state-of-the-art in HCP testing data.","Open source code for reproducibility is available at https://github.com/MICLab-Unicamp/FOD-Swin-Net."],"url":"http://arxiv.org/abs/2402.11775v1","category":"eess.IV"}
{"created":"2024-02-19 00:53:48","title":"SPML: A DSL for Defending Language Models Against Prompt Attacks","abstract":"Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language design challenges. Additionally, we introduce a groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering the inaugural language and benchmark for chatbot definition evaluation. Experiments across datasets demonstrate SPML's proficiency in understanding attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data and codes are publicly available at: https://prompt-compiler.github.io/SPML/.","sentences":["Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots.","However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses.","Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored.","This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots.","SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs.","It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language design challenges.","Additionally, we introduce a groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering the inaugural language and benchmark for chatbot definition evaluation.","Experiments across datasets demonstrate SPML's proficiency in understanding attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA.","Our data and codes are publicly available at: https://prompt-compiler.github.io/SPML/."],"url":"http://arxiv.org/abs/2402.11755v1","category":"cs.LG"}
{"created":"2024-02-19 00:39:31","title":"In-Context Learning Demonstration Selection via Influence Analysis","abstract":"Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance. To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.","sentences":["Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update.","Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations.","Selecting effective demonstrations for ICL is still an open research challenge.","To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions.","Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance.","To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning.","We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.11750v1","category":"cs.CL"}
{"created":"2024-02-18 23:59:54","title":"Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance","abstract":"Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation strategies to mitigate the issue. Our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pretrained features may have unknown biases that can be diagnosed through their spectra.","sentences":["Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance.","This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets.","In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice.","To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting.","We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation strategies to mitigate the issue.","Our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pretrained features may have unknown biases that can be diagnosed through their spectra."],"url":"http://arxiv.org/abs/2402.11742v1","category":"cs.LG"}
{"created":"2024-02-18 23:39:00","title":"Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding","abstract":"Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional. In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. We prove that it does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computationally cheaper methods. Moreover, we provide early experimental evidence that a faster rate of convergence, though not worst-case, is likely.","sentences":["Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS).","In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional.","In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding.","We prove that it does outperform i.i.d.","Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error.","While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computationally cheaper methods.","Moreover, we provide early experimental evidence that a faster rate of convergence, though not worst-case, is likely."],"url":"http://arxiv.org/abs/2402.11736v1","category":"cs.LG"}
{"created":"2024-02-18 22:36:19","title":"How Susceptible are Large Language Models to Ideological Manipulation?","abstract":"Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.","sentences":["Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information.","This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated.","In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data.","Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs.","Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones.","The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators.","It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs."],"url":"http://arxiv.org/abs/2402.11725v1","category":"cs.CL"}
{"created":"2024-02-18 22:29:04","title":"Large Language Models as Data Augmenters for Cold-Start Item Recommendation","abstract":"The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item recommendation for various recommendation models.","sentences":["The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems.","Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions.","To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training.","We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions.","The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss.","Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item recommendation for various recommendation models."],"url":"http://arxiv.org/abs/2402.11724v1","category":"cs.IR"}
{"created":"2024-02-18 19:11:58","title":"Opening the black box of language acquisition","abstract":"Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory as a key component of the language learning process. Since other animals do not seem to have a faithful sequence memory, this may explain why only humans have developed complex languages.","sentences":["Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data.","However, it is unclear whether or how these models represent grammatical information from the learned languages.","In addition, the models must be pre-trained on large corpora before they can be used.","In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language.","Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking.","The learning mechanism is based on the principles of reinforcement learning.","We test our architecture on a number of natural-like toy languages.","Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning.","Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory as a key component of the language learning process.","Since other animals do not seem to have a faithful sequence memory, this may explain why only humans have developed complex languages."],"url":"http://arxiv.org/abs/2402.11681v1","category":"cs.CL"}
{"created":"2024-02-19 18:57:02","title":"Computing Enclosing Depth","abstract":"Enclosing depth is a recently introduced depth measure which gives a lower bound to many depth measures studied in the literature. So far, enclosing depth has only been studied from a combinatorial perspective. In this work, we give the first algorithms to compute the enclosing depth of a query point with respect to a data point set in any dimension. In the plane we are able to optimize the algorithm to get a runtime of O(n log n). In constant dimension, our algorithms still run in polynomial time.","sentences":["Enclosing depth is a recently introduced depth measure which gives a lower bound to many depth measures studied in the literature.","So far, enclosing depth has only been studied from a combinatorial perspective.","In this work, we give the first algorithms to compute the enclosing depth of a query point with respect to a data point set in any dimension.","In the plane we are able to optimize the algorithm to get a runtime of O(n log n).","In constant dimension, our algorithms still run in polynomial time."],"url":"http://arxiv.org/abs/2402.12371v1","category":"cs.CG"}
{"created":"2024-02-19 18:48:19","title":"SPAM-Robust Multi-axis Quantum Noise Spectroscopy in Temporally Correlated Environments","abstract":"Characterizing temporally correlated (``non-Markovian'') noise is a key prerequisite for achieving noise-tailored error mitigation and optimal device performance. Quantum noise spectroscopy can afford quantitative estimation of the noise spectral features; however, in its current form it is highly vulnerable to implementation non-idealities, notably, state-preparation and measurement (SPAM) errors. Further to that, existing protocols have been mostly developed for dephasing-dominated noise processes, with competing dephasing and relaxation effects being largely unaccounted for. We introduce quantum noise spectroscopy protocols inspired by spin-locking techniques that enable the characterization of arbitrary temporally correlated multi-axis noise on a qubit with fixed energy splitting, while remaining resilient to realistic static SPAM errors. By validating our protocol's performance in both numerical simulation and cloud-based IBM quantum processors, we demonstrate the successful separation and estimation of native noise spectrum components as well as SPAM error rates. We find that SPAM errors can significantly alter or mask important noise features, with spectra overestimated by up to 26.4% in a classical noise regime. Clear signatures of non-classical noise are manifest in the reconstructed IBM-qubit dephasing spectra, once SPAM-error effects are compensated for. Our work provides a timely tool for benchmarking realistic sources of noise in qubit devices.","sentences":["Characterizing temporally correlated (``non-Markovian'') noise is a key prerequisite for achieving noise-tailored error mitigation and optimal device performance.","Quantum noise spectroscopy can afford quantitative estimation of the noise spectral features; however, in its current form it is highly vulnerable to implementation non-idealities, notably, state-preparation and measurement (SPAM) errors.","Further to that, existing protocols have been mostly developed for dephasing-dominated noise processes, with competing dephasing and relaxation effects being largely unaccounted for.","We introduce quantum noise spectroscopy protocols inspired by spin-locking techniques that enable the characterization of arbitrary temporally correlated multi-axis noise on a qubit with fixed energy splitting, while remaining resilient to realistic static SPAM errors.","By validating our protocol's performance in both numerical simulation and cloud-based IBM quantum processors, we demonstrate the successful separation and estimation of native noise spectrum components as well as SPAM error rates.","We find that SPAM errors can significantly alter or mask important noise features, with spectra overestimated by up to 26.4% in a classical noise regime.","Clear signatures of non-classical noise are manifest in the reconstructed IBM-qubit dephasing spectra, once SPAM-error effects are compensated for.","Our work provides a timely tool for benchmarking realistic sources of noise in qubit devices."],"url":"http://arxiv.org/abs/2402.12361v1","category":"quant-ph"}
{"created":"2024-02-19 17:37:50","title":"Memory attacks in network nonlocality and self-testing","abstract":"We study what can or cannot be certified in communication scenarios where the assumption of independence and identical distribution (iid) between experimental rounds fails. In this respect, we prove that membership tests for non-convex sets of correlations cannot be formulated in the non-iid regime. Similarly, it is impossible to self-test non-extreme quantum operations, such as mixed states, or noisy quantum measurements, unless one allows more than a single use thereof within the same experimental round. One consequence of our results is that non-classicality in causal networks without inputs cannot be experimentally demonstrated. By analyzing optimal non-iid strategies in the triangle scenario, we raise the need to take into account the prior communication required to set up a causal network.","sentences":["We study what can or cannot be certified in communication scenarios where the assumption of independence and identical distribution (iid) between experimental rounds fails.","In this respect, we prove that membership tests for non-convex sets of correlations cannot be formulated in the non-iid regime.","Similarly, it is impossible to self-test non-extreme quantum operations, such as mixed states, or noisy quantum measurements, unless one allows more than a single use thereof within the same experimental round.","One consequence of our results is that non-classicality in causal networks without inputs cannot be experimentally demonstrated.","By analyzing optimal non-iid strategies in the triangle scenario, we raise the need to take into account the prior communication required to set up a causal network."],"url":"http://arxiv.org/abs/2402.12318v1","category":"quant-ph"}
{"created":"2024-02-19 16:30:24","title":"The Quantitative Fractional Helly theorem","abstract":"Two celebrated extensions of Helly's theorem are the Fractional Helly theorem of Katchalski and Liu (1979) and the Quantitative Volume theorem of B\\'ar\\'any, Katchalski, and Pach (1982). Improving on several recent works, we prove an optimal combination of these two results. We show that given a family $\\mathcal{F}$ of $n$ convex sets in $\\mathbb{R}^d$ such that at least $\\alpha \\binom{n}{d+1}$ of the $(d+1)$-tuples of $\\mathcal{F}$ have an intersection of volume at least 1, then one can select $\\Omega_{d,\\alpha}(n)$ members of $\\mathcal{F}$ whose intersection has volume at least $\\Omega_d(1)$.   Furthermore, with the help of this theorem, we establish a quantitative version of the $(p,q)$ theorem of Alon and Kleitman. Let $p\\geq q\\geq d+1$ and let $\\mathcal{F}$ be a finite family of convex sets in $\\mathbb{R}^d$ such that among any $p$ elements of $\\mathcal{F}$, there are $q$ that have an intersection of volume at least $1$. Then, we prove that there exists a family $T$ of $O_{p,q}(1)$ ellipsoids of volume $\\Omega_d(1)$ such that every member of $\\mathcal{F}$ contains at least one element of $T$.","sentences":["Two celebrated extensions of Helly's theorem are the Fractional Helly theorem of Katchalski and Liu (1979) and the Quantitative Volume theorem of B\\'ar\\'any, Katchalski, and Pach (1982).","Improving on several recent works, we prove an optimal combination of these two results.","We show that given a family $\\mathcal{F}$ of $n$ convex sets in $\\mathbb{R}^d$ such that at least $\\alpha \\binom{n}{d+1}$ of the $(d+1)$-tuples of $\\mathcal{F}$ have an intersection of volume at least 1, then one can select $\\Omega_{d,\\alpha}(n)$ members of $\\mathcal{F}$ whose intersection has volume at least $\\Omega_d(1)$.   Furthermore, with the help of this theorem, we establish a quantitative version of the $(p,q)$ theorem of Alon and Kleitman.","Let $p\\geq q\\geq d+1$ and let $\\mathcal{F}$ be a finite family of convex sets in $\\mathbb{R}^d$ such that among any $p$ elements of $\\mathcal{F}$, there are $q$ that have an intersection of volume at least $1$.","Then, we prove that there exists a family $T$ of $O_{p,q}(1)$ ellipsoids of volume $\\Omega_d(1)$ such that every member of $\\mathcal{F}$ contains at least one element of $T$."],"url":"http://arxiv.org/abs/2402.12268v1","category":"math.CO"}
{"created":"2024-02-19 16:12:20","title":"Stability of the coronal magnetic field around large confined and eruptive solar flares","abstract":"In order to improve our understanding on the pre-requisites of eruptive solar flares, we study and compare different measures that characterize the eruptive potential of solar active regions - the critical height for torus instability as a local measure and the helicity ratio as a global measure - with the structural properties of the underlying magnetic field, namely the altitude of the center of the current-carrying magnetic structure. Using time series of 3D optimization-based nonlinear force-free magnetic field models for 10 different active regions (ARs) around the time of large solar flares, we determine the altitudes of the current-weighted centers of the non-potential model structures. Based on the potential magnetic field, we inspect the decay index, $n$, in multiple vertical planes oriented along of or perpendicular to the flare-relevant polarity inversion line, and estimate the critical height ($h_{\\mathrm{crit}}$) for torus instability (TI) using different thresholds of $n$. The critical heights are interpreted with respect to the altitudes of the current-weighted centers of the associated non-potential structures, as well as the eruptive character of the associated flares, and the eruptive potential of the host AR, as characterized by the helicity ratio. Our most important findings are that (i) $h_{\\mathrm{crit}}$ is more segregated in terms of flare type than the helicity ratio, and that (ii) coronal field configurations with a higher eruptive potential (in terms of the helicity ratio) also appear to be more prone to TI. Furthermore, we find no pronounced differences in the altitudes of the non-potential structures prior to confined and eruptive flares.","sentences":["In order to improve our understanding on the pre-requisites of eruptive solar flares, we study and compare different measures that characterize the eruptive potential of solar active regions - the critical height for torus instability as a local measure and the helicity ratio as a global measure - with the structural properties of the underlying magnetic field, namely the altitude of the center of the current-carrying magnetic structure.","Using time series of 3D optimization-based nonlinear force-free magnetic field models for 10 different active regions (ARs) around the time of large solar flares, we determine the altitudes of the current-weighted centers of the non-potential model structures.","Based on the potential magnetic field, we inspect the decay index, $n$, in multiple vertical planes oriented along of or perpendicular to the flare-relevant polarity inversion line, and estimate the critical height ($h_{\\mathrm{crit}}$) for torus instability (TI) using different thresholds of $n$. The critical heights are interpreted with respect to the altitudes of the current-weighted centers of the associated non-potential structures, as well as the eruptive character of the associated flares, and the eruptive potential of the host AR, as characterized by the helicity ratio.","Our most important findings are that (i) $h_{\\mathrm{crit}}$ is more segregated in terms of flare type than the helicity ratio, and that (ii) coronal field configurations with a higher eruptive potential (in terms of the helicity ratio) also appear to be more prone to TI.","Furthermore, we find no pronounced differences in the altitudes of the non-potential structures prior to confined and eruptive flares."],"url":"http://arxiv.org/abs/2402.12254v1","category":"astro-ph.SR"}
{"created":"2024-02-19 14:01:34","title":"Local certification of forbidden subgraphs","abstract":"Detecting specific structures in a network has been a very active theme of research in distributed computing for at least a decade. In this paper, we start the study of subgraph detection from the perspective of local certification. Remember that a local certification is a distributed mechanism enabling the nodes of a network to check the correctness of the current configuration, thanks to small pieces of information called certificates. Our main question is: For a given graph $H$, what is the minimum certificate size that allows checking that the network does not contain $H$ as a (possibly induced) subgraph?   We show a variety of lower and upper bounds, uncovering an interesting interplay between the optimal certificate size, the size of the forbidden subgraph, and the locality of the verification. Along the way we introduce several new technical tools, in particular what we call the \\emph{layered map}, which is not specific to forbidden subgraphs and that we expect to be useful for certifying many other properties.","sentences":["Detecting specific structures in a network has been a very active theme of research in distributed computing for at least a decade.","In this paper, we start the study of subgraph detection from the perspective of local certification.","Remember that a local certification is a distributed mechanism enabling the nodes of a network to check the correctness of the current configuration, thanks to small pieces of information called certificates.","Our main question is: For a given graph $H$, what is the minimum certificate size that allows checking that the network does not contain $H$ as a (possibly induced) subgraph?   ","We show a variety of lower and upper bounds, uncovering an interesting interplay between the optimal certificate size, the size of the forbidden subgraph, and the locality of the verification.","Along the way we introduce several new technical tools, in particular what we call the \\emph{layered map}, which is not specific to forbidden subgraphs and that we expect to be useful for certifying many other properties."],"url":"http://arxiv.org/abs/2402.12148v1","category":"cs.DC"}
{"created":"2024-02-19 12:58:53","title":"Persistent Homology-Driven Optimization of Effective Relative Density Range for Triply Periodic Minimal Surface","abstract":"Triply periodic minimal surfaces (TPMSs) play a vital role in the design of porous structures, with applications in bone tissue engineering, chemical engineering, and the creation of lightweight models. However, fabrication of TPMSs via additive manufacturing is feasible only within a specific range of relative densities, termed the effective relative density range (EDR), outside of which TPMSs exhibit unmanufacturable features. In this study, the persistent homology is applied to theoretically calculate and extend the EDRs of TPMSs. The TPMSs with extended EDRs are referred to as extended TPMSs. To achieve this, TPMSs are converted into implicit B-spline representation through fitting. By analyzing the symmetry of TPMSs, a partial fitting method is utilized to preserve the symmetry and enhance fitting precision. A topological objective function is modeled based on the understanding of topological features, resulting in extended TPMSs that possess extended EDRs while maintaining a high degree of similarity to the original TPMSs. Experimental validation confirms the effectiveness of the approach in extending the EDRs of TPMSs. Furthermore, the extended TPMSs demonstrate superior performance in porous model design and topology optimization compared to their original counterparts. The extended TPMSs with increased EDRs hold promise for replacing traditional TPMSs in applications that require porous structures with varying densities.","sentences":["Triply periodic minimal surfaces (TPMSs) play a vital role in the design of porous structures, with applications in bone tissue engineering, chemical engineering, and the creation of lightweight models.","However, fabrication of TPMSs via additive manufacturing is feasible only within a specific range of relative densities, termed the effective relative density range (EDR), outside of which TPMSs exhibit unmanufacturable features.","In this study, the persistent homology is applied to theoretically calculate and extend the EDRs of TPMSs.","The TPMSs with extended EDRs are referred to as extended TPMSs.","To achieve this, TPMSs are converted into implicit B-spline representation through fitting.","By analyzing the symmetry of TPMSs, a partial fitting method is utilized to preserve the symmetry and enhance fitting precision.","A topological objective function is modeled based on the understanding of topological features, resulting in extended TPMSs that possess extended EDRs while maintaining a high degree of similarity to the original TPMSs.","Experimental validation confirms the effectiveness of the approach in extending the EDRs of TPMSs.","Furthermore, the extended TPMSs demonstrate superior performance in porous model design and topology optimization compared to their original counterparts.","The extended TPMSs with increased EDRs hold promise for replacing traditional TPMSs in applications that require porous structures with varying densities."],"url":"http://arxiv.org/abs/2402.12109v1","category":"cs.GR"}
{"created":"2024-02-19 11:42:26","title":"Inexact Restoration via random models for unconstrained noisy optimization","abstract":"We study the Inexact Restoration framework with random models for minimizing functions whose evaluation is subject to errors. We propose a constrained formulation that includes well-known stochastic problems and an algorithm applicable when the evaluation of both the function and its gradient is random and a specified accuracy of such evaluations is guaranteed with sufficiently high probability. The proposed algorithm combines the Inexact Restoration framework with a trust-region methodology based on random first-order models. We analyse the properties of the algorithm and provide the expected number of iterations performed to reach an approximate first-order optimality point. Numerical experiments show that the proposed algorithm compares well with a state-of-the-art competitor.","sentences":["We study the Inexact Restoration framework with random models for minimizing functions whose evaluation is subject to errors.","We propose a constrained formulation that includes well-known stochastic problems and an algorithm applicable when the evaluation of both the function and its gradient is random and a specified accuracy of such evaluations is guaranteed with sufficiently high probability.","The proposed algorithm combines the Inexact Restoration framework with a trust-region methodology based on random first-order models.","We analyse the properties of the algorithm and provide the expected number of iterations performed to reach an approximate first-order optimality point.","Numerical experiments show that the proposed algorithm compares well with a state-of-the-art competitor."],"url":"http://arxiv.org/abs/2402.12069v1","category":"math.OC"}
{"created":"2024-02-19 10:30:10","title":"Projected Block Coordinate Descent for sparse spike estimation","abstract":"We consider the problem of recovering off-the-grid spikes from linear measurements. The state of the art Over-Parametrized Continuous Orthogonal Matching Pursuit (OP-COMP) with Projected Gradient Descent (PGD) successfully recovers those signals. In most cases, the main computational cost lies in a unique global descent on all parameters (positions and amplitudes). In this paper, we propose to improve this algorithm by accelerating this descent step. We introduce a new algorithm, based on Block Coordinate Descent, that takes advantages of the sparse structure of the problem. Based on qualitative theoretical results, this algorithm shows improvement in calculation times in realistic synthetic microscopy experiments.","sentences":["We consider the problem of recovering off-the-grid spikes from linear measurements.","The state of the art Over-Parametrized Continuous Orthogonal Matching Pursuit (OP-COMP) with Projected Gradient Descent (PGD) successfully recovers those signals.","In most cases, the main computational cost lies in a unique global descent on all parameters (positions and amplitudes).","In this paper, we propose to improve this algorithm by accelerating this descent step.","We introduce a new algorithm, based on Block Coordinate Descent, that takes advantages of the sparse structure of the problem.","Based on qualitative theoretical results, this algorithm shows improvement in calculation times in realistic synthetic microscopy experiments."],"url":"http://arxiv.org/abs/2402.12021v1","category":"math.NA"}
{"created":"2024-02-19 10:28:34","title":"Collision-Free Robot Scheduling","abstract":"Robots are becoming an increasingly common part of scientific work within laboratory environments. In this paper, we investigate the problem of designing \\emph{schedules} for completing a set of tasks at fixed locations with multiple robots in a laboratory. We represent the laboratory as a graph with tasks placed on fixed vertices and robots represented as agents, with the constraint that no two robots may occupy the same vertex at any given timestep. Each schedule is partitioned into a set of timesteps, corresponding to a walk through the graph (allowing for a robot to wait at a vertex to complete a task), with each timestep taking time equal to the time for a robot to move from one vertex to another and each task taking some given number of timesteps during the completion of which a robot must stay at the vertex containing the task. The goal is to determine a set of schedules, with one schedule for each robot, minimising the number of timesteps taken by the schedule taking the greatest number of timesteps within the set of schedules.   We show that this problem is NP-complete for many simple classes of graphs, the problem of determining the fastest schedule, defined by the number of time steps required for a robot to visit every vertex in the schedule and complete every task assigned in its assigned schedule. Explicitly, we provide this result for complete graphs, bipartite graphs, star graphs, and planar graphs. Finally, we provide positive results for line graphs, showing that we can find an optimal set of schedules for $k$ robots completing $m$ tasks of equal length of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the length of the tasks is unbounded.","sentences":["Robots are becoming an increasingly common part of scientific work within laboratory environments.","In this paper, we investigate the problem of designing \\emph{schedules} for completing a set of tasks at fixed locations with multiple robots in a laboratory.","We represent the laboratory as a graph with tasks placed on fixed vertices and robots represented as agents, with the constraint that no two robots may occupy the same vertex at any given timestep.","Each schedule is partitioned into a set of timesteps, corresponding to a walk through the graph (allowing for a robot to wait at a vertex to complete a task), with each timestep taking time equal to the time for a robot to move from one vertex to another and each task taking some given number of timesteps during the completion of which a robot must stay at the vertex containing the task.","The goal is to determine a set of schedules, with one schedule for each robot, minimising the number of timesteps taken by the schedule taking the greatest number of timesteps within the set of schedules.   ","We show that this problem is NP-complete for many simple classes of graphs, the problem of determining the fastest schedule, defined by the number of time steps required for a robot to visit every vertex in the schedule and complete every task assigned in its assigned schedule.","Explicitly, we provide this result for complete graphs, bipartite graphs, star graphs, and planar graphs.","Finally, we provide positive results for line graphs, showing that we can find an optimal set of schedules for $k$ robots completing $m$ tasks of equal length of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the length of the tasks is unbounded."],"url":"http://arxiv.org/abs/2402.12019v1","category":"cs.DS"}
{"created":"2024-02-19 10:20:21","title":"Private Interdependent Valuations: New Bounds for Single-Item Auctions and Matroids","abstract":"We study auction design within the widely acclaimed model of interdependent values, introduced by Milgrom and Weber [1982]. In this model, every bidder $i$ has a private signal $s_i$ for the item for sale, and a public valuation function $v_i(s_1,\\ldots,s_n)$ which maps every vector of private signals (of all bidders) into a real value. A recent line of work established the existence of approximately-optimal mechanisms within this framework, even in the more challenging scenario where each bidder's valuation function $v_i$ is also private. This body of work has primarily focused on single-item auctions with two natural classes of valuations: those exhibiting submodularity over signals (SOS) and $d$-critical valuations.   In this work we advance the state of the art on interdependent values with private valuation functions, with respect to both SOS and $d$-critical valuations. For SOS valuations, we devise a new mechanism that gives an improved approximation bound of $5$ for single-item auctions. This mechanism employs a novel variant of an \"eating mechanism\", leveraging LP-duality to achieve feasibility with reduced welfare loss. For $d$-critical valuations, we broaden the scope of existing results beyond single-item auctions, introducing a mechanism that gives a $(d+1)$-approximation for any environment with matroid feasibility constraints on the set of agents that can be simultaneously served. Notably, this approximation bound is tight, even with respect to single-item auctions.","sentences":["We study auction design within the widely acclaimed model of interdependent values, introduced by Milgrom and Weber [1982].","In this model, every bidder $i$ has a private signal $s_i$ for the item for sale, and a public valuation function $v_i(s_1,\\ldots,s_n)$ which maps every vector of private signals (of all bidders) into a real value.","A recent line of work established the existence of approximately-optimal mechanisms within this framework, even in the more challenging scenario where each bidder's valuation function $v_i$ is also private.","This body of work has primarily focused on single-item auctions with two natural classes of valuations: those exhibiting submodularity over signals (SOS) and $d$-critical valuations.   ","In this work we advance the state of the art on interdependent values with private valuation functions, with respect to both SOS and $d$-critical valuations.","For SOS valuations, we devise a new mechanism that gives an improved approximation bound of $5$ for single-item auctions.","This mechanism employs a novel variant of an \"eating mechanism\", leveraging LP-duality to achieve feasibility with reduced welfare loss.","For $d$-critical valuations, we broaden the scope of existing results beyond single-item auctions, introducing a mechanism that gives a $(d+1)$-approximation for any environment with matroid feasibility constraints on the set of agents that can be simultaneously served.","Notably, this approximation bound is tight, even with respect to single-item auctions."],"url":"http://arxiv.org/abs/2402.12017v1","category":"cs.GT"}
{"created":"2024-02-19 09:16:16","title":"Construction of optimized tight-binding models using \\textit{ab initio} Hamiltonian: Application to monolayer $2H$-transition metal dichalcogenides","abstract":"We present optimized tight-binding models with atomic orbitals to improve \\textit{ab initio} tight-binding models constructed by truncating full density functional theory (DFT) Hamiltonian based on localized orbitals. Retaining qualitative features of the original Hamiltonian, the optimization reduces quantitative deviations in overall band structures between the \\textit{ab initio} tight-binding model and the full DFT Hamiltonian. The optimization procedure and related details are demonstrated by using semiconducting and metallic Janus transition metal dichalcogenides monolayers in the $2H$ configuration. Varying the truncation range from partial second neighbors to third ones, we show differences in electronic structures between the truncated tight-binding model and the original full Hamiltonian, and how much the optimization can remedy the quantitative loss induced by truncation. We further elaborate the optimization process so that local electronic properties such as valence and conduction band edges and Fermi surfaces are precisely reproduced by the optimized tight-binding model. We also extend our discussions to tight-binding models including spin-orbit interactions, so we provide the optimized tight-binding model replicating spin-related properties of the original Hamiltonian such as spin textures. The optimization process described here can be readily applied to construct the fine-tuned tight-binding model based on various DFT calculations.","sentences":["We present optimized tight-binding models with atomic orbitals to improve \\textit{ab initio} tight-binding models constructed by truncating full density functional theory (DFT) Hamiltonian based on localized orbitals.","Retaining qualitative features of the original Hamiltonian, the optimization reduces quantitative deviations in overall band structures between the \\textit{ab initio} tight-binding model and the full DFT Hamiltonian.","The optimization procedure and related details are demonstrated by using semiconducting and metallic Janus transition metal dichalcogenides monolayers in the $2H$ configuration.","Varying the truncation range from partial second neighbors to third ones, we show differences in electronic structures between the truncated tight-binding model and the original full Hamiltonian, and how much the optimization can remedy the quantitative loss induced by truncation.","We further elaborate the optimization process so that local electronic properties such as valence and conduction band edges and Fermi surfaces are precisely reproduced by the optimized tight-binding model.","We also extend our discussions to tight-binding models including spin-orbit interactions, so we provide the optimized tight-binding model replicating spin-related properties of the original Hamiltonian such as spin textures.","The optimization process described here can be readily applied to construct the fine-tuned tight-binding model based on various DFT calculations."],"url":"http://arxiv.org/abs/2402.11969v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 06:41:55","title":"Ironing allocations","abstract":"I propose a new approach to solving standard screening problems when the monotonicity constraint binds. A simple geometric argument shows that when virtual values are quasi-concave, the optimal allocation can be found by appropriately truncating the solution to the relaxed problem. I provide a simple algorithm for finding this optimal truncation when virtual values are concave.","sentences":["I propose a new approach to solving standard screening problems when the monotonicity constraint binds.","A simple geometric argument shows that when virtual values are quasi-concave, the optimal allocation can be found by appropriately truncating the solution to the relaxed problem.","I provide a simple algorithm for finding this optimal truncation when virtual values are concave."],"url":"http://arxiv.org/abs/2402.11881v1","category":"econ.TH"}
{"created":"2024-02-19 06:12:07","title":"The Price of Information","abstract":"When an investor is faced with the option to purchase additional information regarding an asset price, how much should she pay? To address this question, we solve for the indifference price of information in a setting where a trader maximizes her expected utility of terminal wealth over a finite time horizon. If she does not purchase the information, then she solves a partial information stochastic control problem, while, if she does purchase the information, then she pays a cost and receives partial information about the asset's trajectory. We further demonstrate that when the investor can purchase the information at any stopping time prior to the end of the trading horizon, she chooses to do so at a deterministic time.","sentences":["When an investor is faced with the option to purchase additional information regarding an asset price, how much should she pay?","To address this question, we solve for the indifference price of information in a setting where a trader maximizes her expected utility of terminal wealth over a finite time horizon.","If she does not purchase the information, then she solves a partial information stochastic control problem, while, if she does purchase the information, then she pays a cost and receives partial information about the asset's trajectory.","We further demonstrate that when the investor can purchase the information at any stopping time prior to the end of the trading horizon, she chooses to do so at a deterministic time."],"url":"http://arxiv.org/abs/2402.11864v1","category":"q-fin.MF"}
{"created":"2024-02-19 06:02:42","title":"Multistep severe plastic deformation to achieve non-rare earth bulk magnets with high \u03b1-MnBi phase content","abstract":"The ferromagnetic {\\alpha}-MnBi phase as non-rare earth magnetic material has gained increasing interest, but fabrication of large volumes containing a significant amount of {\\alpha}-MnBi is still challenging. Targeting successful processing strategies, we apply multistep severe plastic deformation with intermediate magnetic field assisted annealing. The subsequent severe plastic deformation renewed the high amount of material defects and lead to microstructural refinement of the previously formed {\\alpha}-MnBi phase. Thus, {\\alpha}-MnBi phase content is enhanced during final annealing. Secondly, the magnetic coercivity increases. These results suggest that further optimization will pave the way towards non-rare-earth bulk magnetic materials with enhanced {\\alpha}-MnBi phase content.","sentences":["The ferromagnetic {\\alpha}-MnBi phase as non-rare earth magnetic material has gained increasing interest, but fabrication of large volumes containing a significant amount of {\\alpha}-MnBi is still challenging.","Targeting successful processing strategies, we apply multistep severe plastic deformation with intermediate magnetic field assisted annealing.","The subsequent severe plastic deformation renewed the high amount of material defects and lead to microstructural refinement of the previously formed {\\alpha}-MnBi phase.","Thus, {\\alpha}-MnBi phase content is enhanced during final annealing.","Secondly, the magnetic coercivity increases.","These results suggest that further optimization will pave the way towards non-rare-earth bulk magnetic materials with enhanced {\\alpha}-MnBi phase content."],"url":"http://arxiv.org/abs/2402.11859v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 03:26:32","title":"Building a Hierarchical Architecture and Communication Model for the Quantum Internet","abstract":"The research of architecture has tremendous significance in realizing quantum Internet. Although there is not yet a standard quantum Internet architecture, the distributed architecture is one of the possible solutions, which utilizes quantum repeaters or dedicated entanglement sources in a flat structure for entanglement preparation & distribution. In this paper, we analyze the distributed architecture in detail and demonstrate that it has three limitations: 1) possible high maintenance overhead, 2) possible low-performance entanglement distribution, and 3) unable to support optimal entanglement routing. We design a hierarchical quantum Internet architecture and a communication model to solve the problems above. We also present a W-state Based Centralized Entanglement Preparation & Distribution (W-state Based CEPD) scheme and a Centralized Entanglement Routing (CER) algorithm within our hierarchical architecture and perform an experimental comparison with other entanglement preparation & distribution schemes and entanglement routing algorithms within the distributed architecture. The evaluation results show that the entanglement distribution efficiency of hierarchical architecture is 11.5% higher than that of distributed architecture on average (minimum 3.3%, maximum 37.3%), and the entanglement routing performance of hierarchical architecture is much better than that of a distributed architecture according to the fidelity and throughput.","sentences":["The research of architecture has tremendous significance in realizing quantum Internet.","Although there is not yet a standard quantum Internet architecture, the distributed architecture is one of the possible solutions, which utilizes quantum repeaters or dedicated entanglement sources in a flat structure for entanglement preparation & distribution.","In this paper, we analyze the distributed architecture in detail and demonstrate that it has three limitations: 1) possible high maintenance overhead, 2) possible low-performance entanglement distribution, and 3) unable to support optimal entanglement routing.","We design a hierarchical quantum Internet architecture and a communication model to solve the problems above.","We also present a W-state Based Centralized Entanglement Preparation & Distribution (W-state Based CEPD) scheme and a Centralized Entanglement Routing (CER) algorithm within our hierarchical architecture and perform an experimental comparison with other entanglement preparation & distribution schemes and entanglement routing algorithms within the distributed architecture.","The evaluation results show that the entanglement distribution efficiency of hierarchical architecture is 11.5% higher than that of distributed architecture on average (minimum 3.3%, maximum 37.3%), and the entanglement routing performance of hierarchical architecture is much better than that of a distributed architecture according to the fidelity and throughput."],"url":"http://arxiv.org/abs/2402.11806v1","category":"quant-ph"}
{"created":"2024-02-19 02:57:17","title":"Affine FR : an Effective Facial Reduction Algorithm for Semidefinite Relaxations of Combinatorial Problems","abstract":"We develop a new method called affine FR for recovering Slater's condition for semidefinite programming (SDP) relaxations of combinatorial optimization (CO) problems. Affine FR is a user-friendly method, as it is fully automatic and only requires a description of the problem. We provide a rigorous analysis of differences between affine FR and the existing methods. We also present numerical results to demonstrate the effectiveness of affine FR in reducing the size of SDP relaxations for CO problems.","sentences":["We develop a new method called affine FR for recovering Slater's condition for semidefinite programming (SDP) relaxations of combinatorial optimization (CO) problems.","Affine FR is a user-friendly method, as it is fully automatic and only requires a description of the problem.","We provide a rigorous analysis of differences between affine FR and the existing methods.","We also present numerical results to demonstrate the effectiveness of affine FR in reducing the size of SDP relaxations for CO problems."],"url":"http://arxiv.org/abs/2402.11796v1","category":"math.OC"}
{"created":"2024-02-19 01:48:11","title":"Connection-Aware P2P Trading: Simultaneous Trading and Peer Selection","abstract":"Peer-to-peer (P2P) trading is seen as a viable solution to handle the growing number of distributed energy resources in distribution networks. However, when dealing with large-scale consumers, there are several challenges that must be addressed. One of these challenges is limited communication capabilities. Additionally, prosumers may have specific preferences when it comes to trading. Both can result in serious asynchrony in peer-to-peer trading, potentially impacting the effectiveness of negotiations and hindering convergence before the market closes. This paper introduces a connection-aware P2P trading algorithm designed for extensive prosumer trading. The algorithm facilitates asynchronous trading while respecting prosumer's autonomy in trading peer selection, an often overlooked aspect in traditional models. In addition, to optimize the use of limited connection opportunities, a smart trading peer connection selection strategy is developed to guide consumers to communicate strategically to accelerate convergence. A theoretical convergence guarantee is provided for the connection-aware P2P trading algorithm, which further details how smart selection strategies enhance convergence efficiency. Numerical studies are carried out to validate the effectiveness of the connection-aware algorithm and the performance of smart selection strategies in reducing the overall convergence time.","sentences":["Peer-to-peer (P2P) trading is seen as a viable solution to handle the growing number of distributed energy resources in distribution networks.","However, when dealing with large-scale consumers, there are several challenges that must be addressed.","One of these challenges is limited communication capabilities.","Additionally, prosumers may have specific preferences when it comes to trading.","Both can result in serious asynchrony in peer-to-peer trading, potentially impacting the effectiveness of negotiations and hindering convergence before the market closes.","This paper introduces a connection-aware P2P trading algorithm designed for extensive prosumer trading.","The algorithm facilitates asynchronous trading while respecting prosumer's autonomy in trading peer selection, an often overlooked aspect in traditional models.","In addition, to optimize the use of limited connection opportunities, a smart trading peer connection selection strategy is developed to guide consumers to communicate strategically to accelerate convergence.","A theoretical convergence guarantee is provided for the connection-aware P2P trading algorithm, which further details how smart selection strategies enhance convergence efficiency.","Numerical studies are carried out to validate the effectiveness of the connection-aware algorithm and the performance of smart selection strategies in reducing the overall convergence time."],"url":"http://arxiv.org/abs/2402.11769v1","category":"eess.SY"}
{"created":"2024-02-19 01:44:54","title":"Well-Connected Set and Its Application to Multi-Robot Path Planning","abstract":"Parking lots and autonomous warehouses for accommodating many vehicles/robots adopt designs in which the underlying graphs are \\emph{well-connected} to simplify planning and reduce congestion. In this study, we formulate and delve into the \\emph{largest well-connected set} (LWCS) problem and explore its applications in layout design for multi-robot path planning. Roughly speaking, a well-connected set over a connected graph is a set of vertices such that there is a path on the graph connecting any pair of vertices in the set without passing through any additional vertices of the set. Identifying an LWCS has many potential high-utility applications, e.g., for determining parking garage layout and capacity, as prioritized planning can be shown to be complete when start/goal configurations belong to an LWCS. In this work, we establish that computing an LWCS is NP-complete. We further develop optimal and near-optimal LWCS algorithms, with the near-optimal algorithm targeting large maps. A complete prioritized planning method is given for planning paths for multiple robots residing on an LWCS.","sentences":["Parking lots and autonomous warehouses for accommodating many vehicles/robots adopt designs in which the underlying graphs are \\emph{well-connected} to simplify planning and reduce congestion.","In this study, we formulate and delve into the \\emph{largest well-connected set} (LWCS) problem and explore its applications in layout design for multi-robot path planning.","Roughly speaking, a well-connected set over a connected graph is a set of vertices such that there is a path on the graph connecting any pair of vertices in the set without passing through any additional vertices of the set.","Identifying an LWCS has many potential high-utility applications, e.g., for determining parking garage layout and capacity, as prioritized planning can be shown to be complete when start/goal configurations belong to an LWCS.","In this work, we establish that computing an LWCS is NP-complete.","We further develop optimal and near-optimal LWCS algorithms, with the near-optimal algorithm targeting large maps.","A complete prioritized planning method is given for planning paths for multiple robots residing on an LWCS."],"url":"http://arxiv.org/abs/2402.11766v1","category":"cs.RO"}
{"created":"2024-02-19 01:12:21","title":"Conformally rigid graphs","abstract":"Given a finite, simple, connected graph $G=(V,E)$ with $|V|=n$, we consider the associated graph Laplacian matrix $L = D - A$ with eigenvalues $0 = \\lambda_1 < \\lambda_2 \\leq \\dots \\leq \\lambda_n$. One can also consider the same graph equipped with positive edge weights $w:E \\rightarrow \\mathbb{R}_{> 0}$ normalized to $\\sum_{e \\in E} w_e = |E|$ and the associated weighted Laplacian matrix $L_w$. We say that $G$ is conformally rigid if constant edge-weights maximize the second eigenvalue $\\lambda_2(w)$ of $L_w$ over all $w$, and minimize $\\lambda_n(w')$ of $L_{w'}$ over all $w'$, i.e., for all $w,w'$, $$ \\lambda_2(w) \\leq \\lambda_2(1) \\leq \\lambda_n(1) \\leq \\lambda_n(w').$$ Conformal rigidity requires an extraordinary amount of symmetry in $G$. Every edge-transitive graph is conformally rigid. We prove that every distance-regular graph, and hence every strongly-regular graph, is conformally rigid. Certain special graph embeddings can be used to characterize conformal rigidity. Cayley graphs can be conformally rigid but need not be, we prove a sufficient criterion. We also find a small set of conformally rigid graphs that do not belong into any of the above categories; these include the Hoffman graph, the crossing number graph 6B and others. Conformal rigidity can be certified via semidefinite programming, we provide explicit examples.","sentences":["Given a finite, simple, connected graph $G=(V,E)$ with $|V|=n$, we consider the associated graph Laplacian matrix $L = D - A$ with eigenvalues $0 = \\lambda_1 < \\lambda_2 \\leq \\dots \\leq \\lambda_n$. One can also consider the same graph equipped with positive edge weights $w:E \\rightarrow \\mathbb{R}_{> 0}$ normalized to $\\sum_{e \\in E} w_e = |E|$ and the associated weighted Laplacian matrix $L_w$. We say that $G$ is conformally rigid if constant edge-weights maximize the second eigenvalue $\\lambda_2(w)$ of $L_w$ over all $w$, and minimize $\\lambda_n(w')$ of $L_{w'}$ over all $w'$, i.e., for all $w,w'$, $$ \\lambda_2(w)","\\leq \\lambda_2(1)","\\leq \\lambda_n(1) \\leq \\lambda_n(w').$$ Conformal rigidity requires an extraordinary amount of symmetry in $G$. Every edge-transitive graph is conformally rigid.","We prove that every distance-regular graph, and hence every strongly-regular graph, is conformally rigid.","Certain special graph embeddings can be used to characterize conformal rigidity.","Cayley graphs can be conformally rigid but need not be, we prove a sufficient criterion.","We also find a small set of conformally rigid graphs that do not belong into any of the above categories; these include the Hoffman graph, the crossing number graph 6B and others.","Conformal rigidity can be certified via semidefinite programming, we provide explicit examples."],"url":"http://arxiv.org/abs/2402.11758v1","category":"math.CO"}
{"created":"2024-02-18 23:57:03","title":"To Store or Not to Store: a graph theoretical approach for Dataset Versioning","abstract":"In this work, we study the cost efficient data versioning problem, where the goal is to optimize the storage and reconstruction (retrieval) costs of data versions, given a graph of datasets as nodes and edges capturing edit/delta information. One central variant we study is MinSum Retrieval (MSR) where the goal is to minimize the total retrieval costs, while keeping the storage costs bounded. This problem (along with its variants) was introduced by Bhattacherjee et al. [VLDB'15]. While such problems are frequently encountered in collaborative tools (e.g., version control systems and data analysis pipelines), to the best of our knowledge, no existing research studies the theoretical aspects of these problems.   We establish that the currently best-known heuristic, LMG, can perform arbitrarily badly in a simple worst case. Moreover, we show that it is hard to get $o(n)$-approximation for MSR on general graphs even if we relax the storage constraints by an $O(\\log n)$ factor. Similar hardness results are shown for other variants. Meanwhile, we propose poly-time approximation schemes for tree-like graphs, motivated by the fact that the graphs arising in practice from typical edit operations are often not arbitrary. As version graphs typically have low treewidth, we further develop new algorithms for bounded treewidth graphs.   Furthermore, we propose two new heuristics and evaluate them empirically. First, we extend LMG by considering more potential ``moves'', to propose a new heuristic LMG-All. LMG-All consistently outperforms LMG while having comparable run time on a wide variety of datasets, i.e., version graphs. Secondly, we apply our tree algorithms on the minimum-storage arborescence of an instance, yielding algorithms that are qualitatively better than all previous heuristics for MSR, as well as for another variant BoundedMin Retrieval (BMR).","sentences":["In this work, we study the cost efficient data versioning problem, where the goal is to optimize the storage and reconstruction (retrieval) costs of data versions, given a graph of datasets as nodes and edges capturing edit/delta information.","One central variant we study is MinSum Retrieval (MSR) where the goal is to minimize the total retrieval costs, while keeping the storage costs bounded.","This problem (along with its variants) was introduced by Bhattacherjee et al.","[VLDB'15].","While such problems are frequently encountered in collaborative tools (e.g., version control systems and data analysis pipelines), to the best of our knowledge, no existing research studies the theoretical aspects of these problems.   ","We establish that the currently best-known heuristic, LMG, can perform arbitrarily badly in a simple worst case.","Moreover, we show that it is hard to get $o(n)$-approximation for MSR on general graphs even if we relax the storage constraints by an $O(\\log n)$ factor.","Similar hardness results are shown for other variants.","Meanwhile, we propose poly-time approximation schemes for tree-like graphs, motivated by the fact that the graphs arising in practice from typical edit operations are often not arbitrary.","As version graphs typically have low treewidth, we further develop new algorithms for bounded treewidth graphs.   ","Furthermore, we propose two new heuristics and evaluate them empirically.","First, we extend LMG by considering more potential ``moves'', to propose a new heuristic LMG-All.","LMG-All consistently outperforms LMG while having comparable run time on a wide variety of datasets, i.e., version graphs.","Secondly, we apply our tree algorithms on the minimum-storage arborescence of an instance, yielding algorithms that are qualitatively better than all previous heuristics for MSR, as well as for another variant BoundedMin Retrieval (BMR)."],"url":"http://arxiv.org/abs/2402.11741v1","category":"cs.DS"}
{"created":"2024-02-18 21:58:40","title":"Characterization of NbTiN films with thicknesses below 20 nm for low power kinetic inductance amplifiers","abstract":"A quantum-limited amplification chain is a fundamental advantage for any application that may benefit from the detection of very faint signals. Reading out arrays of superconducting detectors (TESs or MKIDs), resonant cavities, or qubits, calls for large bandwidth amplifiers in addition to having the lowest possible noise. At millikelvin temperatures, Kinetic Inductance Traveling-Wave Parametric Amplifiers (KI-TWPAs) working in 3-wave-mixing (3WM) and fabricated from a 20 nm thick NbTiN film have shown promising noise performances, as they can operate close to the quantum limit. However, they still require fairly high pump power. Devices that would require lower pump power would be easier to implement in readout chains, could reach the quantum limit and they would be compatible with qubit readout. A possible solution for obtaining this optimal configuration is to use a thinner superconducting film. In this work we explore the properties of NbTiN films with a thickness less than 20 nm and we report the obtained experimental characterizations in terms of critical temperature, normal resistivity, and kinetic inductance. A new design for a 3WM KI-TWPA amplifier, based on these developed superconducting films, is introduced and discussed.","sentences":["A quantum-limited amplification chain is a fundamental advantage for any application that may benefit from the detection of very faint signals.","Reading out arrays of superconducting detectors (TESs or MKIDs), resonant cavities, or qubits, calls for large bandwidth amplifiers in addition to having the lowest possible noise.","At millikelvin temperatures, Kinetic Inductance Traveling-Wave Parametric Amplifiers (KI-TWPAs) working in 3-wave-mixing (3WM) and fabricated from a 20 nm thick NbTiN film have shown promising noise performances, as they can operate close to the quantum limit.","However, they still require fairly high pump power.","Devices that would require lower pump power would be easier to implement in readout chains, could reach the quantum limit and they would be compatible with qubit readout.","A possible solution for obtaining this optimal configuration is to use a thinner superconducting film.","In this work we explore the properties of NbTiN films with a thickness less than 20 nm","and we report the obtained experimental characterizations in terms of critical temperature, normal resistivity, and kinetic inductance.","A new design for a 3WM KI-TWPA amplifier, based on these developed superconducting films, is introduced and discussed."],"url":"http://arxiv.org/abs/2402.11720v1","category":"cond-mat.supr-con"}
{"created":"2024-02-18 19:52:23","title":"Influence of mechanical compliance of the substrate on the morphology of nanoporous gold thin films","abstract":"Nanoporous gold (np-Au) has found use in applications ranging from catalysis to biosensing where pore morphology plays a critical role in performance. While morphology evolution of bulk np-Au has been widely studied, knowledge about its thin film form is limited. This work hypothesizes that mechanical compliance of the thin film substrate can play a critical role in the morphology evolution. Via experimental and finite-element-analysis approaches, we investigate the morphological variation in np-Au thin films deposited on compliant silicone (PDMS) substrates of a range of thicknesses anchored on rigid glass supports and compare those to the morphology of np-Au deposited on glass. More macroscopic (10s to 100s of microns) cracks and discrete islands form in the np-Au films on PDMS compared to glass. Conversely, uniformly-distributed microscopic (100s of nanometers) cracks form in greater numbers in the np-Au films on glass than on PDMS, with the cracks located within the discrete islands. The np-Au films on glass also show larger ligament and pore sizes possibly due to higher residual stresses compared to the np-Au/PDMS films. The effective elastic modulus of the substrate layers decreases with increasing PDMS thickness, resulting in secondary np-Au morphology effects including a reduction in macroscopic crack-to-crack distance, an increase in microscopic crack coverage, and a widening of the microscopic cracks. However, changes in the ligament/pore widths with PDMS thickness are negligible, allowing for independent optimization for cracking. We expect these results to inform the integration of functional np-Au films on compliant substrates into emerging applications, including flexible electronics.","sentences":["Nanoporous gold (np-Au) has found use in applications ranging from catalysis to biosensing where pore morphology plays a critical role in performance.","While morphology evolution of bulk np-Au has been widely studied, knowledge about its thin film form is limited.","This work hypothesizes that mechanical compliance of the thin film substrate can play a critical role in the morphology evolution.","Via experimental and finite-element-analysis approaches, we investigate the morphological variation in np-Au thin films deposited on compliant silicone (PDMS) substrates of a range of thicknesses anchored on rigid glass supports and compare those to the morphology of np-Au deposited on glass.","More macroscopic (10s to 100s of microns) cracks and discrete islands form in the np-Au films on PDMS compared to glass.","Conversely, uniformly-distributed microscopic (100s of nanometers) cracks form in greater numbers in the np-Au films on glass than on PDMS, with the cracks located within the discrete islands.","The np-Au films on glass also show larger ligament and pore sizes possibly due to higher residual stresses compared to the np-Au/PDMS films.","The effective elastic modulus of the substrate layers decreases with increasing PDMS thickness, resulting in secondary np-Au morphology effects including a reduction in macroscopic crack-to-crack distance, an increase in microscopic crack coverage, and a widening of the microscopic cracks.","However, changes in the ligament/pore widths with PDMS thickness are negligible, allowing for independent optimization for cracking.","We expect these results to inform the integration of functional np-Au films on compliant substrates into emerging applications, including flexible electronics."],"url":"http://arxiv.org/abs/2402.11694v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-18 17:02:39","title":"Theoretical foundations for programmatic reinforcement learning","abstract":"The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops. Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies? How large are optimal programmatic policies? How can we learn them? The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL.","sentences":["The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments.","Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops.","Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies?","How large are optimal programmatic policies?","How can we learn them?","The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL."],"url":"http://arxiv.org/abs/2402.11650v1","category":"cs.LG"}
{"created":"2024-02-18 15:31:09","title":"Filter-free high-performance single photon emission from a quantum dot in a Fabry-Perot microcavity","abstract":"Combining resonant excitation with Purcell-enhanced single quantum dots (QDs) stands out as a prominent strategy for realizing high performance solid-state single photon sources. However, optimizing photon efficiency requires addressing challenges associated with effectively separating the excitation laser from QDs' emission. Traditionally, this involves polarization filtering, which limits the achievable polarization directions and the scalability of photonic states. In this study, we have successfully tackled this challenge by employing spatially-orthogonal resonant excitation of QDs, deterministically coupled to monolithic Fabry-Perot microcavities. Leveraging the membrane cavity structures, we have achieved filter-free single photon resonant fluorescence. The resulting source produces single photons with a simultaneous high extraction efficiency of 0.87, purity of 0.9045(4), and indistinguishability of 0.963(4).","sentences":["Combining resonant excitation with Purcell-enhanced single quantum dots (QDs) stands out as a prominent strategy for realizing high performance solid-state single photon sources.","However, optimizing photon efficiency requires addressing challenges associated with effectively separating the excitation laser from QDs' emission.","Traditionally, this involves polarization filtering, which limits the achievable polarization directions and the scalability of photonic states.","In this study, we have successfully tackled this challenge by employing spatially-orthogonal resonant excitation of QDs, deterministically coupled to monolithic Fabry-Perot microcavities.","Leveraging the membrane cavity structures, we have achieved filter-free single photon resonant fluorescence.","The resulting source produces single photons with a simultaneous high extraction efficiency of 0.87, purity of 0.9045(4), and indistinguishability of 0.963(4)."],"url":"http://arxiv.org/abs/2402.11623v1","category":"quant-ph"}
{"created":"2024-02-18 14:08:48","title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark","abstract":"In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .","sentences":["In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard.","Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge.","Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount.","This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO.","Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes.","Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance.","We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity.","Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning.","Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM ."],"url":"http://arxiv.org/abs/2402.11592v1","category":"cs.LG"}
{"created":"2024-02-18 14:04:39","title":"Optimal impulsive control for time delay systems","abstract":"We introduce discontinuous solutions to nonlinear impulsive control systems with state time delays in the dynamics and derive necessary optimality conditions in the form of a Maximum Principle for associated optimal control problems. In the case without delays, if the measure control is scalar valued, the corresponding discontinuous state trajectory, understood as a limit of classical state trajectories for absolutely continuous controls approximating the measure, is unique. For vector valued measure controls however, the limiting trajectory is not unique and a full description of the control must include additional `attached' controls affecting instantaneous state evolution at a discontinuity. For impulsive control systems with time delays we reveal a new phenomenon, namely that the limiting state trajectory resulting from different approximations of a given measure control needs not to be unique, even in the scalar case. Correspondingly, our framework allows for additional attached controls, even though the measure control is scalar valued.","sentences":["We introduce discontinuous solutions to nonlinear impulsive control systems with state time delays in the dynamics and derive necessary optimality conditions in the form of a Maximum Principle for associated optimal control problems.","In the case without delays, if the measure control is scalar valued, the corresponding discontinuous state trajectory, understood as a limit of classical state trajectories for absolutely continuous controls approximating the measure, is unique.","For vector valued measure controls however, the limiting trajectory is not unique and a full description of the control must include additional `attached' controls affecting instantaneous state evolution at a discontinuity.","For impulsive control systems with time delays we reveal a new phenomenon, namely that the limiting state trajectory resulting from different approximations of a given measure control needs not to be unique, even in the scalar case.","Correspondingly, our framework allows for additional attached controls, even though the measure control is scalar valued."],"url":"http://arxiv.org/abs/2402.11591v1","category":"math.OC"}
{"created":"2024-02-18 12:50:19","title":"Extensible Embedding: A Flexible Multipler For LLM's Context Length","abstract":"Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.","sentences":["Large language models (LLMs) call for extension of context to handle many critical applications.","However, the existing approaches are prone to expensive costs and inferior quality of context extension.","In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness.","Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token.","By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window.","Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages.","1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths.","2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way.","3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component.","Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context."],"url":"http://arxiv.org/abs/2402.11577v1","category":"cs.CL"}
{"created":"2024-02-18 12:41:01","title":"BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models","abstract":"Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.","sentences":["Large language models (LLMs) call for extension of context to handle many critical applications.","However, the existing approaches are prone to expensive costs and inferior quality of context extension.","In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness.","Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token.","By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window.","Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages.","1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths.","2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way.","3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component.","Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context."],"url":"http://arxiv.org/abs/2402.11573v1","category":"cs.CL"}
{"created":"2024-02-18 12:27:59","title":"Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training","abstract":"The 2D human pose estimation is a basic visual problem. However, supervised learning of a model requires massive labeled images, which is expensive and labor-intensive. In this paper, we aim at boosting the accuracy of a pose estimator by excavating extra unlabeled images in a semi-supervised learning (SSL) way. Most previous consistency-based SSL methods strive to constraint the model to predict consistent results for differently augmented images. Following this consensus, we revisit two core aspects including advanced data augmentation methods and concise consistency training frameworks. Specifically, we heuristically dig various collaborative combinations of existing data augmentations, and discover novel superior data augmentation schemes to more effectively add noise on unlabeled samples. They can compose easy-hard augmentation pairs with larger transformation difficulty gaps, which play a crucial role in consistency-based SSL. Moreover, we propose to strongly augment unlabeled images repeatedly with diverse augmentations, generate multi-path predictions sequentially, and optimize corresponding unsupervised consistency losses using one single network. This simple and compact design is on a par with previous methods consisting of dual or triple networks. Furthermore, it can also be integrated with multiple networks to produce better performance. Comparing to state-of-the-art SSL approaches, our method brings substantial improvements on public datasets. Code is released for academic use in \\url{https://github.com/hnuzhy/MultiAugs}.","sentences":["The 2D human pose estimation is a basic visual problem.","However, supervised learning of a model requires massive labeled images, which is expensive and labor-intensive.","In this paper, we aim at boosting the accuracy of a pose estimator by excavating extra unlabeled images in a semi-supervised learning (SSL) way.","Most previous consistency-based SSL methods strive to constraint the model to predict consistent results for differently augmented images.","Following this consensus, we revisit two core aspects including advanced data augmentation methods and concise consistency training frameworks.","Specifically, we heuristically dig various collaborative combinations of existing data augmentations, and discover novel superior data augmentation schemes to more effectively add noise on unlabeled samples.","They can compose easy-hard augmentation pairs with larger transformation difficulty gaps, which play a crucial role in consistency-based SSL.","Moreover, we propose to strongly augment unlabeled images repeatedly with diverse augmentations, generate multi-path predictions sequentially, and optimize corresponding unsupervised consistency losses using one single network.","This simple and compact design is on a par with previous methods consisting of dual or triple networks.","Furthermore, it can also be integrated with multiple networks to produce better performance.","Comparing to state-of-the-art SSL approaches, our method brings substantial improvements on public datasets.","Code is released for academic use in \\url{https://github.com/hnuzhy/MultiAugs}."],"url":"http://arxiv.org/abs/2402.11566v1","category":"cs.CV"}
{"created":"2024-02-18 10:16:45","title":"Bayesian uncertainty quantification on nuclear level density data and their impact on $(p,\u03b3)$ reactions of astrophysical interest","abstract":"The $p$ process nucleosynthesis is responsible for the synthesis of 35 neutron-deficient nuclei from $^{35}$Se to $^{196}$Hg. An important input that can affect the modeling of this process is the nuclear level density at the relevant excitation energies of the nuclei involved in the reaction network. The OSLO method has been extensively used for the measurement of level densities in excitation energies of several MeV. In this work, Bayesian optimization has been used in order to estimate the 95% high density intervals for the parameters of two level density models optimized on the OSLO data. These uncertainties are then propagated on the cross sections of $(p,\\gamma)$ reactions leading to the compound nuclei $^{105,106}$Pd and $^{105,106}$Cd inside the astrophysically relevant energy range. Imposing constraints in this region of the isotopic chart is important for network calculations involving the nearby $p$ nuclei $^{102}$Pd and $^{106}$Cd. We discuss the reduction of the range of cross sections due to the uncertainties arising from the level density data compared to the range of the six default level density models available in TALYS and we highlight the need for level density data inside the astrophysically relevant energy ranges.","sentences":["The $p$ process nucleosynthesis is responsible for the synthesis of 35 neutron-deficient nuclei from $^{35}$Se to $^{196}$Hg.","An important input that can affect the modeling of this process is the nuclear level density at the relevant excitation energies of the nuclei involved in the reaction network.","The OSLO method has been extensively used for the measurement of level densities in excitation energies of several MeV.","In this work, Bayesian optimization has been used in order to estimate the 95% high density intervals for the parameters of two level density models optimized on the OSLO data.","These uncertainties are then propagated on the cross sections of $(p,\\gamma)$ reactions leading to the compound nuclei $^{105,106}$Pd and $^{105,106}$Cd inside the astrophysically relevant energy range.","Imposing constraints in this region of the isotopic chart is important for network calculations involving the nearby $p$ nuclei $^{102}$Pd and $^{106}$Cd.","We discuss the reduction of the range of cross sections due to the uncertainties arising from the level density data compared to the range of the six default level density models available in TALYS and we highlight the need for level density data inside the astrophysically relevant energy ranges."],"url":"http://arxiv.org/abs/2402.11535v1","category":"nucl-th"}
{"created":"2024-02-18 10:09:21","title":"Computational complexity of the Weisfeiler-Leman dimension","abstract":"The Weisfeiler-Leman dimension of a graph $G$ is the least number $k$ such that the $k$-dimensional Weisfeiler-Leman algorithm distinguishes $G$ from every other non-isomorphic graph. The dimension is a standard measure of the descriptive complexity of a graph and recently finds various applications in particular in the context of machine learning. In this paper, we study the computational complexity of computing the Weisfeiler-Leman dimension. We observe that in general the problem of deciding whether the Weisfeiler-Leman dimension of $G$ is at most $k$ is NP-hard. This is also true for the more restricted problem with graphs of color multiplicity at most 4. Therefore, we study parameterized and approximate versions of the problem. We give, for each fixed $k\\geq 2$, a polynomial-time algorithm that decides whether the Weisfeiler-Leman dimension of a given graph of color multiplicity at most $5$ is at most $k$. Moreover, we show that for these color multiplicities this is optimal in the sense that this problem is P-hard under logspace-uniform $\\text{AC}_0$-reductions. Furthermore, for each larger bound $c$ on the color multiplicity and each fixed $k \\geq 2$, we provide a polynomial-time approximation algorithm for the abelian case: given a relational structure with abelian color classes of size at most $c$, the algorithm outputs either that its Weisfeiler-Leman dimension is at most $(k+1)c$ or that it is larger than $k$.","sentences":["The Weisfeiler-Leman dimension of a graph $G$ is the least number $k$ such that the $k$-dimensional Weisfeiler-Leman algorithm distinguishes $G$ from every other non-isomorphic graph.","The dimension is a standard measure of the descriptive complexity of a graph and recently finds various applications in particular in the context of machine learning.","In this paper, we study the computational complexity of computing the Weisfeiler-Leman dimension.","We observe that in general the problem of deciding whether the Weisfeiler-Leman dimension of $G$ is at most $k$ is NP-hard.","This is also true for the more restricted problem with graphs of color multiplicity at most 4.","Therefore, we study parameterized and approximate versions of the problem.","We give, for each fixed $k\\geq 2$, a polynomial-time algorithm that decides whether the Weisfeiler-Leman dimension of a given graph of color multiplicity at most $5$ is at most $k$.","Moreover, we show that for these color multiplicities this is optimal in the sense that this problem is P-hard under logspace-uniform $\\text{AC}_0$-reductions.","Furthermore, for each larger bound $c$ on the color multiplicity and each fixed $k \\geq 2$, we provide a polynomial-time approximation algorithm for the abelian case: given a relational structure with abelian color classes of size at most $c$, the algorithm outputs either that its Weisfeiler-Leman dimension is at most $(k+1)c$ or that it is larger than $k$."],"url":"http://arxiv.org/abs/2402.11531v1","category":"cs.CC"}
{"created":"2024-02-18 09:22:58","title":"Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading","abstract":"Lipreading involves using visual data to recognize spoken words by analyzing the movements of the lips and surrounding area. It is a hot research topic with many potential applications, such as human-machine interaction and enhancing audio speech recognition. Recent deep-learning based works aim to integrate visual features extracted from the mouth region with landmark points on the lip contours. However, employing a simple combination method such as concatenation may not be the most effective approach to get the optimal feature vector. To address this challenge, firstly, we propose a cross-attention fusion-based approach for large lexicon Arabic vocabulary to predict spoken words in videos. Our method leverages the power of cross-attention networks to efficiently integrate visual and geometric features computed on the mouth region. Secondly, we introduce the first large-scale Lip Reading in the Wild for Arabic (LRW-AR) dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers. The experimental results obtained on LRW-AR and ArabicVisual databases showed the effectiveness and robustness of the proposed approach in recognizing Arabic words. Our work provides insights into the feasibility and effectiveness of applying lipreading techniques to the Arabic language, opening doors for further research in this field. Link to the project page: https://crns-smartvision.github.io/lrwar","sentences":["Lipreading involves using visual data to recognize spoken words by analyzing the movements of the lips and surrounding area.","It is a hot research topic with many potential applications, such as human-machine interaction and enhancing audio speech recognition.","Recent deep-learning based works aim to integrate visual features extracted from the mouth region with landmark points on the lip contours.","However, employing a simple combination method such as concatenation may not be the most effective approach to get the optimal feature vector.","To address this challenge, firstly, we propose a cross-attention fusion-based approach for large lexicon Arabic vocabulary to predict spoken words in videos.","Our method leverages the power of cross-attention networks to efficiently integrate visual and geometric features computed on the mouth region.","Secondly, we introduce the first large-scale Lip Reading in the Wild for Arabic (LRW-AR) dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers.","The experimental results obtained on LRW-AR and ArabicVisual databases showed the effectiveness and robustness of the proposed approach in recognizing Arabic words.","Our work provides insights into the feasibility and effectiveness of applying lipreading techniques to the Arabic language, opening doors for further research in this field.","Link to the project page: https://crns-smartvision.github.io/lrwar"],"url":"http://arxiv.org/abs/2402.11520v1","category":"cs.CV"}
{"created":"2024-02-18 09:07:30","title":"Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics","abstract":"Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we demonstrate the optimized framework for a typical AFC problem where near-linear scaling can be obtained for the overall framework. We achieve a significant boost in parallel efficiency from around 49% to approximately 78%, and the training process is accelerated by approximately 47 times using 60 CPU cores. These findings are expected to provide valuable insights for further advancements in DRL-based AFC studies.","sentences":["Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems.","However, the computational cost associated with training DRL models presents a significant performance bottleneck.","To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings.","We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks.","Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies.","Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement.","Finally, we demonstrate the optimized framework for a typical AFC problem where near-linear scaling can be obtained for the overall framework.","We achieve a significant boost in parallel efficiency from around 49% to approximately 78%, and the training process is accelerated by approximately 47 times using 60 CPU cores.","These findings are expected to provide valuable insights for further advancements in DRL-based AFC studies."],"url":"http://arxiv.org/abs/2402.11515v1","category":"cs.LG"}
{"created":"2024-02-18 08:41:24","title":"Thin-film Lithium Niobate on Insulator Surface Acoustic Wave Devices for 6G Centimeter Bands","abstract":"In this work, we investigate the frequency scaling of shear-horizontal (S.H.) surface acoustic wave (SAW) resonators based on a lithium niobate on insulator (LNOI) substrate into the centimeter bands for 6G wireless systems. Prototyped resonators with wavelengths ranging between 240 nm and 400 nm were fabricated, and the experimental results exhibit a successful frequency scaling between 9.05 and 13.37 GHz. However, a noticeable performance degradation can be observed as the resonance frequency (fs) scales. Such an effect is expected to be caused by non-ideal helec/{\\lambda} for smaller {\\lambda} devices. The optimized LNOI SH-SAW with a {\\lambda} of 400 nm exhibits a fs of 9.05 GHz, a keff2 of 15%, Qmax of 213 and a FoM of 32, which indicates a successful implementation for device targeting centimeter bands.","sentences":["In this work, we investigate the frequency scaling of shear-horizontal (S.H.) surface acoustic wave (SAW) resonators based on a lithium niobate on insulator (LNOI) substrate into the centimeter bands for 6G wireless systems.","Prototyped resonators with wavelengths ranging between 240 nm and 400 nm were fabricated, and the experimental results exhibit a successful frequency scaling between 9.05 and 13.37 GHz.","However, a noticeable performance degradation can be observed as the resonance frequency (fs) scales.","Such an effect is expected to be caused by non-ideal helec/{\\lambda} for smaller {\\lambda} devices.","The optimized LNOI SH-SAW with a {\\lambda} of 400 nm exhibits a fs of 9.05 GHz, a keff2 of 15%, Qmax of 213 and a FoM of 32, which indicates a successful implementation for device targeting centimeter bands."],"url":"http://arxiv.org/abs/2402.11508v1","category":"eess.SP"}
{"created":"2024-02-18 08:21:05","title":"GenAD: Generative End-to-End Autonomous Driving","abstract":"Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency.","sentences":["Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently.","Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning.","However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior.","In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes.","We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem.","We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens.","We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling.","We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories.","GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures.","Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency."],"url":"http://arxiv.org/abs/2402.11502v1","category":"cs.CV"}
{"created":"2024-02-18 07:54:30","title":"Point-Wise Vibration Pattern Production via a Sparse Actuator Array for Surface Tactile Feedback","abstract":"Surface vibration tactile feedback is capable of conveying various semantic information to humans via the handheld electronic devices, like smartphone, touch panel,and game controller. However, covering the whole device contacting surface with dense actuator arrangement can affect its normal use, how to produce desired vibration patterns at any contact point with only several sparse actuators deployed on the handled device surface remains a significant challenge. In this work, we develop a tactile feedback board with only five actuators in the size of a smartphone, and achieve the precise vibration pattern production that can focus at any desired position all over the board. Specifically, we investigate the vibration characteristics of single passive coil actuator, and construct its vibration pattern model at any position on the feedback board surface. Optimal phase and amplitude modulation, found with the simulated annealing algorithm, is employed with five actuators in a sparse array. And all actuators' vibration patterns are superimposed linearly to synthetically generate different onboard vibration energy distribution for tactile sensing. Experiments demonstrated that for point-wise vibration pattern production on our tactile board achieved an average level of about 0.9 in the Structural Similarity Index Measure (SSIM) evaluation, when compared to the ideal single-point-focused target vibration pattern. The sparse actuator array can be easily embedded into usual handheld electronic devices, which shows a good significant implication for enriching their haptic interaction functionalities.","sentences":["Surface vibration tactile feedback is capable of conveying various semantic information to humans via the handheld electronic devices, like smartphone, touch panel,and game controller.","However, covering the whole device contacting surface with dense actuator arrangement can affect its normal use, how to produce desired vibration patterns at any contact point with only several sparse actuators deployed on the handled device surface remains a significant challenge.","In this work, we develop a tactile feedback board with only five actuators in the size of a smartphone, and achieve the precise vibration pattern production that can focus at any desired position all over the board.","Specifically, we investigate the vibration characteristics of single passive coil actuator, and construct its vibration pattern model at any position on the feedback board surface.","Optimal phase and amplitude modulation, found with the simulated annealing algorithm, is employed with five actuators in a sparse array.","And all actuators' vibration patterns are superimposed linearly to synthetically generate different onboard vibration energy distribution for tactile sensing.","Experiments demonstrated that for point-wise vibration pattern production on our tactile board achieved an average level of about 0.9 in the Structural Similarity Index Measure (SSIM) evaluation, when compared to the ideal single-point-focused target vibration pattern.","The sparse actuator array can be easily embedded into usual handheld electronic devices, which shows a good significant implication for enriching their haptic interaction functionalities."],"url":"http://arxiv.org/abs/2402.11496v1","category":"cs.RO"}
{"created":"2024-02-18 07:48:15","title":"Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation","abstract":"In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.","sentences":["In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks.","To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs.","We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt.","Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models.","Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust.","To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge.","Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods.","Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary."],"url":"http://arxiv.org/abs/2402.11493v1","category":"cs.CL"}
{"created":"2024-02-18 07:28:37","title":"Visual Concept-driven Image Generation with Text-to-Image Diffusion Model","abstract":"Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent Dense CRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a bi-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively (through user studies) with a number of examples and use cases that can combine up to three entangled concepts.","sentences":["Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes.","Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations.","However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive.","In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges.","We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model.","However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations.","We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating masks encompassing corresponding concepts in user-supplied images.","We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent Dense CRF optimization.","We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a bi-product, latent masks.","We illustrate the benefits of the proposed approach qualitatively and quantitatively (through user studies) with a number of examples and use cases that can combine up to three entangled concepts."],"url":"http://arxiv.org/abs/2402.11487v1","category":"cs.CV"}
{"created":"2024-02-18 07:26:03","title":"Orbital angular momentum spectrum and entanglement in rotating accelerated reference frame","abstract":"The particle definition varies across different theories. The quantum field theory in curved spacetime shows that from the perspective of a linearly accelerated observer, an inertial empty space may be full of thermal particles. This effect is known as the Unruh effect. When the degrees of freedom of orbital angular momentum (OAM) are considered, all OAM modes share the same expected particle number. Here, we examine the OAM spectrum in a rotating accelerated reference frame to see how the spectrum differs from the linear accelerated case. When the observer starts to rotate, not all OAM modes are allowed and some negative energy modes show up. To understand how a rotating accelerated observer actually perceives these particles, the Unruh-DeWitt detector and its detailed balance are studied. This relation is studied both in the comoving inertial frame and in the rest frame. Based on these results, the OAM entanglement degradation is explored in two-dimensional and high-dimensional cases, respectively. The results indicate that the entanglement dimension and the highest order of OAM modes are mainly related to the acceleration and the rotation, respectively. It is then demonstrated that these results can be generalized to all stationary trajectories.","sentences":["The particle definition varies across different theories.","The quantum field theory in curved spacetime shows that from the perspective of a linearly accelerated observer, an inertial empty space may be full of thermal particles.","This effect is known as the Unruh effect.","When the degrees of freedom of orbital angular momentum (OAM) are considered, all OAM modes share the same expected particle number.","Here, we examine the OAM spectrum in a rotating accelerated reference frame to see how the spectrum differs from the linear accelerated case.","When the observer starts to rotate, not all OAM modes are allowed and some negative energy modes show up.","To understand how a rotating accelerated observer actually perceives these particles, the Unruh-DeWitt detector and its detailed balance are studied.","This relation is studied both in the comoving inertial frame and in the rest frame.","Based on these results, the OAM entanglement degradation is explored in two-dimensional and high-dimensional cases, respectively.","The results indicate that the entanglement dimension and the highest order of OAM modes are mainly related to the acceleration and the rotation, respectively.","It is then demonstrated that these results can be generalized to all stationary trajectories."],"url":"http://arxiv.org/abs/2402.11486v1","category":"gr-qc"}
{"created":"2024-02-18 07:20:34","title":"Optimal Quantum State Tomography via Weak Value","abstract":"To improve the efficiency of the state tomography strategy via weak value, we have searched the optimal coupling strength between the system and measuring device. For an arbitrary d-dimensional quantum system, the optimal strengths being used in measuring the real and imaginary parts of the density matrix are obtained. The optimal efficiency of the state tomography has also been studied by using mean square error. The minimal mean square errors in the reconstructed density matrices have been derived. The state tomography strategy studied in this article may be useful in the measurement of the unknown quantum states.","sentences":["To improve the efficiency of the state tomography strategy via weak value, we have searched the optimal coupling strength between the system and measuring device.","For an arbitrary d-dimensional quantum system, the optimal strengths being used in measuring the real and imaginary parts of the density matrix are obtained.","The optimal efficiency of the state tomography has also been studied by using mean square error.","The minimal mean square errors in the reconstructed density matrices have been derived.","The state tomography strategy studied in this article may be useful in the measurement of the unknown quantum states."],"url":"http://arxiv.org/abs/2402.11484v1","category":"quant-ph"}
{"created":"2024-02-18 06:59:50","title":"Federated Reinforcement Learning for Uplink Centric Broadband Communication Optimization over Unlicensed Spectrum","abstract":"To provide Uplink Centric Broadband Communication (UCBC), New Radio Unlicensed (NR-U) network has been standardized to exploit the unlicensed spectrum using Listen Before Talk (LBT) scheme to fairly coexist with the incumbent Wireless Fidelity (WiFi) network. Existing access schemes over unlicensed spectrum are required to perform Clear Channel Assessment (CCA) before transmissions, where fixed Energy Detection (ED) thresholds are adopted to identify the channel as idle or busy. However, fixed ED thresholds setting prevents devices from accessing the channel effectively and efficiently, which leads to the hidden node (HN) and exposed node (EN) problems. In this paper, we first develop a centralized double Deep Q-Network (DDQN) algorithm to optimize the uplink system throughput, where the agent is deployed at the central server to dynamically adjust the ED thresholds for NR-U and WiFi networks. Considering that heterogeneous NR-U and WiFi networks, in practice, cannot share the raw data with the central server directly, we then develop a federated DDQN algorithm, where two agents are deployed in the NR-U and WiFi networks, respectively. Our results have shown that the uplink system throughput increases by over 100%, where cell throughput of NR-U network rises by 150%, and cell throughput of WiFi network decreases by 30%. To guarantee the cell throughput of WiFi network, we redesign the reward function to punish the agent when the cell throughput of WiFi network is below the threshold, and our revised design can still provide over 50% uplink system throughput gain.","sentences":["To provide Uplink Centric Broadband Communication (UCBC), New Radio Unlicensed (NR-U) network has been standardized to exploit the unlicensed spectrum using Listen Before Talk (LBT) scheme to fairly coexist with the incumbent Wireless Fidelity (WiFi) network.","Existing access schemes over unlicensed spectrum are required to perform Clear Channel Assessment (CCA) before transmissions, where fixed Energy Detection (ED) thresholds are adopted to identify the channel as idle or busy.","However, fixed ED thresholds setting prevents devices from accessing the channel effectively and efficiently, which leads to the hidden node (HN) and exposed node (EN) problems.","In this paper, we first develop a centralized double Deep Q-Network (DDQN) algorithm to optimize the uplink system throughput, where the agent is deployed at the central server to dynamically adjust the ED thresholds for NR-U and WiFi networks.","Considering that heterogeneous NR-U and WiFi networks, in practice, cannot share the raw data with the central server directly, we then develop a federated DDQN algorithm, where two agents are deployed in the NR-U and WiFi networks, respectively.","Our results have shown that the uplink system throughput increases by over 100%, where cell throughput of NR-U network rises by 150%, and cell throughput of WiFi network decreases by 30%.","To guarantee the cell throughput of WiFi network, we redesign the reward function to punish the agent when the cell throughput of WiFi network is below the threshold, and our revised design can still provide over 50% uplink system throughput gain."],"url":"http://arxiv.org/abs/2402.11478v1","category":"eess.SY"}
{"created":"2024-02-18 06:18:00","title":"An Optimal Dividend Problem for Skew Brownian Motion with Two-Valued Drift","abstract":"In this paper we propose a skew Brownian motion with a two-valued drift as a risk model with endogenous regime switching. We solve its two-sided exit problem and consider an optimal control problem for the skew Brownian risk model. In particular, we identify sufficient conditions for either a barrier dividend strategy or a band dividend strategy to be optimal.","sentences":["In this paper we propose a skew Brownian motion with a two-valued drift as a risk model with endogenous regime switching.","We solve its two-sided exit problem and consider an optimal control problem for the skew Brownian risk model.","In particular, we identify sufficient conditions for either a barrier dividend strategy or a band dividend strategy to be optimal."],"url":"http://arxiv.org/abs/2402.11471v1","category":"math.PR"}
{"created":"2024-02-18 05:48:18","title":"Nonparametric assessment of regimen response curve estimators","abstract":"Marginal structural models have been widely used in causal inference to estimate mean outcomes under either a static or a prespecified set of treatment decision rules. This approach requires imposing a working model for the mean outcome given a sequence of treatments and possibly baseline covariates. In this paper, we introduce a dynamic marginal structural model that can be used to estimate an optimal decision rule within a class of parametric rules. Specifically, we will estimate the mean outcome as a function of the parameters in the class of decision rules, referred to as a regimen-response curve. In general, misspecification of the working model may lead to a biased estimate with questionable causal interpretability. To mitigate this issue, we will leverage risk to assess \"goodness-of-fit\" of the imposed working model. We consider the counterfactual risk as our target parameter and derive inverse probability weighting and canonical gradients to map it to the observed data. We provide asymptotic properties of the resulting risk estimators, considering both fixed and data-dependent target parameters. We will show that the inverse probability weighting estimator can be efficient and asymptotic linear when the weight functions are estimated using a sieve-based estimator. The proposed method is implemented on the LS1 study to estimate a regimen-response curve for patients with Parkinson's disease.","sentences":["Marginal structural models have been widely used in causal inference to estimate mean outcomes under either a static or a prespecified set of treatment decision rules.","This approach requires imposing a working model for the mean outcome given a sequence of treatments and possibly baseline covariates.","In this paper, we introduce a dynamic marginal structural model that can be used to estimate an optimal decision rule within a class of parametric rules.","Specifically, we will estimate the mean outcome as a function of the parameters in the class of decision rules, referred to as a regimen-response curve.","In general, misspecification of the working model may lead to a biased estimate with questionable causal interpretability.","To mitigate this issue, we will leverage risk to assess \"goodness-of-fit\" of the imposed working model.","We consider the counterfactual risk as our target parameter and derive inverse probability weighting and canonical gradients to map it to the observed data.","We provide asymptotic properties of the resulting risk estimators, considering both fixed and data-dependent target parameters.","We will show that the inverse probability weighting estimator can be efficient and asymptotic linear when the weight functions are estimated using a sieve-based estimator.","The proposed method is implemented on the LS1 study to estimate a regimen-response curve for patients with Parkinson's disease."],"url":"http://arxiv.org/abs/2402.11466v1","category":"stat.ME"}
{"created":"2024-02-18 04:57:19","title":"When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation","abstract":"Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.","sentences":["Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases.","Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations.","However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time.","A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question.","This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA.","In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence.","Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information.","We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence.","Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls."],"url":"http://arxiv.org/abs/2402.11457v1","category":"cs.CL"}
{"created":"2024-02-18 04:13:51","title":"Informative and non-informative decomposition of turbulent flow fields","abstract":"Not all the information in a turbulent field is relevant for understanding particular regions or components of the flow. Here, we present a method for decomposing a source field into its informative $\\mathbf{\\Phi}_{I}(\\mathbf{x},t)$ and residual $\\mathbf{\\Phi}_R(\\mathbf{x},t)$ components relative to another target field. The method is referred to as informative and non-informative decomposition (IND). All the necessary information for physical understanding, reduced-order modeling, and control of the target variable is contained in $\\mathbf{\\Phi}_{I}(\\mathbf{x},t)$ whereas $\\mathbf{\\Phi}_R(\\mathbf{x},t)$ offers no substantial utility in these contexts. The decomposition is formulated as an optimization problem that seeks to maximize the time lagged mutual information of the source variable with the target variable while minimizing the mutual information of its non-informative component. The method is applied to extract the informative and non-informative components of the velocity field in a turbulent channel flow, using the wall shear stress as the target variable. We demonstrate the utility of IND in three scenarios: (i) understanding the effect of the velocity fluctuations on the wall-shear stress, (ii) predicting the wall-shear stress using velocities far from the wall, and (iii) developing control strategies for drag reduction in opposition control.","sentences":["Not all the information in a turbulent field is relevant for understanding particular regions or components of the flow.","Here, we present a method for decomposing a source field into its informative $\\mathbf{\\Phi}_{I}(\\mathbf{x},t)$ and residual $\\mathbf{\\Phi}_R(\\mathbf{x},t)$ components relative to another target field.","The method is referred to as informative and non-informative decomposition (IND).","All the necessary information for physical understanding, reduced-order modeling, and control of the target variable is contained in $\\mathbf{\\Phi}_{I}(\\mathbf{x},t)$ whereas $\\mathbf{\\Phi}_R(\\mathbf{x},t)$ offers no substantial utility in these contexts.","The decomposition is formulated as an optimization problem that seeks to maximize the time lagged mutual information of the source variable with the target variable while minimizing the mutual information of its non-informative component.","The method is applied to extract the informative and non-informative components of the velocity field in a turbulent channel flow, using the wall shear stress as the target variable.","We demonstrate the utility of IND in three scenarios: (i) understanding the effect of the velocity fluctuations on the wall-shear stress, (ii) predicting the wall-shear stress using velocities far from the wall, and (iii) developing control strategies for drag reduction in opposition control."],"url":"http://arxiv.org/abs/2402.11448v1","category":"physics.flu-dyn"}
{"created":"2024-02-18 04:08:10","title":"In-Context Example Ordering Guided by Label Distributions","abstract":"By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary between near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model's probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better in-context examples.","sentences":["By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP.","However, a number of problems persist in ICL.","In particular, its performance is sensitive to the choice and order of in-context examples.","Given the same set of in-context examples with different orderings, model performance may vary between near random to near state-of-the-art.","In this work, we formulate in-context example ordering as an optimization problem.","We examine three problem settings that differ in the assumptions they make about what is known about the task.","Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model's probability predictions.","We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters.","We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better in-context examples."],"url":"http://arxiv.org/abs/2402.11447v1","category":"cs.CL"}
{"created":"2024-02-18 02:49:13","title":"A Robust Error-Resistant View Selection Method for 3D Reconstruction","abstract":"To address the issue of increased triangulation uncertainty caused by selecting views with small camera baselines in Structure from Motion (SFM) view selection, this paper proposes a robust error-resistant view selection method. The method utilizes a triangulation-based computation to obtain an error-resistant model, which is then used to construct an error-resistant matrix. The sorting results of each row in the error-resistant matrix determine the candidate view set for each view. By traversing the candidate view sets of all views and completing the missing views based on the error-resistant matrix, the integrity of 3D reconstruction is ensured. Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are conducted in terms of average reprojection error and absolute trajectory error in the reconstruction results. The proposed method demonstrates an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on the TUM dataset and DTU dataset.","sentences":["To address the issue of increased triangulation uncertainty caused by selecting views with small camera baselines in Structure from Motion (SFM) view selection, this paper proposes a robust error-resistant view selection method.","The method utilizes a triangulation-based computation to obtain an error-resistant model, which is then used to construct an error-resistant matrix.","The sorting results of each row in the error-resistant matrix determine the candidate view set for each view.","By traversing the candidate view sets of all views and completing the missing views based on the error-resistant matrix, the integrity of 3D reconstruction is ensured.","Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are conducted in terms of average reprojection error and absolute trajectory error in the reconstruction results.","The proposed method demonstrates an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on the TUM dataset and DTU dataset."],"url":"http://arxiv.org/abs/2402.11431v1","category":"cs.CV"}
{"created":"2024-02-18 02:11:54","title":"Online Local False Discovery Rate Control: A Resource Allocation Approach","abstract":"We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\\Omega(\\sqrt{T})$ or even a $\\Omega(T)$ regret. With the observation that canonical policies tend to be too optimistic and over accept arrivals, we propose a novel policy that incorporates budget buffers. We show that small additional logarithmic buffers suffice to reduce the regret from $\\Omega(\\sqrt{T})$ or even $\\Omega(T)$ to $O(\\ln^2 T)$. Numerical experiments are conducted to validate our theoretical findings. Our formulation may have wider applications beyond the problem considered in this paper, and our results emphasize how effective policies should be designed to reach a balance between circumventing wrong accept and reducing wrong reject in online resource allocation problems with uncertain budgets.","sentences":["We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries.","We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment.","We start with general arrival distributions and propose a simple policy that achieves a $O(\\sqrt{T})$ regret.","We complement the result by showing that such regret rate is in general not improvable.","We then shift our focus to discrete arrival distributions.","We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\\Omega(\\sqrt{T})$ or even a $\\Omega(T)$ regret.","With the observation that canonical policies tend to be too optimistic and over accept arrivals, we propose a novel policy that incorporates budget buffers.","We show that small additional logarithmic buffers suffice to reduce the regret from $\\Omega(\\sqrt{T})$ or even $\\Omega(T)$ to $O(\\ln^2 T)$. Numerical experiments are conducted to validate our theoretical findings.","Our formulation may have wider applications beyond the problem considered in this paper, and our results emphasize how effective policies should be designed to reach a balance between circumventing wrong accept and reducing wrong reject in online resource allocation problems with uncertain budgets."],"url":"http://arxiv.org/abs/2402.11425v1","category":"stat.ME"}
{"created":"2024-02-18 01:04:38","title":"Distributionally Robust Ground Delay Programs with Learning-Driven Airport Capacity Predictions","abstract":"Strategic Traffic Management Initiatives (TMIs) such as Ground Delay Programs (GDPs) play a crucial role in mitigating operational costs associated with demand-capacity imbalances. However, GDPs can only be planned (e.g., duration, delay assignments) with confidence if the future capacities at constrained resources (i.e., airports) are predictable. In reality, such future capacities are uncertain, and predictive models may provide forecasts that are vulnerable to errors and distribution shifts. Motivated by the goal of planning optimal GDPs that are \\emph{distributionally robust} against airport capacity prediction errors, we study a fully integrated learning-driven optimization framework. We design a deep learning-based prediction model capable of forecasting arrival and departure capacity distributions across a network of airports. We then integrate the forecasts into a distributionally robust formulation of the multi-airport ground holding problem (\\textsc{dr-MAGHP}). We show how \\textsc{dr-MAGHP} can outperform stochastic optimization when distribution shifts occur, and conclude with future research directions to improve both the learning and optimization stages.","sentences":["Strategic Traffic Management Initiatives (TMIs) such as Ground Delay Programs (GDPs) play a crucial role in mitigating operational costs associated with demand-capacity imbalances.","However, GDPs can only be planned (e.g., duration, delay assignments) with confidence if the future capacities at constrained resources (i.e., airports) are predictable.","In reality, such future capacities are uncertain, and predictive models may provide forecasts that are vulnerable to errors and distribution shifts.","Motivated by the goal of planning optimal GDPs that are \\emph{distributionally robust} against airport capacity prediction errors, we study a fully integrated learning-driven optimization framework.","We design a deep learning-based prediction model capable of forecasting arrival and departure capacity distributions across a network of airports.","We then integrate the forecasts into a distributionally robust formulation of the multi-airport ground holding problem (\\textsc{dr-MAGHP}).","We show how \\textsc{dr-MAGHP} can outperform stochastic optimization when distribution shifts occur, and conclude with future research directions to improve both the learning and optimization stages."],"url":"http://arxiv.org/abs/2402.11415v1","category":"math.OC"}
{"created":"2024-02-18 00:56:16","title":"Aligning Modalities in Vision Large Language Models via Preference Fine-tuning","abstract":"Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. Our data and code are available at https://github.com/YiyangZhou/POVID.","sentences":["Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks.","These approaches merge strong pre-trained vision models and large language models (LLMs).","Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs.","This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations.","In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning.","Specifically, we propose POVID to generate feedback data with AI models.","We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data.","First, we prompt GPT-4V to inject plausible hallucinations into the correct answer.","Second, we distort the image to trigger the inherent hallucination behavior of the VLLM.","This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable.","Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization.","In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches.","Our data and code are available at https://github.com/YiyangZhou/POVID."],"url":"http://arxiv.org/abs/2402.11411v1","category":"cs.LG"}
{"created":"2024-02-18 00:04:40","title":"Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection","abstract":"The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness.","sentences":["The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention.","Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice.","However, the extent to which LLMs effectively address this issue remains insufficiently examined.","This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task).","Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods.","Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech.","(2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complexity.","Consequently, the calibration performance is heavily reliant on primary classification accuracy.","These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes.","This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness."],"url":"http://arxiv.org/abs/2402.11406v1","category":"cs.CL"}
{"created":"2024-02-17 21:56:53","title":"Spaceport Facility Location Planning within the US National Airspace System","abstract":"The burgeoning commercial space transportation industry necessitates an expansion of launch infrastructure to meet rising demands. However, future operations from these large-scale infrastructures can result in new impacts, particularly to air traffic operations. To rigorously reason about where such future spaceports might be located and what their impacts might be, we introduce a facility location planning model for future US spaceports (SPFLP). Central considerations for the SPFLP include population density, space launch trajectories, and potential impacts to air traffic within the US National Airspace System (NAS). The SPFLP outputs a cost-optimal set of candidate locations for future spaceports while satisfying a range of operational constraints. By conducting sensitivity analyses on the SPFLP, we are able to examine differences in flight rerouting costs and optimal launch mission allocations. Our model and numerical experiments offer valuable insights for future spaceport site selection, contributing to the strategic development of commercial space transportation while keeping in mind the need to integrate these operations within the NAS.","sentences":["The burgeoning commercial space transportation industry necessitates an expansion of launch infrastructure to meet rising demands.","However, future operations from these large-scale infrastructures can result in new impacts, particularly to air traffic operations.","To rigorously reason about where such future spaceports might be located and what their impacts might be, we introduce a facility location planning model for future US spaceports (SPFLP).","Central considerations for the SPFLP include population density, space launch trajectories, and potential impacts to air traffic within the US National Airspace System (NAS).","The SPFLP outputs a cost-optimal set of candidate locations for future spaceports while satisfying a range of operational constraints.","By conducting sensitivity analyses on the SPFLP, we are able to examine differences in flight rerouting costs and optimal launch mission allocations.","Our model and numerical experiments offer valuable insights for future spaceport site selection, contributing to the strategic development of commercial space transportation while keeping in mind the need to integrate these operations within the NAS."],"url":"http://arxiv.org/abs/2402.11389v1","category":"math.OC"}
{"created":"2024-02-17 21:35:44","title":"Unitarity effects in elastic scattering at the LHC","abstract":"We study the high-energy behavior of the elastic scattering amplitude using two distinct unitarization schemes: the eikonal and the $U$-matrix. Our analysis begins with a formalism involving solely Pomerons, incorporating pion-loop insertions in the Pomeron trajectory representing the nearest singularity generated by $t$-channel unitarity. Subsequently, we explore a scenario that includes the presence of an Odderon. In our analyses, we explore the tension between the TOTEM and the ATLAS measurements for $\\sigma_{tot}$ and $d\\sigma/dt$ at 7, 8, and 13 TeV, and the subsequent implications for the properties of both the Pomeron and Odderon. Our results show that the Odderon phase factor $\\xi_{\\Bbb O}= -1$ is favored in both unitarization schemes. More interestingly, this specific phase factor stands as the sole one that aligns with results consistent with a non-zero Odderon coupling.","sentences":["We study the high-energy behavior of the elastic scattering amplitude using two distinct unitarization schemes: the eikonal and the $U$-matrix.","Our analysis begins with a formalism involving solely Pomerons, incorporating pion-loop insertions in the Pomeron trajectory representing the nearest singularity generated by $t$-channel unitarity.","Subsequently, we explore a scenario that includes the presence of an Odderon.","In our analyses, we explore the tension between the TOTEM and the ATLAS measurements for $\\sigma_{tot}$ and $d\\sigma/dt$ at 7, 8, and 13 TeV, and the subsequent implications for the properties of both the Pomeron and Odderon.","Our results show that the Odderon phase factor $\\xi_{\\Bbb O}= -1$ is favored in both unitarization schemes.","More interestingly, this specific phase factor stands as the sole one that aligns with results consistent with a non-zero Odderon coupling."],"url":"http://arxiv.org/abs/2402.11385v1","category":"hep-ph"}
{"created":"2024-02-17 18:48:08","title":"Intrinsic femtosecond structure of extreme contrast harmonic pulses: influence on relativistic laser-solid interactions","abstract":"Extreme intensity contrast is considered essential for ultraintense, femtosecond laser excitation of solid targets, in particular for studies with structured or ultra-thin targets. Second-harmonic generation has been used to maximize the contrast in the nanosecond and picosecond timescales but the resulting pulses can have intense broad femtosecond structures in the rising edge of the pulse. We show that femtosecond scale structures that arise in this process critically modify the interaction, by altering the local field structures and hence redirecting the electron trajectories and distributions, especially concerning resonant phenomena such as surface plasmon excitation in structured targets. Particle-in-cell (PIC) simulations fully support and give further insight into our experimental results. Our findings have important implications not only for the use of harmonic pulses on solid targets but also for two-color schemes based on second harmonic pulses.","sentences":["Extreme intensity contrast is considered essential for ultraintense, femtosecond laser excitation of solid targets, in particular for studies with structured or ultra-thin targets.","Second-harmonic generation has been used to maximize the contrast in the nanosecond and picosecond timescales but the resulting pulses can have intense broad femtosecond structures in the rising edge of the pulse.","We show that femtosecond scale structures that arise in this process critically modify the interaction, by altering the local field structures and hence redirecting the electron trajectories and distributions, especially concerning resonant phenomena such as surface plasmon excitation in structured targets.","Particle-in-cell (PIC) simulations fully support and give further insight into our experimental results.","Our findings have important implications not only for the use of harmonic pulses on solid targets but also for two-color schemes based on second harmonic pulses."],"url":"http://arxiv.org/abs/2402.11360v1","category":"physics.plasm-ph"}
{"created":"2024-02-17 18:08:37","title":"Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search","abstract":"Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate that equipping PEOs can increase throughput on a commonly utilized graph index (HNSW) by a factor of 1.6 to 2.5, and its efficiency consistently outperforms the leading-edge routing technique by 1.1 to 1.4 times.","sentences":["Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning.","In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art.","Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing.","This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph.","We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques.","Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice.","Our experiments demonstrate that equipping PEOs can increase throughput on a commonly utilized graph index (HNSW) by a factor of 1.6 to 2.5, and its efficiency consistently outperforms the leading-edge routing technique by 1.1 to 1.4 times."],"url":"http://arxiv.org/abs/2402.11354v1","category":"cs.LG"}
