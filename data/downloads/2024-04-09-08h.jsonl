{"created":"2024-04-08 17:59:26","title":"Hodge-Chern classes and strata-effectivity in tautological rings","abstract":"Given a connected, reductive $\\mathbf{F}_p$-group $G$, a cocharacter $\\mu \\in X_*(G)$ and a smooth zip period map $\\zeta:X \\to \\mathop{\\text{$G$-{\\tt Zip}}}\\nolimits^{\\mu}$, we study which classes in the Wedhorn-Ziegler tautological rings $T^*(X), T^*(Y)$ of $X$ and its flag space $Y \\to G-ZipFlag^{\\mu}$ are \\textit{strata-effective}, meaning that they are non-negative rational linear combinations of pullbacks of classes of zip (flag) strata closures. Two special cases are: (1) When $X=G\\text{-Zip}^{\\mu}$ and the tautological rings $\\T^*(X)=\\text{CH}_{\\mathbf{Q}}(G-Zip^{\\mu})$, $T^*(Y)=\\text{CH}_{\\mathbf{Q}}(G-ZipFlag^{\\mu})$ are the entire Chow ring, and (2) When $X$ is the special fiber of an integral canonical model of a Hodge-type Shimura variety -- in this case the strata are also known as Ekedahl-Oort strata. We focus on the strata-effectivity of three types of classes: (a) Effective tautological classes, (b) Chern classes of Griffiths-Hodge bundles and (c) Generically $w$-ordinary curves. We connect the question of strata-effectivity in (a) to the global section `Cone Conjecture' of Goldring-Koskivirta. For every representation $r$ of $G$, we conjecture that the Chern classes of the Griffiths-Hodge bundle associated to $(G, \\mu,r)$ are all strata-effective. This provides a vast generalization of a result of Ekedahl-van der Geer that the Chern classes of the Hodge vector bundle on the moduli space of principally polarized abelian varieties $\\Acal_{g,\\mathbf{F}_p}$ in characteristic $p$ are represented by the closures of $p$-rank strata. We prove several instances of our conjecture","sentences":["Given a connected, reductive $\\mathbf{F}_p$-group $G$, a cocharacter $\\mu \\in X_*(G)$ and a smooth zip period map $\\zeta:X \\to \\mathop{\\text{$G$-{\\tt Zip}}}\\nolimits^{\\mu}$, we study which classes in the Wedhorn-Ziegler tautological rings $T^*(X), T^*(Y)$ of $X$ and its flag space $Y \\to G-ZipFlag^{\\mu}$ are \\textit{strata-effective}, meaning that they are non-negative rational linear combinations of pullbacks of classes of zip (flag) strata closures.","Two special cases are: (1) When $X=G\\text{-Zip}^{\\mu}$ and the tautological rings $\\T^*(X)=\\text{CH}_{\\mathbf{Q}}(G-Zip^{\\mu})$, $T^*(Y)=\\text{CH}_{\\mathbf{Q}}(G-ZipFlag^{\\mu})$ are the entire Chow ring, and (2) When $X$ is the special fiber of an integral canonical model of a Hodge-type Shimura variety -- in this case the strata are also known as Ekedahl-Oort strata.","We focus on the strata-effectivity of three types of classes: (a) Effective tautological classes, (b) Chern classes of Griffiths-Hodge bundles and (c) Generically","$w$-ordinary curves.","We connect the question of strata-effectivity in (a) to the global section `Cone Conjecture' of Goldring-Koskivirta.","For every representation $r$ of $G$, we conjecture that the Chern classes of the Griffiths-Hodge bundle associated to $(G, \\mu,r)$ are all strata-effective.","This provides a vast generalization of a result of Ekedahl-van der Geer that the Chern classes of the Hodge vector bundle on the moduli space of principally polarized abelian varieties $\\Acal_{g,\\mathbf{F}_p}$ in characteristic $p$ are represented by the closures of $p$-rank strata.","We prove several instances of our conjecture"],"url":"http://arxiv.org/abs/2404.05727v1","category":"math.AG"}
{"created":"2024-04-08 17:59:11","title":"The MOSDEF Survey: Properties of Warm Ionised Outflows at $z=$ 1.4-3.8","abstract":"We use the large spectroscopic data set of the MOSFIRE Deep Evolution Field survey to investigate the kinematics and energetics of ionised gas outflows. Using a sample of 598 star-forming galaxies at redshift 1.4 < $z$ < 3.8, we decompose $\\rm{H}\\alpha$ and [OIII] emission lines into narrow and broad components, finding significant detections of broad components in 10% of the sample. The ionised outflow velocity from individual galaxies appears independent of galaxy properties, such as stellar mass, star-formation rate (SFR), and star-formation-rate surface density ($\\Sigma_{\\rm SFR}$). Adopting a simple outflow model, we estimate the mass-, energy- and momentum-loading factors of the ionised outflows, finding modest values with averages of 0.33, 0.04, and 0.22, respectively. The larger momentum- than energy-loading factors, for the adopted physical parameters, imply that these ionised outflows are primarily momentum-driven. We further find a marginal correlation (2.5$\\sigma$) between the mass-loading factor and stellar mass in agreement with predictions by simulations, scaling as $\\eta_{m}$ $\\propto M_{\\star}^{-0.45}$. This shallow scaling relation is consistent with these ionised outflows being driven by a combination of mechanical energy generated by supernovae explosions and radiation pressure acting on dusty material. In a majority of galaxies, the outflowing material does not appear to have sufficient velocity to escape the gravitational potential of their host, likely recycling back at later times. Together, these results suggest that the ionised outflows traced by nebular emission lines are negligible, with the bulk of mass and energy carried out in other gaseous phases.","sentences":["We use the large spectroscopic data set of the MOSFIRE Deep Evolution Field survey to investigate the kinematics and energetics of ionised gas outflows.","Using a sample of 598 star-forming galaxies at redshift 1.4 <","$z$ <","3.8, we decompose $\\rm{H}\\alpha$ and [OIII] emission lines into narrow and broad components, finding significant detections of broad components in 10% of the sample.","The ionised outflow velocity from individual galaxies appears independent of galaxy properties, such as stellar mass, star-formation rate (SFR), and star-formation-rate surface density ($\\Sigma_{\\rm SFR}$).","Adopting a simple outflow model, we estimate the mass-, energy- and momentum-loading factors of the ionised outflows, finding modest values with averages of 0.33, 0.04, and 0.22, respectively.","The larger momentum- than energy-loading factors, for the adopted physical parameters, imply that these ionised outflows are primarily momentum-driven.","We further find a marginal correlation (2.5$\\sigma$) between the mass-loading factor and stellar mass in agreement with predictions by simulations, scaling as $\\eta_{m}$ $\\propto M_{\\star}^{-0.45}$. This shallow scaling relation is consistent with these ionised outflows being driven by a combination of mechanical energy generated by supernovae explosions and radiation pressure acting on dusty material.","In a majority of galaxies, the outflowing material does not appear to have sufficient velocity to escape the gravitational potential of their host, likely recycling back at later times.","Together, these results suggest that the ionised outflows traced by nebular emission lines are negligible, with the bulk of mass and energy carried out in other gaseous phases."],"url":"http://arxiv.org/abs/2404.05725v1","category":"astro-ph.GA"}
{"created":"2024-04-08 17:58:22","title":"Predicting Overtakes in Trucks Using CAN Data","abstract":"Safe overtakes in trucks are crucial to prevent accidents, reduce congestion, and ensure efficient traffic flow, making early prediction essential for timely and informed driving decisions. Accordingly, we investigate the detection of truck overtakes from CAN data. Three classifiers, Artificial Neural Networks (ANN), Random Forest, and Support Vector Machines (SVM), are employed for the task. Our analysis covers up to 10 seconds before the overtaking event, using an overlapping sliding window of 1 second to extract CAN features. We observe that the prediction scores of the overtake class tend to increase as we approach the overtake trigger, while the no-overtake class remain stable or oscillates depending on the classifier. Thus, the best accuracy is achieved when approaching the trigger, making early overtaking prediction challenging. The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%), but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90% and below 60% for one SVM variant). We further combine two classifiers (Random Forest and linear SVM) by averaging their output scores. The fusion is observed to improve no-overtake classification (TNR > 92%) at the expense of reducing overtake accuracy (TPR). However, the latter is kept above 91% near the overtake trigger. Therefore, the fusion balances TPR and TNR, providing more consistent performance than individual classifiers.","sentences":["Safe overtakes in trucks are crucial to prevent accidents, reduce congestion, and ensure efficient traffic flow, making early prediction essential for timely and informed driving decisions.","Accordingly, we investigate the detection of truck overtakes from CAN data.","Three classifiers, Artificial Neural Networks (ANN), Random Forest, and Support Vector Machines (SVM), are employed for the task.","Our analysis covers up to 10 seconds before the overtaking event, using an overlapping sliding window of 1 second to extract CAN features.","We observe that the prediction scores of the overtake class tend to increase as we approach the overtake trigger, while the no-overtake class remain stable or oscillates depending on the classifier.","Thus, the best accuracy is achieved when approaching the trigger, making early overtaking prediction challenging.","The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%), but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90% and below 60% for one SVM variant).","We further combine two classifiers (Random Forest and linear SVM) by averaging their output scores.","The fusion is observed to improve no-overtake classification (TNR > 92%) at the expense of reducing overtake accuracy (TPR).","However, the latter is kept above 91% near the overtake trigger.","Therefore, the fusion balances TPR and TNR, providing more consistent performance than individual classifiers."],"url":"http://arxiv.org/abs/2404.05723v1","category":"cs.LG"}
{"created":"2024-04-08 17:56:43","title":"Language-Independent Representations Improve Zero-Shot Summarization","abstract":"Finetuning pretrained models on downstream generation tasks often leads to catastrophic forgetting in zero-shot conditions. In this work, we focus on summarization and tackle the problem through the lens of language-independent representations. After training on monolingual summarization, we perform zero-shot transfer to new languages or language pairs. We first show naively finetuned models are highly language-specific in both output behavior and internal representations, resulting in poor zero-shot performance. Next, we propose query-key (QK) finetuning to decouple task-specific knowledge from the pretrained language generation abilities. Then, after showing downsides of the standard adversarial language classifier, we propose a balanced variant that more directly enforces language-agnostic representations. Moreover, our qualitative analyses show removing source language identity correlates to zero-shot summarization performance. Our code is openly available.","sentences":["Finetuning pretrained models on downstream generation tasks often leads to catastrophic forgetting in zero-shot conditions.","In this work, we focus on summarization and tackle the problem through the lens of language-independent representations.","After training on monolingual summarization, we perform zero-shot transfer to new languages or language pairs.","We first show naively finetuned models are highly language-specific in both output behavior and internal representations, resulting in poor zero-shot performance.","Next, we propose query-key (QK) finetuning to decouple task-specific knowledge from the pretrained language generation abilities.","Then, after showing downsides of the standard adversarial language classifier, we propose a balanced variant that more directly enforces language-agnostic representations.","Moreover, our qualitative analyses show removing source language identity correlates to zero-shot summarization performance.","Our code is openly available."],"url":"http://arxiv.org/abs/2404.05720v1","category":"cs.CL"}
{"created":"2024-04-08 17:55:44","title":"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs","abstract":"Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate \"any resolution\" on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens). Both sub-images are encoded separately before being sent to LLMs. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.","sentences":["Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens.","In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities.","Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate \"any resolution\" on top of Ferret to magnify details and leverage enhanced visual features.","Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens).","Both sub-images are encoded separately before being sent to LLMs.","We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing.","These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding.","To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference.","After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions.","For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks.","Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks."],"url":"http://arxiv.org/abs/2404.05719v1","category":"cs.CV"}
{"created":"2024-04-08 17:52:43","title":"Electronic and structural properties of group IV materials and their polytypes","abstract":"Nanotechnology's impact on semiconductor industry advancement, particularly through the engineering of nanostructures like nanowires, opens new possibilities for material functionality due to the tunable physical properties of nanostructures compared to bulk materials. This paper presents a comprehensive study on group IV semiconductors and their binaries across four polytypes: 2H, 3C, 4H, and 6H, focusing on their optoelectronic application potential. Deep understanding of these polytypes is particularly relevant for nanowire-based technologies. Through first principles modeling, we examine the structural and electronic properties of these materials, emphasizing their band structure, stability, and the feasibility for light-emitting applications. We use a generalized Ising model to discuss materials stability and tendency for polytypism. We also determine relative band edge positions and employ a six $k\\cdot p$ model for a detailed understanding of the materials' electronic properties. Due to the comprehensive nature of this study, we provide insight on the chemical trends present in all of the studied properties. Our theoretical predictions align well with existing experimental data, suggesting new avenues for nanostructure-based device development. The discussion extends to the implications of these findings for the fabrication of optoelectronic devices with the studied IV-IV materials, highlighting the challenges and opportunities for future research in nanowire synthesis and their application.","sentences":["Nanotechnology's impact on semiconductor industry advancement, particularly through the engineering of nanostructures like nanowires, opens new possibilities for material functionality due to the tunable physical properties of nanostructures compared to bulk materials.","This paper presents a comprehensive study on group IV semiconductors and their binaries across four polytypes: 2H, 3C, 4H, and 6H, focusing on their optoelectronic application potential.","Deep understanding of these polytypes is particularly relevant for nanowire-based technologies.","Through first principles modeling, we examine the structural and electronic properties of these materials, emphasizing their band structure, stability, and the feasibility for light-emitting applications.","We use a generalized Ising model to discuss materials stability and tendency for polytypism.","We also determine relative band edge positions and employ a six $k\\cdot p$ model for a detailed understanding of the materials' electronic properties.","Due to the comprehensive nature of this study, we provide insight on the chemical trends present in all of the studied properties.","Our theoretical predictions align well with existing experimental data, suggesting new avenues for nanostructure-based device development.","The discussion extends to the implications of these findings for the fabrication of optoelectronic devices with the studied IV-IV materials, highlighting the challenges and opportunities for future research in nanowire synthesis and their application."],"url":"http://arxiv.org/abs/2404.05718v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 17:52:29","title":"SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing","abstract":"Effective editing of personal content holds a pivotal role in enabling individuals to express their creativity, weaving captivating narratives within their visual stories, and elevate the overall quality and impact of their visual content. Therefore, in this work, we introduce SwapAnything, a novel framework that can swap any objects in an image with personalized concepts given by the reference, while keeping the context unchanged. Compared with existing methods for personalized subject swapping, SwapAnything has three unique advantages: (1) precise control of arbitrary objects and parts rather than the main subject, (2) more faithful preservation of context pixels, (3) better adaptation of the personalized concept to the image. First, we propose targeted variable swapping to apply region control over latent feature maps and swap masked variables for faithful context preservation and initial semantic concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt the semantic concept into the original image in terms of target location, shape, style, and content during the image generation process. Extensive results on both human and automatic evaluation demonstrate significant improvements of our approach over baseline methods on personalized swapping. Furthermore, SwapAnything shows its precise and faithful swapping abilities across single object, multiple objects, partial object, and cross-domain swapping tasks. SwapAnything also achieves great performance on text-based swapping and tasks beyond swapping such as object insertion.","sentences":["Effective editing of personal content holds a pivotal role in enabling individuals to express their creativity, weaving captivating narratives within their visual stories, and elevate the overall quality and impact of their visual content.","Therefore, in this work, we introduce SwapAnything, a novel framework that can swap any objects in an image with personalized concepts given by the reference, while keeping the context unchanged.","Compared with existing methods for personalized subject swapping, SwapAnything has three unique advantages: (1) precise control of arbitrary objects and parts rather than the main subject, (2) more faithful preservation of context pixels, (3) better adaptation of the personalized concept to the image.","First, we propose targeted variable swapping to apply region control over latent feature maps and swap masked variables for faithful context preservation and initial semantic concept swapping.","Then, we introduce appearance adaptation, to seamlessly adapt the semantic concept into the original image in terms of target location, shape, style, and content during the image generation process.","Extensive results on both human and automatic evaluation demonstrate significant improvements of our approach over baseline methods on personalized swapping.","Furthermore, SwapAnything shows its precise and faithful swapping abilities across single object, multiple objects, partial object, and cross-domain swapping tasks.","SwapAnything also achieves great performance on text-based swapping and tasks beyond swapping such as object insertion."],"url":"http://arxiv.org/abs/2404.05717v1","category":"cs.CV"}
{"created":"2024-04-08 17:49:00","title":"Effective Luttinger parameter and Kane-Fisher effect in quasiperiodic systems","abstract":"The ground states of interacting one-dimensional metals are generically Luttinger liquids. Luttinger liquid theory is usually considered for translation invariant systems. The Luttinger liquid description remains valid for weak quasiperiodic modulations; however, as the quasiperiodic modulation gets increasingly strong, it is increasingly renormalized and eventually fails, as the system becomes localized. We explore how quasiperiodic modulation renormalizes the Luttinger parameter characterizing this emergent Luttinger liquid, using the renormalization of transmission coefficients across a barrier as a proxy that remains valid for general quasiperiodic modulation. We find, unexpectedly, that quasiperiodic modulation weakens the effects of short-range interactions, but enhances those of long-range interactions. We support the former finding with matrix-product numerics. We also discuss how interactions affect the localization phase boundary.","sentences":["The ground states of interacting one-dimensional metals are generically Luttinger liquids.","Luttinger liquid theory is usually considered for translation invariant systems.","The Luttinger liquid description remains valid for weak quasiperiodic modulations; however, as the quasiperiodic modulation gets increasingly strong, it is increasingly renormalized and eventually fails, as the system becomes localized.","We explore how quasiperiodic modulation renormalizes the Luttinger parameter characterizing this emergent Luttinger liquid, using the renormalization of transmission coefficients across a barrier as a proxy that remains valid for general quasiperiodic modulation.","We find, unexpectedly, that quasiperiodic modulation weakens the effects of short-range interactions, but enhances those of long-range interactions.","We support the former finding with matrix-product numerics.","We also discuss how interactions affect the localization phase boundary."],"url":"http://arxiv.org/abs/2404.05711v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 17:47:15","title":"Endpoint-homogeneous fans","abstract":"A fan $F$ is \\emph{endpoint-homogeneous} if for any two endpoints $e,e'$ of $F$, there is a homeomorphism $h: F \\rightarrow F$ such that $h(e) = e'$.   We prove there are uncountably many distinct homeomorphism types of endpoint-homogeneous smooth fans. To do this, we associate to each such fan $F$ a topological invariant, in the form of a characteristic subset $EPG(F) \\subseteq [0,1]$ describing how the endpoints of $F$ limit onto any given blade of $F$. We describe precisely all the uncountably many different $X \\subseteq [0,1]$ that can arise as $EPG(F)$ for some endpoint-homogeneous smooth fan $F$. We also prove the existence of $\\frac{1}{n}$-homogeneous smooth fans for all $n \\geq 5$.","sentences":["A fan $F$ is \\emph{endpoint-homogeneous} if for any two endpoints $e,e'$ of $F$, there is a homeomorphism $h: F \\rightarrow F$ such that $h(e) = e'$.   ","We prove there are uncountably many distinct homeomorphism types of endpoint-homogeneous smooth fans.","To do this, we associate to each such fan $F$ a topological invariant, in the form of a characteristic subset $EPG(F)","\\subseteq [0,1]$ describing how the endpoints of $F$ limit onto any given blade of $F$. We describe precisely all the uncountably many different $X","\\subseteq [0,1]$ that can arise as $EPG(F)$ for some endpoint-homogeneous smooth fan $F$.","We also prove the existence of $\\frac{1}{n}$-homogeneous smooth fans for all $n \\geq 5$."],"url":"http://arxiv.org/abs/2404.05709v1","category":"math.GN"}
{"created":"2024-04-08 17:43:53","title":"An Ogus Principle for Zip period maps: The Hasse invariant's vanishing order via `Frobenius and the Hodge filtration'","abstract":"This paper generalizes a result of Ogus that, under certain technical conditions, the vanishing order of the Hasse invariant of a family $Y/X$ of $n$-dimensional Calabi-Yau varieties in characteristic $p$ at a point $x$ of $X$ equals the \"conjugate line position\" of $H^n_{\\text{dR}}(Y/X)$ at $x$, i.e. the largest $i$ such that the line of the conjugate filtration is contained in $\\text{Fil}^i$ of the Hodge filtration. For every triple $(G,\\mu,r)$ consisting of a connected, reductive $\\mathbf{F}_p$-group $G$, a cocharacter $\\mu \\in X_*(G)$ and an $\\mathbf{F}_p$-representation $r$ of $G$, we state a generalized Ogus Principle. If $\\zeta:X \\to \\text{$G$-$\\mathtt{Zip}$}^{\\mu}$ is a smooth morphism (=`Zip period map'), then the group theoretic Ogus Principle implies an Ogus Principle on $X$. We deduce an Ogus Principle for several Hodge and abelian-type Shimura varieties and the moduli space of K3 surfaces.","sentences":["This paper generalizes a result of Ogus that, under certain technical conditions, the vanishing order of the Hasse invariant of a family $Y/X$ of $n$-dimensional Calabi-Yau varieties in characteristic $p$ at a point $x$ of $X$ equals the \"conjugate line position\" of $H^n_{\\text{dR}}(Y/X)$ at $x$, i.e. the largest $i$ such that the line of the conjugate filtration is contained in $\\text{Fil}^i$ of the Hodge filtration.","For every triple $(G,\\mu,r)$ consisting of a connected, reductive $\\mathbf{F}_p$-group $G$, a cocharacter $\\mu \\in X_*(G)$ and an $\\mathbf{F}_p$-representation $r$ of $G$, we state a generalized Ogus Principle.","If $\\zeta:X \\to \\text{$G$-$\\mathtt{Zip}$}^{\\mu}$ is a smooth morphism (=`Zip period map'), then the group theoretic Ogus Principle implies an Ogus Principle on $X$. We deduce an Ogus Principle for several Hodge and abelian-type Shimura varieties and the moduli space of K3 surfaces."],"url":"http://arxiv.org/abs/2404.05707v1","category":"math.AG"}
{"created":"2024-04-08 17:43:01","title":"Weakly closed semigroups generated by operator-valued functions","abstract":"The function $P(T)=\\sum_{i=0}^\\infty c_i T^i$ is admissible if $c_i\\geq 0$, $\\sum_{i=0}^\\infty c_i\\leq 1$. For any given set of admissible functions $P_1,\\dots, P_k$ there is a unitary operator $T$ of dynamic origin such that the weak closure of its powers is a semigroup generated by the operators $0$, $T$, $P_1(T),\\dots, P_k(T)$, $T^\\ast$, $P_1(T^\\ast ),\\dots, P_k(T^\\ast)$.","sentences":["The function $P(T)=\\sum_{i=0}^\\infty c_i T^i$ is admissible if $c_i\\geq 0$, $\\sum_{i=0}^\\infty c_i\\leq 1$.","For any given set of admissible functions $P_1,\\dots, P_k$ there is a unitary operator $T$ of dynamic origin such that the weak closure of its powers is a semigroup generated by the operators $0$, $T$, $P_1(T),\\dots, P_k(T)$, $T^\\ast$, $P_1(T^\\ast ),\\dots, P_k(T^\\ast)$."],"url":"http://arxiv.org/abs/2404.05706v1","category":"math.DS"}
{"created":"2024-04-08 17:42:08","title":"Learning 3D-Aware GANs from Unposed Images with Template Feature Field","abstract":"Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice. This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (TeFF). Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field. Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data. Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives.","sentences":["Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice.","This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (TeFF).","Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field.","Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data.","Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives."],"url":"http://arxiv.org/abs/2404.05705v1","category":"cs.CV"}
{"created":"2024-04-08 17:26:28","title":"Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer","abstract":"Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer. The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/.","sentences":["Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment.","Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies.","This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer.","The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/."],"url":"http://arxiv.org/abs/2404.05695v1","category":"cs.RO"}
{"created":"2024-04-08 17:24:04","title":"Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding","abstract":"Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can struggle in specialized domains such as medicine, where unique domain-specific terminologies, domain-specific abbreviations, and varying document structures are common. This paper explores strategies for adapting these models to domain-specific requirements, primarily through continuous pre-training on domain-specific data. We pre-trained several German medical language models on 2.4B tokens derived from translated public English medical data and 3B tokens of German clinical data. The resulting models were evaluated on various German downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question answering. Our results suggest that models augmented by clinical and translation-based pre-training typically outperform general domain models in medical contexts. We conclude that continuous pre-training has demonstrated the ability to match or even exceed the performance of clinical models trained from scratch. Furthermore, pre-training on clinical data or leveraging translated texts have proven to be reliable methods for domain adaptation in medical NLP tasks.","sentences":["Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa.","While these models demonstrate remarkable performance on general datasets, they can struggle in specialized domains such as medicine, where unique domain-specific terminologies, domain-specific abbreviations, and varying document structures are common.","This paper explores strategies for adapting these models to domain-specific requirements, primarily through continuous pre-training on domain-specific data.","We pre-trained several German medical language models on 2.4B tokens derived from translated public English medical data and 3B tokens of German clinical data.","The resulting models were evaluated on various German downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question answering.","Our results suggest that models augmented by clinical and translation-based pre-training typically outperform general domain models in medical contexts.","We conclude that continuous pre-training has demonstrated the ability to match or even exceed the performance of clinical models trained from scratch.","Furthermore, pre-training on clinical data or leveraging translated texts have proven to be reliable methods for domain adaptation in medical NLP tasks."],"url":"http://arxiv.org/abs/2404.05694v1","category":"cs.CL"}
{"created":"2024-04-08 17:18:30","title":"Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic Segmentation for Satellite Imagery","abstract":"Satellite imagery is crucial for tasks like environmental monitoring and urban planning. Typically, it relies on semantic segmentation or Land Use Land Cover (LULC) classification to categorize each pixel. Despite the advancements brought about by Deep Neural Networks (DNNs), their performance in segmentation tasks is hindered by challenges such as limited availability of labeled data, class imbalance and the inherent variability and complexity of satellite images. In order to mitigate those issues, our study explores the effectiveness of a Cut-and-Paste augmentation technique for semantic segmentation in satellite images. We adapt this augmentation, which usually requires labeled instances, to the case of semantic segmentation. By leveraging the connected components in the semantic segmentation labels, we extract instances that are then randomly pasted during training. Using the DynamicEarthNet dataset and a U-Net model for evaluation, we found that this augmentation significantly enhances the mIoU score on the test set from 37.9 to 44.1. This finding highlights the potential of the Cut-and-Paste augmentation to improve the generalization capabilities of semantic segmentation models in satellite imagery.","sentences":["Satellite imagery is crucial for tasks like environmental monitoring and urban planning.","Typically, it relies on semantic segmentation or Land Use Land Cover (LULC) classification to categorize each pixel.","Despite the advancements brought about by Deep Neural Networks (DNNs), their performance in segmentation tasks is hindered by challenges such as limited availability of labeled data, class imbalance and the inherent variability and complexity of satellite images.","In order to mitigate those issues, our study explores the effectiveness of a Cut-and-Paste augmentation technique for semantic segmentation in satellite images.","We adapt this augmentation, which usually requires labeled instances, to the case of semantic segmentation.","By leveraging the connected components in the semantic segmentation labels, we extract instances that are then randomly pasted during training.","Using the DynamicEarthNet dataset and a U-Net model for evaluation, we found that this augmentation significantly enhances the mIoU score on the test set from 37.9 to 44.1.","This finding highlights the potential of the Cut-and-Paste augmentation to improve the generalization capabilities of semantic segmentation models in satellite imagery."],"url":"http://arxiv.org/abs/2404.05693v1","category":"cs.CV"}
{"created":"2024-04-08 17:18:04","title":"Evaluating Mathematical Reasoning Beyond Accuracy","abstract":"The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs $\\textit{validity}$ and $\\textit{redundancy}$ to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. Instantiated by base models that possess strong mathematical knowledge and trained with high-quality labeled data, ReasonEval achieves state-of-the-art performance on human-labeled datasets and can accurately detect different types of errors generated by perturbation. When applied to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We release the best-performing model, meta-evaluation script, and all evaluation results at https://github.com/GAIR-NLP/ReasonEval.","sentences":["The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated.","However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps.","This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process.","To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps.","ReasonEval employs $\\textit{validity}$ and $\\textit{redundancy}$ to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically.","Instantiated by base models that possess strong mathematical knowledge and trained with high-quality labeled data, ReasonEval achieves state-of-the-art performance on human-labeled datasets and can accurately detect different types of errors generated by perturbation.","When applied to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems.","Additionally, we observe that ReasonEval can play a significant role in data selection.","We release the best-performing model, meta-evaluation script, and all evaluation results at https://github.com/GAIR-NLP/ReasonEval."],"url":"http://arxiv.org/abs/2404.05692v1","category":"cs.CL"}
{"created":"2024-04-08 17:16:16","title":"GW230529_181500: A Potential Primordial Binary Black Hole Merger in the Mass Gap","abstract":"During the fourth observing run of the LIGO-Virgo-KAGRA detector network, the LIGO Livingston observatory detected a coalescing compact binary, GW230529_181500, with component masses of $2.5-4.5\\, M_\\odot$ and $1.2-2.0\\, M_\\odot$ at the $90\\%$ credible level. The gravitational-wave data alone is insufficient to determine whether the components are neutron stars or black holes. In this paper, we propose that GW230529_181500 originated from the merger of two primordial black holes (PBHs). We estimate a merger rate of $5.0^{+47.0}_{-4.9} \\mathrm{Gpc}^{-3}\\,\\mathrm{yr}^{-1}$ for compact binary coalescences with properties similar to GW230529_181500. Assuming the source is a PBH-PBH merger, GW230529-like events lead to approximately $1.7^{+36.2}_{-1.5} \\times 10^{-3}$ of the dark matter in the form of PBHs. The required abundance of PBHs to explain this event is consistent with existing upper limits derived from microlensing, cosmic microwave background observations and the null detection of gravitational wave background by LIGO-Virgo-KAGRA.","sentences":["During the fourth observing run of the LIGO-Virgo-KAGRA detector network, the LIGO Livingston observatory detected a coalescing compact binary, GW230529_181500, with component masses of $2.5-4.5\\, M_\\odot$ and $1.2-2.0\\, M_\\odot$ at the $90\\%$ credible level.","The gravitational-wave data alone is insufficient to determine whether the components are neutron stars or black holes.","In this paper, we propose that GW230529_181500 originated from the merger of two primordial black holes (PBHs).","We estimate a merger rate of $5.0^{+47.0}_{-4.9} \\mathrm{Gpc}^{-3}\\,\\mathrm{yr}^{-1}$ for compact binary coalescences with properties similar to GW230529_181500.","Assuming the source is a PBH-PBH merger, GW230529-like events lead to approximately $1.7^{+36.2}_{-1.5} \\times 10^{-3}$ of the dark matter in the form of PBHs.","The required abundance of PBHs to explain this event is consistent with existing upper limits derived from microlensing, cosmic microwave background observations and the null detection of gravitational wave background by LIGO-Virgo-KAGRA."],"url":"http://arxiv.org/abs/2404.05691v1","category":"gr-qc"}
{"created":"2024-04-08 17:15:37","title":"Automated discovery of symbolic laws governing skill acquisition from naturally occurring data","abstract":"Skill acquisition is a key area of research in cognitive psychology as it encompasses multiple psychological processes. The laws discovered under experimental paradigms are controversial and lack generalizability. This paper aims to unearth the laws of skill learning from large-scale training log data. A two-stage algorithm was developed to tackle the issues of unobservable cognitive states and algorithmic explosion in searching. Initially a deep learning model is employed to determine the learner's cognitive state and assess the feature importance. Subsequently, symbolic regression algorithms are utilized to parse the neural network model into algebraic equations. The experimental results of simulated data demonstrate that the proposed algorithm can accurately restore various preset laws within a certain range of noise, in continues feedback setting. Application of proposed method to Lumosity training data demonstrates superior performance compared to traditional and latest models in terms of fitness. The results indicate the discovery of two new forms of skill acquisition laws, while some previous findings have been reaffirmed.","sentences":["Skill acquisition is a key area of research in cognitive psychology as it encompasses multiple psychological processes.","The laws discovered under experimental paradigms are controversial and lack generalizability.","This paper aims to unearth the laws of skill learning from large-scale training log data.","A two-stage algorithm was developed to tackle the issues of unobservable cognitive states and algorithmic explosion in searching.","Initially a deep learning model is employed to determine the learner's cognitive state and assess the feature importance.","Subsequently, symbolic regression algorithms are utilized to parse the neural network model into algebraic equations.","The experimental results of simulated data demonstrate that the proposed algorithm can accurately restore various preset laws within a certain range of noise, in continues feedback setting.","Application of proposed method to Lumosity training data demonstrates superior performance compared to traditional and latest models in terms of fitness.","The results indicate the discovery of two new forms of skill acquisition laws, while some previous findings have been reaffirmed."],"url":"http://arxiv.org/abs/2404.05689v1","category":"cs.LG"}
{"created":"2024-04-08 17:14:32","title":"David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs at the Deep Edge","abstract":"ML is shifting from the cloud to the edge. Edge computing reduces the surface exposing private data and enables reliable throughput guarantees in real-time applications. Of the panoply of devices deployed at the edge, resource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders of magnitude cheaper, and less power-hungry than application processors or GPUs. Thus, enabling intelligence at the deep edge is the zeitgeist, with researchers focusing on unveiling novel approaches to deploy ANNs on these constrained devices. Quantization is a well-established technique that has proved effective in enabling the deployment of neural networks on MCUs; however, it is still an open question to understand the robustness of QNNs in the face of adversarial examples.   To fill this gap, we empirically evaluate the effectiveness of attacks and defenses from (full-precision) ANNs on (constrained) QNNs. Our evaluation includes three QNNs targeting TinyML applications, ten attacks, and six defenses. With this study, we draw a set of interesting findings. First, quantization increases the point distance to the decision boundary and leads the gradient estimated by some attacks to explode or vanish. Second, quantization can act as a noise attenuator or amplifier, depending on the noise magnitude, and causes gradient misalignment. Regarding adversarial defenses, we conclude that input pre-processing defenses show impressive results on small perturbations; however, they fall short as the perturbation increases. At the same time, train-based defenses increase the average point distance to the decision boundary, which holds after quantization. However, we argue that train-based defenses still need to smooth the quantization-shift and gradient misalignment phenomenons to counteract adversarial example transferability to QNNs. All artifacts are open-sourced to enable independent validation of results.","sentences":["ML is shifting from the cloud to the edge.","Edge computing reduces the surface exposing private data and enables reliable throughput guarantees in real-time applications.","Of the panoply of devices deployed at the edge, resource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders of magnitude cheaper, and less power-hungry than application processors or GPUs.","Thus, enabling intelligence at the deep edge is the zeitgeist, with researchers focusing on unveiling novel approaches to deploy ANNs on these constrained devices.","Quantization is a well-established technique that has proved effective in enabling the deployment of neural networks on MCUs; however, it is still an open question to understand the robustness of QNNs in the face of adversarial examples.   ","To fill this gap, we empirically evaluate the effectiveness of attacks and defenses from (full-precision) ANNs on (constrained) QNNs.","Our evaluation includes three QNNs targeting TinyML applications, ten attacks, and six defenses.","With this study, we draw a set of interesting findings.","First, quantization increases the point distance to the decision boundary and leads the gradient estimated by some attacks to explode or vanish.","Second, quantization can act as a noise attenuator or amplifier, depending on the noise magnitude, and causes gradient misalignment.","Regarding adversarial defenses, we conclude that input pre-processing defenses show impressive results on small perturbations; however, they fall short as the perturbation increases.","At the same time, train-based defenses increase the average point distance to the decision boundary, which holds after quantization.","However, we argue that train-based defenses still need to smooth the quantization-shift and gradient misalignment phenomenons to counteract adversarial example transferability to QNNs.","All artifacts are open-sourced to enable independent validation of results."],"url":"http://arxiv.org/abs/2404.05688v1","category":"cs.LG"}
{"created":"2024-04-08 17:10:45","title":"Retrieval-Augmented Open-Vocabulary Object Detection","abstract":"Open-vocabulary object detection (OVD) has been studied with Vision-Language Models (VLMs) to detect novel objects beyond the pre-trained categories. Previous approaches improve the generalization ability to expand the knowledge of the detector, using 'positive' pseudo-labels with additional 'class' names, e.g., sock, iPod, and alligator. To extend the previous methods in two aspects, we propose Retrieval-Augmented Losses and visual Features (RALF). Our method retrieves related 'negative' classes and augments loss functions. Also, visual features are augmented with 'verbalized concepts' of classes, e.g., worn on the feet, handheld music player, and sharp teeth. Specifically, RALF consists of two modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual Features (RAF). RAL constitutes two losses reflecting the semantic similarity with negative vocabularies. In addition, RAF augments visual features with the verbalized concepts from a large language model (LLM). Our experiments demonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We achieve improvement up to 3.4 box AP$_{50}^{\\text{N}}$ on novel categories of the COCO dataset and 3.6 mask AP$_{\\text{r}}$ gains on the LVIS dataset. Code is available at https://github.com/mlvlab/RALF .","sentences":["Open-vocabulary object detection (OVD) has been studied with Vision-Language Models (VLMs) to detect novel objects beyond the pre-trained categories.","Previous approaches improve the generalization ability to expand the knowledge of the detector, using 'positive' pseudo-labels with additional 'class' names, e.g., sock, iPod, and alligator.","To extend the previous methods in two aspects, we propose Retrieval-Augmented Losses and visual Features (RALF).","Our method retrieves related 'negative' classes and augments loss functions.","Also, visual features are augmented with 'verbalized concepts' of classes, e.g., worn on the feet, handheld music player, and sharp teeth.","Specifically, RALF consists of two modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual Features (RAF).","RAL constitutes two losses reflecting the semantic similarity with negative vocabularies.","In addition, RAF augments visual features with the verbalized concepts from a large language model (LLM).","Our experiments demonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets.","We achieve improvement up to 3.4 box AP$_{50}^{\\text{N}}$ on novel categories of the COCO dataset and 3.6 mask AP$_{\\text{r}}$ gains on the LVIS dataset.","Code is available at https://github.com/mlvlab/RALF ."],"url":"http://arxiv.org/abs/2404.05687v1","category":"cs.CV"}
{"created":"2024-04-08 17:10:11","title":"Chebyshev pseudosite matrix product state approach for cluster perturbation theory","abstract":"We introduce the Chebyshev pseudosite matrix product state approach (ChePSMPS) as a solver for cluster perturbation theory (CPT), crucial for simulating spectral functions in two-dimensional electron-phonon ($e$-ph) coupling systems. ChePSMPS distinguishes itself from conventional exact diagonalization solvers by supporting larger clusters, thereby significantly mitigating finite-size effects. Free from the fermion sign problem, ChePSMPS enhances its ability to explore $e$-ph effects and generate high-resolution spectral functions in doped Mott insulators. We use this method to simulate the spectra for both one- and two-dimensional Hubbard-Holstein models, highlighting its superiority over other methods. Our findings validate ChePSMPS as a powerful and reliable Green's function solver. In conjunction with embedding methods, ChePSMPS emerges as an essential tool for simulating strongly correlated $e$-ph coupling systems.","sentences":["We introduce the Chebyshev pseudosite matrix product state approach (ChePSMPS) as a solver for cluster perturbation theory (CPT), crucial for simulating spectral functions in two-dimensional electron-phonon ($e$-ph) coupling systems.","ChePSMPS distinguishes itself from conventional exact diagonalization solvers by supporting larger clusters, thereby significantly mitigating finite-size effects.","Free from the fermion sign problem, ChePSMPS enhances its ability to explore $e$-ph effects and generate high-resolution spectral functions in doped Mott insulators.","We use this method to simulate the spectra for both one-","and two-dimensional Hubbard-Holstein models, highlighting its superiority over other methods.","Our findings validate ChePSMPS as a powerful and reliable Green's function solver.","In conjunction with embedding methods, ChePSMPS emerges as an essential tool for simulating strongly correlated $e$-ph coupling systems."],"url":"http://arxiv.org/abs/2404.05686v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 16:59:39","title":"Even Faster Knapsack via Rectangular Monotone Min-Plus Convolution and Balancing","abstract":"We present a pseudopolynomial-time algorithm for the Knapsack problem that has running time $\\widetilde{O}(n + t\\sqrt{p_{\\max}})$, where $n$ is the number of items, $t$ is the knapsack capacity, and $p_{\\max}$ is the maximum item profit. This improves over the $\\widetilde{O}(n + t \\, p_{\\max})$-time algorithm based on the convolution and prediction technique by Bateni et al.~(STOC 2018). Moreover, we give some evidence, based on a strengthening of the Min-Plus Convolution Hypothesis, that our running time might be optimal.   Our algorithm uses two new technical tools, which might be of independent interest. First, we generalize the $\\widetilde{O}(n^{1.5})$-time algorithm for bounded monotone min-plus convolution by Chi et al.~(STOC 2022) to the \\emph{rectangular} case where the range of entries can be different from the sequence length. Second, we give a reduction from general knapsack instances to \\emph{balanced} instances, where all items have nearly the same profit-to-weight ratio, up to a constant factor.   Using these techniques, we can also obtain algorithms that run in time $\\widetilde{O}(n + OPT\\sqrt{w_{\\max}})$, $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3}t^{2/3})$, and $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3} OPT^{2/3})$, where $OPT$ is the optimal total profit and $w_{\\max}$ is the maximum item weight.","sentences":["We present a pseudopolynomial-time algorithm for the Knapsack problem that has running time $\\widetilde{O}(n + t\\sqrt{p_{\\max}})$, where $n$ is the number of items, $t$ is the knapsack capacity, and $p_{\\max}$ is the maximum item profit.","This improves over the $\\widetilde{O}(n + t \\, p_{\\max})$-time algorithm based on the convolution and prediction technique by Bateni et al.~(STOC 2018).","Moreover, we give some evidence, based on a strengthening of the Min-Plus Convolution Hypothesis, that our running time might be optimal.   ","Our algorithm uses two new technical tools, which might be of independent interest.","First, we generalize the $\\widetilde{O}(n^{1.5})$-time algorithm for bounded monotone min-plus convolution by Chi et al.~(STOC 2022) to the \\emph{rectangular} case where the range of entries can be different from the sequence length.","Second, we give a reduction from general knapsack instances to \\emph{balanced} instances, where all items have nearly the same profit-to-weight ratio, up to a constant factor.   ","Using these techniques, we can also obtain algorithms that run in time $\\widetilde{O}(n + OPT\\sqrt{w_{\\max}})$, $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3}t^{2/3})$, and $\\widetilde{O}(n + (nw_{\\max}p_{\\max})^{1/3} OPT^{2/3})$, where $OPT$ is the optimal total profit and $w_{\\max}$ is the maximum item weight."],"url":"http://arxiv.org/abs/2404.05681v1","category":"cs.DS"}
{"created":"2024-04-08 16:58:31","title":"SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation","abstract":"While recent advances in 3D-aware Generative Adversarial Networks (GANs) have aided the development of near-frontal view human face synthesis, the challenge of comprehensively synthesizing a full 3D head viewable from all angles still persists. Although PanoHead proves the possibilities of using a large-scale dataset with images of both frontal and back views for full-head synthesis, it often causes artifacts for back views. Based on our in-depth analysis, we found the reasons are mainly twofold. First, from network architecture perspective, we found each plane in the utilized tri-plane/tri-grid representation space tends to confuse the features from both sides, causing \"mirroring\" artifacts (e.g., the glasses appear in the back). Second, from data supervision aspect, we found that existing discriminator training in 3D GANs mainly focuses on the quality of the rendered image itself, and does not care much about its plausibility with the perspective from which it was rendered. This makes it possible to generate \"face\" in non-frontal views, due to its easiness to fool the discriminator. In response, we propose SphereHead, a novel tri-plane representation in the spherical coordinate system that fits the human head's geometric characteristics and efficiently mitigates many of the generated artifacts. We further introduce a view-image consistency loss for the discriminator to emphasize the correspondence of the camera parameters and the images. The combination of these efforts results in visually superior outcomes with significantly fewer artifacts. Our code and dataset are publicly available at https://lhyfst.github.io/spherehead.","sentences":["While recent advances in 3D-aware Generative Adversarial Networks (GANs) have aided the development of near-frontal view human face synthesis, the challenge of comprehensively synthesizing a full 3D head viewable from all angles still persists.","Although PanoHead proves the possibilities of using a large-scale dataset with images of both frontal and back views for full-head synthesis, it often causes artifacts for back views.","Based on our in-depth analysis, we found the reasons are mainly twofold.","First, from network architecture perspective, we found each plane in the utilized tri-plane/tri-grid representation space tends to confuse the features from both sides, causing \"mirroring\" artifacts (e.g., the glasses appear in the back).","Second, from data supervision aspect, we found that existing discriminator training in 3D GANs mainly focuses on the quality of the rendered image itself, and does not care much about its plausibility with the perspective from which it was rendered.","This makes it possible to generate \"face\" in non-frontal views, due to its easiness to fool the discriminator.","In response, we propose SphereHead, a novel tri-plane representation in the spherical coordinate system that fits the human head's geometric characteristics and efficiently mitigates many of the generated artifacts.","We further introduce a view-image consistency loss for the discriminator to emphasize the correspondence of the camera parameters and the images.","The combination of these efforts results in visually superior outcomes with significantly fewer artifacts.","Our code and dataset are publicly available at https://lhyfst.github.io/spherehead."],"url":"http://arxiv.org/abs/2404.05680v1","category":"cs.CV"}
{"created":"2024-04-08 16:56:45","title":"Pulsar Timing Array Harmonic Analysis and Source Angular Correlations","abstract":"Gravitational waves (GW) influence the arrival times of radio signals coming from pulsars. Here, we investigate the harmonic space approach to describing the pulsar response to a GW. We derive and discuss the \"diagonalized form\" of the response, which is a sum of spin-2-weighted spherical harmonics of the GW direction multiplied by normal (spin-weight 0) spherical harmonics of the pulsar direction. We show how this allows many useful objects, for example the Hellings and Downs two-point function, to be easily calculated. The approach also provides a clear description of the gauge dependence. We then employ this harmonic approach to model the effects of angular correlations in the sky locations of GW sources (sometimes called \"statistical isotropy\"). To do this, we construct rotationally invariant ensembles made up of many Gaussian subensembles, each of which breaks rotational invariance. Using harmonic techniques, we compute the cosmic covariance and the total covariance of the Hellings and Downs correlation in these models. The results may be used to assess the impact of angular source correlations on the Hellings and Downs correlation, and for optimal reconstruction of the Hellings and Downs curve in models where GW sources have correlated sky locations.","sentences":["Gravitational waves (GW) influence the arrival times of radio signals coming from pulsars.","Here, we investigate the harmonic space approach to describing the pulsar response to a GW.","We derive and discuss the \"diagonalized form\" of the response, which is a sum of spin-2-weighted spherical harmonics of the GW direction multiplied by normal (spin-weight 0) spherical harmonics of the pulsar direction.","We show how this allows many useful objects, for example the Hellings and Downs two-point function, to be easily calculated.","The approach also provides a clear description of the gauge dependence.","We then employ this harmonic approach to model the effects of angular correlations in the sky locations of GW sources (sometimes called \"statistical isotropy\").","To do this, we construct rotationally invariant ensembles made up of many Gaussian subensembles, each of which breaks rotational invariance.","Using harmonic techniques, we compute the cosmic covariance and the total covariance of the Hellings and Downs correlation in these models.","The results may be used to assess the impact of angular source correlations on the Hellings and Downs correlation, and for optimal reconstruction of the Hellings and Downs curve in models where GW sources have correlated sky locations."],"url":"http://arxiv.org/abs/2404.05677v1","category":"gr-qc"}
{"created":"2024-04-08 16:55:49","title":"MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation","abstract":"In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.","sentences":["In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities.","As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows.","Addressing this need, MoMA specializes in subject-driven personalized image generation.","Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator.","This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model.","To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images.","Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness.","Our work is open-source, thereby providing universal access to these advancements."],"url":"http://arxiv.org/abs/2404.05674v1","category":"cs.CV"}
{"created":"2024-04-08 16:55:39","title":"CoReS: Orchestrating the Dance of Reasoning and Segmentation","abstract":"The reasoning segmentation task, which demands a nuanced comprehension of intricate queries to accurately pinpoint object regions, is attracting increasing attention. However, Multi-modal Large Language Models (MLLM) often find it difficult to accurately localize the objects described in complex reasoning contexts. We believe that the act of reasoning segmentation should mirror the cognitive stages of human visual search, where each step is a progressive refinement of thought toward the final object. Thus we introduce the Chains of Reasoning and Segmenting (CoReS) and find this top-down visual hierarchy indeed enhances the visual search process. Specifically, we propose a dual-chain structure that generates multi-modal, chain-like outputs to aid the segmentation process. Furthermore, to steer the MLLM's outputs into this intended hierarchy, we incorporate in-context inputs as guidance. Extensive experiments demonstrate the superior performance of our CoReS, which surpasses the state-of-the-art method by 7.1\\% on the ReasonSeg dataset. The code will be released at https://github.com/baoxiaoyi/CoReS.","sentences":["The reasoning segmentation task, which demands a nuanced comprehension of intricate queries to accurately pinpoint object regions, is attracting increasing attention.","However, Multi-modal Large Language Models (MLLM) often find it difficult to accurately localize the objects described in complex reasoning contexts.","We believe that the act of reasoning segmentation should mirror the cognitive stages of human visual search, where each step is a progressive refinement of thought toward the final object.","Thus we introduce the Chains of Reasoning and Segmenting (CoReS) and find this top-down visual hierarchy indeed enhances the visual search process.","Specifically, we propose a dual-chain structure that generates multi-modal, chain-like outputs to aid the segmentation process.","Furthermore, to steer the MLLM's outputs into this intended hierarchy, we incorporate in-context inputs as guidance.","Extensive experiments demonstrate the superior performance of our CoReS, which surpasses the state-of-the-art method by 7.1\\% on the ReasonSeg dataset.","The code will be released at https://github.com/baoxiaoyi/CoReS."],"url":"http://arxiv.org/abs/2404.05673v1","category":"cs.CV"}
{"created":"2024-04-08 16:55:16","title":"Enumerating runs, valleys, and peaks in Catalan words","abstract":"We provide generating functions, formulas, and asymptotic expressions for the number of Catalan words based on the number of runs of ascents (descents), runs of weak ascents (descents), $\\ell$-valleys, valleys, symmetric valleys, $\\ell$-peaks, peaks, and symmetric peaks. We also establish some bijections with restricted Dyck paths and ordered trees that transports some statistics.","sentences":["We provide generating functions, formulas, and asymptotic expressions for the number of Catalan words based on the number of runs of ascents (descents), runs of weak ascents (descents), $\\ell$-valleys, valleys, symmetric valleys, $\\ell$-peaks, peaks, and symmetric peaks.","We also establish some bijections with restricted Dyck paths and ordered trees that transports some statistics."],"url":"http://arxiv.org/abs/2404.05672v1","category":"math.CO"}
{"created":"2024-04-08 16:53:12","title":"The impact of large-scale galaxy clustering on the variance of the Hellings-Downs correlation","abstract":"The origin of the stochastic gravitational wave (GW) background, recently discovered from pulsar timing array experiments, is still unclear. If this background is of astrophysical origin, we expect the distribution of GW sources to follow the one of galaxies. Since galaxies are not perfectly isotropically distributed at large scales, but follow the cosmological large-scale structure, this would lead to an intrinsic anisotropy in the distribution of GW sources. In this work, we develop a formalism to account for this anisotropy, by considering a Gaussian ensemble of sources in each realization of the universe and then taking ensemble averages over all such realizations. We find that large-scale galaxy clustering has no impact on the Hellings-Downs curve, describing the expectation value of pulsar timing residual correlations. However, it introduces a new contribution to the variance of the Hellings-Downs correlation. Hence, due to the anisotropic distribution of sources, the measurements of pulsar timing residual correlations in our Universe may differ from the Hellings-Downs curve. This indicates that the variance of the Hellings-Downs correlation can be utilized as a new cosmological observable that might help us to unveil the nature of current background observations in the nHz band.","sentences":["The origin of the stochastic gravitational wave (GW) background, recently discovered from pulsar timing array experiments, is still unclear.","If this background is of astrophysical origin, we expect the distribution of GW sources to follow the one of galaxies.","Since galaxies are not perfectly isotropically distributed at large scales, but follow the cosmological large-scale structure, this would lead to an intrinsic anisotropy in the distribution of GW sources.","In this work, we develop a formalism to account for this anisotropy, by considering a Gaussian ensemble of sources in each realization of the universe and then taking ensemble averages over all such realizations.","We find that large-scale galaxy clustering has no impact on the Hellings-Downs curve, describing the expectation value of pulsar timing residual correlations.","However, it introduces a new contribution to the variance of the Hellings-Downs correlation.","Hence, due to the anisotropic distribution of sources, the measurements of pulsar timing residual correlations in our Universe may differ from the Hellings-Downs curve.","This indicates that the variance of the Hellings-Downs correlation can be utilized as a new cosmological observable that might help us to unveil the nature of current background observations in the nHz band."],"url":"http://arxiv.org/abs/2404.05670v1","category":"astro-ph.CO"}
{"created":"2024-04-08 16:52:21","title":"NAF-DPM: A Nonlinear Activation-Free Diffusion Probabilistic Model for Document Enhancement","abstract":"Real-world documents may suffer various forms of degradation, often resulting in lower accuracy in optical character recognition (OCR) systems. Therefore, a crucial preprocessing step is essential to eliminate noise while preserving text and key features of documents. In this paper, we propose NAF-DPM, a novel generative framework based on a diffusion probabilistic model (DPM) designed to restore the original quality of degraded documents. While DPMs are recognized for their high-quality generated images, they are also known for their large inference time. To mitigate this problem we provide the DPM with an efficient nonlinear activation-free (NAF) network and we employ as a sampler a fast solver of ordinary differential equations, which can converge in a few iterations. To better preserve text characters, we introduce an additional differentiable module based on convolutional recurrent neural networks, simulating the behavior of an OCR system during training. Experiments conducted on various datasets showcase the superiority of our approach, achieving state-of-the-art performance in terms of pixel-level and perceptual similarity metrics. Furthermore, the results demonstrate a notable character error reduction made by OCR systems when transcribing real-world document images enhanced by our framework. Code and pre-trained models are available at https://github.com/ispamm/NAF-DPM.","sentences":["Real-world documents may suffer various forms of degradation, often resulting in lower accuracy in optical character recognition (OCR) systems.","Therefore, a crucial preprocessing step is essential to eliminate noise while preserving text and key features of documents.","In this paper, we propose NAF-DPM, a novel generative framework based on a diffusion probabilistic model (DPM) designed to restore the original quality of degraded documents.","While DPMs are recognized for their high-quality generated images, they are also known for their large inference time.","To mitigate this problem we provide the DPM with an efficient nonlinear activation-free (NAF) network and we employ as a sampler a fast solver of ordinary differential equations, which can converge in a few iterations.","To better preserve text characters, we introduce an additional differentiable module based on convolutional recurrent neural networks, simulating the behavior of an OCR system during training.","Experiments conducted on various datasets showcase the superiority of our approach, achieving state-of-the-art performance in terms of pixel-level and perceptual similarity metrics.","Furthermore, the results demonstrate a notable character error reduction made by OCR systems when transcribing real-world document images enhanced by our framework.","Code and pre-trained models are available at https://github.com/ispamm/NAF-DPM."],"url":"http://arxiv.org/abs/2404.05669v1","category":"cs.CV"}
{"created":"2024-04-08 16:52:15","title":"Assessment of practical satellite quantum key distribution architectures for current and near-future missions","abstract":"Quantum key distribution (QKD) allows the generation of cryptographic keys beyond the computational hardness paradigm and is befitting for secure data transmission requiring long-term security. The communication distance of fibre-based QKD, however, is limited to a few hundred kilometers due to the exponential scaling of signal attenuation. Satellite QKD (SatQKD) can instead leverage free-space optical links to establish long-range connections and enable global-scale QKD. In this work we review the manifold of design choices that concur to form the set of possible SatQKD architectures. These include the choice of the QKD protocol and its physical implementation, but also the satellite orbit, the optical link direction, and whether or not to use trusted-node relays. The possible SatQKD architectures are then evaluated in terms of key generation throughput, latency and maximum reachable communication distance, but also the system-level security and implementation complexity. Given the technical challenges of realising SatQKD systems it is paramount, for near-future satellite missions, to adhere to the simplest possible architecture that still allows to deliver the QKD service. We thus identify as advisable options the use of low-Earth orbit satellites as trusted nodes for prepare-and-measure discrete-variable QKD downlinks with weak laser pulses. The decoy-state version of BB84 is found to be the most promising QKD protocols due to the maturity of the security proofs, the high key generation rate and low system complexity. These findings are confirmed by the multitude of current and planned SatQKD missions that are adopting these architectural choices.","sentences":["Quantum key distribution (QKD) allows the generation of cryptographic keys beyond the computational hardness paradigm and is befitting for secure data transmission requiring long-term security.","The communication distance of fibre-based QKD, however, is limited to a few hundred kilometers due to the exponential scaling of signal attenuation.","Satellite QKD (SatQKD) can instead leverage free-space optical links to establish long-range connections and enable global-scale QKD.","In this work we review the manifold of design choices that concur to form the set of possible SatQKD architectures.","These include the choice of the QKD protocol and its physical implementation, but also the satellite orbit, the optical link direction, and whether or not to use trusted-node relays.","The possible SatQKD architectures are then evaluated in terms of key generation throughput, latency and maximum reachable communication distance, but also the system-level security and implementation complexity.","Given the technical challenges of realising SatQKD systems it is paramount, for near-future satellite missions, to adhere to the simplest possible architecture that still allows to deliver the QKD service.","We thus identify as advisable options the use of low-Earth orbit satellites as trusted nodes for prepare-and-measure discrete-variable QKD downlinks with weak laser pulses.","The decoy-state version of BB84 is found to be the most promising QKD protocols due to the maturity of the security proofs, the high key generation rate and low system complexity.","These findings are confirmed by the multitude of current and planned SatQKD missions that are adopting these architectural choices."],"url":"http://arxiv.org/abs/2404.05668v1","category":"quant-ph"}
{"created":"2024-04-08 16:51:33","title":"AlignZeg: Mitigating Objective Misalignment for Zero-shot Semantic Segmentation","abstract":"A serious issue that harms the performance of zero-shot visual recognition is named objective misalignment, i.e., the learning objective prioritizes improving the recognition accuracy of seen classes rather than unseen classes, while the latter is the true target to pursue. This issue becomes more significant in zero-shot image segmentation because the stronger (i.e., pixel-level) supervision brings a larger gap between seen and unseen classes. To mitigate it, we propose a novel architecture named AlignZeg, which embodies a comprehensive improvement of the segmentation pipeline, including proposal extraction, classification, and correction, to better fit the goal of zero-shot segmentation. (1) Mutually-Refined Proposal Extraction. AlignZeg harnesses a mutual interaction between mask queries and visual features, facilitating detailed class-agnostic mask proposal extraction. (2) Generalization-Enhanced Proposal Classification. AlignZeg introduces synthetic data and incorporates multiple background prototypes to allocate a more generalizable feature space. (3) Predictive Bias Correction. During the inference stage, AlignZeg uses a class indicator to find potential unseen class proposals followed by a prediction postprocess to correct the prediction bias. Experiments demonstrate that AlignZeg markedly enhances zero-shot semantic segmentation, as shown by an average 3.8% increase in hIoU, primarily attributed to a 7.1% improvement in identifying unseen classes, and we further validate that the improvement comes from alleviating the objective misalignment issue.","sentences":["A serious issue that harms the performance of zero-shot visual recognition is named objective misalignment, i.e., the learning objective prioritizes improving the recognition accuracy of seen classes rather than unseen classes, while the latter is the true target to pursue.","This issue becomes more significant in zero-shot image segmentation because the stronger (i.e., pixel-level) supervision brings a larger gap between seen and unseen classes.","To mitigate it, we propose a novel architecture named AlignZeg, which embodies a comprehensive improvement of the segmentation pipeline, including proposal extraction, classification, and correction, to better fit the goal of zero-shot segmentation.","(1) Mutually-Refined Proposal Extraction.","AlignZeg harnesses a mutual interaction between mask queries and visual features, facilitating detailed class-agnostic mask proposal extraction.","(2) Generalization-Enhanced Proposal Classification.","AlignZeg introduces synthetic data and incorporates multiple background prototypes to allocate a more generalizable feature space.","(3) Predictive Bias Correction.","During the inference stage, AlignZeg uses a class indicator to find potential unseen class proposals followed by a prediction postprocess to correct the prediction bias.","Experiments demonstrate that AlignZeg markedly enhances zero-shot semantic segmentation, as shown by an average 3.8% increase in hIoU, primarily attributed to a 7.1% improvement in identifying unseen classes, and we further validate that the improvement comes from alleviating the objective misalignment issue."],"url":"http://arxiv.org/abs/2404.05667v1","category":"cs.CV"}
{"created":"2024-04-08 16:51:19","title":"YaART: Yet Another ART Rendering Technology","abstract":"In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models.","sentences":["In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier.","This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF).","During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before.","In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice.","Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training.","From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models."],"url":"http://arxiv.org/abs/2404.05666v1","category":"cs.CV"}
{"created":"2024-04-08 16:50:51","title":"Substructures of the Weyl group and their physical applications","abstract":"We study substructures of the Weyl group of conformal transformations of the metric of (pseudo)Riemannian manifolds. These substructures are identified by differential constraints on the conformal factors of the transformations which are chosen such that their composition is associative. Mathematically, apart from rare exceptions, they are partial associative groupoids, not groups, so they do not have an algebra of infinitesimal transformations, but this limitation can be partially circumvented using some of their properties cleverly. We classify and discuss the substructures with two-derivatives differential constraints, the most famous of which being known as the harmonic or restricted Weyl group in the physics literature, but we also show the existence of a lightcone constraint which realizes a proper subgroup of the Weyl group. We then show the physical implications that come from invariance under the two most important substructures, concentrating on classical properties of the energy-momentum tensor and a generalization of the quantum trace anomaly. We also elaborate further on the harmonic substructure, which can be interpreted as partial gauge fixing of full Weyl invariance using BRST methods. Finally, we discuss how to construct differential constraints of arbitrary higher-derivative order and present, as examples, generalizations involving scalar constraints with four and six derivatives.","sentences":["We study substructures of the Weyl group of conformal transformations of the metric of (pseudo)Riemannian manifolds.","These substructures are identified by differential constraints on the conformal factors of the transformations which are chosen such that their composition is associative.","Mathematically, apart from rare exceptions, they are partial associative groupoids, not groups, so they do not have an algebra of infinitesimal transformations, but this limitation can be partially circumvented using some of their properties cleverly.","We classify and discuss the substructures with two-derivatives differential constraints, the most famous of which being known as the harmonic or restricted Weyl group in the physics literature, but we also show the existence of a lightcone constraint which realizes a proper subgroup of the Weyl group.","We then show the physical implications that come from invariance under the two most important substructures, concentrating on classical properties of the energy-momentum tensor and a generalization of the quantum trace anomaly.","We also elaborate further on the harmonic substructure, which can be interpreted as partial gauge fixing of full Weyl invariance using BRST methods.","Finally, we discuss how to construct differential constraints of arbitrary higher-derivative order and present, as examples, generalizations involving scalar constraints with four and six derivatives."],"url":"http://arxiv.org/abs/2404.05665v1","category":"hep-th"}
{"created":"2024-04-08 16:46:25","title":"BinaryDM: Towards Accurate Binarization of Diffusion Model","abstract":"With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs. However, the highly discrete representation leads to severe accuracy degradation, hindering the quantization of diffusion models to ultra-low bit-widths. In this paper, we propose BinaryDM, a novel accurate quantization-aware training approach to push the weights of diffusion models towards the limit of 1-bit. Firstly, we present a Learnable Multi-basis Binarizer (LMB) to recover the representations generated by the binarized DM, which improves the information in details of representations crucial to the DM. Secondly, a Low-rank Representation Mimicking (LRM) is applied to enhance the binarization-aware optimization of the DM, alleviating the optimization direction ambiguity caused by fine-grained alignment. Moreover, a progressive initialization strategy is applied to training DMs to avoid convergence difficulties. Comprehensive experiments demonstrate that BinaryDM achieves significant accuracy and efficiency gains compared to SOTA quantization methods of DMs under ultra-low bit-widths. As the first binarization method for diffusion models, BinaryDM achieves impressive 16.0 times FLOPs and 27.1 times storage savings with 1-bit weight and 4-bit activation, showcasing its substantial advantages and potential for deploying DMs on resource-limited scenarios.","sentences":["With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs.","However, the highly discrete representation leads to severe accuracy degradation, hindering the quantization of diffusion models to ultra-low bit-widths.","In this paper, we propose BinaryDM, a novel accurate quantization-aware training approach to push the weights of diffusion models towards the limit of 1-bit.","Firstly, we present a Learnable Multi-basis Binarizer (LMB) to recover the representations generated by the binarized DM, which improves the information in details of representations crucial to the DM.","Secondly, a Low-rank Representation Mimicking (LRM) is applied to enhance the binarization-aware optimization of the DM, alleviating the optimization direction ambiguity caused by fine-grained alignment.","Moreover, a progressive initialization strategy is applied to training DMs to avoid convergence difficulties.","Comprehensive experiments demonstrate that BinaryDM achieves significant accuracy and efficiency gains compared to SOTA quantization methods of DMs under ultra-low bit-widths.","As the first binarization method for diffusion models, BinaryDM achieves impressive 16.0 times FLOPs and 27.1 times storage savings with 1-bit weight and 4-bit activation, showcasing its substantial advantages and potential for deploying DMs on resource-limited scenarios."],"url":"http://arxiv.org/abs/2404.05662v1","category":"cs.CV"}
{"created":"2024-04-08 16:46:07","title":"Automatic Controllable Colorization via Imagination","abstract":"We propose a framework for automatic colorization that allows for iterative editing and modifications. The core of our framework lies in an imagination module: by understanding the content within a grayscale image, we utilize a pre-trained image generation model to generate multiple images that contain the same content. These images serve as references for coloring, mimicking the process of human experts. As the synthesized images can be imperfect or different from the original grayscale image, we propose a Reference Refinement Module to select the optimal reference composition. Unlike most previous end-to-end automatic colorization algorithms, our framework allows for iterative and localized modifications of the colorization results because we explicitly model the coloring samples. Extensive experiments demonstrate the superiority of our framework over existing automatic colorization algorithms in editability and flexibility. Project page: https://xy-cong.github.io/imagine-colorization.","sentences":["We propose a framework for automatic colorization that allows for iterative editing and modifications.","The core of our framework lies in an imagination module: by understanding the content within a grayscale image, we utilize a pre-trained image generation model to generate multiple images that contain the same content.","These images serve as references for coloring, mimicking the process of human experts.","As the synthesized images can be imperfect or different from the original grayscale image, we propose a Reference Refinement Module to select the optimal reference composition.","Unlike most previous end-to-end automatic colorization algorithms, our framework allows for iterative and localized modifications of the colorization results because we explicitly model the coloring samples.","Extensive experiments demonstrate the superiority of our framework over existing automatic colorization algorithms in editability and flexibility.","Project page: https://xy-cong.github.io/imagine-colorization."],"url":"http://arxiv.org/abs/2404.05661v1","category":"cs.CV"}
{"created":"2024-04-08 16:43:52","title":"VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain","abstract":"Due to privacy restrictions, there's a shortage of publicly available speech recognition datasets in the medical domain. In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech. To our best knowledge, VietMed is by far the world's largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available here: https://github.com/leduckhai/MultiMed.","sentences":["Due to privacy restrictions, there's a shortage of publicly available speech recognition datasets in the medical domain.","In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech.","To our best knowledge, VietMed is by far the world's largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents.","VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration.","Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country.","Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR.","Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%).","All code, data and models are made publicly available here: https://github.com/leduckhai/MultiMed."],"url":"http://arxiv.org/abs/2404.05659v1","category":"cs.CL"}
{"created":"2024-04-08 16:39:34","title":"Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid Framework","abstract":"Industry-wide nuclear power plant operating experience is a critical source of raw data for performing parameter estimations in reliability and risk models. Much operating experience information pertains to failure events and is stored as reports containing unstructured data, such as narratives. Event reports are essential for understanding how failures are initiated and propagated, including the numerous causal relations involved. Causal relation extraction using deep learning represents a significant frontier in the field of natural language processing (NLP), and is crucial since it enables the interpretation of intricate narratives and connections contained within vast amounts of written information. This paper proposed a hybrid framework for causality detection and extraction from nuclear licensee event reports. The main contributions include: (1) we compiled an LER corpus with 20,129 text samples for causality analysis, (2) developed an interactive tool for labeling cause effect pairs, (3) built a deep-learning-based approach for causal relation detection, and (4) developed a knowledge based cause-effect extraction approach.","sentences":["Industry-wide nuclear power plant operating experience is a critical source of raw data for performing parameter estimations in reliability and risk models.","Much operating experience information pertains to failure events and is stored as reports containing unstructured data, such as narratives.","Event reports are essential for understanding how failures are initiated and propagated, including the numerous causal relations involved.","Causal relation extraction using deep learning represents a significant frontier in the field of natural language processing (NLP), and is crucial since it enables the interpretation of intricate narratives and connections contained within vast amounts of written information.","This paper proposed a hybrid framework for causality detection and extraction from nuclear licensee event reports.","The main contributions include: (1) we compiled an LER corpus with 20,129 text samples for causality analysis, (2) developed an interactive tool for labeling cause effect pairs, (3) built a deep-learning-based approach for causal relation detection, and (4) developed a knowledge based cause-effect extraction approach."],"url":"http://arxiv.org/abs/2404.05656v1","category":"cs.CL"}
{"created":"2024-04-08 16:38:12","title":"The impact of Population III stars on the astrophysical gravitational-wave background","abstract":"We probe the astrophysical gravitational-wave background resulting from compact binary coalescences, focusing on Population III binary black holes. We exploit results of state-of-the-art simulations on the evolution of Population I-II and III binaries, considering a variety of initial condition and star formation rate models for the latter. The contribution from Population III binary black holes is found to be very small, with no effect on the gravitational-wave spectrum. A network of third-generation detectors will detect easier individual Population III binaries, due to their significantly higher masses, hence decreasing even further their residual contribution.","sentences":["We probe the astrophysical gravitational-wave background resulting from compact binary coalescences, focusing on Population III binary black holes.","We exploit results of state-of-the-art simulations on the evolution of Population I-II and III binaries, considering a variety of initial condition and star formation rate models for the latter.","The contribution from Population III binary black holes is found to be very small, with no effect on the gravitational-wave spectrum.","A network of third-generation detectors will detect easier individual Population III binaries, due to their significantly higher masses, hence decreasing even further their residual contribution."],"url":"http://arxiv.org/abs/2404.05653v1","category":"astro-ph.CO"}
{"created":"2024-04-08 16:34:35","title":"Resistive Memory-based Neural Differential Equation Solver for Score-based Diffusion Model","abstract":"Human brains image complicated scenes when reading a novel. Replicating this imagination is one of the ultimate goals of AI-Generated Content (AIGC). However, current AIGC methods, such as score-based diffusion, are still deficient in terms of rapidity and efficiency. This deficiency is rooted in the difference between the brain and digital computers. Digital computers have physically separated storage and processing units, resulting in frequent data transfers during iterative calculations, incurring large time and energy overheads. This issue is further intensified by the conversion of inherently continuous and analog generation dynamics, which can be formulated by neural differential equations, into discrete and digital operations. Inspired by the brain, we propose a time-continuous and analog in-memory neural differential equation solver for score-based diffusion, employing emerging resistive memory. The integration of storage and computation within resistive memory synapses surmount the von Neumann bottleneck, benefiting the generative speed and energy efficiency. The closed-loop feedback integrator is time-continuous, analog, and compact, physically implementing an infinite-depth neural network. Moreover, the software-hardware co-design is intrinsically robust to analog noise. We experimentally validate our solution with 180 nm resistive memory in-memory computing macros. Demonstrating equivalent generative quality to the software baseline, our system achieved remarkable enhancements in generative speed for both unconditional and conditional generation tasks, by factors of 64.8 and 156.5, respectively. Moreover, it accomplished reductions in energy consumption by factors of 5.2 and 4.1. Our approach heralds a new horizon for hardware solutions in edge computing for generative AI applications.","sentences":["Human brains image complicated scenes when reading a novel.","Replicating this imagination is one of the ultimate goals of AI-Generated Content (AIGC).","However, current AIGC methods, such as score-based diffusion, are still deficient in terms of rapidity and efficiency.","This deficiency is rooted in the difference between the brain and digital computers.","Digital computers have physically separated storage and processing units, resulting in frequent data transfers during iterative calculations, incurring large time and energy overheads.","This issue is further intensified by the conversion of inherently continuous and analog generation dynamics, which can be formulated by neural differential equations, into discrete and digital operations.","Inspired by the brain, we propose a time-continuous and analog in-memory neural differential equation solver for score-based diffusion, employing emerging resistive memory.","The integration of storage and computation within resistive memory synapses surmount the von Neumann bottleneck, benefiting the generative speed and energy efficiency.","The closed-loop feedback integrator is time-continuous, analog, and compact, physically implementing an infinite-depth neural network.","Moreover, the software-hardware co-design is intrinsically robust to analog noise.","We experimentally validate our solution with 180 nm resistive memory in-memory computing macros.","Demonstrating equivalent generative quality to the software baseline, our system achieved remarkable enhancements in generative speed for both unconditional and conditional generation tasks, by factors of 64.8 and 156.5, respectively.","Moreover, it accomplished reductions in energy consumption by factors of 5.2 and 4.1.","Our approach heralds a new horizon for hardware solutions in edge computing for generative AI applications."],"url":"http://arxiv.org/abs/2404.05648v1","category":"cs.AR"}
{"created":"2024-04-08 16:31:54","title":"Linear and Nonlinear Coupling of Twin-Resonators with Kerr Nonlinearity","abstract":"Nonlinear effects in microresonators are efficient building blocks for all-optical computing and telecom systems. With the latest advances in microfabrication, coupled microresonators are used in a rapidly growing number of applications. In this work, we investigate the coupling between twin-resonators in the presence of Kerr-nonlinearity. We use an experimental setup with controllable coupling between two high-Q resonators and discuss the effects caused by the simultaneous presence of linear and non-linear coupling between the optical fields. Linear-coupling-induced mode splitting is observed at low input powers, with the controllable coupling leading to a tunable mode splitting. At high input powers, the hybridized resonances show spontaneous symmetry breaking (SSB) effects, in which the optical power is unevenly distributed between the resonators. Our experimental results are supported by a detailed theoretical model of nonlinear twin-resonators. With the recent interest in coupled resonator systems for neuromorphic computing, quantum systems, and optical frequency comb generation, our work provides important insights into the behavior of these systems at high circulating powers.","sentences":["Nonlinear effects in microresonators are efficient building blocks for all-optical computing and telecom systems.","With the latest advances in microfabrication, coupled microresonators are used in a rapidly growing number of applications.","In this work, we investigate the coupling between twin-resonators in the presence of Kerr-nonlinearity.","We use an experimental setup with controllable coupling between two high-Q resonators and discuss the effects caused by the simultaneous presence of linear and non-linear coupling between the optical fields.","Linear-coupling-induced mode splitting is observed at low input powers, with the controllable coupling leading to a tunable mode splitting.","At high input powers, the hybridized resonances show spontaneous symmetry breaking (SSB) effects, in which the optical power is unevenly distributed between the resonators.","Our experimental results are supported by a detailed theoretical model of nonlinear twin-resonators.","With the recent interest in coupled resonator systems for neuromorphic computing, quantum systems, and optical frequency comb generation, our work provides important insights into the behavior of these systems at high circulating powers."],"url":"http://arxiv.org/abs/2404.05646v1","category":"physics.optics"}
{"created":"2024-04-08 16:25:45","title":"Existence and uniqueness of a saddle-node bifurcation point for nonlinear equations in general domains","abstract":"This paper provides a direct method of establishing the existence and uniqueness of saddle-node bifurcations for nonlinear equations in general domains. The method employs the scaled extended quotient whose saddle points correspond to the saddle-node bifurcations. The uniqueness of the saddle-node bifurcation point directly stems from the uniqueness of the saddle point. The method is applied to solving open problems involving the existence and uniqueness of the maximum saddle-node bifurcation for the positive solutions curve to an elliptic boundary value problem with a convex-concave nonlinearity in general domains.","sentences":["This paper provides a direct method of establishing the existence and uniqueness of saddle-node bifurcations for nonlinear equations in general domains.","The method employs the scaled extended quotient whose saddle points correspond to the saddle-node bifurcations.","The uniqueness of the saddle-node bifurcation point directly stems from the uniqueness of the saddle point.","The method is applied to solving open problems involving the existence and uniqueness of the maximum saddle-node bifurcation for the positive solutions curve to an elliptic boundary value problem with a convex-concave nonlinearity in general domains."],"url":"http://arxiv.org/abs/2404.05643v1","category":"math.AP"}
{"created":"2024-04-08 16:20:15","title":"Investigating the Impact of Quantization on Adversarial Robustness","abstract":"Quantization is a promising technique for reducing the bit-width of deep models to improve their runtime performance and storage efficiency, and thus becomes a fundamental step for deployment. In real-world scenarios, quantized models are often faced with adversarial attacks which cause the model to make incorrect inferences by introducing slight perturbations. However, recent studies have paid less attention to the impact of quantization on the model robustness. More surprisingly, existing studies on this topic even present inconsistent conclusions, which prompted our in-depth investigation. In this paper, we conduct a first-time analysis of the impact of the quantization pipeline components that can incorporate robust optimization under the settings of Post-Training Quantization and Quantization-Aware Training. Through our detailed analysis, we discovered that this inconsistency arises from the use of different pipelines in different studies, specifically regarding whether robust optimization is performed and at which quantization stage it occurs. Our research findings contribute insights into deploying more secure and robust quantized networks, assisting practitioners in reference for scenarios with high-security requirements and limited resources.","sentences":["Quantization is a promising technique for reducing the bit-width of deep models to improve their runtime performance and storage efficiency, and thus becomes a fundamental step for deployment.","In real-world scenarios, quantized models are often faced with adversarial attacks which cause the model to make incorrect inferences by introducing slight perturbations.","However, recent studies have paid less attention to the impact of quantization on the model robustness.","More surprisingly, existing studies on this topic even present inconsistent conclusions, which prompted our in-depth investigation.","In this paper, we conduct a first-time analysis of the impact of the quantization pipeline components that can incorporate robust optimization under the settings of Post-Training Quantization and Quantization-Aware Training.","Through our detailed analysis, we discovered that this inconsistency arises from the use of different pipelines in different studies, specifically regarding whether robust optimization is performed and at which quantization stage it occurs.","Our research findings contribute insights into deploying more secure and robust quantized networks, assisting practitioners in reference for scenarios with high-security requirements and limited resources."],"url":"http://arxiv.org/abs/2404.05639v1","category":"cs.LG"}
{"created":"2024-04-08 16:19:01","title":"Holographic supersymmetric Renyi entropies from hyperbolic black holes with scalar hair","abstract":"We study holographic supersymmetric Renyi entropies from a family of hyperbolic black holes in an Einstein-Maxwell-dilaton (EMD) system under the BPS condition. We calculate the thermodynamic quantities of these hyperbolic black holes. We find a remarkably simple formula of the supersymmetric Renyi entropy that unifies (interpolates) 11 cases embeddable to 10 or 11 dimensional supergravity. It reproduces many known results in the literature, and gives new results with distinctive features. We show that the supersymmetric version of the modular entropy and the capacity of entanglement cannot be mapped to thermal quantities, due to the dependence of the temperature and the chemical potential by the BPS condition. We also calculate the entanglement spectrum. We derive the potential of the EMD system from a $V=0$ solution and obtain two neutral solutions with scalar hair as a byproduct.","sentences":["We study holographic supersymmetric Renyi entropies from a family of hyperbolic black holes in an Einstein-Maxwell-dilaton (EMD) system under the BPS condition.","We calculate the thermodynamic quantities of these hyperbolic black holes.","We find a remarkably simple formula of the supersymmetric Renyi entropy that unifies (interpolates) 11 cases embeddable to 10 or 11 dimensional supergravity.","It reproduces many known results in the literature, and gives new results with distinctive features.","We show that the supersymmetric version of the modular entropy and the capacity of entanglement cannot be mapped to thermal quantities, due to the dependence of the temperature and the chemical potential by the BPS condition.","We also calculate the entanglement spectrum.","We derive the potential of the EMD system from a $V=0$ solution and obtain two neutral solutions with scalar hair as a byproduct."],"url":"http://arxiv.org/abs/2404.05638v1","category":"hep-th"}
{"created":"2024-04-08 16:16:58","title":"First-order phase transitions in the cores of neutron stars","abstract":"I explore various scenarios for the phase transition within neutron-star matter. I do so by generating large model-agnostic ensemble using Gaussian Processes, both with and without explicit inclusion of first-order phase transitions (PTs). The ensemble is conditioned with state-of-the-art astrophysical and theoretical inputs in a fully Bayesian approach. I study how the current data affect the posterior probability of the location and the strength of the first-order PT. I find that peak structure of the sound speed is stable against inclusion of PTs. Furthermore, while the current data cannot differentiate between a crossover and a first-order PT, it suggests an exceedingly low probability of the absence of either within the stable branch of neutron stars.","sentences":["I explore various scenarios for the phase transition within neutron-star matter.","I do so by generating large model-agnostic ensemble using Gaussian Processes, both with and without explicit inclusion of first-order phase transitions (PTs).","The ensemble is conditioned with state-of-the-art astrophysical and theoretical inputs in a fully Bayesian approach.","I study how the current data affect the posterior probability of the location and the strength of the first-order PT.","I find that peak structure of the sound speed is stable against inclusion of PTs.","Furthermore, while the current data cannot differentiate between a crossover and a first-order PT, it suggests an exceedingly low probability of the absence of either within the stable branch of neutron stars."],"url":"http://arxiv.org/abs/2404.05637v1","category":"nucl-th"}
{"created":"2024-04-08 16:11:15","title":"Semi-Infinite Programs for Robust Control and Optimization: Efficient Solutions and Extensions to Existence Constraints","abstract":"Discrete-time robust optimal control problems generally take a min-max structure over continuous variable spaces, which can be difficult to solve in practice. In this paper, we extend the class of such problems that can be solved through a previously proposed local reduction method to consider those with existence constraints on the uncountable variables. We also consider the possibility of non-unique trajectories that satisfy equality and inequality constraints. Crucially, we show that the problems of interest can be cast into a standard semi-infinite program and demonstrate how to generate optimal uncertainty scenario sets in order to obtain numerical solutions. We also include examples on model predictive control for obstacle avoidance with logical conditions, control with input saturation affected by uncertainty, and optimal parameter estimation to highlight the need for the proposed extension. Our method solves each of the examples considered, producing violation-free and locally optimal solutions.","sentences":["Discrete-time robust optimal control problems generally take a min-max structure over continuous variable spaces, which can be difficult to solve in practice.","In this paper, we extend the class of such problems that can be solved through a previously proposed local reduction method to consider those with existence constraints on the uncountable variables.","We also consider the possibility of non-unique trajectories that satisfy equality and inequality constraints.","Crucially, we show that the problems of interest can be cast into a standard semi-infinite program and demonstrate how to generate optimal uncertainty scenario sets in order to obtain numerical solutions.","We also include examples on model predictive control for obstacle avoidance with logical conditions, control with input saturation affected by uncertainty, and optimal parameter estimation to highlight the need for the proposed extension.","Our method solves each of the examples considered, producing violation-free and locally optimal solutions."],"url":"http://arxiv.org/abs/2404.05635v1","category":"math.OC"}
{"created":"2024-04-08 16:04:26","title":"Fighting crime with Transformers: Empirical analysis of address parsing methods in payment data","abstract":"In the financial industry, identifying the location of parties involved in payments is a major challenge in the context of various regulatory requirements. For this purpose address parsing entails extracting fields such as street, postal code, or country from free text message attributes. While payment processing platforms are updating their standards with more structured formats such as SWIFT with ISO 20022, address parsing remains essential for a considerable volume of messages. With the emergence of Transformers and Generative Large Language Models (LLM), we explore the performance of state-of-the-art solutions given the constraint of processing a vast amount of daily data. This paper also aims to show the need for training robust models capable of dealing with real-world noisy transactional data. Our results suggest that a well fine-tuned Transformer model using early-stopping significantly outperforms other approaches. Nevertheless, generative LLMs demonstrate strong zero-shot performance and warrant further investigations.","sentences":["In the financial industry, identifying the location of parties involved in payments is a major challenge in the context of various regulatory requirements.","For this purpose address parsing entails extracting fields such as street, postal code, or country from free text message attributes.","While payment processing platforms are updating their standards with more structured formats such as SWIFT with ISO 20022, address parsing remains essential for a considerable volume of messages.","With the emergence of Transformers and Generative Large Language Models (LLM), we explore the performance of state-of-the-art solutions given the constraint of processing a vast amount of daily data.","This paper also aims to show the need for training robust models capable of dealing with real-world noisy transactional data.","Our results suggest that a well fine-tuned Transformer model using early-stopping significantly outperforms other approaches.","Nevertheless, generative LLMs demonstrate strong zero-shot performance and warrant further investigations."],"url":"http://arxiv.org/abs/2404.05632v1","category":"cs.CL"}
{"created":"2024-04-08 16:02:10","title":"Multi Digit Ising Mapping for Low Precision Ising Solvers","abstract":"The last couple of years have seen an ever-increasing interest in using different Ising solvers, like Quantum annealers, Coherent Ising machines, and Oscillator-based Ising machines, for solving tough computational problems in various domains. Although the simulations predict massive performance improvements for several tough computational problems, the real implementations of the Ising solvers tend to have limited precision, which can cause significant performance deterioration. This paper presents a novel methodology for mapping the problem on the Ising solvers to artificially increase the effective precision. We further evaluate our method for the Multiple-Input-Multiple-Output signal detection problem.","sentences":["The last couple of years have seen an ever-increasing interest in using different Ising solvers, like Quantum annealers, Coherent Ising machines, and Oscillator-based Ising machines, for solving tough computational problems in various domains.","Although the simulations predict massive performance improvements for several tough computational problems, the real implementations of the Ising solvers tend to have limited precision, which can cause significant performance deterioration.","This paper presents a novel methodology for mapping the problem on the Ising solvers to artificially increase the effective precision.","We further evaluate our method for the Multiple-Input-Multiple-Output signal detection problem."],"url":"http://arxiv.org/abs/2404.05631v1","category":"cs.ET"}
{"created":"2024-04-08 15:59:29","title":"Learning a Category-level Object Pose Estimator without Pose Annotations","abstract":"3D object pose estimation is a challenging task. Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling. In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations. Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images. Directly using the original diffusion model leads to images with noisy poses and artifacts. To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps. Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses. Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks.","sentences":["3D object pose estimation is a challenging task.","Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling.","In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations.","Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images.","Directly using the original diffusion model leads to images with noisy poses and artifacts.","To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps.","Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses.","Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks."],"url":"http://arxiv.org/abs/2404.05626v1","category":"cs.CV"}
{"created":"2024-04-08 15:54:02","title":"LTNER: Large Language Model Tagging for Named Entity Recognition with Contextualized Entity Marking","abstract":"The use of LLMs for natural language processing has become a popular trend in the past two years, driven by their formidable capacity for context comprehension and learning, which has inspired a wave of research from academics and industry professionals. However, for certain NLP tasks, such as NER, the performance of LLMs still falls short when compared to supervised learning methods. In our research, we developed a NER processing framework called LTNER that incorporates a revolutionary Contextualized Entity Marking Gen Method. By leveraging the cost-effective GPT-3.5 coupled with context learning that does not require additional training, we significantly improved the accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset increased from the initial 85.9% to 91.9%, approaching the performance of supervised fine-tuning. This outcome has led to a deeper understanding of the potential of LLMs.","sentences":["The use of LLMs for natural language processing has become a popular trend in the past two years, driven by their formidable capacity for context comprehension and learning, which has inspired a wave of research from academics and industry professionals.","However, for certain NLP tasks, such as NER, the performance of LLMs still falls short when compared to supervised learning methods.","In our research, we developed a NER processing framework called LTNER that incorporates a revolutionary Contextualized Entity Marking Gen Method.","By leveraging the cost-effective GPT-3.5 coupled with context learning that does not require additional training, we significantly improved the accuracy of LLMs in handling NER tasks.","The F1 score on the CoNLL03 dataset increased from the initial 85.9% to 91.9%, approaching the performance of supervised fine-tuning.","This outcome has led to a deeper understanding of the potential of LLMs."],"url":"http://arxiv.org/abs/2404.05624v1","category":"cs.CL"}
{"created":"2024-04-08 15:47:28","title":"Experimental observation of a time rondeau crystal: Temporal Disorder in Spatiotemporal Order","abstract":"Our understanding of phases of matter relies on symmetry breaking, one example being water ice whose crystalline structure breaks the continuous translation symmetry of space. Recently, breaking of time translation symmetry was observed in systems not in thermal equilibrium. The associated notion of time crystallinity has led to a surge of interest, raising the question about the extent to which highly controllable quantum simulators can generate rich and tunable temporal orders, beyond the conventional classification of order in static systems. Here, we investigate different kinds of partial temporal orders, stabilized by non-periodic yet structured drives, which we call rondeau order. Using a $^{13}$C-nuclear-spin diamond quantum simulator, we report the first experimental observation of a -- tunable degree of -- short-time disorder in a system exhibiting long-time stroboscopic order. This is based on a novel spin control architecture that allows us to implement a family of drives ranging from structureless via structured random to quasiperiodic and periodic drives. Leveraging a high throughput read-out scheme, we continuously observe the spin polarization over 105 pulses to probe rondeau order, with controllable lifetimes exceeding 4 seconds. Using the freedom in the short-time temporal disorder of rondeau order, we show the capacity to encode information in the response of observables. Our work broadens the landscape of observed nonequilibrium temporal order, paving the way for new applications harnessing driven quantum matter.","sentences":["Our understanding of phases of matter relies on symmetry breaking, one example being water ice whose crystalline structure breaks the continuous translation symmetry of space.","Recently, breaking of time translation symmetry was observed in systems not in thermal equilibrium.","The associated notion of time crystallinity has led to a surge of interest, raising the question about the extent to which highly controllable quantum simulators can generate rich and tunable temporal orders, beyond the conventional classification of order in static systems.","Here, we investigate different kinds of partial temporal orders, stabilized by non-periodic yet structured drives, which we call rondeau order.","Using a $^{13}$C-nuclear-spin diamond quantum simulator, we report the first experimental observation of a -- tunable degree of -- short-time disorder in a system exhibiting long-time stroboscopic order.","This is based on a novel spin control architecture that allows us to implement a family of drives ranging from structureless via structured random to quasiperiodic and periodic drives.","Leveraging a high throughput read-out scheme, we continuously observe the spin polarization over 105 pulses to probe rondeau order, with controllable lifetimes exceeding 4 seconds.","Using the freedom in the short-time temporal disorder of rondeau order, we show the capacity to encode information in the response of observables.","Our work broadens the landscape of observed nonequilibrium temporal order, paving the way for new applications harnessing driven quantum matter."],"url":"http://arxiv.org/abs/2404.05620v1","category":"quant-ph"}
{"created":"2024-04-08 15:40:22","title":"Deep Representation Learning for Multi-functional Degradation Modeling of Community-dwelling Aging Population","abstract":"As the aging population grows, particularly for the baby boomer generation, the United States is witnessing a significant increase in the elderly population experiencing multifunctional disabilities. These disabilities, stemming from a variety of chronic diseases, injuries, and impairments, present a complex challenge due to their multidimensional nature, encompassing both physical and cognitive aspects. Traditional methods often use univariate regression-based methods to model and predict single degradation conditions and assume population homogeneity, which is inadequate to address the complexity and diversity of aging-related degradation. This study introduces a novel framework for multi-functional degradation modeling that captures the multidimensional (e.g., physical and cognitive) and heterogeneous nature of elderly disabilities. Utilizing deep learning, our approach predicts health degradation scores and uncovers latent heterogeneity from elderly health histories, offering both efficient estimation and explainable insights into the diverse effects and causes of aging-related degradation. A real-case study demonstrates the effectiveness and marks a pivotal contribution to accurately modeling the intricate dynamics of elderly degradation, and addresses the healthcare challenges in the aging population.","sentences":["As the aging population grows, particularly for the baby boomer generation, the United States is witnessing a significant increase in the elderly population experiencing multifunctional disabilities.","These disabilities, stemming from a variety of chronic diseases, injuries, and impairments, present a complex challenge due to their multidimensional nature, encompassing both physical and cognitive aspects.","Traditional methods often use univariate regression-based methods to model and predict single degradation conditions and assume population homogeneity, which is inadequate to address the complexity and diversity of aging-related degradation.","This study introduces a novel framework for multi-functional degradation modeling that captures the multidimensional (e.g., physical and cognitive) and heterogeneous nature of elderly disabilities.","Utilizing deep learning, our approach predicts health degradation scores and uncovers latent heterogeneity from elderly health histories, offering both efficient estimation and explainable insights into the diverse effects and causes of aging-related degradation.","A real-case study demonstrates the effectiveness and marks a pivotal contribution to accurately modeling the intricate dynamics of elderly degradation, and addresses the healthcare challenges in the aging population."],"url":"http://arxiv.org/abs/2404.05613v1","category":"cs.LG"}
{"created":"2024-04-08 15:35:58","title":"On global solutions of heat equations with time-dependent nonlinearities on unimodular Lie groups","abstract":"In this work, we study the global well-posedeness of the heat equation with variable time-dependent nonlinearity of the form $\\varphi(t)f(u)$ on unimodular Lie groups when the differential operator arises as the sum of squares of H\\\"ormander vector fields. For general unimodular Lie groups, we derive the necessary conditions for the nonexistence of global positive solutions. This gives different conditions in the cases of compact, polynomial, and exponential volume growth groups. In the case of the Heisenberg groups $\\mathbb{H}^{n}$, we also derive sufficient conditions, which coincide with the necessary ones in the case of $\\mathbb{H}^{1}$ (and this is also true for $\\mathbb{R}^{n}$). In particular, in the case of the Heisenberg group $\\mathbb{H}^{1}$ we obtain the necessary and sufficient conditions under which the aforesaid initial value problem with variable nonlinearity has a global positive solution.","sentences":["In this work, we study the global well-posedeness of the heat equation with variable time-dependent nonlinearity of the form $\\varphi(t)f(u)$ on unimodular Lie groups when the differential operator arises as the sum of squares of H\\\"ormander vector fields.","For general unimodular Lie groups, we derive the necessary conditions for the nonexistence of global positive solutions.","This gives different conditions in the cases of compact, polynomial, and exponential volume growth groups.","In the case of the Heisenberg groups $\\mathbb{H}^{n}$, we also derive sufficient conditions, which coincide with the necessary ones in the case of $\\mathbb{H}^{1}$ (and this is also true for $\\mathbb{R}^{n}$).","In particular, in the case of the Heisenberg group $\\mathbb{H}^{1}$ we obtain the necessary and sufficient conditions under which the aforesaid initial value problem with variable nonlinearity has a global positive solution."],"url":"http://arxiv.org/abs/2404.05611v1","category":"math.AP"}
{"created":"2024-04-08 15:35:03","title":"KaMPIng: Flexible and (Near) Zero-overhead C++ Bindings for MPI","abstract":"The Message-Passing Interface (MPI) and C++ form the backbone of high-performance computing, but MPI only provides C and Fortran bindings. While this offers great language interoperability, high-level programming languages like C++ make software development quicker and less error-prone.   We propose novel C++ language bindings that cover all abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to C++. This enables rapid prototyping and fine-tuning runtime behavior and memory management. A flexible type system and additional safeness guarantees help to prevent programming errors.   By exploiting C++'s template-metaprogramming capabilities, this has (near) zero-overhead, as only required code paths are generated at compile time.   We demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference.","sentences":["The Message-Passing Interface (MPI) and C++ form the backbone of high-performance computing, but MPI only provides C and Fortran bindings.","While this offers great language interoperability, high-level programming languages like C++ make software development quicker and less error-prone.   ","We propose novel","C++ language bindings that cover all abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to C++.","This enables rapid prototyping and fine-tuning runtime behavior and memory management.","A flexible type system and additional safeness guarantees help to prevent programming errors.   ","By exploiting C++'s template-metaprogramming capabilities, this has (near) zero-overhead, as only required code paths are generated at compile time.   ","We demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference."],"url":"http://arxiv.org/abs/2404.05610v1","category":"cs.DC"}
{"created":"2024-04-08 15:33:12","title":"Thermal Structure Determines Kinematics: Vertical Shear Instability in Stellar Irradiated Protoplanetary Disks","abstract":"Turbulence is crucial for protoplanetary disk dynamics, and Vertical Shear Instability (VSI) is a promising mechanism in outer disk regions to generate turbulence. We use Athena++ radiation module to study VSI in full and transition disks, accounting for radiation transport and stellar irradiation. We find that the thermal structure and cooling timescale significantly influence VSI behavior. The inner rim location and radial optical depth affect disk kinematics. Compared with previous vertically-isothermal simulations, our full disk and transition disks with small cavities have a superheated atmosphere and cool midplane with long cooling timescales, which suppresses the corrugation mode and the associated meridional circulation. This temperature structure also produces a strong vertical shear at $\\mathrm{\\tau_*}$ = 1, producing an outgoing flow layer at $\\tau_* < 1$ on top of an ingoing flow layer at $\\tau_* \\sim 1$. The midplane becomes less turbulent, while the surface becomes more turbulent with effective $\\alpha$ reaching $\\sim10^{-2}$ at $\\tau_* \\lesssim$1. This large surface stress drives significant surface accretion, producing substructures. Using temperature and cooling time measured/estimated from radiation-hydro simulations, we demonstrate that less computationally-intensive simulations incorporating simple orbital cooling can almost reproduce radiation-hydro results. By generating synthetic images, we find that substructures are more pronounced in disks with larger cavities. The higher velocity dispersion at the gap edge could also slow particle settling. Both properties are consistent with recent Near-IR and ALMA observations. Our simulations predict that regions with significant temperature changes are accompanied by significant velocity changes, which can be tested by ALMA kinematics/chemistry observations.","sentences":["Turbulence is crucial for protoplanetary disk dynamics, and Vertical Shear Instability (VSI) is a promising mechanism in outer disk regions to generate turbulence.","We use Athena++ radiation module to study VSI in full and transition disks, accounting for radiation transport and stellar irradiation.","We find that the thermal structure and cooling timescale significantly influence VSI behavior.","The inner rim location and radial optical depth affect disk kinematics.","Compared with previous vertically-isothermal simulations, our full disk and transition disks with small cavities have a superheated atmosphere and cool midplane with long cooling timescales, which suppresses the corrugation mode and the associated meridional circulation.","This temperature structure also produces a strong vertical shear at $\\mathrm{\\tau_*}$ = 1, producing an outgoing flow layer at $\\tau_*","< 1$ on top of an ingoing flow layer at $\\tau_*","\\sim 1$.","The midplane becomes less turbulent, while the surface becomes more turbulent with effective $\\alpha$ reaching $\\sim10^{-2}$ at $\\tau_* \\lesssim$1.","This large surface stress drives significant surface accretion, producing substructures.","Using temperature and cooling time measured/estimated from radiation-hydro simulations, we demonstrate that less computationally-intensive simulations incorporating simple orbital cooling can almost reproduce radiation-hydro results.","By generating synthetic images, we find that substructures are more pronounced in disks with larger cavities.","The higher velocity dispersion at the gap edge could also slow particle settling.","Both properties are consistent with recent Near-IR and ALMA observations.","Our simulations predict that regions with significant temperature changes are accompanied by significant velocity changes, which can be tested by ALMA kinematics/chemistry observations."],"url":"http://arxiv.org/abs/2404.05608v1","category":"astro-ph.EP"}
{"created":"2024-04-08 15:29:46","title":"A Training-Free Plug-and-Play Watermark Framework for Stable Diffusion","abstract":"Nowadays, the family of Stable Diffusion (SD) models has gained prominence for its high quality outputs and scalability. This has also raised security concerns on social media, as malicious users can create and disseminate harmful content. Existing approaches involve training components or entire SDs to embed a watermark in generated images for traceability and responsibility attribution. However, in the era of AI-generated content (AIGC), the rapid iteration of SDs renders retraining with watermark models costly. To address this, we propose a training-free plug-and-play watermark framework for SDs. Without modifying any components of SDs, we embed diverse watermarks in the latent space, adapting to the denoising process. Our experimental findings reveal that our method effectively harmonizes image quality and watermark invisibility. Furthermore, it performs robustly under various attacks. We also have validated that our method is generalized to multiple versions of SDs, even without retraining the watermark model.","sentences":["Nowadays, the family of Stable Diffusion (SD) models has gained prominence for its high quality outputs and scalability.","This has also raised security concerns on social media, as malicious users can create and disseminate harmful content.","Existing approaches involve training components or entire SDs to embed a watermark in generated images for traceability and responsibility attribution.","However, in the era of AI-generated content (AIGC), the rapid iteration of SDs renders retraining with watermark models costly.","To address this, we propose a training-free plug-and-play watermark framework for SDs.","Without modifying any components of SDs, we embed diverse watermarks in the latent space, adapting to the denoising process.","Our experimental findings reveal that our method effectively harmonizes image quality and watermark invisibility.","Furthermore, it performs robustly under various attacks.","We also have validated that our method is generalized to multiple versions of SDs, even without retraining the watermark model."],"url":"http://arxiv.org/abs/2404.05607v1","category":"cs.CV"}
{"created":"2024-04-08 15:25:25","title":"Graph Neural Networks Automated Design and Deployment on Device-Edge Co-Inference Systems","abstract":"The key to device-edge co-inference paradigm is to partition models into computation-friendly and computation-intensive parts across the device and the edge, respectively. However, for Graph Neural Networks (GNNs), we find that simply partitioning without altering their structures can hardly achieve the full potential of the co-inference paradigm due to various computational-communication overheads of GNN operations over heterogeneous devices. We present GCoDE, the first automatic framework for GNN that innovatively Co-designs the architecture search and the mapping of each operation on Device-Edge hierarchies. GCoDE abstracts the device communication process into an explicit operation and fuses the search of architecture and the operations mapping in a unified space for joint-optimization. Also, the performance-awareness approach, utilized in the constraint-based search process of GCoDE, enables effective evaluation of architecture efficiency in diverse heterogeneous systems. We implement the co-inference engine and runtime dispatcher in GCoDE to enhance the deployment efficiency. Experimental results show that GCoDE can achieve up to $44.9\\times$ speedup and $98.2\\%$ energy reduction compared to existing approaches across various applications and system configurations.","sentences":["The key to device-edge co-inference paradigm is to partition models into computation-friendly and computation-intensive parts across the device and the edge, respectively.","However, for Graph Neural Networks (GNNs), we find that simply partitioning without altering their structures can hardly achieve the full potential of the co-inference paradigm due to various computational-communication overheads of GNN operations over heterogeneous devices.","We present GCoDE, the first automatic framework for GNN that innovatively Co-designs the architecture search and the mapping of each operation on Device-Edge hierarchies.","GCoDE abstracts the device communication process into an explicit operation and fuses the search of architecture and the operations mapping in a unified space for joint-optimization.","Also, the performance-awareness approach, utilized in the constraint-based search process of GCoDE, enables effective evaluation of architecture efficiency in diverse heterogeneous systems.","We implement the co-inference engine and runtime dispatcher in GCoDE to enhance the deployment efficiency.","Experimental results show that GCoDE can achieve up to $44.9\\times$ speedup and $98.2\\%$ energy reduction compared to existing approaches across various applications and system configurations."],"url":"http://arxiv.org/abs/2404.05605v1","category":"cs.LG"}
{"created":"2024-04-08 15:22:38","title":"Self-Explainable Affordance Learning with Embodied Caption","abstract":"In the field of visual affordance learning, previous methods mainly used abundant images or videos that delineate human behavior patterns to identify action possibility regions for object manipulation, with a variety of applications in robotic tasks. However, they encounter a main challenge of action ambiguity, illustrated by the vagueness like whether to beat or carry a drum, and the complexities involved in processing intricate scenes. Moreover, it is important for human intervention to rectify robot errors in time. To address these issues, we introduce Self-Explainable Affordance learning (SEA) with embodied caption. This innovation enables robots to articulate their intentions and bridge the gap between explainable vision-language caption and visual affordance learning. Due to a lack of appropriate dataset, we unveil a pioneering dataset and metrics tailored for this task, which integrates images, heatmaps, and embodied captions. Furthermore, we propose a novel model to effectively combine affordance grounding with self-explanation in a simple but efficient manner. Extensive quantitative and qualitative experiments demonstrate our method's effectiveness.","sentences":["In the field of visual affordance learning, previous methods mainly used abundant images or videos that delineate human behavior patterns to identify action possibility regions for object manipulation, with a variety of applications in robotic tasks.","However, they encounter a main challenge of action ambiguity, illustrated by the vagueness like whether to beat or carry a drum, and the complexities involved in processing intricate scenes.","Moreover, it is important for human intervention to rectify robot errors in time.","To address these issues, we introduce Self-Explainable Affordance learning (SEA) with embodied caption.","This innovation enables robots to articulate their intentions and bridge the gap between explainable vision-language caption and visual affordance learning.","Due to a lack of appropriate dataset, we unveil a pioneering dataset and metrics tailored for this task, which integrates images, heatmaps, and embodied captions.","Furthermore, we propose a novel model to effectively combine affordance grounding with self-explanation in a simple but efficient manner.","Extensive quantitative and qualitative experiments demonstrate our method's effectiveness."],"url":"http://arxiv.org/abs/2404.05603v1","category":"cs.CV"}
{"created":"2024-04-08 15:21:17","title":"SpeechAlign: Aligning Speech Generation to Human Preferences","abstract":"Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.","sentences":["Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out.","However, the integration of human feedback to align speech outputs to human preferences is often neglected.","This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance.","Then we explore leveraging learning from human feedback to bridge the distribution gap.","We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences.","SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model.","This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones.","Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model.","Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models.","Code and models will be available at https://github.com/0nutation/SpeechGPT."],"url":"http://arxiv.org/abs/2404.05600v1","category":"cs.CL"}
{"created":"2024-04-08 15:20:14","title":"The Argument for Meta-Modeling-Based Approaches to Hardware Generation Languages","abstract":"The rapid evolution of Integrated Circuit (IC) development necessitates innovative methodologies such as code generation to manage complexity and increase productivity. Using the right methodology for generator development to maximize the capability and, most notably, the feasibility of generators is a crucial part of this work. Meta-Modeling-based approaches drawing on the principles of Model Driven Architecture (MDA) are a promising methodology for generator development. The goal of this paper is to show why such an MDA-based approach can provide extremely powerful generators with minimal implementation effort and to demonstrate that this approach is a superior alternative to the most advanced hardware generation languages such as SpinalHDL and Chisel. For this purpose, this paper provides an in-depth comparison of the Meta-Modeling approach against these hardware generation languages, highlighting the unique advantages of a Meta-Modeling-based approach and summarizes the benefits.","sentences":["The rapid evolution of Integrated Circuit (IC) development necessitates innovative methodologies such as code generation to manage complexity and increase productivity.","Using the right methodology for generator development to maximize the capability and, most notably, the feasibility of generators is a crucial part of this work.","Meta-Modeling-based approaches drawing on the principles of Model Driven Architecture (MDA) are a promising methodology for generator development.","The goal of this paper is to show why such an MDA-based approach can provide extremely powerful generators with minimal implementation effort and to demonstrate that this approach is a superior alternative to the most advanced hardware generation languages such as SpinalHDL and Chisel.","For this purpose, this paper provides an in-depth comparison of the Meta-Modeling approach against these hardware generation languages, highlighting the unique advantages of a Meta-Modeling-based approach and summarizes the benefits."],"url":"http://arxiv.org/abs/2404.05599v1","category":"cs.SE"}
{"created":"2024-04-08 15:18:42","title":"Hook-in Privacy Techniques for gRPC-based Microservice Communication","abstract":"gRPC is at the heart of modern distributed system architectures. Based on HTTP/2 and Protocol Buffers, it provides highly performant, standardized, and polyglot communication across loosely coupled microservices and is increasingly preferred over REST- or GraphQL-based service APIs in practice. Despite its widespread adoption, gRPC lacks any advanced privacy techniques beyond transport encryption and basic token-based authentication. Such advanced techniques are, however, increasingly important for fulfilling regulatory requirements. For instance, anonymizing or otherwise minimizing (personal) data before responding to requests, or pre-processing data based on the purpose of the access may be crucial in certain usecases. In this paper, we therefore propose a novel approach for integrating such advanced privacy techniques into the gRPC framework in a practically viable way. Specifically, we present a general approach along with a working prototype that implements privacy techniques, such as data minimization and purpose limitation, in a configurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We also showcase how to integrate this contribution into a realistic example of a food delivery use case. Alongside these implementations, a preliminary performance evaluation shows practical applicability with reasonable overheads. Altogether, we present a viable solution for integrating advanced privacy techniques into real-world gRPC-based microservice architectures, thereby facilitating regulatory compliance ``by design''.","sentences":["gRPC is at the heart of modern distributed system architectures.","Based on HTTP/2 and Protocol Buffers, it provides highly performant, standardized, and polyglot communication across loosely coupled microservices and is increasingly preferred over REST- or GraphQL-based service APIs in practice.","Despite its widespread adoption, gRPC lacks any advanced privacy techniques beyond transport encryption and basic token-based authentication.","Such advanced techniques are, however, increasingly important for fulfilling regulatory requirements.","For instance, anonymizing or otherwise minimizing (personal) data before responding to requests, or pre-processing data based on the purpose of the access may be crucial in certain usecases.","In this paper, we therefore propose a novel approach for integrating such advanced privacy techniques into the gRPC framework in a practically viable way.","Specifically, we present a general approach along with a working prototype that implements privacy techniques, such as data minimization and purpose limitation, in a configurable, extensible, and gRPC-native way utilizing a gRPC interceptor.","We also showcase how to integrate this contribution into a realistic example of a food delivery use case.","Alongside these implementations, a preliminary performance evaluation shows practical applicability with reasonable overheads.","Altogether, we present a viable solution for integrating advanced privacy techniques into real-world gRPC-based microservice architectures, thereby facilitating regulatory compliance ``by design''."],"url":"http://arxiv.org/abs/2404.05598v1","category":"cs.CR"}
{"created":"2024-04-08 15:17:37","title":"Little Rip and Pseudo Rip cosmological models with coupled dark energy based on a new generalized entropy","abstract":"We study Little Rip (LR) and Pseudo Rip (PR) cosmological models containing two coupled fluids: dark energy and dark matter. We assume a spatially flat Friedmann-Robertson-Walker (FRW) universe. The interaction between the dark energy and the dark matter fluid components is described in terms of the parameters in the generalized equation of state (EoS) in presence of the bulk viscosity. We consider entropic cosmology and use a description based on a new generalized entropy function, which was proposed by Nojiri-Odintsov-Faraoni [1]. Conditions for the appearance of the (LR) and the (PR) in terms of the parameters of the (EoS) are obtained. Introducing an energy density $\\rho_g$ corresponding to a specified entropy function $S_g$, together with an interaction term $Q$ in the gravitational equations of motion, we derive modified forms of the EoS parameters. We discuss the corrections of the thermodynamic parameters associated with the generalized entropy function. Properties of the late universe as well as in the early universe in this formalism are pointed out.","sentences":["We study Little Rip (LR) and Pseudo Rip (PR) cosmological models containing two coupled fluids: dark energy and dark matter.","We assume a spatially flat Friedmann-Robertson-Walker (FRW) universe.","The interaction between the dark energy and the dark matter fluid components is described in terms of the parameters in the generalized equation of state (EoS) in presence of the bulk viscosity.","We consider entropic cosmology and use a description based on a new generalized entropy function, which was proposed by Nojiri-Odintsov-Faraoni [1].","Conditions for the appearance of the (LR) and the (PR) in terms of the parameters of the (EoS) are obtained.","Introducing an energy density $\\rho_g$ corresponding to a specified entropy function $S_g$, together with an interaction term $Q$ in the gravitational equations of motion, we derive modified forms of the EoS parameters.","We discuss the corrections of the thermodynamic parameters associated with the generalized entropy function.","Properties of the late universe as well as in the early universe in this formalism are pointed out."],"url":"http://arxiv.org/abs/2404.05597v1","category":"gr-qc"}
{"created":"2024-04-08 15:14:20","title":"UniFL: Improve Stable Diffusion via Unified Feedback Learning","abstract":"Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff.","sentences":["Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications.","However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight.","To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively.","UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL.","Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed.","In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration.","For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference.","Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff."],"url":"http://arxiv.org/abs/2404.05595v1","category":"cs.CV"}
{"created":"2024-04-08 15:12:54","title":"Spin Radiation of Electrons, Excitons, and Phonons","abstract":"In the celebrated Stern-Gerlach experiment an inhomogeneous static magnetic field separates a beam of charge-neutral atoms with opposite spins, thereby driving a ``spin current\" normal to the propagation direction. Here we generalize it to the dynamic scenario by demonstrating a spin transfer between an AC inhomogeneous magnetic field and intraband electrons or charge-neutral excitons and phonons. We predict that parametric pumping can efficiently radiate their DC spin currents from local AC magnetic sources with van der Waals semiconductors as prototypes. This mechanism brings a unified and efficient paradigm in the spin transport of distinct mobile carriers.","sentences":["In the celebrated Stern-Gerlach experiment an inhomogeneous static magnetic field separates a beam of charge-neutral atoms with opposite spins, thereby driving a ``spin current\" normal to the propagation direction.","Here we generalize it to the dynamic scenario by demonstrating a spin transfer between an AC inhomogeneous magnetic field and intraband electrons or charge-neutral excitons and phonons.","We predict that parametric pumping can efficiently radiate their DC spin currents from local AC magnetic sources with van der Waals semiconductors as prototypes.","This mechanism brings a unified and efficient paradigm in the spin transport of distinct mobile carriers."],"url":"http://arxiv.org/abs/2404.05593v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 15:09:53","title":"Unruh-DeWitt Particle Detectors in Bouncing Cosmologies","abstract":"We study semi-classical particle production in non-singular bouncing cosmologies by employing the Unruh-DeWitt model of a particle detector propagating in this class of spacetimes. The scale factor for the bouncing cosmology is derived analytically and is inspired by the modified Friedmann equation employed in the loop quantum cosmology literature. We examine how the detector response varies with the free parameters in this model such as the equation of state during the contraction phase and the critical energy density during the bounce phase. We also investigate whether such a signature in the particle detector survives at late times.","sentences":["We study semi-classical particle production in non-singular bouncing cosmologies by employing the Unruh-DeWitt model of a particle detector propagating in this class of spacetimes.","The scale factor for the bouncing cosmology is derived analytically and is inspired by the modified Friedmann equation employed in the loop quantum cosmology literature.","We examine how the detector response varies with the free parameters in this model such as the equation of state during the contraction phase and the critical energy density during the bounce phase.","We also investigate whether such a signature in the particle detector survives at late times."],"url":"http://arxiv.org/abs/2404.05592v1","category":"gr-qc"}
{"created":"2024-04-08 15:04:51","title":"Variable-Pitch-Propeller Mechanism Design, and Development of Heliquad for Mid-flight Flipping and Fault-Tolerant-Control","abstract":"This paper presents the design of Variable-Pitch-Propeller mechanism and its application on a quadcopter called Heliquad to demonstrate its unique capabilities. The input-output relationship is estimated for a generic mechanism. Various singularities and actuator sizing requirements are also analyzed. The mechanism is manufactured, and the validated input-output relationship is implemented in the controller of Heliquad. Heliquad is controlled by a unified non-switching cascaded attitude-rate controller, followed by a unique Neural-Network-based reconfigurable control allocation to approximate nonlinear relationship between the control input and actuator command. The Heliquad prototype's mid-flight flip experiment validates the controller's tracking performance in upright as well as inverted conditions. The prototype is then flown in upright condition with only three of its working actuators. To the best of the authors' knowledge, the cambered airfoil propeller-equipped Heliquad prototype demonstrates full-attitude control, including yaw-rate, on three working actuators for the first time in the literature. Finally, the utility of this novel capability is demonstrated by safe recovery and precise landing post-mid-flight actuator failure crisis. Overall, the controller tracks the references well for all the experiments, and the output of the NN-based control allocation remains bounded throughout.","sentences":["This paper presents the design of Variable-Pitch-Propeller mechanism and its application on a quadcopter called Heliquad to demonstrate its unique capabilities.","The input-output relationship is estimated for a generic mechanism.","Various singularities and actuator sizing requirements are also analyzed.","The mechanism is manufactured, and the validated input-output relationship is implemented in the controller of Heliquad.","Heliquad is controlled by a unified non-switching cascaded attitude-rate controller, followed by a unique Neural-Network-based reconfigurable control allocation to approximate nonlinear relationship between the control input and actuator command.","The Heliquad prototype's mid-flight flip experiment validates the controller's tracking performance in upright as well as inverted conditions.","The prototype is then flown in upright condition with only three of its working actuators.","To the best of the authors' knowledge, the cambered airfoil propeller-equipped Heliquad prototype demonstrates full-attitude control, including yaw-rate, on three working actuators for the first time in the literature.","Finally, the utility of this novel capability is demonstrated by safe recovery and precise landing post-mid-flight actuator failure crisis.","Overall, the controller tracks the references well for all the experiments, and the output of the NN-based control allocation remains bounded throughout."],"url":"http://arxiv.org/abs/2404.05591v1","category":"eess.SY"}
{"created":"2024-04-08 15:03:57","title":"MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering","abstract":"Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support, which has been demonstrated by their competitive performances in Medical QA. However, while impressive, the required quality bar for medical applications remains far from being achieved. Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content. Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions. Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic. In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering. To the best of our knowledge, MedExpQA includes for the first time reference gold explanations written by medical doctors which can be leveraged to establish various gold-based upper-bounds for comparison with LLMs performance. Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs still has large room for improvement, especially for languages other than English. Furthermore, and despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering. So far the benchmark is available in four languages, but we hope that this work may encourage further development to other languages.","sentences":["Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support, which has been demonstrated by their competitive performances in Medical QA.","However, while impressive, the required quality bar for medical applications remains far from being achieved.","Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content.","Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions.","Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic.","In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering.","To the best of our knowledge, MedExpQA includes for the first time reference gold explanations written by medical doctors which can be leveraged to establish various gold-based upper-bounds for comparison with LLMs performance.","Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs still has large room for improvement, especially for languages other than English.","Furthermore, and despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering.","So far the benchmark is available in four languages, but we hope that this work may encourage further development to other languages."],"url":"http://arxiv.org/abs/2404.05590v1","category":"cs.CL"}
{"created":"2024-04-08 15:00:36","title":"Enhancing Software Related Information Extraction with Generative Language Models through Single-Choice Question Answering","abstract":"This paper describes our participation in the Shared Task on Software Mentions Disambiguation (SOMD), with a focus on improving relation extraction in scholarly texts through Generative Language Models (GLMs) using single-choice question-answering. The methodology prioritises the use of in-context learning capabilities of GLMs to extract software-related entities and their descriptive attributes, such as distributive information. Our approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for Named Entity Recognition (NER) and Attributive NER to identify relationships between extracted software entities, providing a structured solution for analysing software citations in academic literature. The paper provides a detailed description of our approach, demonstrating how using GLMs in a single-choice QA paradigm can greatly enhance IE methodologies. Our participation in the SOMD shared task highlights the importance of precise software citation practices and showcases our system's ability to overcome the challenges of disambiguating and extracting relationships between software mentions. This sets the groundwork for future research and development in this field.","sentences":["This paper describes our participation in the Shared Task on Software Mentions Disambiguation (SOMD), with a focus on improving relation extraction in scholarly texts through Generative Language Models (GLMs) using single-choice question-answering.","The methodology prioritises the use of in-context learning capabilities of GLMs to extract software-related entities and their descriptive attributes, such as distributive information.","Our approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for Named Entity Recognition (NER) and Attributive NER to identify relationships between extracted software entities, providing a structured solution for analysing software citations in academic literature.","The paper provides a detailed description of our approach, demonstrating how using GLMs in a single-choice QA paradigm can greatly enhance IE methodologies.","Our participation in the SOMD shared task highlights the importance of precise software citation practices and showcases our system's ability to overcome the challenges of disambiguating and extracting relationships between software mentions.","This sets the groundwork for future research and development in this field."],"url":"http://arxiv.org/abs/2404.05587v1","category":"cs.CL"}
{"created":"2024-04-08 14:58:52","title":"Towards More General Video-based Deepfake Detection through Facial Feature Guided Adaptation for Foundation Model","abstract":"With the rise of deep learning, generative models have enabled the creation of highly realistic synthetic images, presenting challenges due to their potential misuse. While research in Deepfake detection has grown rapidly in response, many detection methods struggle with unseen Deepfakes generated by new synthesis techniques. To address this generalisation challenge, we propose a novel Deepfake detection approach by adapting rich information encoded inside the Foundation Models with rich information encoded inside, specifically using the image encoder from CLIP which has demonstrated strong zero-shot capability for downstream tasks. Inspired by the recent advances of parameter efficient fine-tuning, we propose a novel side-network-based decoder to extract spatial and temporal cues from the given video clip, with the promotion of the Facial Component Guidance (FCG) to guidencourage the spatial feature to include features of key facial parts for more robust and general Deepfake detection. Through extensive cross-dataset evaluations, our approach exhibits superior effectiveness in identifying unseen Deepfake samples, achieving notable performance improvementsuccess even with limited training samples and manipulation types. Our model secures an average performance enhancement of 0.9% AUROC in cross-dataset assessments comparing with state-of-the-art methods, especiallytablishing a significant lead of achieving 4.4% improvement on the challenging DFDC dataset.","sentences":["With the rise of deep learning, generative models have enabled the creation of highly realistic synthetic images, presenting challenges due to their potential misuse.","While research in Deepfake detection has grown rapidly in response, many detection methods struggle with unseen Deepfakes generated by new synthesis techniques.","To address this generalisation challenge, we propose a novel Deepfake detection approach by adapting rich information encoded inside the Foundation Models with rich information encoded inside, specifically using the image encoder from CLIP which has demonstrated strong zero-shot capability for downstream tasks.","Inspired by the recent advances of parameter efficient fine-tuning, we propose a novel side-network-based decoder to extract spatial and temporal cues from the given video clip, with the promotion of the Facial Component Guidance (FCG) to guidencourage the spatial feature to include features of key facial parts for more robust and general Deepfake detection.","Through extensive cross-dataset evaluations, our approach exhibits superior effectiveness in identifying unseen Deepfake samples, achieving notable performance improvementsuccess even with limited training samples and manipulation types.","Our model secures an average performance enhancement of 0.9% AUROC in cross-dataset assessments comparing with state-of-the-art methods, especiallytablishing a significant lead of achieving 4.4% improvement on the challenging DFDC dataset."],"url":"http://arxiv.org/abs/2404.05583v1","category":"cs.CV"}
{"created":"2024-04-08 14:57:16","title":"Learning Prehensile Dexterity by Imitating and Emulating State-only Observations","abstract":"When humans learn physical skills (e.g., learn to play tennis), we tend to first observe and learn what an expert is doing. But this is often insufficient. Therefore, we subsequently engage in practice, where we try to emulate the expert. Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations. CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system. This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels. The second stage involves emulation: learn a motion refinement policy to make adjustments to the motion prior of the robot hand such that the desired object motion is reenacted. CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no need for additional teleoperated or labeled demonstrations). Detailed experiments reveal that i) Imitation alone is insufficient, but adding emulation drastically improves performance, ii) CIMER outperforms existing methods in terms of sample efficiency and the ability to generate realistic and stable motions, iii) CIMER can either zero-shot generalize or learn to adapt to novel objects from the YCB dataset, even outperforming expert policies trained with action labels in most cases.","sentences":["When humans learn physical skills (e.g., learn to play tennis), we tend to first observe and learn what an expert is doing.","But this is often insufficient.","Therefore, we subsequently engage in practice, where we try to emulate the expert.","Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations.","CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system.","This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels.","The second stage involves emulation: learn a motion refinement policy to make adjustments to the motion prior of the robot hand such that the desired object motion is reenacted.","CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no need for additional teleoperated or labeled demonstrations).","Detailed experiments reveal that i)","Imitation alone is insufficient, but adding emulation drastically improves performance, ii) CIMER outperforms existing methods in terms of sample efficiency and the ability to generate realistic and stable motions, iii) CIMER can either zero-shot generalize or learn to adapt to novel objects from the YCB dataset, even outperforming expert policies trained with action labels in most cases."],"url":"http://arxiv.org/abs/2404.05582v1","category":"cs.RO"}
{"created":"2024-04-08 14:56:56","title":"Design and Simulation of Time-energy Optimal Anti-swing Trajectory Planner for Autonomous Tower Cranes","abstract":"For autonomous crane lifting, optimal trajectories of the crane are required as reference inputs to the crane controller to facilitate feedforward control. Reducing the unactuated payload motion is a crucial issue for under-actuated tower cranes with spherical pendulum dynamics. The planned trajectory should be optimal in terms of both operating time and energy consumption, to facilitate optimum output spending optimum effort. This article proposes an anti-swing tower crane trajectory planner that can provide time-energy optimal solutions for the Computer-Aided Lift Planning (CALP) system developed at Nanyang Technological University, which facilitates collision-free lifting path planning of robotized tower cranes in autonomous construction sites. The current work introduces a trajectory planning module to the system that utilizes the geometric outputs from the path planning module and optimally scales them with time information. Firstly, analyzing the non-linear dynamics of the crane operations, the tower crane is established as differentially flat. Subsequently, the multi-objective trajectory optimization problems for all the crane operations are formulated in the flat output space through consideration of the mechanical and safety constraints. Two multi-objective evolutionary algorithms, namely Non-dominated Sorting Genetic Algorithm (NSGA-II) and Generalized Differential Evolution 3 (GDE3), are extensively compared via statistical measures based on the closeness of solutions to the Pareto front, distribution of solutions in the solution space and the runtime, to select the optimization engine of the planner. Finally, the crane operation trajectories are obtained via the corresponding planned flat output trajectories. Studies simulating real-world lifting scenarios are conducted to verify the effectiveness and reliability of the proposed module of the lift planning system.","sentences":["For autonomous crane lifting, optimal trajectories of the crane are required as reference inputs to the crane controller to facilitate feedforward control.","Reducing the unactuated payload motion is a crucial issue for under-actuated tower cranes with spherical pendulum dynamics.","The planned trajectory should be optimal in terms of both operating time and energy consumption, to facilitate optimum output spending optimum effort.","This article proposes an anti-swing tower crane trajectory planner that can provide time-energy optimal solutions for the Computer-Aided Lift Planning (CALP) system developed at Nanyang Technological University, which facilitates collision-free lifting path planning of robotized tower cranes in autonomous construction sites.","The current work introduces a trajectory planning module to the system that utilizes the geometric outputs from the path planning module and optimally scales them with time information.","Firstly, analyzing the non-linear dynamics of the crane operations, the tower crane is established as differentially flat.","Subsequently, the multi-objective trajectory optimization problems for all the crane operations are formulated in the flat output space through consideration of the mechanical and safety constraints.","Two multi-objective evolutionary algorithms, namely Non-dominated Sorting Genetic Algorithm (NSGA-II) and Generalized Differential Evolution 3 (GDE3), are extensively compared via statistical measures based on the closeness of solutions to the Pareto front, distribution of solutions in the solution space and the runtime, to select the optimization engine of the planner.","Finally, the crane operation trajectories are obtained via the corresponding planned flat output trajectories.","Studies simulating real-world lifting scenarios are conducted to verify the effectiveness and reliability of the proposed module of the lift planning system."],"url":"http://arxiv.org/abs/2404.05581v1","category":"cs.RO"}
{"created":"2024-04-08 14:56:26","title":"Responsible Visual Editing","abstract":"With recent advancements in visual synthesis, there is a growing risk of encountering images with detrimental effects, such as hate, discrimination, or privacy violations. The research on transforming harmful images into responsible ones remains unexplored. In this paper, we formulate a new task, responsible visual editing, which entails modifying specific concepts within an image to render it more responsible while minimizing changes. However, the concept that needs to be edited is often abstract, making it challenging to locate what needs to be modified and plan how to modify it. To tackle these challenges, we propose a Cognitive Editor (CoEditor) that harnesses the large multimodal model through a two-stage cognitive process: (1) a perceptual cognitive process to focus on what needs to be modified and (2) a behavioral cognitive process to strategize how to modify. To mitigate the negative implications of harmful images on research, we create a transparent and public dataset, AltBear, which expresses harmful information using teddy bears instead of humans. Experiments demonstrate that CoEditor can effectively comprehend abstract concepts within complex scenes and significantly surpass the performance of baseline models for responsible visual editing. We find that the AltBear dataset corresponds well to the harmful content found in real images, offering a consistent experimental evaluation, thereby providing a safer benchmark for future research. Moreover, CoEditor also shows great results in general editing. We release our code and dataset at https://github.com/kodenii/Responsible-Visual-Editing.","sentences":["With recent advancements in visual synthesis, there is a growing risk of encountering images with detrimental effects, such as hate, discrimination, or privacy violations.","The research on transforming harmful images into responsible ones remains unexplored.","In this paper, we formulate a new task, responsible visual editing, which entails modifying specific concepts within an image to render it more responsible while minimizing changes.","However, the concept that needs to be edited is often abstract, making it challenging to locate what needs to be modified and plan how to modify it.","To tackle these challenges, we propose a Cognitive Editor (CoEditor) that harnesses the large multimodal model through a two-stage cognitive process: (1) a perceptual cognitive process to focus on what needs to be modified and (2) a behavioral cognitive process to strategize how to modify.","To mitigate the negative implications of harmful images on research, we create a transparent and public dataset, AltBear, which expresses harmful information using teddy bears instead of humans.","Experiments demonstrate that CoEditor can effectively comprehend abstract concepts within complex scenes and significantly surpass the performance of baseline models for responsible visual editing.","We find that the AltBear dataset corresponds well to the harmful content found in real images, offering a consistent experimental evaluation, thereby providing a safer benchmark for future research.","Moreover, CoEditor also shows great results in general editing.","We release our code and dataset at https://github.com/kodenii/Responsible-Visual-Editing."],"url":"http://arxiv.org/abs/2404.05580v1","category":"cs.CV"}
{"created":"2024-04-08 14:52:48","title":"Dynamic Backtracking in GFlowNet: Enhancing Decision Steps with Reward-Dependent Adjustment Mechanisms","abstract":"Generative Flow Networks (GFlowNets) are probabilistic models predicated on Markov flows, employing specific amortization algorithms to learn stochastic policies that generate compositional substances including biomolecules, chemical materials, and more. Demonstrating formidable prowess in generating high-performance biochemical molecules, GFlowNets accelerate the discovery of scientific substances, effectively circumventing the time-consuming, labor-intensive, and costly shortcomings intrinsic to conventional material discovery. However, previous work often struggles to accumulate exploratory experience and is prone to becoming disoriented within expansive sampling spaces. Attempts to address this issue, such as LS-GFN, are limited to local greedy searches and lack broader global adjustments. This paper introduces a novel GFlowNet variant, the Dynamic Backtracking GFN (DB-GFN), which enhances the adaptability of decision-making steps through a reward-based dynamic backtracking mechanism. DB-GFN permits backtracking during the network construction process according to the current state's reward value, thus correcting disadvantageous decisions and exploring alternative pathways during the exploration process. Applied to generative tasks of biochemical molecules and genetic material sequences, DB-GFN surpasses existing GFlowNet models and traditional reinforcement learning methods in terms of sample quality, exploration sample quantity, and training convergence speed. Furthermore, the orthogonal nature of DB-GFN suggests its potential as a powerful tool for future improvements in GFN networks, with the promise of integrating with other strategies to achieve more efficient search performance.","sentences":["Generative Flow Networks (GFlowNets) are probabilistic models predicated on Markov flows, employing specific amortization algorithms to learn stochastic policies that generate compositional substances including biomolecules, chemical materials, and more.","Demonstrating formidable prowess in generating high-performance biochemical molecules, GFlowNets accelerate the discovery of scientific substances, effectively circumventing the time-consuming, labor-intensive, and costly shortcomings intrinsic to conventional material discovery.","However, previous work often struggles to accumulate exploratory experience and is prone to becoming disoriented within expansive sampling spaces.","Attempts to address this issue, such as LS-GFN, are limited to local greedy searches and lack broader global adjustments.","This paper introduces a novel GFlowNet variant, the Dynamic Backtracking GFN (DB-GFN), which enhances the adaptability of decision-making steps through a reward-based dynamic backtracking mechanism.","DB-GFN permits backtracking during the network construction process according to the current state's reward value, thus correcting disadvantageous decisions and exploring alternative pathways during the exploration process.","Applied to generative tasks of biochemical molecules and genetic material sequences, DB-GFN surpasses existing GFlowNet models and traditional reinforcement learning methods in terms of sample quality, exploration sample quantity, and training convergence speed.","Furthermore, the orthogonal nature of DB-GFN suggests its potential as a powerful tool for future improvements in GFN networks, with the promise of integrating with other strategies to achieve more efficient search performance."],"url":"http://arxiv.org/abs/2404.05576v1","category":"cs.LG"}
{"created":"2024-04-08 14:51:21","title":"Prediction of topotactic transition from black to blue phosphorus induced by surface Br adsorption","abstract":"Based on first-principles calculations, we propose a potential access to the yet unrealized freestanding blue phosphorus (blueP) through transformation of black phosphorus (blackP) induced by surface bromine (Br) adsorption. Formation of the Br-P bonds disrupts the original sp3 configurations in blackP, generates unpaired pz electrons and induces a structural transformation that results in blueP formation by re-pairing the pz orbitals. Ab initio molecular dynamics simulations confirm that randomly adsorbed Br adatoms on bilayer blackP spontaneously diffuse into specific patterns to render the emergence of the blueP phase. The expected obtainment Br-passivated blueP nanoribbons exhibit tunable band gaps in a wide range and high carrier mobilities of the order of 1000 cm2V-1s-1. This study provides an opportunity to fabricate blueP through the conversion from blackP by tuning its surface chemistry.","sentences":["Based on first-principles calculations, we propose a potential access to the yet unrealized freestanding blue phosphorus (blueP) through transformation of black phosphorus (blackP) induced by surface bromine (Br) adsorption.","Formation of the Br-P bonds disrupts the original sp3 configurations in blackP, generates unpaired pz electrons and induces a structural transformation that results in blueP formation by re-pairing the pz orbitals.","Ab initio molecular dynamics simulations confirm that randomly adsorbed Br adatoms on bilayer blackP spontaneously diffuse into specific patterns to render the emergence of the blueP phase.","The expected obtainment Br-passivated blueP nanoribbons exhibit tunable band gaps in a wide range and high carrier mobilities of the order of 1000 cm2V-1s-1.","This study provides an opportunity to fabricate blueP through the conversion from blackP by tuning its surface chemistry."],"url":"http://arxiv.org/abs/2404.05575v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 14:50:30","title":"Toward the van Benthem Characterization Theorem for Non-Distributive Modal Logic","abstract":"In this paper, we introduce the simulations and bisimulations on polarity-based semantics for non-distributive modal logic, which are natural generalizations of those notions on Kripke semantics for modal logic. We also generalize other important model-theoretic notions about Kripke semantics such as image-finite models, modally-saturated models, ultrafilter extension and ultrapower extension to the non-distributive setting. By using these generalizations, we prove the Hennessy-Milner theorem and the van Benthem characterization theorem for non-distributive modal logic based on polarity-based semantics.","sentences":["In this paper, we introduce the simulations and bisimulations on polarity-based semantics for non-distributive modal logic, which are natural generalizations of those notions on Kripke semantics for modal logic.","We also generalize other important model-theoretic notions about Kripke semantics such as image-finite models, modally-saturated models, ultrafilter extension and ultrapower extension to the non-distributive setting.","By using these generalizations, we prove the Hennessy-Milner theorem and the van Benthem characterization theorem for non-distributive modal logic based on polarity-based semantics."],"url":"http://arxiv.org/abs/2404.05574v1","category":"math.LO"}
{"created":"2024-04-08 14:45:42","title":"Topological photon pumping in quantum optical systems","abstract":"We establish the concept of topological pumping in one-dimensional systems with long-range interactions and apply it to the transport of a photon in quantum optical systems. In our theoretical investigation, we introduce an extended version of the Rice-Mele model with all-to-all exchange interactions. By analyzing its properties, we identify the general conditions for topological pumping and demonstrate the topologically protected and dispersionless transport of a photon on a one-dimensional emitter chain. As concrete examples, we investigate three different popular quantum optics platforms, namely Rydberg atom lattices, dense lattices of atoms excited to low-lying electronic states, and atoms coupled to waveguides, using experimentally relevant parameters. We observe that despite the long-ranged character of the dipole-dipole interactions, topological pumping facilitates the transport of a photon with a fidelity per cycle which can reach 99.9%. Moreover, we find that the photon pumping process remains topologically protected against local disorder in the coupling rates.","sentences":["We establish the concept of topological pumping in one-dimensional systems with long-range interactions and apply it to the transport of a photon in quantum optical systems.","In our theoretical investigation, we introduce an extended version of the Rice-Mele model with all-to-all exchange interactions.","By analyzing its properties, we identify the general conditions for topological pumping and demonstrate the topologically protected and dispersionless transport of a photon on a one-dimensional emitter chain.","As concrete examples, we investigate three different popular quantum optics platforms, namely Rydberg atom lattices, dense lattices of atoms excited to low-lying electronic states, and atoms coupled to waveguides, using experimentally relevant parameters.","We observe that despite the long-ranged character of the dipole-dipole interactions, topological pumping facilitates the transport of a photon with a fidelity per cycle which can reach 99.9%.","Moreover, we find that the photon pumping process remains topologically protected against local disorder in the coupling rates."],"url":"http://arxiv.org/abs/2404.05570v1","category":"quant-ph"}
{"created":"2024-04-08 14:43:13","title":"360\u00b0REA: Towards A Reusable Experience Accumulation with 360\u00b0 Assessment for Multi-Agent System","abstract":"Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel 360{\\deg} performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of 360{\\deg}REA.","sentences":["Large language model agents have demonstrated remarkable advancements across various complex tasks.","Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks.","Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents.","We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance.","In this paper, we propose Reusable Experience Accumulation with 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework inspired by corporate organizational practices.","The framework employs a novel 360{\\deg} performance assessment method for multi-perspective performance evaluation with fine-grained assessment.","To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment.","Extensive experiments on complex task datasets demonstrate the effectiveness of 360{\\deg}REA."],"url":"http://arxiv.org/abs/2404.05569v1","category":"cs.AI"}
{"created":"2024-04-08 14:41:41","title":"Relaxed hydrodynamic theory of electrically driven non-equilibrium steady states","abstract":"Hydrodynamics, as an effective theory capturing long-wavelength and late-time dynamics around thermal equilibrium states, finds applications in diverse physical systems, ranging from electron flow in metals, to ocean wave propagation, traffic flow, bacterial motion and the quark-gluon plasma. On the other hand, non-equilibrium steady states (NESS), characterized by a stationary flow of energy or matter in the presence of a driving force, are pervasive in nature but they present significant challenges to the foundational principles of statistical physics, as it is generally unclear if and how they satisfy the axiomatic assumptions of thermodynamics. The capability of hydrodynamics to accurately describe slow and long-wavelength fluctuations around a NESS remains an open question. In this study, we provide positive evidence by specifically addressing electrically driven non-equilibrium charged steady states. Our approach involves introducing gapped modes and extending the effective description into a relaxed hydrodynamic theory (RHT). Leveraging the gauge-gravity duality as a tool for controlled computations within non-equilibrium systems, we establish an ultraviolet complete model for these NESS that confirms the validity of our RHT. In summary, our findings present a concrete realization of a RHT applicable to a NESS. This expands the regime of validity of hydrodynamics beyond thermal equilibrium, offering valuable insights into the dynamics of non-equilibrium systems.","sentences":["Hydrodynamics, as an effective theory capturing long-wavelength and late-time dynamics around thermal equilibrium states, finds applications in diverse physical systems, ranging from electron flow in metals, to ocean wave propagation, traffic flow, bacterial motion and the quark-gluon plasma.","On the other hand, non-equilibrium steady states (NESS), characterized by a stationary flow of energy or matter in the presence of a driving force, are pervasive in nature but they present significant challenges to the foundational principles of statistical physics, as it is generally unclear if and how they satisfy the axiomatic assumptions of thermodynamics.","The capability of hydrodynamics to accurately describe slow and long-wavelength fluctuations around a NESS remains an open question.","In this study, we provide positive evidence by specifically addressing electrically driven non-equilibrium charged steady states.","Our approach involves introducing gapped modes and extending the effective description into a relaxed hydrodynamic theory (RHT).","Leveraging the gauge-gravity duality as a tool for controlled computations within non-equilibrium systems, we establish an ultraviolet complete model for these NESS that confirms the validity of our RHT.","In summary, our findings present a concrete realization of a RHT applicable to a NESS.","This expands the regime of validity of hydrodynamics beyond thermal equilibrium, offering valuable insights into the dynamics of non-equilibrium systems."],"url":"http://arxiv.org/abs/2404.05568v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-08 14:39:49","title":"Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models","abstract":"Mixture-of-Experts (MoE) language models can reduce computational costs by 2-4$\\times$ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally require 2-4$\\times$ times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation. In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference. Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40% of the model's parameters). Performance tests using vLLM show that our DS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like Mistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.","sentences":["Mixture-of-Experts (MoE) language models can reduce computational costs by 2-4$\\times$ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios.","However, MoE models generally require 2-4$\\times$ times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation.","In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference.","Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40% of the model's parameters).","Performance tests using vLLM show that our DS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like Mistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B."],"url":"http://arxiv.org/abs/2404.05567v1","category":"cs.LG"}
{"created":"2024-04-08 14:37:26","title":"Optimal Flow Admission Control in Edge Computing via Safe Reinforcement Learning","abstract":"With the uptake of intelligent data-driven applications, edge computing infrastructures necessitate a new generation of admission control algorithms to maximize system performance under limited and highly heterogeneous resources. In this paper, we study how to optimally select information flows which belong to different classes and dispatch them to multiple edge servers where deployed applications perform flow analytic tasks. The optimal policy is obtained via constrained Markov decision process (CMDP) theory accounting for the demand of each edge application for specific classes of flows, the constraints on computing capacity of edge servers and of the access network.   We develop DR-CPO, a specialized primal-dual Safe Reinforcement Learning (SRL) method which solves the resulting optimal admission control problem by reward decomposition. DR-CPO operates optimal decentralized control and mitigates effectively state-space explosion while preserving optimality. Compared to existing Deep Reinforcement Learning (DRL) solutions, extensive results show that DR-CPO achieves 15\\% higher reward on a wide variety of environments, while requiring on average only 50\\% of the amount of learning episodes to converge. Finally, we show how to match DR-CPO and load-balancing to dispatch optimally information streams to available edge servers and further improve system performance.","sentences":["With the uptake of intelligent data-driven applications, edge computing infrastructures necessitate a new generation of admission control algorithms to maximize system performance under limited and highly heterogeneous resources.","In this paper, we study how to optimally select information flows which belong to different classes and dispatch them to multiple edge servers where deployed applications perform flow analytic tasks.","The optimal policy is obtained via constrained Markov decision process (CMDP) theory accounting for the demand of each edge application for specific classes of flows, the constraints on computing capacity of edge servers and of the access network.   ","We develop DR-CPO, a specialized primal-dual Safe Reinforcement Learning (SRL) method which solves the resulting optimal admission control problem by reward decomposition.","DR-CPO operates optimal decentralized control and mitigates effectively state-space explosion while preserving optimality.","Compared to existing Deep Reinforcement Learning (DRL) solutions, extensive results show that DR-CPO achieves 15\\% higher reward on a wide variety of environments, while requiring on average only 50\\% of the amount of learning episodes to converge.","Finally, we show how to match DR-CPO and load-balancing to dispatch optimally information streams to available edge servers and further improve system performance."],"url":"http://arxiv.org/abs/2404.05564v1","category":"cs.NI"}
{"created":"2024-04-08 14:28:27","title":"On the Convergence of Continual Learning with Adaptive Methods","abstract":"One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. However, the convergence of continual learning for each sequential task is less studied so far. In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks. We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients. The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks.","sentences":["One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma.","However, the convergence of continual learning for each sequential task is less studied so far.","In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.","We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.","The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration.","Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks."],"url":"http://arxiv.org/abs/2404.05555v1","category":"cs.LG"}
{"created":"2024-04-08 14:21:34","title":"Alljoined -- A dataset for EEG-to-Image decoding","abstract":"We present Alljoined, a dataset built specifically for EEG-to-Image decoding. Recognizing that an extensive and unbiased sampling of neural responses to visual stimuli is crucial for image reconstruction efforts, we collected data from 8 participants looking at 10,000 natural images each. We have currently gathered 46,080 epochs of brain responses recorded with a 64-channel EEG headset. The dataset combines response-based stimulus timing, repetition between blocks and sessions, and diverse image classes with the goal of improving signal quality. For transparency, we also provide data quality scores. We publicly release the dataset and all code at https://linktr.ee/alljoined1.","sentences":["We present Alljoined, a dataset built specifically for EEG-to-Image decoding.","Recognizing that an extensive and unbiased sampling of neural responses to visual stimuli is crucial for image reconstruction efforts, we collected data from 8 participants looking at 10,000 natural images each.","We have currently gathered 46,080 epochs of brain responses recorded with a 64-channel EEG headset.","The dataset combines response-based stimulus timing, repetition between blocks and sessions, and diverse image classes with the goal of improving signal quality.","For transparency, we also provide data quality scores.","We publicly release the dataset and all code at https://linktr.ee/alljoined1."],"url":"http://arxiv.org/abs/2404.05553v1","category":"q-bio.NC"}
{"created":"2024-04-08 14:15:56","title":"Evaluating Interventional Reasoning Capabilities of Large Language Models","abstract":"Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts.","sentences":["Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system.","As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial.","A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions.","Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention.","We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning.","These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts.","Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts."],"url":"http://arxiv.org/abs/2404.05545v1","category":"cs.LG"}
{"created":"2024-04-08 14:15:45","title":"Near/Far-Field Channel Estimation For Terahertz Systems With ELAAs: A Block-Sparse-Aware Approach","abstract":"Millimeter wave/Terahertz (mmWave/THz) communication with extremely large-scale antenna arrays (ELAAs) offers a promising solution to meet the escalating demand for high data rates in next-generation communications. A large array aperture, along with the ever increasing carrier frequency within the mmWave/THz bands, leads to a large Rayleigh distance. As a result, the traditional plane-wave assumption may not hold valid for mmWave/THz systems featuring ELAAs. In this paper, we consider the problem of hybrid near/far-field channel estimation by taking spherical wave propagation into account. By analyzing the coherence properties of any two near-field steering vectors, we prove that the hybrid near/far-field channel admits a block-sparse representation on a specially designed orthogonal dictionary. Specifically, the percentage of nonzero elements of such a block-sparse representation decreases in the order of $1/\\sqrt{N}$, which tends to zero as the number of antennas, $N$, grows. Such a block-sparse representation allows to convert channel estimation into a block-sparse signal recovery problem. Simulation results are provided to verify our theoretical results and illustrate the performance of the proposed channel estimation approach in comparison with existing state-of-the-art methods.","sentences":["Millimeter wave/Terahertz (mmWave/THz) communication with extremely large-scale antenna arrays (ELAAs) offers a promising solution to meet the escalating demand for high data rates in next-generation communications.","A large array aperture, along with the ever increasing carrier frequency within the mmWave/THz bands, leads to a large Rayleigh distance.","As a result, the traditional plane-wave assumption may not hold valid for mmWave/THz systems featuring ELAAs.","In this paper, we consider the problem of hybrid near/far-field channel estimation by taking spherical wave propagation into account.","By analyzing the coherence properties of any two near-field steering vectors, we prove that the hybrid near/far-field channel admits a block-sparse representation on a specially designed orthogonal dictionary.","Specifically, the percentage of nonzero elements of such a block-sparse representation decreases in the order of $1/\\sqrt{N}$, which tends to zero as the number of antennas, $N$, grows.","Such a block-sparse representation allows to convert channel estimation into a block-sparse signal recovery problem.","Simulation results are provided to verify our theoretical results and illustrate the performance of the proposed channel estimation approach in comparison with existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.05544v1","category":"eess.SP"}
{"created":"2024-04-08 14:10:14","title":"Optimal Allocation of Tasks and Price of Anarchy of Distributed Optimization in Networked Computing Facilities","abstract":"The allocation of computing tasks for networked distributed services poses a question to service providers on whether centralized allocation management be worth its cost. Existing analytical models were conceived for users accessing computing resources with practically indistinguishable (hence irrelevant for the allocation decision) delays, which is typical of services located in the same distant data center. However, with the rise of the edge-cloud continuum, a simple analysis of the sojourn time that computing tasks observe at the server misses the impact of diverse latency values imposed by server locations. We therefore study the optimization of computing task allocation with a new model that considers both distance of servers and sojourn time in servers. We derive exact algorithms to optimize the system and we show, through numerical analysis and real experiments, that differences in server location in the edge-cloud continuum cannot be neglected. By means of algorithmic game theory, we study the price of anarchy of a distributed implementation of the computing task allocation problem and unveil important practical properties such as the fact that the price of anarchy tends to be small -- except when the system is overloaded -- and its maximum can be computed with low complexity.","sentences":["The allocation of computing tasks for networked distributed services poses a question to service providers on whether centralized allocation management be worth its cost.","Existing analytical models were conceived for users accessing computing resources with practically indistinguishable (hence irrelevant for the allocation decision) delays, which is typical of services located in the same distant data center.","However, with the rise of the edge-cloud continuum, a simple analysis of the sojourn time that computing tasks observe at the server misses the impact of diverse latency values imposed by server locations.","We therefore study the optimization of computing task allocation with a new model that considers both distance of servers and sojourn time in servers.","We derive exact algorithms to optimize the system and we show, through numerical analysis and real experiments, that differences in server location in the edge-cloud continuum cannot be neglected.","By means of algorithmic game theory, we study the price of anarchy of a distributed implementation of the computing task allocation problem and unveil important practical properties such as the fact that the price of anarchy tends to be small -- except when the system is overloaded -- and its maximum can be computed with low complexity."],"url":"http://arxiv.org/abs/2404.05543v1","category":"cs.GT"}
{"created":"2024-04-08 14:08:56","title":"OPSD: an Offensive Persian Social media Dataset and its baseline evaluations","abstract":"The proliferation of hate speech and offensive comments on social media has become increasingly prevalent due to user activities. Such comments can have detrimental effects on individuals' psychological well-being and social behavior. While numerous datasets in the English language exist in this domain, few equivalent resources are available for Persian language. To address this gap, this paper introduces two offensive datasets. The first dataset comprises annotations provided by domain experts, while the second consists of a large collection of unlabeled data obtained through web crawling for unsupervised learning purposes. To ensure the quality of the former dataset, a meticulous three-stage labeling process was conducted, and kappa measures were computed to assess inter-annotator agreement. Furthermore, experiments were performed on the dataset using state-of-the-art language models, both with and without employing masked language modeling techniques, as well as machine learning algorithms, in order to establish the baselines for the dataset using contemporary cutting-edge approaches. The obtained F1-scores for the three-class and two-class versions of the dataset were 76.9% and 89.9% for XLM-RoBERTa, respectively.","sentences":["The proliferation of hate speech and offensive comments on social media has become increasingly prevalent due to user activities.","Such comments can have detrimental effects on individuals' psychological well-being and social behavior.","While numerous datasets in the English language exist in this domain, few equivalent resources are available for Persian language.","To address this gap, this paper introduces two offensive datasets.","The first dataset comprises annotations provided by domain experts, while the second consists of a large collection of unlabeled data obtained through web crawling for unsupervised learning purposes.","To ensure the quality of the former dataset, a meticulous three-stage labeling process was conducted, and kappa measures were computed to assess inter-annotator agreement.","Furthermore, experiments were performed on the dataset using state-of-the-art language models, both with and without employing masked language modeling techniques, as well as machine learning algorithms, in order to establish the baselines for the dataset using contemporary cutting-edge approaches.","The obtained F1-scores for the three-class and two-class versions of the dataset were 76.9% and 89.9% for XLM-RoBERTa, respectively."],"url":"http://arxiv.org/abs/2404.05540v1","category":"cs.CL"}
{"created":"2024-04-08 14:03:37","title":"On the Optimal MMSE Channel Estimation for One-Bit Quantized MIMO Systems","abstract":"This paper focuses on the minimum mean squared error (MMSE) channel estimator for multiple-input multiple-output (MIMO) systems with one-bit quantization at the receiver side. Despite its optimality and significance in estimation theory, the MMSE channel estimator has not been fully investigated in this context due to its general non-linearity and computational complexity. Instead, the typically suboptimal Bussgang linear MMSE (BLMMSE) estimator has been widely adopted. In this work, we develop a new framework to compute the MMSE channel estimator that hinges on computation of the orthant probability of the multivariate normal distribution. Based on this framework, we determine a necessary and sufficient condition for the BLMMSE channel estimator to be optimal and equivalent to the MMSE estimator. Under the assumption of specific channel correlation or pilot symbols, we further utilize the framework to derive analytical expressions for the MMSE channel estimator that are particularly convenient for computation when certain system dimensions become large, thereby enabling a comparison between the BLMMSE and MMSE channel estimators in these cases.","sentences":["This paper focuses on the minimum mean squared error (MMSE) channel estimator for multiple-input multiple-output (MIMO) systems with one-bit quantization at the receiver side.","Despite its optimality and significance in estimation theory, the MMSE channel estimator has not been fully investigated in this context due to its general non-linearity and computational complexity.","Instead, the typically suboptimal Bussgang linear MMSE (BLMMSE) estimator has been widely adopted.","In this work, we develop a new framework to compute the MMSE channel estimator that hinges on computation of the orthant probability of the multivariate normal distribution.","Based on this framework, we determine a necessary and sufficient condition for the BLMMSE channel estimator to be optimal and equivalent to the MMSE estimator.","Under the assumption of specific channel correlation or pilot symbols, we further utilize the framework to derive analytical expressions for the MMSE channel estimator that are particularly convenient for computation when certain system dimensions become large, thereby enabling a comparison between the BLMMSE and MMSE channel estimators in these cases."],"url":"http://arxiv.org/abs/2404.05536v1","category":"cs.IT"}
{"created":"2024-04-08 14:03:33","title":"Robust STL Control Synthesis under Maximal Disturbance Sets","abstract":"This work addresses maximally robust control synthesis under unknown disturbances. We consider a general nonlinear system, subject to a Signal Temporal Logic (STL) specification, and wish to jointly synthesize the maximal possible disturbance bounds and the corresponding controllers that ensure the STL specification is satisfied under these bounds. Many works have considered STL satisfaction under given bounded disturbances. Yet, to the authors' best knowledge, this is the first work that aims to maximize the permissible disturbance set and find the corresponding controllers that ensure satisfying the STL specification with maximum disturbance robustness. We extend the notion of disturbance-robust semantics for STL, which is a property of a specification, dynamical system, and controller, and provide an algorithm to get the maximal disturbance robust controllers satisfying an STL specification using Hamilton-Jacobi reachability. We show its soundness and provide a simulation example with an Autonomous Underwater Vehicle (AUV).","sentences":["This work addresses maximally robust control synthesis under unknown disturbances.","We consider a general nonlinear system, subject to a Signal Temporal Logic (STL) specification, and wish to jointly synthesize the maximal possible disturbance bounds and the corresponding controllers that ensure the STL specification is satisfied under these bounds.","Many works have considered STL satisfaction under given bounded disturbances.","Yet, to the authors' best knowledge, this is the first work that aims to maximize the permissible disturbance set and find the corresponding controllers that ensure satisfying the STL specification with maximum disturbance robustness.","We extend the notion of disturbance-robust semantics for STL, which is a property of a specification, dynamical system, and controller, and provide an algorithm to get the maximal disturbance robust controllers satisfying an STL specification using Hamilton-Jacobi reachability.","We show its soundness and provide a simulation example with an Autonomous Underwater Vehicle (AUV)."],"url":"http://arxiv.org/abs/2404.05535v1","category":"cs.RO"}
{"created":"2024-04-08 14:00:50","title":"Ordre public exceptions for algorithmic surveillance patents","abstract":"This chapter explores the role of patent protection in algorithmic surveillance and whether ordre public exceptions from patentability should apply to such patents, due to their potential to enable human rights violations. It concludes that in most cases, it is undesirable to exclude algorithmic surveillance patents from patentability, as the patent system is ill-equipped to evaluate the impacts of the exploitation of such technologies. Furthermore, the disclosure of such patents has positive externalities from the societal perspective by opening the black box of surveillance for public scrutiny.","sentences":["This chapter explores the role of patent protection in algorithmic surveillance and whether ordre public exceptions from patentability should apply to such patents, due to their potential to enable human rights violations.","It concludes that in most cases, it is undesirable to exclude algorithmic surveillance patents from patentability, as the patent system is ill-equipped to evaluate the impacts of the exploitation of such technologies.","Furthermore, the disclosure of such patents has positive externalities from the societal perspective by opening the black box of surveillance for public scrutiny."],"url":"http://arxiv.org/abs/2404.05534v1","category":"cs.CY"}
{"created":"2024-04-08 13:59:02","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","abstract":"Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1-5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences.","RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used.","In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process.","We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets.","Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1-5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative).","The findings from our experiments also shed light on strategies to defend against the preference poisoning attack."],"url":"http://arxiv.org/abs/2404.05530v1","category":"cs.CL"}
{"created":"2024-04-08 13:44:42","title":"$(g-2)_{e,\u03bc}$ anomalies and decays $h, Z\\to e_b e_a $ in 3-3-1 models with inverse seesaw neutrinos","abstract":"The Lepton flavor violating (LFV) decays $h, Z\\to e_b e_a $, and $e_b\\to e_a \\gamma$ are discussed in a class of general 3-3-1 models adding heavy neutral leptons and singly charged Higgs bosons to accommodate experimental data of neutrino oscillation and $(g-2)_{e_a}$ anomalies of charged leptons through the inverse seesaw mechanism. We show that the models with the minimal number of inverse seesaw neutrinos can not reach the $1\\sigma$ range of $(g-2)_{\\mu}$ data due to the experimental upper bounds on decay rates of $(\\tau \\to \\mu \\gamma)$ and $(\\mu \\to e \\gamma)$. Features of LFV decays are presented in detail, focusing on the regions of parameter space satisfying the $1\\sigma$ ranges of $(g-2)_{e,\\mu}$ data.","sentences":["The Lepton flavor violating (LFV) decays $h, Z\\to e_b","e_a $, and $e_b\\to e_a \\gamma$ are discussed in a class of general 3-3-1 models adding heavy neutral leptons and singly charged Higgs bosons to accommodate experimental data of neutrino oscillation and $(g-2)_{e_a}$ anomalies of charged leptons through the inverse seesaw mechanism.","We show that the models with the minimal number of inverse seesaw neutrinos can not reach the $1\\sigma$ range of $(g-2)_{\\mu}$ data due to the experimental upper bounds on decay rates of $(\\tau \\to \\mu \\gamma)$ and $(\\mu \\to e \\gamma)$. Features of LFV decays are presented in detail, focusing on the regions of parameter space satisfying the $1\\sigma$ ranges of $(g-2)_{e,\\mu}$ data."],"url":"http://arxiv.org/abs/2404.05524v1","category":"hep-ph"}
{"created":"2024-04-08 13:43:28","title":"Lattice-Assisted Molecular Excitations in Frustrated Pyrochlores","abstract":"Resonance-like molecular excitations have been discovered in a series of pyrochlore antiferromagnets, yet their origins and relationships with the coexisting magnon excitations remain a puzzle. Here, by incorporating the spin-lattice coupling and the consequent four-spin interactions through the site-phonon model, we accomplish a unified description of the molecular and magnon excitations, revealing the molecular modes as statistical approximations of the dispersive magnon excitations. Our work demonstrates a general approach to understand exotic spin dynamics in magnetoelastic systems.","sentences":["Resonance-like molecular excitations have been discovered in a series of pyrochlore antiferromagnets, yet their origins and relationships with the coexisting magnon excitations remain a puzzle.","Here, by incorporating the spin-lattice coupling and the consequent four-spin interactions through the site-phonon model, we accomplish a unified description of the molecular and magnon excitations, revealing the molecular modes as statistical approximations of the dispersive magnon excitations.","Our work demonstrates a general approach to understand exotic spin dynamics in magnetoelastic systems."],"url":"http://arxiv.org/abs/2404.05523v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 13:43:19","title":"3DMambaIPF: A State Space Model for Iterative Point Cloud Filtering via Differentiable Rendering","abstract":"Noise is an inevitable aspect of point cloud acquisition, necessitating filtering as a fundamental task within the realm of 3D vision. Existing learning-based filtering methods have shown promising capabilities on small-scale synthetic or real-world datasets. Nonetheless, the effectiveness of these methods is constrained when dealing with a substantial quantity of point clouds. This limitation primarily stems from their limited denoising capabilities for large-scale point clouds and their inclination to generate noisy outliers after denoising. The recent introduction of State Space Models (SSMs) for long sequence modeling in Natural Language Processing (NLP) presents a promising solution for handling large-scale data. Encouraged by iterative point cloud filtering methods, we introduce 3DMambaIPF, firstly incorporating Mamba (Selective SSM) architecture to sequentially handle extensive point clouds from large scenes, capitalizing on its strengths in selective input processing and long sequence modeling capabilities. Additionally, we integrate a robust and fast differentiable rendering loss to constrain the noisy points around the surface. In contrast to previous methodologies, this differentiable rendering loss enhances the visual realism of denoised geometric structures and aligns point cloud boundaries more closely with those observed in real-world objects. Extensive evaluation on datasets comprising small-scale synthetic and real-world models (typically with up to 50K points) demonstrate that our method achieves state-of-the-art results. Moreover, we showcase the superior scalability and efficiency of our method on large-scale models with about 500K points, where the majority of the existing learning-based denoising methods are unable to handle.","sentences":["Noise is an inevitable aspect of point cloud acquisition, necessitating filtering as a fundamental task within the realm of 3D vision.","Existing learning-based filtering methods have shown promising capabilities on small-scale synthetic or real-world datasets.","Nonetheless, the effectiveness of these methods is constrained when dealing with a substantial quantity of point clouds.","This limitation primarily stems from their limited denoising capabilities for large-scale point clouds and their inclination to generate noisy outliers after denoising.","The recent introduction of State Space Models (SSMs) for long sequence modeling in Natural Language Processing (NLP) presents a promising solution for handling large-scale data.","Encouraged by iterative point cloud filtering methods, we introduce 3DMambaIPF, firstly incorporating Mamba (Selective SSM) architecture to sequentially handle extensive point clouds from large scenes, capitalizing on its strengths in selective input processing and long sequence modeling capabilities.","Additionally, we integrate a robust and fast differentiable rendering loss to constrain the noisy points around the surface.","In contrast to previous methodologies, this differentiable rendering loss enhances the visual realism of denoised geometric structures and aligns point cloud boundaries more closely with those observed in real-world objects.","Extensive evaluation on datasets comprising small-scale synthetic and real-world models (typically with up to 50K points) demonstrate that our method achieves state-of-the-art results.","Moreover, we showcase the superior scalability and efficiency of our method on large-scale models with about 500K points, where the majority of the existing learning-based denoising methods are unable to handle."],"url":"http://arxiv.org/abs/2404.05522v1","category":"cs.MM"}
{"created":"2024-04-08 13:41:32","title":"The Fact Selection Problem in LLM-Based Program Repair","abstract":"Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.","sentences":["Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs).","Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs?","To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark.","Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial.","Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it.","Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes.","These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance.","We found that there is no one-size-fits-all set of facts for bug repair.","Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt.","This model significantly surpasses the performance of the best generic fact set.","To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods.","On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration."],"url":"http://arxiv.org/abs/2404.05520v1","category":"cs.SE"}
{"created":"2024-04-08 13:40:01","title":"Investigating the Effectiveness of Cross-Attention to Unlock Zero-Shot Editing of Text-to-Video Diffusion Models","abstract":"With recent advances in image and video diffusion models for content creation, a plethora of techniques have been proposed for customizing their generated content. In particular, manipulating the cross-attention layers of Text-to-Image (T2I) diffusion models has shown great promise in controlling the shape and location of objects in the scene. Transferring image-editing techniques to the video domain, however, is extremely challenging as object motion and temporal consistency are difficult to capture accurately. In this work, we take a first look at the role of cross-attention in Text-to-Video (T2V) diffusion models for zero-shot video editing. While one-shot models have shown potential in controlling motion and camera movement, we demonstrate zero-shot control over object shape, position and movement in T2V models. We show that despite the limitations of current T2V models, cross-attention guidance can be a promising approach for editing videos.","sentences":["With recent advances in image and video diffusion models for content creation, a plethora of techniques have been proposed for customizing their generated content.","In particular, manipulating the cross-attention layers of Text-to-Image (T2I) diffusion models has shown great promise in controlling the shape and location of objects in the scene.","Transferring image-editing techniques to the video domain, however, is extremely challenging as object motion and temporal consistency are difficult to capture accurately.","In this work, we take a first look at the role of cross-attention in Text-to-Video (T2V) diffusion models for zero-shot video editing.","While one-shot models have shown potential in controlling motion and camera movement, we demonstrate zero-shot control over object shape, position and movement in T2V models.","We show that despite the limitations of current T2V models, cross-attention guidance can be a promising approach for editing videos."],"url":"http://arxiv.org/abs/2404.05519v1","category":"cs.CV"}
{"created":"2024-04-08 13:35:48","title":"Efficient Distributed Data Structures for Future Many-core Architectures","abstract":"We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory. With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques. To achieve scalability, we study a generic scheme which makes all our implementations hierarchical. We consider a collection of known techniques for improving the scalability of concurrent data structures and we adjust them to work in our setting. We have performed experiments which illustrate that some of these techniques have indeed high impact on achieving scalability. Our experiments also reveal the performance and scalability power of the hierarchical approach. We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures.","sentences":["We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory.","With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques.","To achieve scalability, we study a generic scheme which makes all our implementations hierarchical.","We consider a collection of known techniques for improving the scalability of concurrent data structures and we adjust them to work in our setting.","We have performed experiments which illustrate that some of these techniques have indeed high impact on achieving scalability.","Our experiments also reveal the performance and scalability power of the hierarchical approach.","We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures."],"url":"http://arxiv.org/abs/2404.05515v1","category":"cs.DC"}
{"created":"2024-04-08 13:28:11","title":"Synergy of Large Language Model and Model Driven Engineering for Automated Development of Centralized Vehicular Systems","abstract":"We present a prototype of a tool leveraging the synergy of model driven engineering (MDE) and Large Language Models (LLM) for the purpose of software development process automation in the automotive industry. In this approach, the user-provided input is free form textual requirements, which are first translated to Ecore model instance representation using an LLM, which is afterwards checked for consistency using Object Constraint Language (OCL) rules. After successful consistency check, the model instance is fed as input to another LLM for the purpose of code generation. The generated code is evaluated in a simulated environment using CARLA simulator connected to an example centralized vehicle architecture, in an emergency brake scenario.","sentences":["We present a prototype of a tool leveraging the synergy of model driven engineering (MDE) and Large Language Models (LLM) for the purpose of software development process automation in the automotive industry.","In this approach, the user-provided input is free form textual requirements, which are first translated to Ecore model instance representation using an LLM, which is afterwards checked for consistency using Object Constraint Language (OCL) rules.","After successful consistency check, the model instance is fed as input to another LLM for the purpose of code generation.","The generated code is evaluated in a simulated environment using CARLA simulator connected to an example centralized vehicle architecture, in an emergency brake scenario."],"url":"http://arxiv.org/abs/2404.05508v1","category":"cs.SE"}
{"created":"2024-04-08 13:27:13","title":"FastECPP over MPI","abstract":"The FastECPP algorithm is currently the fastest approach to prove the primality of general numbers, and has the additional benefit of creating certificates that can be checked independently and with a lower complexity. This article shows how by parallelising over a linear number of cores, its quartic time complexity becomes a cubic wallclock time complexity; and it presents the algorithmic choices of the FastECPP implementation in the author's \\cm\\ software https://www.multiprecision.org/cm/, which has been written with massive parallelisation over MPI in mind and used to establish a new primality record, for the ``repunit''$(10^{86453} - 1) / 9$.","sentences":["The FastECPP algorithm is currently the fastest approach to prove the primality of general numbers, and has the additional benefit of creating certificates that can be checked independently and with a lower complexity.","This article shows how by parallelising over a linear number of cores, its quartic time complexity becomes a cubic wallclock time complexity; and it presents the algorithmic choices of the FastECPP implementation in the author's \\cm\\ software https://www.multiprecision.org/cm/, which has been written with massive parallelisation over MPI in mind and used to establish a new primality record, for the ``repunit''$(10^{86453} - 1) / 9$."],"url":"http://arxiv.org/abs/2404.05506v1","category":"math.NT"}
{"created":"2024-04-08 13:27:07","title":"Taming Transformers for Realistic Lidar Point Cloud Generation","abstract":"Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling. However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process. To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks. Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets. Code available at:https://github.com/hamedhaghighi/LidarGRIT.","sentences":["Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling.","However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process.","To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space.","Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks.","Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets.","Code available at:https://github.com/hamedhaghighi/LidarGRIT."],"url":"http://arxiv.org/abs/2404.05505v1","category":"cs.CV"}
{"created":"2024-04-08 13:25:47","title":"Tables in Leibniz: a challenge for the digital humanities","abstract":"Leibniz's mathematical texts are a perfect example of a type of historical document that is extremely difficult to deal with in the context of an editorial enterprise: the draft. The tables in Leibniz's mathematical manuscripts are a particularly good example of these difficulties, as they are equivocal sources containing many implicit operations. The publication of these texts raises the question of the nature of these signs and the economy of implicit relationships between their various components. Peirce's semiological approach provides the philosophical ground for these reflections, while Michel Serres's structuralism is a fertile source of inspiration. The digital tool holds much promise for many issues, including the particular difficulties of tables. We will show that it can be implemented by different computer structures which largely determine the way the historian conceives them a priori but also the way the reader receives them a posteriori. Finally, the tables are the simple case that founds a general problematic on the interpretation of many manuscripts and allows us to study the problem of the writing process at its root.","sentences":["Leibniz's mathematical texts are a perfect example of a type of historical document that is extremely difficult to deal with in the context of an editorial enterprise: the draft.","The tables in Leibniz's mathematical manuscripts are a particularly good example of these difficulties, as they are equivocal sources containing many implicit operations.","The publication of these texts raises the question of the nature of these signs and the economy of implicit relationships between their various components.","Peirce's semiological approach provides the philosophical ground for these reflections, while Michel Serres's structuralism is a fertile source of inspiration.","The digital tool holds much promise for many issues, including the particular difficulties of tables.","We will show that it can be implemented by different computer structures which largely determine the way the historian conceives them a priori but also the way the reader receives them a posteriori.","Finally, the tables are the simple case that founds a general problematic on the interpretation of many manuscripts and allows us to study the problem of the writing process at its root."],"url":"http://arxiv.org/abs/2404.05504v1","category":"math.HO"}
{"created":"2024-04-08 13:25:03","title":"PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM for Emotion-Cause Pair Extraction in Conversations","abstract":"In this paper, we present our submission to the SemEval-2023 Task~3 \"The Competition of Multimodal Emotion Cause Analysis in Conversations\", focusing on extracting emotion-cause pairs from dialogs. Specifically, our approach relies on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based neural network to detect causes. We score 2nd in the ranking for Subtask 1, demonstrating the effectiveness of our approach through one of the highest weighted-average proportional F1 scores recorded at 0.264.","sentences":["In this paper, we present our submission to the SemEval-2023 Task~3 \"The Competition of Multimodal Emotion Cause Analysis in Conversations\", focusing on extracting emotion-cause pairs from dialogs.","Specifically, our approach relies on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based neural network to detect causes.","We score 2nd in the ranking for Subtask 1, demonstrating the effectiveness of our approach through one of the highest weighted-average proportional F1 scores recorded at 0.264."],"url":"http://arxiv.org/abs/2404.05502v1","category":"cs.CL"}
{"created":"2024-04-08 13:25:02","title":"Data Science In Olfaction","abstract":"Advances in neural sensing technology are making it possible to observe the olfactory process in great detail. In this paper, we conceptualize smell from a Data Science and AI perspective, that relates the properties of odorants to how they are sensed and analyzed in the olfactory system from the nose to the brain. Drawing distinctions to color vision, we argue that smell presents unique measurement challenges, including the complexity of stimuli, the high dimensionality of the sensory apparatus, as well as what constitutes ground truth. In the face of these challenges, we argue for the centrality of odorant-receptor interactions in developing a theory of olfaction. Such a theory is likely to find widespread industrial applications, and enhance our understanding of smell, and in the longer-term, how it relates to other senses and language. As an initial use case of the data, we present results using machine learning-based classification of neural responses to odors as they are recorded in the mouse olfactory bulb with calcium imaging.","sentences":["Advances in neural sensing technology are making it possible to observe the olfactory process in great detail.","In this paper, we conceptualize smell from a Data Science and AI perspective, that relates the properties of odorants to how they are sensed and analyzed in the olfactory system from the nose to the brain.","Drawing distinctions to color vision, we argue that smell presents unique measurement challenges, including the complexity of stimuli, the high dimensionality of the sensory apparatus, as well as what constitutes ground truth.","In the face of these challenges, we argue for the centrality of odorant-receptor interactions in developing a theory of olfaction.","Such a theory is likely to find widespread industrial applications, and enhance our understanding of smell, and in the longer-term, how it relates to other senses and language.","As an initial use case of the data, we present results using machine learning-based classification of neural responses to odors as they are recorded in the mouse olfactory bulb with calcium imaging."],"url":"http://arxiv.org/abs/2404.05501v1","category":"q-bio.NC"}
{"created":"2024-04-08 13:22:24","title":"Constraining Large Language Model for Generating Computer-Parsable Content","abstract":"We propose a method to guide Large Language Models (LLMs) in generating structured content adhering to specific conventions without fine-tuning. By utilizing coroutine-based content generation constraints through a pre-agreed context-free grammar (CFG), LLMs are directed during decoding to produce formal language compliant outputs. This enhances stability and consistency in generating target data structures, types, or instructions, reducing application development complexities. Experimentally, error rates of GPT-2 and Gemma exceed 95% for DSLs longer than 36 and 282 tokens, respectively. We introduce YieldLang, a coroutine-based DSL generation framework, and evaluate it with LLMs on various tasks including JSON and Mermaid flowchart generation. Compared to benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs requiring only about 16.5% of the samples to generate JSON effectively. This enhances usability of LLM-generated content for computer programs.","sentences":["We propose a method to guide Large Language Models (LLMs) in generating structured content adhering to specific conventions without fine-tuning.","By utilizing coroutine-based content generation constraints through a pre-agreed context-free grammar (CFG), LLMs are directed during decoding to produce formal language compliant outputs.","This enhances stability and consistency in generating target data structures, types, or instructions, reducing application development complexities.","Experimentally, error rates of GPT-2 and Gemma exceed 95% for DSLs longer than 36 and 282 tokens, respectively.","We introduce YieldLang, a coroutine-based DSL generation framework, and evaluate it with LLMs on various tasks including JSON and Mermaid flowchart generation.","Compared to benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs requiring only about 16.5% of the samples to generate JSON effectively.","This enhances usability of LLM-generated content for computer programs."],"url":"http://arxiv.org/abs/2404.05499v1","category":"cs.SE"}
{"created":"2024-04-08 13:20:51","title":"The Fortuin-Kasteleyn polynomial as a bialgebra morphism and applications to the Tutte polynomial","abstract":"We compute an explicit formula for the antipode of the double bialgebra of graphs in terms of totally acyclic partial orientations, using some general results on double bialgebras. In analogy to what was already proven in Hopf-algebraic terms for the chromatic polynomial of a graph, we show that the Fortuin-Kasteleyn polynomial (a variant of the Tutte polynomial) is a morphism of the double algebra of graphs into that of polynomials, which generalizes the chromatic polynomial. When specialized at particular values, we give combinatorial interpretations of the Tutte polynomial of a graph, via covering graphs and covering forests, and of the Fortuin-Kasteleyn polynomial, via pairs of vertex--edge colorings. Finally we show that the map associating to a graph all its orientations is a Hopf morphism from the double bialgebra of graphs into that one of oriented graphs, allowing to give interpretations of the Fortuin-Kasteleyn polynomial when computed at negative values.","sentences":["We compute an explicit formula for the antipode of the double bialgebra of graphs in terms of totally acyclic partial orientations, using some general results on double bialgebras.","In analogy to what was already proven in Hopf-algebraic terms for the chromatic polynomial of a graph, we show that the Fortuin-Kasteleyn polynomial (a variant of the Tutte polynomial) is a morphism of the double algebra of graphs into that of polynomials, which generalizes the chromatic polynomial.","When specialized at particular values, we give combinatorial interpretations of the Tutte polynomial of a graph, via covering graphs and covering forests, and of the Fortuin-Kasteleyn polynomial, via pairs of vertex--edge colorings.","Finally we show that the map associating to a graph all its orientations is a Hopf morphism from the double bialgebra of graphs into that one of oriented graphs, allowing to give interpretations of the Fortuin-Kasteleyn polynomial when computed at negative values."],"url":"http://arxiv.org/abs/2404.05497v1","category":"math.CO"}
{"created":"2024-04-08 13:17:23","title":"Decisioning Workshop 2023","abstract":"In a knowledge society, the term knowledge must be considered a core resource for organizations. So, beyond being a medium to progress and to innovate, knowledge is one of our most important resources: something necessary to decide.Organizations that are embracing knowledge retention activities are gaining a competitive advantage. Organizational rearrangements from companies, notably outsourcing, increase a possible loss of knowledge, making knowledge retention an essential need for them. When Knowledge is less shared, collaborative decision-making seems harder to obtain insofar as a ``communication breakdown'' characterizes participants' discourse. At best, stakeholders have to finda consensus according to their knowledge. Sharing knowledge ensures its retention and catalyzes the construction of this consensus.   Our vision of collaborative decision-making aims not only at increasing the quality of the first parts of the decision-making process: intelligence and design, but also at increasing the acceptance of the choice. Intelligence and design will be done by more than one individual and constructed together; the decision is more easily accepted. The decided choice will then be shared. Thereby where decision-making could be seen as a constructed model, collaborative decision-making, for us,is seen as the use of socio-technical media to improve decision-making performance and acceptability. The shared decision making is a core activity in a lot of human activities. For example, the sustainable decision-making is the job of not only governments and institutions but also broader society. Recognizing the urgent need for sustainability, we can argue that to realize sustainable development, it must be considered as a decision-making strategy. The location of knowledge in the realization of collaborative decision-making has to be regarded insofar as knowledge sharing leads to improve collaborative decision-making: a ``static view'' has to be structured and constitutes the ``collaborative knowledge.'' Knowledge has an important role in individual decision-making, and we consider that for collaborative decision-making, knowledge has to be shared. What is required is a better understanding of the nature of group work''. Knowledge has to be shared, but how do we share knowledge?","sentences":["In a knowledge society, the term knowledge must be considered a core resource for organizations.","So, beyond being a medium to progress and to innovate, knowledge is one of our most important resources: something necessary to decide.","Organizations that are embracing knowledge retention activities are gaining a competitive advantage.","Organizational rearrangements from companies, notably outsourcing, increase a possible loss of knowledge, making knowledge retention an essential need for them.","When Knowledge is less shared, collaborative decision-making seems harder to obtain insofar as a ``communication breakdown'' characterizes participants' discourse.","At best, stakeholders have to finda consensus according to their knowledge.","Sharing knowledge ensures its retention and catalyzes the construction of this consensus.   ","Our vision of collaborative decision-making aims not only at increasing the quality of the first parts of the decision-making process: intelligence and design, but also at increasing the acceptance of the choice.","Intelligence and design will be done by more than one individual and constructed together; the decision is more easily accepted.","The decided choice will then be shared.","Thereby where decision-making could be seen as a constructed model, collaborative decision-making, for us,is seen as the use of socio-technical media to improve decision-making performance and acceptability.","The shared decision making is a core activity in a lot of human activities.","For example, the sustainable decision-making is the job of not only governments and institutions but also broader society.","Recognizing the urgent need for sustainability, we can argue that to realize sustainable development, it must be considered as a decision-making strategy.","The location of knowledge in the realization of collaborative decision-making has to be regarded insofar as knowledge sharing leads to improve collaborative decision-making: a ``static view'' has to be structured and constitutes the ``collaborative knowledge.''","Knowledge has an important role in individual decision-making, and we consider that for collaborative decision-making, knowledge has to be shared.","What is required is a better understanding of the nature of group work''.","Knowledge has to be shared, but how do we share knowledge?"],"url":"http://arxiv.org/abs/2404.05495v1","category":"cs.CY"}
{"created":"2024-04-08 13:12:38","title":"Vacuum amplitudes and time-like causal unitary in the loop-tree duality","abstract":"We present the first proof-of-concept application to decay processes at higher perturbative orders of LTD causal unitary, a novel methodology that exploits the causal properties of vacuum amplitudes in the loop-tree duality (LTD) and is directly well-defined in the four physical dimensions of the space-time. The generation of loop- and tree-level contributions to the differential decay rates from a kernel multiloop vacuum amplitude is shown in detail, and explicit expressions are presented for selected processes that are suitable for a lightweight understanding of the method. Specifically, we provide a clear physical interpretation of the local cancellation of soft, collinear and unitary threshold singularities, and of the local renormalisation of ultraviolet singularities. The presentation is illustrated with numerical results that showcase the advantages of the method.","sentences":["We present the first proof-of-concept application to decay processes at higher perturbative orders of LTD causal unitary, a novel methodology that exploits the causal properties of vacuum amplitudes in the loop-tree duality (LTD) and is directly well-defined in the four physical dimensions of the space-time.","The generation of loop- and tree-level contributions to the differential decay rates from a kernel multiloop vacuum amplitude is shown in detail, and explicit expressions are presented for selected processes that are suitable for a lightweight understanding of the method.","Specifically, we provide a clear physical interpretation of the local cancellation of soft, collinear and unitary threshold singularities, and of the local renormalisation of ultraviolet singularities.","The presentation is illustrated with numerical results that showcase the advantages of the method."],"url":"http://arxiv.org/abs/2404.05492v1","category":"hep-ph"}
{"created":"2024-04-08 13:11:57","title":"Two-Person Interaction Augmentation with Skeleton Priors","abstract":"Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.","sentences":["Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc.","However, acquiring such skeletal motion is challenging.","While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained.","To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies.","Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes.","Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions."],"url":"http://arxiv.org/abs/2404.05490v1","category":"cs.CV"}
{"created":"2024-04-08 13:10:21","title":"Monogenic Octic Polynomials and Their Galois Groups","abstract":"A monic polynomial $f(x)\\in {\\mathbb Z}[x]$ of degree $N$ is called monogenic if $f(x)$ is irreducible over ${\\mathbb Q}$ and $\\{1,\\theta,\\theta^2,\\ldots ,\\theta^{N-1}\\}$ is a basis for the ring of integers of ${\\mathbb Q}(\\theta)$, where $f(\\theta)=0$. In this article, we use the classification of the Galois groups of quartic polynomials, due to Kappe and Warren, to investigate the existence of infinite collections of monogenic quartic polynomials having a prescribed Galois group, such that each member of the collection generates a distinct quartic field. With the exception of the cyclic case, we provide such an infinite single-parameter collection for each possible Galois group. We believe these examples are new, and we provide evidence to support this belief by showing that they are distinct from other infinite collections in the current literature. Finally, we devote a separate section to a discussion concerning, what we believe to be, the still-unresolved cyclic case.","sentences":["A monic polynomial $f(x)\\in {\\mathbb Z}[x]$ of degree $N$ is called monogenic if $f(x)$ is irreducible over ${\\mathbb Q}$ and $\\{1,\\theta,\\theta^2,\\ldots ,\\theta^{N-1}\\}$ is a basis for the ring of integers of ${\\mathbb Q}(\\theta)$, where $f(\\theta)=0$. In this article, we use the classification of the Galois groups of quartic polynomials, due to Kappe and Warren, to investigate the existence of infinite collections of monogenic quartic polynomials having a prescribed Galois group, such that each member of the collection generates a distinct quartic field.","With the exception of the cyclic case, we provide such an infinite single-parameter collection for each possible Galois group.","We believe these examples are new, and we provide evidence to support this belief by showing that they are distinct from other infinite collections in the current literature.","Finally, we devote a separate section to a discussion concerning, what we believe to be, the still-unresolved cyclic case."],"url":"http://arxiv.org/abs/2404.05487v1","category":"math.NT"}
{"created":"2024-04-08 13:06:23","title":"Tangling-Untangling Cycle for Efficient Learning","abstract":"The conventional wisdom of manifold learning is based on nonlinear dimensionality reduction techniques such as IsoMAP and locally linear embedding (LLE). We challenge this paradigm by exploiting the blessing of dimensionality. Our intuition is simple: it is easier to untangle a low-dimensional manifold in a higher-dimensional space due to its vastness, as guaranteed by Whitney embedding theorem. A new insight brought by this work is to introduce class labels as the context variables in the lifted higher-dimensional space (so supervised learning becomes unsupervised learning). We rigorously show that manifold untangling leads to linearly separable classifiers in the lifted space. To correct the inevitable overfitting, we consider the dual process of manifold untangling -- tangling or aliasing -- which is important for generalization. Using context as the bonding element, we construct a pair of manifold untangling and tangling operators, known as tangling-untangling cycle (TUC). Untangling operator maps context-independent representations (CIR) in low-dimensional space to context-dependent representations (CDR) in high-dimensional space by inducing context as hidden variables. The tangling operator maps CDR back to CIR by a simple integral transformation for invariance and generalization. We also present the hierarchical extensions of TUC based on the Cartesian product and the fractal geometry. Despite the conceptual simplicity, TUC admits a biologically plausible and energy-efficient implementation based on the time-locking behavior of polychronization neural groups (PNG) and sleep-wake cycle (SWC). The TUC-based theory applies to the computational modeling of various cognitive functions by hippocampal-neocortical systems.","sentences":["The conventional wisdom of manifold learning is based on nonlinear dimensionality reduction techniques such as IsoMAP and locally linear embedding (LLE).","We challenge this paradigm by exploiting the blessing of dimensionality.","Our intuition is simple: it is easier to untangle a low-dimensional manifold in a higher-dimensional space due to its vastness, as guaranteed by Whitney embedding theorem.","A new insight brought by this work is to introduce class labels as the context variables in the lifted higher-dimensional space (so supervised learning becomes unsupervised learning).","We rigorously show that manifold untangling leads to linearly separable classifiers in the lifted space.","To correct the inevitable overfitting, we consider the dual process of manifold untangling -- tangling or aliasing -- which is important for generalization.","Using context as the bonding element, we construct a pair of manifold untangling and tangling operators, known as tangling-untangling cycle (TUC).","Untangling operator maps context-independent representations (CIR) in low-dimensional space to context-dependent representations (CDR) in high-dimensional space by inducing context as hidden variables.","The tangling operator maps CDR back to CIR by a simple integral transformation for invariance and generalization.","We also present the hierarchical extensions of TUC based on the Cartesian product and the fractal geometry.","Despite the conceptual simplicity, TUC admits a biologically plausible and energy-efficient implementation based on the time-locking behavior of polychronization neural groups (PNG) and sleep-wake cycle (SWC).","The TUC-based theory applies to the computational modeling of various cognitive functions by hippocampal-neocortical systems."],"url":"http://arxiv.org/abs/2404.05484v1","category":"cs.LG"}
{"created":"2024-04-08 13:05:02","title":"PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of LLM-generated Text?","abstract":"In this paper, we present our submission to the SemEval-2024 Task 8 \"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection\", focusing on the detection of machine-generated texts (MGTs) in English. Specifically, our approach relies on combining embeddings from the RoBERTa-base with diversity features and uses a resampled training set. We score 12th from 124 in the ranking for Subtask A (monolingual track), and our results show that our approach is generalizable across unseen models and domains, achieving an accuracy of 0.91.","sentences":["In this paper, we present our submission to the SemEval-2024 Task 8 \"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection\", focusing on the detection of machine-generated texts (MGTs) in English.","Specifically, our approach relies on combining embeddings from the RoBERTa-base with diversity features and uses a resampled training set.","We score 12th from 124 in the ranking for Subtask A (monolingual track), and our results show that our approach is generalizable across unseen models and domains, achieving an accuracy of 0.91."],"url":"http://arxiv.org/abs/2404.05483v1","category":"cs.CL"}
{"created":"2024-04-08 13:00:32","title":"General position sets, colinear sets, and Sierpi\u0144ski product graphs","abstract":"Let $G \\otimes _f H$ denote the Sierpi\\'nski product of $G$ and $H$ with respect to the function $f$. The Sierpi\\'nski general position number ${\\rm gp}{_{\\rm S}}(G,H)$ is introduced as the cardinality of a largest general position set in $G \\otimes _f H$ over all possible functions $f$. Similarly, the lower Sierpi\\'nski general position number $\\underline{{\\rm gp}}{_{\\rm S}}(G,H)$ is the corresponding smallest cardinality. The concept of vertex-colinear sets is introduced. Bounds for the general position number in terms of extremal vertex-colinear sets, and bounds for the (lower) Sierpi\\'nski general position number are proved. The extremal graphs are investigated. Formulas for the (lower) Sierpi\\'nski general position number of the \\SP{s} with $K_2$ as the first factor are deduced. It is proved that if $m,n\\geq 2$, then ${\\rm gp}{_{\\rm S}}(K_m,K_n) = m(n-1)$ and that if $n\\ge 2m-2$, then $\\underline{{\\rm gp}}{_{\\rm S}}(K_m,K_n) = m(n-m+1)$.","sentences":["Let $G \\otimes _f H$ denote the Sierpi\\'nski product of $G$ and $H$ with respect to the function $f$. The Sierpi\\'nski general position number ${\\rm gp}{_{\\rm S}}(G,H)$ is introduced as the cardinality of a largest general position set in $G \\otimes _f H$ over all possible functions $f$. Similarly, the lower Sierpi\\'nski general position number $\\underline{{\\rm gp}}{_{\\rm S}}(G,H)$ is the corresponding smallest cardinality.","The concept of vertex-colinear sets is introduced.","Bounds for the general position number in terms of extremal vertex-colinear sets, and bounds for the (lower) Sierpi\\'nski general position number are proved.","The extremal graphs are investigated.","Formulas for the (lower) Sierpi\\'nski general position number of the \\SP{s} with $K_2$ as the first factor are deduced.","It is proved that if $m,n\\geq 2$, then ${\\rm gp}{_{\\rm S}}(K_m,K_n) = m(n-1)$ and that if $n\\ge 2m-2$, then $\\underline{{\\rm gp}}{_{\\rm S}}(K_m,K_n) = m(n-m+1)$."],"url":"http://arxiv.org/abs/2404.05481v1","category":"math.CO"}
{"created":"2024-04-08 12:55:00","title":"Behavioural Types for Heterogeneous Systems (Position Paper)","abstract":"Behavioural types provide a promising way to achieve lightweight, language-integrated verification for communication-centric software. However, a large barrier to the adoption of behavioural types is that the current state of the art expects software to be written using the same tools and typing discipline throughout a system, and has little support for components over which a developer has no control.   This position paper describes the outcomes of a working group discussion at Dagstuhl Seminar 24051 (Next-Generation Protocols for Heterogeneous Systems). We propose a methodology for integrating multiple behaviourally-typed components, written in different languages. Our proposed approach involves an extensible protocol description language, a session IR that can describe data transformations and boundary monitoring and which can be compiled into program-specific session proxies, and finally a session middleware to aid session establishment.   We hope that this position paper will stimulate discussion on one of the most pressing challenges facing the widespread adoption of behavioural typing.","sentences":["Behavioural types provide a promising way to achieve lightweight, language-integrated verification for communication-centric software.","However, a large barrier to the adoption of behavioural types is that the current state of the art expects software to be written using the same tools and typing discipline throughout a system, and has little support for components over which a developer has no control.   ","This position paper describes the outcomes of a working group discussion at Dagstuhl Seminar 24051 (Next-Generation Protocols for Heterogeneous Systems).","We propose a methodology for integrating multiple behaviourally-typed components, written in different languages.","Our proposed approach involves an extensible protocol description language, a session IR that can describe data transformations and boundary monitoring and which can be compiled into program-specific session proxies, and finally a session middleware to aid session establishment.   ","We hope that this position paper will stimulate discussion on one of the most pressing challenges facing the widespread adoption of behavioural typing."],"url":"http://arxiv.org/abs/2404.05479v1","category":"cs.PL"}
{"created":"2024-04-08 12:53:42","title":"Photon bunching in high-harmonic emission controlled by quantum light","abstract":"Attosecond spectroscopy comprises several techniques to probe matter through electrons and photons. One frontier of attosecond methods is to reveal complex phenomena arising from quantum-mechanical correlations in the matter system, in the photon fields and among them. Recent theories have laid the groundwork for understanding how quantum-optical properties affect high-field photonics, such as strong-field ionization and acceleration of electrons in quantum-optical fields, and how entanglement between the field modes arises during the interaction. Here we demonstrate a new experimental approach that transduces some properties of a quantum-optical state through a strong-field nonlinearity. We perturb high-harmonic emission from a semiconductor with a bright squeezed vacuum field resulting in the emission of sidebands of the high-harmonics with super-Poissonian statistics, indicating that the emitted photons are bunched. Our results suggest that perturbing strong-field dynamics with quantum-optical states is a viable way to coherently control the generation of these states at short wavelengths, such as extreme ultraviolet or soft X-rays. Quantum correlations will be instrumental to advance attosecond spectroscopy and imaging beyond the classical limits.","sentences":["Attosecond spectroscopy comprises several techniques to probe matter through electrons and photons.","One frontier of attosecond methods is to reveal complex phenomena arising from quantum-mechanical correlations in the matter system, in the photon fields and among them.","Recent theories have laid the groundwork for understanding how quantum-optical properties affect high-field photonics, such as strong-field ionization and acceleration of electrons in quantum-optical fields, and how entanglement between the field modes arises during the interaction.","Here we demonstrate a new experimental approach that transduces some properties of a quantum-optical state through a strong-field nonlinearity.","We perturb high-harmonic emission from a semiconductor with a bright squeezed vacuum field resulting in the emission of sidebands of the high-harmonics with super-Poissonian statistics, indicating that the emitted photons are bunched.","Our results suggest that perturbing strong-field dynamics with quantum-optical states is a viable way to coherently control the generation of these states at short wavelengths, such as extreme ultraviolet or soft X-rays.","Quantum correlations will be instrumental to advance attosecond spectroscopy and imaging beyond the classical limits."],"url":"http://arxiv.org/abs/2404.05474v1","category":"quant-ph"}
{"created":"2024-04-08 12:53:24","title":"Modeling the effects of perturbations and steepest entropy ascent on the time evolution of entanglement","abstract":"This work presents an analysis of the evolution of perturbed Bell diagonal states using the equation of motion of steepest-entropy-ascent quantum thermodynamics (SEAQT), the Lindblad equation, and various measures of loss of entanglement. First, a brief derivation is presented showing that Bell diagonal states are stationary states that are not stable equilibrium states relative to the SEAQT equation of motion, highlighting the need for the development of perturbation methods to study the evolutions of nearby states. This contrasts with the Lindblad equation of motion for which only some of the Bell diagonal states are stationary. Next, two perturbation methods are presented. The first is a general method for perturbing bipartite systems and the second is a method based on a set of unitary operations that are constrained to hold the system energy and system entropy constant. Sets of density operators are randomly generated with each method and the resulting time-varying characteristics of the system's entanglement are analyzed. The findings reveal that the constrained perturbation accurately predicts the loss of non-locality and aligns well with the measured concurrence.","sentences":["This work presents an analysis of the evolution of perturbed Bell diagonal states using the equation of motion of steepest-entropy-ascent quantum thermodynamics (SEAQT), the Lindblad equation, and various measures of loss of entanglement.","First, a brief derivation is presented showing that Bell diagonal states are stationary states that are not stable equilibrium states relative to the SEAQT equation of motion, highlighting the need for the development of perturbation methods to study the evolutions of nearby states.","This contrasts with the Lindblad equation of motion for which only some of the Bell diagonal states are stationary.","Next, two perturbation methods are presented.","The first is a general method for perturbing bipartite systems and the second is a method based on a set of unitary operations that are constrained to hold the system energy and system entropy constant.","Sets of density operators are randomly generated with each method and the resulting time-varying characteristics of the system's entanglement are analyzed.","The findings reveal that the constrained perturbation accurately predicts the loss of non-locality and aligns well with the measured concurrence."],"url":"http://arxiv.org/abs/2404.05473v1","category":"quant-ph"}
{"created":"2024-04-08 12:50:52","title":"Quench dynamics of interacting bosons: generalized coherent states versus multi-mode Glauber states","abstract":"Multi-mode Glauber coherent states (MMGS) as well as Bloch states with zero quasi-momentum, which are a special case of generalized coherent states (GCS), are frequently used to describe condensed phases of bosonic many-body systems. The difference of two-point correlators of MMGS and GCS vanishes in the thermodynamic limit. Using the established expansion of GCS in terms of MMGS, we derive a Fourier-type relation between the (auto-)correlation functions of the two different time-evolved states. This relation reveals that the (auto-)correlation and thus the dynamical free energy density for the two cases are still different, even in the thermodynamic limit, due to the lack of the U(1) symmetry of the MMGS. Analytic results for the deep lattice model of interacting bosons for increasing filling factors show multiple sharp structures in the dynamical free energy-density of increasing complexity. These are explained using the evolution of Husimi functions in phase space.","sentences":["Multi-mode Glauber coherent states (MMGS) as well as Bloch states with zero quasi-momentum, which are a special case of generalized coherent states (GCS), are frequently used to describe condensed phases of bosonic many-body systems.","The difference of two-point correlators of MMGS and GCS vanishes in the thermodynamic limit.","Using the established expansion of GCS in terms of MMGS, we derive a Fourier-type relation between the (auto-)correlation functions of the two different time-evolved states.","This relation reveals that the (auto-)correlation and thus the dynamical free energy density for the two cases are still different, even in the thermodynamic limit, due to the lack of the U(1) symmetry of the MMGS.","Analytic results for the deep lattice model of interacting bosons for increasing filling factors show multiple sharp structures in the dynamical free energy-density of increasing complexity.","These are explained using the evolution of Husimi functions in phase space."],"url":"http://arxiv.org/abs/2404.05471v1","category":"quant-ph"}
{"created":"2024-04-08 12:50:36","title":"Towards Reconfigurable Linearizable Reads","abstract":"Linearizable datastores are desirable because they provide users with the illusion that the datastore is run on a single machine that performs client operations one at a time. To reduce the performance cost of providing this illusion, many specialized algorithms for linearizable reads have been proposed which significantly improve read performance compared to write performance. The main difference between these specialized algorithms is their performance under different workloads. Unfortunately, since a datastore's workload is often unknown or changes over time and system designers must decide on a single read algorithm to implement ahead of time, a datastore's performance is often suboptimal as it cannot adapt to workload changes. In this paper, we lay the groundwork for addressing this problem by proposing Chameleon, an algorithm for linearizable reads that provides a principled approach for datastores to switch between existing read algorithms at runtime. The key observation that enables this generalization is that all existing algorithms are specific read-write quorum systems. Chameleon constructs a generic read-write quorum system, by using tokens that are included to complete write and read operations. This token quorum system enables Chameleon to mimic existing read algorithms and switch between them by transferring these tokens between processes.","sentences":["Linearizable datastores are desirable because they provide users with the illusion that the datastore is run on a single machine that performs client operations one at a time.","To reduce the performance cost of providing this illusion, many specialized algorithms for linearizable reads have been proposed which significantly improve read performance compared to write performance.","The main difference between these specialized algorithms is their performance under different workloads.","Unfortunately, since a datastore's workload is often unknown or changes over time and system designers must decide on a single read algorithm to implement ahead of time, a datastore's performance is often suboptimal as it cannot adapt to workload changes.","In this paper, we lay the groundwork for addressing this problem by proposing Chameleon, an algorithm for linearizable reads that provides a principled approach for datastores to switch between existing read algorithms at runtime.","The key observation that enables this generalization is that all existing algorithms are specific read-write quorum systems.","Chameleon constructs a generic read-write quorum system, by using tokens that are included to complete write and read operations.","This token quorum system enables Chameleon to mimic existing read algorithms and switch between them by transferring these tokens between processes."],"url":"http://arxiv.org/abs/2404.05470v1","category":"cs.DC"}
{"created":"2024-04-08 12:47:59","title":"Concerning the stability of exponential systems and Fourier matrices","abstract":"Fourier matrices naturally appear in many applications and their stability is closely tied to performance guarantees of algorithms. The starting point of this article is a result that characterizes properties of an exponential system on a union of cubes in $\\mathbb{R}^d$ in terms of a general class of Fourier matrices and their extreme singular values. This relationship is flexible in the sense that it holds for any dimension $d$, for many types of exponential systems (Riesz bases, Riesz sequences, or frames) and for Fourier matrices with an arbitrary number of rows and columns. From there, we prove new stability results for Fourier matrices by exploiting this connection and using powerful stability theorems for exponential systems. This paper provides a systematic exploration of this connection and suggests some natural open questions.","sentences":["Fourier matrices naturally appear in many applications and their stability is closely tied to performance guarantees of algorithms.","The starting point of this article is a result that characterizes properties of an exponential system on a union of cubes in $\\mathbb{R}^d$ in terms of a general class of Fourier matrices and their extreme singular values.","This relationship is flexible in the sense that it holds for any dimension $d$, for many types of exponential systems (Riesz bases, Riesz sequences, or frames) and for Fourier matrices with an arbitrary number of rows and columns.","From there, we prove new stability results for Fourier matrices by exploiting this connection and using powerful stability theorems for exponential systems.","This paper provides a systematic exploration of this connection and suggests some natural open questions."],"url":"http://arxiv.org/abs/2404.05469v1","category":"math.CA"}
{"created":"2024-04-08 12:46:39","title":"Mind-to-Image: Projecting Visual Mental Imagination of the Brain from fMRI","abstract":"The reconstruction of images observed by subjects from fMRI data collected during visual stimuli has made significant strides in the past decade, thanks to the availability of extensive fMRI datasets and advancements in generative models for image generation. However, the application of visual reconstruction has remained limited. Reconstructing visual imagination presents a greater challenge, with potentially revolutionary applications ranging from aiding individuals with disabilities to verifying witness accounts in court. The primary hurdles in this field are the absence of data collection protocols for visual imagery and the lack of datasets on the subject. Traditionally, fMRI-to-image relies on data collected from subjects exposed to visual stimuli, which poses issues for generating visual imagery based on the difference of brain activity between visual stimulation and visual imagery. For the first time, we have compiled a substantial dataset (around 6h of scans) on visual imagery along with a proposed data collection protocol. We then train a modified version of an fMRI-to-image model and demonstrate the feasibility of reconstructing images from two modes of imagination: from memory and from pure imagination. This marks an important step towards creating a technology that allow direct reconstruction of visual imagery.","sentences":["The reconstruction of images observed by subjects from fMRI data collected during visual stimuli has made significant strides in the past decade, thanks to the availability of extensive fMRI datasets and advancements in generative models for image generation.","However, the application of visual reconstruction has remained limited.","Reconstructing visual imagination presents a greater challenge, with potentially revolutionary applications ranging from aiding individuals with disabilities to verifying witness accounts in court.","The primary hurdles in this field are the absence of data collection protocols for visual imagery and the lack of datasets on the subject.","Traditionally, fMRI-to-image relies on data collected from subjects exposed to visual stimuli, which poses issues for generating visual imagery based on the difference of brain activity between visual stimulation and visual imagery.","For the first time, we have compiled a substantial dataset (around 6h of scans) on visual imagery along with a proposed data collection protocol.","We then train a modified version of an fMRI-to-image model and demonstrate the feasibility of reconstructing images from two modes of imagination: from memory and from pure imagination.","This marks an important step towards creating a technology that allow direct reconstruction of visual imagery."],"url":"http://arxiv.org/abs/2404.05468v1","category":"q-bio.NC"}
{"created":"2024-04-08 12:43:32","title":"HAMMR: HierArchical MultiModal React agents for generic VQA","abstract":"Combining Large Language Models (LLMs) with external specialized tools (LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual Question Answering (VQA). While this approach was demonstrated to work well when optimized and evaluated for each individual benchmark, in practice it is crucial for the next generation of real-world AI systems to handle a broad range of multimodal problems. Therefore we pose the VQA problem from a unified perspective and evaluate a single system on a varied suite of VQA tasks including counting, spatial reasoning, OCR-based reasoning, visual pointing, external knowledge, and more. In this setting, we demonstrate that naively applying the LLM+tools approach using the combined set of all tools leads to poor results. This motivates us to introduce HAMMR: HierArchical MultiModal React. We start from a multimodal ReAct-based system and make it hierarchical by enabling our HAMMR agents to call upon other specialized agents. This enhances the compositionality of the LLM+tools approach, which we show to be critical for obtaining high accuracy on generic VQA. Concretely, on our generic VQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%. Additionally, HAMMR achieves state-of-the-art results on this task, outperforming the generic standalone PaLI-X VQA model by 5.0%.","sentences":["Combining Large Language Models (LLMs) with external specialized tools (LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual Question Answering (VQA).","While this approach was demonstrated to work well when optimized and evaluated for each individual benchmark, in practice it is crucial for the next generation of real-world AI systems to handle a broad range of multimodal problems.","Therefore we pose the VQA problem from a unified perspective and evaluate a single system on a varied suite of VQA tasks including counting, spatial reasoning, OCR-based reasoning, visual pointing, external knowledge, and more.","In this setting, we demonstrate that naively applying the LLM+tools approach using the combined set of all tools leads to poor results.","This motivates us to introduce HAMMR:","HierArchical MultiModal React.","We start from a multimodal ReAct-based system and make it hierarchical by enabling our HAMMR agents to call upon other specialized agents.","This enhances the compositionality of the LLM+tools approach, which we show to be critical for obtaining high accuracy on generic VQA.","Concretely, on our generic VQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.","Additionally, HAMMR achieves state-of-the-art results on this task, outperforming the generic standalone PaLI-X VQA model by 5.0%."],"url":"http://arxiv.org/abs/2404.05465v1","category":"cs.CV"}
{"created":"2024-04-08 12:42:02","title":"Parity violation in primordial tensor non-Gaussianities from matter bounce cosmology","abstract":"It has been shown that primordial tensor non-Gaussianities from a cubic Weyl action with a non-dynamical coupling are suppressed by the so-called slow-roll parameter in a conventional framework of slow-roll inflation. In this paper, we consider matter bounce cosmology in which the background spacetime is no longer quasi-de Sitter, and hence one might expect that the matter bounce models could predict non-suppressed non-Gaussianities. Nevertheless, we first show that the corresponding non-Gaussian amplitudes from the cubic Weyl term with a non-dynamical coupling are much smaller than those from the conventional slow-roll inflation, in spite of the fact that there is no slow-roll suppression. We then introduce a dynamical coupling that can boost the magnitude of graviton cubic interactions and clarify that there is a parameter region where the tensor non-Gaussianities can be enhanced and can potentially be tested by cosmic microwave background experiments.","sentences":["It has been shown that primordial tensor non-Gaussianities from a cubic Weyl action with a non-dynamical coupling are suppressed by the so-called slow-roll parameter in a conventional framework of slow-roll inflation.","In this paper, we consider matter bounce cosmology in which the background spacetime is no longer quasi-de Sitter, and hence one might expect that the matter bounce models could predict non-suppressed non-Gaussianities.","Nevertheless, we first show that the corresponding non-Gaussian amplitudes from the cubic Weyl term with a non-dynamical coupling are much smaller than those from the conventional slow-roll inflation, in spite of the fact that there is no slow-roll suppression.","We then introduce a dynamical coupling that can boost the magnitude of graviton cubic interactions and clarify that there is a parameter region where the tensor non-Gaussianities can be enhanced and can potentially be tested by cosmic microwave background experiments."],"url":"http://arxiv.org/abs/2404.05464v1","category":"gr-qc"}
{"created":"2024-04-08 12:41:22","title":"Interactive Formal Specification for Mathematical Problems of Engineers","abstract":"The paper presents the second part of a precise description of the prototype that has been developed in the course of the ISAC project over the last two decades. This part describes the \"specify-phase\", while the first part describing the \"solve-phase\" is already published.   In the specify-phase a student interactively constructs a formal specification. The ISAC prototype implements formal specifications as established in theoretical computer science, however, the input language for the construction avoids requiring users to have knowledge of logic; this makes the system useful for various engineering faculties (and also for high school).   The paper discusses not only ISAC's design of the specify-phase in detail, but also gives a brief introduction to implementation with the aim of advertising the re-use of formal frameworks (inclusive respective front-ends) with their generic tools for language definition and their rich pool of software components for formal mathematics.","sentences":["The paper presents the second part of a precise description of the prototype that has been developed in the course of the ISAC project over the last two decades.","This part describes the \"specify-phase\", while the first part describing the \"solve-phase\" is already published.   ","In the specify-phase a student interactively constructs a formal specification.","The ISAC prototype implements formal specifications as established in theoretical computer science, however, the input language for the construction avoids requiring users to have knowledge of logic; this makes the system useful for various engineering faculties (and also for high school).   ","The paper discusses not only ISAC's design of the specify-phase in detail, but also gives a brief introduction to implementation with the aim of advertising the re-use of formal frameworks (inclusive respective front-ends) with their generic tools for language definition and their rich pool of software components for formal mathematics."],"url":"http://arxiv.org/abs/2404.05462v1","category":"cs.HC"}
{"created":"2024-04-08 12:41:18","title":"Density Classification with Non-Unitary Quantum Cellular Automata","abstract":"The density classification (DC) task, a computation which maps global density information to local density, is studied using one-dimensional non-unitary quantum cellular automata (QCAs). Two approaches are considered: one that preserves the number density and one that performs majority voting. For the DC, two QCAs are introduced that reach the fixed point solution in a time scaling quadratically with the system size. One of the QCAs is based on a known classical probabilistic cellular automaton which has been studied in the context of DC. The second QCA for DC is a new quantum model that is designed to demonstrate additional quantum features and is restricted to only two-body interactions. Both can be generated by continuous-time Lindblad dynamics. A third QCA is a hybrid rule defined by discrete-time three-body interactions that is shown to solve the majority voting problem within a time that scales linearly with the system size.","sentences":["The density classification (DC) task, a computation which maps global density information to local density, is studied using one-dimensional non-unitary quantum cellular automata (QCAs).","Two approaches are considered: one that preserves the number density and one that performs majority voting.","For the DC, two QCAs are introduced that reach the fixed point solution in a time scaling quadratically with the system size.","One of the QCAs is based on a known classical probabilistic cellular automaton which has been studied in the context of DC.","The second QCA for DC is a new quantum model that is designed to demonstrate additional quantum features and is restricted to only two-body interactions.","Both can be generated by continuous-time Lindblad dynamics.","A third QCA is a hybrid rule defined by discrete-time three-body interactions that is shown to solve the majority voting problem within a time that scales linearly with the system size."],"url":"http://arxiv.org/abs/2404.05461v1","category":"quant-ph"}
{"created":"2024-04-08 12:40:27","title":"Teaching Higher-Order Logic Using Isabelle","abstract":"We present a formalization of higher-order logic in the Isabelle proof assistant, building directly on the foundational framework Isabelle/Pure and developed to be as small and readable as possible. It should therefore serve as a good introduction for someone looking into learning about higher-order logic and proof assistants, without having to study the much more complex Isabelle/HOL with heavier automation. To showcase our development and approach we explain a sample proof, describe the axioms and rules of our higher-order logic, and discuss our experience with teaching the subject in a classroom setting.","sentences":["We present a formalization of higher-order logic in the Isabelle proof assistant, building directly on the foundational framework Isabelle/Pure and developed to be as small and readable as possible.","It should therefore serve as a good introduction for someone looking into learning about higher-order logic and proof assistants, without having to study the much more complex Isabelle/HOL with heavier automation.","To showcase our development and approach we explain a sample proof, describe the axioms and rules of our higher-order logic, and discuss our experience with teaching the subject in a classroom setting."],"url":"http://arxiv.org/abs/2404.05458v1","category":"cs.LO"}
{"created":"2024-04-08 12:31:23","title":"RoT: Enhancing Large Language Models with Reflection on Search Trees","abstract":"Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.","sentences":["Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods.","However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process.","To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods.","It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM.","The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process.","In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines.","In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS).","Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience."],"url":"http://arxiv.org/abs/2404.05449v1","category":"cs.CL"}
{"created":"2024-04-08 12:29:46","title":"Pansharpening of PRISMA products for archaeological prospection","abstract":"Hyperspectral data recorded from satellite platforms are often ill-suited for geo-archaeological prospection due to low spatial resolution. The established potential of hyperspectral data from airborne sensors in identifying archaeological features has, on the other side, generated increased interest in enhancing hyperspectral data to achieve higher spatial resolution. This improvement is crucial for detecting traces linked to sub-surface geo-archaeological features and can make satellite hyperspectral acquisitions more suitable for archaeological research. This research assesses the usability of pansharpened PRISMA satellite products in geo-archaeological prospections. Three pan-sharpening methods (GSA, MTF-GLP and HySure) are compared quantitatively and qualitatively and tested over the archaeological landscape of Aquileia (Italy). The results suggest that the application of pansharpening techniques makes hyperspectral satellite imagery highly suitable, under certain conditions, to the identification of sub-surface archaeological features of small and large size.","sentences":["Hyperspectral data recorded from satellite platforms are often ill-suited for geo-archaeological prospection due to low spatial resolution.","The established potential of hyperspectral data from airborne sensors in identifying archaeological features has, on the other side, generated increased interest in enhancing hyperspectral data to achieve higher spatial resolution.","This improvement is crucial for detecting traces linked to sub-surface geo-archaeological features and can make satellite hyperspectral acquisitions more suitable for archaeological research.","This research assesses the usability of pansharpened PRISMA satellite products in geo-archaeological prospections.","Three pan-sharpening methods (GSA, MTF-GLP and HySure) are compared quantitatively and qualitatively and tested over the archaeological landscape of Aquileia (Italy).","The results suggest that the application of pansharpening techniques makes hyperspectral satellite imagery highly suitable, under certain conditions, to the identification of sub-surface archaeological features of small and large size."],"url":"http://arxiv.org/abs/2404.05447v1","category":"cs.CV"}
{"created":"2024-04-08 12:29:07","title":"XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with Long-range Dependencies","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks but are constrained by their small context window sizes. Various efforts have been proposed to expand the context window to accommodate even up to 200K input tokens. Meanwhile, building high-quality benchmarks with much longer text lengths and more demanding tasks to provide comprehensive evaluations is of immense practical interest to facilitate long context understanding research of LLMs. However, prior benchmarks create datasets that ostensibly cater to long-text comprehension by expanding the input of traditional tasks, which falls short to exhibit the unique characteristics of long-text understanding, including long dependency tasks and longer text length compatible with modern LLMs' context window size. In this paper, we introduce a benchmark for extremely long context understanding with long-range dependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading, Paper Reading, and Law Reading, and four tasks of increasing complexity: Memory Retrieval, Detailed Understanding, Overall Understanding, and Open-ended Generation, covering 27 subtasks in English and Chinese. It has an average length of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six leading LLMs on XL$^2$Bench, we find that their performance significantly lags behind human levels. Moreover, the observed decline in performance across both the original and enhanced datasets underscores the efficacy of our approach to mitigating data contamination.","sentences":["Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks but are constrained by their small context window sizes.","Various efforts have been proposed to expand the context window to accommodate even up to 200K input tokens.","Meanwhile, building high-quality benchmarks with much longer text lengths and more demanding tasks to provide comprehensive evaluations is of immense practical interest to facilitate long context understanding research of LLMs.","However, prior benchmarks create datasets that ostensibly cater to long-text comprehension by expanding the input of traditional tasks, which falls short to exhibit the unique characteristics of long-text understanding, including long dependency tasks and longer text length compatible with modern LLMs' context window size.","In this paper, we introduce a benchmark for extremely long context understanding with long-range dependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading, Paper Reading, and Law Reading, and four tasks of increasing complexity: Memory Retrieval, Detailed Understanding, Overall Understanding, and Open-ended Generation, covering 27 subtasks in English and Chinese.","It has an average length of 100K+ words (English) and 200K+ characters (Chinese).","Evaluating six leading LLMs on XL$^2$Bench, we find that their performance significantly lags behind human levels.","Moreover, the observed decline in performance across both the original and enhanced datasets underscores the efficacy of our approach to mitigating data contamination."],"url":"http://arxiv.org/abs/2404.05446v1","category":"cs.CL"}
{"created":"2024-04-08 12:22:39","title":"Unlocking Adaptive User Experience with Generative AI","abstract":"Developing user-centred applications that address diverse user needs requires rigorous user research. This is time, effort and cost-consuming. With the recent rise of generative AI techniques based on Large Language Models (LLMs), there is a possibility that these powerful tools can be used to develop adaptive interfaces. This paper presents a novel approach to develop user personas and adaptive interface candidates for a specific domain using ChatGPT. We develop user personas and adaptive interfaces using both ChatGPT and a traditional manual process and compare these outcomes. To obtain data for the personas we collected data from 37 survey participants and 4 interviews in collaboration with a not-for-profit organisation. The comparison of ChatGPT generated content and manual content indicates promising results that encourage using LLMs in the adaptive interfaces design process.","sentences":["Developing user-centred applications that address diverse user needs requires rigorous user research.","This is time, effort and cost-consuming.","With the recent rise of generative AI techniques based on Large Language Models (LLMs), there is a possibility that these powerful tools can be used to develop adaptive interfaces.","This paper presents a novel approach to develop user personas and adaptive interface candidates for a specific domain using ChatGPT.","We develop user personas and adaptive interfaces using both ChatGPT and a traditional manual process and compare these outcomes.","To obtain data for the personas we collected data from 37 survey participants and 4 interviews in collaboration with a not-for-profit organisation.","The comparison of ChatGPT generated content and manual content indicates promising results that encourage using LLMs in the adaptive interfaces design process."],"url":"http://arxiv.org/abs/2404.05442v1","category":"cs.HC"}
{"created":"2024-04-08 12:20:12","title":"A birdcage resonant antenna for helicon wave generation in TORPEX","abstract":"A birdcage resonant helicon antenna is designed, mounted and tested in the toroidal device TORPEX. The birdcage resonant antenna is an alternative to the usual Boswell or half-helical antenna designs commonly used for $\\sim$ 10 cm diameter helicon sources in low temperature plasma devices. The main advantage of the birdcage antenna lies in its resonant nature, which makes it easily operational even at large scales, an appealing feature for the TORPEX device whose poloidal cross section is 40 cm in diameter. With this antenna helicon waves are shown to be launched and sustained throughout the whole torus of TORPEX. The helicon waves can be launched at low power on a pre-existing magnetron-generated plasma with little effect on the density profiles. The birdcage antenna can also be used alone to produce plasma, which removes the constraint of a narrow range of applied magnetic fields required by the magnetron, opening the way to a new range of studies on TORPEX with the external magnetic field as a control parameter.","sentences":["A birdcage resonant helicon antenna is designed, mounted and tested in the toroidal device TORPEX.","The birdcage resonant antenna is an alternative to the usual Boswell or half-helical antenna designs commonly used for $\\sim$ 10 cm diameter helicon sources in low temperature plasma devices.","The main advantage of the birdcage antenna lies in its resonant nature, which makes it easily operational even at large scales, an appealing feature for the TORPEX device whose poloidal cross section is 40 cm in diameter.","With this antenna helicon waves are shown to be launched and sustained throughout the whole torus of TORPEX.","The helicon waves can be launched at low power on a pre-existing magnetron-generated plasma with little effect on the density profiles.","The birdcage antenna can also be used alone to produce plasma, which removes the constraint of a narrow range of applied magnetic fields required by the magnetron, opening the way to a new range of studies on TORPEX with the external magnetic field as a control parameter."],"url":"http://arxiv.org/abs/2404.05441v1","category":"physics.plasm-ph"}
{"created":"2024-04-08 12:19:04","title":"Tree Search-Based Policy Optimization under Stochastic Execution Delay","abstract":"The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately. However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic. In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. Armed with this insight, we devise DEZ, a model-based algorithm that optimizes over the class of Markov policies. DEZ leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through a series of experiments on the Atari suite, we demonstrate that although the previous baseline outperforms the naive method in scenarios with constant delay, it underperforms in the face of stochastic delays. In contrast, our approach significantly outperforms the baselines, for both constant and stochastic delays. The code is available at http://github.com/davidva1/Delayed-EZ .","sentences":["The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately.","However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic.","In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation.","We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case.","Armed with this insight, we devise DEZ, a model-based algorithm that optimizes over the class of Markov policies.","DEZ leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue.","Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero.","Through a series of experiments on the Atari suite, we demonstrate that although the previous baseline outperforms the naive method in scenarios with constant delay, it underperforms in the face of stochastic delays.","In contrast, our approach significantly outperforms the baselines, for both constant and stochastic delays.","The code is available at http://github.com/davidva1/Delayed-EZ ."],"url":"http://arxiv.org/abs/2404.05440v1","category":"cs.AI"}
{"created":"2024-04-08 12:18:01","title":"Action-conditioned video data improves predictability","abstract":"Long-term video generation and prediction remain challenging tasks in computer vision, particularly in partially observable scenarios where cameras are mounted on moving platforms. The interaction between observed image frames and the motion of the recording agent introduces additional complexities. To address these issues, we introduce the Action-Conditioned Video Generation (ACVG) framework, a novel approach that investigates the relationship between actions and generated image frames through a deep dual Generator-Actor architecture. ACVG generates video sequences conditioned on the actions of robots, enabling exploration and analysis of how vision and action mutually influence one another in dynamic environments. We evaluate the framework's effectiveness on an indoor robot motion dataset which consists of sequences of image frames along with the sequences of actions taken by the robotic agent, conducting a comprehensive empirical study comparing ACVG to other state-of-the-art frameworks along with a detailed ablation study.","sentences":["Long-term video generation and prediction remain challenging tasks in computer vision, particularly in partially observable scenarios where cameras are mounted on moving platforms.","The interaction between observed image frames and the motion of the recording agent introduces additional complexities.","To address these issues, we introduce the Action-Conditioned Video Generation (ACVG) framework, a novel approach that investigates the relationship between actions and generated image frames through a deep dual Generator-Actor architecture.","ACVG generates video sequences conditioned on the actions of robots, enabling exploration and analysis of how vision and action mutually influence one another in dynamic environments.","We evaluate the framework's effectiveness on an indoor robot motion dataset which consists of sequences of image frames along with the sequences of actions taken by the robotic agent, conducting a comprehensive empirical study comparing ACVG to other state-of-the-art frameworks along with a detailed ablation study."],"url":"http://arxiv.org/abs/2404.05439v1","category":"cs.CV"}
{"created":"2024-04-08 12:16:34","title":"SrRuO3 under tensile strain: Thickness-dependent electronic and magnetic properties","abstract":"The burgeoning fields of spintronics and topological electronics require materials possessing a unique combination of properties: ferromagnetism, metallicity, and chemical stability. SrRuO3 (SRO) stands out as a compelling candidate due to its exceptional combination of these attributes. However, understanding its behavior under tensile strain, especially its thickness-dependent changes, remains elusive. This study employs machine-learning-assisted molecular beam epitaxy to investigate SRO films with thicknesses from 1 to 10 nm. This work complements the existing focus on compressive-strained SRO, opening a new avenue for exploring its hitherto concealed potential. Using soft X-ray magnetic circular dichroism, we uncover an intriguing interplay between film thickness, electronic structure, and magnetic properties. Our key findings reveal an intensified localization of Ru 4d t2g-O 2p hybridized states at lower thicknesses, attributed to the weakened orbital hybridization. Furthermore, we find a progressive reduction of magnetic moments for both Ru and O ions as film thickness decreases. Notably, a non-ferromagnetic insulating state emerges at a critical thickness of 1 nm, marking a pivotal transition from the metallic ferromagnetic phase. These insights emphasize the importance of considering thickness-dependent properties when tailoring SRO for next-generation spintronic and topological electronic devices.","sentences":["The burgeoning fields of spintronics and topological electronics require materials possessing a unique combination of properties: ferromagnetism, metallicity, and chemical stability.","SrRuO3 (SRO) stands out as a compelling candidate due to its exceptional combination of these attributes.","However, understanding its behavior under tensile strain, especially its thickness-dependent changes, remains elusive.","This study employs machine-learning-assisted molecular beam epitaxy to investigate SRO films with thicknesses from 1 to 10 nm.","This work complements the existing focus on compressive-strained SRO, opening a new avenue for exploring its hitherto concealed potential.","Using soft X-ray magnetic circular dichroism, we uncover an intriguing interplay between film thickness, electronic structure, and magnetic properties.","Our key findings reveal an intensified localization of Ru 4d t2g-O 2p hybridized states at lower thicknesses, attributed to the weakened orbital hybridization.","Furthermore, we find a progressive reduction of magnetic moments for both Ru and O ions as film thickness decreases.","Notably, a non-ferromagnetic insulating state emerges at a critical thickness of 1 nm, marking a pivotal transition from the metallic ferromagnetic phase.","These insights emphasize the importance of considering thickness-dependent properties when tailoring SRO for next-generation spintronic and topological electronic devices."],"url":"http://arxiv.org/abs/2404.05438v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 11:55:09","title":"AutoCodeRover: Autonomous Program Improvement","abstract":"Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum based fault localization using tests, further sharpens the context. Experiments on the recently proposed SWE-bench-lite which consists of 300 real-life Github issues involving bug fixing and feature additions show increased efficacy (resolving more than 20% on SWE-bench-lite), as compared to recent efforts from the AI community. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.","sentences":["Researchers have made significant progress in automating the software development process in the past decades.","Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers.","Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding.","Nevertheless software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions).","In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement.","In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch.","In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented.","We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files.","Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search.","The use of spectrum based fault localization using tests, further sharpens the context.","Experiments on the recently proposed SWE-bench-lite which consists of 300 real-life Github issues involving bug fixing and feature additions show increased efficacy (resolving more than 20% on SWE-bench-lite), as compared to recent efforts from the AI community.","We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved."],"url":"http://arxiv.org/abs/2404.05427v1","category":"cs.SE"}
{"created":"2024-04-08 11:54:49","title":"Test-Time Zero-Shot Temporal Action Localization","abstract":"Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM). T3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.","sentences":["Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training.","Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data.","While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications.","Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos.","These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data.","To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL).","In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM).","T3AL operates in three steps.","First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video.","Then, action localization is performed adopting a novel procedure inspired by self-supervised learning.","Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals.","We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets.","Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach."],"url":"http://arxiv.org/abs/2404.05426v1","category":"cs.CV"}
{"created":"2024-04-08 11:47:46","title":"What Are the Odds? Improving the foundations of Statistical Model Checking","abstract":"Markov decision processes (MDPs) are a fundamental model for decision making under uncertainty. They exhibit non-deterministic choice as well as probabilistic uncertainty. Traditionally, verification algorithms assume exact knowledge of the probabilities that govern the behaviour of an MDP. As this assumption is often unrealistic in practice, statistical model checking (SMC) was developed in the past two decades. It allows to analyse MDPs with unknown transition probabilities and provide probably approximately correct (PAC) guarantees on the result. Model-based SMC algorithms sample the MDP and build a model of it by estimating all transition probabilities, essentially for every transition answering the question: ``What are the odds?'' However, so far the statistical methods employed by the state of the art SMC algorithms are quite naive. Our contribution are several fundamental improvements to those methods: On the one hand, we survey statistics literature for better concentration inequalities; on the other hand, we propose specialised approaches that exploit our knowledge of the MDP. Our improvements are generally applicable to many kinds of problem statements because they are largely independent of the setting. Moreover, our experimental evaluation shows that they lead to significant gains, reducing the number of samples that the SMC algorithm has to collect by up to two orders of magnitude.","sentences":["Markov decision processes (MDPs) are a fundamental model for decision making under uncertainty.","They exhibit non-deterministic choice as well as probabilistic uncertainty.","Traditionally, verification algorithms assume exact knowledge of the probabilities that govern the behaviour of an MDP.","As this assumption is often unrealistic in practice, statistical model checking (SMC) was developed in the past two decades.","It allows to analyse MDPs with unknown transition probabilities and provide probably approximately correct (PAC) guarantees on the result.","Model-based SMC algorithms sample the MDP and build a model of it by estimating all transition probabilities, essentially for every transition answering the question: ``What are the odds?''","However, so far the statistical methods employed by the state of the art SMC algorithms are quite naive.","Our contribution are several fundamental improvements to those methods: On the one hand, we survey statistics literature for better concentration inequalities; on the other hand, we propose specialised approaches that exploit our knowledge of the MDP.","Our improvements are generally applicable to many kinds of problem statements because they are largely independent of the setting.","Moreover, our experimental evaluation shows that they lead to significant gains, reducing the number of samples that the SMC algorithm has to collect by up to two orders of magnitude."],"url":"http://arxiv.org/abs/2404.05424v1","category":"cs.AI"}
{"created":"2024-04-08 11:43:40","title":"Residual Chain Prediction for Autonomous Driving Path Planning","abstract":"In the rapidly evolving field of autonomous driving systems, the refinement of path planning algorithms is paramount for navigating vehicles through dynamic environments, particularly in complex urban scenarios. Traditional path planning algorithms, which are heavily reliant on static rules and manually defined parameters, often fall short in such contexts, highlighting the need for more adaptive, learning-based approaches. Among these, behavior cloning emerges as a noteworthy strategy for its simplicity and efficiency, especially within the realm of end-to-end path planning. However, behavior cloning faces challenges, such as covariate shift when employing traditional Manhattan distance as the metric. Addressing this, our study introduces the novel concept of Residual Chain Loss. Residual Chain Loss dynamically adjusts the loss calculation process to enhance the temporal dependency and accuracy of predicted path points, significantly improving the model's performance without additional computational overhead. Through testing on the nuScenes dataset, we underscore the method's substantial advancements in addressing covariate shift, facilitating dynamic loss adjustments, and ensuring seamless integration with end-to-end path planning frameworks. Our findings highlight the potential of Residual Chain Loss to revolutionize planning component of autonomous driving systems, marking a significant step forward in the quest for level 5 autonomous driving system.","sentences":["In the rapidly evolving field of autonomous driving systems, the refinement of path planning algorithms is paramount for navigating vehicles through dynamic environments, particularly in complex urban scenarios.","Traditional path planning algorithms, which are heavily reliant on static rules and manually defined parameters, often fall short in such contexts, highlighting the need for more adaptive, learning-based approaches.","Among these, behavior cloning emerges as a noteworthy strategy for its simplicity and efficiency, especially within the realm of end-to-end path planning.","However, behavior cloning faces challenges, such as covariate shift when employing traditional Manhattan distance as the metric.","Addressing this, our study introduces the novel concept of Residual Chain Loss.","Residual Chain Loss dynamically adjusts the loss calculation process to enhance the temporal dependency and accuracy of predicted path points, significantly improving the model's performance without additional computational overhead.","Through testing on the nuScenes dataset, we underscore the method's substantial advancements in addressing covariate shift, facilitating dynamic loss adjustments, and ensuring seamless integration with end-to-end path planning frameworks.","Our findings highlight the potential of Residual Chain Loss to revolutionize planning component of autonomous driving systems, marking a significant step forward in the quest for level 5 autonomous driving system."],"url":"http://arxiv.org/abs/2404.05423v1","category":"cs.RO"}
{"created":"2024-04-08 11:36:28","title":"Joint Active and Passive Beamforming for IRS-Aided Wireless Energy Transfer Network Exploiting One-Bit Feedback","abstract":"To reap the active and passive beamforming gain in an intelligent reflecting surface (IRS)-aided wireless network, a typical way is to first acquire the channel state information (CSI) relying on the pilot signal, and then perform the joint beamforming design. However, it is a great challenge when the receiver can neither send pilot signals nor have complex signal processing capabilities due to its hardware limitation. To tackle this problem, we study in this paper an IRS-aided wireless energy transfer (WET) network and propose two joint beamforming design methods, namely, the channel-estimationbased method and the distributed-beamforming-based method, that require only one-bit feedback from the energy receiver (ER) to the energy transmitter (ET). Specifically, for the channelestimation-based method, according to the feedback information, the ET is able to infer the cascaded ET-IRS-ER channel by continually adjusting its transmit beamformer while applying the analytic center cutting plane method (ACCPM). Then, based on the estimated cascaded CSI, the joint beamforming design can be performed by using the existing optimization techniques. While for the distributed-beamforming-based method, we first apply the distributed beamforming algorithm to optimize the IRS reflection coefficients, which is theoretically proven to converge to a local optimum almost surely. Then, the optimal ET's transmit covariance matrix is obtained based on the effective ET-ER channel learned by applying the ACCPM only once. Numerical results demonstrate the effectiveness of our proposed one-bitfeedback-based joint beamforming design schemes while greatly reducing the requirement on the hardware complexity of the ER. In particular, the high accuracy of our IRS-involved cascaded channel estimation method exploiting one-bit feedback is also validated.","sentences":["To reap the active and passive beamforming gain in an intelligent reflecting surface (IRS)-aided wireless network, a typical way is to first acquire the channel state information (CSI) relying on the pilot signal, and then perform the joint beamforming design.","However, it is a great challenge when the receiver can neither send pilot signals nor have complex signal processing capabilities due to its hardware limitation.","To tackle this problem, we study in this paper an IRS-aided wireless energy transfer (WET) network and propose two joint beamforming design methods, namely, the channel-estimationbased method and the distributed-beamforming-based method, that require only one-bit feedback from the energy receiver (ER) to the energy transmitter (ET).","Specifically, for the channelestimation-based method, according to the feedback information, the ET is able to infer the cascaded ET-IRS-ER channel by continually adjusting its transmit beamformer while applying the analytic center cutting plane method (ACCPM).","Then, based on the estimated cascaded CSI, the joint beamforming design can be performed by using the existing optimization techniques.","While for the distributed-beamforming-based method, we first apply the distributed beamforming algorithm to optimize the IRS reflection coefficients, which is theoretically proven to converge to a local optimum almost surely.","Then, the optimal ET's transmit covariance matrix is obtained based on the effective ET-ER channel learned by applying the ACCPM only once.","Numerical results demonstrate the effectiveness of our proposed one-bitfeedback-based joint beamforming design schemes while greatly reducing the requirement on the hardware complexity of the ER.","In particular, the high accuracy of our IRS-involved cascaded channel estimation method exploiting one-bit feedback is also validated."],"url":"http://arxiv.org/abs/2404.05418v1","category":"eess.SY"}
{"created":"2024-04-08 11:33:58","title":"Indexing Analytics to Instances: How Integrating a Dashboard can Support Design Education","abstract":"We investigate how to use AI-based analytics to support design education. The analytics at hand measure multiscale design, that is, students' use of space and scale to visually and conceptually organize their design work. With the goal of making the analytics intelligible to instructors, we developed a research artifact integrating a design analytics dashboard with design instances, and the design environment that students use to create them. We theorize about how Suchman's notion of mutual intelligibility requires contextualized investigation of AI in order to develop findings about how analytics work for people. We studied the research artifact in 5 situated course contexts, in 3 departments. A total of 236 students used the multiscale design environment. The 9 instructors who taught those students experienced the analytics via the new research artifact.   We derive findings from a qualitative analysis of interviews with instructors regarding their experiences. Instructors reflected on how the analytics and their presentation in the dashboard have the potential to affect design education. We develop research implications addressing: (1) how indexing design analytics in the dashboard to actual design work instances helps design instructors reflect on what they mean and, more broadly, is a technique for how AI-based design analytics can support instructors' assessment and feedback experiences in situated course contexts; and (2) how multiscale design analytics, in particular, have the potential to support design education. By indexing, we mean linking which provides context, here connecting the numbers of the analytics with visually annotated design work instances.","sentences":["We investigate how to use AI-based analytics to support design education.","The analytics at hand measure multiscale design, that is, students' use of space and scale to visually and conceptually organize their design work.","With the goal of making the analytics intelligible to instructors, we developed a research artifact integrating a design analytics dashboard with design instances, and the design environment that students use to create them.","We theorize about how Suchman's notion of mutual intelligibility requires contextualized investigation of AI in order to develop findings about how analytics work for people.","We studied the research artifact in 5 situated course contexts, in 3 departments.","A total of 236 students used the multiscale design environment.","The 9 instructors who taught those students experienced the analytics via the new research artifact.   ","We derive findings from a qualitative analysis of interviews with instructors regarding their experiences.","Instructors reflected on how the analytics and their presentation in the dashboard have the potential to affect design education.","We develop research implications addressing: (1) how indexing design analytics in the dashboard to actual design work instances helps design instructors reflect on what they mean and, more broadly, is a technique for how AI-based design analytics can support instructors' assessment and feedback experiences in situated course contexts; and (2) how multiscale design analytics, in particular, have the potential to support design education.","By indexing, we mean linking which provides context, here connecting the numbers of the analytics with visually annotated design work instances."],"url":"http://arxiv.org/abs/2404.05417v1","category":"cs.HC"}
{"created":"2024-04-08 11:33:47","title":"Geometry of infinite dimensional Cartan Developments","abstract":"The Cartan development takes a Lie algebra valued 1-form satisfying the Maurer-Cartan equation on a simply connected manifold $M$ to a smooth mapping from $M$ into the Lie group. In this paper this is generalized to infinite dimensional $M$ for infinite dimensional regular Lie groups. The Cartan development is viewed as a generalization of the evolution map of a regular Lie group. The tangent mapping of a Cartan development is identified as another Cartan development.","sentences":["The Cartan development takes a Lie algebra valued 1-form satisfying the Maurer-Cartan equation on a simply connected manifold $M$ to a smooth mapping from $M$ into the Lie group.","In this paper this is generalized to infinite dimensional $M$ for infinite dimensional regular Lie groups.","The Cartan development is viewed as a generalization of the evolution map of a regular Lie group.","The tangent mapping of a Cartan development is identified as another Cartan development."],"url":"http://arxiv.org/abs/2404.05416v1","category":"math.DG"}
{"created":"2024-04-08 11:33:00","title":"Relation Extraction Using Large Language Models: A Case Study on Acupuncture Point Locations","abstract":"In acupuncture therapy, the accurate location of acupoints is essential for its effectiveness. The advanced language understanding capabilities of large language models (LLMs) like Generative Pre-trained Transformers (GPT) present a significant opportunity for extracting relations related to acupoint locations from textual knowledge sources. This study aims to compare the performance of GPT with traditional deep learning models (Long Short-Term Memory (LSTM) and Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT)) in extracting acupoint-related location relations and assess the impact of pretraining and fine-tuning on GPT's performance. We utilized the World Health Organization Standard Acupuncture Point Locations in the Western Pacific Region (WHO Standard) as our corpus, which consists of descriptions of 361 acupoints. Five types of relations ('direction_of,' 'distance_of,' 'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints were annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5, and fine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics included micro-average exact match precision, recall, and F1 scores. Our results demonstrate that fine-tuned GPT-3.5 consistently outperformed other models in F1 scores across all relation types. Overall, it achieved the highest micro-average F1 score of 0.92. This study underscores the effectiveness of LLMs like GPT in extracting relations related to acupoint locations, with implications for accurately modeling acupuncture knowledge and promoting standard implementation in acupuncture training and practice. The findings also contribute to advancing informatics applications in traditional and complementary medicine, showcasing the potential of LLMs in natural language processing.","sentences":["In acupuncture therapy, the accurate location of acupoints is essential for its effectiveness.","The advanced language understanding capabilities of large language models (LLMs) like Generative Pre-trained Transformers (GPT) present a significant opportunity for extracting relations related to acupoint locations from textual knowledge sources.","This study aims to compare the performance of GPT with traditional deep learning models (Long Short-Term Memory (LSTM) and Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT)) in extracting acupoint-related location relations and assess the impact of pretraining and fine-tuning on GPT's performance.","We utilized the World Health Organization Standard Acupuncture Point Locations in the Western Pacific Region (WHO Standard) as our corpus, which consists of descriptions of 361 acupoints.","Five types of relations ('direction_of,' 'distance_of,' 'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints were annotated.","Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5, and fine-tuned GPT-3.5, as well as pre-trained GPT-4.","Performance metrics included micro-average exact match precision, recall, and F1 scores.","Our results demonstrate that fine-tuned GPT-3.5 consistently outperformed other models in F1 scores across all relation types.","Overall, it achieved the highest micro-average F1 score of 0.92.","This study underscores the effectiveness of LLMs like GPT in extracting relations related to acupoint locations, with implications for accurately modeling acupuncture knowledge and promoting standard implementation in acupuncture training and practice.","The findings also contribute to advancing informatics applications in traditional and complementary medicine, showcasing the potential of LLMs in natural language processing."],"url":"http://arxiv.org/abs/2404.05415v1","category":"cs.CL"}
{"created":"2024-04-08 11:25:30","title":"Know When To Stop: A Study of Semantic Drift in Text Generation","abstract":"In this work, we explicitly show that modern LLMs tend to generate correct facts first, then \"drift away\" and generate incorrect facts later: this was occasionally observed but never properly measured. We develop a semantic drift score that measures the degree of separation between correct and incorrect facts in generated texts and confirm our hypothesis when generating Wikipedia-style biographies. This correct-then-incorrect generation pattern suggests that factual accuracy can be improved by knowing when to stop generation. Therefore, we explore the trade-off between information quantity and factual accuracy for several early stopping methods and manage to improve factuality by a large margin. We further show that reranking with semantic similarity can further improve these results, both compared to the baseline and when combined with early stopping. Finally, we try calling external API to bring the model back to the right generation path, but do not get positive results. Overall, our methods generalize and can be applied to any long-form text generation to produce more reliable information, by balancing trade-offs between factual accuracy, information quantity and computational cost.","sentences":["In this work, we explicitly show that modern LLMs tend to generate correct facts first, then \"drift away\" and generate incorrect facts later: this was occasionally observed but never properly measured.","We develop a semantic drift score that measures the degree of separation between correct and incorrect facts in generated texts and confirm our hypothesis when generating Wikipedia-style biographies.","This correct-then-incorrect generation pattern suggests that factual accuracy can be improved by knowing when to stop generation.","Therefore, we explore the trade-off between information quantity and factual accuracy for several early stopping methods and manage to improve factuality by a large margin.","We further show that reranking with semantic similarity can further improve these results, both compared to the baseline and when combined with early stopping.","Finally, we try calling external API to bring the model back to the right generation path, but do not get positive results.","Overall, our methods generalize and can be applied to any long-form text generation to produce more reliable information, by balancing trade-offs between factual accuracy, information quantity and computational cost."],"url":"http://arxiv.org/abs/2404.05411v1","category":"cs.CL"}
{"created":"2024-04-08 11:21:06","title":"Solving quantum impurity problems on the L-shaped Kadanoff-Baym contour","abstract":"The path integral formalism is the building block of many powerful numerical methods for quantum impurity problems. However, existing path integral based numerical calculations have only been performed in either the imaginary-time or the real-time axis, while the most generic scenario formulated on the L-shaped Kadanoff-Baym contour is left unexplored. In this work, we extended the recently developed Grassmann time-evolving matrix product operator (GTEMPO) method to solve quantum impurity problems directly on the Kadanoff-Baym contour. The resulting method is numerically exact, with only two sources of numerical errors, e.g., the time discretization error and the matrix product state bond truncation error, which can both be well controlled. The accuracy of this method is numerically demonstrated against exact solutions in the noninteracting case, and against existing calculations on the real- and imaginary-time axes for the single-orbital Anderson impurity model. Our method is a perfect benchmarking baseline for its alternatives which often employ less-controlled approximations, and can also be used as a real-time impurity solver in dynamical mean field theory and its non-equilibrium extension.","sentences":["The path integral formalism is the building block of many powerful numerical methods for quantum impurity problems.","However, existing path integral based numerical calculations have only been performed in either the imaginary-time or the real-time axis, while the most generic scenario formulated on the L-shaped Kadanoff-Baym contour is left unexplored.","In this work, we extended the recently developed Grassmann time-evolving matrix product operator (GTEMPO) method to solve quantum impurity problems directly on the Kadanoff-Baym contour.","The resulting method is numerically exact, with only two sources of numerical errors, e.g., the time discretization error and the matrix product state bond truncation error, which can both be well controlled.","The accuracy of this method is numerically demonstrated against exact solutions in the noninteracting case, and against existing calculations on the real- and imaginary-time axes for the single-orbital Anderson impurity model.","Our method is a perfect benchmarking baseline for its alternatives which often employ less-controlled approximations, and can also be used as a real-time impurity solver in dynamical mean field theory and its non-equilibrium extension."],"url":"http://arxiv.org/abs/2404.05410v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 11:14:58","title":"PerkwE_COQA: enhance Persian Conversational Question Answering by combining contextual keyword extraction with Large Language Models","abstract":"Smart cities need the involvement of their residents to enhance quality of life. Conversational query-answering is an emerging approach for user engagement. There is an increasing demand of an advanced conversational question-answering that goes beyond classic systems. Existing approaches have shown that LLMs offer promising capabilities for CQA, but may struggle to capture the nuances of conversational contexts. The new approach involves understanding the content and engaging in a multi-step conversation with the user to fulfill their needs. This paper presents a novel method to elevate the performance of Persian Conversational question-answering (CQA) systems. It combines the strengths of Large Language Models (LLMs) with contextual keyword extraction. Our method extracts keywords specific to the conversational flow, providing the LLM with additional context to understand the user's intent and generate more relevant and coherent responses. We evaluated the effectiveness of this combined approach through various metrics, demonstrating significant improvements in CQA performance compared to an LLM-only baseline. The proposed method effectively handles implicit questions, delivers contextually relevant answers, and tackles complex questions that rely heavily on conversational context. The findings indicate that our method outperformed the evaluation benchmarks up to 8% higher than existing methods and the LLM-only baseline.","sentences":["Smart cities need the involvement of their residents to enhance quality of life.","Conversational query-answering is an emerging approach for user engagement.","There is an increasing demand of an advanced conversational question-answering that goes beyond classic systems.","Existing approaches have shown that LLMs offer promising capabilities for CQA, but may struggle to capture the nuances of conversational contexts.","The new approach involves understanding the content and engaging in a multi-step conversation with the user to fulfill their needs.","This paper presents a novel method to elevate the performance of Persian Conversational question-answering (CQA) systems.","It combines the strengths of Large Language Models (LLMs) with contextual keyword extraction.","Our method extracts keywords specific to the conversational flow, providing the LLM with additional context to understand the user's intent and generate more relevant and coherent responses.","We evaluated the effectiveness of this combined approach through various metrics, demonstrating significant improvements in CQA performance compared to an LLM-only baseline.","The proposed method effectively handles implicit questions, delivers contextually relevant answers, and tackles complex questions that rely heavily on conversational context.","The findings indicate that our method outperformed the evaluation benchmarks up to 8% higher than existing methods and the LLM-only baseline."],"url":"http://arxiv.org/abs/2404.05406v1","category":"cs.CL"}
{"created":"2024-04-08 11:11:31","title":"Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws","abstract":"Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.   More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include:   * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.   * Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.","sentences":["Scaling laws describe the relationship between the size of language models and their capabilities.","Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores.","We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page.","Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications.","Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.   ","More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity.","Notable insights include:   *","The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations.","This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.   ","* Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity.","Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity."],"url":"http://arxiv.org/abs/2404.05405v1","category":"cs.CL"}
{"created":"2024-04-08 11:05:45","title":"SoK: Gradient Leakage in Federated Learning","abstract":"Federated learning (FL) enables collaborative model training among multiple clients without raw data exposure. However, recent studies have shown that clients' private training data can be reconstructed from the gradients they share in FL, known as gradient inversion attacks (GIAs). While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored. To address this gap, we conduct a comprehensive study on GIAs in this work. We start with a survey of GIAs that establishes a milestone to trace their evolution and develops a systematization to uncover their inherent threats. Specifically, we categorize the auxiliary assumptions used by existing GIAs based on their practical accessibility to potential adversaries. To facilitate deeper analysis, we highlight the challenges that GIAs face in practical FL systems from three perspectives: \\textit{local training}, \\textit{model}, and \\textit{post-processing}. We then perform extensive theoretical and empirical evaluations of state-of-the-art GIAs across diverse settings, utilizing eight datasets and thirteen models. Our findings indicate that GIAs have inherent limitations when reconstructing data under practical local training settings. Furthermore, their efficacy is sensitive to the trained model, and even simple post-processing measures applied to gradients can be effective defenses. Overall, our work provides crucial insights into the limited effectiveness of GIAs in practical FL systems. By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic.","sentences":["Federated learning (FL) enables collaborative model training among multiple clients without raw data exposure.","However, recent studies have shown that clients' private training data can be reconstructed from the gradients they share in FL, known as gradient inversion attacks (GIAs).","While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored.","To address this gap, we conduct a comprehensive study on GIAs in this work.","We start with a survey of GIAs that establishes a milestone to trace their evolution and develops a systematization to uncover their inherent threats.","Specifically, we categorize the auxiliary assumptions used by existing GIAs based on their practical accessibility to potential adversaries.","To facilitate deeper analysis, we highlight the challenges that GIAs face in practical FL systems from three perspectives: \\textit{local training}, \\textit{model}, and \\textit{post-processing}.","We then perform extensive theoretical and empirical evaluations of state-of-the-art GIAs across diverse settings, utilizing eight datasets and thirteen models.","Our findings indicate that GIAs have inherent limitations when reconstructing data under practical local training settings.","Furthermore, their efficacy is sensitive to the trained model, and even simple post-processing measures applied to gradients can be effective defenses.","Overall, our work provides crucial insights into the limited effectiveness of GIAs in practical FL systems.","By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic."],"url":"http://arxiv.org/abs/2404.05403v1","category":"cs.CR"}
{"created":"2024-04-08 10:58:01","title":"Generating Galaxy Clusters Mass Density Maps from Mock Multiview Images via Deep Learning","abstract":"Galaxy clusters are composed of dark matter, gas and stars. Their dark matter component, which amounts to around 80\\% of the total mass, cannot be directly observed but traced by the distribution of diffused gas and galaxy members. In this work, we aim to infer the cluster's projected total mass distribution from mock observational data, i.e. stars, Sunyaev-Zeldovich, and X-ray, by training deep learning models. To this end, we have created a multiview images dataset from {\\sc{The Three Hundred}} simulation that is optimal for training Machine Learning models. We further study deep learning architectures based on the U-Net to account for single-input and multi-input models. We show that the predicted mass distribution agrees well with the true one.","sentences":["Galaxy clusters are composed of dark matter, gas and stars.","Their dark matter component, which amounts to around 80\\% of the total mass, cannot be directly observed but traced by the distribution of diffused gas and galaxy members.","In this work, we aim to infer the cluster's projected total mass distribution from mock observational data, i.e. stars, Sunyaev-Zeldovich, and X-ray, by training deep learning models.","To this end, we have created a multiview images dataset from {\\sc{The Three Hundred}} simulation that is optimal for training Machine Learning models.","We further study deep learning architectures based on the U-Net to account for single-input and multi-input models.","We show that the predicted mass distribution agrees well with the true one."],"url":"http://arxiv.org/abs/2404.05400v1","category":"astro-ph.CO"}
{"created":"2024-04-08 10:57:25","title":"SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety","abstract":"The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by introducing an abundance of new datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for a given use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 102 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English datasets. We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we commit to updating continuously as the field of LLM safety develops.","sentences":["The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs).","Researchers and practitioners have met these concerns by introducing an abundance of new datasets for evaluating and improving LLM safety.","However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential.","This makes it difficult for researchers and practitioners to find the most relevant datasets for a given use case, and to identify gaps in dataset coverage that future work may fill.","To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety.","We review 102 datasets, which we identified through an iterative and community-driven process over the course of several months.","We highlight patterns and trends, such as a a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English datasets.","We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets.","Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we commit to updating continuously as the field of LLM safety develops."],"url":"http://arxiv.org/abs/2404.05399v1","category":"cs.CL"}
{"created":"2024-04-08 10:54:34","title":"Emergent polar order in non-polar mixtures with non-reciprocal interactions","abstract":"Phenomenological rules that govern the collective behaviour of complex physical systems are powerful tools because they can make concrete predictions about their universality class based on generic considerations, such as symmetries, conservation laws, and dimensionality. While in most cases such considerations are manifestly ingrained in the constituents, novel phenomenology can emerge when composite units associated with emergent symmetries dominate the behaviour of the system. We study a generic class of active matter systems with non-reciprocal interactions and demonstrate the existence of true long-range polar order in two dimensions and above, both at the linear level and by including all relevant nonlinearities in the Renormalization Group sense. We achieve this by uncovering a mapping of our scalar active mixture theory to the Toner-Tu theory of dry polar active matter by employing a suitably defined polar order parameter. We then demonstrate that the complete effective field theory -- which includes all the soft modes and the relevant nonlinear terms -- belongs to the (Burgers-) Kardar-Parisi-Zhang universality class. This classification allows us to prove the stability of the emergent polar long-range order in scalar non-reciprocal mixtures in two dimensions, and hence a conclusive violation of the Mermin-Wagner theorem.","sentences":["Phenomenological rules that govern the collective behaviour of complex physical systems are powerful tools because they can make concrete predictions about their universality class based on generic considerations, such as symmetries, conservation laws, and dimensionality.","While in most cases such considerations are manifestly ingrained in the constituents, novel phenomenology can emerge when composite units associated with emergent symmetries dominate the behaviour of the system.","We study a generic class of active matter systems with non-reciprocal interactions and demonstrate the existence of true long-range polar order in two dimensions and above, both at the linear level and by including all relevant nonlinearities in the Renormalization Group sense.","We achieve this by uncovering a mapping of our scalar active mixture theory to the Toner-Tu theory of dry polar active matter by employing a suitably defined polar order parameter.","We then demonstrate that the complete effective field theory -- which includes all the soft modes and the relevant nonlinear terms -- belongs to the (Burgers-) Kardar-Parisi-Zhang universality class.","This classification allows us to prove the stability of the emergent polar long-range order in scalar non-reciprocal mixtures in two dimensions, and hence a conclusive violation of the Mermin-Wagner theorem."],"url":"http://arxiv.org/abs/2404.05396v1","category":"cond-mat.soft"}
{"created":"2024-04-08 10:52:29","title":"PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation","abstract":"Beyond class frequency, we recognize the impact of class-wise relationships among various class-specific predictions and the imbalance in label masks on long-tailed segmentation learning. To address these challenges, we propose an innovative Pixel-wise Adaptive Training (PAT) technique tailored for long-tailed segmentation. PAT has two key features: 1) class-wise gradient magnitude homogenization, and 2) pixel-wise class-specific loss adaptation (PCLA). First, the class-wise gradient magnitude homogenization helps alleviate the imbalance among label masks by ensuring equal consideration of the class-wise impact on model updates. Second, PCLA tackles the detrimental impact of both rare classes within the long-tailed distribution and inaccurate predictions from previous training stages by encouraging learning classes with low prediction confidence and guarding against forgetting classes with high confidence. This combined approach fosters robust learning while preventing the model from forgetting previously learned knowledge. PAT exhibits significant performance improvements, surpassing the current state-of-the-art by 2.2% in the NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and intersection over union value by 2.07%, with a particularly notable declination of 0.39% in detecting rare classes compared to Balance Logits Variation, as demonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and NYU.","sentences":["Beyond class frequency, we recognize the impact of class-wise relationships among various class-specific predictions and the imbalance in label masks on long-tailed segmentation learning.","To address these challenges, we propose an innovative Pixel-wise Adaptive Training (PAT) technique tailored for long-tailed segmentation.","PAT has two key features: 1) class-wise gradient magnitude homogenization, and 2) pixel-wise class-specific loss adaptation (PCLA).","First, the class-wise gradient magnitude homogenization helps alleviate the imbalance among label masks by ensuring equal consideration of the class-wise impact on model updates.","Second, PCLA tackles the detrimental impact of both rare classes within the long-tailed distribution and inaccurate predictions from previous training stages by encouraging learning classes with low prediction confidence and guarding against forgetting classes with high confidence.","This combined approach fosters robust learning while preventing the model from forgetting previously learned knowledge.","PAT exhibits significant performance improvements, surpassing the current state-of-the-art by 2.2% in the NyU dataset.","Moreover, it enhances overall pixel-wise accuracy by 2.85% and intersection over union value by 2.07%, with a particularly notable declination of 0.39% in detecting rare classes compared to Balance Logits Variation, as demonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and NYU."],"url":"http://arxiv.org/abs/2404.05393v1","category":"cs.CV"}
{"created":"2024-04-08 10:50:47","title":"Escape time in bistable neuronal populations driven by colored synaptic noise","abstract":"Local networks of neurons are nonlinear systems driven by synaptic currents elicited by its own spiking activity and the input received from other brain areas. Synaptic currents are well approximated by correlated Gaussian noise. Besides, the population dynamics of neuronal networks is often found to be multistable, allowing the noise source to induce state transitions. State changes in neuronal systems underlies the way information is encoded and transformed. The characterization of the escape time from metastable states is then a cornerstone to understand how information is processed in the brain. The effects of correlated input forcing bistable systems have been studied for over half a century, nonetheless most results are perturbative or valid only when a separation of time scales is present. Here, we present a novel and exact result holding when the correlation time of the noise source is identical to that of the neural population, hence solving in a very general setting the mean escape time problem.","sentences":["Local networks of neurons are nonlinear systems driven by synaptic currents elicited by its own spiking activity and the input received from other brain areas.","Synaptic currents are well approximated by correlated Gaussian noise.","Besides, the population dynamics of neuronal networks is often found to be multistable, allowing the noise source to induce state transitions.","State changes in neuronal systems underlies the way information is encoded and transformed.","The characterization of the escape time from metastable states is then a cornerstone to understand how information is processed in the brain.","The effects of correlated input forcing bistable systems have been studied for over half a century, nonetheless most results are perturbative or valid only when a separation of time scales is present.","Here, we present a novel and exact result holding when the correlation time of the noise source is identical to that of the neural population, hence solving in a very general setting the mean escape time problem."],"url":"http://arxiv.org/abs/2404.05391v1","category":"q-bio.NC"}
{"created":"2024-04-08 10:49:58","title":"On the existence and rigidity of critical Z2 eigenvalues","abstract":"In this article, we study the eigenvalues and eigenfunction problems for the Laplace operator on multivalued functions, defined on the complement of the 2n points on the round sphere. These eigenvalues and eigensections could also be viewed as functions on the configuration spaces of points, introduced and systematically studied by Taubes-Wu. Critical eigenfunctions, which serve as local singularity models for gauge theoretical problems, are of particular interest.   Our study focuses on the existence and rigidity problems pertaining to these critical eigenfunctions. We prove that for generic configurations, the critical eigenfunctions do not exist. Furthermore, for each n>1, we construct infinitely many configurations that admit critical eigensections. Additionally, we show that the Taubes-Wu tetrahedral eigensections are deformation rigid and non-degenerate. Our main tools are algebraic identities developed by Taubes-Wu and finite group representation theory.","sentences":["In this article, we study the eigenvalues and eigenfunction problems for the Laplace operator on multivalued functions, defined on the complement of the 2n points on the round sphere.","These eigenvalues and eigensections could also be viewed as functions on the configuration spaces of points, introduced and systematically studied by Taubes-Wu.","Critical eigenfunctions, which serve as local singularity models for gauge theoretical problems, are of particular interest.   ","Our study focuses on the existence and rigidity problems pertaining to these critical eigenfunctions.","We prove that for generic configurations, the critical eigenfunctions do not exist.","Furthermore, for each n>1, we construct infinitely many configurations that admit critical eigensections.","Additionally, we show that the Taubes-Wu tetrahedral eigensections are deformation rigid and non-degenerate.","Our main tools are algebraic identities developed by Taubes-Wu and finite group representation theory."],"url":"http://arxiv.org/abs/2404.05387v1","category":"math.DG"}
{"created":"2024-04-08 10:45:29","title":"Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance","abstract":"Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG.","sentences":["Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space.","However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality.","To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models.","Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step.","In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions.","Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level.","Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost.","our codes are available at https://github.com/SmilesDZgk/S-CFG."],"url":"http://arxiv.org/abs/2404.05384v1","category":"cs.CV"}
{"created":"2024-04-08 10:37:44","title":"Regular occupation measures of Volterra processes","abstract":"In this work, we give a local non-determinism condition applicable to general Volterra Ito processes that allow us to obtain the space-time regularity of the occupation measure and the self-intersection measure. For the particular case of solutions to a stochastic Volterra equation, we also obtain the regularity of the one-dimensional distributions and study the absolute continuity of the finite-dimensional distributions. Finally, based on the previously shown regularity of the self-intersection measure, we prove the existence, uniqueness and stability of Volterra equations with distributional drifts of \"self-intersection\" type in terms of corresponding nonlinear Young equations.","sentences":["In this work, we give a local non-determinism condition applicable to general Volterra Ito processes that allow us to obtain the space-time regularity of the occupation measure and the self-intersection measure.","For the particular case of solutions to a stochastic Volterra equation, we also obtain the regularity of the one-dimensional distributions and study the absolute continuity of the finite-dimensional distributions.","Finally, based on the previously shown regularity of the self-intersection measure, we prove the existence, uniqueness and stability of Volterra equations with distributional drifts of \"self-intersection\" type in terms of corresponding nonlinear Young equations."],"url":"http://arxiv.org/abs/2404.05381v1","category":"math.PR"}
{"created":"2024-04-08 10:35:05","title":"Nonlinear chiral forms in the Sen formulation","abstract":"The Sen formulation for chiral $(2p)$-form in $4p+2$ dimensions describes a system with two separate sectors, one is physical while the other is unphysical. Each contains a chiral form and a metric. In this paper, we focus on the cases where the self-duality condition for the unphysical sector is linear while for the physical sector can be nonlinear. We show that in order for the decoupling to be realised at the Hamiltonian and Lagrangian levels, the action should take a certain form. The decoupling at the Hamiltonian level follows the idea in the literature. Then by an appropriate field redefinition of the corresponding first-order Lagrangian, the separation at the Lagrangian level follows. We derive the diffeomorphism of the theory in the case where the chiral form in the physical sector is nonlinear and couple to external $(2p+1)$-form field. Explicit forms of Sen theories are also discussed. The Lagrangian for the quadratic theory is separated into two Henneaux-Teitelboim Lagrangians. We also discuss the method of generating explicit nonlinear theories with $p=1$ is discussed. Finally, we also show that the M5-brane action in the Sen formulation is separated into a Henneaux-Teitleboim action in unphysical sector and a gauge-fixed PST in physical sector.","sentences":["The Sen formulation for chiral $(2p)$-form in $4p+2$ dimensions describes a system with two separate sectors, one is physical while the other is unphysical.","Each contains a chiral form and a metric.","In this paper, we focus on the cases where the self-duality condition for the unphysical sector is linear while for the physical sector can be nonlinear.","We show that in order for the decoupling to be realised at the Hamiltonian and Lagrangian levels, the action should take a certain form.","The decoupling at the Hamiltonian level follows the idea in the literature.","Then by an appropriate field redefinition of the corresponding first-order Lagrangian, the separation at the Lagrangian level follows.","We derive the diffeomorphism of the theory in the case where the chiral form in the physical sector is nonlinear and couple to external $(2p+1)$-form field.","Explicit forms of Sen theories are also discussed.","The Lagrangian for the quadratic theory is separated into two Henneaux-Teitelboim Lagrangians.","We also discuss the method of generating explicit nonlinear theories with $p=1$ is discussed.","Finally, we also show that the M5-brane action in the Sen formulation is separated into a Henneaux-Teitleboim action in unphysical sector and a gauge-fixed PST in physical sector."],"url":"http://arxiv.org/abs/2404.05380v1","category":"hep-th"}
{"created":"2024-04-08 10:28:45","title":"Seamlessly merging radar ranging/imaging, wireless communications, and spectrum sensing, for 6G empowered by microwave photonics","abstract":"Integration of radar, wireless communications, and spectrum sensing is being investigated for 6G with an increased spectral efficiency. Microwave photonics (MWP), a technique that combines microwave engineering and photonic technology to take advantage of the wide bandwidth offered by photonics for microwave signal generation and processing is considered an effective solution for the implementation of the integration. In this paper, an MWP-assisted joint radar, wireless communications, and spectrum sensing (JRCSS) system that enables precise perception of the surrounding physical and electromagnetic environments while maintaining high-speed data communication is proposed and demonstrated. Communication signals and frequency-sweep signals are merged in the optical domain to achieve high-speed radar ranging and imaging, high-data-rate wireless communications, and wideband spectrum sensing. In an experimental demonstration, a JRCSS system supporting radar ranging with a measurement error within $\\pm$ 4 cm, two-dimensional imaging with a resolution of 25 $\\times$ 24.7 mm, wireless communications with a data rate of 2 Gbaud, and spectrum sensing with a frequency measurement error within $\\pm$ 10 MHz in a 6-GHz bandwidth, is demonstrated.","sentences":["Integration of radar, wireless communications, and spectrum sensing is being investigated for 6G with an increased spectral efficiency.","Microwave photonics (MWP), a technique that combines microwave engineering and photonic technology to take advantage of the wide bandwidth offered by photonics for microwave signal generation and processing is considered an effective solution for the implementation of the integration.","In this paper, an MWP-assisted joint radar, wireless communications, and spectrum sensing (JRCSS) system that enables precise perception of the surrounding physical and electromagnetic environments while maintaining high-speed data communication is proposed and demonstrated.","Communication signals and frequency-sweep signals are merged in the optical domain to achieve high-speed radar ranging and imaging, high-data-rate wireless communications, and wideband spectrum sensing.","In an experimental demonstration, a JRCSS system supporting radar ranging with a measurement error within $\\pm$ 4 cm, two-dimensional imaging with a resolution of 25 $\\times$ 24.7 mm, wireless communications with a data rate of 2 Gbaud, and spectrum sensing with a frequency measurement error within $\\pm$ 10 MHz in a 6-GHz bandwidth, is demonstrated."],"url":"http://arxiv.org/abs/2404.05374v1","category":"eess.SP"}
{"created":"2024-04-08 10:28:32","title":"Testing of Pythia modes to study identified particle production in high-multiplicity pp collisions at $\\mathbf{\\sqrt{s}}$ = 7 TeV","abstract":"This study presents a comprehensive analysis of particle production in proton-proton ($pp$) collisions at $\\sqrt{s}$ = 7 TeV using Pythia~8 event generator. We investigate the transverse momentum $p_T$ spectra of light charged hadrons ($\\pi^\\pm$, $K^\\pm$ and $p(\\bar p)$), their yield ratios ($\\pi^-/\\pi^+$, $K^-/K^+$ and $\\bar{p}/p$), and $p_T$-differential ratios ($(K^++K^-)/(\\pi^++\\pi^-)$, $(\\overline{p}+p)/(\\pi^++\\pi^-)$) and mean transverse momentum ($\\langle p_\\mathrm{T} \\rangle$). Our analysis employs various Pythia~8 tunes (Simple, Vincia, and Dire) to explore the impact of different model configurations on particle production. We optimize a key parameter ($p_\\mathrm{T}HatMin$) within each tune to achieve the best agreement between the simulated \\ppt spectra and those measured by the CMS collaboration. Interestingly, we find that the optimal values for $p_\\mathrm{T}HatMin$ differ between hadron species, potentially reflecting the influence of particle mass on production mechanisms. It is not possible to simultaneously and qualitatively describe both, the strangeness enhancement and collectivity in $pp$ collisions from \\pythia~8. Further investigation such as final-state effects such as color ropes or junctions may require to explain these effects. These types of studies help us identify limitations in current models and refine their parameters to better explain experimental observations.","sentences":["This study presents a comprehensive analysis of particle production in proton-proton ($pp$) collisions at $\\sqrt{s}$ = 7 TeV using Pythia~8 event generator.","We investigate the transverse momentum $p_T$ spectra of light charged hadrons ($\\pi^\\pm$, $K^\\pm$ and $p(\\bar p)$), their yield ratios ($\\pi^-/\\pi^+$, $K^-/K^+$ and $\\bar{p}/p$), and $p_T$-differential ratios ($(K^++K^-)/(\\pi^++\\pi^-)$, $(\\overline{p}+p)/(\\pi^++\\pi^-)$) and mean transverse momentum ($\\langle p_\\mathrm{T} \\rangle$).","Our analysis employs various Pythia~8 tunes (Simple, Vincia, and Dire) to explore the impact of different model configurations on particle production.","We optimize a key parameter ($p_\\mathrm{T}HatMin$) within each tune to achieve the best agreement between the simulated \\ppt spectra and those measured by the CMS collaboration.","Interestingly, we find that the optimal values for $p_\\mathrm{T}HatMin$ differ between hadron species, potentially reflecting the influence of particle mass on production mechanisms.","It is not possible to simultaneously and qualitatively describe both, the strangeness enhancement and collectivity in $pp$ collisions from \\pythia~8.","Further investigation such as final-state effects such as color ropes or junctions may require to explain these effects.","These types of studies help us identify limitations in current models and refine their parameters to better explain experimental observations."],"url":"http://arxiv.org/abs/2404.05373v1","category":"hep-ph"}
{"created":"2024-04-08 10:24:59","title":"The PEAL Method: a mathematical framework to streamline securitization structuring","abstract":"Securitization is a financial process where the cash flows of income-generating assets are sold to institutional investors as securities, liquidating illiquid assets. This practice presents persistent challenges due to the absence of a comprehensive mathematical framework for structuring asset-backed securities. While existing literature provides technical analysis of credit risk modeling, there remains a need for a definitive framework detailing the allocation of the inbound cash flows to the outbound positions. To fill this gap, we introduce the PEAL Method: a 10-step mathematical framework to streamline the securitization structuring across all time periods.   The PEAL Method offers a rigorous and versatile approach, allowing practitioners to structure various types of securitizations, including those with complex vertical positions. By employing standardized equations, it facilitates the delineation of payment priorities and enhances risk characterization for both the asset and the liability sides throughout the securitization life cycle.   In addition to its technical contributions, the PEAL Method aims to elevate industry standards by addressing longstanding challenges in securitization. By providing detailed information to investors and enabling transparent risk profile comparisons, it promotes market transparency and enables stronger regulatory oversight.   In summary, the PEAL Method represents a significant advancement in securitization literature, offering a standardized framework for precision and efficiency in structuring transactions. Its adoption has the potential to drive innovation and enhance risk management practices in the securitization market.","sentences":["Securitization is a financial process where the cash flows of income-generating assets are sold to institutional investors as securities, liquidating illiquid assets.","This practice presents persistent challenges due to the absence of a comprehensive mathematical framework for structuring asset-backed securities.","While existing literature provides technical analysis of credit risk modeling, there remains a need for a definitive framework detailing the allocation of the inbound cash flows to the outbound positions.","To fill this gap, we introduce the PEAL Method: a 10-step mathematical framework to streamline the securitization structuring across all time periods.   ","The PEAL Method offers a rigorous and versatile approach, allowing practitioners to structure various types of securitizations, including those with complex vertical positions.","By employing standardized equations, it facilitates the delineation of payment priorities and enhances risk characterization for both the asset and the liability sides throughout the securitization life cycle.   ","In addition to its technical contributions, the PEAL Method aims to elevate industry standards by addressing longstanding challenges in securitization.","By providing detailed information to investors and enabling transparent risk profile comparisons, it promotes market transparency and enables stronger regulatory oversight.   ","In summary, the PEAL Method represents a significant advancement in securitization literature, offering a standardized framework for precision and efficiency in structuring transactions.","Its adoption has the potential to drive innovation and enhance risk management practices in the securitization market."],"url":"http://arxiv.org/abs/2404.05372v1","category":"q-fin.RM"}
{"created":"2024-04-08 10:10:30","title":"Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural Network Accelerators","abstract":"Energy efficiency and memory footprint of a convolutional neural network (CNN) implemented on a CNN inference accelerator depend on many factors, including a weight quantization strategy (i.e., data types and bit-widths) and mapping (i.e., placement and scheduling of DNN elementary operations on hardware units of the accelerator). We show that enabling rich mixed quantization schemes during the implementation can open a previously hidden space of mappings that utilize the hardware resources more effectively. CNNs utilizing quantized weights and activations and suitable mappings can significantly improve trade-offs among the accuracy, energy, and memory requirements compared to less carefully optimized CNN implementations. To find, analyze, and exploit these mappings, we: (i) extend a general-purpose state-of-the-art mapping tool (Timeloop) to support mixed quantization, which is not currently available; (ii) propose an efficient multi-objective optimization algorithm to find the most suitable bit-widths and mapping for each DNN layer executed on the accelerator; and (iii) conduct a detailed experimental evaluation to validate the proposed method. On two CNNs (MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show that for a given quality metric (such as the accuracy on ImageNet), energy savings are up to 37% without any accuracy drop.","sentences":["Energy efficiency and memory footprint of a convolutional neural network (CNN) implemented on a CNN inference accelerator depend on many factors, including a weight quantization strategy (i.e., data types and bit-widths) and mapping (i.e., placement and scheduling of DNN elementary operations on hardware units of the accelerator).","We show that enabling rich mixed quantization schemes during the implementation can open a previously hidden space of mappings that utilize the hardware resources more effectively.","CNNs utilizing quantized weights and activations and suitable mappings can significantly improve trade-offs among the accuracy, energy, and memory requirements compared to less carefully optimized CNN implementations.","To find, analyze, and exploit these mappings, we: (i) extend a general-purpose state-of-the-art mapping tool (Timeloop) to support mixed quantization, which is not currently available; (ii) propose an efficient multi-objective optimization algorithm to find the most suitable bit-widths and mapping for each DNN layer executed on the accelerator; and (iii) conduct a detailed experimental evaluation to validate the proposed method.","On two CNNs (MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show that for a given quality metric (such as the accuracy on ImageNet), energy savings are up to 37% without any accuracy drop."],"url":"http://arxiv.org/abs/2404.05368v1","category":"cs.AR"}
{"created":"2024-04-08 10:05:24","title":"CDAD-Net: Bridging Domain Gaps in Generalized Category Discovery","abstract":"In Generalized Category Discovery (GCD), we cluster unlabeled samples of known and novel classes, leveraging a training dataset of known classes. A salient challenge arises due to domain shifts between these datasets. To address this, we present a novel setting: Across Domain Generalized Category Discovery (AD-GCD) and bring forth CDAD-NET (Class Discoverer Across Domains) as a remedy. CDAD-NET is architected to synchronize potential known class samples across both the labeled (source) and unlabeled (target) datasets, while emphasizing the distinct categorization of the target data. To facilitate this, we propose an entropy-driven adversarial learning strategy that accounts for the distance distributions of target samples relative to source-domain class prototypes. Parallelly, the discriminative nature of the shared space is upheld through a fusion of three metric learning objectives. In the source domain, our focus is on refining the proximity between samples and their affiliated class prototypes, while in the target domain, we integrate a neighborhood-centric contrastive learning mechanism, enriched with an adept neighborsmining approach. To further accentuate the nuanced feature interrelation among semantically aligned images, we champion the concept of conditional image inpainting, underscoring the premise that semantically analogous images prove more efficacious to the task than their disjointed counterparts. Experimentally, CDAD-NET eclipses existing literature with a performance increment of 8-15% on three AD-GCD benchmarks we present.","sentences":["In Generalized Category Discovery (GCD), we cluster unlabeled samples of known and novel classes, leveraging a training dataset of known classes.","A salient challenge arises due to domain shifts between these datasets.","To address this, we present a novel setting:","Across Domain Generalized Category Discovery (AD-GCD) and bring forth CDAD-NET (Class Discoverer Across Domains) as a remedy.","CDAD-NET is architected to synchronize potential known class samples across both the labeled (source) and unlabeled (target) datasets, while emphasizing the distinct categorization of the target data.","To facilitate this, we propose an entropy-driven adversarial learning strategy that accounts for the distance distributions of target samples relative to source-domain class prototypes.","Parallelly, the discriminative nature of the shared space is upheld through a fusion of three metric learning objectives.","In the source domain, our focus is on refining the proximity between samples and their affiliated class prototypes, while in the target domain, we integrate a neighborhood-centric contrastive learning mechanism, enriched with an adept neighborsmining approach.","To further accentuate the nuanced feature interrelation among semantically aligned images, we champion the concept of conditional image inpainting, underscoring the premise that semantically analogous images prove more efficacious to the task than their disjointed counterparts.","Experimentally, CDAD-NET eclipses existing literature with a performance increment of 8-15% on three AD-GCD benchmarks we present."],"url":"http://arxiv.org/abs/2404.05366v1","category":"cs.CV"}
{"created":"2024-04-08 10:04:55","title":"NLP Progress in Indigenous Latin American Languages","abstract":"The paper focuses on the marginalization of indigenous language communities in the face of rapid technological advancements. We highlight the cultural richness of these languages and the risk they face of being overlooked in the realm of Natural Language Processing (NLP). We aim to bridge the gap between these communities and researchers, emphasizing the need for inclusive technological advancements that respect indigenous community perspectives. We show the NLP progress of indigenous Latin American languages and the survey that covers the status of indigenous languages in Latin America, their representation in NLP, and the challenges and innovations required for their preservation and development. The paper contributes to the current literature in understanding the need and progress of NLP for indigenous communities of Latin America, specifically low-resource and indigenous communities in general.","sentences":["The paper focuses on the marginalization of indigenous language communities in the face of rapid technological advancements.","We highlight the cultural richness of these languages and the risk they face of being overlooked in the realm of Natural Language Processing (NLP).","We aim to bridge the gap between these communities and researchers, emphasizing the need for inclusive technological advancements that respect indigenous community perspectives.","We show the NLP progress of indigenous Latin American languages and the survey that covers the status of indigenous languages in Latin America, their representation in NLP, and the challenges and innovations required for their preservation and development.","The paper contributes to the current literature in understanding the need and progress of NLP for indigenous communities of Latin America, specifically low-resource and indigenous communities in general."],"url":"http://arxiv.org/abs/2404.05365v1","category":"cs.CL"}
{"created":"2024-04-08 10:00:09","title":"Autoregressive Search of Gravitational Waves: Denoising","abstract":"Because of the small strain amplitudes of gravitational-wave (GW) signals, unveiling them in the presence of detector/environmental noise is challenging. For visualizing the signals and extracting its waveform for a comparison with theoretical prediction, a frequency-domain whitening process is commonly adopted for filtering the data. In this work, we propose an alternative template-free framework based on autoregressive modeling for denoising the GW data and extracting the waveform. We have tested our framework on extracting the injected signals from the simulated data as well as a series of known compact binary coalescence (CBC) events from the LIGO data. Comparing with the conventional whitening procedure, our methodology generally yields improved cross-correlation and reduced root mean square errors with respect to the signal model.","sentences":["Because of the small strain amplitudes of gravitational-wave (GW) signals, unveiling them in the presence of detector/environmental noise is challenging.","For visualizing the signals and extracting its waveform for a comparison with theoretical prediction, a frequency-domain whitening process is commonly adopted for filtering the data.","In this work, we propose an alternative template-free framework based on autoregressive modeling for denoising the GW data and extracting the waveform.","We have tested our framework on extracting the injected signals from the simulated data as well as a series of known compact binary coalescence (CBC) events from the LIGO data.","Comparing with the conventional whitening procedure, our methodology generally yields improved cross-correlation and reduced root mean square errors with respect to the signal model."],"url":"http://arxiv.org/abs/2404.05364v1","category":"astro-ph.HE"}
{"created":"2024-04-08 09:52:19","title":"Improving Algorithm-Selection and Performance-Prediction via Learning Discriminating Training Samples","abstract":"The choice of input-data used to train algorithm-selection models is recognised as being a critical part of the model success. Recently, feature-free methods for algorithm-selection that use short trajectories obtained from running a solver as input have shown promise. However, it is unclear to what extent these trajectories reliably discriminate between solvers. We propose a meta approach to generating discriminatory trajectories with respect to a portfolio of solvers. The algorithm-configuration tool irace is used to tune the parameters of a simple Simulated Annealing algorithm (SA) to produce trajectories that maximise the performance metrics of ML models trained on this data. We show that when the trajectories obtained from the tuned SA algorithm are used in ML models for algorithm-selection and performance prediction, we obtain significantly improved performance metrics compared to models trained both on raw trajectory data and on exploratory landscape features.","sentences":["The choice of input-data used to train algorithm-selection models is recognised as being a critical part of the model success.","Recently, feature-free methods for algorithm-selection that use short trajectories obtained from running a solver as input have shown promise.","However, it is unclear to what extent these trajectories reliably discriminate between solvers.","We propose a meta approach to generating discriminatory trajectories with respect to a portfolio of solvers.","The algorithm-configuration tool irace is used to tune the parameters of a simple Simulated Annealing algorithm (SA) to produce trajectories that maximise the performance metrics of ML models trained on this data.","We show that when the trajectories obtained from the tuned SA algorithm are used in ML models for algorithm-selection and performance prediction, we obtain significantly improved performance metrics compared to models trained both on raw trajectory data and on exploratory landscape features."],"url":"http://arxiv.org/abs/2404.05359v1","category":"cs.NE"}
{"created":"2024-04-08 09:47:29","title":"Necessary Conditions for Earthly Life Floating in the Venusian Atmosphere","abstract":"Millimeter-waveband spectra of Venus from both the James Clerk Maxwell Telescope (JCMT) and the Atacama Large Millimeter/submillimeter Array (ALMA) provide conclusive evidence (signal-to-noise ratio of about $15\\sigma$) of a phosphine absorption-line profile against the thermal background from deeper, hotter layers of the atmosphere. Phosphine is an important biomarker; e.g., the trace of phosphine in the Earth's atmosphere is uniquivocally associated with anthropogenic activity and microbial life (which produces this highly reducing gas even in an overall oxidizing environment). Motivated by the JCMT and ALMA tantalizing observations we reexamine whether Venus could accommodate Earthly life. More concretly, we hypothesize that the microorganisms populating the venusian atmosphere are not free floating but confined to the liquid environment inside cloud aerosols or droplets. Armed with this hypothesis, we generalize a study of airborne germ transmission to constrain the maximum size of droplets that could be floating in the venusian atmosphere and estimate whether their Stokes fallout times to reach moderately high temperatures are pronouncedly larger than the microbe's replication time. We also comment on the effect of cosmic ray showers on the evolution of aerial microbial life.","sentences":["Millimeter-waveband spectra of Venus from both the James Clerk Maxwell Telescope (JCMT) and the Atacama Large Millimeter/submillimeter Array (ALMA) provide conclusive evidence (signal-to-noise ratio of about $15\\sigma$) of a phosphine absorption-line profile against the thermal background from deeper, hotter layers of the atmosphere.","Phosphine is an important biomarker; e.g., the trace of phosphine in the Earth's atmosphere is uniquivocally associated with anthropogenic activity and microbial life (which produces this highly reducing gas even in an overall oxidizing environment).","Motivated by the JCMT and ALMA tantalizing observations we reexamine whether Venus could accommodate Earthly life.","More concretly, we hypothesize that the microorganisms populating the venusian atmosphere are not free floating but confined to the liquid environment inside cloud aerosols or droplets.","Armed with this hypothesis, we generalize a study of airborne germ transmission to constrain the maximum size of droplets that could be floating in the venusian atmosphere and estimate whether their Stokes fallout times to reach moderately high temperatures are pronouncedly larger than the microbe's replication time.","We also comment on the effect of cosmic ray showers on the evolution of aerial microbial life."],"url":"http://arxiv.org/abs/2404.05356v1","category":"astro-ph.EP"}
{"created":"2024-04-08 09:46:01","title":"Nonlinear Model Reduction to Temporally Aperiodic Spectral Submanifolds","abstract":"We extend the theory of spectral submanifolds (SSMs) to general non-autonomous dynamical systems that are either weakly forced or slowly varying. Examples of such systems arise in structural dynamics, fluid-structure interactions and control problems. The time-dependent SSMs we construct under these assumptions are normally hyperbolic and hence will persist for larger forcing and faster time dependence that are beyond the reach of our precise existence theory. For this reason, we also derive formal asymptotic expansions that, under explicitly verifiable nonresonance conditions, approximate SSMs and their aperiodic anchor trajectories accurately for stronger, faster or even temporally discontinuous forcing. Reducing the dynamical system to these persisting SSMs provides a mathematically justified model reduction technique for non-autonomous physical systems whose time dependance is moderate either in magnitude or speed. We illustrate the existence, persistence and computation of temporally aperiodic SSMs in mechanical examples under chaotic forcing.","sentences":["We extend the theory of spectral submanifolds (SSMs) to general non-autonomous dynamical systems that are either weakly forced or slowly varying.","Examples of such systems arise in structural dynamics, fluid-structure interactions and control problems.","The time-dependent SSMs we construct under these assumptions are normally hyperbolic and hence will persist for larger forcing and faster time dependence that are beyond the reach of our precise existence theory.","For this reason, we also derive formal asymptotic expansions that, under explicitly verifiable nonresonance conditions, approximate SSMs and their aperiodic anchor trajectories accurately for stronger, faster or even temporally discontinuous forcing.","Reducing the dynamical system to these persisting SSMs provides a mathematically justified model reduction technique for non-autonomous physical systems whose time dependance is moderate either in magnitude or speed.","We illustrate the existence, persistence and computation of temporally aperiodic SSMs in mechanical examples under chaotic forcing."],"url":"http://arxiv.org/abs/2404.05355v1","category":"math.DS"}
{"created":"2024-04-08 09:38:40","title":"Semi-Supervised Novelty Detection for Precise Ultra-Wideband Error Signal Prediction","abstract":"Ultra-Wideband (UWB) technology is an emerging low-cost solution for localization in a generic environment. However, UWB signal can be affected by signal reflections and non-line-of-sight (NLoS) conditions between anchors; hence, in a broader sense, the specific geometry of the environment and the disposition of obstructing elements in the map may drastically hinder the reliability of UWB for precise robot localization. This work aims to mitigate this problem by learning a map-specific characterization of the UWB quality signal with a fingerprint semi-supervised novelty detection methodology. An unsupervised autoencoder neural network is trained on nominal UWB map conditions, and then it is used to predict errors derived from the introduction of perturbing novelties in the environment. This work poses a step change in the understanding of UWB localization and its reliability in evolving environmental conditions. The resulting performance of the proposed method is proved by fine-grained experiments obtained with a visual tracking ground truth.","sentences":["Ultra-Wideband (UWB) technology is an emerging low-cost solution for localization in a generic environment.","However, UWB signal can be affected by signal reflections and non-line-of-sight (NLoS) conditions between anchors; hence, in a broader sense, the specific geometry of the environment and the disposition of obstructing elements in the map may drastically hinder the reliability of UWB for precise robot localization.","This work aims to mitigate this problem by learning a map-specific characterization of the UWB quality signal with a fingerprint semi-supervised novelty detection methodology.","An unsupervised autoencoder neural network is trained on nominal UWB map conditions, and then it is used to predict errors derived from the introduction of perturbing novelties in the environment.","This work poses a step change in the understanding of UWB localization and its reliability in evolving environmental conditions.","The resulting performance of the proposed method is proved by fine-grained experiments obtained with a visual tracking ground truth."],"url":"http://arxiv.org/abs/2404.05351v1","category":"cs.RO"}
{"created":"2024-04-08 09:33:40","title":"Iterative Refinement Strategy for Automated Data Labeling: Facial Landmark Diagnosis in Medical Imaging","abstract":"Automated data labeling techniques are crucial for accelerating the development of deep learning models, particularly in complex medical imaging applications. However, ensuring accuracy and efficiency remains challenging. This paper presents iterative refinement strategies for automated data labeling in facial landmark diagnosis to enhance accuracy and efficiency for deep learning models in medical applications, including dermatology, plastic surgery, and ophthalmology. Leveraging feedback mechanisms and advanced algorithms, our approach iteratively refines initial labels, reducing reliance on manual intervention while improving label quality. Through empirical evaluation and case studies, we demonstrate the effectiveness of our proposed strategies in deep learning tasks across medical imaging domains. Our results highlight the importance of iterative refinement in automated data labeling to enhance the capabilities of deep learning systems in medical imaging applications.","sentences":["Automated data labeling techniques are crucial for accelerating the development of deep learning models, particularly in complex medical imaging applications.","However, ensuring accuracy and efficiency remains challenging.","This paper presents iterative refinement strategies for automated data labeling in facial landmark diagnosis to enhance accuracy and efficiency for deep learning models in medical applications, including dermatology, plastic surgery, and ophthalmology.","Leveraging feedback mechanisms and advanced algorithms, our approach iteratively refines initial labels, reducing reliance on manual intervention while improving label quality.","Through empirical evaluation and case studies, we demonstrate the effectiveness of our proposed strategies in deep learning tasks across medical imaging domains.","Our results highlight the importance of iterative refinement in automated data labeling to enhance the capabilities of deep learning systems in medical imaging applications."],"url":"http://arxiv.org/abs/2404.05348v1","category":"cs.CV"}
{"created":"2024-04-08 09:28:34","title":"Beyond the Sequence: Statistics-Driven Pre-training for Stabilizing Sequential Recommendation Model","abstract":"The sequential recommendation task aims to predict the item that user is interested in according to his/her historical action sequence. However, inevitable random action, i.e. user randomly accesses an item among multiple candidates or clicks several items at random order, cause the sequence fails to provide stable and high-quality signals. To alleviate the issue, we propose the StatisTics-Driven Pre-traing framework (called STDP briefly). The main idea of the work lies in the exploration of utilizing the statistics information along with the pre-training paradigm to stabilize the optimization of recommendation model. Specifically, we derive two types of statistical information: item co-occurrence across sequence and attribute frequency within the sequence. And we design the following pre-training tasks: 1) The co-occurred items prediction task, which encourages the model to distribute its attention on multiple suitable targets instead of just focusing on the next item that may be unstable. 2) We generate a paired sequence by replacing items with their co-occurred items and enforce its representation close with the original one, thus enhancing the model's robustness to the random noise. 3) To reduce the impact of random on user's long-term preferences, we encourage the model to capture sequence-level frequent attributes. The significant improvement over six datasets demonstrates the effectiveness and superiority of the proposal, and further analysis verified the generalization of the STDP framework on other models.","sentences":["The sequential recommendation task aims to predict the item that user is interested in according to his/her historical action sequence.","However, inevitable random action, i.e. user randomly accesses an item among multiple candidates or clicks several items at random order, cause the sequence fails to provide stable and high-quality signals.","To alleviate the issue, we propose the StatisTics-Driven Pre-traing framework (called STDP briefly).","The main idea of the work lies in the exploration of utilizing the statistics information along with the pre-training paradigm to stabilize the optimization of recommendation model.","Specifically, we derive two types of statistical information: item co-occurrence across sequence and attribute frequency within the sequence.","And we design the following pre-training tasks: 1) The co-occurred items prediction task, which encourages the model to distribute its attention on multiple suitable targets instead of just focusing on the next item that may be unstable.","2) We generate a paired sequence by replacing items with their co-occurred items and enforce its representation close with the original one, thus enhancing the model's robustness to the random noise.","3) To reduce the impact of random on user's long-term preferences, we encourage the model to capture sequence-level frequent attributes.","The significant improvement over six datasets demonstrates the effectiveness and superiority of the proposal, and further analysis verified the generalization of the STDP framework on other models."],"url":"http://arxiv.org/abs/2404.05342v1","category":"cs.IR"}
{"created":"2024-04-08 09:26:59","title":"Robust Classical and Quantum Polarimetry with a Single Nanostructured Metagrating","abstract":"We formulate a new conceptual approach for one-shot complete polarization state measurement with nanostructured metasurfaces applicable to classical light and multi-photon quantum states, by drawing on the principles of generalized quantum measurements based on positive operator-valued measures (POVMs). Accurate polarization reconstruction from a combination of photon counts or correlations from several diffraction orders is robust with respect to even strong fabrication inaccuracies, requiring only a single classical calibration of metasurface transmission.   Furthermore, this approach operates with a single metagrating without interleaving, allowing for the metasurface size reduction while preserving the high transmission efficiency and output beam quality. We theoretically obtained original metasurface designs, fabricated the metasurface from amorphous silicon nanostructures deposited on glass, and experimentally confirmed accurate polarization reconstruction for laser beams.   We also anticipate robust operation under changes in environmental conditions, opening new possibilities for space-based imaging and satellite optics.","sentences":["We formulate a new conceptual approach for one-shot complete polarization state measurement with nanostructured metasurfaces applicable to classical light and multi-photon quantum states, by drawing on the principles of generalized quantum measurements based on positive operator-valued measures (POVMs).","Accurate polarization reconstruction from a combination of photon counts or correlations from several diffraction orders is robust with respect to even strong fabrication inaccuracies, requiring only a single classical calibration of metasurface transmission.   ","Furthermore, this approach operates with a single metagrating without interleaving, allowing for the metasurface size reduction while preserving the high transmission efficiency and output beam quality.","We theoretically obtained original metasurface designs, fabricated the metasurface from amorphous silicon nanostructures deposited on glass, and experimentally confirmed accurate polarization reconstruction for laser beams.   ","We also anticipate robust operation under changes in environmental conditions, opening new possibilities for space-based imaging and satellite optics."],"url":"http://arxiv.org/abs/2404.05340v1","category":"physics.optics"}
{"created":"2024-04-08 09:26:47","title":"Neuromorphic Control of a Pendulum","abstract":"We illustrate the potential of neuromorphic control on the simple mechanical model of a pendulum, with both event-based actuation and sensing. The controller and the pendulum are regarded as event-based systems that occasionally interact to coordinate their respective rhythms. Control occurs through a proper timing of the interacting events. We illustrate the mixed nature of the control design: the design of an automaton, able to generate the right sequence of events, and the design of a regulator, able to tune the timing of events.","sentences":["We illustrate the potential of neuromorphic control on the simple mechanical model of a pendulum, with both event-based actuation and sensing.","The controller and the pendulum are regarded as event-based systems that occasionally interact to coordinate their respective rhythms.","Control occurs through a proper timing of the interacting events.","We illustrate the mixed nature of the control design: the design of an automaton, able to generate the right sequence of events, and the design of a regulator, able to tune the timing of events."],"url":"http://arxiv.org/abs/2404.05339v1","category":"eess.SY"}
{"created":"2024-04-08 09:25:32","title":"Towards Objectively Benchmarking Social Intelligence for Language Agents at Action Level","abstract":"Prominent large language models have exhibited human-level performance in many domains, even enabling the derived agents to simulate human and social interactions. While practical works have substantiated the practicability of grounding language agents in sandbox simulation or embodied simulators, current social intelligence benchmarks either stay at the language level or use subjective metrics. In pursuit of a more realistic and objective evaluation, we introduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which assesses language agents \\textbf{objectively} at the \\textbf{action level} by scrutinizing the goal achievements within the multi-agent simulation. Additionally, we sample conversation scenarios to build a language-level benchmark to provide an economically prudent preliminary evaluation and align with prevailing benchmarks. To gauge the significance of agent architecture, we implement a target-driven planning (TDP) module as an adjunct to the existing agent. Our evaluative findings highlight that the STSS benchmark is challenging for state-of-the-art language agents. Furthermore, it effectively discriminates between distinct language agents, suggesting its usefulness as a benchmark for evaluating both language models and agent architectures.","sentences":["Prominent large language models have exhibited human-level performance in many domains, even enabling the derived agents to simulate human and social interactions.","While practical works have substantiated the practicability of grounding language agents in sandbox simulation or embodied simulators, current social intelligence benchmarks either stay at the language level or use subjective metrics.","In pursuit of a more realistic and objective evaluation, we introduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which assesses language agents \\textbf{objectively} at the \\textbf{action level} by scrutinizing the goal achievements within the multi-agent simulation.","Additionally, we sample conversation scenarios to build a language-level benchmark to provide an economically prudent preliminary evaluation and align with prevailing benchmarks.","To gauge the significance of agent architecture, we implement a target-driven planning (TDP) module as an adjunct to the existing agent.","Our evaluative findings highlight that the STSS benchmark is challenging for state-of-the-art language agents.","Furthermore, it effectively discriminates between distinct language agents, suggesting its usefulness as a benchmark for evaluating both language models and agent architectures."],"url":"http://arxiv.org/abs/2404.05337v1","category":"cs.CL"}
{"created":"2024-04-08 09:20:22","title":"Dark matter phenomenology and phase transition dynamics of the next to minimal composite Higgs model with dilaton","abstract":"In this paper, we conduct a comprehensive study of the Next-to-Minimal Composite Higgs Model (NMCHM) extended with a dilaton field $\\chi$ (denoted as NMCHM$_\\chi$). A pseudo-Nambu-Goldstone boson (pNGB) $\\eta$, resulting from the SO(6)$\\to$SO(5) breaking, serves as a dark matter (DM) candidate. The inclusion of the dilaton field is helpful for evading the stringent constraints from dark matter direct detection, as it allows for an accidental cancellation between the amplitudes of DM-nucleon scattering, an outcome of the mixing between the dilaton and Higgs fields. The presence of the dilaton field also enriches the phase transition patterns in the early universe. We identify two types of phase transitions: (i) a 1-step phase transition, where the chiral symmetry and electroweak symmetry breaking (EWSB) occur simultaneously, and (ii) a 2-step phase transition, where the chiral symmetry breaking transition takes place first, followed by a second phase transition corresponding to EWSB. Since the first-order phase transitions can be strong due to supercooling in our model, we also examine the stochastic background of gravitational waves generated by these phase transitions. We find that these gravitational waves hold promise for detection in future space-based gravitational wave experiments, such as LISA, Taiji, BBO, and DECIGO.","sentences":["In this paper, we conduct a comprehensive study of the Next-to-Minimal Composite Higgs Model (NMCHM) extended with a dilaton field $\\chi$ (denoted as NMCHM$_\\chi$).","A pseudo-Nambu-Goldstone boson (pNGB) $\\eta$, resulting from the SO(6)$\\to$SO(5) breaking, serves as a dark matter (DM) candidate.","The inclusion of the dilaton field is helpful for evading the stringent constraints from dark matter direct detection, as it allows for an accidental cancellation between the amplitudes of DM-nucleon scattering, an outcome of the mixing between the dilaton and Higgs fields.","The presence of the dilaton field also enriches the phase transition patterns in the early universe.","We identify two types of phase transitions: (i) a 1-step phase transition, where the chiral symmetry and electroweak symmetry breaking (EWSB) occur simultaneously, and (ii) a 2-step phase transition, where the chiral symmetry breaking transition takes place first, followed by a second phase transition corresponding to EWSB.","Since the first-order phase transitions can be strong due to supercooling in our model, we also examine the stochastic background of gravitational waves generated by these phase transitions.","We find that these gravitational waves hold promise for detection in future space-based gravitational wave experiments, such as LISA, Taiji, BBO, and DECIGO."],"url":"http://arxiv.org/abs/2404.05332v1","category":"hep-ph"}
{"created":"2024-04-08 09:18:32","title":"Mask-ControlNet: Higher-Quality Image Generation with An Additional Mask Prompt","abstract":"Text-to-image generation has witnessed great progress, especially with the recent advancements in diffusion models. Since texts cannot provide detailed conditions like object appearance, reference images are usually leveraged for the control of objects in the generated images. However, existing methods still suffer limited accuracy when the relationship between the foreground and background is complicated. To address this issue, we develop a framework termed Mask-ControlNet by introducing an additional mask prompt. Specifically, we first employ large vision models to obtain masks to segment the objects of interest in the reference image. Then, the object images are employed as additional prompts to facilitate the diffusion model to better understand the relationship between foreground and background regions during image generation. Experiments show that the mask prompts enhance the controllability of the diffusion model to maintain higher fidelity to the reference image while achieving better image quality. Comparison with previous text-to-image generation methods demonstrates our method's superior quantitative and qualitative performance on the benchmark datasets.","sentences":["Text-to-image generation has witnessed great progress, especially with the recent advancements in diffusion models.","Since texts cannot provide detailed conditions like object appearance, reference images are usually leveraged for the control of objects in the generated images.","However, existing methods still suffer limited accuracy when the relationship between the foreground and background is complicated.","To address this issue, we develop a framework termed Mask-ControlNet by introducing an additional mask prompt.","Specifically, we first employ large vision models to obtain masks to segment the objects of interest in the reference image.","Then, the object images are employed as additional prompts to facilitate the diffusion model to better understand the relationship between foreground and background regions during image generation.","Experiments show that the mask prompts enhance the controllability of the diffusion model to maintain higher fidelity to the reference image while achieving better image quality.","Comparison with previous text-to-image generation methods demonstrates our method's superior quantitative and qualitative performance on the benchmark datasets."],"url":"http://arxiv.org/abs/2404.05331v1","category":"cs.CV"}
{"created":"2024-04-08 09:17:56","title":"Rydberg superatoms: An artificial quantum system for quantum information processing and quantum optics","abstract":"Dense Rydberg atom ensembles display intriguing collective behaviors mediated by their strong, long-range dipole-dipole interactions. These collective effects, often modeled using Rydberg superatoms, have gained significant attention across various fields due to their potential applications in quantum information processing and quantum optics. In this review article, we delve into the theoretical foundations of Rydberg interactions and explore experimental techniques for their manipulation and detection. We also discuss the latest advancements in harnessing Rydberg collective effects for quantum computation and optical quantum technologies. By synthesizing insights from theoretical studies and experimental demonstrations, we aim to provide a comprehensive overview of this rapidly evolving field and its potential impact on the future of quantum technologies.","sentences":["Dense Rydberg atom ensembles display intriguing collective behaviors mediated by their strong, long-range dipole-dipole interactions.","These collective effects, often modeled using Rydberg superatoms, have gained significant attention across various fields due to their potential applications in quantum information processing and quantum optics.","In this review article, we delve into the theoretical foundations of Rydberg interactions and explore experimental techniques for their manipulation and detection.","We also discuss the latest advancements in harnessing Rydberg collective effects for quantum computation and optical quantum technologies.","By synthesizing insights from theoretical studies and experimental demonstrations, we aim to provide a comprehensive overview of this rapidly evolving field and its potential impact on the future of quantum technologies."],"url":"http://arxiv.org/abs/2404.05330v1","category":"quant-ph"}
{"created":"2024-04-08 09:13:55","title":"$XY\\!Z$ spectroscopy at electron-hadron facilities III: Semi-inclusive processes with vector exchanges","abstract":"Inclusive production processes will be important for the first observations of $XYZ$ states at new generation electron-hadron colliders, as they generally benefit from larger cross sections than their exclusive counterparts. We make predictions of semi-inclusive photoproduction of the $\\chi_{c1}(1P)$ and $X(3872)$, whose peripheral production is assumed to be dominated by vector exchanges. We validate the applicability of Vector Meson Dominance in the axial-vector charmonium sector and calculate production rates at center-of-mass energies relevant for future experimental facilities. We find the semi-inclusive cross sections near threshold to be enhanced by a factor of $\\sim 2-3$ compared to the exclusive reaction and well suited for a first observation in photoproduction.","sentences":["Inclusive production processes will be important for the first observations of $XYZ$ states at new generation electron-hadron colliders, as they generally benefit from larger cross sections than their exclusive counterparts.","We make predictions of semi-inclusive photoproduction of the $\\chi_{c1}(1P)$ and $X(3872)$, whose peripheral production is assumed to be dominated by vector exchanges.","We validate the applicability of Vector Meson Dominance in the axial-vector charmonium sector and calculate production rates at center-of-mass energies relevant for future experimental facilities.","We find the semi-inclusive cross sections near threshold to be enhanced by a factor of $\\sim 2-3$ compared to the exclusive reaction and well suited for a first observation in photoproduction."],"url":"http://arxiv.org/abs/2404.05326v1","category":"hep-ph"}
{"created":"2024-04-08 09:13:16","title":"Back to the Future: GNN-based NO$_2$ Forecasting via Future Covariates","abstract":"Due to the latest environmental concerns in keeping at bay contaminants emissions in urban areas, air pollution forecasting has been rising the forefront of all researchers around the world. When predicting pollutant concentrations, it is common to include the effects of environmental factors that influence these concentrations within an extended period, like traffic, meteorological conditions and geographical information. Most of the existing approaches exploit this information as past covariates, i.e., past exogenous variables that affected the pollutant but were not affected by it. In this paper, we present a novel forecasting methodology to predict NO$_2$ concentration via both past and future covariates. Future covariates are represented by weather forecasts and future calendar events, which are already known at prediction time. In particular, we deal with air quality observations in a city-wide network of ground monitoring stations, modeling the data structure and estimating the predictions with a Spatiotemporal Graph Neural Network (STGNN). We propose a conditioning block that embeds past and future covariates into the current observations. After extracting meaningful spatiotemporal representations, these are fused together and projected into the forecasting horizon to generate the final prediction. To the best of our knowledge, it is the first time that future covariates are included in time series predictions in a structured way. Remarkably, we find that conditioning on future weather information has a greater impact than considering past traffic conditions. We release our code implementation at https://github.com/polimi-ispl/MAGCRN.","sentences":["Due to the latest environmental concerns in keeping at bay contaminants emissions in urban areas, air pollution forecasting has been rising the forefront of all researchers around the world.","When predicting pollutant concentrations, it is common to include the effects of environmental factors that influence these concentrations within an extended period, like traffic, meteorological conditions and geographical information.","Most of the existing approaches exploit this information as past covariates, i.e., past exogenous variables that affected the pollutant but were not affected by it.","In this paper, we present a novel forecasting methodology to predict NO$_2$ concentration via both past and future covariates.","Future covariates are represented by weather forecasts and future calendar events, which are already known at prediction time.","In particular, we deal with air quality observations in a city-wide network of ground monitoring stations, modeling the data structure and estimating the predictions with a Spatiotemporal Graph Neural Network (STGNN).","We propose a conditioning block that embeds past and future covariates into the current observations.","After extracting meaningful spatiotemporal representations, these are fused together and projected into the forecasting horizon to generate the final prediction.","To the best of our knowledge, it is the first time that future covariates are included in time series predictions in a structured way.","Remarkably, we find that conditioning on future weather information has a greater impact than considering past traffic conditions.","We release our code implementation at https://github.com/polimi-ispl/MAGCRN."],"url":"http://arxiv.org/abs/2404.05324v1","category":"cs.LG"}
{"created":"2024-04-08 09:13:08","title":"Singular Spectrum Analysis of Exoplanetary Transits","abstract":"Transit photometry is currently the most efficient and sensitive method for detecting extrasolar planets (exoplanets) and a large majority of confirmed exoplanets have been detected with this method. The substantial success of space-based missions such as NASA's Kepler/K2 and Transiting Exoplanet Survey Satellite (TESS) has generated a large and diverse sample of confirmed and candidate exoplanets. Singular Spectrum Analysis (SSA) provides a useful tool for studying photometric time series and exoplanetary transits. SSA is a technique for decomposing a time series into a sum of its main components, where each component is a separate time series that incorporates specific information from the behavior of the initial time series. SSA can be implemented for extracting important information (such as main trends and signals) from the photometry data or reducing the noise factors. The detectability and accurate characterization of an exoplanetary transit signal is principally determined by its signal-to-noise ratio (SNR). Stellar variability of the host star, small planet to star radius ratio, background noises from other sources in the field of observations and instrumental noise can cause lower SNRs and consequently, more complexities or inaccuracies in the modeling of the transit signals, which in turn leads to the inaccurate inference of the astrophysical parameters of the planetary object. Therefore, implementing SSA leads to a more accurate characterization of exoplanetary transits and is also capable of detecting transits with low SNRs ($SNR<10$). In this paper, after discussing the principles and properties of SSA, we investigate its applications for studying photometric transit data and detecting low SNR exoplanet candidates.","sentences":["Transit photometry is currently the most efficient and sensitive method for detecting extrasolar planets (exoplanets) and a large majority of confirmed exoplanets have been detected with this method.","The substantial success of space-based missions such as NASA's Kepler/K2 and Transiting Exoplanet Survey Satellite (TESS) has generated a large and diverse sample of confirmed and candidate exoplanets.","Singular Spectrum Analysis (SSA) provides a useful tool for studying photometric time series and exoplanetary transits.","SSA is a technique for decomposing a time series into a sum of its main components, where each component is a separate time series that incorporates specific information from the behavior of the initial time series.","SSA can be implemented for extracting important information (such as main trends and signals) from the photometry data or reducing the noise factors.","The detectability and accurate characterization of an exoplanetary transit signal is principally determined by its signal-to-noise ratio (SNR).","Stellar variability of the host star, small planet to star radius ratio, background noises from other sources in the field of observations and instrumental noise can cause lower SNRs and consequently, more complexities or inaccuracies in the modeling of the transit signals, which in turn leads to the inaccurate inference of the astrophysical parameters of the planetary object.","Therefore, implementing SSA leads to a more accurate characterization of exoplanetary transits and is also capable of detecting transits with low SNRs ($SNR<10$).","In this paper, after discussing the principles and properties of SSA, we investigate its applications for studying photometric transit data and detecting low SNR exoplanet candidates."],"url":"http://arxiv.org/abs/2404.05323v1","category":"astro-ph.EP"}
{"created":"2024-04-08 09:11:04","title":"A Power Management and Control System for Portable Ecosystem Monitoring Devices","abstract":"Recent advances in Internet of Things (IoT) and Artificial Intelligence (AI) technologies help ecosystem monitoring to shift towards automated monitoring with low power sensors and embedded vision on powerful processing units. Vision-based monitoring devices need an effective power management and control system (PMCS) with system-adapted power input and output capabilities to achieve power-efficient and self-sustainable operation. Here, we present a universal power management solution for automated ecosystem monitoring devices, compatible with commonly used off-the-shelf edge processing units (EPUs). The proposed design is specifically adapted for battery-powered EPU systems by incorporating power-matched energy harvesting (EH), a power switch with low-power sleep mode, and simple system integration in an MCU-less architecture with automated operation. We use a 4-month environmental case study to monitor plant growth under 4mg microplastic (MP) exposure, demonstrating that the setup achieved continuous and sustainable operation. In this plant phenology case study, our power management module is deployed in an embedded vision camera equipped with a 5W solar panel and five various environmental sensors. This work shows the usability of the power management board in environmentally relevant use cases and for tasks in agricultural applications.","sentences":["Recent advances in Internet of Things (IoT) and Artificial Intelligence (AI) technologies help ecosystem monitoring to shift towards automated monitoring with low power sensors and embedded vision on powerful processing units.","Vision-based monitoring devices need an effective power management and control system (PMCS) with system-adapted power input and output capabilities to achieve power-efficient and self-sustainable operation.","Here, we present a universal power management solution for automated ecosystem monitoring devices, compatible with commonly used off-the-shelf edge processing units (EPUs).","The proposed design is specifically adapted for battery-powered EPU systems by incorporating power-matched energy harvesting (EH), a power switch with low-power sleep mode, and simple system integration in an MCU-less architecture with automated operation.","We use a 4-month environmental case study to monitor plant growth under 4mg microplastic (MP) exposure, demonstrating that the setup achieved continuous and sustainable operation.","In this plant phenology case study, our power management module is deployed in an embedded vision camera equipped with a 5W solar panel and five various environmental sensors.","This work shows the usability of the power management board in environmentally relevant use cases and for tasks in agricultural applications."],"url":"http://arxiv.org/abs/2404.05322v1","category":"eess.SY"}
{"created":"2024-04-08 09:09:50","title":"Mapping indefinite causal order processes to composable quantum protocols in a spacetime","abstract":"Formalisms for higher order quantum processes provide a theoretical formalisation of quantum processes where the order of agents' operations need not be definite and acyclic, but may be subject to quantum superpositions. This has led to the concept of indefinite causal structures (ICS) which have garnered much interest. However, the interface between these information-theoretic approaches and spatiotemporal notions of causality is less understood, and questions relating to the physical realisability of ICS in a spatiotemporal context persist despite progress in their information-theoretic characterisation. Further, previous work suggests that composition of processes is not so straightforward in ICS frameworks, which raises the question of how this connects with the observed composability of physical experiments in spacetime. To address these points, we compare the formalism of quantum circuits with quantum control of causal order (QC-QC), which models an interesting class of ICS processes, with that of causal boxes, which models composable quantum information protocols in spacetime. We incorporate the set-up assumptions of the QC-QC framework into the spatiotemporal perspective and show that every QC-QC can be mapped to a causal box that satisfies these set up assumptions and acts on a Fock space while reproducing the QC-QC's behaviour in a relevant subspace defined by the assumptions. Using a recently introduced concept of fine-graining, we show that the causal box corresponds to a fine-graining of the QC-QC, which unravels the original ICS of the QC-QC into a set of quantum operations with a well-defined and acyclic causal order, compatible with the spacetime structure. Our results also clarify how the composability of physical experiments is recovered, while highlighting the essential role of relativistic causality and the Fock space structure.","sentences":["Formalisms for higher order quantum processes provide a theoretical formalisation of quantum processes where the order of agents' operations need not be definite and acyclic, but may be subject to quantum superpositions.","This has led to the concept of indefinite causal structures (ICS) which have garnered much interest.","However, the interface between these information-theoretic approaches and spatiotemporal notions of causality is less understood, and questions relating to the physical realisability of ICS in a spatiotemporal context persist despite progress in their information-theoretic characterisation.","Further, previous work suggests that composition of processes is not so straightforward in ICS frameworks, which raises the question of how this connects with the observed composability of physical experiments in spacetime.","To address these points, we compare the formalism of quantum circuits with quantum control of causal order (QC-QC), which models an interesting class of ICS processes, with that of causal boxes, which models composable quantum information protocols in spacetime.","We incorporate the set-up assumptions of the QC-QC framework into the spatiotemporal perspective and show that every QC-QC can be mapped to a causal box that satisfies these set up assumptions and acts on a Fock space while reproducing the QC-QC's behaviour in a relevant subspace defined by the assumptions.","Using a recently introduced concept of fine-graining, we show that the causal box corresponds to a fine-graining of the QC-QC, which unravels the original ICS of the QC-QC into a set of quantum operations with a well-defined and acyclic causal order, compatible with the spacetime structure.","Our results also clarify how the composability of physical experiments is recovered, while highlighting the essential role of relativistic causality and the Fock space structure."],"url":"http://arxiv.org/abs/2404.05319v1","category":"quant-ph"}
{"created":"2024-04-08 09:08:59","title":"Stochastic Online Optimization for Cyber-Physical and Robotic Systems","abstract":"We propose a novel gradient-based online optimization framework for solving stochastic programming problems that frequently arise in the context of cyber-physical and robotic systems. Our problem formulation accommodates constraints that model the evolution of a cyber-physical system, which has, in general, a continuous state and action space, is nonlinear, and where the state is only partially observed. We also incorporate an approximate model of the dynamics as prior knowledge into the learning process and show that even rough estimates of the dynamics can significantly improve the convergence of our algorithms. Our online optimization framework encompasses both gradient descent and quasi-Newton methods, and we provide a unified convergence analysis of our algorithms in a non-convex setting. We also characterize the impact of modeling errors in the system dynamics on the convergence rate of the algorithms. Finally, we evaluate our algorithms in simulations of a flexible beam, a four-legged walking robot, and in real-world experiments with a ping-pong playing robot.","sentences":["We propose a novel gradient-based online optimization framework for solving stochastic programming problems that frequently arise in the context of cyber-physical and robotic systems.","Our problem formulation accommodates constraints that model the evolution of a cyber-physical system, which has, in general, a continuous state and action space, is nonlinear, and where the state is only partially observed.","We also incorporate an approximate model of the dynamics as prior knowledge into the learning process and show that even rough estimates of the dynamics can significantly improve the convergence of our algorithms.","Our online optimization framework encompasses both gradient descent and quasi-Newton methods, and we provide a unified convergence analysis of our algorithms in a non-convex setting.","We also characterize the impact of modeling errors in the system dynamics on the convergence rate of the algorithms.","Finally, we evaluate our algorithms in simulations of a flexible beam, a four-legged walking robot, and in real-world experiments with a ping-pong playing robot."],"url":"http://arxiv.org/abs/2404.05318v1","category":"cs.LG"}
{"created":"2024-04-08 09:01:27","title":"Second law of horizon thermodynamics during cosmic evolution","abstract":"We examine the second law of thermodynamics in the context of horizon cosmology, in particular, whether the change of total entropy (i.e. the sum of the entropy for the apparent horizon and the entropy for the matter fields) proves to be positive with the cosmic expansion of the universe. The matter fields inside the horizon obey the thermodynamics of an open system as the matter fields has a flux through the apparent horizon, which is either outward or inward depending on the background cosmological dynamics. Regarding the entropy of the apparent horizon, we consider different forms of the horizon entropy like the Tsallis entropy, the R\\'{e}nyi entropy, the Kaniadakis entropy, or even the 4-parameter generalized entropy; and determine the appropriate conditions on the respective entropic parameters coming from the second law of horizon thermodynamics. The constraints on the entropic parameters are found in such a way that it validates the second law of thermodynamics during a wide range of cosmic era of the universe, particularly from inflation to radiation dominated epoch followed by a reheating stage. Importantly, the present work provides a model independent way to constrain the entropic parameters directly from the second law of thermodynamics for the apparent horizon.","sentences":["We examine the second law of thermodynamics in the context of horizon cosmology, in particular, whether the change of total entropy (i.e. the sum of the entropy for the apparent horizon and the entropy for the matter fields) proves to be positive with the cosmic expansion of the universe.","The matter fields inside the horizon obey the thermodynamics of an open system as the matter fields has a flux through the apparent horizon, which is either outward or inward depending on the background cosmological dynamics.","Regarding the entropy of the apparent horizon, we consider different forms of the horizon entropy like the Tsallis entropy, the R\\'{e}nyi entropy, the Kaniadakis entropy, or even the 4-parameter generalized entropy; and determine the appropriate conditions on the respective entropic parameters coming from the second law of horizon thermodynamics.","The constraints on the entropic parameters are found in such a way that it validates the second law of thermodynamics during a wide range of cosmic era of the universe, particularly from inflation to radiation dominated epoch followed by a reheating stage.","Importantly, the present work provides a model independent way to constrain the entropic parameters directly from the second law of thermodynamics for the apparent horizon."],"url":"http://arxiv.org/abs/2404.05312v1","category":"gr-qc"}
{"created":"2024-04-08 08:59:26","title":"BruSLeAttack: A Query-Efficient Score-Based Black-Box Sparse Adversarial Attack","abstract":"We study the unique, less-well understood problem of generating sparse adversarial samples simply by observing the score-based replies to model queries. Sparse attacks aim to discover a minimum number-the l0 bounded-perturbations to model inputs to craft adversarial examples and misguide model decisions. But, in contrast to query-based dense attack counterparts against black-box models, constructing sparse adversarial perturbations, even when models serve confidence score information to queries in a score-based setting, is non-trivial. Because, such an attack leads to i) an NP-hard problem; and ii) a non-differentiable search space. We develop the BruSLeAttack-a new, faster (more query-efficient) Bayesian algorithm for the problem. We conduct extensive attack evaluations including an attack demonstration against a Machine Learning as a Service (MLaaS) offering exemplified by Google Cloud Vision and robustness testing of adversarial training regimes and a recent defense against black-box attacks. The proposed attack scales to achieve state-of-the-art attack success rates and query efficiency on standard computer vision tasks such as ImageNet across different model architectures. Our artefacts and DIY attack samples are available on GitHub. Importantly, our work facilitates faster evaluation of model vulnerabilities and raises our vigilance on the safety, security and reliability of deployed systems.","sentences":["We study the unique, less-well understood problem of generating sparse adversarial samples simply by observing the score-based replies to model queries.","Sparse attacks aim to discover a minimum number-the l0 bounded-perturbations to model inputs to craft adversarial examples and misguide model decisions.","But, in contrast to query-based dense attack counterparts against black-box models, constructing sparse adversarial perturbations, even when models serve confidence score information to queries in a score-based setting, is non-trivial.","Because, such an attack leads to i) an NP-hard problem; and ii) a non-differentiable search space.","We develop the BruSLeAttack-a new, faster (more query-efficient) Bayesian algorithm for the problem.","We conduct extensive attack evaluations including an attack demonstration against a Machine Learning as a Service (MLaaS) offering exemplified by Google Cloud Vision and robustness testing of adversarial training regimes and a recent defense against black-box attacks.","The proposed attack scales to achieve state-of-the-art attack success rates and query efficiency on standard computer vision tasks such as ImageNet across different model architectures.","Our artefacts and DIY attack samples are available on GitHub.","Importantly, our work facilitates faster evaluation of model vulnerabilities and raises our vigilance on the safety, security and reliability of deployed systems."],"url":"http://arxiv.org/abs/2404.05311v1","category":"cs.LG"}
{"created":"2024-04-08 08:57:41","title":"Energy exchange statistics and fluctuation theorem for non-thermal asymptotic states","abstract":"Exchange energy statistics between two bodies at different thermal equilibrium obey the Jarzynski-W\\'ojcik fluctuation theorem. The corresponding energy scale factor is the difference of the inverse temperatures associated to the bodies at equilibrium. In this work, we consider a dissipative quantum dynamics leading the quantum system towards a, possibly non-thermal, asymptotic state. To generalize the Jarzynski-W\\'ojcik theorem to non-thermal states, we identify a sufficient condition ${\\cal I}$ for the existence of an energy scale factor $\\eta^{*}$ that is unique, finite and time-independent, such that the characteristic function of the exchange energy distribution becomes identically equal to $1$ for any time. This $\\eta^*$ plays the role of the difference of inverse temperatures. We discuss the physical interpretation of the condition ${\\cal I}$, showing that it amounts to an almost complete memory loss of the initial state. The robustness of our results against quantifiable deviations from the validity of ${\\cal I}$ is evaluated by experimental studies on a single nitrogen-vacancy center subjected to a sequence of laser pulses and dissipation.","sentences":["Exchange energy statistics between two bodies at different thermal equilibrium obey the Jarzynski-W\\'ojcik fluctuation theorem.","The corresponding energy scale factor is the difference of the inverse temperatures associated to the bodies at equilibrium.","In this work, we consider a dissipative quantum dynamics leading the quantum system towards a, possibly non-thermal, asymptotic state.","To generalize the Jarzynski-W\\'ojcik theorem to non-thermal states, we identify a sufficient condition ${\\cal I}$ for the existence of an energy scale factor $\\eta^{*}$ that is unique, finite and time-independent, such that the characteristic function of the exchange energy distribution becomes identically equal to $1$ for any time.","This $\\eta^*$ plays the role of the difference of inverse temperatures.","We discuss the physical interpretation of the condition ${\\cal I}$, showing that it amounts to an almost complete memory loss of the initial state.","The robustness of our results against quantifiable deviations from the validity of ${\\cal I}$ is evaluated by experimental studies on a single nitrogen-vacancy center subjected to a sequence of laser pulses and dissipation."],"url":"http://arxiv.org/abs/2404.05310v1","category":"quant-ph"}
{"created":"2024-04-08 08:53:54","title":"Human Detection from 4D Radar Data in Low-Visibility Field Conditions","abstract":"Autonomous driving technology is increasingly being used on public roads and in industrial settings such as mines. While it is essential to detect pedestrians, vehicles, or other obstacles, adverse field conditions negatively affect the performance of classical sensors such as cameras or lidars. Radar, on the other hand, is a promising modality that is less affected by, e.g., dust, smoke, water mist or fog. In particular, modern 4D imaging radars provide target responses across the range, vertical angle, horizontal angle and Doppler velocity dimensions. We propose TMVA4D, a CNN architecture that leverages this 4D radar modality for semantic segmentation. The CNN is trained to distinguish between the background and person classes based on a series of 2D projections of the 4D radar data that include the elevation, azimuth, range, and Doppler velocity dimensions. We also outline the process of compiling a novel dataset consisting of data collected in industrial settings with a car-mounted 4D radar and describe how the ground-truth labels were generated from reference thermal images. Using TMVA4D on this dataset, we achieve an mIoU score of 78.2% and an mDice score of 86.1%, evaluated on the two classes background and person","sentences":["Autonomous driving technology is increasingly being used on public roads and in industrial settings such as mines.","While it is essential to detect pedestrians, vehicles, or other obstacles, adverse field conditions negatively affect the performance of classical sensors such as cameras or lidars.","Radar, on the other hand, is a promising modality that is less affected by, e.g., dust, smoke, water mist or fog.","In particular, modern 4D imaging radars provide target responses across the range, vertical angle, horizontal angle and Doppler velocity dimensions.","We propose TMVA4D, a CNN architecture that leverages this 4D radar modality for semantic segmentation.","The CNN is trained to distinguish between the background and person classes based on a series of 2D projections of the 4D radar data that include the elevation, azimuth, range, and Doppler velocity dimensions.","We also outline the process of compiling a novel dataset consisting of data collected in industrial settings with a car-mounted 4D radar and describe how the ground-truth labels were generated from reference thermal images.","Using TMVA4D on this dataset, we achieve an mIoU score of 78.2% and an mDice score of 86.1%, evaluated on the two classes background and person"],"url":"http://arxiv.org/abs/2404.05307v1","category":"cs.CV"}
{"created":"2024-04-08 08:50:24","title":"Faithlessness in Gaussian graphical models","abstract":"The implication problem for conditional independence (CI) asks whether the fact that a probability distribution obeys a given finite set of CI relations implies that a further CI statement also holds in this distribution. This problem has a long and fascinating history, cumulating in positive results about implications now known as the semigraphoid axioms as well as impossibility results about a general finite characterization of CI implications. Motivated by violation of faithfulness assumptions in causal discovery, we study the implication problem in the special setting where the CI relations are obtained from a directed acyclic graphical (DAG) model along with one additional CI statement. Focusing on the Gaussian case, we give a complete characterization of when such an implication is graphical by using algebraic techniques. Moreover, prompted by the relevance of strong faithfulness in statistical guarantees for causal discovery algorithms, we give a graphical solution for an approximate CI implication problem, in which we ask whether small values of one additional partial correlation entail small values for yet a further partial correlation.","sentences":["The implication problem for conditional independence (CI) asks whether the fact that a probability distribution obeys a given finite set of CI relations implies that a further CI statement also holds in this distribution.","This problem has a long and fascinating history, cumulating in positive results about implications now known as the semigraphoid axioms as well as impossibility results about a general finite characterization of CI implications.","Motivated by violation of faithfulness assumptions in causal discovery, we study the implication problem in the special setting where the CI relations are obtained from a directed acyclic graphical (DAG) model along with one additional CI statement.","Focusing on the Gaussian case, we give a complete characterization of when such an implication is graphical by using algebraic techniques.","Moreover, prompted by the relevance of strong faithfulness in statistical guarantees for causal discovery algorithms, we give a graphical solution for an approximate CI implication problem, in which we ask whether small values of one additional partial correlation entail small values for yet a further partial correlation."],"url":"http://arxiv.org/abs/2404.05306v1","category":"math.ST"}
{"created":"2024-04-08 08:46:40","title":"SARIS: Accelerating Stencil Computations on Energy-Efficient RISC-V Compute Clusters with Indirect Stream Registers","abstract":"Stencil codes are performance-critical in many compute-intensive applications, but suffer from significant address calculation and irregular memory access overheads. This work presents SARIS, a general and highly flexible methodology for stencil acceleration using register-mapped indirect streams. We demonstrate SARIS for various stencil codes on an eight-core RISC-V compute cluster with indirect stream registers, achieving significant speedups of 2.72x, near-ideal FPU utilizations of 81%, and energy efficiency improvements of 1.58x over an RV32G baseline on average. Scaling out to a 256-core manycore system, we estimate an average FPU utilization of 64%, an average speedup of 2.14x, and up to 15% higher fractions of peak compute than a leading GPU code generator.","sentences":["Stencil codes are performance-critical in many compute-intensive applications, but suffer from significant address calculation and irregular memory access overheads.","This work presents SARIS, a general and highly flexible methodology for stencil acceleration using register-mapped indirect streams.","We demonstrate SARIS for various stencil codes on an eight-core RISC-V compute cluster with indirect stream registers, achieving significant speedups of 2.72x, near-ideal FPU utilizations of 81%, and energy efficiency improvements of 1.58x over an RV32G baseline on average.","Scaling out to a 256-core manycore system, we estimate an average FPU utilization of 64%, an average speedup of 2.14x, and up to 15% higher fractions of peak compute than a leading GPU code generator."],"url":"http://arxiv.org/abs/2404.05303v1","category":"cs.MS"}
{"created":"2024-04-08 08:46:37","title":"A New Perspective on Kaluza-Klein Theories","abstract":"By assuming that the geometry of spacetime is uniquely determined by the energy momentum tensor of matter alone, i.e. without any interactions, enables us to construct the Lagrangian from which the metric of higher dimensional spacetime follows. From the geodesic equations that follow it becomes clear that the incorrect mass of elementary particles predicted by Kaluza-Klein theories arises from the assumption that in the absence of gravity the solution to the Einstein field equations reduces to the Minkowski metric. From construction of a consistent theory of $4\\mathcal{D}$ electromagnetism, we find that this assumption does not only result in the incorrect mass of elementary particles, but also the incorrect value of the cosmological constant. This suggests that these incorrect predictions, which are often regarded as major flaws of Kaluza-Klein theories, just reflects the inconsistency of some postulates of general relativity and gauge theories. Abandoning this assumption results in modifications of general relativity. We show that the unified description of fundamental interactions naturally incorporates the Higgs mechanism. For non-Abelian gauge fields, we find that the manifold comprising the extra dimensions has to be a group manifold and show that the standard model is realised in 16$\\mathcal{D}$ spacetime. We show that charge and spin are the same concept, but what makes them different is that the former follows from symmetry of $4\\mathcal{D}$ spacetime while the latter follows from symmetry of the internal space.","sentences":["By assuming that the geometry of spacetime is uniquely determined by the energy momentum tensor of matter alone, i.e. without any interactions, enables us to construct the Lagrangian from which the metric of higher dimensional spacetime follows.","From the geodesic equations that follow it becomes clear that the incorrect mass of elementary particles predicted by Kaluza-Klein theories arises from the assumption that in the absence of gravity the solution to the Einstein field equations reduces to the Minkowski metric.","From construction of a consistent theory of $4\\mathcal{D}$ electromagnetism, we find that this assumption does not only result in the incorrect mass of elementary particles, but also the incorrect value of the cosmological constant.","This suggests that these incorrect predictions, which are often regarded as major flaws of Kaluza-Klein theories, just reflects the inconsistency of some postulates of general relativity and gauge theories.","Abandoning this assumption results in modifications of general relativity.","We show that the unified description of fundamental interactions naturally incorporates the Higgs mechanism.","For non-Abelian gauge fields, we find that the manifold comprising the extra dimensions has to be a group manifold and show that the standard model is realised in 16$\\mathcal{D}$ spacetime.","We show that charge and spin are the same concept, but what makes them different is that the former follows from symmetry of $4\\mathcal{D}$ spacetime while the latter follows from symmetry of the internal space."],"url":"http://arxiv.org/abs/2404.05302v1","category":"gr-qc"}
{"created":"2024-04-08 08:44:09","title":"Cosmological fluids with boundary term couplings","abstract":"Cosmological models can be studied effectively using dynamical systems techniques. Starting from Brown's formulation of the variational principle for relativistic fluids, we introduce new types of couplings involving a perfect fluid, a scalar field, and boundary terms. We describe three different coupling models, one of which turns out to be particularly relevant for cosmology. Its behaviour is similar to that of models in which dark matter decays into dark energy. In particular, for a constant coupling, the model mimics well-known dynamical dark energy models while the non-constant couplings offer a rich dynamical structure, unseen before. We are able to achieve this richness whilst working in a two-dimensional phase space. This is a significant advantage which allows us to provide a clear physical interpretation of the key features and draw analogies with previously studied models.","sentences":["Cosmological models can be studied effectively using dynamical systems techniques.","Starting from Brown's formulation of the variational principle for relativistic fluids, we introduce new types of couplings involving a perfect fluid, a scalar field, and boundary terms.","We describe three different coupling models, one of which turns out to be particularly relevant for cosmology.","Its behaviour is similar to that of models in which dark matter decays into dark energy.","In particular, for a constant coupling, the model mimics well-known dynamical dark energy models while the non-constant couplings offer a rich dynamical structure, unseen before.","We are able to achieve this richness whilst working in a two-dimensional phase space.","This is a significant advantage which allows us to provide a clear physical interpretation of the key features and draw analogies with previously studied models."],"url":"http://arxiv.org/abs/2404.05301v1","category":"gr-qc"}
{"created":"2024-04-08 08:35:15","title":"Automated Attack Synthesis for Constant Product Market Makers","abstract":"Decentralized Finance enables many novel applications that were impossible in traditional finances. However, it also introduces new types of vulnerabilities, such as composability bugs. The composability bugs refer to issues that lead to erroneous behaviors when multiple smart contracts operate together. One typical example of composability bugs is those between token contracts and Constant Product Market Makers (CPMM), the most widely used model for Decentralized Exchanges. Since 2022, 23 exploits of such kind have resulted in a total loss of 2.2M USD. BlockSec, a smart contract auditing company, once reported that 138 exploits of such kind occurred just in February 2023. We propose CPMM-Exploiter, which automatically detects and generates end-to-end exploits for CPMM composability bugs. Generating such end-to-end exploits is challenging due to the large search space of multiple contracts and various fees involved with financial services. To tackle this, we investigated real-world exploits regarding these vulnerabilities and identified that they arise due to violating two safety invariants. Based on this observation, we implemented CPMM-Exploiter, a new grammar-based fuzzer targeting the detection of these bugs. CPMM-Exploiter uses fuzzing to find transactions that break the invariants. It then refines these transactions to make them profitable for the attacker. We evaluated CPMM-Exploiter on two real-world exploit datasets. CPMM-Exploiter obtained recalls of 0.91 and 0.89, respectively, while five baselines achieved maximum recalls of 0.36 and 0.58, respectively. We further evaluated CPMM-Exploiter by running it on the latest blocks of the Ethereum and Binance networks. It successfully generated 18 new exploits, which can result in 12.9K USD profit in total.","sentences":["Decentralized Finance enables many novel applications that were impossible in traditional finances.","However, it also introduces new types of vulnerabilities, such as composability bugs.","The composability bugs refer to issues that lead to erroneous behaviors when multiple smart contracts operate together.","One typical example of composability bugs is those between token contracts and Constant Product Market Makers (CPMM), the most widely used model for Decentralized Exchanges.","Since 2022, 23 exploits of such kind have resulted in a total loss of 2.2M USD.","BlockSec, a smart contract auditing company, once reported that 138 exploits of such kind occurred just in February 2023.","We propose CPMM-Exploiter, which automatically detects and generates end-to-end exploits for CPMM composability bugs.","Generating such end-to-end exploits is challenging due to the large search space of multiple contracts and various fees involved with financial services.","To tackle this, we investigated real-world exploits regarding these vulnerabilities and identified that they arise due to violating two safety invariants.","Based on this observation, we implemented CPMM-Exploiter, a new grammar-based fuzzer targeting the detection of these bugs.","CPMM-Exploiter uses fuzzing to find transactions that break the invariants.","It then refines these transactions to make them profitable for the attacker.","We evaluated CPMM-Exploiter on two real-world exploit datasets.","CPMM-Exploiter obtained recalls of 0.91 and 0.89, respectively, while five baselines achieved maximum recalls of 0.36 and 0.58, respectively.","We further evaluated CPMM-Exploiter by running it on the latest blocks of the Ethereum and Binance networks.","It successfully generated 18 new exploits, which can result in 12.9K USD profit in total."],"url":"http://arxiv.org/abs/2404.05297v1","category":"cs.CR"}
{"created":"2024-04-08 08:29:00","title":"Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models","abstract":"We present a large language model (LLM) based system to empower quadrupedal robots with problem-solving abilities for long-horizon tasks beyond short-term motions. Long-horizon tasks for quadrupeds are challenging since they require both a high-level understanding of the semantics of the problem for task planning and a broad range of locomotion and manipulation skills to interact with the environment. Our system builds a high-level reasoning layer with large language models, which generates hybrid discrete-continuous plans as robot code from task descriptions. It comprises multiple LLM agents: a semantic planner for sketching a plan, a parameter calculator for predicting arguments in the plan, and a code generator to convert the plan into executable robot code. At the low level, we adopt reinforcement learning to train a set of motion planning and control skills to unleash the flexibility of quadrupeds for rich environment interactions. Our system is tested on long-horizon tasks that are infeasible to complete with one single skill. Simulation and real-world experiments show that it successfully figures out multi-step strategies and demonstrates non-trivial behaviors, including building tools or notifying a human for help.","sentences":["We present a large language model (LLM) based system to empower quadrupedal robots with problem-solving abilities for long-horizon tasks beyond short-term motions.","Long-horizon tasks for quadrupeds are challenging since they require both a high-level understanding of the semantics of the problem for task planning and a broad range of locomotion and manipulation skills to interact with the environment.","Our system builds a high-level reasoning layer with large language models, which generates hybrid discrete-continuous plans as robot code from task descriptions.","It comprises multiple LLM agents: a semantic planner for sketching a plan, a parameter calculator for predicting arguments in the plan, and a code generator to convert the plan into executable robot code.","At the low level, we adopt reinforcement learning to train a set of motion planning and control skills to unleash the flexibility of quadrupeds for rich environment interactions.","Our system is tested on long-horizon tasks that are infeasible to complete with one single skill.","Simulation and real-world experiments show that it successfully figures out multi-step strategies and demonstrates non-trivial behaviors, including building tools or notifying a human for help."],"url":"http://arxiv.org/abs/2404.05291v1","category":"cs.RO"}
{"created":"2024-04-08 08:28:19","title":"MindSet: Vision. A toolbox for testing DNNs on key psychological experiments","abstract":"Multiple benchmarks have been developed to assess the alignment between deep neural networks (DNNs) and human vision. In almost all cases these benchmarks are observational in the sense they are composed of behavioural and brain responses to naturalistic images that have not been manipulated to test hypotheses regarding how DNNs or humans perceive and identify objects. Here we introduce the toolbox MindSet: Vision, consisting of a collection of image datasets and related scripts designed to test DNNs on 30 psychological findings. In all experimental conditions, the stimuli are systematically manipulated to test specific hypotheses regarding human visual perception and object recognition. In addition to providing pre-generated datasets of images, we provide code to regenerate these datasets, offering many configurable parameters which greatly extend the dataset versatility for different research contexts, and code to facilitate the testing of DNNs on these image datasets using three different methods (similarity judgments, out-of-distribution classification, and decoder method), accessible at https://github.com/MindSetVision/mindset-vision. We test ResNet-152 on each of these methods as an example of how the toolbox can be used.","sentences":["Multiple benchmarks have been developed to assess the alignment between deep neural networks (DNNs) and human vision.","In almost all cases these benchmarks are observational in the sense they are composed of behavioural and brain responses to naturalistic images that have not been manipulated to test hypotheses regarding how DNNs or humans perceive and identify objects.","Here we introduce the toolbox MindSet: Vision, consisting of a collection of image datasets and related scripts designed to test DNNs on 30 psychological findings.","In all experimental conditions, the stimuli are systematically manipulated to test specific hypotheses regarding human visual perception and object recognition.","In addition to providing pre-generated datasets of images, we provide code to regenerate these datasets, offering many configurable parameters which greatly extend the dataset versatility for different research contexts, and code to facilitate the testing of DNNs on these image datasets using three different methods (similarity judgments, out-of-distribution classification, and decoder method), accessible at https://github.com/MindSetVision/mindset-vision.","We test ResNet-152 on each of these methods as an example of how the toolbox can be used."],"url":"http://arxiv.org/abs/2404.05290v1","category":"cs.CV"}
{"created":"2024-04-08 08:26:30","title":"Scalar induced gravitational waves in chiral scalar-tensor theory of gravity","abstract":"We study the scalar induced gravitational waves (SIGWs) from a chiral scalar-tensor theory of gravity. The parity-violating (PV) Lagrangian contains the Chern-Simons (CS) term and PV scalar-tensor terms, which are built of the quadratic Riemann tensor term and first-order derivatives of a scalar field. We consider SIGWs in two cases, in which the semi-analytic expression to calculate SIGWs can be obtained. Then, we calculate the fractional energy density of SIGWs with a monochromatic power spectrum for the curvature perturbation. We find that the SIGWs in chiral scalar-tensor gravity behave differently from those in GR before and after the peak frequency, which results in a large degree of circular polarization.","sentences":["We study the scalar induced gravitational waves (SIGWs) from a chiral scalar-tensor theory of gravity.","The parity-violating (PV) Lagrangian contains the Chern-Simons (CS) term and PV scalar-tensor terms, which are built of the quadratic Riemann tensor term and first-order derivatives of a scalar field.","We consider SIGWs in two cases, in which the semi-analytic expression to calculate SIGWs can be obtained.","Then, we calculate the fractional energy density of SIGWs with a monochromatic power spectrum for the curvature perturbation.","We find that the SIGWs in chiral scalar-tensor gravity behave differently from those in GR before and after the peak frequency, which results in a large degree of circular polarization."],"url":"http://arxiv.org/abs/2404.05289v1","category":"gr-qc"}
{"created":"2024-04-08 08:25:15","title":"Effective-one-body waveform model for non-circularized, planar, coalescing black hole binaries: the importance of radiation reaction","abstract":"We present an updated version of the TEOBResumS-Dali effective-one-body (EOB) waveform model for spin aligned binaries on non-circularized orbits. Recently computed 4PN (nonspinning) terms are incorporated in the waveform and radiation reaction. The model is informed by a restricted sample ($\\sim60$) of spin-aligned, quasi-circular, Numerical Relativity (NR) simulations. In the quasi-circular limit, the model displays EOB/NR unfaithfulness ${\\bar{\\cal F}}^{\\rm max}_{\\rm EOBNR}\\lesssim 10^{-2}$ (with median~ $1.06\\times 10^{-3}$) (with Advanced LIGO noise and in the total mass range $10-200M_\\odot$) for the dominant $\\ell=m=2$ mode all over the 534 spin-aligned configurations available through the Simulating eXtreme Spacetime catalog of NR waveforms. Similar figures are also obtained with the 28 public eccentric SXS simulations and good compatibility between EOB and NR scattering angles is found. The quasi-circular limit of TEOBResumS-Dali is also found to be highly consistent with the TEOBResumS-GIOTTO quasi-circular model. We then systematically explore the importance of NR-tuning {\\it also} the radiation reaction of the system. When this is done, the median of the distribution of quasi-circular ${\\bar{\\cal F}}^{\\rm max}_{\\rm EOBNR}$ is lowered to $3.92\\times 10^{-4}$, though balanced by a tail up to $\\sim 0.1$ for large, positive spins. The same is true for the eccentric-inspiral datasets. We conclude that an improvement of the analytical description of the spin-dependent flux (and its interplay with the conservative part) is likely to be the cornerstone to lower the EOB/NR unfaithfulness below the $10^{-4}$ level all over the parameter space, thus grazing the current NR uncertainties as well as the expected needs for next generation of GW detector like Einstein Telescope.","sentences":["We present an updated version of the TEOBResumS-Dali effective-one-body (EOB) waveform model for spin aligned binaries on non-circularized orbits.","Recently computed 4PN (nonspinning) terms are incorporated in the waveform and radiation reaction.","The model is informed by a restricted sample ($\\sim60$) of spin-aligned, quasi-circular, Numerical Relativity (NR) simulations.","In the quasi-circular limit, the model displays EOB/NR unfaithfulness ${\\bar{\\cal F}}^{\\rm max}_{\\rm EOBNR}\\lesssim 10^{-2}$ (with median~ $1.06\\times 10^{-3}$) (with Advanced LIGO noise and in the total mass range $10-200M_\\odot$) for the dominant $\\ell=m=2$ mode all over the 534 spin-aligned configurations available through the Simulating eXtreme Spacetime catalog of NR waveforms.","Similar figures are also obtained with the 28 public eccentric SXS simulations and good compatibility between EOB and NR scattering angles is found.","The quasi-circular limit of TEOBResumS-Dali is also found to be highly consistent with the TEOBResumS-GIOTTO quasi-circular model.","We then systematically explore the importance of NR-tuning {\\it also} the radiation reaction of the system.","When this is done, the median of the distribution of quasi-circular ${\\bar{\\cal F}}^{\\rm max}_{\\rm EOBNR}$ is lowered to $3.92\\times 10^{-4}$, though balanced by a tail up to $\\sim 0.1$ for large, positive spins.","The same is true for the eccentric-inspiral datasets.","We conclude that an improvement of the analytical description of the spin-dependent flux (and its interplay with the conservative part) is likely to be the cornerstone to lower the EOB/NR unfaithfulness below the $10^{-4}$ level all over the parameter space, thus grazing the current NR uncertainties as well as the expected needs for next generation of GW detector like Einstein Telescope."],"url":"http://arxiv.org/abs/2404.05288v1","category":"gr-qc"}
{"created":"2024-04-08 08:20:53","title":"Detecting Every Object from Events","abstract":"Object detection is critical in autonomous driving, and it is more practical yet challenging to localize objects of unknown categories: an endeavour known as Class-Agnostic Object Detection (CAOD). Existing studies on CAOD predominantly rely on ordinary cameras, but these frame-based sensors usually have high latency and limited dynamic range, leading to safety risks in real-world scenarios. In this study, we turn to a new modality enabled by the so-called event camera, featured by its sub-millisecond latency and high dynamic range, for robust CAOD. We propose Detecting Every Object in Events (DEOE), an approach tailored for achieving high-speed, class-agnostic open-world object detection in event-based vision. Built upon the fast event-based backbone: recurrent vision transformer, we jointly consider the spatial and temporal consistencies to identify potential objects. The discovered potential objects are assimilated as soft positive samples to avoid being suppressed as background. Moreover, we introduce a disentangled objectness head to separate the foreground-background classification and novel object discovery tasks, enhancing the model's generalization in localizing novel objects while maintaining a strong ability to filter out the background. Extensive experiments confirm the superiority of our proposed DEOE in comparison with three strong baseline methods that integrate the state-of-the-art event-based object detector with advancements in RGB-based CAOD. Our code is available at https://github.com/Hatins/DEOE.","sentences":["Object detection is critical in autonomous driving, and it is more practical yet challenging to localize objects of unknown categories: an endeavour known as Class-Agnostic Object Detection (CAOD).","Existing studies on CAOD predominantly rely on ordinary cameras, but these frame-based sensors usually have high latency and limited dynamic range, leading to safety risks in real-world scenarios.","In this study, we turn to a new modality enabled by the so-called event camera, featured by its sub-millisecond latency and high dynamic range, for robust CAOD.","We propose Detecting Every Object in Events (DEOE), an approach tailored for achieving high-speed, class-agnostic open-world object detection in event-based vision.","Built upon the fast event-based backbone: recurrent vision transformer, we jointly consider the spatial and temporal consistencies to identify potential objects.","The discovered potential objects are assimilated as soft positive samples to avoid being suppressed as background.","Moreover, we introduce a disentangled objectness head to separate the foreground-background classification and novel object discovery tasks, enhancing the model's generalization in localizing novel objects while maintaining a strong ability to filter out the background.","Extensive experiments confirm the superiority of our proposed DEOE in comparison with three strong baseline methods that integrate the state-of-the-art event-based object detector with advancements in RGB-based CAOD.","Our code is available at https://github.com/Hatins/DEOE."],"url":"http://arxiv.org/abs/2404.05285v1","category":"cs.CV"}
{"created":"2024-04-08 08:19:53","title":"Tackling Challenges in 21cm Global Spectrum Experiment: the Impact of Ionosphere and Beam Distortion","abstract":"The HI 21cm global signal from the Cosmic Dawn and the Epoch of Reionization (EoR) offers critical insights into the evolution of our Universe. Yet, its detection presents significant challenges due to its extremely low signal-to-contamination ratio and complex instrumental systematics. In this paper, we examine the effects of the ionosphere and antenna beam on data analysis. The ionosphere, an ionized plasma layer in the Earth's atmosphere, refracts, absorbs, and emits radio waves in the relevant frequency range. This interaction results in additional spectral distortion of the observed signal, complicating the process of foreground subtraction. Additionally, chromatic variations in the beam can also introduce further contamination into the global spectrum measurement. Notably, the ionospheric effect, being dependent on the direction of incoming light, interacts with the instrumental beam, adding another layer of complexity. To address this, we evaluate three different fitting templates of foreground: the logarithmic polynomial, the physically motivated EDGES template, and a SVD-based template. Our findings indicate that the EDGES and SVD templates generally surpass logarithmic polynomials in performance. Recognizing the significance of beam chromaticity, we further investigate specific beam distortion models and their impacts on the signal extraction process.","sentences":["The HI 21cm global signal from the Cosmic Dawn and the Epoch of Reionization (EoR) offers critical insights into the evolution of our Universe.","Yet, its detection presents significant challenges due to its extremely low signal-to-contamination ratio and complex instrumental systematics.","In this paper, we examine the effects of the ionosphere and antenna beam on data analysis.","The ionosphere, an ionized plasma layer in the Earth's atmosphere, refracts, absorbs, and emits radio waves in the relevant frequency range.","This interaction results in additional spectral distortion of the observed signal, complicating the process of foreground subtraction.","Additionally, chromatic variations in the beam can also introduce further contamination into the global spectrum measurement.","Notably, the ionospheric effect, being dependent on the direction of incoming light, interacts with the instrumental beam, adding another layer of complexity.","To address this, we evaluate three different fitting templates of foreground: the logarithmic polynomial, the physically motivated EDGES template, and a SVD-based template.","Our findings indicate that the EDGES and SVD templates generally surpass logarithmic polynomials in performance.","Recognizing the significance of beam chromaticity, we further investigate specific beam distortion models and their impacts on the signal extraction process."],"url":"http://arxiv.org/abs/2404.05284v1","category":"astro-ph.CO"}
{"created":"2024-04-08 08:11:56","title":"MOSE: Boosting Vision-based Roadside 3D Object Detection with Scene Cues","abstract":"3D object detection based on roadside cameras is an additional way for autonomous driving to alleviate the challenges of occlusion and short perception range from vehicle cameras. Previous methods for roadside 3D object detection mainly focus on modeling the depth or height of objects, neglecting the stationary of cameras and the characteristic of inter-frame consistency. In this work, we propose a novel framework, namely MOSE, for MOnocular 3D object detection with Scene cuEs. The scene cues are the frame-invariant scene-specific features, which are crucial for object localization and can be intuitively regarded as the height between the surface of the real road and the virtual ground plane. In the proposed framework, a scene cue bank is designed to aggregate scene cues from multiple frames of the same scene with a carefully designed extrinsic augmentation strategy. Then, a transformer-based decoder lifts the aggregated scene cues as well as the 3D position embeddings for 3D object location, which boosts generalization ability in heterologous scenes. The extensive experiment results on two public benchmarks demonstrate the state-of-the-art performance of the proposed method, which surpasses the existing methods by a large margin.","sentences":["3D object detection based on roadside cameras is an additional way for autonomous driving to alleviate the challenges of occlusion and short perception range from vehicle cameras.","Previous methods for roadside 3D object detection mainly focus on modeling the depth or height of objects, neglecting the stationary of cameras and the characteristic of inter-frame consistency.","In this work, we propose a novel framework, namely MOSE, for MOnocular 3D object detection with Scene cuEs.","The scene cues are the frame-invariant scene-specific features, which are crucial for object localization and can be intuitively regarded as the height between the surface of the real road and the virtual ground plane.","In the proposed framework, a scene cue bank is designed to aggregate scene cues from multiple frames of the same scene with a carefully designed extrinsic augmentation strategy.","Then, a transformer-based decoder lifts the aggregated scene cues as well as the 3D position embeddings for 3D object location, which boosts generalization ability in heterologous scenes.","The extensive experiment results on two public benchmarks demonstrate the state-of-the-art performance of the proposed method, which surpasses the existing methods by a large margin."],"url":"http://arxiv.org/abs/2404.05280v1","category":"cs.CV"}
{"created":"2024-04-08 08:06:14","title":"Free Field Realization of Supersymmetric W-algebras","abstract":"We show that supersymmetric(SUSY) W-algebra of generic level can be realized as an intersection of the kernels of the screening operators. Applying this result to the principal SUSY W-algebras, we get the free field realization of them inside the SUSY Heisenberg vertex algebras. Furthermore, the screening operators for principal SUSY W-algebras allow us to analyze their structure via the principal SUSY W-algebras associated with $\\mathfrak{osp}(1|2)$, $\\mathfrak{osp}(2|2)$, $\\mathfrak{osp}(3|2)$ and $\\mathfrak{osp}(4|2)$.","sentences":["We show that supersymmetric(SUSY) W-algebra of generic level can be realized as an intersection of the kernels of the screening operators.","Applying this result to the principal SUSY W-algebras, we get the free field realization of them inside the SUSY Heisenberg vertex algebras.","Furthermore, the screening operators for principal SUSY W-algebras allow us to analyze their structure via the principal SUSY W-algebras associated with $\\mathfrak{osp}(1|2)$, $\\mathfrak{osp}(2|2)$, $\\mathfrak{osp}(3|2)$ and $\\mathfrak{osp}(4|2)$."],"url":"http://arxiv.org/abs/2404.05275v1","category":"math-ph"}
{"created":"2024-04-08 08:02:28","title":"Creating highly symmetric qudit heralded entanglement through highly symmetric graphs","abstract":"Recent attention has turned to exploring quantum information within larger Hilbert spaces by utilizing qudits, which offer increased information capacity and potential for robust quantum communications. While the efficient generation of multipartite qudit entanglement is crucial for studying quantum correlations in high-dimensional Hilbert spaces, the increased dimension makes the circuit design challanging, especially when the entanglement is generated by heralding detections. In this work, we demonstrate that the graph picture of linear quantum networks (LQG picture) can provide a simplified method to generate qudit multipartite heralded entanglement of high symmetries. The LQG picture enables the reduction of circuit complexity by directly imposing the state symmetry onto the circuit structure. Leveraging this insight, we propose heralded schemes for generating $N$-partite $N$-level anti-symmetric (singlet) and symmetric (Dicke) states. Our study shed light on the optimal circuit design of high-dimensional entanglement with a systematic graphical strategy.","sentences":["Recent attention has turned to exploring quantum information within larger Hilbert spaces by utilizing qudits, which offer increased information capacity and potential for robust quantum communications.","While the efficient generation of multipartite qudit entanglement is crucial for studying quantum correlations in high-dimensional Hilbert spaces, the increased dimension makes the circuit design challanging, especially when the entanglement is generated by heralding detections.","In this work, we demonstrate that the graph picture of linear quantum networks (LQG picture) can provide a simplified method to generate qudit multipartite heralded entanglement of high symmetries.","The LQG picture enables the reduction of circuit complexity by directly imposing the state symmetry onto the circuit structure.","Leveraging this insight, we propose heralded schemes for generating $N$-partite $N$-level anti-symmetric (singlet) and symmetric (Dicke) states.","Our study shed light on the optimal circuit design of high-dimensional entanglement with a systematic graphical strategy."],"url":"http://arxiv.org/abs/2404.05273v1","category":"quant-ph"}
{"created":"2024-04-08 08:02:18","title":"Constructing Data Transaction Chains Based on Opportunity Cost Exploration","abstract":"Data trading is increasingly gaining attention. However, the inherent replicability and privacy concerns of data make it challenging to directly apply traditional trading theories to data markets. This paper compares data trading markets with traditional ones, focusing particularly on how the replicability and privacy of data impact data markets. We discuss how data's replicability fundamentally alters the concept of opportunity cost in traditional microeconomics within the context of data markets. Additionally, we explore how to leverage this change to maximize benefits without compromising data privacy. This paper outlines the constraints for data circulation within the privacy domain chain and presents a model that maximizes data's value under these constraints. Specific application scenarios are provided, and experiments demonstrate the solvability of this model.","sentences":["Data trading is increasingly gaining attention.","However, the inherent replicability and privacy concerns of data make it challenging to directly apply traditional trading theories to data markets.","This paper compares data trading markets with traditional ones, focusing particularly on how the replicability and privacy of data impact data markets.","We discuss how data's replicability fundamentally alters the concept of opportunity cost in traditional microeconomics within the context of data markets.","Additionally, we explore how to leverage this change to maximize benefits without compromising data privacy.","This paper outlines the constraints for data circulation within the privacy domain chain and presents a model that maximizes data's value under these constraints.","Specific application scenarios are provided, and experiments demonstrate the solvability of this model."],"url":"http://arxiv.org/abs/2404.05272v1","category":"cs.AI"}
{"created":"2024-04-08 07:59:04","title":"MC$^2$: Multi-concept Guidance for Customized Multi-concept Generation","abstract":"Customized text-to-image generation aims to synthesize instantiations of user-specified concepts and has achieved unprecedented progress in handling individual concept. However, when extending to multiple customized concepts, existing methods exhibit limitations in terms of flexibility and fidelity, only accommodating the combination of limited types of models and potentially resulting in a mix of characteristics from different concepts. In this paper, we introduce the Multi-concept guidance for Multi-concept customization, termed MC$^2$, for improved flexibility and fidelity. MC$^2$ decouples the requirements for model architecture via inference time optimization, allowing the integration of various heterogeneous single-concept customized models. It adaptively refines the attention weights between visual and textual tokens, directing image regions to focus on their associated words while diminishing the impact of irrelevant ones. Extensive experiments demonstrate that MC$^2$ even surpasses previous methods that require additional training in terms of consistency with input prompt and reference images. Moreover, MC$^2$ can be extended to elevate the compositional capabilities of text-to-image generation, yielding appealing results. Code will be publicly available at https://github.com/JIANGJiaXiu/MC-2.","sentences":["Customized text-to-image generation aims to synthesize instantiations of user-specified concepts and has achieved unprecedented progress in handling individual concept.","However, when extending to multiple customized concepts, existing methods exhibit limitations in terms of flexibility and fidelity, only accommodating the combination of limited types of models and potentially resulting in a mix of characteristics from different concepts.","In this paper, we introduce the Multi-concept guidance for Multi-concept customization, termed MC$^2$, for improved flexibility and fidelity.","MC$^2$ decouples the requirements for model architecture via inference time optimization, allowing the integration of various heterogeneous single-concept customized models.","It adaptively refines the attention weights between visual and textual tokens, directing image regions to focus on their associated words while diminishing the impact of irrelevant ones.","Extensive experiments demonstrate that MC$^2$ even surpasses previous methods that require additional training in terms of consistency with input prompt and reference images.","Moreover, MC$^2$ can be extended to elevate the compositional capabilities of text-to-image generation, yielding appealing results.","Code will be publicly available at https://github.com/JIANGJiaXiu/MC-2."],"url":"http://arxiv.org/abs/2404.05268v1","category":"cs.CV"}
{"created":"2024-04-08 07:54:18","title":"Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security","abstract":"Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities that increasingly influence various aspects of our daily lives, constantly defining the new boundary of Artificial General Intelligence (AGI). Image modalities, enriched with profound semantic information and a more continuous mathematical nature compared to other modalities, greatly enhance the functionalities of MLLMs when integrated. However, this integration serves as a double-edged sword, providing attackers with expansive vulnerabilities to exploit for highly covert and harmful attacks. The pursuit of reliable AI systems like powerful MLLMs has emerged as a pivotal area of contemporary research. In this paper, we endeavor to demostrate the multifaceted risks associated with the incorporation of image modalities into MLLMs. Initially, we delineate the foundational components and training processes of MLLMs. Subsequently, we construct a threat model, outlining the security vulnerabilities intrinsic to MLLMs. Moreover, we analyze and summarize existing scholarly discourses on MLLMs' attack and defense mechanisms, culminating in suggestions for the future research on MLLM security. Through this comprehensive analysis, we aim to deepen the academic understanding of MLLM security challenges and propel forward the development of trustworthy MLLM systems.","sentences":["Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities that increasingly influence various aspects of our daily lives, constantly defining the new boundary of Artificial General Intelligence (AGI).","Image modalities, enriched with profound semantic information and a more continuous mathematical nature compared to other modalities, greatly enhance the functionalities of MLLMs when integrated.","However, this integration serves as a double-edged sword, providing attackers with expansive vulnerabilities to exploit for highly covert and harmful attacks.","The pursuit of reliable AI systems like powerful MLLMs has emerged as a pivotal area of contemporary research.","In this paper, we endeavor to demostrate the multifaceted risks associated with the incorporation of image modalities into MLLMs.","Initially, we delineate the foundational components and training processes of MLLMs.","Subsequently, we construct a threat model, outlining the security vulnerabilities intrinsic to MLLMs.","Moreover, we analyze and summarize existing scholarly discourses on MLLMs' attack and defense mechanisms, culminating in suggestions for the future research on MLLM security.","Through this comprehensive analysis, we aim to deepen the academic understanding of MLLM security challenges and propel forward the development of trustworthy MLLM systems."],"url":"http://arxiv.org/abs/2404.05264v1","category":"cs.CR"}
{"created":"2024-04-08 07:51:08","title":"T3DRIS: Advancing Conformal RIS Design through In-depth Analysis of Mutual Coupling Effects","abstract":"This paper presents a theoretical and mathematical framework for the design of a conformal reconfigurable intelligent surface (RIS) that adapts to non-planar geometries, which is a critical advancement for the deployment of RIS on non-planar and irregular surfaces as envisioned in smart radio environments. Previous research focused mainly on the optimization of RISs assuming a predetermined shape, while neglecting the intricate interplay between shape optimization, phase optimization, and mutual coupling effects. Our contribution, the T3DRIS framework, addresses this fundamental problem by integrating the configuration and shape optimization of RISs into a unified model and design framework, thus facilitating the application of RIS technology to a wider spectrum of environmental objects. The mathematical core of T3DRIS is rooted in optimizing the 3D deployment of the unit cells and tuning circuits, aiming at maximizing the communication performance. Through rigorous full-wave simulations and a comprehensive set of numerical analyses, we validate the proposed approach and demonstrate its superior performance and applicability over contemporary designs. This study-the first of its kind-paves the way for a new direction in RIS research, emphasizing the importance of a theoretical and mathematical perspective in tackling the challenges of conformal RISs.","sentences":["This paper presents a theoretical and mathematical framework for the design of a conformal reconfigurable intelligent surface (RIS) that adapts to non-planar geometries, which is a critical advancement for the deployment of RIS on non-planar and irregular surfaces as envisioned in smart radio environments.","Previous research focused mainly on the optimization of RISs assuming a predetermined shape, while neglecting the intricate interplay between shape optimization, phase optimization, and mutual coupling effects.","Our contribution, the T3DRIS framework, addresses this fundamental problem by integrating the configuration and shape optimization of RISs into a unified model and design framework, thus facilitating the application of RIS technology to a wider spectrum of environmental objects.","The mathematical core of T3DRIS is rooted in optimizing the 3D deployment of the unit cells and tuning circuits, aiming at maximizing the communication performance.","Through rigorous full-wave simulations and a comprehensive set of numerical analyses, we validate the proposed approach and demonstrate its superior performance and applicability over contemporary designs.","This study-the first of its kind-paves the way for a new direction in RIS research, emphasizing the importance of a theoretical and mathematical perspective in tackling the challenges of conformal RISs."],"url":"http://arxiv.org/abs/2404.05261v1","category":"cs.IT"}
{"created":"2024-04-08 07:49:57","title":"SRAM-PG: Power Delivery Network Benchmarks from SRAM Circuits","abstract":"Designing the power delivery network (PDN) in very large-scale integrated (VLSI) circuits is increasingly important, especially for nowadays low-power integrated circuit (IC) design. In order to ensure that the designed PDN enables a low level of voltage drop and noise which is required for the success of IC design, accurate analysis of PDN is largely demanded and brings a challenge of computation during the whole process of IC design. This promotes the research of efficient and scalable simulation methods for PDN. However, the lack of sufficient public PDN benchmarks hinders the relevant research. % on this aspect since it is hard to conduct a rapid and clear comparison between different approaches to solving this problem. To this end, we construct and release a set of PDN benchmarks (named \\emph{SRAM-PG}) from SRAM circuit design in this work. The benchmarks are obtained from realistic and state-of-the-art SRAM designs, following a workflow for generating the post-layout PDN netlists with full RC parasitics. With careful modeling of load currents, the benchmarks reflect the dynamic work mode of the IC and can be used for both transient and DC analysis. The benchmarks are derived from the designs for diverse applications. And, sharing them in the public domain with detailed descriptions would largely benefit the relevant research. The whole set of benchmarks is available at \\href{github}{https://github.com/ShenShan123/SRAM-PG}.","sentences":["Designing the power delivery network (PDN) in very large-scale integrated (VLSI) circuits is increasingly important, especially for nowadays low-power integrated circuit (IC) design.","In order to ensure that the designed PDN enables a low level of voltage drop and noise which is required for the success of IC design, accurate analysis of PDN is largely demanded and brings a challenge of computation during the whole process of IC design.","This promotes the research of efficient and scalable simulation methods for PDN.","However, the lack of sufficient public PDN benchmarks hinders the relevant research.","% on this aspect since it is hard to conduct a rapid and clear comparison between different approaches to solving this problem.","To this end, we construct and release a set of PDN benchmarks (named \\emph{SRAM-PG}) from SRAM circuit design in this work.","The benchmarks are obtained from realistic and state-of-the-art SRAM designs, following a workflow for generating the post-layout PDN netlists with full RC parasitics.","With careful modeling of load currents, the benchmarks reflect the dynamic work mode of the IC and can be used for both transient and DC analysis.","The benchmarks are derived from the designs for diverse applications.","And, sharing them in the public domain with detailed descriptions would largely benefit the relevant research.","The whole set of benchmarks is available at \\href{github}{https://github.com/ShenShan123/SRAM-PG}."],"url":"http://arxiv.org/abs/2404.05260v1","category":"cs.AR"}
{"created":"2024-04-08 16:21:22","title":"3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules","abstract":"We introduce 3D-COCO, an extension of the original MS-COCO dataset providing 3D models and 2D-3D alignment annotations. 3D-COCO was designed to achieve computer vision tasks such as 3D reconstruction or image detection configurable with textual, 2D image, and 3D CAD model queries. We complete the existing MS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. By using an IoU-based method, we match each MS-COCO annotation with the best 3D models to provide a 2D-3D alignment. The open-source nature of 3D-COCO is a premiere that should pave the way for new research on 3D-related topics. The dataset and its source codes is available at https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/","sentences":["We introduce 3D-COCO, an extension of the original MS-COCO dataset providing 3D models and 2D-3D alignment annotations.","3D-COCO was designed to achieve computer vision tasks such as 3D reconstruction or image detection configurable with textual, 2D image, and 3D CAD model queries.","We complete the existing MS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse.","By using an IoU-based method, we match each MS-COCO annotation with the best 3D models to provide a 2D-3D alignment.","The open-source nature of 3D-COCO is a premiere that should pave the way for new research on 3D-related topics.","The dataset and its source codes is available at https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/"],"url":"http://arxiv.org/abs/2404.05641v1","category":"cs.CV"}
{"created":"2024-04-08 15:21:34","title":"Towards an understanding of particle-scale flaws and microstructure evolution in cold-spray via accumulation of single particle impacts","abstract":"Cold spray coatings are the sum of countless individual bonding events between single particles impacting on top of one another at high velocities. Thus, the collective behavior of microparticles must be considered to elucidate the origins of coating flaws at the scale of the particles and larger, or the dynamic evolution of the overall coating microstructure. Laser-induced particle impact testing (LIPIT) has been extensively used to study single-particle impacts, and in this work is adapted to study the accumulation of numerous particles with knowledge of each individual particle's impact parameters (particle size, velocity). The method reproducibly deposits stacks of gold particles (>20 particles) with different characteristic spectra of impact velocity. The quantitative single-particle data are analyzed in a correlative manner to the structure and flaws in the resulting stacks, providing some first semi-quantitative connections between, e.g., strain and recrystallization, or aberrant particle characteristics and defects. The results highlight opportunities for the study of many-particle phenomena in microparticle impact--from interaction of particles in cold spray to multi-step erosion processes--with a quantitative view of the behavior of single particles.","sentences":["Cold spray coatings are the sum of countless individual bonding events between single particles impacting on top of one another at high velocities.","Thus, the collective behavior of microparticles must be considered to elucidate the origins of coating flaws at the scale of the particles and larger, or the dynamic evolution of the overall coating microstructure.","Laser-induced particle impact testing (LIPIT) has been extensively used to study single-particle impacts, and in this work is adapted to study the accumulation of numerous particles with knowledge of each individual particle's impact parameters (particle size, velocity).","The method reproducibly deposits stacks of gold particles (>20 particles) with different characteristic spectra of impact velocity.","The quantitative single-particle data are analyzed in a correlative manner to the structure and flaws in the resulting stacks, providing some first semi-quantitative connections between, e.g., strain and recrystallization, or aberrant particle characteristics and defects.","The results highlight opportunities for the study of many-particle phenomena in microparticle impact--from interaction of particles in cold spray to multi-step erosion processes--with a quantitative view of the behavior of single particles."],"url":"http://arxiv.org/abs/2404.05601v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 13:21:25","title":"Characterising the Higgs boson with ATLAS data from Run 2 of the LHC","abstract":"The Higgs boson was discovered by the ATLAS and CMS Collaborations in 2012 using data from Run 1 of the Large Hadron Collider (2010$-$2012). In Run 2 (2015$-$2018), about 140 fb$^{-1}$ of proton$-$proton collisions at a centre-of-mass energy of 13 TeV were collected by the ATLAS experiment. This review presents the most important Run 2 results obtained by the ATLAS Collaboration regarding the properties of the Higgs boson and its interactions with other particles. The performed studies significantly enhance the understanding of the Higgs boson, while hunting for deviations from the predictions of the Standard Model of particle physics.","sentences":["The Higgs boson was discovered by the ATLAS and CMS Collaborations in 2012 using data from Run 1 of the Large Hadron Collider (2010$-$2012).","In Run 2 (2015$-$2018), about 140 fb$^{-1}$ of proton$-$proton collisions at a centre-of-mass energy of 13 TeV were collected by the ATLAS experiment.","This review presents the most important Run 2 results obtained by the ATLAS Collaboration regarding the properties of the Higgs boson and its interactions with other particles.","The performed studies significantly enhance the understanding of the Higgs boson, while hunting for deviations from the predictions of the Standard Model of particle physics."],"url":"http://arxiv.org/abs/2404.05498v1","category":"hep-ex"}
{"created":"2024-04-08 13:11:11","title":"The Impact of Sanctions on GitHub Developers and Activities","abstract":"The GitHub platform has fueled the creation of truly global software, enabling contributions from developers across various geographical regions of the world. As software becomes more entwined with global politics and social regulations, it becomes similarly subject to government sanctions. In 2019, GitHub restricted access to certain services for users in specific locations but rolled back these restrictions for some communities (e.g., the Iranian community) in 2021. We conducted a large-scale empirical study, collecting approximately 156 thousand user profiles and their 41 million activity points from 2008 to 2022, to understand the response of developers. Our results indicate that many of these targeted developers were able to navigate through the sanctions. Furthermore, once these sanctions were lifted, these developers opted to return to GitHub instead of withdrawing their contributions to the platform. The study indicates that platforms like GitHub play key roles in sustaining global contributions to Open Source Software.","sentences":["The GitHub platform has fueled the creation of truly global software, enabling contributions from developers across various geographical regions of the world.","As software becomes more entwined with global politics and social regulations, it becomes similarly subject to government sanctions.","In 2019, GitHub restricted access to certain services for users in specific locations but rolled back these restrictions for some communities (e.g., the Iranian community) in 2021.","We conducted a large-scale empirical study, collecting approximately 156 thousand user profiles and their 41 million activity points from 2008 to 2022, to understand the response of developers.","Our results indicate that many of these targeted developers were able to navigate through the sanctions.","Furthermore, once these sanctions were lifted, these developers opted to return to GitHub instead of withdrawing their contributions to the platform.","The study indicates that platforms like GitHub play key roles in sustaining global contributions to Open Source Software."],"url":"http://arxiv.org/abs/2404.05489v1","category":"cs.SE"}
{"created":"2024-04-08 11:55:44","title":"Language Models on a Diet: Cost-Efficient Development of Encoders for Closely-Related Languages via Additional Pretraining","abstract":"The world of language models is going through turbulent times, better and ever larger models are coming out at an unprecedented speed. However, we argue that, especially for the scientific community, encoder models of up to 1 billion parameters are still very much needed, their primary usage being in enriching large collections of data with metadata necessary for downstream research. We investigate the best way to ensure the existence of such encoder models on the set of very closely related languages - Croatian, Serbian, Bosnian and Montenegrin, by setting up a diverse benchmark for these languages, and comparing the trained-from-scratch models with the new models constructed via additional pretraining of existing multilingual models. We show that comparable performance to dedicated from-scratch models can be obtained by additionally pretraining available multilingual models even with a limited amount of computation. We also show that neighboring languages, in our case Slovenian, can be included in the additional pretraining with little to no loss in the performance of the final model.","sentences":["The world of language models is going through turbulent times, better and ever larger models are coming out at an unprecedented speed.","However, we argue that, especially for the scientific community, encoder models of up to 1 billion parameters are still very much needed, their primary usage being in enriching large collections of data with metadata necessary for downstream research.","We investigate the best way to ensure the existence of such encoder models on the set of very closely related languages - Croatian, Serbian, Bosnian and Montenegrin, by setting up a diverse benchmark for these languages, and comparing the trained-from-scratch models with the new models constructed via additional pretraining of existing multilingual models.","We show that comparable performance to dedicated from-scratch models can be obtained by additionally pretraining available multilingual models even with a limited amount of computation.","We also show that neighboring languages, in our case Slovenian, can be included in the additional pretraining with little to no loss in the performance of the final model."],"url":"http://arxiv.org/abs/2404.05428v1","category":"cs.CL"}
{"created":"2024-04-08 10:50:29","title":"Design and implementation of a synchronous Hardware Performance Monitor for a RISC-V space-oriented processor","abstract":"The ability to collect statistics about the execution of a program within a CPU is of the utmost importance across all fields of computing since it allows characterizing the timing performance of a program. This capability is even more relevant in safety-critical software systems, where it is mandatory to analyze software timing requirements to ensure the correct operation of the programs. Moreover, in order to properly evaluate and verify the extra-functional properties of these systems, besides timing performance, there are many other statistics available on a CPU, such as those associated with resource utilization. In this paper, we showcase a Performance Measurement Unit, also known as Hardware Performance Monitor, integrated into a RISC-V On-Board Computer designed for space applications by our research group. The monitoring technique features a novel approach whereby the events triggered are not counted immediately but instead are propagated through the pipeline so that their annotation is synchronized with the executed instruction. Additionally, we demonstrate the use of this PMU in a process to characterize the execution model of the processor. Finally, as an example of the statistics provided by the PMU, the results obtained running the CoreMark and Dhrystone benchmarks on the RISC-V OBC are shown.","sentences":["The ability to collect statistics about the execution of a program within a CPU is of the utmost importance across all fields of computing since it allows characterizing the timing performance of a program.","This capability is even more relevant in safety-critical software systems, where it is mandatory to analyze software timing requirements to ensure the correct operation of the programs.","Moreover, in order to properly evaluate and verify the extra-functional properties of these systems, besides timing performance, there are many other statistics available on a CPU, such as those associated with resource utilization.","In this paper, we showcase a Performance Measurement Unit, also known as Hardware Performance Monitor, integrated into a RISC-V On-Board Computer designed for space applications by our research group.","The monitoring technique features a novel approach whereby the events triggered are not counted immediately but instead are propagated through the pipeline so that their annotation is synchronized with the executed instruction.","Additionally, we demonstrate the use of this PMU in a process to characterize the execution model of the processor.","Finally, as an example of the statistics provided by the PMU, the results obtained running the CoreMark and Dhrystone benchmarks on the RISC-V OBC are shown."],"url":"http://arxiv.org/abs/2404.05389v1","category":"cs.AR"}
{"created":"2024-04-08 10:49:53","title":"MealRec$^+$: A Meal Recommendation Dataset with Meal-Course Affiliation for Personalization and Healthiness","abstract":"Meal recommendation, as a typical health-related recommendation task, contains complex relationships between users, courses, and meals. Among them, meal-course affiliation associates user-meal and user-course interactions. However, an extensive literature review demonstrates that there is a lack of publicly available meal recommendation datasets including meal-course affiliation. Meal recommendation research has been constrained in exploring the impact of cooperation between two levels of interaction on personalization and healthiness. To pave the way for meal recommendation research, we introduce a new benchmark dataset called MealRec$^+$. Due to constraints related to user health privacy and meal scenario characteristics, the collection of data that includes both meal-course affiliation and two levels of interactions is impeded. Therefore, a simulation method is adopted to derive meal-course affiliation and user-meal interaction from the user's dining sessions simulated based on user-course interaction data. Then, two well-known nutritional standards are used to calculate the healthiness scores of meals. Moreover, we experiment with several baseline models, including separate and cooperative interaction learning methods. Our experiment demonstrates that cooperating the two levels of interaction in appropriate ways is beneficial for meal recommendations. Furthermore, in response to the less healthy recommendation phenomenon found in the experiment, we explore methods to enhance the healthiness of meal recommendations. The dataset is available on GitHub (https://github.com/WUT-IDEA/MealRecPlus).","sentences":["Meal recommendation, as a typical health-related recommendation task, contains complex relationships between users, courses, and meals.","Among them, meal-course affiliation associates user-meal and user-course interactions.","However, an extensive literature review demonstrates that there is a lack of publicly available meal recommendation datasets including meal-course affiliation.","Meal recommendation research has been constrained in exploring the impact of cooperation between two levels of interaction on personalization and healthiness.","To pave the way for meal recommendation research, we introduce a new benchmark dataset called MealRec$^+$. Due to constraints related to user health privacy and meal scenario characteristics, the collection of data that includes both meal-course affiliation and two levels of interactions is impeded.","Therefore, a simulation method is adopted to derive meal-course affiliation and user-meal interaction from the user's dining sessions simulated based on user-course interaction data.","Then, two well-known nutritional standards are used to calculate the healthiness scores of meals.","Moreover, we experiment with several baseline models, including separate and cooperative interaction learning methods.","Our experiment demonstrates that cooperating the two levels of interaction in appropriate ways is beneficial for meal recommendations.","Furthermore, in response to the less healthy recommendation phenomenon found in the experiment, we explore methods to enhance the healthiness of meal recommendations.","The dataset is available on GitHub (https://github.com/WUT-IDEA/MealRecPlus)."],"url":"http://arxiv.org/abs/2404.05386v1","category":"cs.IR"}
{"created":"2024-04-08 09:22:41","title":"PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for the Neural Processing of Portuguese","abstract":"Leveraging research on the neural modelling of Portuguese, we contribute a collection of datasets for an array of language processing tasks and a corresponding collection of fine-tuned neural language models on these downstream tasks. To align with mainstream benchmarks in the literature, originally developed in English, and to kick start their Portuguese counterparts, the datasets were machine-translated from English with a state-of-the-art translation engine. The resulting PORTULAN ExtraGLUE benchmark is a basis for research on Portuguese whose improvement can be pursued in future work. Similarly, the respective fine-tuned neural language models, developed with a low-rank adaptation approach, are made available as baselines that can stimulate future work on the neural processing of Portuguese. All datasets and models have been developed and are made available for two variants of Portuguese: European and Brazilian.","sentences":["Leveraging research on the neural modelling of Portuguese, we contribute a collection of datasets for an array of language processing tasks and a corresponding collection of fine-tuned neural language models on these downstream tasks.","To align with mainstream benchmarks in the literature, originally developed in English, and to kick start their Portuguese counterparts, the datasets were machine-translated from English with a state-of-the-art translation engine.","The resulting PORTULAN ExtraGLUE benchmark is a basis for research on Portuguese whose improvement can be pursued in future work.","Similarly, the respective fine-tuned neural language models, developed with a low-rank adaptation approach, are made available as baselines that can stimulate future work on the neural processing of Portuguese.","All datasets and models have been developed and are made available for two variants of Portuguese: European and Brazilian."],"url":"http://arxiv.org/abs/2404.05333v1","category":"cs.CL"}
{"created":"2024-04-08 08:57:32","title":"CLIPping the Limits: Finding the Sweet Spot for Relevant Images in Automated Driving Systems Perception Testing","abstract":"Perception systems, especially cameras, are the eyes of automated driving systems. Ensuring that they function reliably and robustly is therefore an important building block in the automation of vehicles. There are various approaches to test the perception of automated driving systems. Ultimately, however, it always comes down to the investigation of the behavior of perception systems under specific input data. Camera images are a crucial part of the input data. Image data sets are therefore collected for the testing of automated driving systems, but it is non-trivial to find specific images in these data sets. Thanks to recent developments in neural networks, there are now methods for sorting the images in a data set according to their similarity to a prompt in natural language. In order to further automate the provision of search results, we make a contribution by automating the threshold definition in these sorted results and returning only the images relevant to the prompt as a result. Our focus is on preventing false positives and false negatives equally. It is also important that our method is robust and in the case that our assumptions are not fulfilled, we provide a fallback solution.","sentences":["Perception systems, especially cameras, are the eyes of automated driving systems.","Ensuring that they function reliably and robustly is therefore an important building block in the automation of vehicles.","There are various approaches to test the perception of automated driving systems.","Ultimately, however, it always comes down to the investigation of the behavior of perception systems under specific input data.","Camera images are a crucial part of the input data.","Image data sets are therefore collected for the testing of automated driving systems, but it is non-trivial to find specific images in these data sets.","Thanks to recent developments in neural networks, there are now methods for sorting the images in a data set according to their similarity to a prompt in natural language.","In order to further automate the provision of search results, we make a contribution by automating the threshold definition in these sorted results and returning only the images relevant to the prompt as a result.","Our focus is on preventing false positives and false negatives equally.","It is also important that our method is robust and in the case that our assumptions are not fulfilled, we provide a fallback solution."],"url":"http://arxiv.org/abs/2404.05309v1","category":"cs.CV"}
{"created":"2024-04-08 08:42:47","title":"Texture Classification Network Integrating Adaptive Wavelet Transform","abstract":"Graves' disease is a common condition that is diagnosed clinically by determining the smoothness of the thyroid texture and its morphology in ultrasound images. Currently, the most widely used approach for the automated diagnosis of Graves' disease utilizes Convolutional Neural Networks (CNNs) for both feature extraction and classification. However, these methods demonstrate limited efficacy in capturing texture features. Given the high capacity of wavelets in describing texture features, this research integrates learnable wavelet modules utilizing the Lifting Scheme into CNNs and incorporates a parallel wavelet branch into the ResNet18 model to enhance texture feature extraction. Our model can analyze texture features in spatial and frequency domains simultaneously, leading to optimized classification accuracy. We conducted experiments on collected ultrasound datasets and publicly available natural image texture datasets, our proposed network achieved 97.27% accuracy and 95.60% recall on ultrasound datasets, 60.765% accuracy on natural image texture datasets, surpassing the accuracy of ResNet and conrming the effectiveness of our approach.","sentences":["Graves' disease is a common condition that is diagnosed clinically by determining the smoothness of the thyroid texture and its morphology in ultrasound images.","Currently, the most widely used approach for the automated diagnosis of Graves' disease utilizes Convolutional Neural Networks (CNNs) for both feature extraction and classification.","However, these methods demonstrate limited efficacy in capturing texture features.","Given the high capacity of wavelets in describing texture features, this research integrates learnable wavelet modules utilizing the Lifting Scheme into CNNs and incorporates a parallel wavelet branch into the ResNet18 model to enhance texture feature extraction.","Our model can analyze texture features in spatial and frequency domains simultaneously, leading to optimized classification accuracy.","We conducted experiments on collected ultrasound datasets and publicly available natural image texture datasets, our proposed network achieved 97.27% accuracy and 95.60% recall on ultrasound datasets, 60.765% accuracy on natural image texture datasets, surpassing the accuracy of ResNet and conrming the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.05300v1","category":"cs.CV"}
{"created":"2024-04-08 07:49:52","title":"Cellular automata, many-valued logic, and deep neural networks","abstract":"We develop a theory characterizing the fundamental capability of deep neural networks to learn, from evolution traces, the logical rules governing the behavior of cellular automata (CA). This is accomplished by first establishing a novel connection between CA and Lukasiewicz propositional logic. While binary CA have been known for decades to essentially perform operations in Boolean logic, no such relationship exists for general CA. We demonstrate that many-valued (MV) logic, specifically Lukasiewicz propositional logic, constitutes a suitable language for characterizing general CA as logical machines. This is done by interpolating CA transition functions to continuous piecewise linear functions, which, by virtue of the McNaughton theorem, yield formulae in MV logic characterizing the CA. Recognizing that deep rectified linear unit (ReLU) networks realize continuous piecewise linear functions, it follows that these formulae are naturally extracted from CA evolution traces by deep ReLU networks. A corresponding algorithm together with a software implementation is provided. Finally, we show that the dynamical behavior of CA can be realized by recurrent neural networks.","sentences":["We develop a theory characterizing the fundamental capability of deep neural networks to learn, from evolution traces, the logical rules governing the behavior of cellular automata (CA).","This is accomplished by first establishing a novel connection between CA and Lukasiewicz propositional logic.","While binary CA have been known for decades to essentially perform operations in Boolean logic, no such relationship exists for general CA.","We demonstrate that many-valued (MV) logic, specifically Lukasiewicz propositional logic, constitutes a suitable language for characterizing general CA as logical machines.","This is done by interpolating CA transition functions to continuous piecewise linear functions, which, by virtue of the McNaughton theorem, yield formulae in MV logic characterizing the CA.","Recognizing that deep rectified linear unit (ReLU) networks realize continuous piecewise linear functions, it follows that these formulae are naturally extracted from CA evolution traces by deep ReLU networks.","A corresponding algorithm together with a software implementation is provided.","Finally, we show that the dynamical behavior of CA can be realized by recurrent neural networks."],"url":"http://arxiv.org/abs/2404.05259v1","category":"cs.AI"}
{"created":"2024-04-08 07:43:23","title":"Text-to-Image Synthesis for Any Artistic Styles: Advancements in Personalized Artistic Image Generation via Subdivision and Dual Binding","abstract":"Recent advancements in text-to-image models, such as Stable Diffusion, have demonstrated their ability to synthesize visual images through natural language prompts. One approach of personalizing text-to-image models, exemplified by DreamBooth, fine-tunes the pre-trained model by binding unique text identifiers with a few images of a specific subject. Although existing fine-tuning methods have demonstrated competence in rendering images according to the styles of famous painters, it is still challenging to learn to produce images encapsulating distinct art styles due to abstract and broad visual perceptions of stylistic attributes such as lines, shapes, textures, and colors. In this paper, we introduce a new method, Single-StyleForge, for personalization. It fine-tunes pre-trained text-to-image diffusion models to generate diverse images in specified styles from text prompts. By using around 15-20 images of the target style, the approach establishes a foundational binding of a unique token identifier with a broad range of the target style. It also utilizes auxiliary images to strengthen this binding, resulting in offering specific guidance on representing elements such as persons in a target style-consistent manner. In addition, we present ways to improve the quality of style and text-image alignment through a method called Multi-StyleForge, which inherits the strategy used in StyleForge and learns tokens in multiple. Experimental evaluation conducted on six distinct artistic styles demonstrates substantial improvements in both the quality of generated images and the perceptual fidelity metrics, such as FID, KID, and CLIP scores.","sentences":["Recent advancements in text-to-image models, such as Stable Diffusion, have demonstrated their ability to synthesize visual images through natural language prompts.","One approach of personalizing text-to-image models, exemplified by DreamBooth, fine-tunes the pre-trained model by binding unique text identifiers with a few images of a specific subject.","Although existing fine-tuning methods have demonstrated competence in rendering images according to the styles of famous painters, it is still challenging to learn to produce images encapsulating distinct art styles due to abstract and broad visual perceptions of stylistic attributes such as lines, shapes, textures, and colors.","In this paper, we introduce a new method, Single-StyleForge, for personalization.","It fine-tunes pre-trained text-to-image diffusion models to generate diverse images in specified styles from text prompts.","By using around 15-20 images of the target style, the approach establishes a foundational binding of a unique token identifier with a broad range of the target style.","It also utilizes auxiliary images to strengthen this binding, resulting in offering specific guidance on representing elements such as persons in a target style-consistent manner.","In addition, we present ways to improve the quality of style and text-image alignment through a method called Multi-StyleForge, which inherits the strategy used in StyleForge and learns tokens in multiple.","Experimental evaluation conducted on six distinct artistic styles demonstrates substantial improvements in both the quality of generated images and the perceptual fidelity metrics, such as FID, KID, and CLIP scores."],"url":"http://arxiv.org/abs/2404.05256v1","category":"cs.CV"}
{"created":"2024-04-08 07:33:52","title":"A model for heating the super-hot corona in solar active regions","abstract":"What physical mechanisms heat the outer solar or stellar atmosphere to million-Kelvin temperatures is a fundamental but long-standing open question. In particular, the solar corona in active region cores contains an even hotter component reaching ten million Kelvin, manifesting as persistent coronal loops in extreme ultraviolet and soft X-ray images, which imposes a more stringent energy budget. Here, we present a self-consistent coronal heating model using a state-of-the-art three-dimensional radiative magnetohydrodynamics simulation. We find that the continuous magnetic flux emergence in active regions keeps driving magnetic reconnections that release energy impulsively but, on time average, persistently. As a result, numerous sub-structures are heated to ten million Kelvin and then evolve independently, which collectively form long-lived and stable coronal loops as in observations. This provides a heating model explaining the origin of the super-hot coronal plasma and the persistence of hot coronal loops in emerging active regions.","sentences":["What physical mechanisms heat the outer solar or stellar atmosphere to million-Kelvin temperatures is a fundamental but long-standing open question.","In particular, the solar corona in active region cores contains an even hotter component reaching ten million Kelvin, manifesting as persistent coronal loops in extreme ultraviolet and soft X-ray images, which imposes a more stringent energy budget.","Here, we present a self-consistent coronal heating model using a state-of-the-art three-dimensional radiative magnetohydrodynamics simulation.","We find that the continuous magnetic flux emergence in active regions keeps driving magnetic reconnections that release energy impulsively but, on time average, persistently.","As a result, numerous sub-structures are heated to ten million Kelvin and then evolve independently, which collectively form long-lived and stable coronal loops as in observations.","This provides a heating model explaining the origin of the super-hot coronal plasma and the persistence of hot coronal loops in emerging active regions."],"url":"http://arxiv.org/abs/2404.05252v1","category":"astro-ph.SR"}
{"created":"2024-04-08 07:25:25","title":"SAFE-GIL: SAFEty Guided Imitation Learning","abstract":"Behavior Cloning is a popular approach to Imitation Learning, in which a robot observes an expert supervisor and learns a control policy. However, behavior cloning suffers from the \"compounding error\" problem - the policy errors compound as it deviates from the expert demonstrations and might lead to catastrophic system failures, limiting its use in safety-critical applications. On-policy data aggregation methods are able to address this issue at the cost of rolling out and repeated training of the imitation policy, which can be tedious and computationally prohibitive. We propose SAFE-GIL, an off-policy behavior cloning method that guides the expert via adversarial disturbance during data collection. The algorithm abstracts the imitation error as an adversarial disturbance in the system dynamics, injects it during data collection to expose the expert to safety critical states, and collects corrective actions. Our method biases training to more closely replicate expert behavior in safety-critical states and allows more variance in less critical states. We compare our method with several behavior cloning techniques and DAgger on autonomous navigation and autonomous taxiing tasks and show higher task success and safety, especially in low data regimes where the likelihood of error is higher, at a slight drop in the performance.","sentences":["Behavior Cloning is a popular approach to Imitation Learning, in which a robot observes an expert supervisor and learns a control policy.","However, behavior cloning suffers from the \"compounding error\" problem - the policy errors compound as it deviates from the expert demonstrations and might lead to catastrophic system failures, limiting its use in safety-critical applications.","On-policy data aggregation methods are able to address this issue at the cost of rolling out and repeated training of the imitation policy, which can be tedious and computationally prohibitive.","We propose SAFE-GIL, an off-policy behavior cloning method that guides the expert via adversarial disturbance during data collection.","The algorithm abstracts the imitation error as an adversarial disturbance in the system dynamics, injects it during data collection to expose the expert to safety critical states, and collects corrective actions.","Our method biases training to more closely replicate expert behavior in safety-critical states and allows more variance in less critical states.","We compare our method with several behavior cloning techniques and DAgger on autonomous navigation and autonomous taxiing tasks and show higher task success and safety, especially in low data regimes where the likelihood of error is higher, at a slight drop in the performance."],"url":"http://arxiv.org/abs/2404.05249v1","category":"cs.RO"}
{"created":"2024-04-08 07:15:06","title":"Product Description and QA Assisted Self-Supervised Opinion Summarization","abstract":"In e-commerce, opinion summarization is the process of summarizing the consensus opinions found in product reviews. However, the potential of additional sources such as product description and question-answers (QA) has been considered less often. Moreover, the absence of any supervised training data makes this task challenging. To address this, we propose a novel synthetic dataset creation (SDC) strategy that leverages information from reviews as well as additional sources for selecting one of the reviews as a pseudo-summary to enable supervised training. Our Multi-Encoder Decoder framework for Opinion Summarization (MEDOS) employs a separate encoder for each source, enabling effective selection of information while generating the summary. For evaluation, due to the unavailability of test sets with additional sources, we extend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to annotate summaries. Experiments across nine test sets demonstrate that the combination of our SDC approach and MEDOS model achieves on average a 14.5% improvement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis underlines the significance of incorporating additional sources for generating more informative summaries. Human evaluations further indicate that MEDOS scores relatively higher in coherence and fluency with 0.41 and 0.5 (-1 to 1) respectively, compared to existing models. To the best of our knowledge, we are the first to generate opinion summaries leveraging additional sources in a self-supervised setting.","sentences":["In e-commerce, opinion summarization is the process of summarizing the consensus opinions found in product reviews.","However, the potential of additional sources such as product description and question-answers (QA) has been considered less often.","Moreover, the absence of any supervised training data makes this task challenging.","To address this, we propose a novel synthetic dataset creation (SDC) strategy that leverages information from reviews as well as additional sources for selecting one of the reviews as a pseudo-summary to enable supervised training.","Our Multi-Encoder Decoder framework for Opinion Summarization (MEDOS) employs a separate encoder for each source, enabling effective selection of information while generating the summary.","For evaluation, due to the unavailability of test sets with additional sources, we extend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to annotate summaries.","Experiments across nine test sets demonstrate that the combination of our SDC approach and MEDOS model achieves on average a 14.5% improvement in ROUGE-1 F1 over the SOTA.","Moreover, comparative analysis underlines the significance of incorporating additional sources for generating more informative summaries.","Human evaluations further indicate that MEDOS scores relatively higher in coherence and fluency with 0.41 and 0.5 (-1 to 1) respectively, compared to existing models.","To the best of our knowledge, we are the first to generate opinion summaries leveraging additional sources in a self-supervised setting."],"url":"http://arxiv.org/abs/2404.05243v1","category":"cs.CL"}
{"created":"2024-04-08 07:11:15","title":"Computational Propaganda Theory and Bot Detection System: Critical Literature Review","abstract":"According to the classical definition, propaganda is the management of collective attitudes by manipulation of significant symbols. However this definition has changed to computational propaganda, the way manipulation takes place in digital medium. Computational propaganda is the use of algorithms, automation and human curation to purposefully distribute misleading information over social media networks to manipulate public opinion, for political polarization etc. Digital media platforms have introduced new modalities of propaganda such as the use of social bots and state-organized 'troll armies' for social astroturfing to simulate public support or opposition towards a particular topic. Along with this digital media has blurred the line between different forms of propaganda. Hence existing conceptual and epistemological frameworks in propaganda studies need a revision. One of the methods to detect the computational propaganda is to identify automation and bots. Many supervised machine learning based frameworks have been proposed for bot detection but these systems can only identify single accounts, not the coordinated activities of botnets and also these systems depend on the data structure provided by the social media platforms. Similarly, current systems have not included the image features in their detection system. Most of the systems are mainly built for Twitter while there are still uncharted areas of research in other social media platforms. Therefore, there are many unexplored research questions and methods in bot detection systems.","sentences":["According to the classical definition, propaganda is the management of collective attitudes by manipulation of significant symbols.","However this definition has changed to computational propaganda, the way manipulation takes place in digital medium.","Computational propaganda is the use of algorithms, automation and human curation to purposefully distribute misleading information over social media networks to manipulate public opinion, for political polarization etc.","Digital media platforms have introduced new modalities of propaganda such as the use of social bots and state-organized 'troll armies' for social astroturfing to simulate public support or opposition towards a particular topic.","Along with this digital media has blurred the line between different forms of propaganda.","Hence existing conceptual and epistemological frameworks in propaganda studies need a revision.","One of the methods to detect the computational propaganda is to identify automation and bots.","Many supervised machine learning based frameworks have been proposed for bot detection but these systems can only identify single accounts, not the coordinated activities of botnets and also these systems depend on the data structure provided by the social media platforms.","Similarly, current systems have not included the image features in their detection system.","Most of the systems are mainly built for Twitter while there are still uncharted areas of research in other social media platforms.","Therefore, there are many unexplored research questions and methods in bot detection systems."],"url":"http://arxiv.org/abs/2404.05240v1","category":"cs.SI"}
{"created":"2024-04-08 07:10:12","title":"Spatially Correlated RIS-Aided Secure Massive MIMO Under CSI and Hardware Imperfections","abstract":"This paper investigates the integration of a reconfigurable intelligent surface (RIS) into a secure multiuser massive multiple-input multiple-output (MIMO) system in the presence of transceiver hardware impairments (HWI), imperfect channel state information (CSI), and spatially correlated channels. We first introduce a linear minimum-mean-square error estimation algorithm for the aggregate channel by considering the impact of transceiver HWI and RIS phase-shift errors. Then, we derive a lower bound for the achievable ergodic secrecy rate in the presence of a multi-antenna eavesdropper when artificial noise (AN) is employed at the base station (BS). In addition, the obtained expressions of the ergodic secrecy rate are further simplified in some noteworthy special cases to obtain valuable insights. To counteract the effects of HWI, we present a power allocation optimization strategy between the confidential signals and AN, which admits a fixed-point equation solution. Our analysis reveals that a non-zero ergodic secrecy rate is preserved if the total transmit power decreases no faster than $1/N$, where $N$ is the number of RIS elements. Moreover, the ergodic secrecy rate grows logarithmically with the number of BS antennas $M$ and approaches a certain limit in the asymptotic regime $N\\rightarrow\\infty$. Simulation results are provided to verify the derived analytical results. They reveal the impact of key design parameters on the secrecy rate. It is shown that, with the proposed power allocation strategy, the secrecy rate loss due to HWI can be counteracted by increasing the number of low-cost RIS elements.","sentences":["This paper investigates the integration of a reconfigurable intelligent surface (RIS) into a secure multiuser massive multiple-input multiple-output (MIMO) system in the presence of transceiver hardware impairments (HWI), imperfect channel state information (CSI), and spatially correlated channels.","We first introduce a linear minimum-mean-square error estimation algorithm for the aggregate channel by considering the impact of transceiver HWI and RIS phase-shift errors.","Then, we derive a lower bound for the achievable ergodic secrecy rate in the presence of a multi-antenna eavesdropper when artificial noise (AN) is employed at the base station (BS).","In addition, the obtained expressions of the ergodic secrecy rate are further simplified in some noteworthy special cases to obtain valuable insights.","To counteract the effects of HWI, we present a power allocation optimization strategy between the confidential signals and AN, which admits a fixed-point equation solution.","Our analysis reveals that a non-zero ergodic secrecy rate is preserved if the total transmit power decreases no faster than $1/N$, where $N$ is the number of RIS elements.","Moreover, the ergodic secrecy rate grows logarithmically with the number of BS antennas $M$ and approaches a certain limit in the asymptotic regime $N\\rightarrow\\infty$. Simulation results are provided to verify the derived analytical results.","They reveal the impact of key design parameters on the secrecy rate.","It is shown that, with the proposed power allocation strategy, the secrecy rate loss due to HWI can be counteracted by increasing the number of low-cost RIS elements."],"url":"http://arxiv.org/abs/2404.05239v1","category":"cs.IT"}
{"created":"2024-04-08 07:01:42","title":"Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation","abstract":"Recently, a surge of 3D style transfer methods has been proposed that leverage the scene reconstruction power of a pre-trained neural radiance field (NeRF). To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene. However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality. Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style? In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures. We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations. We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation. Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency.","sentences":["Recently, a surge of 3D style transfer methods has been proposed that leverage the scene reconstruction power of a pre-trained neural radiance field (NeRF).","To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene.","However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality.","Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style?","In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures.","We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations.","We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation.","Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency."],"url":"http://arxiv.org/abs/2404.05236v1","category":"cs.CV"}
{"created":"2024-04-08 07:01:35","title":"Novelty Heuristics, Multi-Queue Search, and Portfolios for Numeric Planning","abstract":"Heuristic search is a powerful approach for solving planning problems and numeric planning is no exception. In this paper, we boost the performance of heuristic search for numeric planning with various powerful techniques orthogonal to improving heuristic informedness: numeric novelty heuristics, the Manhattan distance heuristic, and exploring the use of multi-queue search and portfolios for combining heuristics.","sentences":["Heuristic search is a powerful approach for solving planning problems and numeric planning is no exception.","In this paper, we boost the performance of heuristic search for numeric planning with various powerful techniques orthogonal to improving heuristic informedness: numeric novelty heuristics, the Manhattan distance heuristic, and exploring the use of multi-queue search and portfolios for combining heuristics."],"url":"http://arxiv.org/abs/2404.05235v1","category":"cs.AI"}
{"created":"2024-04-08 06:40:03","title":"Iof-maint -- Modular maintenance ontology","abstract":"In this paper we present a publicly-available maintenance ontology (Iof-maint). Iof-maint is a modular ontology aligned with the Industrial Ontology Foundry Core (IOF Core) and contains 20 classes and 2 relations. It provides a set of maintenance-specific terms used in a wide variety of practical data-driven use cases. Iof-maint supports OWL DL reasoning, is documented, and is actively maintained on GitHub. In this paper, we describe the evolution of the Iof-maint reference ontology based on the extraction of common concepts identified in a number of application ontologies working with industry maintenance work order, procedure and failure mode data.","sentences":["In this paper we present a publicly-available maintenance ontology (Iof-maint).","Iof-maint is a modular ontology aligned with the Industrial Ontology Foundry Core (IOF Core) and contains 20 classes and 2 relations.","It provides a set of maintenance-specific terms used in a wide variety of practical data-driven use cases.","Iof-maint supports OWL DL reasoning, is documented, and is actively maintained on GitHub.","In this paper, we describe the evolution of the Iof-maint reference ontology based on the extraction of common concepts identified in a number of application ontologies working with industry maintenance work order, procedure and failure mode data."],"url":"http://arxiv.org/abs/2404.05224v1","category":"cs.AI"}
{"created":"2024-04-08 06:36:42","title":"ITA-ECBS: A Bounded-Suboptimal Algorithm for Combined Target-Assignment and Path-Finding Problem","abstract":"Multi-Agent Path Finding (MAPF), i.e., finding collision-free paths for multiple robots, plays a critical role in many applications. Sometimes, assigning a specific target to each agent also presents a challenge. The Combined Target-Assignment and Path-Finding (TAPF) problem, a variant of MAPF, requires simultaneously assigning targets to agents and planning collision-free paths. Several algorithms, including CBM, CBS-TA, and ITA-CBS, can optimally solve the TAPF problem, with ITA-CBS being the leading method of flowtime. However, the only existing suboptimal method ECBS-TA, is derived from CBS-TA rather than ITA-CBS, and adapting the optimal ITA-CBS method to its bounded-suboptimal variant is a challenge due to the variability of target assignment solutions in different search nodes. We introduce ITA-ECBS as the first bounded-suboptimal variant of ITA-CBS. ITA-ECBS employs focal search to enhance efficiency and determines target assignments based on a new lower bound matrix. We show that ITA-ECBS outperforms the baseline method ECBS-TA in 87.42% of 54,033 test cases.","sentences":["Multi-Agent Path Finding (MAPF), i.e., finding collision-free paths for multiple robots, plays a critical role in many applications.","Sometimes, assigning a specific target to each agent also presents a challenge.","The Combined Target-Assignment and Path-Finding (TAPF) problem, a variant of MAPF, requires simultaneously assigning targets to agents and planning collision-free paths.","Several algorithms, including CBM, CBS-TA, and ITA-CBS, can optimally solve the TAPF problem, with ITA-CBS being the leading method of flowtime.","However, the only existing suboptimal method ECBS-TA, is derived from CBS-TA rather than ITA-CBS, and adapting the optimal ITA-CBS method to its bounded-suboptimal variant is a challenge due to the variability of target assignment solutions in different search nodes.","We introduce ITA-ECBS as the first bounded-suboptimal variant of ITA-CBS.","ITA-ECBS employs focal search to enhance efficiency and determines target assignments based on a new lower bound matrix.","We show that ITA-ECBS outperforms the baseline method ECBS-TA in 87.42% of 54,033 test cases."],"url":"http://arxiv.org/abs/2404.05223v1","category":"cs.AI"}
{"created":"2024-04-08 06:35:09","title":"LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models","abstract":"Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.","sentences":["Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability.","Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge.","The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison.","This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation.","Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks.","In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria.","(2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components.","With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP).","The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc."],"url":"http://arxiv.org/abs/2404.05221v1","category":"cs.CL"}
{"created":"2024-04-08 06:15:13","title":"Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning","abstract":"Human pose forecasting garners attention for its diverse applications. However, challenges in modeling the multi-modal nature of human motion and intricate interactions among agents persist, particularly with longer timescales and more agents. In this paper, we propose an interaction-aware trajectory-conditioned long-term multi-agent human pose forecasting model, utilizing a coarse-to-fine prediction approach: multi-modal global trajectories are initially forecasted, followed by respective local pose forecasts conditioned on each mode. In doing so, our Trajectory2Pose model introduces a graph-based agent-wise interaction module for a reciprocal forecast of local motion-conditioned global trajectory and trajectory-conditioned local pose. Our model effectively handles the multi-modality of human motion and the complexity of long-term multi-agent interactions, improving performance in complex environments. Furthermore, we address the lack of long-term (6s+) multi-agent (5+) datasets by constructing a new dataset from real-world images and 2D annotations, enabling a comprehensive evaluation of our proposed model. State-of-the-art prediction performance on both complex and simpler datasets confirms the generalized effectiveness of our method. The code is available at https://github.com/Jaewoo97/T2P.","sentences":["Human pose forecasting garners attention for its diverse applications.","However, challenges in modeling the multi-modal nature of human motion and intricate interactions among agents persist, particularly with longer timescales and more agents.","In this paper, we propose an interaction-aware trajectory-conditioned long-term multi-agent human pose forecasting model, utilizing a coarse-to-fine prediction approach: multi-modal global trajectories are initially forecasted, followed by respective local pose forecasts conditioned on each mode.","In doing so, our Trajectory2Pose model introduces a graph-based agent-wise interaction module for a reciprocal forecast of local motion-conditioned global trajectory and trajectory-conditioned local pose.","Our model effectively handles the multi-modality of human motion and the complexity of long-term multi-agent interactions, improving performance in complex environments.","Furthermore, we address the lack of long-term (6s+) multi-agent (5+) datasets by constructing a new dataset from real-world images and 2D annotations, enabling a comprehensive evaluation of our proposed model.","State-of-the-art prediction performance on both complex and simpler datasets confirms the generalized effectiveness of our method.","The code is available at https://github.com/Jaewoo97/T2P."],"url":"http://arxiv.org/abs/2404.05218v1","category":"cs.CV"}
{"created":"2024-04-08 06:00:14","title":"Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor When Adopting LLMs in HCI Research","abstract":"There is increasing interest in the adoption of LLMs in HCI research. However, LLMs may often be regarded as a panacea because of their powerful capabilities with an accompanying oversight on whether they are suitable for their intended tasks. We contend that LLMs should be adopted in a critical manner following rigorous evaluation. Accordingly, we present the evaluation of an LLM in identifying logical fallacies that will form part of a digital misinformation intervention. By comparing to a labeled dataset, we found that GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes invalid or unidentified instances, an accuracy of 0.90. This gives us the confidence to proceed with the application of the LLM while keeping in mind the areas where it still falls short. The paper describes our evaluation approach, results and reflections on the use of the LLM for our intended task.","sentences":["There is increasing interest in the adoption of LLMs in HCI research.","However, LLMs may often be regarded as a panacea because of their powerful capabilities with an accompanying oversight on whether they are suitable for their intended tasks.","We contend that LLMs should be adopted in a critical manner following rigorous evaluation.","Accordingly, we present the evaluation of an LLM in identifying logical fallacies that will form part of a digital misinformation intervention.","By comparing to a labeled dataset, we found that GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes invalid or unidentified instances, an accuracy of 0.90.","This gives us the confidence to proceed with the application of the LLM while keeping in mind the areas where it still falls short.","The paper describes our evaluation approach, results and reflections on the use of the LLM for our intended task."],"url":"http://arxiv.org/abs/2404.05213v1","category":"cs.HC"}
{"created":"2024-04-08 05:19:28","title":"SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos","abstract":"We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.","sentences":["We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos.","Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not.","We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks."],"url":"http://arxiv.org/abs/2404.05206v1","category":"cs.CV"}
{"created":"2024-04-08 05:10:40","title":"Effect of collective spin excitation on electronic transport in topological spin texture","abstract":"We develop an efficient real-time simulation method for the spin-charge coupled system in the velocity gauge. This method enables us to compute the real-time simulation for the two-dimensional system with the complex spin texture. We focus on the effect of the collective excitation of the localized spins on the electronic transport properties of the non-trivial topological state in real space. To investigate this effect, we calculate the linear optical conductivity by calculating the real-time evolution of the Kondo lattice model on the triangular lattice, which hosts an all-in/all-out magnetic structure. In the linear conductivity spectra, we observe multiple peaks below the bandgap regime, attributed to the resonant contributions of collective modes similar to the skyrmionic system, alongside broadband modifications resulting from off-resonant spin dynamics. The result shows that the collective excitation, similar to the skyrmionic system, influences the optical response of the electron systems based on symmetry analysis. We elucidate the interference between the contributions from the different spin excitations to the optical conductivity in the multiple spin texture, pointing out the mode-dependent electrical activity. We show the complex interplay between the complex spin texture and the itinerant electrons in the two-dimensional spin-charge coupled system.","sentences":["We develop an efficient real-time simulation method for the spin-charge coupled system in the velocity gauge.","This method enables us to compute the real-time simulation for the two-dimensional system with the complex spin texture.","We focus on the effect of the collective excitation of the localized spins on the electronic transport properties of the non-trivial topological state in real space.","To investigate this effect, we calculate the linear optical conductivity by calculating the real-time evolution of the Kondo lattice model on the triangular lattice, which hosts an all-in/all-out magnetic structure.","In the linear conductivity spectra, we observe multiple peaks below the bandgap regime, attributed to the resonant contributions of collective modes similar to the skyrmionic system, alongside broadband modifications resulting from off-resonant spin dynamics.","The result shows that the collective excitation, similar to the skyrmionic system, influences the optical response of the electron systems based on symmetry analysis.","We elucidate the interference between the contributions from the different spin excitations to the optical conductivity in the multiple spin texture, pointing out the mode-dependent electrical activity.","We show the complex interplay between the complex spin texture and the itinerant electrons in the two-dimensional spin-charge coupled system."],"url":"http://arxiv.org/abs/2404.05204v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 05:02:48","title":"Decision Transformer for Wireless Communications: A New Paradigm of Resource Management","abstract":"As the next generation of mobile systems evolves, artificial intelligence (AI) is expected to deeply integrate with wireless communications for resource management in variable environments. In particular, deep reinforcement learning (DRL) is an important tool for addressing stochastic optimization issues of resource allocation. However, DRL has to start each new training process from the beginning once the state and action spaces change, causing low sample efficiency and poor generalization ability. Moreover, each DRL training process may take a large number of epochs to converge, which is unacceptable for time-sensitive scenarios. In this paper, we adopt an alternative AI technology, namely, the Decision Transformer (DT), and propose a DT-based adaptive decision architecture for wireless resource management. This architecture innovates through constructing pre-trained models in the cloud and then fine-tuning personalized models at the edges. By leveraging the power of DT models learned over extensive datasets, the proposed architecture is expected to achieve rapid convergence with many fewer training epochs and higher performance in a new context, e.g., similar tasks with different state and action spaces, compared with DRL. We then design DT frameworks for two typical communication scenarios: Intelligent reflecting surfaces-aided communications and unmanned aerial vehicle-aided edge computing. Simulations demonstrate that the proposed DT frameworks achieve over $3$-$6$ times speedup in convergence and better performance relative to the classic DRL method, namely, proximal policy optimization.","sentences":["As the next generation of mobile systems evolves, artificial intelligence (AI) is expected to deeply integrate with wireless communications for resource management in variable environments.","In particular, deep reinforcement learning (DRL) is an important tool for addressing stochastic optimization issues of resource allocation.","However, DRL has to start each new training process from the beginning once the state and action spaces change, causing low sample efficiency and poor generalization ability.","Moreover, each DRL training process may take a large number of epochs to converge, which is unacceptable for time-sensitive scenarios.","In this paper, we adopt an alternative AI technology, namely, the Decision Transformer (DT), and propose a DT-based adaptive decision architecture for wireless resource management.","This architecture innovates through constructing pre-trained models in the cloud and then fine-tuning personalized models at the edges.","By leveraging the power of DT models learned over extensive datasets, the proposed architecture is expected to achieve rapid convergence with many fewer training epochs and higher performance in a new context, e.g., similar tasks with different state and action spaces, compared with DRL.","We then design DT frameworks for two typical communication scenarios: Intelligent reflecting surfaces-aided communications and unmanned aerial vehicle-aided edge computing.","Simulations demonstrate that the proposed DT frameworks achieve over $3$-$6$ times speedup in convergence and better performance relative to the classic DRL method, namely, proximal policy optimization."],"url":"http://arxiv.org/abs/2404.05199v1","category":"eess.SP"}
{"created":"2024-04-08 04:56:27","title":"Fair Lotteries for Participatory Budgeting","abstract":"In pursuit of participatory budgeting (PB) outcomes with broader fairness guarantees, we initiate the study of lotteries over discrete PB outcomes. As the projects have heterogeneous costs, the amount spent may not be equal ex ante and ex post. To address this, we develop a technique to bound the amount by which the ex-post spend differs from the ex-ante spend -- the property is termed budget balanced up to one project (BB1). With respect to fairness, we take a best-of-both-worlds perspective, seeking outcomes that are both ex-ante and ex-post fair. Towards this goal, we initiate a study of ex-ante fairness properties in PB, including Individual Fair Share (IFS), Unanimous Fair Share (UFS) and their stronger variants, as well as Group Fair Share (GFS). We show several incompatibility results between these ex-ante fairness notions and existing ex-post concepts based on justified representation. One of our main contributions is a randomized algorithm which simultaneously satisfies ex-ante Strong UFS, ex-post full justified representation (FJR) and ex-post BB1 for PB with binary utilities.","sentences":["In pursuit of participatory budgeting (PB) outcomes with broader fairness guarantees, we initiate the study of lotteries over discrete PB outcomes.","As the projects have heterogeneous costs, the amount spent may not be equal ex ante and ex post.","To address this, we develop a technique to bound the amount by which the ex-post spend differs from the ex-ante spend -- the property is termed budget balanced up to one project (BB1).","With respect to fairness, we take a best-of-both-worlds perspective, seeking outcomes that are both ex-ante and ex-post fair.","Towards this goal, we initiate a study of ex-ante fairness properties in PB, including Individual Fair Share (IFS), Unanimous Fair Share (UFS) and their stronger variants, as well as Group Fair Share (GFS).","We show several incompatibility results between these ex-ante fairness notions and existing ex-post concepts based on justified representation.","One of our main contributions is a randomized algorithm which simultaneously satisfies ex-ante Strong UFS, ex-post full justified representation (FJR) and ex-post BB1 for PB with binary utilities."],"url":"http://arxiv.org/abs/2404.05198v1","category":"cs.GT"}
{"created":"2024-04-08 04:30:33","title":"Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging","abstract":"Model merging is a promising lightweight model empowerment technique that does not rely on expensive computing devices (e.g., GPUs) or require the collection of specific training data. Instead, it involves editing different upstream model parameters to absorb their downstream task capabilities. However, uncertified model merging can infringe upon the Intellectual Property (IP) rights of the original upstream models. In this paper, we conduct the first study on the robustness of IP protection methods in model merging scenarios. We investigate two state-of-the-art IP protection techniques: Quantization Watermarking and Instructional Fingerprint, along with various advanced model merging technologies, such as Task Arithmetic, TIES-MERGING, and so on. Experimental results indicate that current Large Language Model (LLM) watermarking techniques cannot survive in the merged models, whereas model fingerprinting techniques can. Our research aims to highlight that model merging should be an indispensable consideration in the robustness assessment of model IP protection techniques, thereby promoting the healthy development of the open-source LLM community.","sentences":["Model merging is a promising lightweight model empowerment technique that does not rely on expensive computing devices (e.g., GPUs) or require the collection of specific training data.","Instead, it involves editing different upstream model parameters to absorb their downstream task capabilities.","However, uncertified model merging can infringe upon the Intellectual Property (IP) rights of the original upstream models.","In this paper, we conduct the first study on the robustness of IP protection methods in model merging scenarios.","We investigate two state-of-the-art IP protection techniques: Quantization Watermarking and Instructional Fingerprint, along with various advanced model merging technologies, such as Task Arithmetic, TIES-MERGING, and so on.","Experimental results indicate that current Large Language Model (LLM) watermarking techniques cannot survive in the merged models, whereas model fingerprinting techniques can.","Our research aims to highlight that model merging should be an indispensable consideration in the robustness assessment of model IP protection techniques, thereby promoting the healthy development of the open-source LLM community."],"url":"http://arxiv.org/abs/2404.05188v1","category":"cs.CR"}
{"created":"2024-04-08 04:18:54","title":"Predicting the Geothermal Gradient in Colombia: a Machine Learning Approach","abstract":"Accurate determination of the geothermal gradient is critical for assessing the geothermal energy potential of a given region. Of particular interest is the case of Colombia, a country with abundant geothermal resources. A history of active oil and gas exploration and production has left drilled boreholes in different geological settings, providing direct measurements of the geothermal gradient. Unfortunately, large regions of the country where geothermal resources might exist lack such measurements. Indirect geophysical measurements are costly and difficult to perform at regional scales. Computational thermal models could be constructed, but they require very detailed knowledge of the underlying geology and uniform sampling of subsurface temperatures to be well-constrained. We present an alternative approach that leverages recent advances in supervised machine learning and available direct measurements to predict the geothermal gradient in regions where only global-scale geophysical datasets and course geological knowledge are available. We find that a Gradient Boosted Regression Tree algorithm yields optimal predictions and extensively validate the trained model. We show that predictions of our model are within 12\\% accuracy and that independent measurements performed by other authors agree well with our model. Finnally, we present a geothermal gradient map for Colombia that highlights regions where futher exploration and data collection should be performed.","sentences":["Accurate determination of the geothermal gradient is critical for assessing the geothermal energy potential of a given region.","Of particular interest is the case of Colombia, a country with abundant geothermal resources.","A history of active oil and gas exploration and production has left drilled boreholes in different geological settings, providing direct measurements of the geothermal gradient.","Unfortunately, large regions of the country where geothermal resources might exist lack such measurements.","Indirect geophysical measurements are costly and difficult to perform at regional scales.","Computational thermal models could be constructed, but they require very detailed knowledge of the underlying geology and uniform sampling of subsurface temperatures to be well-constrained.","We present an alternative approach that leverages recent advances in supervised machine learning and available direct measurements to predict the geothermal gradient in regions where only global-scale geophysical datasets and course geological knowledge are available.","We find that a Gradient Boosted Regression Tree algorithm yields optimal predictions and extensively validate the trained model.","We show that predictions of our model are within 12\\% accuracy and that independent measurements performed by other authors agree well with our model.","Finnally, we present a geothermal gradient map for Colombia that highlights regions where futher exploration and data collection should be performed."],"url":"http://arxiv.org/abs/2404.05184v1","category":"physics.geo-ph"}
{"created":"2024-04-08 04:17:27","title":"Progressive Alignment with VLM-LLM Feature to Augment Defect Classification for the ASE Dataset","abstract":"Traditional defect classification approaches are facing with two barriers. (1) Insufficient training data and unstable data quality. Collecting sufficient defective sample is expensive and time-costing, consequently leading to dataset variance. It introduces the difficulty on recognition and learning. (2) Over-dependence on visual modality. When the image pattern and texture is monotonic for all defect classes in a given dataset, the performance of conventional AOI system cannot be guaranteed. In scenarios where image quality is compromised due to mechanical failures or when defect information is inherently difficult to discern, the performance of deep models cannot be guaranteed. A main question is, \"how to solve those two problems when they occur at the same time?\" The feasible strategy is to explore another feature within dataset and combine an eminent vision-language model (VLM) and Large-Language model (LLM) with their astonishing zero-shot capability. In this work, we propose the special ASE dataset, including rich data description recorded on image, for defect classification, but the defect feature is uneasy to learn directly. Secondly, We present the prompting for VLM-LLM against defect classification with the proposed ASE dataset to activate extra-modality feature from images to enhance performance. Then, We design the novel progressive feature alignment (PFA) block to refine image-text feature to alleviate the difficulty of alignment under few-shot scenario. Finally, the proposed Cross-modality attention fusion (CMAF) module can effectively fuse different modality feature. Experiment results have demonstrated our method's effectiveness over several defect classification methods for the ASE dataset.","sentences":["Traditional defect classification approaches are facing with two barriers.","(1) Insufficient training data and unstable data quality.","Collecting sufficient defective sample is expensive and time-costing, consequently leading to dataset variance.","It introduces the difficulty on recognition and learning.","(2) Over-dependence on visual modality.","When the image pattern and texture is monotonic for all defect classes in a given dataset, the performance of conventional AOI system cannot be guaranteed.","In scenarios where image quality is compromised due to mechanical failures or when defect information is inherently difficult to discern, the performance of deep models cannot be guaranteed.","A main question is, \"how to solve those two problems when they occur at the same time?\"","The feasible strategy is to explore another feature within dataset and combine an eminent vision-language model (VLM) and Large-Language model (LLM) with their astonishing zero-shot capability.","In this work, we propose the special ASE dataset, including rich data description recorded on image, for defect classification, but the defect feature is uneasy to learn directly.","Secondly, We present the prompting for VLM-LLM against defect classification with the proposed ASE dataset to activate extra-modality feature from images to enhance performance.","Then, We design the novel progressive feature alignment (PFA) block to refine image-text feature to alleviate the difficulty of alignment under few-shot scenario.","Finally, the proposed Cross-modality attention fusion (CMAF) module can effectively fuse different modality feature.","Experiment results have demonstrated our method's effectiveness over several defect classification methods for the ASE dataset."],"url":"http://arxiv.org/abs/2404.05183v1","category":"cs.CV"}
{"created":"2024-04-08 04:14:02","title":"DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model","abstract":"To enhance the performance of large language models (LLM) on downstream tasks, one solution is to fine-tune certain LLM parameters and make it better align with the characteristics of the training dataset. This process is commonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server). This necessitates the sharing of sensitive user data across public environments, thereby raising potential privacy concerns. To tackle these challenges, we propose a distributed PEFT framework called DLoRA. DLoRA enables scalable PEFT operations to be performed collaboratively between the cloud and user devices. Coupled with the proposed Kill and Revive algorithm, the evaluation results demonstrate that DLoRA can significantly reduce the computation and communication workload over the user devices while achieving superior accuracy and privacy protection.","sentences":["To enhance the performance of large language models (LLM) on downstream tasks, one solution is to fine-tune certain LLM parameters and make it better align with the characteristics of the training dataset.","This process is commonly known as parameter-efficient fine-tuning (PEFT).","Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server).","This necessitates the sharing of sensitive user data across public environments, thereby raising potential privacy concerns.","To tackle these challenges, we propose a distributed PEFT framework called DLoRA.","DLoRA enables scalable PEFT operations to be performed collaboratively between the cloud and user devices.","Coupled with the proposed Kill and Revive algorithm, the evaluation results demonstrate that DLoRA can significantly reduce the computation and communication workload over the user devices while achieving superior accuracy and privacy protection."],"url":"http://arxiv.org/abs/2404.05182v1","category":"cs.LG"}
{"created":"2024-04-08 04:10:50","title":"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery","abstract":"Solar Photovoltaic (PV) technology is increasingly recognized as a pivotal solution in the global pursuit of clean and renewable energy. This technology addresses the urgent need for sustainable energy alternatives by converting solar power into electricity without greenhouse gas emissions. It not only curtails global carbon emissions but also reduces reliance on finite, non-renewable energy sources. In this context, monitoring solar panel farms becomes essential for understanding and facilitating the worldwide shift toward clean energy. This study contributes to this effort by developing the first comprehensive global dataset of multispectral satellite imagery of solar panel farms. This dataset is intended to form the basis for training robust machine learning models, which can accurately map and analyze the expansion and distribution of solar panel farms globally. The insights gained from this endeavor will be instrumental in guiding informed decision-making for a sustainable energy future. https://github.com/yzyly1992/GloSoFarID","sentences":["Solar Photovoltaic (PV) technology is increasingly recognized as a pivotal solution in the global pursuit of clean and renewable energy.","This technology addresses the urgent need for sustainable energy alternatives by converting solar power into electricity without greenhouse gas emissions.","It not only curtails global carbon emissions but also reduces reliance on finite, non-renewable energy sources.","In this context, monitoring solar panel farms becomes essential for understanding and facilitating the worldwide shift toward clean energy.","This study contributes to this effort by developing the first comprehensive global dataset of multispectral satellite imagery of solar panel farms.","This dataset is intended to form the basis for training robust machine learning models, which can accurately map and analyze the expansion and distribution of solar panel farms globally.","The insights gained from this endeavor will be instrumental in guiding informed decision-making for a sustainable energy future.","https://github.com/yzyly1992/GloSoFarID"],"url":"http://arxiv.org/abs/2404.05180v1","category":"cs.CV"}
{"created":"2024-04-08 03:08:59","title":"Rendering-Enhanced Automatic Image-to-Point Cloud Registration for Roadside Scenes","abstract":"Prior point cloud provides 3D environmental context, which enhances the capabilities of monocular camera in downstream vision tasks, such as 3D object detection, via data fusion. However, the absence of accurate and automated registration methods for estimating camera extrinsic parameters in roadside scene point clouds notably constrains the potential applications of roadside cameras. This paper proposes a novel approach for the automatic registration between prior point clouds and images from roadside scenes. The main idea involves rendering photorealistic grayscale views taken at specific perspectives from the prior point cloud with the help of their features like RGB or intensity values. These generated views can reduce the modality differences between images and prior point clouds, thereby improve the robustness and accuracy of the registration results. Particularly, we specify an efficient algorithm, named neighbor rendering, for the rendering process. Then we introduce a method for automatically estimating the initial guess using only rough guesses of camera's position. At last, we propose a procedure for iteratively refining the extrinsic parameters by minimizing the reprojection error for line features extracted from both generated and camera images using Segment Anything Model (SAM). We assess our method using a self-collected dataset, comprising eight cameras strategically positioned throughout the university campus. Experiments demonstrate our method's capability to automatically align prior point cloud with roadside camera image, achieving a rotation accuracy of 0.202 degrees and a translation precision of 0.079m. Furthermore, we validate our approach's effectiveness in visual applications by substantially improving monocular 3D object detection performance.","sentences":["Prior point cloud provides 3D environmental context, which enhances the capabilities of monocular camera in downstream vision tasks, such as 3D object detection, via data fusion.","However, the absence of accurate and automated registration methods for estimating camera extrinsic parameters in roadside scene point clouds notably constrains the potential applications of roadside cameras.","This paper proposes a novel approach for the automatic registration between prior point clouds and images from roadside scenes.","The main idea involves rendering photorealistic grayscale views taken at specific perspectives from the prior point cloud with the help of their features like RGB or intensity values.","These generated views can reduce the modality differences between images and prior point clouds, thereby improve the robustness and accuracy of the registration results.","Particularly, we specify an efficient algorithm, named neighbor rendering, for the rendering process.","Then we introduce a method for automatically estimating the initial guess using only rough guesses of camera's position.","At last, we propose a procedure for iteratively refining the extrinsic parameters by minimizing the reprojection error for line features extracted from both generated and camera images using Segment Anything Model (SAM).","We assess our method using a self-collected dataset, comprising eight cameras strategically positioned throughout the university campus.","Experiments demonstrate our method's capability to automatically align prior point cloud with roadside camera image, achieving a rotation accuracy of 0.202 degrees and a translation precision of 0.079m.","Furthermore, we validate our approach's effectiveness in visual applications by substantially improving monocular 3D object detection performance."],"url":"http://arxiv.org/abs/2404.05164v1","category":"cs.RO"}
{"created":"2024-04-08 03:02:55","title":"Probing Plexciton Emission from 2D Materials on Gold Nanotrenches","abstract":"Probing strongly coupled quasiparticle excitations at their intrinsic length scales offers unique insights into their properties and facilitates the design of devices with novel functionalities. In this work, we investigate the formation and emission characteristics of plexcitons, arising from the interaction between surface plasmons in narrow gold nanotrenches and excitons in monolayer WSe2. We study this strong plasmon-exciton coupling in both the far-field and the near-field. Specifically, we observe a Rabi splitting in the far-field reflection spectra of about 80 meV under ambient conditions, consistent with our theoretical modeling. Using a custom-designed near-field probe, we find that plexciton emission originates predominantly from the lower-frequency branch, which we can directly probe and map the local field distribution. We precisely determine the plexciton extension, similar to the trench width, with nanometric precision via collecting spectra at controlled probe locations. Our work opens exciting prospects for nanoscale mapping and engineering of plexcitons in complex nanostructures with potential applications in nanophotonic devices, optoelectronics, and quantum electrodynamics in nanoscale cavities.","sentences":["Probing strongly coupled quasiparticle excitations at their intrinsic length scales offers unique insights into their properties and facilitates the design of devices with novel functionalities.","In this work, we investigate the formation and emission characteristics of plexcitons, arising from the interaction between surface plasmons in narrow gold nanotrenches and excitons in monolayer WSe2.","We study this strong plasmon-exciton coupling in both the far-field and the near-field.","Specifically, we observe a Rabi splitting in the far-field reflection spectra of about 80 meV under ambient conditions, consistent with our theoretical modeling.","Using a custom-designed near-field probe, we find that plexciton emission originates predominantly from the lower-frequency branch, which we can directly probe and map the local field distribution.","We precisely determine the plexciton extension, similar to the trench width, with nanometric precision via collecting spectra at controlled probe locations.","Our work opens exciting prospects for nanoscale mapping and engineering of plexcitons in complex nanostructures with potential applications in nanophotonic devices, optoelectronics, and quantum electrodynamics in nanoscale cavities."],"url":"http://arxiv.org/abs/2404.05161v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 02:21:15","title":"Intelligent Reflecting Surface Aided Target Localization With Unknown Transceiver-IRS Channel State Information","abstract":"Integrating wireless sensing capabilities into base stations (BSs) has become a widespread trend in the future beyond fifth-generation (B5G)/sixth-generation (6G) wireless networks. In this paper, we investigate intelligent reflecting surface (IRS) enabled wireless localization, in which an IRS is deployed to assist a BS in locating a target in its non-line-of-sight (NLoS) region. In particular, we consider the case where the BS-IRS channel state information (CSI) is unknown. Specifically, we first propose a separate BS-IRS channel estimation scheme in which the BS operates in full-duplex mode (FDM), i.e., a portion of the BS antennas send downlink pilot signals to the IRS, while the remaining BS antennas receive the uplink pilot signals reflected by the IRS. However, we can only obtain an incomplete BS-IRS channel matrix based on our developed iterative coordinate descent-based channel estimation algorithm due to the \"sign ambiguity issue\". Then, we employ the multiple hypotheses testing framework to perform target localization based on the incomplete estimated channel, in which the probability of each hypothesis is updated using Bayesian inference at each cycle. Moreover, we formulate a joint BS transmit waveform and IRS phase shifts optimization problem to improve the target localization performance by maximizing the weighted sum distance between each two hypotheses. However, the objective function is essentially a quartic function of the IRS phase shift vector, thus motivating us to resort to the penalty-based method to tackle this challenge. Simulation results validate the effectiveness of our proposed target localization scheme and show that the scheme's performance can be further improved by finely designing the BS transmit waveform and IRS phase shifts intending to maximize the weighted sum distance between different hypotheses.","sentences":["Integrating wireless sensing capabilities into base stations (BSs) has become a widespread trend in the future beyond fifth-generation (B5G)/sixth-generation (6G) wireless networks.","In this paper, we investigate intelligent reflecting surface (IRS) enabled wireless localization, in which an IRS is deployed to assist a BS in locating a target in its non-line-of-sight (NLoS) region.","In particular, we consider the case where the BS-IRS channel state information (CSI) is unknown.","Specifically, we first propose a separate BS-IRS channel estimation scheme in which the BS operates in full-duplex mode (FDM), i.e., a portion of the BS antennas send downlink pilot signals to the IRS, while the remaining BS antennas receive the uplink pilot signals reflected by the IRS.","However, we can only obtain an incomplete BS-IRS channel matrix based on our developed iterative coordinate descent-based channel estimation algorithm due to the \"sign ambiguity issue\".","Then, we employ the multiple hypotheses testing framework to perform target localization based on the incomplete estimated channel, in which the probability of each hypothesis is updated using Bayesian inference at each cycle.","Moreover, we formulate a joint BS transmit waveform and IRS phase shifts optimization problem to improve the target localization performance by maximizing the weighted sum distance between each two hypotheses.","However, the objective function is essentially a quartic function of the IRS phase shift vector, thus motivating us to resort to the penalty-based method to tackle this challenge.","Simulation results validate the effectiveness of our proposed target localization scheme and show that the scheme's performance can be further improved by finely designing the BS transmit waveform and IRS phase shifts intending to maximize the weighted sum distance between different hypotheses."],"url":"http://arxiv.org/abs/2404.05149v1","category":"cs.ET"}
{"created":"2024-04-08 01:55:28","title":"Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac Patients","abstract":"Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication. However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors. The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in healthcare. This study directly addresses the inefficiencies and inaccuracies in creating discharge notes manually, particularly for cardiac patients, by employing AI techniques, specifically large language model (LLM). Utilizing a substantial dataset from a cardiology center, encompassing wide-ranging medical records and physician assessments, our research evaluates the capability of LLM to enhance the documentation process. Among the various models assessed, Mistral-7B distinguished itself by accurately generating discharge notes that significantly improve both documentation efficiency and the continuity of care for patients. These notes underwent rigorous qualitative evaluation by medical expert, receiving high marks for their clinical relevance, completeness, readability, and contribution to informed decision-making and care planning. Coupled with quantitative analyses, these results confirm Mistral-7B's efficacy in distilling complex medical information into concise, coherent summaries. Overall, our findings illuminate the considerable promise of specialized LLM, such as Mistral-7B, in refining healthcare documentation workflows and advancing patient care. This study lays the groundwork for further integrating advanced AI technologies in healthcare, demonstrating their potential to revolutionize patient documentation and support better care outcomes.","sentences":["Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication.","However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors.","The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in healthcare.","This study directly addresses the inefficiencies and inaccuracies in creating discharge notes manually, particularly for cardiac patients, by employing AI techniques, specifically large language model (LLM).","Utilizing a substantial dataset from a cardiology center, encompassing wide-ranging medical records and physician assessments, our research evaluates the capability of LLM to enhance the documentation process.","Among the various models assessed, Mistral-7B distinguished itself by accurately generating discharge notes that significantly improve both documentation efficiency and the continuity of care for patients.","These notes underwent rigorous qualitative evaluation by medical expert, receiving high marks for their clinical relevance, completeness, readability, and contribution to informed decision-making and care planning.","Coupled with quantitative analyses, these results confirm Mistral-7B's efficacy in distilling complex medical information into concise, coherent summaries.","Overall, our findings illuminate the considerable promise of specialized LLM, such as Mistral-7B, in refining healthcare documentation workflows and advancing patient care.","This study lays the groundwork for further integrating advanced AI technologies in healthcare, demonstrating their potential to revolutionize patient documentation and support better care outcomes."],"url":"http://arxiv.org/abs/2404.05144v1","category":"cs.CL"}
{"created":"2024-04-08 01:54:28","title":"Plug and Play with Prompts: A Prompt Tuning Approach for Controlling Text Generation","abstract":"Transformer-based Large Language Models (LLMs) have shown exceptional language generation capabilities in response to text-based prompts. However, controlling the direction of generation via textual prompts has been challenging, especially with smaller models. In this work, we explore the use of Prompt Tuning to achieve controlled language generation. Generated text is steered using prompt embeddings, which are trained using a small language model, used as a discriminator. Moreover, we demonstrate that these prompt embeddings can be trained with a very small dataset, with as low as a few hundred training examples. Our method thus offers a data and parameter efficient solution towards controlling language model outputs. We carry out extensive evaluation on four datasets: SST-5 and Yelp (sentiment analysis), GYAFC (formality) and JIGSAW (toxic language). Finally, we demonstrate the efficacy of our method towards mitigating harmful, toxic, and biased text generated by language models.","sentences":["Transformer-based Large Language Models (LLMs) have shown exceptional language generation capabilities in response to text-based prompts.","However, controlling the direction of generation via textual prompts has been challenging, especially with smaller models.","In this work, we explore the use of Prompt Tuning to achieve controlled language generation.","Generated text is steered using prompt embeddings, which are trained using a small language model, used as a discriminator.","Moreover, we demonstrate that these prompt embeddings can be trained with a very small dataset, with as low as a few hundred training examples.","Our method thus offers a data and parameter efficient solution towards controlling language model outputs.","We carry out extensive evaluation on four datasets: SST-5 and Yelp (sentiment analysis), GYAFC (formality) and JIGSAW (toxic language).","Finally, we demonstrate the efficacy of our method towards mitigating harmful, toxic, and biased text generated by language models."],"url":"http://arxiv.org/abs/2404.05143v1","category":"cs.CL"}
{"created":"2024-04-08 01:29:10","title":"Self-Supervised Multi-Object Tracking with Path Consistency","abstract":"In this paper, we propose a novel concept of path consistency to learn robust object matching without using manual object identity supervision. Our key idea is that, to track a object through frames, we can obtain multiple different association results from a model by varying the frames it can observe, i.e., skipping frames in observation. As the differences in observations do not alter the identities of objects, the obtained association results should be consistent. Based on this rationale, we generate multiple observation paths, each specifying a different set of frames to be skipped, and formulate the Path Consistency Loss that enforces the association results are consistent across different observation paths. We use the proposed loss to train our object matching model with only self-supervision. By extensive experiments on three tracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method outperforms existing unsupervised methods with consistent margins on various evaluation metrics, and even achieves performance close to supervised methods.","sentences":["In this paper, we propose a novel concept of path consistency to learn robust object matching without using manual object identity supervision.","Our key idea is that, to track a object through frames, we can obtain multiple different association results from a model by varying the frames it can observe, i.e., skipping frames in observation.","As the differences in observations do not alter the identities of objects, the obtained association results should be consistent.","Based on this rationale, we generate multiple observation paths, each specifying a different set of frames to be skipped, and formulate the Path Consistency Loss that enforces the association results are consistent across different observation paths.","We use the proposed loss to train our object matching model with only self-supervision.","By extensive experiments on three tracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method outperforms existing unsupervised methods with consistent margins on various evaluation metrics, and even achieves performance close to supervised methods."],"url":"http://arxiv.org/abs/2404.05136v1","category":"cs.CV"}
{"created":"2024-04-08 01:21:11","title":"EcoVerse: An Annotated Twitter Dataset for Eco-Relevance Classification, Environmental Impact Analysis, and Stance Detection","abstract":"Anthropogenic ecological crisis constitutes a significant challenge that all within the academy must urgently face, including the Natural Language Processing (NLP) community. While recent years have seen increasing work revolving around climate-centric discourse, crucial environmental and ecological topics outside of climate change remain largely unaddressed, despite their prominent importance. Mainstream NLP tasks, such as sentiment analysis, dominate the scene, but there remains an untouched space in the literature involving the analysis of environmental impacts of certain events and practices. To address this gap, this paper presents EcoVerse, an annotated English Twitter dataset of 3,023 tweets spanning a wide spectrum of environmental topics. We propose a three-level annotation scheme designed for Eco-Relevance Classification, Stance Detection, and introducing an original approach for Environmental Impact Analysis. We detail the data collection, filtering, and labeling process that led to the creation of the dataset. Remarkable Inter-Annotator Agreement indicates that the annotation scheme produces consistent annotations of high quality. Subsequent classification experiments using BERT-based models, including ClimateBERT, are presented. These yield encouraging results, while also indicating room for a model specifically tailored for environmental texts. The dataset is made freely available to stimulate further research.","sentences":["Anthropogenic ecological crisis constitutes a significant challenge that all within the academy must urgently face, including the Natural Language Processing (NLP) community.","While recent years have seen increasing work revolving around climate-centric discourse, crucial environmental and ecological topics outside of climate change remain largely unaddressed, despite their prominent importance.","Mainstream NLP tasks, such as sentiment analysis, dominate the scene, but there remains an untouched space in the literature involving the analysis of environmental impacts of certain events and practices.","To address this gap, this paper presents EcoVerse, an annotated English Twitter dataset of 3,023 tweets spanning a wide spectrum of environmental topics.","We propose a three-level annotation scheme designed for Eco-Relevance Classification, Stance Detection, and introducing an original approach for Environmental Impact Analysis.","We detail the data collection, filtering, and labeling process that led to the creation of the dataset.","Remarkable Inter-Annotator Agreement indicates that the annotation scheme produces consistent annotations of high quality.","Subsequent classification experiments using BERT-based models, including ClimateBERT, are presented.","These yield encouraging results, while also indicating room for a model specifically tailored for environmental texts.","The dataset is made freely available to stimulate further research."],"url":"http://arxiv.org/abs/2404.05133v1","category":"cs.CL"}
{"created":"2024-04-08 01:16:56","title":"Enabling Privacy-Preserving Cyber Threat Detection with Federated Learning","abstract":"Despite achieving good performance and wide adoption, machine learning based security detection models (e.g., malware classifiers) are subject to concept drift and evasive evolution of attackers, which renders up-to-date threat data as a necessity. However, due to enforcement of various privacy protection regulations (e.g., GDPR), it is becoming increasingly challenging or even prohibitive for security vendors to collect individual-relevant and privacy-sensitive threat datasets, e.g., SMS spam/non-spam messages from mobile devices. To address such obstacles, this study systematically profiles the (in)feasibility of federated learning for privacy-preserving cyber threat detection in terms of effectiveness, byzantine resilience, and efficiency. This is made possible by the build-up of multiple threat datasets and threat detection models, and more importantly, the design of realistic and security-specific experiments.   We evaluate FL on two representative threat detection tasks, namely SMS spam detection and Android malware detection. It shows that FL-trained detection models can achieve a performance that is comparable to centrally trained counterparts. Also, most non-IID data distributions have either minor or negligible impact on the model performance, while a label-based non-IID distribution of a high extent can incur non-negligible fluctuation and delay in FL training. Then, under a realistic threat model, FL turns out to be adversary-resistant to attacks of both data poisoning and model poisoning. Particularly, the attacking impact of a practical data poisoning attack is no more than 0.14\\% loss in model accuracy. Regarding FL efficiency, a bootstrapping strategy turns out to be effective to mitigate the training delay as observed in label-based non-IID scenarios.","sentences":["Despite achieving good performance and wide adoption, machine learning based security detection models (e.g., malware classifiers) are subject to concept drift and evasive evolution of attackers, which renders up-to-date threat data as a necessity.","However, due to enforcement of various privacy protection regulations (e.g., GDPR), it is becoming increasingly challenging or even prohibitive for security vendors to collect individual-relevant and privacy-sensitive threat datasets, e.g., SMS spam/non-spam messages from mobile devices.","To address such obstacles, this study systematically profiles the (in)feasibility of federated learning for privacy-preserving cyber threat detection in terms of effectiveness, byzantine resilience, and efficiency.","This is made possible by the build-up of multiple threat datasets and threat detection models, and more importantly, the design of realistic and security-specific experiments.   ","We evaluate FL on two representative threat detection tasks, namely SMS spam detection and Android malware detection.","It shows that FL-trained detection models can achieve a performance that is comparable to centrally trained counterparts.","Also, most non-IID data distributions have either minor or negligible impact on the model performance, while a label-based non-IID distribution of a high extent can incur non-negligible fluctuation and delay in FL training.","Then, under a realistic threat model, FL turns out to be adversary-resistant to attacks of both data poisoning and model poisoning.","Particularly, the attacking impact of a practical data poisoning attack is no more than 0.14\\% loss in model accuracy.","Regarding FL efficiency, a bootstrapping strategy turns out to be effective to mitigate the training delay as observed in label-based non-IID scenarios."],"url":"http://arxiv.org/abs/2404.05130v1","category":"cs.CR"}
{"created":"2024-04-07 23:48:26","title":"Factorization of functions in the Schur-Agler class related to test functions","abstract":"We provide necessary and sufficient conditions for operator-valued functions on arbitrary sets associated with a collection of test functions to have factorizations in several situations.","sentences":["We provide necessary and sufficient conditions for operator-valued functions on arbitrary sets associated with a collection of test functions to have factorizations in several situations."],"url":"http://arxiv.org/abs/2404.05109v1","category":"math.FA"}
{"created":"2024-04-07 22:53:43","title":"StockGPT: A GenAI Model for Stock Prediction and Trading","abstract":"This paper introduces StockGPT, an autoregressive \"number\" model pretrained directly on the history of daily U.S. stock returns. Treating each return series as a sequence of tokens, the model excels at understanding and predicting the highly intricate stock return dynamics. Instead of relying on handcrafted trading patterns using historical stock prices, StockGPT automatically learns the hidden representations predictive of future returns via its attention mechanism. On a held-out test sample from 2001 to 2023, a daily rebalanced long-short portfolio formed from StockGPT predictions earns an annual return of 119% with a Sharpe ratio of 6.5. The StockGPT-based portfolio completely explains away momentum and long-/short-term reversals, eliminating the need for manually crafted price-based strategies and also encompasses most leading stock market factors. This highlights the immense promise of generative AI in surpassing human in making complex financial investment decisions and illustrates the efficacy of the attention mechanism of large language models when applied to a completely different domain.","sentences":["This paper introduces StockGPT, an autoregressive \"number\" model pretrained directly on the history of daily U.S. stock returns.","Treating each return series as a sequence of tokens, the model excels at understanding and predicting the highly intricate stock return dynamics.","Instead of relying on handcrafted trading patterns using historical stock prices, StockGPT automatically learns the hidden representations predictive of future returns via its attention mechanism.","On a held-out test sample from 2001 to 2023, a daily rebalanced long-short portfolio formed from StockGPT predictions earns an annual return of 119% with a Sharpe ratio of 6.5.","The StockGPT-based portfolio completely explains away momentum and long-/short-term reversals, eliminating the need for manually crafted price-based strategies and also encompasses most leading stock market factors.","This highlights the immense promise of generative AI in surpassing human in making complex financial investment decisions and illustrates the efficacy of the attention mechanism of large language models when applied to a completely different domain."],"url":"http://arxiv.org/abs/2404.05101v1","category":"q-fin.CP"}
{"created":"2024-04-07 22:31:34","title":"Active Test-Time Adaptation: Theoretical Analyses and An Algorithm","abstract":"Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies.   To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting.   We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA","sentences":["Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings.","Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies.   ","To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting.   ","We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee.","We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF).","We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques.","Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods.","Our code is available at https://github.com/divelab/ATTA"],"url":"http://arxiv.org/abs/2404.05094v1","category":"cs.LG"}
{"created":"2024-04-07 22:15:13","title":"How Bad is Training on Synthetic Data? A Statistical Analysis of Language Model Collapse","abstract":"The phenomenon of model collapse, introduced in (Shumailov et al., 2023), refers to the deterioration in performance that occurs when new models are trained on synthetic data generated from previously trained models. This recursive training loop makes the tails of the original distribution disappear, thereby making future-generation models forget about the initial (real) distribution. With the aim of rigorously understanding model collapse in language models, we consider in this paper a statistical model that allows us to characterize the impact of various recursive training scenarios. Specifically, we demonstrate that model collapse cannot be avoided when training solely on synthetic data. However, when mixing both real and synthetic data, we provide an estimate of a maximal amount of synthetic data below which model collapse can eventually be avoided. Our theoretical conclusions are further supported by empirical validations.","sentences":["The phenomenon of model collapse, introduced in (Shumailov et al., 2023), refers to the deterioration in performance that occurs when new models are trained on synthetic data generated from previously trained models.","This recursive training loop makes the tails of the original distribution disappear, thereby making future-generation models forget about the initial (real) distribution.","With the aim of rigorously understanding model collapse in language models, we consider in this paper a statistical model that allows us to characterize the impact of various recursive training scenarios.","Specifically, we demonstrate that model collapse cannot be avoided when training solely on synthetic data.","However, when mixing both real and synthetic data, we provide an estimate of a maximal amount of synthetic data below which model collapse can eventually be avoided.","Our theoretical conclusions are further supported by empirical validations."],"url":"http://arxiv.org/abs/2404.05090v1","category":"cs.LG"}
{"created":"2024-04-07 22:00:50","title":"A Note on LoRA","abstract":"LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently adapting Large Language Models (LLMs) with remarkable simplicity and efficacy. This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale. Without introducing new experiments, we aim to improve the understanding and application of LoRA.","sentences":["LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently adapting Large Language Models (LLMs) with remarkable simplicity and efficacy.","This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale.","Without introducing new experiments, we aim to improve the understanding and application of LoRA."],"url":"http://arxiv.org/abs/2404.05086v1","category":"cs.LG"}
{"created":"2024-04-07 21:06:52","title":"On the Uniqueness of Solution for the Bellman Equation of LTL Objectives","abstract":"Surrogate rewards for linear temporal logic (LTL) objectives are commonly utilized in planning problems for LTL objectives. In a widely-adopted surrogate reward approach, two discount factors are used to ensure that the expected return approximates the satisfaction probability of the LTL objective. The expected return then can be estimated by methods using the Bellman updates such as reinforcement learning. However, the uniqueness of the solution to the Bellman equation with two discount factors has not been explicitly discussed. We demonstrate with an example that when one of the discount factors is set to one, as allowed in many previous works, the Bellman equation may have multiple solutions, leading to inaccurate evaluation of the expected return. We then propose a condition for the Bellman equation to have the expected return as the unique solution, requiring the solutions for states inside a rejecting bottom strongly connected component (BSCC) to be 0. We prove this condition is sufficient by showing that the solutions for the states with discounting can be separated from those for the states without discounting under this condition","sentences":["Surrogate rewards for linear temporal logic (LTL) objectives are commonly utilized in planning problems for LTL objectives.","In a widely-adopted surrogate reward approach, two discount factors are used to ensure that the expected return approximates the satisfaction probability of the LTL objective.","The expected return then can be estimated by methods using the Bellman updates such as reinforcement learning.","However, the uniqueness of the solution to the Bellman equation with two discount factors has not been explicitly discussed.","We demonstrate with an example that when one of the discount factors is set to one, as allowed in many previous works, the Bellman equation may have multiple solutions, leading to inaccurate evaluation of the expected return.","We then propose a condition for the Bellman equation to have the expected return as the unique solution, requiring the solutions for states inside a rejecting bottom strongly connected component (BSCC) to be 0.","We prove this condition is sufficient by showing that the solutions for the states with discounting can be separated from those for the states without discounting under this condition"],"url":"http://arxiv.org/abs/2404.05074v1","category":"cs.AI"}
{"created":"2024-04-07 20:50:13","title":"Test-Time Training for Depression Detection","abstract":"Previous works on depression detection use datasets collected in similar environments to train and test the models. In practice, however, the train and test distributions cannot be guaranteed to be identical. Distribution shifts can be introduced due to variations such as recording environment (e.g., background noise) and demographics (e.g., gender, age, etc). Such distributional shifts can surprisingly lead to severe performance degradation of the depression detection models. In this paper, we analyze the application of test-time training (TTT) to improve robustness of models trained for depression detection. When compared to regular testing of the models, we find TTT can significantly improve the robustness of the model under a variety of distributional shifts introduced due to: (a) background-noise, (b) gender-bias, and (c) data collection and curation procedure (i.e., train and test samples are from separate datasets).","sentences":["Previous works on depression detection use datasets collected in similar environments to train and test the models.","In practice, however, the train and test distributions cannot be guaranteed to be identical.","Distribution shifts can be introduced due to variations such as recording environment (e.g., background noise) and demographics (e.g., gender, age, etc).","Such distributional shifts can surprisingly lead to severe performance degradation of the depression detection models.","In this paper, we analyze the application of test-time training (TTT) to improve robustness of models trained for depression detection.","When compared to regular testing of the models, we find TTT can significantly improve the robustness of the model under a variety of distributional shifts introduced due to: (a) background-noise, (b) gender-bias, and (c) data collection and curation procedure (i.e., train and test samples are from separate datasets)."],"url":"http://arxiv.org/abs/2404.05071v1","category":"cs.LG"}
{"created":"2024-04-07 20:15:40","title":"Automated Prediction of Breast Cancer Response to Neoadjuvant Chemotherapy from DWI Data","abstract":"Effective surgical planning for breast cancer hinges on accurately predicting pathological complete response (pCR) to neoadjuvant chemotherapy (NAC). Diffusion-weighted MRI (DWI) and machine learning offer a non-invasive approach for early pCR assessment. However, most machine-learning models require manual tumor segmentation, a cumbersome and error-prone task. We propose a deep learning model employing \"Size-Adaptive Lesion Weighting\" for automatic DWI tumor segmentation to enhance pCR prediction accuracy. Despite histopathological changes during NAC complicating DWI image segmentation, our model demonstrates robust performance. Utilizing the BMMR2 challenge dataset, it matches human experts in pCR prediction pre-NAC with an area under the curve (AUC) of 0.76 vs. 0.796, and surpasses standard automated methods mid-NAC, with an AUC of 0.729 vs. 0.654 and 0.576. Our approach represents a significant advancement in automating breast cancer treatment planning, enabling more reliable pCR predictions without manual segmentation.","sentences":["Effective surgical planning for breast cancer hinges on accurately predicting pathological complete response (pCR) to neoadjuvant chemotherapy (NAC).","Diffusion-weighted MRI (DWI) and machine learning offer a non-invasive approach for early pCR assessment.","However, most machine-learning models require manual tumor segmentation, a cumbersome and error-prone task.","We propose a deep learning model employing \"Size-Adaptive Lesion Weighting\" for automatic DWI tumor segmentation to enhance pCR prediction accuracy.","Despite histopathological changes during NAC complicating DWI image segmentation, our model demonstrates robust performance.","Utilizing the BMMR2 challenge dataset, it matches human experts in pCR prediction pre-NAC with an area under the curve (AUC) of 0.76 vs. 0.796, and surpasses standard automated methods mid-NAC, with an AUC of 0.729 vs. 0.654 and 0.576.","Our approach represents a significant advancement in automating breast cancer treatment planning, enabling more reliable pCR predictions without manual segmentation."],"url":"http://arxiv.org/abs/2404.05061v1","category":"cs.CV"}
{"created":"2024-04-07 19:29:09","title":"Percentile Criterion Optimization in Offline Reinforcement Learning","abstract":"In reinforcement learning, robust policies for high-stakes decision-making problems with limited data are usually computed by optimizing the \\emph{percentile criterion}. The percentile criterion is approximately solved by constructing an \\emph{ambiguity set} that contains the true model with high probability and optimizing the policy for the worst model in the set. Since the percentile criterion is non-convex, constructing ambiguity sets is often challenging. Existing work uses \\emph{Bayesian credible regions} as ambiguity sets, but they are often unnecessarily large and result in learning overly conservative policies. To overcome these shortcomings, we propose a novel Value-at-Risk based dynamic programming algorithm to optimize the percentile criterion without explicitly constructing any ambiguity sets. Our theoretical and empirical results show that our algorithm implicitly constructs much smaller ambiguity sets and learns less conservative robust policies.","sentences":["In reinforcement learning, robust policies for high-stakes decision-making problems with limited data are usually computed by optimizing the \\emph{percentile criterion}.","The percentile criterion is approximately solved by constructing an \\emph{ambiguity set} that contains the true model with high probability and optimizing the policy for the worst model in the set.","Since the percentile criterion is non-convex, constructing ambiguity sets is often challenging.","Existing work uses \\emph{Bayesian credible regions} as ambiguity sets, but they are often unnecessarily large and result in learning overly conservative policies.","To overcome these shortcomings, we propose a novel Value-at-Risk based dynamic programming algorithm to optimize the percentile criterion without explicitly constructing any ambiguity sets.","Our theoretical and empirical results show that our algorithm implicitly constructs much smaller ambiguity sets and learns less conservative robust policies."],"url":"http://arxiv.org/abs/2404.05055v1","category":"cs.LG"}
{"created":"2024-04-07 19:10:02","title":"PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated Segmentation Learning","abstract":"Automatic License Plate Recognition (ALPR) is an integral component of an intelligent transport system with extensive applications in secure transportation, vehicle-to-vehicle communication, stolen vehicles detection, traffic violations, and traffic flow management. The existing license plate detection system focuses on one-shot learners or pre-trained models that operate with a geometric bounding box, limiting the model's performance. Furthermore, continuous video data streams uploaded to the central server result in network and complexity issues. To combat this, PlateSegFL was introduced, which implements U-Net-based segmentation along with Federated Learning (FL). U-Net is well-suited for multi-class image segmentation tasks because it can analyze a large number of classes and generate a pixel-level segmentation map for each class. Federated Learning is used to reduce the quantity of data required while safeguarding the user's privacy. Different computing platforms, such as mobile phones, are able to collaborate on the development of a standard prediction model where it makes efficient use of one's time; incorporates more diverse data; delivers projections in real-time; and requires no physical effort from the user; resulting around 95% F1 score.","sentences":["Automatic License Plate Recognition (ALPR) is an integral component of an intelligent transport system with extensive applications in secure transportation, vehicle-to-vehicle communication, stolen vehicles detection, traffic violations, and traffic flow management.","The existing license plate detection system focuses on one-shot learners or pre-trained models that operate with a geometric bounding box, limiting the model's performance.","Furthermore, continuous video data streams uploaded to the central server result in network and complexity issues.","To combat this, PlateSegFL was introduced, which implements U-Net-based segmentation along with Federated Learning (FL).","U-Net is well-suited for multi-class image segmentation tasks because it can analyze a large number of classes and generate a pixel-level segmentation map for each class.","Federated Learning is used to reduce the quantity of data required while safeguarding the user's privacy.","Different computing platforms, such as mobile phones, are able to collaborate on the development of a standard prediction model where it makes efficient use of one's time; incorporates more diverse data; delivers projections in real-time; and requires no physical effort from the user; resulting around 95% F1 score."],"url":"http://arxiv.org/abs/2404.05049v1","category":"cs.CV"}
{"created":"2024-04-07 19:00:45","title":"FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback","abstract":"Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through Fine-Grained Artificial Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Specifically, We first utilize AI tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback. Then, based on the collected reward data, three specialized reward models are trained to produce dense rewards. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters.","sentences":["Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks.","However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship.","To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs.","However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive.","To handle these limitations, we propose an innovative method to align modalities in LVLMs through Fine-Grained Artificial Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward.","Specifically, We first utilize AI tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback.","Then, based on the collected reward data, three specialized reward models are trained to produce dense rewards.","Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm.","Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method.","Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters."],"url":"http://arxiv.org/abs/2404.05046v1","category":"cs.CV"}
{"created":"2024-04-07 19:00:12","title":"Spanners in Planar Domains via Steiner Spanners and non-Steiner Tree Covers","abstract":"We study spanners in planar domains, including polygonal domains, polyhedral terrain, and planar metrics. Previous work showed that for any constant $\\epsilon\\in (0,1)$, one could construct a $(2+\\epsilon)$-spanner with $O(n\\log(n))$ edges (SICOMP 2019), and there is a lower bound of $\\Omega(n^2)$ edges for any $(2-\\epsilon)$-spanner (SoCG 2015). The main open question is whether a linear number of edges suffices and the stretch can be reduced to $2$. We resolve this problem by showing that for stretch $2$, one needs $\\Omega(n\\log n)$ edges, and for stretch $2+\\epsilon$ for any fixed $\\epsilon \\in (0,1)$, $O(n)$ edges are sufficient. Our lower bound is the first super-linear lower bound for stretch $2$.   En route to achieve our result, we introduce the problem of constructing non-Steiner tree covers for metrics, which is a natural variant of the well-known Steiner point removal problem for trees (SODA 2001). Given a tree and a set of terminals in the tree, our goal is to construct a collection of a small number of dominating trees such that for every two points, at least one tree in the collection preserves their distance within a small stretch factor. Here, we identify an unexpected threshold phenomenon around $2$ where a sharp transition from $n$ trees to $\\Theta(\\log n)$ trees and then to $O(1)$ trees happens. Specifically, (i) for stretch $ 2-\\epsilon$, one needs $\\Omega(n)$ trees; (ii) for stretch $2$, $\\Theta(\\log n)$ tree is necessary and sufficient; and (iii) for stretch $2+\\epsilon$, a constant number of trees suffice. Furthermore, our lower bound technique for the non-Steiner tree covers of stretch $2$ has further applications in proving lower bounds for two related constructions in tree metrics: reliable spanners and locality-sensitive orderings. Our lower bound for locality-sensitive orderings matches the best upper bound (STOC 2022).","sentences":["We study spanners in planar domains, including polygonal domains, polyhedral terrain, and planar metrics.","Previous work showed that for any constant $\\epsilon\\in (0,1)$, one could construct a $(2+\\epsilon)$-spanner with $O(n\\log(n))$ edges (SICOMP 2019), and there is a lower bound of $\\Omega(n^2)$ edges for any $(2-\\epsilon)$-spanner (SoCG 2015).","The main open question is whether a linear number of edges suffices and the stretch can be reduced to $2$. We resolve this problem by showing that for stretch $2$, one needs $\\Omega(n\\log n)$ edges, and for stretch $2+\\epsilon$ for any fixed $\\epsilon \\in (0,1)$, $O(n)$ edges are sufficient.","Our lower bound is the first super-linear lower bound for stretch $2$.   En route to achieve our result, we introduce the problem of constructing non-Steiner tree covers for metrics, which is a natural variant of the well-known Steiner point removal problem for trees (SODA 2001).","Given a tree and a set of terminals in the tree, our goal is to construct a collection of a small number of dominating trees such that for every two points, at least one tree in the collection preserves their distance within a small stretch factor.","Here, we identify an unexpected threshold phenomenon around $2$ where a sharp transition from $n$ trees to $\\Theta(\\log n)$ trees and then to $O(1)$ trees happens.","Specifically, (i) for stretch $ 2-\\epsilon$, one needs $\\Omega(n)$ trees; (ii) for stretch $2$, $\\Theta(\\log n)$ tree is necessary and sufficient; and (iii) for stretch $2+\\epsilon$, a constant number of trees suffice.","Furthermore, our lower bound technique for the non-Steiner tree covers of stretch $2$ has further applications in proving lower bounds for two related constructions in tree metrics: reliable spanners and locality-sensitive orderings.","Our lower bound for locality-sensitive orderings matches the best upper bound (STOC 2022)."],"url":"http://arxiv.org/abs/2404.05045v1","category":"cs.CG"}
{"created":"2024-04-07 18:47:52","title":"StaccaToe: A Single-Leg Robot that Mimics the Human Leg and Toe","abstract":"We introduce StaccaToe, a human-scale, electric motor-powered single-leg robot designed to rival the agility of human locomotion through two distinctive attributes: an actuated toe and a co-actuation configuration inspired by the human leg. Leveraging the foundational design of HyperLeg's lower leg mechanism, we develop a stand-alone robot by incorporating new link designs, custom-designed power electronics, and a refined control system. Unlike previous jumping robots that rely on either special mechanisms (e.g., springs and clutches) or hydraulic/pneumatic actuators, StaccaToe employs electric motors without energy storage mechanisms. This choice underscores our ultimate goal of developing a practical, high-performance humanoid robot capable of human-like, stable walking as well as explosive dynamic movements. In this paper, we aim to empirically evaluate the balance capability and the exertion of explosive ground reaction forces of our toe and co-actuation mechanisms. Throughout extensive hardware and controller development, StaccaToe showcases its control fidelity by demonstrating a balanced tip-toe stance and dynamic jump. This study is significant for three key reasons: 1) StaccaToe represents the first human-scale, electric motor-driven single-leg robot to execute dynamic maneuvers without relying on specialized mechanisms; 2) our research provides empirical evidence of the benefits of replicating critical human leg attributes in robotic design; and 3) we explain the design process for creating agile legged robots, the details that have been scantily covered in academic literature.","sentences":["We introduce StaccaToe, a human-scale, electric motor-powered single-leg robot designed to rival the agility of human locomotion through two distinctive attributes: an actuated toe and a co-actuation configuration inspired by the human leg.","Leveraging the foundational design of HyperLeg's lower leg mechanism, we develop a stand-alone robot by incorporating new link designs, custom-designed power electronics, and a refined control system.","Unlike previous jumping robots that rely on either special mechanisms (e.g., springs and clutches) or hydraulic/pneumatic actuators, StaccaToe employs electric motors without energy storage mechanisms.","This choice underscores our ultimate goal of developing a practical, high-performance humanoid robot capable of human-like, stable walking as well as explosive dynamic movements.","In this paper, we aim to empirically evaluate the balance capability and the exertion of explosive ground reaction forces of our toe and co-actuation mechanisms.","Throughout extensive hardware and controller development, StaccaToe showcases its control fidelity by demonstrating a balanced tip-toe stance and dynamic jump.","This study is significant for three key reasons: 1) StaccaToe represents the first human-scale, electric motor-driven single-leg robot to execute dynamic maneuvers without relying on specialized mechanisms; 2) our research provides empirical evidence of the benefits of replicating critical human leg attributes in robotic design; and 3) we explain the design process for creating agile legged robots, the details that have been scantily covered in academic literature."],"url":"http://arxiv.org/abs/2404.05039v1","category":"cs.RO"}
{"created":"2024-04-07 17:38:03","title":"On two-coloring bipartite uniform hypergraphs","abstract":"Of a given bipartite graph $G = (V, E)$, it is elementary to construct a bipartition in time $O(|E|)$. For a given $k$-graph $H = H^{(k)}$ with $k \\geq 3$ fixed, Lov\\'asz proved that deciding whether $H$ is bipartite is NP-complete. Let $\\mathcal{B}_n$ denote the collection of all $[n]$-vertex bipartite $k$-graphs. We construct, of a given $H \\in \\mathcal{B}_n$, a bipartition in time averaging $O(n^k)$ over the class $\\mathcal{B}_n$. (When $k=3$, our result expedites one of Person and Schacht.) We also consider an application to the class of all $[n]$-vertex 3-graphs $H$ forbidding the Fano plane as a subhypergraph.","sentences":["Of a given bipartite graph $G = (V, E)$, it is elementary to construct a bipartition in time $O(|E|)$. For a given $k$-graph $H = H^{(k)}$ with $k \\geq 3$ fixed, Lov\\'asz proved that deciding whether $H$ is bipartite is NP-complete.","Let $\\mathcal{B}_n$ denote the collection of all $[n]$-vertex bipartite $k$-graphs.","We construct, of a given $H \\in \\mathcal{B}_n$, a bipartition in time averaging $O(n^k)$ over the class $\\mathcal{B}_n$. (When $k=3$, our result expedites one of Person and Schacht.)","We also consider an application to the class of all $[n]$-vertex 3-graphs $H$ forbidding the Fano plane as a subhypergraph."],"url":"http://arxiv.org/abs/2404.05026v1","category":"math.CO"}
{"created":"2024-04-07 17:25:52","title":"DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology","abstract":"In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images. To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images. To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift. We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin. A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets. All models are available at github.com/marrlab/DinoBloom.","sentences":["In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears.","However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images.","To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline.","Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images.","To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift.","We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin.","A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets.","All models are available at github.com/marrlab/DinoBloom."],"url":"http://arxiv.org/abs/2404.05022v1","category":"cs.CV"}
{"created":"2024-04-07 17:06:22","title":"Hyperbolic Learning with Synthetic Captions for Open-World Detection","abstract":"Open-world detection poses significant challenges, as it requires the detection of any object using either object class labels or free-form texts. Existing related works often use large-scale manual annotated caption datasets for training, which are extremely expensive to collect. Instead, we propose to transfer knowledge from vision-language models (VLMs) to enrich the open-vocabulary descriptions automatically. Specifically, we bootstrap dense synthetic captions using pre-trained VLMs to provide rich descriptions on different regions in images, and incorporate these captions to train a novel detector that generalizes to novel concepts. To mitigate the noise caused by hallucination in synthetic captions, we also propose a novel hyperbolic vision-language learning approach to impose a hierarchy between visual and caption embeddings. We call our detector ``HyperLearner''. We conduct extensive experiments on a wide variety of open-world detection benchmarks (COCO, LVIS, Object Detection in the Wild, RefCOCO) and our results show that our model consistently outperforms existing state-of-the-art methods, such as GLIP, GLIPv2 and Grounding DINO, when using the same backbone.","sentences":["Open-world detection poses significant challenges, as it requires the detection of any object using either object class labels or free-form texts.","Existing related works often use large-scale manual annotated caption datasets for training, which are extremely expensive to collect.","Instead, we propose to transfer knowledge from vision-language models (VLMs) to enrich the open-vocabulary descriptions automatically.","Specifically, we bootstrap dense synthetic captions using pre-trained VLMs to provide rich descriptions on different regions in images, and incorporate these captions to train a novel detector that generalizes to novel concepts.","To mitigate the noise caused by hallucination in synthetic captions, we also propose a novel hyperbolic vision-language learning approach to impose a hierarchy between visual and caption embeddings.","We call our detector ``HyperLearner''.","We conduct extensive experiments on a wide variety of open-world detection benchmarks (COCO, LVIS, Object Detection in the Wild, RefCOCO) and our results show that our model consistently outperforms existing state-of-the-art methods, such as GLIP, GLIPv2 and Grounding DINO, when using the same backbone."],"url":"http://arxiv.org/abs/2404.05016v1","category":"cs.CV"}
{"created":"2024-04-07 16:35:53","title":"Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats","abstract":"Chatbots can serve as a viable tool for preliminary depression diagnosis via interactive conversations with potential patients. Nevertheless, the blend of task-oriented and chit-chat in diagnosis-related dialogues necessitates professional expertise and empathy. Such unique requirements challenge traditional dialogue frameworks geared towards single optimization goals. To address this, we propose an innovative ontology definition and generation framework tailored explicitly for depression diagnosis dialogues, combining the reliability of task-oriented conversations with the appeal of empathy-related chit-chat. We further apply the framework to D$^4$, the only existing public dialogue dataset on depression diagnosis-oriented chats. Exhaustive experimental results indicate significant improvements in task completion and emotional support generation in depression diagnosis, fostering a more comprehensive approach to task-oriented chat dialogue system development and its applications in digital mental health.","sentences":["Chatbots can serve as a viable tool for preliminary depression diagnosis via interactive conversations with potential patients.","Nevertheless, the blend of task-oriented and chit-chat in diagnosis-related dialogues necessitates professional expertise and empathy.","Such unique requirements challenge traditional dialogue frameworks geared towards single optimization goals.","To address this, we propose an innovative ontology definition and generation framework tailored explicitly for depression diagnosis dialogues, combining the reliability of task-oriented conversations with the appeal of empathy-related chit-chat.","We further apply the framework to D$^4$, the only existing public dialogue dataset on depression diagnosis-oriented chats.","Exhaustive experimental results indicate significant improvements in task completion and emotional support generation in depression diagnosis, fostering a more comprehensive approach to task-oriented chat dialogue system development and its applications in digital mental health."],"url":"http://arxiv.org/abs/2404.05012v1","category":"cs.AI"}
{"created":"2024-04-07 15:58:25","title":"Camera-Based Remote Physiology Sensing for Hundreds of Subjects Across Skin Tones","abstract":"Remote photoplethysmography (rPPG) emerges as a promising method for non-invasive, convenient measurement of vital signs, utilizing the widespread presence of cameras. Despite advancements, existing datasets fall short in terms of size and diversity, limiting comprehensive evaluation under diverse conditions. This paper presents an in-depth analysis of the VitalVideo dataset, the largest real-world rPPG dataset to date, encompassing 893 subjects and 6 Fitzpatrick skin tones. Our experimentation with six unsupervised methods and three supervised models demonstrates that datasets comprising a few hundred subjects(i.e., 300 for UBFC-rPPG, 500 for PURE, and 700 for MMPD-Simple) are sufficient for effective rPPG model training. Our findings highlight the importance of diversity and consistency in skin tones for precise performance evaluation across different datasets.","sentences":["Remote photoplethysmography (rPPG) emerges as a promising method for non-invasive, convenient measurement of vital signs, utilizing the widespread presence of cameras.","Despite advancements, existing datasets fall short in terms of size and diversity, limiting comprehensive evaluation under diverse conditions.","This paper presents an in-depth analysis of the VitalVideo dataset, the largest real-world rPPG dataset to date, encompassing 893 subjects and 6 Fitzpatrick skin tones.","Our experimentation with six unsupervised methods and three supervised models demonstrates that datasets comprising a few hundred subjects(i.e., 300 for UBFC-rPPG, 500 for PURE, and 700 for MMPD-Simple) are sufficient for effective rPPG model training.","Our findings highlight the importance of diversity and consistency in skin tones for precise performance evaluation across different datasets."],"url":"http://arxiv.org/abs/2404.05003v1","category":"cs.CV"}
{"created":"2024-04-07 15:48:33","title":"Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval","abstract":"Deep quantization methods have shown high efficiency on large-scale image retrieval. However, current models heavily rely on ground-truth information, hindering the application of quantization in label-hungry scenarios. A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users. Though such sketchy tags do not obviously reveal the labels, they actually contain useful semantic information for supervising deep quantization. To this end, we propose Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first work to learn deep quantization from weakly tagged images. Specifically, 1) we use word embeddings to represent the tags and enhance their semantic information based on a tag correlation graph. 2) To better preserve semantic information in quantization codes and reduce quantization error, we jointly learn semantics-preserving embeddings and supervised quantizer on hypersphere by employing a well-designed fusion layer and tailor-made loss functions. Extensive experiments show that WSDHQ can achieve state-of-art performance on weakly-supervised compact coding. Code is available at https://github.com/gimpong/AAAI21-WSDHQ.","sentences":["Deep quantization methods have shown high efficiency on large-scale image retrieval.","However, current models heavily rely on ground-truth information, hindering the application of quantization in label-hungry scenarios.","A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users.","Though such sketchy tags do not obviously reveal the labels, they actually contain useful semantic information for supervising deep quantization.","To this end, we propose Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first work to learn deep quantization from weakly tagged images.","Specifically, 1) we use word embeddings to represent the tags and enhance their semantic information based on a tag correlation graph.","2) To better preserve semantic information in quantization codes and reduce quantization error, we jointly learn semantics-preserving embeddings and supervised quantizer on hypersphere by employing a well-designed fusion layer and tailor-made loss functions.","Extensive experiments show that WSDHQ can achieve state-of-art performance on weakly-supervised compact coding.","Code is available at https://github.com/gimpong/AAAI21-WSDHQ."],"url":"http://arxiv.org/abs/2404.04998v1","category":"cs.CV"}
{"created":"2024-04-07 15:44:20","title":"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression","abstract":"The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.","sentences":["The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny.","Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations.","This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms.","Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts.","This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks.","We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content.","By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability.","Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications.","This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions."],"url":"http://arxiv.org/abs/2404.04997v1","category":"cs.LG"}
{"created":"2024-04-07 15:34:40","title":"Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM","abstract":"As an important pillar of underwater intelligence, Marine Animal Segmentation (MAS) involves segmenting animals within marine environments. Previous methods don't excel in extracting long-range contextual features and overlook the connectivity between discrete pixels. Recently, Segment Anything Model (SAM) offers a universal framework for general segmentation tasks. Unfortunately, trained with natural images, SAM does not obtain the prior knowledge from marine images. In addition, the single-position prompt of SAM is very insufficient for prior guidance. To address these issues, we propose a novel feature learning framework, named Dual-SAM for high-performance MAS. To this end, we first introduce a dual structure with SAM's paradigm to enhance feature learning of marine images. Then, we propose a Multi-level Coupled Prompt (MCP) strategy to instruct comprehensive underwater prior information, and enhance the multi-level features of SAM's encoder with adapters. Subsequently, we design a Dilated Fusion Attention Module (DFAM) to progressively integrate multi-level features from SAM's encoder. Finally, instead of directly predicting the masks of marine animals, we propose a Criss-Cross Connectivity Prediction (C$^3$P) paradigm to capture the inter-connectivity between discrete pixels. With dual decoders, it generates pseudo-labels and achieves mutual supervision for complementary feature representations, resulting in considerable improvements over previous techniques. Extensive experiments verify that our proposed method achieves state-of-the-art performances on five widely-used MAS datasets. The code is available at https://github.com/Drchip61/Dual_SAM.","sentences":["As an important pillar of underwater intelligence, Marine Animal Segmentation (MAS) involves segmenting animals within marine environments.","Previous methods don't excel in extracting long-range contextual features and overlook the connectivity between discrete pixels.","Recently, Segment Anything Model (SAM) offers a universal framework for general segmentation tasks.","Unfortunately, trained with natural images, SAM does not obtain the prior knowledge from marine images.","In addition, the single-position prompt of SAM is very insufficient for prior guidance.","To address these issues, we propose a novel feature learning framework, named Dual-SAM for high-performance MAS.","To this end, we first introduce a dual structure with SAM's paradigm to enhance feature learning of marine images.","Then, we propose a Multi-level Coupled Prompt (MCP) strategy to instruct comprehensive underwater prior information, and enhance the multi-level features of SAM's encoder with adapters.","Subsequently, we design a Dilated Fusion Attention Module (DFAM) to progressively integrate multi-level features from SAM's encoder.","Finally, instead of directly predicting the masks of marine animals, we propose a Criss-Cross Connectivity Prediction (C$^3$P) paradigm to capture the inter-connectivity between discrete pixels.","With dual decoders, it generates pseudo-labels and achieves mutual supervision for complementary feature representations, resulting in considerable improvements over previous techniques.","Extensive experiments verify that our proposed method achieves state-of-the-art performances on five widely-used MAS datasets.","The code is available at https://github.com/Drchip61/Dual_SAM."],"url":"http://arxiv.org/abs/2404.04996v1","category":"cs.CV"}
{"created":"2024-04-07 15:25:13","title":"OSS Malicious Package Analysis in the Wild","abstract":"The open-source software (OSS) ecosystem suffers from various security threats and risks, and malicious packages play a central role in software supply chain (SSC) attacks. Although malware research has a history of over thirty years, less attention has been paid to OSS malware. Its existing research has three limitations: a lack of high-quality datasets, malware diversity, and attack campaign context. In this paper, we first build and curate the largest dataset of 23,425 malicious packages from scattered online sources. We then propose a knowledge graph to represent the OSS malware corpus and conduct malicious package analysis in the wild. Our main findings include (1) it is essential to collect malicious packages from various online sources because there is little data overlap between different sources; (2) despite the sheer volume of SSC attack campaigns, many malicious packages are similar, and unknown/sophisticated attack behaviors have yet to emerge or be detected; (3) OSS malicious package has its distinct life cycle, denoted as {changing->release->detection->removal}, and slightly changing the package (different name) is a widespread attack manner; (4) while malicious packages often lack context about how and who released them, security reports disclose the information about corresponding SSC attack campaigns.","sentences":["The open-source software (OSS) ecosystem suffers from various security threats and risks, and malicious packages play a central role in software supply chain (SSC) attacks.","Although malware research has a history of over thirty years, less attention has been paid to OSS malware.","Its existing research has three limitations: a lack of high-quality datasets, malware diversity, and attack campaign context.","In this paper, we first build and curate the largest dataset of 23,425 malicious packages from scattered online sources.","We then propose a knowledge graph to represent the OSS malware corpus and conduct malicious package analysis in the wild.","Our main findings include (1) it is essential to collect malicious packages from various online sources because there is little data overlap between different sources; (2) despite the sheer volume of SSC attack campaigns, many malicious packages are similar, and unknown/sophisticated attack behaviors have yet to emerge or be detected; (3) OSS malicious package has its distinct life cycle, denoted as {changing->release->detection->removal}, and slightly changing the package (different name) is a widespread attack manner; (4) while malicious packages often lack context about how and who released them, security reports disclose the information about corresponding SSC attack campaigns."],"url":"http://arxiv.org/abs/2404.04991v1","category":"cs.CR"}
{"created":"2024-04-07 15:06:48","title":"Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection","abstract":"We introduce Dynamic Distinction Learning (DDL) for Video Anomaly Detection, a novel video anomaly detection methodology that combines pseudo-anomalies, dynamic anomaly weighting, and a distinction loss function to improve detection accuracy. By training on pseudo-anomalies, our approach adapts to the variability of normal and anomalous behaviors without fixed anomaly thresholds. Our model showcases superior performance on the Ped2, Avenue and ShanghaiTech datasets, where individual models are tailored for each scene. These achievements highlight DDL's effectiveness in advancing anomaly detection, offering a scalable and adaptable solution for video surveillance challenges.","sentences":["We introduce Dynamic Distinction Learning (DDL) for Video Anomaly Detection, a novel video anomaly detection methodology that combines pseudo-anomalies, dynamic anomaly weighting, and a distinction loss function to improve detection accuracy.","By training on pseudo-anomalies, our approach adapts to the variability of normal and anomalous behaviors without fixed anomaly thresholds.","Our model showcases superior performance on the Ped2, Avenue and ShanghaiTech datasets, where individual models are tailored for each scene.","These achievements highlight DDL's effectiveness in advancing anomaly detection, offering a scalable and adaptable solution for video surveillance challenges."],"url":"http://arxiv.org/abs/2404.04986v1","category":"cs.CV"}
{"created":"2024-04-07 15:03:46","title":"Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning","abstract":"The diagnosis of primary liver cancers (PLCs) can be challenging, especially on biopsies and for combined hepatocellular-cholangiocarcinoma (cHCC-CCA). We automatically classified PLCs on routine-stained biopsies using a weakly supervised learning method. Weak tumour/non-tumour annotations served as labels for training a Resnet18 neural network, and the network's last convolutional layer was used to extract new tumour tile features. Without knowledge of the precise labels of the malignancies, we then applied an unsupervised clustering algorithm. Our model identified specific features of hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (iCCA). Despite no specific features of cHCC-CCA being recognized, the identification of HCC and iCCA tiles within a slide could facilitate the diagnosis of primary liver cancers, particularly cHCC-CCA.   Method and results: 166 PLC biopsies were divided into training, internal and external validation sets: 90, 29 and 47 samples. Two liver pathologists reviewed each whole-slide hematein eosin saffron (HES)-stained image (WSI). After annotating the tumour/non-tumour areas, 256x256 pixel tiles were extracted from the WSIs and used to train a ResNet18. The network was used to extract new tile features. An unsupervised clustering algorithm was then applied to the new tile features. In a two-cluster model, Clusters 0 and 1 contained mainly HCC and iCCA histological features. The diagnostic agreement between the pathological diagnosis and the model predictions in the internal and external validation sets was 100% (11/11) and 96% (25/26) for HCC and 78% (7/9) and 87% (13/15) for iCCA, respectively. For cHCC-CCA, we observed a highly variable proportion of tiles from each cluster (Cluster 0: 5-97%; Cluster 1: 2-94%).","sentences":["The diagnosis of primary liver cancers (PLCs) can be challenging, especially on biopsies and for combined hepatocellular-cholangiocarcinoma (cHCC-CCA).","We automatically classified PLCs on routine-stained biopsies using a weakly supervised learning method.","Weak tumour/non-tumour annotations served as labels for training a Resnet18 neural network, and the network's last convolutional layer was used to extract new tumour tile features.","Without knowledge of the precise labels of the malignancies, we then applied an unsupervised clustering algorithm.","Our model identified specific features of hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (iCCA).","Despite no specific features of cHCC-CCA being recognized, the identification of HCC and iCCA tiles within a slide could facilitate the diagnosis of primary liver cancers, particularly cHCC-CCA.   Method and results: 166 PLC biopsies were divided into training, internal and external validation sets: 90, 29 and 47 samples.","Two liver pathologists reviewed each whole-slide hematein eosin saffron (HES)-stained image (WSI).","After annotating the tumour/non-tumour areas, 256x256 pixel tiles were extracted from the WSIs and used to train a ResNet18.","The network was used to extract new tile features.","An unsupervised clustering algorithm was then applied to the new tile features.","In a two-cluster model, Clusters 0 and 1 contained mainly HCC and iCCA histological features.","The diagnostic agreement between the pathological diagnosis and the model predictions in the internal and external validation sets was 100% (11/11) and 96% (25/26) for HCC and 78% (7/9) and 87% (13/15) for iCCA, respectively.","For cHCC-CCA, we observed a highly variable proportion of tiles from each cluster (Cluster 0: 5-97%; Cluster 1: 2-94%)."],"url":"http://arxiv.org/abs/2404.04983v1","category":"q-bio.TO"}
{"created":"2024-04-07 14:33:06","title":"Neural Network Modeling for Forecasting Tourism Demand in Stopi\u0107a Cave: A Serbian Cave Tourism Study","abstract":"For modeling the number of visits in Stopi\\'{c}a cave (Serbia) we consider the classical Auto-regressive Integrated Moving Average (ARIMA) model, Machine Learning (ML) method Support Vector Regression (SVR), and hybrid NeuralPropeth method which combines classical and ML concepts. The most accurate predictions were obtained with NeuralPropeth which includes the seasonal component and growing trend of time-series. In addition, non-linearity is modeled by shallow Neural Network (NN), and Google Trend is incorporated as an exogenous variable. Modeling tourist demand represents great importance for management structures and decision-makers due to its applicability in establishing sustainable tourism utilization strategies in environmentally vulnerable destinations such as caves. The data provided insights into the tourist demand in Stopi\\'{c}a cave and preliminary data for addressing the issues of carrying capacity within the most visited cave in Serbia.","sentences":["For modeling the number of visits in Stopi\\'{c}a cave (Serbia) we consider the classical Auto-regressive Integrated Moving Average (ARIMA) model, Machine Learning (ML) method Support Vector Regression (SVR), and hybrid NeuralPropeth method which combines classical and ML concepts.","The most accurate predictions were obtained with NeuralPropeth which includes the seasonal component and growing trend of time-series.","In addition, non-linearity is modeled by shallow Neural Network (NN), and Google Trend is incorporated as an exogenous variable.","Modeling tourist demand represents great importance for management structures and decision-makers due to its applicability in establishing sustainable tourism utilization strategies in environmentally vulnerable destinations such as caves.","The data provided insights into the tourist demand in Stopi\\'{c}a cave and preliminary data for addressing the issues of carrying capacity within the most visited cave in Serbia."],"url":"http://arxiv.org/abs/2404.04974v1","category":"econ.EM"}
{"created":"2024-04-07 14:19:22","title":"Temporal Generalization Estimation in Evolving Graphs","abstract":"Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve. We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time. To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., RNN) before deployment and use this model afterwards, but the estimation is far from satisfactory. In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution. Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through self-supervised graph reconstruction. In synthetic random graphs, we further refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance. Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving graphs. The ablation studies underscore the necessity of graph reconstruction. For example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction.","sentences":["Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve.","We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time.","To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., RNN) before deployment and use this model afterwards, but the estimation is far from satisfactory.","In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution.","Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through self-supervised graph reconstruction.","In synthetic random graphs, we further refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance.","Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving graphs.","The ablation studies underscore the necessity of graph reconstruction.","For example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction."],"url":"http://arxiv.org/abs/2404.04969v1","category":"cs.LG"}
{"created":"2024-04-07 13:58:41","title":"SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials","abstract":"Large Language Models (LLMs) are at the forefront of NLP achievements but fall short in dealing with shortcut learning, factual inconsistency, and vulnerability to adversarial inputs.These shortcomings are especially critical in medical contexts, where they can misrepresent actual model capabilities. Addressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for ClinicalTrials. Our contributions include the refined NLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials - Perturbed), designed to challenge LLMs with interventional and causal reasoning tasks, along with a comprehensive evaluation of methods and results for participant submissions. A total of 106 participants registered for the task contributing to over 1200 individual submissions and 25 system overview papers. This initiative aims to advance the robustness and applicability of NLI models in healthcare, ensuring safer and more dependable AI assistance in clinical decision-making. We anticipate that the dataset, models, and outcomes of this task can support future research in the field of biomedical NLI. The dataset, competition leaderboard, and website are publicly available.","sentences":["Large Language Models (LLMs) are at the forefront of NLP achievements but fall short in dealing with shortcut learning, factual inconsistency, and vulnerability to adversarial inputs.","These shortcomings are especially critical in medical contexts, where they can misrepresent actual model capabilities.","Addressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for ClinicalTrials.","Our contributions include the refined NLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials - Perturbed), designed to challenge LLMs with interventional and causal reasoning tasks, along with a comprehensive evaluation of methods and results for participant submissions.","A total of 106 participants registered for the task contributing to over 1200 individual submissions and 25 system overview papers.","This initiative aims to advance the robustness and applicability of NLI models in healthcare, ensuring safer and more dependable AI assistance in clinical decision-making.","We anticipate that the dataset, models, and outcomes of this task can support future research in the field of biomedical NLI.","The dataset, competition leaderboard, and website are publicly available."],"url":"http://arxiv.org/abs/2404.04963v1","category":"cs.CL"}
{"created":"2024-04-07 13:40:29","title":"PairAug: What Can Augmented Image-Text Pairs Do for Radiology?","abstract":"Current vision-language pre-training (VLP) methodologies predominantly depend on paired image-text datasets, a resource that is challenging to acquire in radiology due to privacy considerations and labelling complexities. Data augmentation provides a practical solution to overcome the issue of data scarcity, however, most augmentation methods exhibit a limited focus, prioritising either image or text augmentation exclusively. Acknowledging this limitation, our objective is to devise a framework capable of concurrently augmenting medical image and text data. We design a Pairwise Augmentation (PairAug) approach that contains an Inter-patient Augmentation (InterAug) branch and an Intra-patient Augmentation (IntraAug) branch. Specifically, the InterAug branch of our approach generates radiology images using synthesised yet plausible reports derived from a Large Language Model (LLM). The generated pairs can be considered a collection of new patient cases since they are artificially created and may not exist in the original dataset. In contrast, the IntraAug branch uses newly generated reports to manipulate images. This process allows us to create new paired data for each individual with diverse medical conditions. Our extensive experiments on various downstream tasks covering medical image classification zero-shot and fine-tuning analysis demonstrate that our PairAug, concurrently expanding both image and text data, substantially outperforms image-/text-only expansion baselines and advanced medical VLP baselines. Our code is released at \\url{https://github.com/YtongXie/PairAug}.","sentences":["Current vision-language pre-training (VLP) methodologies predominantly depend on paired image-text datasets, a resource that is challenging to acquire in radiology due to privacy considerations and labelling complexities.","Data augmentation provides a practical solution to overcome the issue of data scarcity, however, most augmentation methods exhibit a limited focus, prioritising either image or text augmentation exclusively.","Acknowledging this limitation, our objective is to devise a framework capable of concurrently augmenting medical image and text data.","We design a Pairwise Augmentation (PairAug) approach that contains an Inter-patient Augmentation (InterAug) branch and an Intra-patient Augmentation (IntraAug) branch.","Specifically, the InterAug branch of our approach generates radiology images using synthesised yet plausible reports derived from a Large Language Model (LLM).","The generated pairs can be considered a collection of new patient cases since they are artificially created and may not exist in the original dataset.","In contrast, the IntraAug branch uses newly generated reports to manipulate images.","This process allows us to create new paired data for each individual with diverse medical conditions.","Our extensive experiments on various downstream tasks covering medical image classification zero-shot and fine-tuning analysis demonstrate that our PairAug, concurrently expanding both image and text data, substantially outperforms image-/text-only expansion baselines and advanced medical VLP baselines.","Our code is released at \\url{https://github.com/YtongXie/PairAug}."],"url":"http://arxiv.org/abs/2404.04960v1","category":"cs.CV"}
{"created":"2024-04-07 12:57:46","title":"Gull: A Generative Multifunctional Audio Codec","abstract":"We introduce Gull, a generative multifunctional audio codec. Gull is a general purpose neural audio compression and decompression model which can be applied to a wide range of tasks and applications such as real-time communication, audio super-resolution, and codec language models. The key components of Gull include (1) universal-sample-rate modeling via subband modeling schemes motivated by recent progress in audio source separation, (2) gain-shape representations motivated by traditional audio codecs, (3) improved residual vector quantization modules for simpler training, (4) elastic decoder network that enables user-defined model size and complexity during inference time, (5) built-in ability for audio super-resolution without the increase of bitrate. We compare Gull with existing traditional and neural audio codecs and show that Gull is able to achieve on par or better performance across various sample rates, bitrates and model complexities in both subjective and objective evaluation metrics.","sentences":["We introduce Gull, a generative multifunctional audio codec.","Gull is a general purpose neural audio compression and decompression model which can be applied to a wide range of tasks and applications such as real-time communication, audio super-resolution, and codec language models.","The key components of Gull include (1) universal-sample-rate modeling via subband modeling schemes motivated by recent progress in audio source separation, (2) gain-shape representations motivated by traditional audio codecs, (3) improved residual vector quantization modules for simpler training, (4) elastic decoder network that enables user-defined model size and complexity during inference time, (5) built-in ability for audio super-resolution without the increase of bitrate.","We compare Gull with existing traditional and neural audio codecs and show that Gull is able to achieve on par or better performance across various sample rates, bitrates and model complexities in both subjective and objective evaluation metrics."],"url":"http://arxiv.org/abs/2404.04947v1","category":"eess.AS"}
{"created":"2024-04-07 12:40:37","title":"Chiplet Placement Order Exploration Based on Learning to Rank with Graph Representation","abstract":"Chiplet-based systems, integrating various silicon dies manufactured at different integrated circuit technology nodes on a carrier interposer, have garnered significant attention in recent years due to their cost-effectiveness and competitive performance. The widespread adoption of reinforcement learning as a sequential placement method has introduced a new challenge in determining the optimal placement order for each chiplet. The order in which chiplets are placed on the interposer influences the spatial resources available for earlier and later placed chiplets, making the placement results highly sensitive to the sequence of chiplet placement. To address these challenges, we propose a learning to rank approach with graph representation, building upon the reinforcement learning framework RLPlanner. This method aims to select the optimal chiplet placement order for each chiplet-based system. Experimental results demonstrate that compared to placement order obtained solely based on the descending order of the chiplet area and the number of interconnect wires between the chiplets, utilizing the placement order obtained from the learning to rank network leads to further improvements in system temperature and inter-chiplet wirelength. Specifically, applying the top-ranked placement order obtained from the learning to rank network results in a 10.05% reduction in total inter-chiplet wirelength and a 1.01% improvement in peak system temperature during the chiplet placement process.","sentences":["Chiplet-based systems, integrating various silicon dies manufactured at different integrated circuit technology nodes on a carrier interposer, have garnered significant attention in recent years due to their cost-effectiveness and competitive performance.","The widespread adoption of reinforcement learning as a sequential placement method has introduced a new challenge in determining the optimal placement order for each chiplet.","The order in which chiplets are placed on the interposer influences the spatial resources available for earlier and later placed chiplets, making the placement results highly sensitive to the sequence of chiplet placement.","To address these challenges, we propose a learning to rank approach with graph representation, building upon the reinforcement learning framework RLPlanner.","This method aims to select the optimal chiplet placement order for each chiplet-based system.","Experimental results demonstrate that compared to placement order obtained solely based on the descending order of the chiplet area and the number of interconnect wires between the chiplets, utilizing the placement order obtained from the learning to rank network leads to further improvements in system temperature and inter-chiplet wirelength.","Specifically, applying the top-ranked placement order obtained from the learning to rank network results in a 10.05% reduction in total inter-chiplet wirelength and a 1.01% improvement in peak system temperature during the chiplet placement process."],"url":"http://arxiv.org/abs/2404.04943v1","category":"cs.LG"}
{"created":"2024-04-07 12:21:02","title":"Optimizing Information Propagation for Blockchain-empowered Mobile AIGC: A Graph Attention Network Approach","abstract":"Artificial Intelligence-Generated Content (AIGC) is a rapidly evolving field that utilizes advanced AI algorithms to generate content. Through integration with mobile edge networks, mobile AIGC networks have gained significant attention, which can provide real-time customized and personalized AIGC services and products. Since blockchains can facilitate decentralized and transparent data management, AIGC products can be securely managed by blockchain to avoid tampering and plagiarization. However, the evolution of blockchain-empowered mobile AIGC is still in its nascent phase, grappling with challenges such as improving information propagation efficiency to enable blockchain-empowered mobile AIGC. In this paper, we design a Graph Attention Network (GAT)-based information propagation optimization framework for blockchain-empowered mobile AIGC. We first innovatively apply age of information as a data-freshness metric to measure information propagation efficiency in public blockchains. Considering that GATs possess the excellent ability to process graph-structured data, we utilize the GAT to obtain the optimal information propagation trajectory. Numerical results demonstrate that the proposed scheme exhibits the most outstanding information propagation efficiency compared with traditional routing mechanisms.","sentences":["Artificial Intelligence-Generated Content (AIGC) is a rapidly evolving field that utilizes advanced AI algorithms to generate content.","Through integration with mobile edge networks, mobile AIGC networks have gained significant attention, which can provide real-time customized and personalized AIGC services and products.","Since blockchains can facilitate decentralized and transparent data management, AIGC products can be securely managed by blockchain to avoid tampering and plagiarization.","However, the evolution of blockchain-empowered mobile AIGC is still in its nascent phase, grappling with challenges such as improving information propagation efficiency to enable blockchain-empowered mobile AIGC.","In this paper, we design a Graph Attention Network (GAT)-based information propagation optimization framework for blockchain-empowered mobile AIGC.","We first innovatively apply age of information as a data-freshness metric to measure information propagation efficiency in public blockchains.","Considering that GATs possess the excellent ability to process graph-structured data, we utilize the GAT to obtain the optimal information propagation trajectory.","Numerical results demonstrate that the proposed scheme exhibits the most outstanding information propagation efficiency compared with traditional routing mechanisms."],"url":"http://arxiv.org/abs/2404.04937v1","category":"cs.CR"}
{"created":"2024-04-07 12:10:04","title":"Towards Understanding the Influence of Reward Margin on Preference Model Performance","abstract":"Reinforcement Learning from Human Feedback (RLHF) is a widely used framework for the training of language models. However, the process of using RLHF to develop a language model that is well-aligned presents challenges, especially when it comes to optimizing the reward model. Our research has found that existing reward models, when trained using the traditional ranking objective based on human preference data, often struggle to effectively distinguish between responses that are more or less favorable in real-world scenarios. To bridge this gap, our study introduces a novel method to estimate the preference differences without the need for detailed, exhaustive labels from human annotators. Our experimental results provide empirical evidence that incorporating margin values into the training process significantly improves the effectiveness of reward models. This comparative analysis not only demonstrates the superiority of our approach in terms of reward prediction accuracy but also highlights its effectiveness in practical applications.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is a widely used framework for the training of language models.","However, the process of using RLHF to develop a language model that is well-aligned presents challenges, especially when it comes to optimizing the reward model.","Our research has found that existing reward models, when trained using the traditional ranking objective based on human preference data, often struggle to effectively distinguish between responses that are more or less favorable in real-world scenarios.","To bridge this gap, our study introduces a novel method to estimate the preference differences without the need for detailed, exhaustive labels from human annotators.","Our experimental results provide empirical evidence that incorporating margin values into the training process significantly improves the effectiveness of reward models.","This comparative analysis not only demonstrates the superiority of our approach in terms of reward prediction accuracy but also highlights its effectiveness in practical applications."],"url":"http://arxiv.org/abs/2404.04932v1","category":"cs.CL"}
{"created":"2024-04-07 12:05:47","title":"RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models","abstract":"Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions. In this paper, we introduce a novel Robotic Multimodal Perception-Planning (RoboMP$^2$) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines.","sentences":["Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains.","It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents.","However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions.","In this paper, we introduce a novel Robotic Multimodal Perception-Planning (RoboMP$^2$) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP).","Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization.","RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner.","Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines."],"url":"http://arxiv.org/abs/2404.04929v1","category":"cs.RO"}
{"created":"2024-04-07 11:52:44","title":"Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers","abstract":"Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs.","sentences":["Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks.","Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field.","To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature.","The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards.","We hope our work can provide the community with quick access and spur breakthrough research in MLLMs."],"url":"http://arxiv.org/abs/2404.04925v1","category":"cs.CL"}
{"created":"2024-04-07 11:48:07","title":"GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets","abstract":"Vision Transformers (ViTs) have achieved impressive results in large-scale image classification. However, when training from scratch on small datasets, there is still a significant performance gap between ViTs and Convolutional Neural Networks (CNNs), which is attributed to the lack of inductive bias. To address this issue, we propose a Graph-based Vision Transformer (GvT) that utilizes graph convolutional projection and graph-pooling. In each block, queries and keys are calculated through graph convolutional projection based on the spatial adjacency matrix, while dot-product attention is used in another graph convolution to generate values. When using more attention heads, the queries and keys become lower-dimensional, making their dot product an uninformative matching function. To overcome this low-rank bottleneck in attention heads, we employ talking-heads technology based on bilinear pooled features and sparse selection of attention tensors. This allows interaction among filtered attention scores and enables each attention mechanism to depend on all queries and keys. Additionally, we apply graph-pooling between two intermediate blocks to reduce the number of tokens and aggregate semantic information more effectively. Our experimental results show that GvT produces comparable or superior outcomes to deep convolutional networks and surpasses vision transformers without pre-training on large datasets. The code for our proposed model is publicly available on the website.","sentences":["Vision Transformers (ViTs) have achieved impressive results in large-scale image classification.","However, when training from scratch on small datasets, there is still a significant performance gap between ViTs and Convolutional Neural Networks (CNNs), which is attributed to the lack of inductive bias.","To address this issue, we propose a Graph-based Vision Transformer (GvT) that utilizes graph convolutional projection and graph-pooling.","In each block, queries and keys are calculated through graph convolutional projection based on the spatial adjacency matrix, while dot-product attention is used in another graph convolution to generate values.","When using more attention heads, the queries and keys become lower-dimensional, making their dot product an uninformative matching function.","To overcome this low-rank bottleneck in attention heads, we employ talking-heads technology based on bilinear pooled features and sparse selection of attention tensors.","This allows interaction among filtered attention scores and enables each attention mechanism to depend on all queries and keys.","Additionally, we apply graph-pooling between two intermediate blocks to reduce the number of tokens and aggregate semantic information more effectively.","Our experimental results show that GvT produces comparable or superior outcomes to deep convolutional networks and surpasses vision transformers without pre-training on large datasets.","The code for our proposed model is publicly available on the website."],"url":"http://arxiv.org/abs/2404.04924v1","category":"cs.CV"}
{"created":"2024-04-07 11:25:04","title":"Efficient Learnable Collaborative Attention for Single Image Super-Resolution","abstract":"Non-Local Attention (NLA) is a powerful technique for capturing long-range feature correlations in deep single image super-resolution (SR). However, NLA suffers from high computational complexity and memory consumption, as it requires aggregating all non-local feature information for each query response and recalculating the similarity weight distribution for different abstraction levels of features. To address these challenges, we propose a novel Learnable Collaborative Attention (LCoA) that introduces inductive bias into non-local modeling. Our LCoA consists of two components: Learnable Sparse Pattern (LSP) and Collaborative Attention (CoA). LSP uses the k-means clustering algorithm to dynamically adjust the sparse attention pattern of deep features, which reduces the number of non-local modeling rounds compared with existing sparse solutions. CoA leverages the sparse attention pattern and weights learned by LSP, and co-optimizes the similarity matrix across different abstraction levels, which avoids redundant similarity matrix calculations. The experimental results show that our LCoA can reduce the non-local modeling time by about 83% in the inference stage. In addition, we integrate our LCoA into a deep Learnable Collaborative Attention Network (LCoAN), which achieves competitive performance in terms of inference time, memory consumption, and reconstruction quality compared with other state-of-the-art SR methods.","sentences":["Non-Local Attention (NLA) is a powerful technique for capturing long-range feature correlations in deep single image super-resolution (SR).","However, NLA suffers from high computational complexity and memory consumption, as it requires aggregating all non-local feature information for each query response and recalculating the similarity weight distribution for different abstraction levels of features.","To address these challenges, we propose a novel Learnable Collaborative Attention (LCoA) that introduces inductive bias into non-local modeling.","Our LCoA consists of two components: Learnable Sparse Pattern (LSP) and Collaborative Attention (CoA).","LSP uses the k-means clustering algorithm to dynamically adjust the sparse attention pattern of deep features, which reduces the number of non-local modeling rounds compared with existing sparse solutions.","CoA leverages the sparse attention pattern and weights learned by LSP, and co-optimizes the similarity matrix across different abstraction levels, which avoids redundant similarity matrix calculations.","The experimental results show that our LCoA can reduce the non-local modeling time by about 83% in the inference stage.","In addition, we integrate our LCoA into a deep Learnable Collaborative Attention Network (LCoAN), which achieves competitive performance in terms of inference time, memory consumption, and reconstruction quality compared with other state-of-the-art SR methods."],"url":"http://arxiv.org/abs/2404.04922v1","category":"cs.CV"}
{"created":"2024-04-07 11:03:10","title":"Search for $\u03b7_c(2S)\\to 2(\u03c0^+\u03c0^-)$ and improved measurement of $\u03c7_{cJ}\\to 2(\u03c0^+\u03c0^-)$","abstract":"We search for the hadronic decay $\\eta_c(2S)\\to 2(\\pi^+\\pi^-)$ in the $\\psi(3686)\\to\\gamma\\eta_c(2S)$ radiative decay using $(27.12\\pm 0.14)\\times 10^8$ $\\psi(3686)$ events collected by the BESIII detector at the BEPCII collider. No significant signal is found, and the upper limit of $\\mathcal{B}[\\psi(3686)\\to\\gamma\\eta_c(2S)]\\mathcal{B}[\\eta_c(2S)\\to 2(\\pi^+\\pi^-)]$ is determined to be $0.78\\times 10^{-6}$ at the 90\\% confidence level. Using $\\psi(3686)\\to\\gamma\\chi_{cJ}$ transitions, we also measure the branching fractions of $\\mathcal{B}[\\chi_{cJ(J=0,1,2)}\\to 2(\\pi^+\\pi^-)]$, which are $\\mathcal{B}[\\chi_{c0}\\to 2(\\pi^+\\pi^-)]=(2.127\\pm 0.002~(\\mathrm{stat.})\\pm 0.101~(\\mathrm{syst.}))$\\%, $\\mathcal{B}[\\chi_{c1}\\to 2(\\pi^+\\pi^-)]=(0.685\\pm 0.001~(\\mathrm{stat.})\\pm 0.031~\\mathrm{syst.}))$\\%, and $\\mathcal{B}[\\chi_{c2}\\to 2(\\pi^+\\pi^-)]=(1.153\\pm 0.001~(\\mathrm{stat.})\\pm 0.063~(\\mathrm{syst.}))$\\%.","sentences":["We search for the hadronic decay $\\eta_c(2S)\\to 2(\\pi^+\\pi^-)$ in the $\\psi(3686)\\to\\gamma\\eta_c(2S)$ radiative decay using $(27.12\\pm 0.14)\\times 10^8$ $\\psi(3686)$ events collected by the BESIII detector at the BEPCII collider.","No significant signal is found, and the upper limit of $\\mathcal{B}[\\psi(3686)\\to\\gamma\\eta_c(2S)]\\mathcal{B}[\\eta_c(2S)\\to 2(\\pi^+\\pi^-)]$ is determined to be $0.78\\times 10^{-6}$ at the 90\\% confidence level.","Using $\\psi(3686)\\to\\gamma\\chi_{cJ}$ transitions, we also measure the branching fractions of $\\mathcal{B}[\\chi_{cJ(J=0,1,2)}\\to 2(\\pi^+\\pi^-)]$, which are $\\mathcal{B}[\\chi_{c0}\\to 2(\\pi^+\\pi^-)]=(2.127\\pm 0.002~(\\mathrm{stat.})\\pm","0.101~(\\mathrm{syst.}))$\\%, $\\mathcal{B}[\\chi_{c1}\\to 2(\\pi^+\\pi^-)]=(0.685\\pm 0.001~(\\mathrm{stat.})\\pm","0.031~\\mathrm{syst.}))$\\%, and $\\mathcal{B}[\\chi_{c2}\\to 2(\\pi^+\\pi^-)]=(1.153\\pm 0.001~(\\mathrm{stat.})\\pm","0.063~(\\mathrm{syst.}))$\\%."],"url":"http://arxiv.org/abs/2404.04917v1","category":"hep-ex"}
{"created":"2024-04-07 10:56:06","title":"Measurement of the $e^+e^- \\to \u03c0^+\u03c0^-\u03c0^0$ cross section in the energy range 0.62-3.50 GeV at Belle II","abstract":"We report a measurement of the $e^+e^- \\to \\pi^+\\pi^-\\pi^0$ cross section in the energy range from 0.62 to 3.50 GeV using an initial-state radiation technique. We use an $e^+e^-$ data sample corresponding to 191 $\\text{fb}^{-1}$ of integrated luminosity, collected at a center-of-mass energy at or near the $\\Upsilon{(4S)}$ resonance with the Belle II detector at the SuperKEKB collider. Signal yields are extracted by fitting the two-photon mass distribution in $e^+e^- \\to \\pi^+\\pi^-\\pi^0\\gamma$ events, which involve a $\\pi^0 \\to \\gamma\\gamma$ decay and an energetic photon radiated from the initial state. Signal efficiency corrections with an accuracy of 1.6% are obtained from several control data samples. The uncertainty on the cross section at the $\\omega$ and $\\phi$ resonances is dominated by the systematic uncertainty of 2.2%. The resulting cross sections in the 0.62-1.80 GeV energy range yield $ a_\\mu^{3\\pi} = [48.91 \\pm 0.23~(\\mathrm{stat}) \\pm 1.07~(\\mathrm{syst})] \\times 10^{-10} $ for the leading-order hadronic vacuum polarization contribution to the muon anomalous magnetic moment. This result differs by $2.5$ standard deviations from the most precise current determination.","sentences":["We report a measurement of the $e^+e^- \\to \\pi^+\\pi^-\\pi^0$ cross section in the energy range from 0.62 to 3.50 GeV using an initial-state radiation technique.","We use an $e^+e^-$ data sample corresponding to 191 $\\text{fb}^{-1}$ of integrated luminosity, collected at a center-of-mass energy at or near the $\\Upsilon{(4S)}$ resonance with the Belle II detector at the SuperKEKB collider.","Signal yields are extracted by fitting the two-photon mass distribution in $e^+e^- \\to \\pi^+\\pi^-\\pi^0\\gamma$ events, which involve a $\\pi^0 \\to \\gamma\\gamma$ decay and an energetic photon radiated from the initial state.","Signal efficiency corrections with an accuracy of 1.6% are obtained from several control data samples.","The uncertainty on the cross section at the $\\omega$ and $\\phi$ resonances is dominated by the systematic uncertainty of 2.2%.","The resulting cross sections in the 0.62-1.80 GeV energy range yield $ a_\\mu^{3\\pi} =","[48.91 \\pm 0.23~(\\mathrm{stat})","\\pm 1.07~(\\mathrm{syst})]","\\times 10^{-10} $ for the leading-order hadronic vacuum polarization contribution to the muon anomalous magnetic moment.","This result differs by $2.5$ standard deviations from the most precise current determination."],"url":"http://arxiv.org/abs/2404.04915v1","category":"hep-ex"}
{"created":"2024-04-07 10:28:01","title":"Dual-Camera Smooth Zoom on Mobile Phones","abstract":"When zooming between dual cameras on a mobile, noticeable jumps in geometric content and image color occur in the preview, inevitably affecting the user's zoom experience. In this work, we introduce a new task, ie, dual-camera smooth zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI) technique is a potential solution but struggles with ground-truth collection. To address the issue, we suggest a data factory solution where continuous virtual cameras are assembled to generate DCSZ data by rendering reconstructed 3D models of the scene. In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is introduced to construct a specific 3D model for each virtual camera. With the proposed data factory, we construct a synthetic dataset for DCSZ, and we utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom images without ground-truth for evaluation. Extensive experiments are conducted with multiple FI methods. The results show that the fine-tuned FI models achieve a significant performance improvement over the original ones on DCSZ task. The datasets, codes, and pre-trained models will be publicly available.","sentences":["When zooming between dual cameras on a mobile, noticeable jumps in geometric content and image color occur in the preview, inevitably affecting the user's zoom experience.","In this work, we introduce a new task, ie, dual-camera smooth zoom (DCSZ) to achieve a smooth zoom preview.","The frame interpolation (FI) technique is a potential solution but struggles with ground-truth collection.","To address the issue, we suggest a data factory solution where continuous virtual cameras are assembled to generate DCSZ data by rendering reconstructed 3D models of the scene.","In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is introduced to construct a specific 3D model for each virtual camera.","With the proposed data factory, we construct a synthetic dataset for DCSZ, and we utilize it to fine-tune FI models.","In addition, we collect real-world dual-zoom images without ground-truth for evaluation.","Extensive experiments are conducted with multiple FI methods.","The results show that the fine-tuned FI models achieve a significant performance improvement over the original ones on DCSZ task.","The datasets, codes, and pre-trained models will be publicly available."],"url":"http://arxiv.org/abs/2404.04908v1","category":"cs.CV"}
{"created":"2024-04-07 10:11:22","title":"Review for Handling Missing Data with special missing mechanism","abstract":"Missing data poses a significant challenge in data science, affecting decision-making processes and outcomes. Understanding what missing data is, how it occurs, and why it is crucial to handle it appropriately is paramount when working with real-world data, especially in tabular data, one of the most commonly used data types in the real world. Three missing mechanisms are defined in the literature: Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), each presenting unique challenges in imputation. Most existing work are focused on MCAR that is relatively easy to handle. The special missing mechanisms of MNAR and MAR are less explored and understood. This article reviews existing literature on handling missing values. It compares and contrasts existing methods in terms of their ability to handle different missing mechanisms and data types. It identifies research gap in the existing literature and lays out potential directions for future research in the field. The information in this review will help data analysts and researchers to adopt and promote good practices for handling missing data in real-world problems.","sentences":["Missing data poses a significant challenge in data science, affecting decision-making processes and outcomes.","Understanding what missing data is, how it occurs, and why it is crucial to handle it appropriately is paramount when working with real-world data, especially in tabular data, one of the most commonly used data types in the real world.","Three missing mechanisms are defined in the literature: Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), each presenting unique challenges in imputation.","Most existing work are focused on MCAR that is relatively easy to handle.","The special missing mechanisms of MNAR and MAR are less explored and understood.","This article reviews existing literature on handling missing values.","It compares and contrasts existing methods in terms of their ability to handle different missing mechanisms and data types.","It identifies research gap in the existing literature and lays out potential directions for future research in the field.","The information in this review will help data analysts and researchers to adopt and promote good practices for handling missing data in real-world problems."],"url":"http://arxiv.org/abs/2404.04905v1","category":"stat.ME"}
{"created":"2024-04-07 10:10:15","title":"Cross-Domain Audio Deepfake Detection: Dataset and Analysis","abstract":"Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.","sentences":["Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy.","Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance.","However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models.","In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models.","To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets.","Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively.","Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data.","Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research."],"url":"http://arxiv.org/abs/2404.04904v1","category":"cs.SD"}
{"created":"2024-04-07 10:07:56","title":"Online Learning under Haphazard Input Conditions: A Comprehensive Review and Analysis","abstract":"The domain of online learning has experienced multifaceted expansion owing to its prevalence in real-life applications. Nonetheless, this progression operates under the assumption that the input feature space of the streaming data remains constant. In this survey paper, we address the topic of online learning in the context of haphazard inputs, explicitly foregoing such an assumption. We discuss, classify, evaluate, and compare the methodologies that are adept at modeling haphazard inputs, additionally providing the corresponding code implementations and their carbon footprint. Moreover, we classify the datasets related to the field of haphazard inputs and introduce evaluation metrics specifically designed for datasets exhibiting imbalance. The code of each methodology can be found at https://github.com/Rohit102497/HaphazardInputsReview","sentences":["The domain of online learning has experienced multifaceted expansion owing to its prevalence in real-life applications.","Nonetheless, this progression operates under the assumption that the input feature space of the streaming data remains constant.","In this survey paper, we address the topic of online learning in the context of haphazard inputs, explicitly foregoing such an assumption.","We discuss, classify, evaluate, and compare the methodologies that are adept at modeling haphazard inputs, additionally providing the corresponding code implementations and their carbon footprint.","Moreover, we classify the datasets related to the field of haphazard inputs and introduce evaluation metrics specifically designed for datasets exhibiting imbalance.","The code of each methodology can be found at https://github.com/Rohit102497/HaphazardInputsReview"],"url":"http://arxiv.org/abs/2404.04903v1","category":"cs.LG"}
{"created":"2024-04-07 10:02:09","title":"AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications","abstract":"We introduce AI2Apps, a Visual Integrated Development Environment (Visual IDE) with full-cycle capabilities that accelerates developers to build deployable LLM-based AI agent Applications. This Visual IDE prioritizes both the Integrity of its development tools and the Visuality of its components, ensuring a smooth and efficient building experience.On one hand, AI2Apps integrates a comprehensive development toolkit ranging from a prototyping canvas and AI-assisted code editor to agent debugger, management system, and deployment tools all within a web-based graphical user interface. On the other hand, AI2Apps visualizes reusable front-end and back-end code as intuitive drag-and-drop components. Furthermore, a plugin system named AI2Apps Extension (AAE) is designed for Extensibility, showcasing how a new plugin with 20 components enables web agent to mimic human-like browsing behavior. Our case study demonstrates substantial efficiency improvements, with AI2Apps reducing token consumption and API calls when debugging a specific sophisticated multimodal agent by approximately 90% and 80%, respectively. The AI2Apps, including an online demo, open-source code, and a screencast video, is now publicly accessible.","sentences":["We introduce AI2Apps, a Visual Integrated Development Environment (Visual IDE) with full-cycle capabilities that accelerates developers to build deployable LLM-based AI agent Applications.","This Visual IDE prioritizes both the Integrity of its development tools and the Visuality of its components, ensuring a smooth and efficient building experience.","On one hand, AI2Apps integrates a comprehensive development toolkit ranging from a prototyping canvas and AI-assisted code editor to agent debugger, management system, and deployment tools all within a web-based graphical user interface.","On the other hand, AI2Apps visualizes reusable front-end and back-end code as intuitive drag-and-drop components.","Furthermore, a plugin system named AI2Apps Extension (AAE) is designed for Extensibility, showcasing how a new plugin with 20 components enables web agent to mimic human-like browsing behavior.","Our case study demonstrates substantial efficiency improvements, with AI2Apps reducing token consumption and API calls when debugging a specific sophisticated multimodal agent by approximately 90% and 80%, respectively.","The AI2Apps, including an online demo, open-source code, and a screencast video, is now publicly accessible."],"url":"http://arxiv.org/abs/2404.04902v1","category":"cs.AI"}
{"created":"2024-04-07 09:06:14","title":"PagPassGPT: Pattern Guided Password Guessing via Generative Pretrained Transformer","abstract":"Amidst the surge in deep learning-based password guessing models, challenges of generating high-quality passwords and reducing duplicate passwords persist. To address these challenges, we present PagPassGPT, a password guessing model constructed on Generative Pretrained Transformer (GPT). It can perform pattern guided guessing by incorporating pattern structure information as background knowledge, resulting in a significant increase in the hit rate. Furthermore, we propose D&C-GEN to reduce the repeat rate of generated passwords, which adopts the concept of a divide-and-conquer approach. The primary task of guessing passwords is recursively divided into non-overlapping subtasks. Each subtask inherits the knowledge from the parent task and predicts succeeding tokens. In comparison to the state-of-the-art model, our proposed scheme exhibits the capability to correctly guess 12% more passwords while producing 25% fewer duplicates.","sentences":["Amidst the surge in deep learning-based password guessing models, challenges of generating high-quality passwords and reducing duplicate passwords persist.","To address these challenges, we present PagPassGPT, a password guessing model constructed on Generative Pretrained Transformer (GPT).","It can perform pattern guided guessing by incorporating pattern structure information as background knowledge, resulting in a significant increase in the hit rate.","Furthermore, we propose D&C-GEN to reduce the repeat rate of generated passwords, which adopts the concept of a divide-and-conquer approach.","The primary task of guessing passwords is recursively divided into non-overlapping subtasks.","Each subtask inherits the knowledge from the parent task and predicts succeeding tokens.","In comparison to the state-of-the-art model, our proposed scheme exhibits the capability to correctly guess 12% more passwords while producing 25% fewer duplicates."],"url":"http://arxiv.org/abs/2404.04886v1","category":"cs.CR"}
{"created":"2024-04-07 08:51:31","title":"GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric Reconstruction Using Gaussian Splatting and NeRF","abstract":"We introduce a novel large-scale scene reconstruction benchmark that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2 encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth. This dataset offers a unique blend of urban and academic environments for advanced spatial analysis, covering more than 6.5 km2. We also provide detailed supplementary information on data collection protocols. Furthermore, we present an easy-to-follow pipeline to align the COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of U-Scene, which includes a detailed analysis across various novel viewpoints using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory results when applying geometric-based metrics, such as Chamfer distance. This leads to doubts about the reliability of current image-based measurement matrices and geometric extraction methods on Gaussian Splatting. We also make the dataset available on the following anonymous project page","sentences":["We introduce a novel large-scale scene reconstruction benchmark that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields, on our expansive GauU-Scene V2 dataset.","GauU-Scene V2 encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth.","This dataset offers a unique blend of urban and academic environments for advanced spatial analysis, covering more than 6.5 km2.","We also provide detailed supplementary information on data collection protocols.","Furthermore, we present an easy-to-follow pipeline to align the COLMAP sparse point cloud with the detailed LiDAR dataset.","Our evaluation of U-Scene, which includes a detailed analysis across various novel viewpoints using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory results when applying geometric-based metrics, such as Chamfer distance.","This leads to doubts about the reliability of current image-based measurement matrices and geometric extraction methods on Gaussian Splatting.","We also make the dataset available on the following anonymous project page"],"url":"http://arxiv.org/abs/2404.04880v1","category":"cs.CV"}
{"created":"2024-04-07 08:38:35","title":"Graph Neural Networks for Binary Programming","abstract":"This paper investigates a link between Graph Neural Networks (GNNs) and Binary Programming (BP) problems, laying the groundwork for GNNs to approximate solutions for these computationally challenging problems. By analyzing the sensitivity of BP problems, we are able to frame the solution of BP problems as a heterophilic node classification task. We then propose Binary-Programming GNN (BPGNN), an architecture that integrates graph representation learning techniques with BP-aware features to approximate BP solutions efficiently. Additionally, we introduce a self-supervised data generation mechanism, to enable efficient and tractable training data acquisition even for large-scale BP problems. Experimental evaluations of BPGNN across diverse BP problem sizes showcase its superior performance compared to exhaustive search and heuristic approaches. Finally, we discuss open challenges in the under-explored field of BP problems with GNNs.","sentences":["This paper investigates a link between Graph Neural Networks (GNNs) and Binary Programming (BP) problems, laying the groundwork for GNNs to approximate solutions for these computationally challenging problems.","By analyzing the sensitivity of BP problems, we are able to frame the solution of BP problems as a heterophilic node classification task.","We then propose Binary-Programming GNN (BPGNN), an architecture that integrates graph representation learning techniques with BP-aware features to approximate BP solutions efficiently.","Additionally, we introduce a self-supervised data generation mechanism, to enable efficient and tractable training data acquisition even for large-scale BP problems.","Experimental evaluations of BPGNN across diverse BP problem sizes showcase its superior performance compared to exhaustive search and heuristic approaches.","Finally, we discuss open challenges in the under-explored field of BP problems with GNNs."],"url":"http://arxiv.org/abs/2404.04874v1","category":"cs.LG"}
{"created":"2024-04-07 08:32:16","title":"Data Stream Sampling with Fuzzy Task Boundaries and Noisy Labels","abstract":"In the realm of continual learning, the presence of noisy labels within data streams represents a notable obstacle to model reliability and fairness. We focus on the data stream scenario outlined in pertinent literature, characterized by fuzzy task boundaries and noisy labels. To address this challenge, we introduce a novel and intuitive sampling method called Noisy Test Debiasing (NTD) to mitigate noisy labels in evolving data streams and establish a fair and robust continual learning algorithm. NTD is straightforward to implement, making it feasible across various scenarios. Our experiments benchmark four datasets, including two synthetic noise datasets (CIFAR10 and CIFAR100) and real-world noise datasets (mini-WebVision and Food-101N). The results validate the efficacy of NTD for online continual learning in scenarios with noisy labels in data streams. Compared to the previous leading approach, NTD achieves a training speedup enhancement over two times while maintaining or surpassing accuracy levels. Moreover, NTD utilizes less than one-fifth of the GPU memory resources compared to previous leading methods.","sentences":["In the realm of continual learning, the presence of noisy labels within data streams represents a notable obstacle to model reliability and fairness.","We focus on the data stream scenario outlined in pertinent literature, characterized by fuzzy task boundaries and noisy labels.","To address this challenge, we introduce a novel and intuitive sampling method called Noisy Test Debiasing (NTD) to mitigate noisy labels in evolving data streams and establish a fair and robust continual learning algorithm.","NTD is straightforward to implement, making it feasible across various scenarios.","Our experiments benchmark four datasets, including two synthetic noise datasets (CIFAR10 and CIFAR100) and real-world noise datasets (mini-WebVision and Food-101N).","The results validate the efficacy of NTD for online continual learning in scenarios with noisy labels in data streams.","Compared to the previous leading approach, NTD achieves a training speedup enhancement over two times while maintaining or surpassing accuracy levels.","Moreover, NTD utilizes less than one-fifth of the GPU memory resources compared to previous leading methods."],"url":"http://arxiv.org/abs/2404.04871v1","category":"cs.LG"}
{"created":"2024-04-07 08:31:12","title":"Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs","abstract":"The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a `pure-language' strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models.","sentences":["The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature.","However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a `pure-language' strategy.","In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens.","Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects.","1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models.","2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios.","The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA.","These performance metrics are comparable to the most advanced driving models."],"url":"http://arxiv.org/abs/2404.04869v1","category":"cs.RO"}
{"created":"2024-04-07 08:03:42","title":"Msmsfnet: a multi-stream and multi-scale fusion net for edge detection","abstract":"Edge detection is a long standing problem in computer vision. Recent deep learning based algorithms achieve state of-the-art performance in publicly available datasets. Despite the efficiency of these algorithms, their performance, however, relies heavily on the pretrained weights of the backbone network on the ImageNet dataset. This limits heavily the design space of deep learning based edge detectors. Whenever we want to devise a new model, we have to train this new model on the ImageNet dataset first, and then fine tune the model using the edge detection datasets. The comparison would be unfair otherwise. However, it is usually not feasible for many researchers to train a model on the ImageNet dataset due to the limited computation resources. In this work, we study the performance that can be achieved by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi scale fusion net (msmsfnet), for edge detection. We show in our experiments that by training all models from scratch to ensure the fairness of comparison, out model outperforms state-of-the art deep learning based edge detectors in three publicly available datasets.","sentences":["Edge detection is a long standing problem in computer vision.","Recent deep learning based algorithms achieve state of-the-art performance in publicly available datasets.","Despite the efficiency of these algorithms, their performance, however, relies heavily on the pretrained weights of the backbone network on the ImageNet dataset.","This limits heavily the design space of deep learning based edge detectors.","Whenever we want to devise a new model, we have to train this new model on the ImageNet dataset first, and then fine tune the model using the edge detection datasets.","The comparison would be unfair otherwise.","However, it is usually not feasible for many researchers to train a model on the ImageNet dataset due to the limited computation resources.","In this work, we study the performance that can be achieved by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi scale fusion net (msmsfnet), for edge detection.","We show in our experiments that by training all models from scratch to ensure the fairness of comparison, out model outperforms state-of-the art deep learning based edge detectors in three publicly available datasets."],"url":"http://arxiv.org/abs/2404.04856v1","category":"cs.CV"}
{"created":"2024-04-07 07:56:14","title":"Contextual Chart Generation for Cyber Deception","abstract":"Honeyfiles are security assets designed to attract and detect intruders on compromised systems. Honeyfiles are a type of honeypot that mimic real, sensitive documents, creating the illusion of the presence of valuable data. Interaction with a honeyfile reveals the presence of an intruder, and can provide insights into their goals and intentions. Their practical use, however, is limited by the time, cost and effort associated with manually creating realistic content. The introduction of large language models has made high-quality text generation accessible, but honeyfiles contain a variety of content including charts, tables and images. This content needs to be plausible and realistic, as well as semantically consistent both within honeyfiles and with the real documents they mimic, to successfully deceive an intruder.   In this paper, we focus on an important component of the honeyfile content generation problem: document charts. Charts are ubiquitous in corporate documents and are commonly used to communicate quantitative and scientific data. Existing image generation models, such as DALL-E, are rather prone to generating charts with incomprehensible text and unconvincing data. We take a multi-modal approach to this problem by combining two purpose-built generative models: a multitask Transformer and a specialized multi-head autoencoder. The Transformer generates realistic captions and plot text, while the autoencoder generates the underlying tabular data for the plot.   To advance the field of automated honeyplot generation, we also release a new document-chart dataset and propose a novel metric Keyword Semantic Matching (KSM). This metric measures the semantic consistency between keywords of a corpus and a smaller bag of words. Extensive experiments demonstrate excellent performance against multiple large language models, including ChatGPT and GPT4.","sentences":["Honeyfiles are security assets designed to attract and detect intruders on compromised systems.","Honeyfiles are a type of honeypot that mimic real, sensitive documents, creating the illusion of the presence of valuable data.","Interaction with a honeyfile reveals the presence of an intruder, and can provide insights into their goals and intentions.","Their practical use, however, is limited by the time, cost and effort associated with manually creating realistic content.","The introduction of large language models has made high-quality text generation accessible, but honeyfiles contain a variety of content including charts, tables and images.","This content needs to be plausible and realistic, as well as semantically consistent both within honeyfiles and with the real documents they mimic, to successfully deceive an intruder.   ","In this paper, we focus on an important component of the honeyfile content generation problem: document charts.","Charts are ubiquitous in corporate documents and are commonly used to communicate quantitative and scientific data.","Existing image generation models, such as DALL-E, are rather prone to generating charts with incomprehensible text and unconvincing data.","We take a multi-modal approach to this problem by combining two purpose-built generative models: a multitask Transformer and a specialized multi-head autoencoder.","The Transformer generates realistic captions and plot text, while the autoencoder generates the underlying tabular data for the plot.   ","To advance the field of automated honeyplot generation, we also release a new document-chart dataset and propose a novel metric Keyword Semantic Matching (KSM).","This metric measures the semantic consistency between keywords of a corpus and a smaller bag of words.","Extensive experiments demonstrate excellent performance against multiple large language models, including ChatGPT and GPT4."],"url":"http://arxiv.org/abs/2404.04854v1","category":"cs.LG"}
{"created":"2024-04-07 07:50:26","title":"EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation","abstract":"To align mobile robot navigation policies with user preferences through reinforcement learning from human feedback (RLHF), reliable and behavior-diverse user queries are required. However, deterministic policies fail to generate a variety of navigation trajectory suggestions for a given navigation task configuration. We introduce EnQuery, a query generation approach using an ensemble of policies that achieve behavioral diversity through a regularization term. For a given navigation task, EnQuery produces multiple navigation trajectory suggestions, thereby optimizing the efficiency of preference data collection with fewer queries. Our methodology demonstrates superior performance in aligning navigation policies with user preferences in low-query regimes, offering enhanced policy convergence from sparse preference queries. The evaluation is complemented with a novel explainability representation, capturing full scene navigation behavior of the mobile robot in a single plot.","sentences":["To align mobile robot navigation policies with user preferences through reinforcement learning from human feedback (RLHF), reliable and behavior-diverse user queries are required.","However, deterministic policies fail to generate a variety of navigation trajectory suggestions for a given navigation task configuration.","We introduce EnQuery, a query generation approach using an ensemble of policies that achieve behavioral diversity through a regularization term.","For a given navigation task, EnQuery produces multiple navigation trajectory suggestions, thereby optimizing the efficiency of preference data collection with fewer queries.","Our methodology demonstrates superior performance in aligning navigation policies with user preferences in low-query regimes, offering enhanced policy convergence from sparse preference queries.","The evaluation is complemented with a novel explainability representation, capturing full scene navigation behavior of the mobile robot in a single plot."],"url":"http://arxiv.org/abs/2404.04852v1","category":"cs.RO"}
{"created":"2024-04-07 07:42:12","title":"Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection","abstract":"Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts aimed at exploiting the models to generate malicious content. Existing jailbreak attacks can successfully deceive the LLMs, however they cannot deceive the human. This paper proposes a new type of jailbreak attacks which can deceive both the LLMs and human (i.e., security analyst). The key insight of our idea is borrowed from the social psychology - that is human are easily deceived if the lie is hidden in truth. Based on this insight, we proposed the logic-chain injection attacks to inject malicious intention into benign truth. Logic-chain injection attack firstly dissembles its malicious target into a chain of benign narrations, and then distribute narrations into a related benign article, with undoubted facts. In this way, newly generate prompt cannot only deceive the LLMs, but also deceive human.","sentences":["Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts aimed at exploiting the models to generate malicious content.","Existing jailbreak attacks can successfully deceive the LLMs, however they cannot deceive the human.","This paper proposes a new type of jailbreak attacks which can deceive both the LLMs and human (i.e., security analyst).","The key insight of our idea is borrowed from the social psychology - that is human are easily deceived if the lie is hidden in truth.","Based on this insight, we proposed the logic-chain injection attacks to inject malicious intention into benign truth.","Logic-chain injection attack firstly dissembles its malicious target into a chain of benign narrations, and then distribute narrations into a related benign article, with undoubted facts.","In this way, newly generate prompt cannot only deceive the LLMs, but also deceive human."],"url":"http://arxiv.org/abs/2404.04849v1","category":"cs.CR"}
{"created":"2024-04-07 07:42:04","title":"Task-Aware Encoder Control for Deep Video Compression","abstract":"Prior research on deep video compression (DVC) for machine tasks typically necessitates training a unique codec for each specific task, mandating a dedicated decoder per task. In contrast, traditional video codecs employ a flexible encoder controller, enabling the adaptation of a single codec to different tasks through mechanisms like mode prediction. Drawing inspiration from this, we introduce an innovative encoder controller for deep video compression for machines. This controller features a mode prediction and a Group of Pictures (GoP) selection module. Our approach centralizes control at the encoding stage, allowing for adaptable encoder adjustments across different tasks, such as detection and tracking, while maintaining compatibility with a standard pre-trained DVC decoder. Empirical evidence demonstrates that our method is applicable across multiple tasks with various existing pre-trained DVCs. Moreover, extensive experiments demonstrate that our method outperforms previous DVC by about 25% bitrate for different tasks, with only one pre-trained decoder.","sentences":["Prior research on deep video compression (DVC) for machine tasks typically necessitates training a unique codec for each specific task, mandating a dedicated decoder per task.","In contrast, traditional video codecs employ a flexible encoder controller, enabling the adaptation of a single codec to different tasks through mechanisms like mode prediction.","Drawing inspiration from this, we introduce an innovative encoder controller for deep video compression for machines.","This controller features a mode prediction and a Group of Pictures (GoP) selection module.","Our approach centralizes control at the encoding stage, allowing for adaptable encoder adjustments across different tasks, such as detection and tracking, while maintaining compatibility with a standard pre-trained DVC decoder.","Empirical evidence demonstrates that our method is applicable across multiple tasks with various existing pre-trained DVCs.","Moreover, extensive experiments demonstrate that our method outperforms previous DVC by about 25% bitrate for different tasks, with only one pre-trained decoder."],"url":"http://arxiv.org/abs/2404.04848v1","category":"eess.IV"}
{"created":"2024-04-07 07:34:49","title":"SLPL SHROOM at SemEval\\-2024 Task 06: A comprehensive study on models ability to detect hallucination","abstract":"Language models, particularly generative models, are susceptible to hallucinations, generating outputs that contradict factual knowledge or the source text. This study explores methods for detecting hallucinations in three SemEval-2024 Task 6 tasks: Machine Translation, Definition Modeling, and Paraphrase Generation. We evaluate two methods: semantic similarity between the generated text and factual references, and an ensemble of language models that judge each other's outputs. Our results show that semantic similarity achieves moderate accuracy and correlation scores in trial data, while the ensemble method offers insights into the complexities of hallucination detection but falls short of expectations. This work highlights the challenges of hallucination detection and underscores the need for further research in this critical area.","sentences":["Language models, particularly generative models, are susceptible to hallucinations, generating outputs that contradict factual knowledge or the source text.","This study explores methods for detecting hallucinations in three SemEval-2024 Task 6 tasks: Machine Translation, Definition Modeling, and Paraphrase Generation.","We evaluate two methods: semantic similarity between the generated text and factual references, and an ensemble of language models that judge each other's outputs.","Our results show that semantic similarity achieves moderate accuracy and correlation scores in trial data, while the ensemble method offers insights into the complexities of hallucination detection but falls short of expectations.","This work highlights the challenges of hallucination detection and underscores the need for further research in this critical area."],"url":"http://arxiv.org/abs/2404.04845v1","category":"cs.CL"}
{"created":"2024-04-07 07:29:07","title":"Self-Evolving Wireless Communications: A Novel Intelligence Trend for 6G and Beyond","abstract":"Wireless communication is rapidly evolving, and future wireless communications (6G and beyond) will be more heterogeneous, multi-layered, and complex, which poses challenges to traditional communications. Adaptive technologies in traditional communication systems respond to environmental changes by modifying system parameters and structures on their own and are not flexible and agile enough to satisfy requirements in future communications. To tackle these challenges, we propose a novel self-evolving communication framework, which consists of three layers: data layer, information layer, and knowledge layer. The first two layers allow communication systems to sense environments, fuse data, and generate a knowledge base for the knowledge layer. When dealing with a variety of application scenarios and environments, the generated knowledge is subsequently fed back to the first two layers for communication in practical application scenarios to obtain self-evolving ability and enhance the robustness of the system. In this paper, we first highlight the limitations of current adaptive communication systems and the need for intelligence, automation, and self-evolution in future wireless communications. We overview the development of self-evolving technologies and conceive the concept of self-evolving communications with its hypothetical architecture. To demonstrate the power of self-evolving modules, we compare the performances of a communication system with and without evolution. We then provide some potential techniques that enable self-evolving communications and challenges in implementing them.","sentences":["Wireless communication is rapidly evolving, and future wireless communications (6G and beyond) will be more heterogeneous, multi-layered, and complex, which poses challenges to traditional communications.","Adaptive technologies in traditional communication systems respond to environmental changes by modifying system parameters and structures on their own and are not flexible and agile enough to satisfy requirements in future communications.","To tackle these challenges, we propose a novel self-evolving communication framework, which consists of three layers: data layer, information layer, and knowledge layer.","The first two layers allow communication systems to sense environments, fuse data, and generate a knowledge base for the knowledge layer.","When dealing with a variety of application scenarios and environments, the generated knowledge is subsequently fed back to the first two layers for communication in practical application scenarios to obtain self-evolving ability and enhance the robustness of the system.","In this paper, we first highlight the limitations of current adaptive communication systems and the need for intelligence, automation, and self-evolution in future wireless communications.","We overview the development of self-evolving technologies and conceive the concept of self-evolving communications with its hypothetical architecture.","To demonstrate the power of self-evolving modules, we compare the performances of a communication system with and without evolution.","We then provide some potential techniques that enable self-evolving communications and challenges in implementing them."],"url":"http://arxiv.org/abs/2404.04844v1","category":"cs.ET"}
{"created":"2024-04-07 07:24:58","title":"AI for DevSecOps: A Landscape and Future Opportunities","abstract":"DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They reduce manual efforts, which can be integrated into DevOps to ensure uninterrupted delivery speed and align with the DevSecOps paradigm simultaneously. This paper seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevOps process and reviewed existing AI-driven security approaches. In RQ2, we discovered 15 challenges encountered by existing AI-driven security approaches and derived future research opportunities. Drawing insights from our findings, we discussed the state-of-the-art AI-driven security approaches, highlighted challenges in existing research, and proposed avenues for future opportunities.","sentences":["DevOps has emerged as one of the most rapidly evolving software development paradigms.","With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow.","However, integrating security into the DevOps workflow can impact agility and impede delivery speed.","Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security.","AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows.","They reduce manual efforts, which can be integrated into DevOps to ensure uninterrupted delivery speed and align with the DevSecOps paradigm simultaneously.","This paper seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes.","We analyzed 99 research papers spanning from 2017 to 2023.","Specifically, we address two key research questions (RQs).","In RQ1, we identified 12 security tasks associated with the DevOps process and reviewed existing AI-driven security approaches.","In RQ2, we discovered 15 challenges encountered by existing AI-driven security approaches and derived future research opportunities.","Drawing insights from our findings, we discussed the state-of-the-art AI-driven security approaches, highlighted challenges in existing research, and proposed avenues for future opportunities."],"url":"http://arxiv.org/abs/2404.04839v1","category":"cs.SE"}
{"created":"2024-04-07 06:56:51","title":"ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model","abstract":"With the development of the large-scale diffusion model, Artificial Intelligence Generated Content (AIGC) techniques are popular recently. However, how to truly make it serve our daily lives remains an open question. To this end, in this paper, we focus on employing AIGC techniques in one filed of E-commerce marketing, i.e., generating hyper-realistic advertising images for displaying user-specified shoes by human. Specifically, we propose a shoe-wearing system, called Shoe-Model, to generate plausible images of human legs interacting with the given shoes. It consists of three modules: (1) shoe wearable-area detection module (WD), (2) leg-pose synthesis module (LpS) and the final (3) shoe-wearing image generation module (SW). Them three are performed in ordered stages. Compared to baselines, our ShoeModel is shown to generalize better to different type of shoes and has ability of keeping the ID-consistency of the given shoes, as well as automatically producing reasonable interactions with human. Extensive experiments show the effectiveness of our proposed shoe-wearing system. Figure 1 shows the input and output examples of our ShoeModel.","sentences":["With the development of the large-scale diffusion model, Artificial Intelligence Generated Content (AIGC) techniques are popular recently.","However, how to truly make it serve our daily lives remains an open question.","To this end, in this paper, we focus on employing AIGC techniques in one filed of E-commerce marketing, i.e., generating hyper-realistic advertising images for displaying user-specified shoes by human.","Specifically, we propose a shoe-wearing system, called Shoe-Model, to generate plausible images of human legs interacting with the given shoes.","It consists of three modules: (1) shoe wearable-area detection module (WD), (2) leg-pose synthesis module (LpS) and the final (3) shoe-wearing image generation module (SW).","Them three are performed in ordered stages.","Compared to baselines, our ShoeModel is shown to generalize better to different type of shoes and has ability of keeping the ID-consistency of the given shoes, as well as automatically producing reasonable interactions with human.","Extensive experiments show the effectiveness of our proposed shoe-wearing system.","Figure 1 shows the input and output examples of our ShoeModel."],"url":"http://arxiv.org/abs/2404.04833v1","category":"cs.CV"}
{"created":"2024-04-07 06:24:47","title":"Gradient-based Design of Computational Granular Crystals","abstract":"There is growing interest in engineering unconventional computing devices that leverage the intrinsic dynamics of physical substrates to perform fast and energy-efficient computations. Granular metamaterials are one such substrate that has emerged as a promising platform for building wave-based information processing devices with the potential to integrate sensing, actuation, and computation. Their high-dimensional and nonlinear dynamics result in nontrivial and sometimes counter-intuitive wave responses that can be shaped by the material properties, geometry, and configuration of individual grains. Such highly tunable rich dynamics can be utilized for mechanical computing in special-purpose applications. However, there are currently no general frameworks for the inverse design of large-scale granular materials. Here, we build upon the similarity between the spatiotemporal dynamics of wave propagation in material and the computational dynamics of Recurrent Neural Networks to develop a gradient-based optimization framework for harmonically driven granular crystals. We showcase how our framework can be utilized to design basic logic gates where mechanical vibrations carry the information at predetermined frequencies. We compare our design methodology with classic gradient-free methods and find that our approach discovers higher-performing configurations with less computational effort. Our findings show that a gradient-based optimization method can greatly expand the design space of metamaterials and provide the opportunity to systematically traverse the parameter space to find materials with the desired functionalities.","sentences":["There is growing interest in engineering unconventional computing devices that leverage the intrinsic dynamics of physical substrates to perform fast and energy-efficient computations.","Granular metamaterials are one such substrate that has emerged as a promising platform for building wave-based information processing devices with the potential to integrate sensing, actuation, and computation.","Their high-dimensional and nonlinear dynamics result in nontrivial and sometimes counter-intuitive wave responses that can be shaped by the material properties, geometry, and configuration of individual grains.","Such highly tunable rich dynamics can be utilized for mechanical computing in special-purpose applications.","However, there are currently no general frameworks for the inverse design of large-scale granular materials.","Here, we build upon the similarity between the spatiotemporal dynamics of wave propagation in material and the computational dynamics of Recurrent Neural Networks to develop a gradient-based optimization framework for harmonically driven granular crystals.","We showcase how our framework can be utilized to design basic logic gates where mechanical vibrations carry the information at predetermined frequencies.","We compare our design methodology with classic gradient-free methods and find that our approach discovers higher-performing configurations with less computational effort.","Our findings show that a gradient-based optimization method can greatly expand the design space of metamaterials and provide the opportunity to systematically traverse the parameter space to find materials with the desired functionalities."],"url":"http://arxiv.org/abs/2404.04825v1","category":"cs.LG"}
{"created":"2024-04-07 06:23:18","title":"Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions","abstract":"Remaining Useful Life (RUL) predictions play vital role for asset planning and maintenance leading to many benefits to industries such as reduced downtime, low maintenance costs, etc. Although various efforts have been devoted to study this topic, most existing works are restricted for i.i.d conditions assuming the same condition of the training phase and the deployment phase. This paper proposes a solution to this problem where a mix-up domain adaptation (MDAN) is put forward. MDAN encompasses a three-staged mechanism where the mix-up strategy is not only performed to regularize the source and target domains but also applied to establish an intermediate mix-up domain where the source and target domains are aligned. The self-supervised learning strategy is implemented to prevent the supervision collapse problem. Rigorous evaluations have been performed where MDAN is compared to recently published works for dynamic RUL predictions. MDAN outperforms its counterparts with substantial margins in 12 out of 12 cases. In addition, MDAN is evaluated with the bearing machine dataset where it beats prior art with significant gaps in 8 of 12 cases. Source codes of MDAN are made publicly available in \\url{https://github.com/furqon3009/MDAN}.","sentences":["Remaining Useful Life (RUL) predictions play vital role for asset planning and maintenance leading to many benefits to industries such as reduced downtime, low maintenance costs, etc.","Although various efforts have been devoted to study this topic, most existing works are restricted for i.i.d conditions assuming the same condition of the training phase and the deployment phase.","This paper proposes a solution to this problem where a mix-up domain adaptation (MDAN) is put forward.","MDAN encompasses a three-staged mechanism where the mix-up strategy is not only performed to regularize the source and target domains but also applied to establish an intermediate mix-up domain where the source and target domains are aligned.","The self-supervised learning strategy is implemented to prevent the supervision collapse problem.","Rigorous evaluations have been performed where MDAN is compared to recently published works for dynamic RUL predictions.","MDAN outperforms its counterparts with substantial margins in 12 out of 12 cases.","In addition, MDAN is evaluated with the bearing machine dataset where it beats prior art with significant gaps in 8 of 12 cases.","Source codes of MDAN are made publicly available in \\url{https://github.com/furqon3009/MDAN}."],"url":"http://arxiv.org/abs/2404.04824v1","category":"cs.LG"}
{"created":"2024-04-07 06:05:25","title":"A Data-to-Product Multimodal Conceptual Framework to Achieve Automated Software Evolution for Context-rich Intelligent Applications","abstract":"While AI is extensively transforming Software Engineering (SE) fields, SE is still in need of a framework to overall consider all phases to facilitate Automated Software Evolution (ASEv), particularly for intelligent applications that are context-rich, instead of conquering each division independently. Its complexity comes from the intricacy of the intelligent applications, the heterogeneity of the data sources, and the constant changes in the context. This study proposes a conceptual framework for achieving automated software evolution, emphasizing the importance of multimodality learning. A Selective Sequential Scope Model (3S) model is developed based on the conceptual framework, and it can be used to categorize existing and future research when it covers different SE phases and multimodal learning tasks. This research is a preliminary step toward the blueprint of a higher-level ASEv. The proposed conceptual framework can act as a practical guideline for practitioners to prepare themselves for diving into this area. Although the study is about intelligent applications, the framework and analysis methods may be adapted for other types of software as AI brings more intelligence into their life cycles.","sentences":["While AI is extensively transforming Software Engineering (SE) fields, SE is still in need of a framework to overall consider all phases to facilitate Automated Software Evolution (ASEv), particularly for intelligent applications that are context-rich, instead of conquering each division independently.","Its complexity comes from the intricacy of the intelligent applications, the heterogeneity of the data sources, and the constant changes in the context.","This study proposes a conceptual framework for achieving automated software evolution, emphasizing the importance of multimodality learning.","A Selective Sequential Scope Model (3S) model is developed based on the conceptual framework, and it can be used to categorize existing and future research when it covers different SE phases and multimodal learning tasks.","This research is a preliminary step toward the blueprint of a higher-level ASEv.","The proposed conceptual framework can act as a practical guideline for practitioners to prepare themselves for diving into this area.","Although the study is about intelligent applications, the framework and analysis methods may be adapted for other types of software as AI brings more intelligence into their life cycles."],"url":"http://arxiv.org/abs/2404.04821v1","category":"cs.SE"}
{"created":"2024-04-07 05:56:42","title":"DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking","abstract":"Multimodal entity linking (MEL) aims to utilize multimodal information (usually textual and visual information) to link ambiguous mentions to unambiguous entities in knowledge base. Current methods facing main issues: (1)treating the entire image as input may contain redundant information. (2)the insufficient utilization of entity-related information, such as attributes in images. (3)semantic inconsistency between the entity in knowledge base and its representation. To this end, we propose DWE+ for multimodal entity linking. DWE+ could capture finer semantics and dynamically maintain semantic consistency with entities. This is achieved by three aspects: (a)we introduce a method for extracting fine-grained image features by partitioning the image into multiple local objects. Then, hierarchical contrastive learning is used to further align semantics between coarse-grained information(text and image) and fine-grained (mention and visual objects). (b)we explore ways to extract visual attributes from images to enhance fusion feature such as facial features and identity. (c)we leverage Wikipedia and ChatGPT to capture the entity representation, achieving semantic enrichment from both static and dynamic perspectives, which better reflects the real-world entity semantics. Experiments on Wikimel, Richpedia, and Wikidiverse datasets demonstrate the effectiveness of DWE+ in improving MEL performance. Specifically, we optimize these datasets and achieve state-of-the-art performance on the enhanced datasets. The code and enhanced datasets are released on https://github.com/season1blue/DWET","sentences":["Multimodal entity linking (MEL) aims to utilize multimodal information (usually textual and visual information) to link ambiguous mentions to unambiguous entities in knowledge base.","Current methods facing main issues: (1)treating the entire image as input may contain redundant information.","(2)the insufficient utilization of entity-related information, such as attributes in images.","(3)semantic inconsistency between the entity in knowledge base and its representation.","To this end, we propose DWE+ for multimodal entity linking.","DWE+ could capture finer semantics and dynamically maintain semantic consistency with entities.","This is achieved by three aspects: (a)we introduce a method for extracting fine-grained image features by partitioning the image into multiple local objects.","Then, hierarchical contrastive learning is used to further align semantics between coarse-grained information(text and image) and fine-grained (mention and visual objects).","(b)we explore ways to extract visual attributes from images to enhance fusion feature such as facial features and identity.","(c)we leverage Wikipedia and ChatGPT to capture the entity representation, achieving semantic enrichment from both static and dynamic perspectives, which better reflects the real-world entity semantics.","Experiments on Wikimel, Richpedia, and Wikidiverse datasets demonstrate the effectiveness of DWE+ in improving MEL performance.","Specifically, we optimize these datasets and achieve state-of-the-art performance on the enhanced datasets.","The code and enhanced datasets are released on https://github.com/season1blue/DWET"],"url":"http://arxiv.org/abs/2404.04818v1","category":"cs.AI"}
{"created":"2024-04-07 05:47:41","title":"Inference-Time Rule Eraser: Distilling and Removing Bias Rules to Mitigate Bias in Deployed Models","abstract":"Fairness is critical for artificial intelligence systems, especially for those deployed in high-stakes applications such as hiring and justice. Existing efforts toward fairness in machine learning fairness require retraining or fine-tuning the neural network weights to meet the fairness criteria. However, this is often not feasible in practice for regular model users due to the inability to access and modify model weights. In this paper, we propose a more flexible fairness paradigm, Inference-Time Rule Eraser, or simply Eraser, which considers the case where model weights can not be accessed and tackles fairness issues from the perspective of biased rules removal at inference-time. We first verified the feasibility of modifying the model output to wipe the biased rule through Bayesian analysis, and deduced Inference-Time Rule Eraser via subtracting the logarithmic value associated with unfair rules (i.e., the model's response to biased features) from the model's logits output as a means of removing biased rules. Moreover, we present a specific implementation of Rule Eraser that involves two stages: (1) limited queries are performed on the model with inaccessible weights to distill its biased rules into an additional patched model, and (2) during inference time, the biased rules already distilled into the patched model are excluded from the output of the original model, guided by the removal strategy outlined in Rule Eraser. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed Rule Eraser in addressing fairness concerns.","sentences":["Fairness is critical for artificial intelligence systems, especially for those deployed in high-stakes applications such as hiring and justice.","Existing efforts toward fairness in machine learning fairness require retraining or fine-tuning the neural network weights to meet the fairness criteria.","However, this is often not feasible in practice for regular model users due to the inability to access and modify model weights.","In this paper, we propose a more flexible fairness paradigm, Inference-Time Rule Eraser, or simply Eraser, which considers the case where model weights can not be accessed and tackles fairness issues from the perspective of biased rules removal at inference-time.","We first verified the feasibility of modifying the model output to wipe the biased rule through Bayesian analysis, and deduced Inference-Time Rule Eraser via subtracting the logarithmic value associated with unfair rules (i.e., the model's response to biased features) from the model's logits output as a means of removing biased rules.","Moreover, we present a specific implementation of Rule Eraser that involves two stages: (1) limited queries are performed on the model with inaccessible weights to distill its biased rules into an additional patched model, and (2) during inference time, the biased rules already distilled into the patched model are excluded from the output of the original model, guided by the removal strategy outlined in Rule Eraser.","Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed Rule Eraser in addressing fairness concerns."],"url":"http://arxiv.org/abs/2404.04814v1","category":"cs.LG"}
{"created":"2024-04-07 04:10:06","title":"Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving","abstract":"Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model's knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving.","sentences":["Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems.","However, these systems often struggle in low-light conditions, potentially compromising their performance and safety.","To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications.","Specifically, we employ a multi-condition controlled diffusion model.","LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead.","It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency.","Furthermore, to align the enhanced images with the detection model's knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning.","Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving."],"url":"http://arxiv.org/abs/2404.04804v1","category":"cs.CV"}
{"created":"2024-04-07 03:31:14","title":"Comparative Study of Sand Drawings in Oceania and Africa","abstract":"People typically consider only European mathematics as orthodox, often intentionally or unintentionally overlooking the existence of mathematics from non-European societies. Inspired by Maria Ascher's two well-known papers on sand drawings in Oceania and Africa, this paper focuses on the strong link between modern mathematics and the mathematics behind the sand drawings. Beginning with a comparison of the geography, history, and the cultural context of sand drawings in Oceania and Africa, we will examine shared geometric features of European graph theory and Indigenous sand drawings, including continuity, cyclicity, and symmetry. The paper will also delve into the origin of graph theory, exploring whether the famous European mathematician Leonhard Euler, who published his solution to the Konigsberg bridge problem in 1736, was the true inventor of graph theory. The potential for incorporating sand drawings into the school curriculum is highlighted at the end. Overall, this paper aims to make readers realise the importance of ethnomathematics studies and appreciate the intelligence of Indigenous people.","sentences":["People typically consider only European mathematics as orthodox, often intentionally or unintentionally overlooking the existence of mathematics from non-European societies.","Inspired by Maria Ascher's two well-known papers on sand drawings in Oceania and Africa, this paper focuses on the strong link between modern mathematics and the mathematics behind the sand drawings.","Beginning with a comparison of the geography, history, and the cultural context of sand drawings in Oceania and Africa, we will examine shared geometric features of European graph theory and Indigenous sand drawings, including continuity, cyclicity, and symmetry.","The paper will also delve into the origin of graph theory, exploring whether the famous European mathematician Leonhard Euler, who published his solution to the Konigsberg bridge problem in 1736, was the true inventor of graph theory.","The potential for incorporating sand drawings into the school curriculum is highlighted at the end.","Overall, this paper aims to make readers realise the importance of ethnomathematics studies and appreciate the intelligence of Indigenous people."],"url":"http://arxiv.org/abs/2404.04798v1","category":"math.HO"}
{"created":"2024-04-07 03:20:09","title":"Range Longest Increasing Subsequence and its Relatives: Beating Quadratic Barrier and Approaching Optimality","abstract":"In this work, we present a plethora of results for the range longest increasing subsequence problem (Range-LIS) and its variants. The input to Range-LIS is a sequence $\\mathcal{S}$ of $n$ real numbers and a collection $\\mathcal{Q}$ of $m$ query ranges and for each query in $\\mathcal{Q}$, the goal is to report the LIS of the sequence $\\mathcal{S}$ restricted to that query. Our two main results are for the following generalizations of the Range-LIS problem:   $\\bullet$ 2D Range Queries: In this variant of the Range-LIS problem, each query is a pair of ranges, one of indices and the other of values, and we provide an algorithm with running time $\\tilde{O}(mn^{1/2}+ n^{3/2} +k)$, where $k$ is the cumulative length of the $m$ output subsequences. This breaks the quadratic barrier of $\\tilde{O}(mn)$ when $m=\\Omega(\\sqrt{n})$. Previously, the only known result breaking the quadratic barrier was of Tiskin [SODA'10] which could only handle 1D range queries (i.e., each query was a range of indices) and also just outputted the length of the LIS (instead of reporting the subsequence achieving that length).   $\\bullet$ Colored Sequences: In this variant of the Range-LIS problem, each element in $\\mathcal{S}$ is colored and for each query in $\\mathcal{Q}$, the goal is to report a monochromatic LIS contained in the sequence $\\mathcal{S}$ restricted to that query. For 2D queries, we provide an algorithm for this colored version with running time $\\tilde{O}(mn^{2/3}+ n^{5/3} +k)$. Moreover, for 1D queries, we provide an improved algorithm with running time $\\tilde{O}(mn^{1/2}+ n^{3/2} +k)$. Thus, we again break the quadratic barrier of $\\tilde{O}(mn)$. Additionally, we prove that assuming the well-known Combinatorial Boolean Matrix Multiplication Hypothesis, that the runtime for 1D queries is essentially tight for combinatorial algorithms.","sentences":["In this work, we present a plethora of results for the range longest increasing subsequence problem (Range-LIS) and its variants.","The input to Range-LIS is a sequence $\\mathcal{S}$ of $n$ real numbers and a collection $\\mathcal{Q}$ of $m$ query ranges and for each query in $\\mathcal{Q}$, the goal is to report the LIS of the sequence $\\mathcal{S}$ restricted to that query.","Our two main results are for the following generalizations of the Range-LIS problem:   $\\bullet$ 2D Range Queries: In this variant of the Range-LIS problem, each query is a pair of ranges, one of indices and the other of values, and we provide an algorithm with running time $\\tilde{O}(mn^{1/2}+ n^{3/2} +k)$, where $k$ is the cumulative length of the $m$ output subsequences.","This breaks the quadratic barrier of $\\tilde{O}(mn)$ when $m=\\Omega(\\sqrt{n})$. Previously, the only known result breaking the quadratic barrier was of Tiskin","[SODA'10] which could only handle 1D range queries (i.e., each query was a range of indices) and also just outputted the length of the LIS (instead of reporting the subsequence achieving that length).   $\\bullet$ Colored Sequences: In this variant of the Range-LIS problem, each element in $\\mathcal{S}$ is colored and for each query in $\\mathcal{Q}$, the goal is to report a monochromatic LIS contained in the sequence $\\mathcal{S}$ restricted to that query.","For 2D queries, we provide an algorithm for this colored version with running time $\\tilde{O}(mn^{2/3}+ n^{5/3}","+k)$.","Moreover, for 1D queries, we provide an improved algorithm with running time $\\tilde{O}(mn^{1/2}+ n^{3/2} +k)$.","Thus, we again break the quadratic barrier of $\\tilde{O}(mn)$. Additionally, we prove that assuming the well-known Combinatorial Boolean Matrix Multiplication Hypothesis, that the runtime for 1D queries is essentially tight for combinatorial algorithms."],"url":"http://arxiv.org/abs/2404.04795v1","category":"cs.DS"}
{"created":"2024-04-07 01:54:33","title":"Fourier Transform-based Wavenumber Domain 3D Imaging in RIS-aided Communication Systems","abstract":"Radio imaging is rapidly gaining prominence in the design of future communication systems, with the potential to utilize reconfigurable intelligent surfaces (RISs) as imaging apertures. Although the sparsity of targets in three-dimensional (3D) space has led most research to adopt compressed sensing (CS)-based imaging algorithms, these often require substantial computational and memory burdens. Drawing inspiration from conventional Fourier transform (FT)-based imaging methods, our research seeks to accelerate radio imaging in RIS-aided communication systems. To begin, we introduce a two-stage wavenumber domain 3D imaging technique: first, we modify RIS phase shifts to recover the equivalent channel response from the user equipment to the RIS array, subsequently employing traditional FT-based wavenumber domain methods to produce target images. We also determine the diffraction resolution limits of the system through k-space analysis, taking into account factors including system bandwidth, transmission direction, operating frequency, and the angle subtended by the RIS. Addressing the challenge of limited pilots in communication systems, we unveil an innovative algorithm that merges the strengths of both FT- and CS-based techniques by substituting the expansive sensing matrix with FT-based operators. Our simulation outcomes confirm that our proposed FT-based methods achieve high-quality images while demanding few time, memory, and communication resources.","sentences":["Radio imaging is rapidly gaining prominence in the design of future communication systems, with the potential to utilize reconfigurable intelligent surfaces (RISs) as imaging apertures.","Although the sparsity of targets in three-dimensional (3D) space has led most research to adopt compressed sensing (CS)-based imaging algorithms, these often require substantial computational and memory burdens.","Drawing inspiration from conventional Fourier transform (FT)-based imaging methods, our research seeks to accelerate radio imaging in RIS-aided communication systems.","To begin, we introduce a two-stage wavenumber domain 3D imaging technique: first, we modify RIS phase shifts to recover the equivalent channel response from the user equipment to the RIS array, subsequently employing traditional FT-based wavenumber domain methods to produce target images.","We also determine the diffraction resolution limits of the system through k-space analysis, taking into account factors including system bandwidth, transmission direction, operating frequency, and the angle subtended by the RIS.","Addressing the challenge of limited pilots in communication systems, we unveil an innovative algorithm that merges the strengths of both FT- and CS-based techniques by substituting the expansive sensing matrix with FT-based operators.","Our simulation outcomes confirm that our proposed FT-based methods achieve high-quality images while demanding few time, memory, and communication resources."],"url":"http://arxiv.org/abs/2404.04783v1","category":"cs.IT"}
{"created":"2024-04-07 00:28:13","title":"GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling","abstract":"Multimodal event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge. It requires reasoning over the entire image, the depicted event, and the interactions between various objects participating in the event. Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains. In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task. Specifically, GenEARL comprises two stages of generative prompting with a frozen vision-language model (VLM) and a frozen large language model (LLM). First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image. Subsequently, a LLM is prompted with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role). We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets, respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset. The framework also allows flexible adaptation and generalization to unseen domains.","sentences":["Multimodal event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge.","It requires reasoning over the entire image, the depicted event, and the interactions between various objects participating in the event.","Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains.","In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task.","Specifically, GenEARL comprises two stages of generative prompting with a frozen vision-language model (VLM) and a frozen large language model (LLM).","First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image.","Subsequently, a LLM is prompted with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role).","We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets, respectively.","In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset.","The framework also allows flexible adaptation and generalization to unseen domains."],"url":"http://arxiv.org/abs/2404.04763v1","category":"cs.CV"}
{"created":"2024-04-06 23:09:26","title":"RIS in Cellular Networks -- Challenges and Issues","abstract":"Reconfigurable intelligent surface (RIS) has been suggested to be a key 6G feature and was suggested to be considered as a study-item in both 3GPP Releases 18 and 19. However, in both releases, it has been decided not to continue with it as a study-item, and to leave it for possible future specification. In this paper, we present the rationale for such a decision. Particularly, we demonstrate the practical issues which may affect the feasibility or usefulness of RIS in cellular networks, and present open problems to be addressed before RIS can be used in practice. Moreover, we compare the performance of RIS with network-controlled repeater, the node with the most similar characteristics to RIS and which has been standardized in 3GPP Release 18. Finally, different simulations are presented to evaluate the performance of RIS-assisted networks.","sentences":["Reconfigurable intelligent surface (RIS) has been suggested to be a key 6G feature and was suggested to be considered as a study-item in both 3GPP Releases 18 and 19.","However, in both releases, it has been decided not to continue with it as a study-item, and to leave it for possible future specification.","In this paper, we present the rationale for such a decision.","Particularly, we demonstrate the practical issues which may affect the feasibility or usefulness of RIS in cellular networks, and present open problems to be addressed before RIS can be used in practice.","Moreover, we compare the performance of RIS with network-controlled repeater, the node with the most similar characteristics to RIS and which has been standardized in 3GPP Release 18.","Finally, different simulations are presented to evaluate the performance of RIS-assisted networks."],"url":"http://arxiv.org/abs/2404.04753v1","category":"cs.NI"}
{"created":"2024-04-06 22:34:07","title":"Challenges Faced by Large Language Models in Solving Multi-Agent Flocking","abstract":"Flocking is a behavior where multiple agents in a system attempt to stay close to each other while avoiding collision and maintaining a desired formation. This is observed in the natural world and has applications in robotics, including natural disaster search and rescue, wild animal tracking, and perimeter surveillance and patrol. Recently, large language models (LLMs) have displayed an impressive ability to solve various collaboration tasks as individual decision-makers. Solving multi-agent flocking with LLMs would demonstrate their usefulness in situations requiring spatial and decentralized decision-making. Yet, when LLM-powered agents are tasked with implementing multi-agent flocking, they fall short of the desired behavior. After extensive testing, we find that agents with LLMs as individual decision-makers typically opt to converge on the average of their initial positions or diverge from each other. After breaking the problem down, we discover that LLMs cannot understand maintaining a shape or keeping a distance in a meaningful way. Solving multi-agent flocking with LLMs would enhance their ability to understand collaborative spatial reasoning and lay a foundation for addressing more complex multi-agent tasks. This paper discusses the challenges LLMs face in multi-agent flocking and suggests areas for future improvement and research.","sentences":["Flocking is a behavior where multiple agents in a system attempt to stay close to each other while avoiding collision and maintaining a desired formation.","This is observed in the natural world and has applications in robotics, including natural disaster search and rescue, wild animal tracking, and perimeter surveillance and patrol.","Recently, large language models (LLMs) have displayed an impressive ability to solve various collaboration tasks as individual decision-makers.","Solving multi-agent flocking with LLMs would demonstrate their usefulness in situations requiring spatial and decentralized decision-making.","Yet, when LLM-powered agents are tasked with implementing multi-agent flocking, they fall short of the desired behavior.","After extensive testing, we find that agents with LLMs as individual decision-makers typically opt to converge on the average of their initial positions or diverge from each other.","After breaking the problem down, we discover that LLMs cannot understand maintaining a shape or keeping a distance in a meaningful way.","Solving multi-agent flocking with LLMs would enhance their ability to understand collaborative spatial reasoning and lay a foundation for addressing more complex multi-agent tasks.","This paper discusses the challenges LLMs face in multi-agent flocking and suggests areas for future improvement and research."],"url":"http://arxiv.org/abs/2404.04752v1","category":"cs.AI"}
{"created":"2024-04-06 22:18:31","title":"Now, Later, and Lasting: Ten Priorities for AI Research, Policy, and Practice","abstract":"Advances in artificial intelligence (AI) will transform many aspects of our lives and society, bringing immense opportunities but also posing significant risks and challenges. The next several decades may well be a turning point for humanity, comparable to the industrial revolution. We write to share a set of recommendations for moving forward from the perspective of the founder and leaders of the One Hundred Year Study on AI. Launched a decade ago, the project is committed to a perpetual series of studies by multidisciplinary experts to evaluate the immediate, longer-term, and far-reaching effects of AI on people and society, and to make recommendations about AI research, policy, and practice. As we witness new capabilities emerging from neural models, it is crucial that we engage in efforts to advance our scientific understanding of these models and their behaviors. We must address the impact of AI on people and society through technical, social, and sociotechnical lenses, incorporating insights from a diverse range of experts including voices from engineering, social, behavioral, and economic disciplines. By fostering dialogue, collaboration, and action among various stakeholders, we can strategically guide the development and deployment of AI in ways that maximize its potential for contributing to human flourishing. Despite the growing divide in the field between focusing on short-term versus long-term implications, we think both are of critical importance. As Alan Turing, one of the pioneers of AI, wrote in 1950, \"We can only see a short distance ahead, but we can see plenty there that needs to be done.\" We offer ten recommendations for action that collectively address both the short- and long-term potential impacts of AI technologies.","sentences":["Advances in artificial intelligence (AI) will transform many aspects of our lives and society, bringing immense opportunities but also posing significant risks and challenges.","The next several decades may well be a turning point for humanity, comparable to the industrial revolution.","We write to share a set of recommendations for moving forward from the perspective of the founder and leaders of the One Hundred Year Study on AI.","Launched a decade ago, the project is committed to a perpetual series of studies by multidisciplinary experts to evaluate the immediate, longer-term, and far-reaching effects of AI on people and society, and to make recommendations about AI research, policy, and practice.","As we witness new capabilities emerging from neural models, it is crucial that we engage in efforts to advance our scientific understanding of these models and their behaviors.","We must address the impact of AI on people and society through technical, social, and sociotechnical lenses, incorporating insights from a diverse range of experts including voices from engineering, social, behavioral, and economic disciplines.","By fostering dialogue, collaboration, and action among various stakeholders, we can strategically guide the development and deployment of AI in ways that maximize its potential for contributing to human flourishing.","Despite the growing divide in the field between focusing on short-term versus long-term implications, we think both are of critical importance.","As Alan Turing, one of the pioneers of AI, wrote in 1950, \"We can only see a short distance ahead, but we can see plenty there that needs to be done.\"","We offer ten recommendations for action that collectively address both the short- and long-term potential impacts of AI technologies."],"url":"http://arxiv.org/abs/2404.04750v1","category":"cs.CY"}
{"created":"2024-04-06 22:08:09","title":"Adapting Multi-objectivized Software Configuration Tuning","abstract":"When tuning software configuration for better performance (e.g., latency or throughput), an important issue that many optimizers face is the presence of local optimum traps, compounded by a highly rugged configuration landscape and expensive measurements. To mitigate these issues, a recent effort has shifted to focus on the level of optimization model (called meta multi-objectivization or MMO) instead of designing better optimizers as in traditional methods. This is done by using an auxiliary performance objective, together with the target performance objective, to help the search jump out of local optima. While effective, MMO needs a fixed weight to balance the two objectives-a parameter that has been found to be crucial as there is a large deviation of the performance between the best and the other settings. However, given the variety of configurable software systems, the \"sweet spot\" of the weight can vary dramatically in different cases and it is not possible to find the right setting without time-consuming trial and error. In this paper, we seek to overcome this significant shortcoming of MMO by proposing a weight adaptation method, dubbed AdMMO. Our key idea is to adaptively adjust the weight at the right time during tuning, such that a good proportion of the nondominated configurations can be maintained. Moreover, we design a partial duplicate retention mechanism to handle the issue of too many duplicate configurations without losing the rich information provided by the \"good\" duplicates.   Experiments on several real-world systems, objectives, and budgets show that, for 71% of the cases, AdMMO is considerably superior to MMO and a wide range of state-of-the-art optimizers while achieving generally better efficiency with the best speedup between 2.2x and 20x.","sentences":["When tuning software configuration for better performance (e.g., latency or throughput), an important issue that many optimizers face is the presence of local optimum traps, compounded by a highly rugged configuration landscape and expensive measurements.","To mitigate these issues, a recent effort has shifted to focus on the level of optimization model (called meta multi-objectivization or MMO) instead of designing better optimizers as in traditional methods.","This is done by using an auxiliary performance objective, together with the target performance objective, to help the search jump out of local optima.","While effective, MMO needs a fixed weight to balance the two objectives-a parameter that has been found to be crucial as there is a large deviation of the performance between the best and the other settings.","However, given the variety of configurable software systems, the \"sweet spot\" of the weight can vary dramatically in different cases and it is not possible to find the right setting without time-consuming trial and error.","In this paper, we seek to overcome this significant shortcoming of MMO by proposing a weight adaptation method, dubbed AdMMO.","Our key idea is to adaptively adjust the weight at the right time during tuning, such that a good proportion of the nondominated configurations can be maintained.","Moreover, we design a partial duplicate retention mechanism to handle the issue of too many duplicate configurations without losing the rich information provided by the \"good\" duplicates.   ","Experiments on several real-world systems, objectives, and budgets show that, for 71% of the cases, AdMMO is considerably superior to MMO and a wide range of state-of-the-art optimizers while achieving generally better efficiency with the best speedup between 2.2x and 20x."],"url":"http://arxiv.org/abs/2404.04744v1","category":"cs.SE"}
{"created":"2024-04-06 21:39:49","title":"ProtoAL: Interpretable Deep Active Learning with prototypes for medical imaging","abstract":"The adoption of Deep Learning algorithms in the medical imaging field is a prominent area of research, with high potential for advancing AI-based Computer-aided diagnosis (AI-CAD) solutions. However, current solutions face challenges due to a lack of interpretability features and high data demands, prompting recent efforts to address these issues. In this study, we propose the ProtoAL method, where we integrate an interpretable DL model into the Deep Active Learning (DAL) framework. This approach aims to address both challenges by focusing on the medical imaging context and utilizing an inherently interpretable model based on prototypes. We evaluated ProtoAL on the Messidor dataset, achieving an area under the precision-recall curve of 0.79 while utilizing only 76.54\\% of the available labeled data. These capabilities can enhances the practical usability of a DL model in the medical field, providing a means of trust calibration in domain experts and a suitable solution for learning in the data scarcity context often found.","sentences":["The adoption of Deep Learning algorithms in the medical imaging field is a prominent area of research, with high potential for advancing AI-based Computer-aided diagnosis (AI-CAD) solutions.","However, current solutions face challenges due to a lack of interpretability features and high data demands, prompting recent efforts to address these issues.","In this study, we propose the ProtoAL method, where we integrate an interpretable DL model into the Deep Active Learning (DAL) framework.","This approach aims to address both challenges by focusing on the medical imaging context and utilizing an inherently interpretable model based on prototypes.","We evaluated ProtoAL on the Messidor dataset, achieving an area under the precision-recall curve of 0.79 while utilizing only 76.54\\% of the available labeled data.","These capabilities can enhances the practical usability of a DL model in the medical field, providing a means of trust calibration in domain experts and a suitable solution for learning in the data scarcity context often found."],"url":"http://arxiv.org/abs/2404.04736v1","category":"cs.CV"}
{"created":"2024-04-06 21:39:01","title":"MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems","abstract":"Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in \\textbf{advanced mathematical problems requiring complex, multi-step logical reasoning}. To enhance their inferential capabilities, current research has delved into \\textit{prompting engineering}, exemplified by methodologies such as the Tree of Thought and Graph of Thought. Nonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability. In response to these limitations, this paper introduces the \\textit{Multi-Agent System for conditional Mining} (\\textbf{MACM}) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\\mathbf{54.68\\%} \\text{ to } \\mathbf{76.73\\%}$. The code is available in \\url{https://github.com/bin123apple/MACM}.","sentences":["Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries.","Despite these advancements, their performance substantially declines in \\textbf{advanced mathematical problems requiring complex, multi-step logical reasoning}.","To enhance their inferential capabilities, current research has delved into \\textit{prompting engineering}, exemplified by methodologies such as the Tree of Thought and Graph of Thought.","Nonetheless, these existing approaches encounter two significant limitations.","Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained.","Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability.","In response to these limitations, this paper introduces the \\textit{Multi-Agent System for conditional Mining} (\\textbf{MACM}) prompting method.","It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts.","With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\\mathbf{54.68\\%} \\text{ to } \\mathbf{76.73\\%}$. The code is available in \\url{https://github.com/bin123apple/MACM}."],"url":"http://arxiv.org/abs/2404.04735v1","category":"cs.AI"}
{"created":"2024-04-06 20:42:46","title":"Navigating the Landscape of Hint Generation Research: From the Past to the Future","abstract":"Digital education has gained popularity in the last decade, especially after the COVID-19 pandemic. With the improving capabilities of large language models to reason and communicate with users, envisioning intelligent tutoring systems (ITSs) that can facilitate self-learning is not very far-fetched. One integral component to fulfill this vision is the ability to give accurate and effective feedback via hints to scaffold the learning process. In this survey article, we present a comprehensive review of prior research on hint generation, aiming to bridge the gap between research in education and cognitive science, and research in AI and Natural Language Processing. Informed by our findings, we propose a formal definition of the hint generation task, and discuss the roadmap of building an effective hint generation system aligned with the formal definition, including open challenges, future directions and ethical considerations.","sentences":["Digital education has gained popularity in the last decade, especially after the COVID-19 pandemic.","With the improving capabilities of large language models to reason and communicate with users, envisioning intelligent tutoring systems (ITSs) that can facilitate self-learning is not very far-fetched.","One integral component to fulfill this vision is the ability to give accurate and effective feedback via hints to scaffold the learning process.","In this survey article, we present a comprehensive review of prior research on hint generation, aiming to bridge the gap between research in education and cognitive science, and research in AI and Natural Language Processing.","Informed by our findings, we propose a formal definition of the hint generation task, and discuss the roadmap of building an effective hint generation system aligned with the formal definition, including open challenges, future directions and ethical considerations."],"url":"http://arxiv.org/abs/2404.04728v1","category":"cs.CL"}
{"created":"2024-04-06 20:20:24","title":"Does nematic order allow groups of elongated cells to sense electric fields better?","abstract":"Collective response to external directional cues like electric fields plays a pivotal role in processes such as tissue development, regeneration, and wound healing. In this study we focus on the impact of anisotropy in cell shape and local cell alignment on the collective response to electric fields. We model elongated cells that have a different accuracy sensing the field depending on their orientation with respect to the field. Elongated cells often line up with their long axes in the same direction - \"nematic order\" - does this help the group of cells sense the field more accurately? We use simulations of a simple model to show that if cells orient themselves perpendicular to their average velocity, alignment of a cell's long axis to its nearest neighbors' orientation can enhance the directional response to electric fields. However, for cells to benefit from aligning, their accuracy of sensing must be strongly dependent on cell orientation. We also show that cell-cell adhesion modulates the accuracy of cells in the group.","sentences":["Collective response to external directional cues like electric fields plays a pivotal role in processes such as tissue development, regeneration, and wound healing.","In this study we focus on the impact of anisotropy in cell shape and local cell alignment on the collective response to electric fields.","We model elongated cells that have a different accuracy sensing the field depending on their orientation with respect to the field.","Elongated cells often line up with their long axes in the same direction - \"nematic order\" - does this help the group of cells sense the field more accurately?","We use simulations of a simple model to show that if cells orient themselves perpendicular to their average velocity, alignment of a cell's long axis to its nearest neighbors' orientation can enhance the directional response to electric fields.","However, for cells to benefit from aligning, their accuracy of sensing must be strongly dependent on cell orientation.","We also show that cell-cell adhesion modulates the accuracy of cells in the group."],"url":"http://arxiv.org/abs/2404.04723v1","category":"q-bio.CB"}
{"created":"2024-04-06 19:42:25","title":"Interpretable Multimodal Learning for Cardiovascular Hemodynamics Assessment","abstract":"Pulmonary Arterial Wedge Pressure (PAWP) is an essential cardiovascular hemodynamics marker to detect heart failure. In clinical practice, Right Heart Catheterization is considered a gold standard for assessing cardiac hemodynamics while non-invasive methods are often needed to screen high-risk patients from a large population. In this paper, we propose a multimodal learning pipeline to predict PAWP marker. We utilize complementary information from Cardiac Magnetic Resonance Imaging (CMR) scans (short-axis and four-chamber) and Electronic Health Records (EHRs). We extract spatio-temporal features from CMR scans using tensor-based learning. We propose a graph attention network to select important EHR features for prediction, where we model subjects as graph nodes and feature relationships as graph edges using the attention mechanism. We design four feature fusion strategies: early, intermediate, late, and hybrid fusion. With a linear classifier and linear fusion strategies, our pipeline is interpretable. We validate our pipeline on a large dataset of $2,641$ subjects from our ASPIRE registry. The comparative study against state-of-the-art methods confirms the superiority of our pipeline. The decision curve analysis further validates that our pipeline can be applied to screen a large population. The code is available at https://github.com/prasunc/hemodynamics.","sentences":["Pulmonary Arterial Wedge Pressure (PAWP) is an essential cardiovascular hemodynamics marker to detect heart failure.","In clinical practice, Right Heart Catheterization is considered a gold standard for assessing cardiac hemodynamics while non-invasive methods are often needed to screen high-risk patients from a large population.","In this paper, we propose a multimodal learning pipeline to predict PAWP marker.","We utilize complementary information from Cardiac Magnetic Resonance Imaging (CMR) scans (short-axis and four-chamber) and Electronic Health Records (EHRs).","We extract spatio-temporal features from CMR scans using tensor-based learning.","We propose a graph attention network to select important EHR features for prediction, where we model subjects as graph nodes and feature relationships as graph edges using the attention mechanism.","We design four feature fusion strategies: early, intermediate, late, and hybrid fusion.","With a linear classifier and linear fusion strategies, our pipeline is interpretable.","We validate our pipeline on a large dataset of $2,641$ subjects from our ASPIRE registry.","The comparative study against state-of-the-art methods confirms the superiority of our pipeline.","The decision curve analysis further validates that our pipeline can be applied to screen a large population.","The code is available at https://github.com/prasunc/hemodynamics."],"url":"http://arxiv.org/abs/2404.04718v1","category":"cs.CV"}
{"created":"2024-04-06 19:27:57","title":"Data Poisoning Attacks on Off-Policy Policy Evaluation Methods","abstract":"Off-policy Evaluation (OPE) methods are a crucial tool for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible, unethical, or expensive. However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored. In this work, we make the first attempt at investigating the sensitivity of OPE methods to marginal adversarial perturbations to the data. We design a generic data poisoning attack framework leveraging influence functions from robust statistics to carefully construct perturbations that maximize error in the policy value estimates. We carry out extensive experimentation with multiple healthcare and control datasets. Our results demonstrate that many existing OPE methods are highly prone to generating value estimates with large errors when subject to data poisoning attacks, even for small adversarial perturbations. These findings question the reliability of policy values derived using OPE methods and motivate the need for developing OPE methods that are statistically robust to train-time data poisoning attacks.","sentences":["Off-policy Evaluation (OPE) methods are a crucial tool for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible, unethical, or expensive.","However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored.","In this work, we make the first attempt at investigating the sensitivity of OPE methods to marginal adversarial perturbations to the data.","We design a generic data poisoning attack framework leveraging influence functions from robust statistics to carefully construct perturbations that maximize error in the policy value estimates.","We carry out extensive experimentation with multiple healthcare and control datasets.","Our results demonstrate that many existing OPE methods are highly prone to generating value estimates with large errors when subject to data poisoning attacks, even for small adversarial perturbations.","These findings question the reliability of policy values derived using OPE methods and motivate the need for developing OPE methods that are statistically robust to train-time data poisoning attacks."],"url":"http://arxiv.org/abs/2404.04714v1","category":"cs.LG"}
{"created":"2024-04-06 19:25:00","title":"Faster Algorithms for Fair Max-Min Diversification in $\\mathbb{R}^d$","abstract":"The task of extracting a diverse subset from a dataset, often referred to as maximum diversification, plays a pivotal role in various real-world applications that have far-reaching consequences. In this work, we delve into the realm of fairness-aware data subset selection, specifically focusing on the problem of selecting a diverse set of size $k$ from a large collection of $n$ data points (FairDiv).   The FairDiv problem is well-studied in the data management and theory community. In this work, we develop the first constant approximation algorithm for FairDiv that runs in near-linear time using only linear space. In contrast, all previously known constant approximation algorithms run in super-linear time (with respect to $n$ or $k$) and use super-linear space. Our approach achieves this efficiency by employing a novel combination of the Multiplicative Weight Update method and advanced geometric data structures to implicitly and approximately solve a linear program. Furthermore, we improve the efficiency of our techniques by constructing a coreset. Using our coreset, we also propose the first efficient streaming algorithm for the FairDiv problem whose efficiency does not depend on the distribution of data points. Empirical evaluation on million-sized datasets demonstrates that our algorithm achieves the best diversity within a minute. All prior techniques are either highly inefficient or do not generate a good solution.","sentences":["The task of extracting a diverse subset from a dataset, often referred to as maximum diversification, plays a pivotal role in various real-world applications that have far-reaching consequences.","In this work, we delve into the realm of fairness-aware data subset selection, specifically focusing on the problem of selecting a diverse set of size $k$ from a large collection of $n$ data points (FairDiv).   ","The FairDiv problem is well-studied in the data management and theory community.","In this work, we develop the first constant approximation algorithm for FairDiv that runs in near-linear time using only linear space.","In contrast, all previously known constant approximation algorithms run in super-linear time (with respect to $n$ or $k$) and use super-linear space.","Our approach achieves this efficiency by employing a novel combination of the Multiplicative Weight Update method and advanced geometric data structures to implicitly and approximately solve a linear program.","Furthermore, we improve the efficiency of our techniques by constructing a coreset.","Using our coreset, we also propose the first efficient streaming algorithm for the FairDiv problem whose efficiency does not depend on the distribution of data points.","Empirical evaluation on million-sized datasets demonstrates that our algorithm achieves the best diversity within a minute.","All prior techniques are either highly inefficient or do not generate a good solution."],"url":"http://arxiv.org/abs/2404.04713v1","category":"cs.DB"}
{"created":"2024-04-06 18:58:26","title":"Gender Bias in Emerging New Research Topics: The Impact of COVID-19 on Women in Science","abstract":"We investigate the impact of new research opportunities on the long-standing under-representation of women in medical and academic leadership by assessing the impact of the emergence of COVID-19 as a new research topic in the life sciences on women's authorship. After collecting publication data from 2019 and 2020 on biomedical publications, where the position of first and last author is most important for future career development, we use the major Medical Subject Heading (MeSH) terms to identify the main research area of each publication and measure the relation of each paper to COVID-19. Using a Difference-in-Difference approach, we find that although the general female authorship trend is upwards, papers in areas related to COVID-19 are less likely to have a woman as first or last author compared to research areas not related to COVID-19. Conversely, new publication opportunities in the COVID-19 research field increase the proportion of women in middle, less-relevant, author positions. Stay-at-home mandates, journal importance, and access to new funds do not fully explain the drop in women's outcomes. The decline in female first authorship is related to the increase of teams in which both lead authors have no prior experience in the COVID-related research field. In addition, pre-existing publishing teams show reduced bias in female key authorship with respect to new teams specifically formed for COVID-related research. This suggests that opportunistic teams, transitioning into research areas with emerging interests, possess greater flexibility in choosing the primary and final authors, potentially reducing uncertainties associated with engaging in productions divergent from their past scientific experiences by excluding women scientists from key authorship positions.","sentences":["We investigate the impact of new research opportunities on the long-standing under-representation of women in medical and academic leadership by assessing the impact of the emergence of COVID-19 as a new research topic in the life sciences on women's authorship.","After collecting publication data from 2019 and 2020 on biomedical publications, where the position of first and last author is most important for future career development, we use the major Medical Subject Heading (MeSH) terms to identify the main research area of each publication and measure the relation of each paper to COVID-19.","Using a Difference-in-Difference approach, we find that although the general female authorship trend is upwards, papers in areas related to COVID-19 are less likely to have a woman as first or last author compared to research areas not related to COVID-19.","Conversely, new publication opportunities in the COVID-19 research field increase the proportion of women in middle, less-relevant, author positions.","Stay-at-home mandates, journal importance, and access to new funds do not fully explain the drop in women's outcomes.","The decline in female first authorship is related to the increase of teams in which both lead authors have no prior experience in the COVID-related research field.","In addition, pre-existing publishing teams show reduced bias in female key authorship with respect to new teams specifically formed for COVID-related research.","This suggests that opportunistic teams, transitioning into research areas with emerging interests, possess greater flexibility in choosing the primary and final authors, potentially reducing uncertainties associated with engaging in productions divergent from their past scientific experiences by excluding women scientists from key authorship positions."],"url":"http://arxiv.org/abs/2404.04707v1","category":"econ.GN"}
{"created":"2024-04-06 17:47:42","title":"EAGLE: The First Event Camera Dataset Gathered by an Agile Quadruped Robot","abstract":"When legged robots perform agile movements, traditional RGB cameras often produce blurred images, posing a challenge for accurate state estimation. Event cameras, inspired by biological vision mechanisms, have emerged as a promising solution for capturing high-speed movements and coping with challenging lighting conditions, owing to their significant advantages, such as low latency, high temporal resolution, and a high dynamic range. However, the integration of event cameras into agile-legged robots is still largely unexplored. Notably, no event camera-based dataset has yet been specifically developed for dynamic legged robots. To bridge this gap, we introduce EAGLE (Event dataset of an AGile LEgged robot), a new dataset comprising data from an event camera, an RGB-D camera, an IMU, a LiDAR, and joint angle encoders, all mounted on a quadruped robotic platform. This dataset features more than 100 sequences from real-world environments, encompassing various indoor and outdoor environments, different lighting conditions, a range of robot gaits (e.g., trotting, bounding, pronking), as well as acrobatic movements such as backflipping. To our knowledge, this is the first event camera dataset to include multi-sensory data collected by an agile quadruped robot.","sentences":["When legged robots perform agile movements, traditional RGB cameras often produce blurred images, posing a challenge for accurate state estimation.","Event cameras, inspired by biological vision mechanisms, have emerged as a promising solution for capturing high-speed movements and coping with challenging lighting conditions, owing to their significant advantages, such as low latency, high temporal resolution, and a high dynamic range.","However, the integration of event cameras into agile-legged robots is still largely unexplored.","Notably, no event camera-based dataset has yet been specifically developed for dynamic legged robots.","To bridge this gap, we introduce EAGLE (Event dataset of an AGile LEgged robot), a new dataset comprising data from an event camera, an RGB-D camera, an IMU, a LiDAR, and joint angle encoders, all mounted on a quadruped robotic platform.","This dataset features more than 100 sequences from real-world environments, encompassing various indoor and outdoor environments, different lighting conditions, a range of robot gaits (e.g., trotting, bounding, pronking), as well as acrobatic movements such as backflipping.","To our knowledge, this is the first event camera dataset to include multi-sensory data collected by an agile quadruped robot."],"url":"http://arxiv.org/abs/2404.04698v1","category":"cs.RO"}
{"created":"2024-04-06 17:41:00","title":"Securing the Skies: An IRS-Assisted AoI-Aware Secure Multi-UAV System with Efficient Task Offloading","abstract":"Unmanned Aerial Vehicles (UAVs) are integral in various sectors like agriculture, surveillance, and logistics, driven by advancements in 5G. However, existing research lacks a comprehensive approach addressing both data freshness and security concerns. In this paper, we address the intricate challenges of data freshness, and security, especially in the context of eavesdropping and jamming in modern UAV networks. Our framework incorporates exponential AoI metrics and emphasizes secrecy rate to tackle eavesdropping and jamming threats. We introduce a transformer-enhanced Deep Reinforcement Learning (DRL) approach to optimize task offloading processes. Comparative analysis with existing algorithms showcases the superiority of our scheme, indicating its promising advancements in UAV network management.","sentences":["Unmanned Aerial Vehicles (UAVs) are integral in various sectors like agriculture, surveillance, and logistics, driven by advancements in 5G.","However, existing research lacks a comprehensive approach addressing both data freshness and security concerns.","In this paper, we address the intricate challenges of data freshness, and security, especially in the context of eavesdropping and jamming in modern UAV networks.","Our framework incorporates exponential AoI metrics and emphasizes secrecy rate to tackle eavesdropping and jamming threats.","We introduce a transformer-enhanced Deep Reinforcement Learning (DRL) approach to optimize task offloading processes.","Comparative analysis with existing algorithms showcases the superiority of our scheme, indicating its promising advancements in UAV network management."],"url":"http://arxiv.org/abs/2404.04692v1","category":"eess.SY"}
{"created":"2024-04-06 17:23:21","title":"Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI","abstract":"Breast cancer has rapidly increased in prevalence in recent years, making it one of the leading causes of mortality worldwide. Among all cancers, it is by far the most common. Diagnosing this illness manually requires significant time and expertise. Since detecting breast cancer is a time-consuming process, preventing its further spread can be aided by creating machine-based forecasts. Machine learning and Explainable AI are crucial in classification as they not only provide accurate predictions but also offer insights into how the model arrives at its decisions, aiding in the understanding and trustworthiness of the classification results. In this study, we evaluate and compare the classification accuracy, precision, recall, and F-1 scores of five different machine learning methods using a primary dataset (500 patients from Dhaka Medical College Hospital). Five different supervised machine learning techniques, including decision tree, random forest, logistic regression, naive bayes, and XGBoost, have been used to achieve optimal results on our dataset. Additionally, this study applied SHAP analysis to the XGBoost model to interpret the model's predictions and understand the impact of each feature on the model's output. We compared the accuracy with which several algorithms classified the data, as well as contrasted with other literature in this field. After final evaluation, this study found that XGBoost achieved the best model accuracy, which is 97%.","sentences":["Breast cancer has rapidly increased in prevalence in recent years, making it one of the leading causes of mortality worldwide.","Among all cancers, it is by far the most common.","Diagnosing this illness manually requires significant time and expertise.","Since detecting breast cancer is a time-consuming process, preventing its further spread can be aided by creating machine-based forecasts.","Machine learning and Explainable AI are crucial in classification as they not only provide accurate predictions but also offer insights into how the model arrives at its decisions, aiding in the understanding and trustworthiness of the classification results.","In this study, we evaluate and compare the classification accuracy, precision, recall, and F-1 scores of five different machine learning methods using a primary dataset (500 patients from Dhaka Medical College Hospital).","Five different supervised machine learning techniques, including decision tree, random forest, logistic regression, naive bayes, and XGBoost, have been used to achieve optimal results on our dataset.","Additionally, this study applied SHAP analysis to the XGBoost model to interpret the model's predictions and understand the impact of each feature on the model's output.","We compared the accuracy with which several algorithms classified the data, as well as contrasted with other literature in this field.","After final evaluation, this study found that XGBoost achieved the best model accuracy, which is 97%."],"url":"http://arxiv.org/abs/2404.04686v1","category":"cs.LG"}
{"created":"2024-04-06 17:02:18","title":"Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) is a compelling framework for learning optimal policies from past experiences without additional interaction with the environment. Nevertheless, offline RL inevitably faces the problem of distributional shifts, where the states and actions encountered during policy execution may not be in the training dataset distribution. A common solution involves incorporating conservatism into the policy or the value function to safeguard against uncertainties and unknowns. In this work, we focus on achieving the same objectives of conservatism but from a different perspective. We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline RL, an approach that pursues conservatism in a compositional manner on top of the transductive reparameterization (Netanyahu et al., 2023), which decomposes the input variable (the state in our case) into an anchor and its difference from the original input. Our COCOA seeks both in-distribution anchors and differences by utilizing the learned reverse dynamics model, encouraging conservatism in the compositional input space for the policy or value function. Such compositional conservatism is independent of and agnostic to the prevalent behavioral conservatism in offline RL. We apply COCOA to four state-of-the-art offline RL algorithms and evaluate them on the D4RL benchmark, where COCOA generally improves the performance of each algorithm. The code is available at https://github.com/runamu/compositional-conservatism.","sentences":["Offline reinforcement learning (RL) is a compelling framework for learning optimal policies from past experiences without additional interaction with the environment.","Nevertheless, offline RL inevitably faces the problem of distributional shifts, where the states and actions encountered during policy execution may not be in the training dataset distribution.","A common solution involves incorporating conservatism into the policy or the value function to safeguard against uncertainties and unknowns.","In this work, we focus on achieving the same objectives of conservatism but from a different perspective.","We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline RL, an approach that pursues conservatism in a compositional manner on top of the transductive reparameterization (Netanyahu et al., 2023), which decomposes the input variable (the state in our case) into an anchor and its difference from the original input.","Our COCOA seeks both in-distribution anchors and differences by utilizing the learned reverse dynamics model, encouraging conservatism in the compositional input space for the policy or value function.","Such compositional conservatism is independent of and agnostic to the prevalent behavioral conservatism in offline RL.","We apply COCOA to four state-of-the-art offline RL algorithms and evaluate them on the D4RL benchmark, where COCOA generally improves the performance of each algorithm.","The code is available at https://github.com/runamu/compositional-conservatism."],"url":"http://arxiv.org/abs/2404.04682v1","category":"cs.LG"}
{"created":"2024-04-06 16:15:32","title":"Collective charge excitations studied by electron energy-loss spectroscopy","abstract":"The dynamic charge susceptibility, $\\chi(q,\\omega)$, is a fundamental observable of all materials, in one, two, and three dimensions, quantifying the collective charge modes, the ability of a material to screen charge, as well as its electronic compressibility. Here, we review the current state of efforts to measure this quantity using inelastic electron scattering, which historically has been called electron energy-loss spectroscopy (EELS). We focus on comparison between transmission (T-EELS) and reflection (R-EELS) geometries as applied to a selection of 3D conductors. While a great deal is understood about simple metals, measurements of more strongly interacting and strange metals are currently contradictory, with different groups obtaining fundamentally conflicting results, emphasizing the importance of improved EELS measurements. Further, current opportunities for improvement in EELS techniques are vast, with the most promising future developments being in hemispherical and time-of-flight analyzers, as well as STEM instruments configured for high momentum resolution. We conclude that, despite more than half a century of work, EELS techniques are currently still in their infancy","sentences":["The dynamic charge susceptibility, $\\chi(q,\\omega)$, is a fundamental observable of all materials, in one, two, and three dimensions, quantifying the collective charge modes, the ability of a material to screen charge, as well as its electronic compressibility.","Here, we review the current state of efforts to measure this quantity using inelastic electron scattering, which historically has been called electron energy-loss spectroscopy (EELS).","We focus on comparison between transmission (T-EELS) and reflection (R-EELS) geometries as applied to a selection of 3D conductors.","While a great deal is understood about simple metals, measurements of more strongly interacting and strange metals are currently contradictory, with different groups obtaining fundamentally conflicting results, emphasizing the importance of improved EELS measurements.","Further, current opportunities for improvement in EELS techniques are vast, with the most promising future developments being in hemispherical and time-of-flight analyzers, as well as STEM instruments configured for high momentum resolution.","We conclude that, despite more than half a century of work, EELS techniques are currently still in their infancy"],"url":"http://arxiv.org/abs/2404.04670v1","category":"cond-mat.str-el"}
{"created":"2024-04-06 15:50:19","title":"Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology","abstract":"Multimodal artificial intelligence (AI) systems have the potential to enhance clinical decision-making by interpreting various types of medical data. However, the effectiveness of these models across all medical fields is uncertain. Each discipline presents unique challenges that need to be addressed for optimal performance. This complexity is further increased when attempting to integrate different fields into a single model. Here, we introduce an alternative approach to multimodal medical AI that utilizes the generalist capabilities of a large language model (LLM) as a central reasoning engine. This engine autonomously coordinates and deploys a set of specialized medical AI tools. These tools include text, radiology and histopathology image interpretation, genomic data processing, web searches, and document retrieval from medical guidelines. We validate our system across a series of clinical oncology scenarios that closely resemble typical patient care workflows. We show that the system has a high capability in employing appropriate tools (97%), drawing correct conclusions (93.6%), and providing complete (94%), and helpful (89.2%) recommendations for individual patient cases while consistently referencing relevant literature (82.5%) upon instruction. This work provides evidence that LLMs can effectively plan and execute domain-specific models to retrieve or synthesize new information when used as autonomous agents. This enables them to function as specialist, patient-tailored clinical assistants. It also simplifies regulatory compliance by allowing each component tool to be individually validated and approved. We believe, that our work can serve as a proof-of-concept for more advanced LLM-agents in the medical domain.","sentences":["Multimodal artificial intelligence (AI) systems have the potential to enhance clinical decision-making by interpreting various types of medical data.","However, the effectiveness of these models across all medical fields is uncertain.","Each discipline presents unique challenges that need to be addressed for optimal performance.","This complexity is further increased when attempting to integrate different fields into a single model.","Here, we introduce an alternative approach to multimodal medical AI that utilizes the generalist capabilities of a large language model (LLM) as a central reasoning engine.","This engine autonomously coordinates and deploys a set of specialized medical AI tools.","These tools include text, radiology and histopathology image interpretation, genomic data processing, web searches, and document retrieval from medical guidelines.","We validate our system across a series of clinical oncology scenarios that closely resemble typical patient care workflows.","We show that the system has a high capability in employing appropriate tools (97%), drawing correct conclusions (93.6%), and providing complete (94%), and helpful (89.2%) recommendations for individual patient cases while consistently referencing relevant literature (82.5%) upon instruction.","This work provides evidence that LLMs can effectively plan and execute domain-specific models to retrieve or synthesize new information when used as autonomous agents.","This enables them to function as specialist, patient-tailored clinical assistants.","It also simplifies regulatory compliance by allowing each component tool to be individually validated and approved.","We believe, that our work can serve as a proof-of-concept for more advanced LLM-agents in the medical domain."],"url":"http://arxiv.org/abs/2404.04667v1","category":"cs.AI"}
{"created":"2024-04-06 15:48:14","title":"Adaptive Intra-Class Variation Contrastive Learning for Unsupervised Person Re-Identification","abstract":"The memory dictionary-based contrastive learning method has achieved remarkable results in the field of unsupervised person Re-ID. However, The method of updating memory based on all samples does not fully utilize the hardest sample to improve the generalization ability of the model, and the method based on hardest sample mining will inevitably introduce false-positive samples that are incorrectly clustered in the early stages of the model. Clustering-based methods usually discard a significant number of outliers, leading to the loss of valuable information. In order to address the issues mentioned before, we propose an adaptive intra-class variation contrastive learning algorithm for unsupervised Re-ID, called AdaInCV. And the algorithm quantitatively evaluates the learning ability of the model for each class by considering the intra-class variations after clustering, which helps in selecting appropriate samples during the training process of the model. To be more specific, two new strategies are proposed: Adaptive Sample Mining (AdaSaM) and Adaptive Outlier Filter (AdaOF). The first one gradually creates more reliable clusters to dynamically refine the memory, while the second can identify and filter out valuable outliers as negative samples.","sentences":["The memory dictionary-based contrastive learning method has achieved remarkable results in the field of unsupervised person Re-ID.","However, The method of updating memory based on all samples does not fully utilize the hardest sample to improve the generalization ability of the model, and the method based on hardest sample mining will inevitably introduce false-positive samples that are incorrectly clustered in the early stages of the model.","Clustering-based methods usually discard a significant number of outliers, leading to the loss of valuable information.","In order to address the issues mentioned before, we propose an adaptive intra-class variation contrastive learning algorithm for unsupervised Re-ID, called AdaInCV.","And the algorithm quantitatively evaluates the learning ability of the model for each class by considering the intra-class variations after clustering, which helps in selecting appropriate samples during the training process of the model.","To be more specific, two new strategies are proposed:","Adaptive Sample Mining (AdaSaM) and Adaptive Outlier Filter (AdaOF).","The first one gradually creates more reliable clusters to dynamically refine the memory, while the second can identify and filter out valuable outliers as negative samples."],"url":"http://arxiv.org/abs/2404.04665v1","category":"cs.CV"}
{"created":"2024-04-06 15:31:57","title":"Focused Active Learning for Histopathological Image Classification","abstract":"Active Learning (AL) has the potential to solve a major problem of digital pathology: the efficient acquisition of labeled data for machine learning algorithms. However, existing AL methods often struggle in realistic settings with artifacts, ambiguities, and class imbalances, as commonly seen in the medical field. The lack of precise uncertainty estimations leads to the acquisition of images with a low informative value. To address these challenges, we propose Focused Active Learning (FocAL), which combines a Bayesian Neural Network with Out-of-Distribution detection to estimate different uncertainties for the acquisition function. Specifically, the weighted epistemic uncertainty accounts for the class imbalance, aleatoric uncertainty for ambiguous images, and an OoD score for artifacts. We perform extensive experiments to validate our method on MNIST and the real-world Panda dataset for the classification of prostate cancer. The results confirm that other AL methods are 'distracted' by ambiguities and artifacts which harm the performance. FocAL effectively focuses on the most informative images, avoiding ambiguities and artifacts during acquisition. For both experiments, FocAL outperforms existing AL approaches, reaching a Cohen's kappa of 0.764 with only 0.69% of the labeled Panda data.","sentences":["Active Learning (AL) has the potential to solve a major problem of digital pathology: the efficient acquisition of labeled data for machine learning algorithms.","However, existing AL methods often struggle in realistic settings with artifacts, ambiguities, and class imbalances, as commonly seen in the medical field.","The lack of precise uncertainty estimations leads to the acquisition of images with a low informative value.","To address these challenges, we propose Focused Active Learning (FocAL), which combines a Bayesian Neural Network with Out-of-Distribution detection to estimate different uncertainties for the acquisition function.","Specifically, the weighted epistemic uncertainty accounts for the class imbalance, aleatoric uncertainty for ambiguous images, and an OoD score for artifacts.","We perform extensive experiments to validate our method on MNIST and the real-world Panda dataset for the classification of prostate cancer.","The results confirm that other AL methods are 'distracted' by ambiguities and artifacts which harm the performance.","FocAL effectively focuses on the most informative images, avoiding ambiguities and artifacts during acquisition.","For both experiments, FocAL outperforms existing AL approaches, reaching a Cohen's kappa of 0.764 with only 0.69% of the labeled Panda data."],"url":"http://arxiv.org/abs/2404.04663v1","category":"cs.CV"}
{"created":"2024-04-06 15:31:17","title":"Transform then Explore: a Simple and Effective Technique for Exploratory Combinatorial Optimization with Reinforcement Learning","abstract":"Many complex problems encountered in both production and daily life can be conceptualized as combinatorial optimization problems (COPs) over graphs. Recent years, reinforcement learning (RL) based models have emerged as a promising direction, which treat the COPs solving as a heuristic learning problem. However, current finite-horizon-MDP based RL models have inherent limitations. They are not allowed to explore adquately for improving solutions at test time, which may be necessary given the complexity of NP-hard optimization tasks. Some recent attempts solve this issue by focusing on reward design and state feature engineering, which are tedious and ad-hoc. In this work, we instead propose a much simpler but more effective technique, named gauge transformation (GT). The technique is originated from physics, but is very effective in enabling RL agents to explore to continuously improve the solutions during test. Morever, GT is very simple, which can be implemented with less than 10 lines of Python codes, and can be applied to a vast majority of RL models. Experimentally, we show that traditional RL models with GT technique produce the state-of-the-art performances on the MaxCut problem. Furthermore, since GT is independent of any RL models, it can be seamlessly integrated into various RL frameworks, paving the way of these models for more effective explorations in the solving of general COPs.","sentences":["Many complex problems encountered in both production and daily life can be conceptualized as combinatorial optimization problems (COPs) over graphs.","Recent years, reinforcement learning (RL) based models have emerged as a promising direction, which treat the COPs solving as a heuristic learning problem.","However, current finite-horizon-MDP based RL models have inherent limitations.","They are not allowed to explore adquately for improving solutions at test time, which may be necessary given the complexity of NP-hard optimization tasks.","Some recent attempts solve this issue by focusing on reward design and state feature engineering, which are tedious and ad-hoc.","In this work, we instead propose a much simpler but more effective technique, named gauge transformation (GT).","The technique is originated from physics, but is very effective in enabling RL agents to explore to continuously improve the solutions during test.","Morever, GT is very simple, which can be implemented with less than 10 lines of Python codes, and can be applied to a vast majority of RL models.","Experimentally, we show that traditional RL models with GT technique produce the state-of-the-art performances on the MaxCut problem.","Furthermore, since GT is independent of any RL models, it can be seamlessly integrated into various RL frameworks, paving the way of these models for more effective explorations in the solving of general COPs."],"url":"http://arxiv.org/abs/2404.04661v1","category":"cs.LG"}
{"created":"2024-04-06 15:20:59","title":"Binary Classifier Optimization for Large Language Model Alignment","abstract":"Aligning Large Language Models (LLMs) to human preferences through preference optimization has been crucial but labor-intensive, necessitating for each prompt a comparison of both a chosen and a rejected text completion by evaluators. Recently, Kahneman-Tversky Optimization (KTO) has demonstrated that LLMs can be aligned using merely binary \"thumbs-up\" or \"thumbs-down\" signals on each prompt-completion pair. In this paper, we present theoretical foundations to explain the successful alignment achieved through these binary signals. Our analysis uncovers a new perspective: optimizing a binary classifier, whose logit is a reward, implicitly induces minimizing the Direct Preference Optimization (DPO) loss. In the process of this discovery, we identified two techniques for effective alignment: reward shift and underlying distribution matching. Consequently, we propose a new algorithm, \\textit{Binary Classifier Optimization}, that integrates the techniques. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO and KTO; and second, on binary signal datasets simulating real-world conditions with divergent underlying distributions between thumbs-up and thumbs-down data. Our model consistently demonstrates effective and robust alignment across two base LLMs and three different binary signal datasets, showcasing the strength of our approach to learning from binary feedback.","sentences":["Aligning Large Language Models (LLMs) to human preferences through preference optimization has been crucial but labor-intensive, necessitating for each prompt a comparison of both a chosen and a rejected text completion by evaluators.","Recently, Kahneman-Tversky Optimization (KTO) has demonstrated that LLMs can be aligned using merely binary \"thumbs-up\" or \"thumbs-down\" signals on each prompt-completion pair.","In this paper, we present theoretical foundations to explain the successful alignment achieved through these binary signals.","Our analysis uncovers a new perspective: optimizing a binary classifier, whose logit is a reward, implicitly induces minimizing the Direct Preference Optimization (DPO) loss.","In the process of this discovery, we identified two techniques for effective alignment: reward shift and underlying distribution matching.","Consequently, we propose a new algorithm, \\textit{Binary Classifier Optimization}, that integrates the techniques.","We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO and KTO; and second, on binary signal datasets simulating real-world conditions with divergent underlying distributions between thumbs-up and thumbs-down data.","Our model consistently demonstrates effective and robust alignment across two base LLMs and three different binary signal datasets, showcasing the strength of our approach to learning from binary feedback."],"url":"http://arxiv.org/abs/2404.04656v1","category":"cs.LG"}
{"created":"2024-04-06 14:27:22","title":"Power-Efficient Image Storage: Leveraging Super Resolution Generative Adversarial Network for Sustainable Compression and Reduced Carbon Footprint","abstract":"In recent years, large-scale adoption of cloud storage solutions has revolutionized the way we think about digital data storage. However, the exponential increase in data volume, especially images, has raised environmental concerns regarding power and resource consumption, as well as the rising digital carbon footprint emissions. The aim of this research is to propose a methodology for cloud-based image storage by integrating image compression technology with SuperResolution Generative Adversarial Networks (SRGAN). Rather than storing images in their original format directly on the cloud, our approach involves initially reducing the image size through compression and downsizing techniques before storage. Upon request, these compressed images will be retrieved and processed by SRGAN to generate images. The efficacy of the proposed method is evaluated in terms of PSNR and SSIM metrics. Additionally, a mathematical analysis is given to calculate power consumption and carbon footprint assesment. The proposed data compression technique provides a significant solution to achieve a reasonable trade off between environmental sustainability and industrial efficiency.","sentences":["In recent years, large-scale adoption of cloud storage solutions has revolutionized the way we think about digital data storage.","However, the exponential increase in data volume, especially images, has raised environmental concerns regarding power and resource consumption, as well as the rising digital carbon footprint emissions.","The aim of this research is to propose a methodology for cloud-based image storage by integrating image compression technology with SuperResolution Generative Adversarial Networks (SRGAN).","Rather than storing images in their original format directly on the cloud, our approach involves initially reducing the image size through compression and downsizing techniques before storage.","Upon request, these compressed images will be retrieved and processed by SRGAN to generate images.","The efficacy of the proposed method is evaluated in terms of PSNR and SSIM metrics.","Additionally, a mathematical analysis is given to calculate power consumption and carbon footprint assesment.","The proposed data compression technique provides a significant solution to achieve a reasonable trade off between environmental sustainability and industrial efficiency."],"url":"http://arxiv.org/abs/2404.04642v1","category":"eess.IV"}
{"created":"2024-04-06 14:23:57","title":"Search for di-photon decays of an axion-like particle in radiative decays of J/psi","abstract":"We search for the di-photon decay of a light pseudoscalar axion-like particle, $a$, in radiative decays of the $J/\\psi$, using 10 billion $J/\\psi$ events collected with the BESIII detector. We find no evidence of a narrow resonance and set upper limits at the $95\\%$ confidence level on the product branching fraction $\\mathcal{B}(J/\\psi \\to \\gamma a) \\times \\mathcal{B}(a \\to \\gamma \\gamma)$ and the axion-like particle photon coupling constant $g_{a \\gamma \\gamma}$ in the ranges of $(3.6-49.8) \\times 10^{-8}$ and $(2.2 -103.8)\\times 10^{-4}$ GeV$^{-1}$, respectively, for $0.18 \\le m_a \\le 2.85~$ GeV/$c^2$. These are the most stringent limits to date in this mass region.","sentences":["We search for the di-photon decay of a light pseudoscalar axion-like particle, $a$, in radiative decays of the $J/\\psi$, using 10 billion $J/\\psi$ events collected with the BESIII detector.","We find no evidence of a narrow resonance and set upper limits at the $95\\%$ confidence level on the product branching fraction $\\mathcal{B}(J/\\psi \\to \\gamma a) \\times \\mathcal{B}(a \\to \\gamma \\gamma)$ and the axion-like particle photon coupling constant $g_{a \\gamma \\gamma}$ in the ranges of $(3.6-49.8) \\times 10^{-8}$ and $(2.2 -103.8)\\times 10^{-4}$ GeV$^{-1}$, respectively, for $0.18 \\le m_a","\\le 2.85~$","GeV/$c^2$.","These are the most stringent limits to date in this mass region."],"url":"http://arxiv.org/abs/2404.04640v1","category":"hep-ex"}
{"created":"2024-04-06 14:08:33","title":"Designing for Complementarity: A Conceptual Framework to Go Beyond the Current Paradigm of Using XAI in Healthcare","abstract":"The widespread use of Artificial Intelligence-based tools in the healthcare sector raises many ethical and legal problems, one of the main reasons being their black-box nature and therefore the seemingly opacity and inscrutability of their characteristics and decision-making process. Literature extensively discusses how this can lead to phenomena of over-reliance and under-reliance, ultimately limiting the adoption of AI. We addressed these issues by building a theoretical framework based on three concepts: Feature Importance, Counterexample Explanations, and Similar-Case Explanations. Grounded in the literature, the model was deployed within a case study in which, using a participatory design approach, we designed and developed a high-fidelity prototype. Through the co-design and development of the prototype and the underlying model, we advanced the knowledge on how to design AI-based systems for enabling complementarity in the decision-making process in the healthcare domain. Our work aims at contributing to the current discourse on designing AI systems to support clinicians' decision-making processes.","sentences":["The widespread use of Artificial Intelligence-based tools in the healthcare sector raises many ethical and legal problems, one of the main reasons being their black-box nature and therefore the seemingly opacity and inscrutability of their characteristics and decision-making process.","Literature extensively discusses how this can lead to phenomena of over-reliance and under-reliance, ultimately limiting the adoption of AI.","We addressed these issues by building a theoretical framework based on three concepts: Feature Importance, Counterexample Explanations, and Similar-Case Explanations.","Grounded in the literature, the model was deployed within a case study in which, using a participatory design approach, we designed and developed a high-fidelity prototype.","Through the co-design and development of the prototype and the underlying model, we advanced the knowledge on how to design AI-based systems for enabling complementarity in the decision-making process in the healthcare domain.","Our work aims at contributing to the current discourse on designing AI systems to support clinicians' decision-making processes."],"url":"http://arxiv.org/abs/2404.04638v1","category":"cs.HC"}
{"created":"2024-04-06 13:38:15","title":"On the Limitations of Large Language Models (LLMs): False Attribution","abstract":"In this work, we provide insight into one important limitation of large language models (LLMs), i.e. false attribution, and introduce a new hallucination metric - Simple Hallucination Index (SHI). The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (LLaMA-2-13B, Mixtral 8x7B, and Gemma-7B), especially as human annotation can be costly. We collected the top 10 most popular books, according to Project Gutenberg, divided each one into equal chunks of 400 words, and asked each LLM to predict the author. We then randomly sampled 162 chunks for human evaluation from each of the annotated books, based on the error margin of 7% and a confidence level of 95% for the book with the most chunks (Great Expectations by Charles Dickens, having 922 chunks). The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.737, 0.249, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as an SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which is generalizable to other tasks. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.","sentences":["In this work, we provide insight into one important limitation of large language models (LLMs), i.e. false attribution, and introduce a new hallucination metric - Simple Hallucination Index (SHI).","The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging.","We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (LLaMA-2-13B, Mixtral 8x7B, and Gemma-7B), especially as human annotation can be costly.","We collected the top 10 most popular books, according to Project Gutenberg, divided each one into equal chunks of 400 words, and asked each LLM to predict the author.","We then randomly sampled 162 chunks for human evaluation from each of the annotated books, based on the error margin of 7% and a confidence level of 95% for the book with the most chunks (Great Expectations by Charles Dickens, having 922 chunks).","The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.737, 0.249, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.","However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as an SHI of 0.87 (in the range 0-1, where 1 is the worst).","The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which is generalizable to other tasks.","We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models."],"url":"http://arxiv.org/abs/2404.04631v1","category":"cs.CL"}
{"created":"2024-04-06 13:24:37","title":"Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective","abstract":"Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT's effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance. To overcome those limitations, the theoretical understanding of DPO are indispensable but still lacking. To this end, we take a step towards theoretically analyzing and understanding the limitations of DPO. Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO. By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement.","sentences":["Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences.","Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT's effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance.","To overcome those limitations, the theoretical understanding of DPO are indispensable but still lacking.","To this end, we take a step towards theoretically analyzing and understanding the limitations of DPO.","Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO.","By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data.","This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement."],"url":"http://arxiv.org/abs/2404.04626v1","category":"cs.CL"}
{"created":"2024-04-06 12:51:00","title":"Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model","abstract":"With the power of large language models (LLMs), open-ended embodied agents can flexibly understand human instructions, generate interpretable guidance strategies, and output executable actions. Nowadays, Multi-modal Language Models~(MLMs) integrate multi-modal signals into LLMs, further bringing richer perception to entity agents and allowing embodied agents to perceive world-understanding tasks more delicately. However, existing works: 1) operate independently by agents, each containing multiple LLMs, from perception to action, resulting in gaps between complex tasks and execution; 2) train MLMs on static data, struggling with dynamics in open-ended scenarios; 3) input prior knowledge directly as prompts, suppressing application flexibility. We propose STEVE-2, a hierarchical knowledge distillation framework for open-ended embodied tasks, characterized by 1) a hierarchical system for multi-granular task division, 2) a mirrored distillation method for parallel simulation data, and 3) an extra expert model for bringing additional knowledge into parallel simulation. After distillation, embodied agents can complete complex, open-ended tasks without additional expert guidance, utilizing the performance and knowledge of a versatile MLM. Extensive evaluations on navigation and creation tasks highlight the superior performance of STEVE-2 in open-ended tasks, with $1.4 \\times$ - $7.3 \\times$ in performance.","sentences":["With the power of large language models (LLMs), open-ended embodied agents can flexibly understand human instructions, generate interpretable guidance strategies, and output executable actions.","Nowadays, Multi-modal Language Models~(MLMs) integrate multi-modal signals into LLMs, further bringing richer perception to entity agents and allowing embodied agents to perceive world-understanding tasks more delicately.","However, existing works: 1) operate independently by agents, each containing multiple LLMs, from perception to action, resulting in gaps between complex tasks and execution; 2) train MLMs on static data, struggling with dynamics in open-ended scenarios; 3) input prior knowledge directly as prompts, suppressing application flexibility.","We propose STEVE-2, a hierarchical knowledge distillation framework for open-ended embodied tasks, characterized by 1) a hierarchical system for multi-granular task division, 2) a mirrored distillation method for parallel simulation data, and 3) an extra expert model for bringing additional knowledge into parallel simulation.","After distillation, embodied agents can complete complex, open-ended tasks without additional expert guidance, utilizing the performance and knowledge of a versatile MLM.","Extensive evaluations on navigation and creation tasks highlight the superior performance of STEVE-2 in open-ended tasks, with $1.4 \\times$ - $7.3 \\times$ in performance."],"url":"http://arxiv.org/abs/2404.04619v1","category":"cs.AI"}
{"created":"2024-04-06 12:49:09","title":"PointSAGE: Mesh-independent superresolution approach to fluid flow predictions","abstract":"Computational Fluid Dynamics (CFD) serves as a powerful tool for simulating fluid flow across diverse industries. High-resolution CFD simulations offer valuable insights into fluid behavior and flow patterns, aiding in optimizing design features or enhancing system performance. However, as resolution increases, computational data requirements and time increase proportionately. This presents a persistent challenge in CFD. Recently, efforts have been directed towards accurately predicting fine-mesh simulations using coarse-mesh simulations, with geometry and boundary conditions as input. Drawing inspiration from models designed for super-resolution, deep learning techniques like UNets have been applied to address this challenge. However, these existing methods are limited to structured data and fail if the mesh is unstructured due to its inability to convolute. Additionally, incorporating geometry/mesh information in the training process introduces drawbacks such as increased data requirements, challenges in generalizing to unseen geometries for the same physical phenomena, and issues with robustness to mesh distortions. To address these concerns, we propose a novel framework, PointSAGE a mesh-independent network that leverages the unordered, mesh-less nature of Pointcloud to learn the complex fluid flow and directly predict fine simulations, completely neglecting mesh information. Utilizing an adaptable framework, the model accurately predicts the fine data across diverse point cloud sizes, regardless of the training dataset's dimension. We have evaluated the effectiveness of PointSAGE on diverse datasets in different scenarios, demonstrating notable results and a significant acceleration in computational time in generating fine simulations compared to standard CFD techniques.","sentences":["Computational Fluid Dynamics (CFD) serves as a powerful tool for simulating fluid flow across diverse industries.","High-resolution CFD simulations offer valuable insights into fluid behavior and flow patterns, aiding in optimizing design features or enhancing system performance.","However, as resolution increases, computational data requirements and time increase proportionately.","This presents a persistent challenge in CFD.","Recently, efforts have been directed towards accurately predicting fine-mesh simulations using coarse-mesh simulations, with geometry and boundary conditions as input.","Drawing inspiration from models designed for super-resolution, deep learning techniques like UNets have been applied to address this challenge.","However, these existing methods are limited to structured data and fail if the mesh is unstructured due to its inability to convolute.","Additionally, incorporating geometry/mesh information in the training process introduces drawbacks such as increased data requirements, challenges in generalizing to unseen geometries for the same physical phenomena, and issues with robustness to mesh distortions.","To address these concerns, we propose a novel framework, PointSAGE a mesh-independent network that leverages the unordered, mesh-less nature of Pointcloud to learn the complex fluid flow and directly predict fine simulations, completely neglecting mesh information.","Utilizing an adaptable framework, the model accurately predicts the fine data across diverse point cloud sizes, regardless of the training dataset's dimension.","We have evaluated the effectiveness of PointSAGE on diverse datasets in different scenarios, demonstrating notable results and a significant acceleration in computational time in generating fine simulations compared to standard CFD techniques."],"url":"http://arxiv.org/abs/2404.04615v1","category":"physics.flu-dyn"}
{"created":"2024-04-08 17:59:44","title":"A Large-Scale Exploration of $\u03bc$-Transfer","abstract":"Large neural network models have become a mainstay of natural language processing and computer vision, yet their initialization and learning rates are set in a largely heuristic fashion, potentially varying from paper to paper and one model size to the next. The $\\mu$-Parameterization ($\\mu$P) offers a potential solution to these challenges, yielding scaling rules for model initialization and learning rates, and reportedly enabling zero-shot hyperparameter transfer from small to large models in a variety of cases.   Despite the evident promise, the $\\mu$P scaling rules are not yet widely adopted, perhaps due to higher implementation complexity, many variations, or complex theoretical background. This work investigates $\\mu$P empirically, focusing on the ubiquitous transformer architecture, and aims to answer a simple question: does $\\mu$-Transfer yield optimal learning rates in practice? From models with 2M to 10B parameters, we show that $\\mu$-Transfer works as intended for the majority of important cases, but also identify some surprising cases where it may not.","sentences":["Large neural network models have become a mainstay of natural language processing and computer vision, yet their initialization and learning rates are set in a largely heuristic fashion, potentially varying from paper to paper and one model size to the next.","The $\\mu$-Parameterization ($\\mu$P) offers a potential solution to these challenges, yielding scaling rules for model initialization and learning rates, and reportedly enabling zero-shot hyperparameter transfer from small to large models in a variety of cases.   ","Despite the evident promise, the $\\mu$P scaling rules are not yet widely adopted, perhaps due to higher implementation complexity, many variations, or complex theoretical background.","This work investigates $\\mu$P empirically, focusing on the ubiquitous transformer architecture, and aims to answer a simple question: does $\\mu$-Transfer yield optimal learning rates in practice?","From models with 2M to 10B parameters, we show that $\\mu$-Transfer works as intended for the majority of important cases, but also identify some surprising cases where it may not."],"url":"http://arxiv.org/abs/2404.05728v1","category":"cs.LG"}
{"created":"2024-04-08 17:59:02","title":"JADES: Primeval Lyman-$\\mathrm\u03b1$ emitting galaxies reveal early sites of reionisation out to redshift $z \\sim 9$","abstract":"$\\require{mediawiki-texvc}$Given the sensitivity of the resonant Lyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by neutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs) have been widely used to probe the ionising capabilities of reionisation-era galaxies and their impact on the intergalactic medium (IGM). However, prior to JWST our understanding of the contribution of fainter sources and of ionised `bubbles' at earlier stages of reionisation remained uncertain. Here, we present the characterisation of three exceptionally distant LAEs at $z>8$, newly discovered by JWST/NIRSpec in the JADES survey. These three similarly bright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small Ly$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta v_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of Ly$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$). The former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions ($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes remarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 71\\%$), which moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$). We find these LAEs are low-mass galaxies dominated by very recent, vigorous bursts of star formation accompanied by strong nebular emission from metal-poor gas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which reveals evidence for ionisation by an active galactic nucleus, may have reasonably produced small ionised bubbles preventing complete IGM absorption of Ly$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical Mpc}$ bubble, indicating faint galaxies have contributed significantly. The most distant LAEs thus continue to be powerful observational probes into the earlier stages of reionisation.","sentences":["$\\require{mediawiki-texvc}$Given the sensitivity of the resonant Lyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by neutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs) have been widely used to probe the ionising capabilities of reionisation-era galaxies and their impact on the intergalactic medium (IGM).","However, prior to JWST our understanding of the contribution of fainter sources and of ionised `bubbles' at earlier stages of reionisation remained uncertain.","Here, we present the characterisation of three exceptionally distant LAEs at $z>8$, newly discovered by JWST/NIRSpec in the JADES survey.","These three similarly bright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$)","LAEs exhibit small Ly$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta v_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of Ly$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).","The former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions ($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes remarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 71\\%$), which moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).","We find these LAEs are low-mass galaxies dominated by very recent, vigorous bursts of star formation accompanied by strong nebular emission from metal-poor gas.","We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which reveals evidence for ionisation by an active galactic nucleus, may have reasonably produced small ionised bubbles preventing complete IGM absorption of Ly$\\mathrm{\\alpha}$.","The third, however, requires a $\\sim 3\\,\\text{physical Mpc}$ bubble, indicating faint galaxies have contributed significantly.","The most distant LAEs thus continue to be powerful observational probes into the earlier stages of reionisation."],"url":"http://arxiv.org/abs/2404.05724v1","category":"astro-ph.GA"}
{"created":"2024-04-08 17:52:17","title":"Infrared Spectroscopy for Diagnosing Superlattice Minibands in Magic-angle Twisted Bilayer Graphene","abstract":"Twisted bilayer graphene (TBG) represents a highly tunable, strongly correlated electron system owed to its unique flat electronic bands. However, understanding the single-particle band structure alone has been challenging due to complex lattice reconstruction effects and a lack of spectroscopic measurements over a broad energy range. Here, we probe the band structure of TBG around the magic angle using infrared spectroscopy. Our measurements reveal spectral features originating from interband transitions whose energies are uniquely defined by the twist angle. By combining with quantum transport, we connect spectral features over a broad energy range (10 to 700 meV) spanning several superlattice minibands and track their evolution with twist angle. We compare our data with calculations of the band structures obtained via the continuum model and find good agreement only when considering a variation of interlayer/intralayer tunnelling parameters with the twist angle. Our analysis suggests that the magic angle also shifts due to lattice relaxation, and is better defined for a wide angular range from 0.9{\\deg} to 1.1{\\deg}. Our work provides spectroscopic insights into TBG's band structure and offers an optical fingerprint of the magic angle for screening heterostructures before nanofabrication.","sentences":["Twisted bilayer graphene (TBG) represents a highly tunable, strongly correlated electron system owed to its unique flat electronic bands.","However, understanding the single-particle band structure alone has been challenging due to complex lattice reconstruction effects and a lack of spectroscopic measurements over a broad energy range.","Here, we probe the band structure of TBG around the magic angle using infrared spectroscopy.","Our measurements reveal spectral features originating from interband transitions whose energies are uniquely defined by the twist angle.","By combining with quantum transport, we connect spectral features over a broad energy range (10 to 700 meV) spanning several superlattice minibands and track their evolution with twist angle.","We compare our data with calculations of the band structures obtained via the continuum model and find good agreement only when considering a variation of interlayer/intralayer tunnelling parameters with the twist angle.","Our analysis suggests that the magic angle also shifts due to lattice relaxation, and is better defined for a wide angular range from 0.9{\\deg} to 1.1{\\deg}.","Our work provides spectroscopic insights into TBG's band structure and offers an optical fingerprint of the magic angle for screening heterostructures before nanofabrication."],"url":"http://arxiv.org/abs/2404.05716v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 17:52:03","title":"What if you have only one copy? Low-depth quantum circuits have no advantage in decision problems!","abstract":"The conventional approach to understanding the characteristics of an unknown quantum state involves having numerous identical independent copies of the system in that state. However, we demonstrate that gleaning insights into specific properties is feasible even with a single-state sample. Perhaps surprisingly, the confidence level of our findings increases proportionally with the number of qubits. Our conclusions apply to quantum states with low circuit complexity, including noise-affected ones. Additionally, this extends to learning from a solitary sample of probability distributions. Our results establish a strong lower bound for discriminating quantum states with low complexity. Furthermore, we reveal no quantum advantage in decision problems involving low-depth quantum circuits. Our results can be used to verify NISQ devices.","sentences":["The conventional approach to understanding the characteristics of an unknown quantum state involves having numerous identical independent copies of the system in that state.","However, we demonstrate that gleaning insights into specific properties is feasible even with a single-state sample.","Perhaps surprisingly, the confidence level of our findings increases proportionally with the number of qubits.","Our conclusions apply to quantum states with low circuit complexity, including noise-affected ones.","Additionally, this extends to learning from a solitary sample of probability distributions.","Our results establish a strong lower bound for discriminating quantum states with low complexity.","Furthermore, we reveal no quantum advantage in decision problems involving low-depth quantum circuits.","Our results can be used to verify NISQ devices."],"url":"http://arxiv.org/abs/2404.05714v1","category":"quant-ph"}
{"created":"2024-04-08 17:50:25","title":"Enhance Low-Carbon Power System Operation via Carbon-Aware Demand Response","abstract":"As the electrification process advances, enormous power flexibility is becoming available on the demand side, which can be harnessed to facilitate power system decarbonization. Hence, this paper studies the carbon-aware demand response (C-DR) paradigm, where individual users aim to minimize their carbon footprints through the optimal scheduling of flexible load devices. The specific operational dynamics and constraints of deferrable loads and thermostatically controlled loads are considered, and the carbon emission flow method is employed to determine users' carbon footprints using nodal carbon intensities. Then, an optimal power dispatch model that integrates the C-DR mechanism is proposed for low-carbon power system operation, based on the carbon-aware optimal power flow (C-OPF) method. Two solution algorithms, including a centralized Karush-Kuhn-Tucker (KKT) reformulation algorithm and an iterative solution algorithm, are developed to solve the bi-level power dispatch optimization model. Numerical simulations on the IEEE New England 39-bus system demonstrate the effectiveness of the proposed methods.","sentences":["As the electrification process advances, enormous power flexibility is becoming available on the demand side, which can be harnessed to facilitate power system decarbonization.","Hence, this paper studies the carbon-aware demand response (C-DR) paradigm, where individual users aim to minimize their carbon footprints through the optimal scheduling of flexible load devices.","The specific operational dynamics and constraints of deferrable loads and thermostatically controlled loads are considered, and the carbon emission flow method is employed to determine users' carbon footprints using nodal carbon intensities.","Then, an optimal power dispatch model that integrates the C-DR mechanism is proposed for low-carbon power system operation, based on the carbon-aware optimal power flow (C-OPF) method.","Two solution algorithms, including a centralized Karush-Kuhn-Tucker (KKT) reformulation algorithm and an iterative solution algorithm, are developed to solve the bi-level power dispatch optimization model.","Numerical simulations on the IEEE New England 39-bus system demonstrate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2404.05713v1","category":"eess.SY"}
{"created":"2024-04-08 17:37:22","title":"Case Study: Neural Network Malware Detection Verification for Feature and Image Datasets","abstract":"Malware, or software designed with harmful intent, is an ever-evolving threat that can have drastic effects on both individuals and institutions. Neural network malware classification systems are key tools for combating these threats but are vulnerable to adversarial machine learning attacks. These attacks perturb input data to cause misclassification, bypassing protective systems. Existing defenses often rely on enhancing the training process, thereby increasing the model's robustness to these perturbations, which is quantified using verification. While training improvements are necessary, we propose focusing on the verification process used to evaluate improvements to training. As such, we present a case study that evaluates a novel verification domain that will help to ensure tangible safeguards against adversaries and provide a more reliable means of evaluating the robustness and effectiveness of anti-malware systems. To do so, we describe malware classification and two types of common malware datasets (feature and image datasets), demonstrate the certified robustness accuracy of malware classifiers using the Neural Network Verification (NNV) and Neural Network Enumeration (nnenum) tools, and outline the challenges and future considerations necessary for the improvement and refinement of the verification of malware classification. By evaluating this novel domain as a case study, we hope to increase its visibility, encourage further research and scrutiny, and ultimately enhance the resilience of digital systems against malicious attacks.","sentences":["Malware, or software designed with harmful intent, is an ever-evolving threat that can have drastic effects on both individuals and institutions.","Neural network malware classification systems are key tools for combating these threats but are vulnerable to adversarial machine learning attacks.","These attacks perturb input data to cause misclassification, bypassing protective systems.","Existing defenses often rely on enhancing the training process, thereby increasing the model's robustness to these perturbations, which is quantified using verification.","While training improvements are necessary, we propose focusing on the verification process used to evaluate improvements to training.","As such, we present a case study that evaluates a novel verification domain that will help to ensure tangible safeguards against adversaries and provide a more reliable means of evaluating the robustness and effectiveness of anti-malware systems.","To do so, we describe malware classification and two types of common malware datasets (feature and image datasets), demonstrate the certified robustness accuracy of malware classifiers using the Neural Network Verification (NNV) and Neural Network Enumeration (nnenum) tools, and outline the challenges and future considerations necessary for the improvement and refinement of the verification of malware classification.","By evaluating this novel domain as a case study, we hope to increase its visibility, encourage further research and scrutiny, and ultimately enhance the resilience of digital systems against malicious attacks."],"url":"http://arxiv.org/abs/2404.05703v1","category":"cs.CR"}
{"created":"2024-04-08 17:35:41","title":"On the estimation of complex statistics combining different surveys","abstract":"The importance of exploring a potential integration among surveys has been acknowledged in order to enhance effectiveness and minimize expenses. In this work, we employ the alignment method to combine information from two different surveys for the estimation of complex statistics. The derivation of the alignment weights poses challenges in case of complex statistics due to their non-linear form. To overcome this, we propose to use a linearized variable associated with the complex statistic under consideration. Linearized variables have been widely used to derive variance estimates, thus allowing for the estimation of the variance of the combined complex statistics estimates. Simulations conducted show the effectiveness of the proposed approach, resulting to the reduction of the variance of the combined complex statistics estimates. Also, in some cases, the usage of the alignment weights derived using the linearized variable associated with a complex statistic, could result in a further reduction of the variance of the combined estimates.","sentences":["The importance of exploring a potential integration among surveys has been acknowledged in order to enhance effectiveness and minimize expenses.","In this work, we employ the alignment method to combine information from two different surveys for the estimation of complex statistics.","The derivation of the alignment weights poses challenges in case of complex statistics due to their non-linear form.","To overcome this, we propose to use a linearized variable associated with the complex statistic under consideration.","Linearized variables have been widely used to derive variance estimates, thus allowing for the estimation of the variance of the combined complex statistics estimates.","Simulations conducted show the effectiveness of the proposed approach, resulting to the reduction of the variance of the combined complex statistics estimates.","Also, in some cases, the usage of the alignment weights derived using the linearized variable associated with a complex statistic, could result in a further reduction of the variance of the combined estimates."],"url":"http://arxiv.org/abs/2404.05702v1","category":"stat.ME"}
{"created":"2024-04-08 17:33:56","title":"Negative Photo Conductivity Triggered with Visible Light in Wide Bandgap Oxide-Based Optoelectronic Crossbar Memristive Array for Photograph Sensing and Neuromorphic Computing Applications","abstract":"Photoresponsivity studies of wide-bandgap oxide-based devices have emerged as a vibrant and popular research area. Researchers have explored various material systems in their quest to develop devices capable of responding to illumination. In this study, we engineered a mature wide bandgap oxide-based bilayer heterostructure synaptic memristor to emulate the human brain for applications in neuromorphic computing and photograph sensing. The device exhibits advanced electric and electro-photonic synaptic functions, such as long-term potentiation (LTP), long-term depression (LTD), and paired pulse facilitation (PPF), by applying successive electric and photonic pulses. Moreover, the device exhibits exceptional electrical SET and photonic RESET endurance, maintaining its stability for a minimum of 1200 cycles without any degradation. Density functional theory calculations of the band structures provide insights into the conduction mechanism of the device. Based on this memristor array, we developed an autoencoder and convolutional neural network for noise reduction and image recognition tasks, which achieves a peak signal-to-noise ratio of 562 and high accuracy of 84.23%, while consuming lower energy by four orders of magnitude compared with the Tesla P40 GPU. This groundbreaking research not only opens doors for the integration of our device into image processing but also represents a significant advancement in the realm of in-memory computing and photograph sensing features in a single cell.","sentences":["Photoresponsivity studies of wide-bandgap oxide-based devices have emerged as a vibrant and popular research area.","Researchers have explored various material systems in their quest to develop devices capable of responding to illumination.","In this study, we engineered a mature wide bandgap oxide-based bilayer heterostructure synaptic memristor to emulate the human brain for applications in neuromorphic computing and photograph sensing.","The device exhibits advanced electric and electro-photonic synaptic functions, such as long-term potentiation (LTP), long-term depression (LTD), and paired pulse facilitation (PPF), by applying successive electric and photonic pulses.","Moreover, the device exhibits exceptional electrical SET and photonic RESET endurance, maintaining its stability for a minimum of 1200 cycles without any degradation.","Density functional theory calculations of the band structures provide insights into the conduction mechanism of the device.","Based on this memristor array, we developed an autoencoder and convolutional neural network for noise reduction and image recognition tasks, which achieves a peak signal-to-noise ratio of 562 and high accuracy of 84.23%, while consuming lower energy by four orders of magnitude compared with the Tesla P40 GPU.","This groundbreaking research not only opens doors for the integration of our device into image processing but also represents a significant advancement in the realm of in-memory computing and photograph sensing features in a single cell."],"url":"http://arxiv.org/abs/2404.05701v1","category":"physics.app-ph"}
{"created":"2024-04-08 17:32:07","title":"In-situ Imaging of a Single-Atom Wave Packet in Continuous Space","abstract":"The wave nature of matter remains one of the most striking aspects of quantum mechanics. Since its inception, a wealth of experiments has demonstrated the interference, diffraction or scattering of massive particles. More recently, experiments with ever increasing control and resolution have allowed imaging the wavefunction of individual atoms. Here, we use quantum gas microscopy to image the in-situ spatial distribution of deterministically prepared single-atom wave packets as they expand in a plane. We achieve this by controllably projecting the expanding wavefunction onto the sites of a deep optical lattice and subsequently performing single-atom imaging. The protocol established here for imaging extended wave packets via quantum gas microscopy is readily applicable to the wavefunction of interacting many-body systems in continuous space, promising a direct access to their microscopic properties, including spatial correlation functions up to high order and large distances.","sentences":["The wave nature of matter remains one of the most striking aspects of quantum mechanics.","Since its inception, a wealth of experiments has demonstrated the interference, diffraction or scattering of massive particles.","More recently, experiments with ever increasing control and resolution have allowed imaging the wavefunction of individual atoms.","Here, we use quantum gas microscopy to image the in-situ spatial distribution of deterministically prepared single-atom wave packets as they expand in a plane.","We achieve this by controllably projecting the expanding wavefunction onto the sites of a deep optical lattice and subsequently performing single-atom imaging.","The protocol established here for imaging extended wave packets via quantum gas microscopy is readily applicable to the wavefunction of interacting many-body systems in continuous space, promising a direct access to their microscopic properties, including spatial correlation functions up to high order and large distances."],"url":"http://arxiv.org/abs/2404.05699v1","category":"quant-ph"}
{"created":"2024-04-08 17:29:07","title":"BOLD v4: A Centralized Bioinformatics Platform for DNA-based Biodiversity Data","abstract":"BOLD, the Barcode of Life Data System, supports the acquisition, storage, validation, analysis, and publication of DNA barcodes, activities requiring the integration of molecular, morphological, and distributional data. Its pivotal role in curating the reference library of DNA barcodes, coupled with its data management and analysis capabilities, make it a central resource for biodiversity science. It enables rapid, accurate identification of specimens and also reveals patterns of genetic diversity and evolutionary relationships among taxa. Launched in 2005, BOLD has become an increasingly powerful tool for advancing understanding of planetary biodiversity. It currently hosts 17 million specimen records and 14 million barcodes that provide coverage for more than a million species from every continent and ocean. The platform has the long-term goal of providing a consistent, accurate system for identifying all species of eukaryotes. BOLD's integrated analytical tools, full data lifecycle support, and secure collaboration framework distinguish it from other biodiversity platforms. BOLD v4 brought enhanced data management and analysis capabilities as well as novel functionality for data dissemination and publication. Its next version will include features to strengthen its utility to the research community, governments, industry, and society-at-large.","sentences":["BOLD, the Barcode of Life Data System, supports the acquisition, storage, validation, analysis, and publication of DNA barcodes, activities requiring the integration of molecular, morphological, and distributional data.","Its pivotal role in curating the reference library of DNA barcodes, coupled with its data management and analysis capabilities, make it a central resource for biodiversity science.","It enables rapid, accurate identification of specimens and also reveals patterns of genetic diversity and evolutionary relationships among taxa.","Launched in 2005, BOLD has become an increasingly powerful tool for advancing understanding of planetary biodiversity.","It currently hosts 17 million specimen records and 14 million barcodes that provide coverage for more than a million species from every continent and ocean.","The platform has the long-term goal of providing a consistent, accurate system for identifying all species of eukaryotes.","BOLD's integrated analytical tools, full data lifecycle support, and secure collaboration framework distinguish it from other biodiversity platforms.","BOLD v4 brought enhanced data management and analysis capabilities as well as novel functionality for data dissemination and publication.","Its next version will include features to strengthen its utility to the research community, governments, industry, and society-at-large."],"url":"http://arxiv.org/abs/2404.05696v1","category":"cs.DB"}
{"created":"2024-04-08 17:10:07","title":"Global phase diagram of doped quantum spin liquid on the Kagome lattice","abstract":"It has long been believed that doped quantum spin liquids (QSLs) can give rise to fascinating quantum phases, including the possibility of high-temperature superconductivity (SC) as proposed by P. W. Anderson's resonating valence bond (RVB) scenario. The Kagome lattice $t$-$J$ model is known to exhibit spin liquid behavior at half-filling, making it an ideal system for studying the properties of doped QSL. In this study, we employ the fermionic projected entangled simplex state (PESS) method to investigate the ground state properties of the Kagome lattice $t$-$J$ model with $t/J = 3.0$. Our results reveal a phase transition from charge density wave (CDW) states to uniform states around a critical doping level $\\delta_c \\approx 0.27$. Within the CDW phase, we observe different types of Wigner crystal (WC) formulated by doped holes that are energetically favored. As we enter the uniform phase, a non-Fermi liquid (NFL) state emerges within the doping range $0.27 < \\delta < 0.32$, characterized by an exponential decay of all correlation functions. With further hole doping, we discover the appearance of a pair density wave (PDW) state within a narrow doping region $0.32 < \\delta < 1/3$. We also discuss the potential experimental implications of our findings.","sentences":["It has long been believed that doped quantum spin liquids (QSLs) can give rise to fascinating quantum phases, including the possibility of high-temperature superconductivity (SC) as proposed by P. W. Anderson's resonating valence bond (RVB) scenario.","The Kagome lattice $t$-$J$ model is known to exhibit spin liquid behavior at half-filling, making it an ideal system for studying the properties of doped QSL.","In this study, we employ the fermionic projected entangled simplex state (PESS) method to investigate the ground state properties of the Kagome lattice $t$-$J$ model with $t/J = 3.0$. Our results reveal a phase transition from charge density wave (CDW) states to uniform states around a critical doping level $\\delta_c \\approx 0.27$.","Within the CDW phase, we observe different types of Wigner crystal (WC) formulated by doped holes that are energetically favored.","As we enter the uniform phase, a non-Fermi liquid (NFL) state emerges within the doping range $0.27 < \\delta < 0.32$, characterized by an exponential decay of all correlation functions.","With further hole doping, we discover the appearance of a pair density wave (PDW) state within a narrow doping region $0.32 <","\\delta <","1/3$. We also discuss the potential experimental implications of our findings."],"url":"http://arxiv.org/abs/2404.05685v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 17:05:38","title":"Efficient formation of a massive quiescent galaxy at redshift 4.9","abstract":"Within the established framework of structure formation, galaxies start as systems of low stellar mass and gradually grow into far more massive galaxies. The existence of massive galaxies in the first billion years of the Universe, suggested by recent observations, appears to challenge this model, as such galaxies would require highly efficient conversion of baryons into stars. An even greater challenge in this epoch is the existence of massive galaxies that have already ceased forming stars. However, robust detections of early massive quiescent galaxies have been challenging due to the coarse wavelength sampling of photometric surveys. Here we report the spectroscopic confirmation with the James Webb Space Telescope of the quiescent galaxy RUBIES-EGS-QG-1 at redshift $z=4.896$, 1.2 billion years after the Big Bang. Deep stellar absorption features in the spectrum reveal that the galaxy's stellar mass of $10^{10.9}\\,M_\\odot$, corroborated by the mass implied by its gas kinematics, formed in a short $340\\,$Myr burst of star formation, after which star formation activity dropped rapidly and persistently. According to current galaxy formation models, systems with such rapid stellar mass growth and early quenching are too rare to plausibly occur in the small area probed spectroscopically with JWST. Instead, the discovery of RUBIES-EGS-QG-1 implies that early massive quiescent galaxies can be quenched earlier or exhaust gas available for star formation more efficiently than currently assumed.","sentences":["Within the established framework of structure formation, galaxies start as systems of low stellar mass and gradually grow into far more massive galaxies.","The existence of massive galaxies in the first billion years of the Universe, suggested by recent observations, appears to challenge this model, as such galaxies would require highly efficient conversion of baryons into stars.","An even greater challenge in this epoch is the existence of massive galaxies that have already ceased forming stars.","However, robust detections of early massive quiescent galaxies have been challenging due to the coarse wavelength sampling of photometric surveys.","Here we report the spectroscopic confirmation with the James Webb Space Telescope of the quiescent galaxy RUBIES-EGS-QG-1 at redshift $z=4.896$, 1.2 billion years after the Big Bang.","Deep stellar absorption features in the spectrum reveal that the galaxy's stellar mass of $10^{10.9}\\,M_\\odot$, corroborated by the mass implied by its gas kinematics, formed in a short $340\\,$Myr burst of star formation, after which star formation activity dropped rapidly and persistently.","According to current galaxy formation models, systems with such rapid stellar mass growth and early quenching are too rare to plausibly occur in the small area probed spectroscopically with JWST.","Instead, the discovery of RUBIES-EGS-QG-1 implies that early massive quiescent galaxies can be quenched earlier or exhaust gas available for star formation more efficiently than currently assumed."],"url":"http://arxiv.org/abs/2404.05683v1","category":"astro-ph.GA"}
{"created":"2024-04-08 16:58:19","title":"Overview of projective quantum measurements","abstract":"We provide an overview of standard \"projective\" quantum measurements with the goal of elucidating connections between theory and experiment. We make use of a unitary \"Stinespring\" representation of measurements on a dilated Hilbert space that includes both the physical degrees of freedom and those of the measurement apparatus. We explain how this unitary representation (i) is guaranteed by the axioms of quantum mechanics, (ii) relates to both the Kraus and von Neumann representations, and (iii) corresponds to the physical time evolution of the system and apparatus during the measurement process. The Stinespring representation also offers significant conceptual insight into measurements, helps connects theory and experiment, is particularly useful in describing protocols involving midcircuit measurements and outcome-dependent operations, and establishes that all quantum operations are compatible with relativistic locality, among other insights.","sentences":["We provide an overview of standard \"projective\" quantum measurements with the goal of elucidating connections between theory and experiment.","We make use of a unitary \"Stinespring\" representation of measurements on a dilated Hilbert space that includes both the physical degrees of freedom and those of the measurement apparatus.","We explain how this unitary representation (i) is guaranteed by the axioms of quantum mechanics, (ii) relates to both the Kraus and von Neumann representations, and (iii) corresponds to the physical time evolution of the system and apparatus during the measurement process.","The Stinespring representation also offers significant conceptual insight into measurements, helps connects theory and experiment, is particularly useful in describing protocols involving midcircuit measurements and outcome-dependent operations, and establishes that all quantum operations are compatible with relativistic locality, among other insights."],"url":"http://arxiv.org/abs/2404.05679v1","category":"quant-ph"}
{"created":"2024-04-08 16:54:41","title":"Bayesian Inverse Ising Problem with Three-body Interactions","abstract":"In this paper, we solve the inverse Ising problem with three-body interaction. Using the mean-field approximation, we find a tractable expansion of the normalizing constant. This facilitates estimation, which is known to be quite challenging for the Ising model. We then develop a novel hybrid MCMC algorithm that integrates Adaptive Metropolis Hastings (AMH), Hamiltonian Monte Carlo (HMC), and the Manifold-Adjusted Langevin Algorithm (MALA), which converges quickly and mixes well. We demonstrate the robustness of our algorithm using data simulated with a structure under which parameter estimation is known to be challenging, such as in the presence of a phase transition and at the critical point of the system.","sentences":["In this paper, we solve the inverse Ising problem with three-body interaction.","Using the mean-field approximation, we find a tractable expansion of the normalizing constant.","This facilitates estimation, which is known to be quite challenging for the Ising model.","We then develop a novel hybrid MCMC algorithm that integrates Adaptive Metropolis Hastings (AMH), Hamiltonian Monte Carlo (HMC), and the Manifold-Adjusted Langevin Algorithm (MALA), which converges quickly and mixes well.","We demonstrate the robustness of our algorithm using data simulated with a structure under which parameter estimation is known to be challenging, such as in the presence of a phase transition and at the critical point of the system."],"url":"http://arxiv.org/abs/2404.05671v1","category":"stat.ME"}
{"created":"2024-04-08 16:48:20","title":"BFS versus DFS for random targets in ordered trees","abstract":"Consider a search from the root of an ordered tree with $n$ edges to some target node at a fixed distance $\\ell$ from that root. We compare the average time complexity of the breadth-first search (BFS) and depth-first search (DFS) algorithms, when the target node is selected uniformly at random among all nodes at level $\\ell$ in the ordered trees with $n$ edges. Intuition suggests that BFS should have better average performance when $\\ell$ is small, while DFS must have an advantage when $\\ell$ is large. But where exactly is the threshold, as a function of $n$, and is it unique? We obtain explicit formulas for the expected number of steps of both BFS and DFS, by using results on the occupation measure of Brownian excursions, as well as a combinatorial proof of an identity related to lattice paths. This allows us to show that there exists a unique constant $\\lambda\\approx 0.789004$, such that in expectation BFS is asymptotically faster than DFS if and only if $\\ell\\leq \\lambda\\sqrt{n}$. Furthermore, we find the asymptotic average time complexity of BFS in the given setting for any class of Galton$\\unicode{x2013}$Watson trees, including binary trees and ordered trees. Finally, we introduce the truncated DFS algorithm, which performs better than both BFS and DFS when $\\ell$ is known in advance, and we find a formula evaluating the average time complexity of this algorithm.","sentences":["Consider a search from the root of an ordered tree with $n$ edges to some target node at a fixed distance $\\ell$ from that root.","We compare the average time complexity of the breadth-first search (BFS) and depth-first search (DFS) algorithms, when the target node is selected uniformly at random among all nodes at level $\\ell$ in the ordered trees with $n$ edges.","Intuition suggests that BFS should have better average performance when $\\ell$ is small, while DFS must have an advantage when $\\ell$ is large.","But where exactly is the threshold, as a function of $n$, and is it unique?","We obtain explicit formulas for the expected number of steps of both BFS and DFS, by using results on the occupation measure of Brownian excursions, as well as a combinatorial proof of an identity related to lattice paths.","This allows us to show that there exists a unique constant $\\lambda\\approx 0.789004$, such that in expectation BFS is asymptotically faster than DFS","if and only if $\\ell\\leq \\lambda\\sqrt{n}$.","Furthermore, we find the asymptotic average time complexity of BFS in the given setting for any class of Galton$\\unicode{x2013}$Watson trees, including binary trees and ordered trees.","Finally, we introduce the truncated DFS algorithm, which performs better than both BFS and DFS when $\\ell$ is known in advance, and we find a formula evaluating the average time complexity of this algorithm."],"url":"http://arxiv.org/abs/2404.05664v1","category":"cs.DS"}
{"created":"2024-04-08 16:35:34","title":"Realization of a three-dimensional photonic higher-order topological insulator","abstract":"The discovery of photonic higher-order topological insulators (HOTIs) has significantly expanded our understanding of band topology and provided unprecedented lower-dimensional topological boundary states for robust photonic devices. However, due to the vectorial and leaky nature of electromagnetic waves, it is challenging to discover three-dimensional (3D) topological photonic systems and photonic HOTIs have so far still been limited to two dimensions (2D). Here, we report on the first experimental realization of a 3D Wannier-type photonic HOTI in a tight-binding-like metal-cage photonic crystal, whose band structure matches well with that of a 3D tight-binding model due to the confined Mie resonances. By microwave near-field measurements, we directly observe coexisting topological surface, hinge, and corner states in a single 3D photonic HOTI, as predicted by the tight-binding model and simulation results. Moreover, we demonstrate that all-order topological boundary states are self-guided even in the light cone continuum and can be exposed to air without ancillary cladding, making them well-suited for practical applications. Our work thus opens routes to the multi-dimensional robust manipulation of electromagnetic waves at the outer surfaces of 3D cladding-free photonic bandgap materials and may find novel applications in 3D topological integrated photonics devices.","sentences":["The discovery of photonic higher-order topological insulators (HOTIs) has significantly expanded our understanding of band topology and provided unprecedented lower-dimensional topological boundary states for robust photonic devices.","However, due to the vectorial and leaky nature of electromagnetic waves, it is challenging to discover three-dimensional (3D) topological photonic systems and photonic HOTIs have so far still been limited to two dimensions (2D).","Here, we report on the first experimental realization of a 3D Wannier-type photonic HOTI in a tight-binding-like metal-cage photonic crystal, whose band structure matches well with that of a 3D tight-binding model due to the confined Mie resonances.","By microwave near-field measurements, we directly observe coexisting topological surface, hinge, and corner states in a single 3D photonic HOTI, as predicted by the tight-binding model and simulation results.","Moreover, we demonstrate that all-order topological boundary states are self-guided even in the light cone continuum and can be exposed to air without ancillary cladding, making them well-suited for practical applications.","Our work thus opens routes to the multi-dimensional robust manipulation of electromagnetic waves at the outer surfaces of 3D cladding-free photonic bandgap materials and may find novel applications in 3D topological integrated photonics devices."],"url":"http://arxiv.org/abs/2404.05649v1","category":"physics.optics"}
{"created":"2024-04-08 16:31:06","title":"Measures of maximal entropy that are SRB","abstract":"How often does it occur that the measure of maximal entropy of a system is an SRB measure? We study this question for $C^{1+\\alpha}$ partially hyperbolic diffeomorphisms isotopic to Anosov (DA-diffeomorphisms) on ${\\mathbb T}^{3}$, and establish a rigidity result: the measure of maximal entropy is an SRB measure if and only if the sum of its positive Lyapunov exponents coincides with that of the linear Anosov map $A$ on all periodic orbits of the support of the measure. In that case, the measure is also the unique physical measure.\\par We also show non-Anosov examples satisfying this condition, both in the conservative and in the non-conservative setting. \\par Finally, we prove that a volume-preserving $C^{1+\\alpha}$ DA-diffeomorphism on ${\\mathbb T}^{3}$ is Anosov if all Lyapunov exponents coincide almost everywhere with those of the linear Anosov in the isotopy class. Consequently, a smooth DA-diffeomorphism is smoothly conjugated to its linear part if and only if all Lyapunov exponents coincide almost everywhere with those of its linear part.","sentences":["How often does it occur that the measure of maximal entropy of a system is an SRB measure?","We study this question for $C^{1+\\alpha}$ partially hyperbolic diffeomorphisms isotopic to Anosov (DA-diffeomorphisms) on ${\\mathbb T}^{3}$, and establish a rigidity result: the measure of maximal entropy is an SRB measure if and only if the sum of its positive Lyapunov exponents coincides with that of the linear Anosov map $A$ on all periodic orbits of the support of the measure.","In that case, the measure is also the unique physical measure.\\par We also show non-Anosov examples satisfying this condition, both in the conservative and in the non-conservative setting.","\\par","Finally, we prove that a volume-preserving $C^{1+\\alpha}$ DA-diffeomorphism on ${\\mathbb T}^{3}$ is Anosov if all Lyapunov exponents coincide almost everywhere with those of the linear Anosov in the isotopy class.","Consequently, a smooth DA-diffeomorphism is smoothly conjugated to its linear part if and only if all Lyapunov exponents coincide almost everywhere with those of its linear part."],"url":"http://arxiv.org/abs/2404.05645v1","category":"math.DS"}
{"created":"2024-04-08 16:20:38","title":"Stability Enhancement of LCL-Type Grid-Following Inverters Using Capacitor Voltage Active Damping","abstract":"An LCL filter offers superior attenuation for high-frequency harmonics for three-phase grid-following inverters compared to LC and L filters. However, it also introduces an inherent resonance peak, which can lead to power quality issues or even instability of the inverter control system. Active damping (AD) is widely employed to effectively mitigate this resonance. Capacitor voltage feedback (CVF) and capacitor current feedback (CCF) are effective AD methods for LCL resonance damping. CVF is preferred due to its lower sensor requirement compared to CCF. However, a derivative term appears in the active damping loop, which introduces high-frequency noise into the system. This paper proposes a noise-immune approach by replacing the derivative term with a discrete function suitable for digital implementation. The LCL resonance can be damped effectively, resulting in enhanced stability of the inverter control system. Simulation results verify the proposed effectiveness of the method with grid inductance variation and weak grid conditions","sentences":["An LCL filter offers superior attenuation for high-frequency harmonics for three-phase grid-following inverters compared to LC and L filters.","However, it also introduces an inherent resonance peak, which can lead to power quality issues or even instability of the inverter control system.","Active damping (AD) is widely employed to effectively mitigate this resonance.","Capacitor voltage feedback (CVF) and capacitor current feedback (CCF) are effective AD methods for LCL resonance damping.","CVF is preferred due to its lower sensor requirement compared to CCF.","However, a derivative term appears in the active damping loop, which introduces high-frequency noise into the system.","This paper proposes a noise-immune approach by replacing the derivative term with a discrete function suitable for digital implementation.","The LCL resonance can be damped effectively, resulting in enhanced stability of the inverter control system.","Simulation results verify the proposed effectiveness of the method with grid inductance variation and weak grid conditions"],"url":"http://arxiv.org/abs/2404.05640v1","category":"eess.SY"}
{"created":"2024-04-08 16:01:25","title":"Busemann-Petty type problems on complex vector spaces","abstract":"Busemann-Petty type problems for the recently introduced complex projection, centroid and $L_p$-intersection body operators are examined. Moreover, it is shown that, as their real counterparts, they can be linked to the spherical Fourier transform.","sentences":["Busemann-Petty type problems for the recently introduced complex projection, centroid and $L_p$-intersection body operators are examined.","Moreover, it is shown that, as their real counterparts, they can be linked to the spherical Fourier transform."],"url":"http://arxiv.org/abs/2404.05630v1","category":"math.MG"}
{"created":"2024-04-08 16:00:21","title":"An oscilloscope based method for pulsed Optically Detected Magnetic Resonance in an ensemble of NV centers in diamond","abstract":"In this work, we have demonstrated the acquisition of pulsed ODMR data with the help of a high speed oscilloscope. We show how the on-system data averaging and the memory of the oscilloscope can be utilized to obtain high SNR data at high speeds. The problem of experimental drift and the strategy for eliminating the same is discussed. Two distinct methods of data acquisition and processing are discussed, and their applicability to different pulsed protocols is investigated. Rabi Oscillations, Ramsey Interferometry, T1 measurement, and spin echo are demonstrated.","sentences":["In this work, we have demonstrated the acquisition of pulsed ODMR data with the help of a high speed oscilloscope.","We show how the on-system data averaging and the memory of the oscilloscope can be utilized to obtain high SNR data at high speeds.","The problem of experimental drift and the strategy for eliminating the same is discussed.","Two distinct methods of data acquisition and processing are discussed, and their applicability to different pulsed protocols is investigated.","Rabi Oscillations, Ramsey Interferometry, T1 measurement, and spin echo are demonstrated."],"url":"http://arxiv.org/abs/2404.05629v1","category":"quant-ph"}
{"created":"2024-04-08 15:59:47","title":"OtterROS: Picking and Programming an Uncrewed Surface Vessel for Experimental Field Robotics Research with ROS 2","abstract":"There exist a wide range of options for field robotics research using ground and aerial mobile robots, but there are comparatively few robust and research-ready uncrewed surface vessels (USVs). This workshop paper starts with a snapshot of USVs currently available to the research community and then describes \"OtterROS\", an open source ROS 2 solution for the Otter USV. Field experiments using OtterROS are described, which highlight the utility of the Otter USV and the benefits of using ROS 2 in aquatic robotics research. For those interested in USV research, the paper details recommended hardware to run OtterROS and includes an example ROS 2 package using OtterROS, removing unnecessary non-recurring engineering from field robotics research activities.","sentences":["There exist a wide range of options for field robotics research using ground and aerial mobile robots, but there are comparatively few robust and research-ready uncrewed surface vessels (USVs).","This workshop paper starts with a snapshot of USVs currently available to the research community and then describes \"OtterROS\", an open source ROS 2 solution for the Otter USV.","Field experiments using OtterROS are described, which highlight the utility of the Otter USV and the benefits of using ROS 2 in aquatic robotics research.","For those interested in USV research, the paper details recommended hardware to run OtterROS and includes an example ROS 2 package using OtterROS, removing unnecessary non-recurring engineering from field robotics research activities."],"url":"http://arxiv.org/abs/2404.05627v1","category":"cs.RO"}
{"created":"2024-04-08 15:57:31","title":"Robust Control using Control Lyapunov Function and Hamilton-Jacobi Reachability","abstract":"The paper presents a robust control technique that combines the Control Lyapunov function and Hamilton-Jacobi Reachability to compute a controller and its Region of Attraction (ROA). The Control Lyapunov function uses a linear system model with an assumed additive uncertainty to calculate a control gain and the level sets of the ROA as a function of the uncertainty. Next, Hamilton-Jacobi reachability uses the nonlinear model with the modeled uncertainty, which need not be additive, to compute the backward reachable set (BRS). Finally, by juxtaposing the level sets of the ROA with BRS, we can calculate the worst-case additive disturbance and the ROA of the nonlinear model. We illustrate our approach on a 2D quadcopter tracking trajectory and a 2D quadcopter with height and velocity regulation in simulation.","sentences":["The paper presents a robust control technique that combines the Control Lyapunov function and Hamilton-Jacobi Reachability to compute a controller and its Region of Attraction (ROA).","The Control Lyapunov function uses a linear system model with an assumed additive uncertainty to calculate a control gain and the level sets of the ROA as a function of the uncertainty.","Next, Hamilton-Jacobi reachability uses the nonlinear model with the modeled uncertainty, which need not be additive, to compute the backward reachable set (BRS).","Finally, by juxtaposing the level sets of the ROA with BRS, we can calculate the worst-case additive disturbance and the ROA of the nonlinear model.","We illustrate our approach on a 2D quadcopter tracking trajectory and a 2D quadcopter with height and velocity regulation in simulation."],"url":"http://arxiv.org/abs/2404.05625v1","category":"cs.RO"}
{"created":"2024-04-08 15:53:29","title":"How to Evaluate Entity Resolution Systems: An Entity-Centric Framework with Application to Inventor Name Disambiguation","abstract":"Entity resolution (record linkage, microclustering) systems are notoriously difficult to evaluate. Looking for a needle in a haystack, traditional evaluation methods use sophisticated, application-specific sampling schemes to find matching pairs of records among an immense number of non-matches. We propose an alternative that facilitates the creation of representative, reusable benchmark data sets without necessitating complex sampling schemes. These benchmark data sets can then be used for model training and a variety of evaluation tasks. Specifically, we propose an entity-centric data labeling methodology that integrates with a unified framework for monitoring summary statistics, estimating key performance metrics such as cluster and pairwise precision and recall, and analyzing root causes for errors. We validate the framework in an application to inventor name disambiguation and through simulation studies. Software: https://github.com/OlivierBinette/er-evaluation/","sentences":["Entity resolution (record linkage, microclustering) systems are notoriously difficult to evaluate.","Looking for a needle in a haystack, traditional evaluation methods use sophisticated, application-specific sampling schemes to find matching pairs of records among an immense number of non-matches.","We propose an alternative that facilitates the creation of representative, reusable benchmark data sets without necessitating complex sampling schemes.","These benchmark data sets can then be used for model training and a variety of evaluation tasks.","Specifically, we propose an entity-centric data labeling methodology that integrates with a unified framework for monitoring summary statistics, estimating key performance metrics such as cluster and pairwise precision and recall, and analyzing root causes for errors.","We validate the framework in an application to inventor name disambiguation and through simulation studies.","Software: https://github.com/OlivierBinette/er-evaluation/"],"url":"http://arxiv.org/abs/2404.05622v1","category":"cs.CL"}
{"created":"2024-04-08 15:33:32","title":"Feedback Stability Under Mixed Gain and Phase Uncertainty","abstract":"In this study, we investigate the robust feedback stability problem for multiple-input-multiple-output linear time-invariant systems involving sectored-disk uncertainty, namely, dynamic uncertainty subject to simultaneous gain and phase constraints. This problem is thereby called a sectored-disk problem. Employing a frequency-wise analysis approach, we derive a fundamental static matrix problem that serves as a key component in addressing the feedback stability. The study of this matrix problem heavily relies on the Davis-Wielandt (DW) shells of matrices, providing a profound insight into matrices subjected to simultaneous gain and phase constraints. This understanding is pivotal for establishing a less conservative sufficient condition for the matrix sectored-disk problem, from which we formulate several robust feedback stability conditions against sectored-disk uncertainty. Finally, several conditions based on linear matrix inequalities are developed for efficient computation and verification of feedback robust stability against sectored-disk uncertainty.","sentences":["In this study, we investigate the robust feedback stability problem for multiple-input-multiple-output linear time-invariant systems involving sectored-disk uncertainty, namely, dynamic uncertainty subject to simultaneous gain and phase constraints.","This problem is thereby called a sectored-disk problem.","Employing a frequency-wise analysis approach, we derive a fundamental static matrix problem that serves as a key component in addressing the feedback stability.","The study of this matrix problem heavily relies on the Davis-Wielandt (DW) shells of matrices, providing a profound insight into matrices subjected to simultaneous gain and phase constraints.","This understanding is pivotal for establishing a less conservative sufficient condition for the matrix sectored-disk problem, from which we formulate several robust feedback stability conditions against sectored-disk uncertainty.","Finally, several conditions based on linear matrix inequalities are developed for efficient computation and verification of feedback robust stability against sectored-disk uncertainty."],"url":"http://arxiv.org/abs/2404.05609v1","category":"math.OC"}
{"created":"2024-04-08 15:25:50","title":"Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction","abstract":"Face meshes in consistent topology serve as the foundation for many face-related applications, such as 3DMM constrained face reconstruction and expression retargeting. Traditional methods commonly acquire topology uniformed face meshes by two separate steps: multi-view stereo (MVS) to reconstruct shapes followed by non-rigid registration to align topology, but struggles with handling noise and non-lambertian surfaces. Recently neural volume rendering techniques have been rapidly evolved and shown great advantages in 3D reconstruction or novel view synthesis. Our goal is to leverage the superiority of neural volume rendering into multi-view reconstruction of face mesh with consistent topology. We propose a mesh volume rendering method that enables directly optimizing mesh geometry while preserving topology, and learning implicit features to model complex facial appearance from multi-view images. The key innovation lies in spreading sparse mesh features into the surrounding space to simulate radiance field required for volume rendering, which facilitates backpropagation of gradients from images to mesh geometry and implicit appearance features. Our proposed feature spreading module exhibits deformation invariance, enabling photorealistic rendering seamlessly after mesh editing. We conduct experiments on multi-view face image dataset to evaluate the reconstruction and implement an application for photorealistic rendering of animated face mesh.","sentences":["Face meshes in consistent topology serve as the foundation for many face-related applications, such as 3DMM constrained face reconstruction and expression retargeting.","Traditional methods commonly acquire topology uniformed face meshes by two separate steps: multi-view stereo (MVS) to reconstruct shapes followed by non-rigid registration to align topology, but struggles with handling noise and non-lambertian surfaces.","Recently neural volume rendering techniques have been rapidly evolved and shown great advantages in 3D reconstruction or novel view synthesis.","Our goal is to leverage the superiority of neural volume rendering into multi-view reconstruction of face mesh with consistent topology.","We propose a mesh volume rendering method that enables directly optimizing mesh geometry while preserving topology, and learning implicit features to model complex facial appearance from multi-view images.","The key innovation lies in spreading sparse mesh features into the surrounding space to simulate radiance field required for volume rendering, which facilitates backpropagation of gradients from images to mesh geometry and implicit appearance features.","Our proposed feature spreading module exhibits deformation invariance, enabling photorealistic rendering seamlessly after mesh editing.","We conduct experiments on multi-view face image dataset to evaluate the reconstruction and implement an application for photorealistic rendering of animated face mesh."],"url":"http://arxiv.org/abs/2404.05606v1","category":"cs.CV"}
{"created":"2024-04-08 15:22:03","title":"AI-Enabled System for Efficient and Effective Cyber Incident Detection and Response in Cloud Environments","abstract":"The escalating sophistication and volume of cyber threats in cloud environments necessitate a paradigm shift in strategies. Recognising the need for an automated and precise response to cyber threats, this research explores the application of AI and ML and proposes an AI-powered cyber incident response system for cloud environments. This system, encompassing Network Traffic Classification, Web Intrusion Detection, and post-incident Malware Analysis (built as a Flask application), achieves seamless integration across platforms like Google Cloud and Microsoft Azure. The findings from this research highlight the effectiveness of the Random Forest model, achieving an accuracy of 90% for the Network Traffic Classifier and 96% for the Malware Analysis Dual Model application. Our research highlights the strengths of AI-powered cyber security. The Random Forest model excels at classifying cyber threats, offering an efficient and robust solution. Deep learning models significantly improve accuracy, and their resource demands can be managed using cloud-based TPUs and GPUs. Cloud environments themselves provide a perfect platform for hosting these AI/ML systems, while container technology ensures both efficiency and scalability. These findings demonstrate the contribution of the AI-led system in guaranteeing a robust and scalable cyber incident response solution in the cloud.","sentences":["The escalating sophistication and volume of cyber threats in cloud environments necessitate a paradigm shift in strategies.","Recognising the need for an automated and precise response to cyber threats, this research explores the application of AI and ML and proposes an AI-powered cyber incident response system for cloud environments.","This system, encompassing Network Traffic Classification, Web Intrusion Detection, and post-incident Malware Analysis (built as a Flask application), achieves seamless integration across platforms like Google Cloud and Microsoft Azure.","The findings from this research highlight the effectiveness of the Random Forest model, achieving an accuracy of 90% for the Network Traffic Classifier and 96% for the Malware Analysis Dual Model application.","Our research highlights the strengths of AI-powered cyber security.","The Random Forest model excels at classifying cyber threats, offering an efficient and robust solution.","Deep learning models significantly improve accuracy, and their resource demands can be managed using cloud-based TPUs and GPUs.","Cloud environments themselves provide a perfect platform for hosting these AI/ML systems, while container technology ensures both efficiency and scalability.","These findings demonstrate the contribution of the AI-led system in guaranteeing a robust and scalable cyber incident response solution in the cloud."],"url":"http://arxiv.org/abs/2404.05602v1","category":"cs.CR"}
{"created":"2024-04-08 14:54:03","title":"On the Stability of swelling porous elastic soils with a single internal fractional damping","abstract":"We study polynomial stability to the one-dimensional system in the linear isothermal theory of swelling porous elastic soils with an internal fractional damping. We establish an optimal decay result by frequency domain method","sentences":["We study polynomial stability to the one-dimensional system in the linear isothermal theory of swelling porous elastic soils with an internal fractional damping.","We establish an optimal decay result by frequency domain method"],"url":"http://arxiv.org/abs/2404.05577v1","category":"math.AP"}
{"created":"2024-04-08 14:46:32","title":"Wetting on Silicone Surfaces","abstract":"Silicone is frequently used as a model system to investigate and tune wetting on soft materials. Silicone is biocompatible and shows excellent thermal, chemical, and UV stability. Moreover, the mechanical properties of the surface can be easily varied by several orders of magnitude in a controlled manner. Polydimethylsiloxane (PDMS) is a popular choice for coating applications such as lubrication, self-cleaning, and drag reduction, facilitated by low surface energy. Aiming to understand the underlying interactions and forces, motivated numerous and detailed investigations of the static and dynamic wetting behavior of drops on PDMS-based surfaces. Here, we recognize the three most prevalent PDMS surface variants, namely liquid-infused (SLIPS/LIS), elastomeric, and liquid-like (SOCAL) surfaces. To understand, optimize, and tune the wetting properties of these PDMS surfaces, we review and compare their similarities and differences by discussing (i) the chemical and molecular structure, and (ii) the static and dynamic wetting behavior. We also provide (iii) an overview of methods and techniques to characterize PDMS-based surfaces and their wetting behavior. The static and dynamic wetting ridge is given particular attention, as it dominates energy dissipation, adhesion, and friction of sliding drops and influences the durability of the surfaces. We also discuss special features such as cloaking and wetting-induced phase separation. Key challenges and opportunities of these three surface variants are outlined.","sentences":["Silicone is frequently used as a model system to investigate and tune wetting on soft materials.","Silicone is biocompatible and shows excellent thermal, chemical, and UV stability.","Moreover, the mechanical properties of the surface can be easily varied by several orders of magnitude in a controlled manner.","Polydimethylsiloxane (PDMS) is a popular choice for coating applications such as lubrication, self-cleaning, and drag reduction, facilitated by low surface energy.","Aiming to understand the underlying interactions and forces, motivated numerous and detailed investigations of the static and dynamic wetting behavior of drops on PDMS-based surfaces.","Here, we recognize the three most prevalent PDMS surface variants, namely liquid-infused (SLIPS/LIS), elastomeric, and liquid-like (SOCAL) surfaces.","To understand, optimize, and tune the wetting properties of these PDMS surfaces, we review and compare their similarities and differences by discussing (i) the chemical and molecular structure, and (ii) the static and dynamic wetting behavior.","We also provide (iii) an overview of methods and techniques to characterize PDMS-based surfaces and their wetting behavior.","The static and dynamic wetting ridge is given particular attention, as it dominates energy dissipation, adhesion, and friction of sliding drops and influences the durability of the surfaces.","We also discuss special features such as cloaking and wetting-induced phase separation.","Key challenges and opportunities of these three surface variants are outlined."],"url":"http://arxiv.org/abs/2404.05571v1","category":"cond-mat.soft"}
{"created":"2024-04-08 14:37:35","title":"Extremal problems in BMO and VMO involving the Garsia norm","abstract":"Given an $L^2$ function $f$ on the unit circle $\\mathbb T$, we put $$\\Phi_f(z):=\\mathcal P(|f|^2)(z)-|\\mathcal Pf(z)|^2,\\qquad z\\in\\mathbb D,$$ where $\\mathbb D$ is the open unit disk and $\\mathcal P$ is the Poisson integral operator. The Garsia norm $\\|f\\|_G$ is then defined as $\\sup_{z\\in\\mathbb D}\\left(\\Phi_f(z)\\right)^{1/2}$, and the space ${\\rm BMO}$ is formed by the functions $f\\in L^2$ with $\\|f\\|_G<\\infty$. If $\\|f\\|^2_G=\\Phi_f(z_0)$ for some point $z_0\\in\\mathbb D$, then $f$ is said to be a norm-attaining ${\\rm BMO}$ function, written as $f\\in{\\rm BMO}_{\\rm na}$. Note that ${\\rm BMO}_{\\rm na}$ contains ${\\rm VMO}$, the space of functions with vanishing mean oscillation. We study, first, the functions $f$ in $L^\\infty$ (as well as in $L^\\infty\\cap{\\rm BMO}_{\\rm na}$) with the property that $\\|f\\|_G=\\|f\\|_\\infty$. The analytic case, where $L^\\infty$ gets replaced by $H^\\infty$, is discussed in more detail. Secondly, we prove that every function $f\\in{\\rm BMO}_{\\rm na}$ with $\\|f\\|_G=1$ is an extreme point of ${\\rm ball}\\,({\\rm BMO})$, the unit ball of ${\\rm BMO}$ with respect to the Garsia norm. This implies that the extreme points of ${\\rm ball}\\,({\\rm VMO})$ are precisely the unit-norm ${\\rm VMO}$ functions. As another consequence, we arrive at an amusing \"geometric\" characterization of inner functions.","sentences":["Given an $L^2$ function $f$ on the unit circle $\\mathbb T$, we put $$\\Phi_f(z):=\\mathcal P(|f|^2)(z)-|\\mathcal Pf(z)|^2,\\qquad","z\\in\\mathbb D,$$ where $\\mathbb D$ is the open unit disk and $\\mathcal P$ is the Poisson integral operator.","The Garsia norm $\\|f\\|_G$ is then defined as $\\sup_{z\\in\\mathbb D}\\left(\\Phi_f(z)\\right)^{1/2}$, and the space ${\\rm BMO}$ is formed by the functions $f\\in L^2$ with $\\|f\\|_G<\\infty$. If $\\|f\\|^2_G=\\Phi_f(z_0)$ for some point $z_0\\in\\mathbb D$, then $f$ is said to be a norm-attaining ${\\rm BMO}$ function, written as $f\\in{\\rm BMO}_{\\rm na}$. Note that ${\\rm BMO}_{\\rm na}$ contains ${\\rm VMO}$, the space of functions with vanishing mean oscillation.","We study, first, the functions $f$ in $L^\\infty$ (as well as in $L^\\infty\\cap{\\rm BMO}_{\\rm na}$) with the property that $\\|f\\|_G=\\|f\\|_\\infty$. The analytic case, where $L^\\infty$ gets replaced by $H^\\infty$, is discussed in more detail.","Secondly, we prove that every function $f\\in{\\rm BMO}_{\\rm na}$ with $\\|f\\|_G=1$ is an extreme point of ${\\rm ball}\\,({\\rm BMO})$, the unit ball of ${\\rm BMO}$ with respect to the Garsia norm.","This implies that the extreme points of ${\\rm ball}\\,({\\rm VMO})$ are precisely the unit-norm ${\\rm VMO}$ functions.","As another consequence, we arrive at an amusing \"geometric\" characterization of inner functions."],"url":"http://arxiv.org/abs/2404.05565v1","category":"math.CV"}
{"created":"2024-04-08 14:36:45","title":"Predefined Software Environment Runtimes As A Measure For Reproducibility","abstract":"As part of Mathematical Research Data Initiative (MaRDI), we have developed a way to preserve a software package into an easy to deploy and use sandbox environment we call a \"runtime\", via a program we developed called MaPS : MaRDI Packaging System. The program relies on Linux user namespaces to isolate a library environment from the host system, making the sandboxed software reproducible on other systems, with minimal effort. Moreover an overlay filesystem makes local edits persistent. This project will aid reproducibility efforts of research papers: both mathematical and from other disciplines. As a proof of concept, we provide runtimes for the OSCAR Computer Algebra System, polymake software for research in polyhedral geometry, and VIBRANT Virus Identification By iteRative ANnoTation. The software is in a prerelease state: the interface for creating, deploying, and executing runtimes is final, and an interface for easily publishing runtimes is under active development. We thus propose publishing predefined, distributable software environment runtimes along with research papers in an effort to make research with software based results reproducible.","sentences":["As part of Mathematical Research Data Initiative (MaRDI), we have developed a way to preserve a software package into an easy to deploy and use sandbox environment we call a \"runtime\", via a program we developed called MaPS : MaRDI Packaging System.","The program relies on Linux user namespaces to isolate a library environment from the host system, making the sandboxed software reproducible on other systems, with minimal effort.","Moreover an overlay filesystem makes local edits persistent.","This project will aid reproducibility efforts of research papers: both mathematical and from other disciplines.","As a proof of concept, we provide runtimes for the OSCAR Computer Algebra System, polymake software for research in polyhedral geometry, and VIBRANT Virus Identification By iteRative ANnoTation.","The software is in a prerelease state: the interface for creating, deploying, and executing runtimes is final, and an interface for easily publishing runtimes is under active development.","We thus propose publishing predefined, distributable software environment runtimes along with research papers in an effort to make research with software based results reproducible."],"url":"http://arxiv.org/abs/2404.05563v1","category":"cs.MS"}
{"created":"2024-04-08 14:28:41","title":"Bathymetry reconstruction from experimental data using PDE-constrained optimisation","abstract":"Knowledge of the bottom topography, also called bathymetry, of rivers, seas or the ocean is important for many areas of maritime science and civil engineering. While direct measurements are possible, they are time consuming and expensive. Therefore, many approaches have been proposed how to infer the bathymetry from measurements of surface waves. Mathematically, this is an inverse problem where an unknown system state needs to be reconstructed from observations with a suitable model for the flow as constraint. In many cases, the shallow water equations can be used to describe the flow. While theoretical studies of the efficacy of such a PDE-constrained optimisation approach for bathymetry reconstruction exist, there seem to be few publications that study its application to data obtained from real-world measurements. This paper shows that the approach can, at least qualitatively, reconstruct a Gaussian-shaped bathymetry in a wave flume from measurements of the water height at up to three points. Achieved normalized root mean square errors (NRMSE) are in line with other approaches.","sentences":["Knowledge of the bottom topography, also called bathymetry, of rivers, seas or the ocean is important for many areas of maritime science and civil engineering.","While direct measurements are possible, they are time consuming and expensive.","Therefore, many approaches have been proposed how to infer the bathymetry from measurements of surface waves.","Mathematically, this is an inverse problem where an unknown system state needs to be reconstructed from observations with a suitable model for the flow as constraint.","In many cases, the shallow water equations can be used to describe the flow.","While theoretical studies of the efficacy of such a PDE-constrained optimisation approach for bathymetry reconstruction exist, there seem to be few publications that study its application to data obtained from real-world measurements.","This paper shows that the approach can, at least qualitatively, reconstruct a Gaussian-shaped bathymetry in a wave flume from measurements of the water height at up to three points.","Achieved normalized root mean square errors (NRMSE) are in line with other approaches."],"url":"http://arxiv.org/abs/2404.05556v1","category":"math.NA"}
{"created":"2024-04-08 14:18:24","title":"Quasirandom and quasisimple groups","abstract":"Fix $\\varepsilon>0$. We say that a finite group $G$ is $\\varepsilon$-quasirandom if every nontrivial irreducible complex representation of $G$ has degree at least $|G|^\\varepsilon$. In this paper, we give a structure theorem for large $\\varepsilon$-quasirandom groups, and we completely classify the $\\frac{1}{5}$-quasirandom groups.","sentences":["Fix $\\varepsilon>0$. We say that a finite group $G$ is $\\varepsilon$-quasirandom if every nontrivial irreducible complex representation of $G$ has degree at least $|G|^\\varepsilon$. In this paper, we give a structure theorem for large $\\varepsilon$-quasirandom groups, and we completely classify the $\\frac{1}{5}$-quasirandom groups."],"url":"http://arxiv.org/abs/2404.05550v1","category":"math.GR"}
{"created":"2024-04-08 14:18:07","title":"Perspective and open problems on birational properties and singularities of moduli scheme of sheaves on surfaces","abstract":"For complex projective smooth surface $X$, let $M$ be the coarse moduli scheme of rank-two stable sheaves with fixed Chern classes. Grasping the birational structure of $M$, for example its Kodaira dimension, is a fundamental problem. However, in the case where $\\kappa(X)>0$, the study of this problem has not necessarily been active in recent years. In this article we survey the study of this problem, especially for the case where $\\kappa(X)=1$ and $c_1=0$. We will also survey some research on the structure of singularities of $M$, and a minimal model program of $M$. While explaining motivations, we raise several unsolved problems.","sentences":["For complex projective smooth surface $X$, let $M$ be the coarse moduli scheme of rank-two stable sheaves with fixed Chern classes.","Grasping the birational structure of $M$, for example its Kodaira dimension, is a fundamental problem.","However, in the case where $\\kappa(X)>0$, the study of this problem has not necessarily been active in recent years.","In this article we survey the study of this problem, especially for the case where $\\kappa(X)=1$ and $c_1=0$. We will also survey some research on the structure of singularities of $M$, and a minimal model program of $M$. While explaining motivations, we raise several unsolved problems."],"url":"http://arxiv.org/abs/2404.05549v1","category":"math.AG"}
{"created":"2024-04-08 14:06:52","title":"Cell-Free Multi-User MIMO Equalization via In-Context Learning","abstract":"Large pre-trained sequence models, such as transformers, excel as few-shot learners capable of in-context learning (ICL). In ICL, a model is trained to adapt its operation to a new task based on limited contextual information, typically in the form of a few training examples for the given task. Previous work has explored the use of ICL for channel equalization in single-user multi-input and multiple-output (MIMO) systems. In this work, we demonstrate that ICL can be also used to tackle the problem of multi-user equalization in cell-free MIMO systems with limited fronthaul capacity. In this scenario, a task is defined by channel statistics, signal-to-noise ratio, and modulation schemes. The context encompasses the users' pilot sequences, the corresponding quantized received signals, and the current received data signal. Different prompt design strategies are proposed and evaluated that encompass also large-scale fading and modulation information. Experiments demonstrate that ICL-based equalization provides estimates with lower mean squared error as compared to the linear minimum mean squared error equalizer, especially in the presence of limited fronthaul capacity and pilot contamination.","sentences":["Large pre-trained sequence models, such as transformers, excel as few-shot learners capable of in-context learning (ICL).","In ICL, a model is trained to adapt its operation to a new task based on limited contextual information, typically in the form of a few training examples for the given task.","Previous work has explored the use of ICL for channel equalization in single-user multi-input and multiple-output (MIMO) systems.","In this work, we demonstrate that ICL can be also used to tackle the problem of multi-user equalization in cell-free MIMO systems with limited fronthaul capacity.","In this scenario, a task is defined by channel statistics, signal-to-noise ratio, and modulation schemes.","The context encompasses the users' pilot sequences, the corresponding quantized received signals, and the current received data signal.","Different prompt design strategies are proposed and evaluated that encompass also large-scale fading and modulation information.","Experiments demonstrate that ICL-based equalization provides estimates with lower mean squared error as compared to the linear minimum mean squared error equalizer, especially in the presence of limited fronthaul capacity and pilot contamination."],"url":"http://arxiv.org/abs/2404.05538v1","category":"cs.IT"}
{"created":"2024-04-08 14:06:42","title":"Distributing Arbitrary Quantum Cluster States by Graph Transformation","abstract":"Quantum cluster state is a special class of nonlocal state among multiple quantum particles, underpinning several nonclassical and promising applications such as quantum computing and quantum secret sharing. Recently, establishing quantum cluster states among physically distant nodes has gained increasing popularity owing to its potential in expanding current quantum applications in scale. Existing research on this topic relies on a two-step approach: first distributing low-dimension elementary entanglement to target nodes, and then fusing them into a high-dimension quantum cluster state. However, most existing studies focus solely on minimizing costs (e.g., the number of elementary entanglements consumed) to entangle target nodes, while neglecting the structure of the final quantum cluster state. This can easily result in weak system entanglement, jeopardizing the cluster state under partial measurement or noises.   In this paper, we aim to establish any arbitrary quantum cluster states of strong entanglement structures at a much lower cost than the state of the art. The method is to search for and establish an alternative state to the target state that is of lowest cost in creation. Subsequently, we transform such an alternative state back to the target state via compressed single-qubit Clifford operations. To verify the performance of our developed algorithm, we conduct comprehensive simulations based on an open dataset containing all cluster state structures up to 8 qubits. The results demonstrate fast algorithm convergence, an increased success probability in distributing any cluster states, and 53.57% saving in ERP cost compared with the state-of-the-art baseline.","sentences":["Quantum cluster state is a special class of nonlocal state among multiple quantum particles, underpinning several nonclassical and promising applications such as quantum computing and quantum secret sharing.","Recently, establishing quantum cluster states among physically distant nodes has gained increasing popularity owing to its potential in expanding current quantum applications in scale.","Existing research on this topic relies on a two-step approach: first distributing low-dimension elementary entanglement to target nodes, and then fusing them into a high-dimension quantum cluster state.","However, most existing studies focus solely on minimizing costs (e.g., the number of elementary entanglements consumed) to entangle target nodes, while neglecting the structure of the final quantum cluster state.","This can easily result in weak system entanglement, jeopardizing the cluster state under partial measurement or noises.   ","In this paper, we aim to establish any arbitrary quantum cluster states of strong entanglement structures at a much lower cost than the state of the art.","The method is to search for and establish an alternative state to the target state that is of lowest cost in creation.","Subsequently, we transform such an alternative state back to the target state via compressed single-qubit Clifford operations.","To verify the performance of our developed algorithm, we conduct comprehensive simulations based on an open dataset containing all cluster state structures up to 8 qubits.","The results demonstrate fast algorithm convergence, an increased success probability in distributing any cluster states, and 53.57% saving in ERP cost compared with the state-of-the-art baseline."],"url":"http://arxiv.org/abs/2404.05537v1","category":"quant-ph"}
{"created":"2024-04-08 13:59:07","title":"Provably Convergent and Robust Newton-Raphson Method: A New Dawn in Primitive Variable Recovery for Relativistic MHD","abstract":"A long-standing and formidable challenge faced by all conservative schemes for relativistic magnetohydrodynamics (RMHD) is the recovery of primitive variables from conservative ones. This process involves solving highly nonlinear equations subject to physical constraints. An ideal solver should be \"robust, accurate, and fast -- it is at the heart of all conservative RMHD schemes,\" as emphasized in [S.C. Noble et al., ApJ, 641:626-637, 2006]. Despite over three decades of research, seeking efficient solvers that can provably guarantee stability and convergence remains an open problem.   This paper presents the first theoretical analysis for designing a robust, physical-constraint-preserving (PCP), and provably (quadratically) convergent Newton-Raphson (NR) method for primitive variable recovery in RMHD. Our key innovation is a unified approach for the initial guess, devised based on sophisticated analysis. It ensures that the NR iteration consistently converges and adheres to physical constraints. Given the extreme nonlinearity and complexity of the iterative function, the theoretical analysis is highly nontrivial and technical. We discover a pivotal inequality for delineating the convexity and concavity of the iterative function and establish theories to guarantee the PCP property and convergence. We also develop theories to determine a computable initial guess within a theoretical \"safe\" interval. Intriguingly, we find that the unique positive root of a cubic polynomial always falls within this interval. Our PCP NR method is versatile and can be seamlessly integrated into any RMHD scheme that requires the recovery of primitive variables, potentially leading to a broad impact in this field. As an application, we incorporate it into a discontinuous Galerkin method, resulting in fully PCP schemes. Several numerical experiments demonstrate the efficiency and robustness of the PCP NR method.","sentences":["A long-standing and formidable challenge faced by all conservative schemes for relativistic magnetohydrodynamics (RMHD) is the recovery of primitive variables from conservative ones.","This process involves solving highly nonlinear equations subject to physical constraints.","An ideal solver should be \"robust, accurate, and fast -- it is at the heart of all conservative RMHD schemes,\" as emphasized in [S.C. Noble et al., ApJ, 641:626-637, 2006].","Despite over three decades of research, seeking efficient solvers that can provably guarantee stability and convergence remains an open problem.   ","This paper presents the first theoretical analysis for designing a robust, physical-constraint-preserving (PCP), and provably (quadratically) convergent Newton-Raphson (NR) method for primitive variable recovery in RMHD.","Our key innovation is a unified approach for the initial guess, devised based on sophisticated analysis.","It ensures that the NR iteration consistently converges and adheres to physical constraints.","Given the extreme nonlinearity and complexity of the iterative function, the theoretical analysis is highly nontrivial and technical.","We discover a pivotal inequality for delineating the convexity and concavity of the iterative function and establish theories to guarantee the PCP property and convergence.","We also develop theories to determine a computable initial guess within a theoretical \"safe\" interval.","Intriguingly, we find that the unique positive root of a cubic polynomial always falls within this interval.","Our PCP NR method is versatile and can be seamlessly integrated into any RMHD scheme that requires the recovery of primitive variables, potentially leading to a broad impact in this field.","As an application, we incorporate it into a discontinuous Galerkin method, resulting in fully PCP schemes.","Several numerical experiments demonstrate the efficiency and robustness of the PCP NR method."],"url":"http://arxiv.org/abs/2404.05531v1","category":"math.NA"}
{"created":"2024-04-08 13:52:06","title":"NAND-like SOT-MRAM-based Approximate Storage for Error-Tolerant Applications","abstract":"We demonstrate approximate storage based on NAND-like spin-orbit torque (SOT) MRAM, through \"device-modeling-architecture\" explorations. We experimentally achieve down to 1E-5 level selectivity. Selectivity and low-power solutions are established by numerical calculation workflow. System-level power consumption is evaluated in the 512 KB last-level cache according to 5 quality levels. Error-tolerant applications, such as image processing, alleviate the demand for selectivity down to the 5E-2 level, leading to 54% ~ 61% energy-saving. Our proposal paves the novel and suitable path for high-density and low-power NAND-like SOT-MRAM.","sentences":["We demonstrate approximate storage based on NAND-like spin-orbit torque (SOT) MRAM, through \"device-modeling-architecture\" explorations.","We experimentally achieve down to 1E-5 level selectivity.","Selectivity and low-power solutions are established by numerical calculation workflow.","System-level power consumption is evaluated in the 512 KB last-level cache according to 5 quality levels.","Error-tolerant applications, such as image processing, alleviate the demand for selectivity down to the 5E-2 level, leading to 54% ~ 61% energy-saving.","Our proposal paves the novel and suitable path for high-density and low-power NAND-like SOT-MRAM."],"url":"http://arxiv.org/abs/2404.05528v1","category":"physics.app-ph"}
{"created":"2024-04-08 13:51:02","title":"Entanglement bounds for single-excitation energy eigenstates of quantum oscillator systems","abstract":"We provide an analytic method for estimating the entanglement of the non-gaussian energy eigenstates of disordered harmonic oscillator systems. We invoke the explicit formulas of the eigenstates of the oscillator systems to establish bounds for their $\\epsilon$-R\\'enyi entanglement entropy $\\epsilon\\in(0,1)$. Our main result is a logarithmically corrected area law for the entanglement of eigenstates, corresponding to one excitation, of the disordered harmonic oscillator systems.","sentences":["We provide an analytic method for estimating the entanglement of the non-gaussian energy eigenstates of disordered harmonic oscillator systems.","We invoke the explicit formulas of the eigenstates of the oscillator systems to establish bounds for their $\\epsilon$-R\\'enyi entanglement entropy $\\epsilon\\in(0,1)$. Our main result is a logarithmically corrected area law for the entanglement of eigenstates, corresponding to one excitation, of the disordered harmonic oscillator systems."],"url":"http://arxiv.org/abs/2404.05527v1","category":"math-ph"}
{"created":"2024-04-08 13:50:50","title":"Hamiltonian Learning using Machine Learning Models Trained with Continuous Measurements","abstract":"We build upon recent work on using Machine Learning models to estimate Hamiltonian parameters using continuous weak measurement of qubits as input. We consider two settings for the training of our model: (1) supervised learning where the weak measurement training record can be labeled with known Hamiltonian parameters, and (2) unsupervised learning where no labels are available. The first has the advantage of not requiring an explicit representation of the quantum state, thus potentially scaling very favorably to larger number of qubits. The second requires the implementation of a physical model to map the Hamiltonian parameters to a measurement record, which we implement using an integrator of the physical model with a recurrent neural network to provide a model-free correction at every time step to account for small effects not captured by the physical model. We test our construction on a system of two qubits and demonstrate accurate prediction of multiple physical parameters in both the supervised and unsupervised context. We demonstrate that the model benefits from larger training sets establishing that it is in fact \"learning,\" and we show robustness to errors in the assumed physical model by achieving accurate parameter estimation in the presence of unanticipated single particle relaxation.","sentences":["We build upon recent work on using Machine Learning models to estimate Hamiltonian parameters using continuous weak measurement of qubits as input.","We consider two settings for the training of our model: (1) supervised learning where the weak measurement training record can be labeled with known Hamiltonian parameters, and (2) unsupervised learning where no labels are available.","The first has the advantage of not requiring an explicit representation of the quantum state, thus potentially scaling very favorably to larger number of qubits.","The second requires the implementation of a physical model to map the Hamiltonian parameters to a measurement record, which we implement using an integrator of the physical model with a recurrent neural network to provide a model-free correction at every time step to account for small effects not captured by the physical model.","We test our construction on a system of two qubits and demonstrate accurate prediction of multiple physical parameters in both the supervised and unsupervised context.","We demonstrate that the model benefits from larger training sets establishing that it is in fact \"learning,\" and we show robustness to errors in the assumed physical model by achieving accurate parameter estimation in the presence of unanticipated single particle relaxation."],"url":"http://arxiv.org/abs/2404.05526v1","category":"quant-ph"}
{"created":"2024-04-08 13:36:29","title":"Quantum Optimization Methods for Satellite Mission Planning","abstract":"Satellite mission planning for Earth observation satellites is a combinatorial optimization problem that consists of selecting the optimal subset of imaging requests, subject to constraints, to be fulfilled during an orbit pass of a satellite. The ever-growing amount of satellites in orbit underscores the need to operate them efficiently, which requires solving many instances of the problem in short periods of time. However, current classical algorithms often fail to find the global optimum or take too long to execute. Here, we approach the problem from a quantum computing point of view, which offers a promising alternative that could lead to significant improvements in solution quality or execution speed in the future. To this end, we study a planning problem with a variety of intricate constraints and discuss methods to encode them for quantum computers. Additionally, we experimentally assess the performance of quantum annealing and the quantum approximate optimization algorithm on a realistic and diverse dataset. Our results identify key aspects like graph connectivity and constraint structure that influence the performance of the methods. We explore the limits of today's quantum algorithms and hardware, providing bounds on the problems that can be currently solved successfully and showing how the solution degrades as the complexity grows. This work aims to serve as a baseline for further research in the field and establish realistic expectations on current quantum optimization capabilities.","sentences":["Satellite mission planning for Earth observation satellites is a combinatorial optimization problem that consists of selecting the optimal subset of imaging requests, subject to constraints, to be fulfilled during an orbit pass of a satellite.","The ever-growing amount of satellites in orbit underscores the need to operate them efficiently, which requires solving many instances of the problem in short periods of time.","However, current classical algorithms often fail to find the global optimum or take too long to execute.","Here, we approach the problem from a quantum computing point of view, which offers a promising alternative that could lead to significant improvements in solution quality or execution speed in the future.","To this end, we study a planning problem with a variety of intricate constraints and discuss methods to encode them for quantum computers.","Additionally, we experimentally assess the performance of quantum annealing and the quantum approximate optimization algorithm on a realistic and diverse dataset.","Our results identify key aspects like graph connectivity and constraint structure that influence the performance of the methods.","We explore the limits of today's quantum algorithms and hardware, providing bounds on the problems that can be currently solved successfully and showing how the solution degrades as the complexity grows.","This work aims to serve as a baseline for further research in the field and establish realistic expectations on current quantum optimization capabilities."],"url":"http://arxiv.org/abs/2404.05516v1","category":"quant-ph"}
{"created":"2024-04-08 13:34:39","title":"A High-Performant Multi-Parametric Quadratic Programming Solver","abstract":"We propose a combinatorial method for computing explicit solutions to multi-parametric quadratic programs, which can be used to compute explicit control laws for linear model predictive control. In contrast to classical methods, which are based on geometrical adjacency, the proposed method is based on combinatorial adjacency. After introducing the notion of combinatorial adjacency, we show that the explicit solution forms a connected graph in terms of it. We then leverage this connectedness to propose an algorithm that computes the explicit solution. The purely combinatorial nature of the algorithm leads to computational advantages since it enables demanding geometrical operations (such as computing facets of polytopes) to be avoided. Compared with classical combinatorial methods, the proposed method requires fewer combinations to be considered by exploiting combinatorial connectedness. We show that an implementation of the proposed method can yield a speedup of about two orders of magnitude compared with state-of-the-art software packages such as MPT and POP.","sentences":["We propose a combinatorial method for computing explicit solutions to multi-parametric quadratic programs, which can be used to compute explicit control laws for linear model predictive control.","In contrast to classical methods, which are based on geometrical adjacency, the proposed method is based on combinatorial adjacency.","After introducing the notion of combinatorial adjacency, we show that the explicit solution forms a connected graph in terms of it.","We then leverage this connectedness to propose an algorithm that computes the explicit solution.","The purely combinatorial nature of the algorithm leads to computational advantages since it enables demanding geometrical operations (such as computing facets of polytopes) to be avoided.","Compared with classical combinatorial methods, the proposed method requires fewer combinations to be considered by exploiting combinatorial connectedness.","We show that an implementation of the proposed method can yield a speedup of about two orders of magnitude compared with state-of-the-art software packages such as MPT and POP."],"url":"http://arxiv.org/abs/2404.05511v1","category":"math.OC"}
{"created":"2024-04-08 13:25:13","title":"Level-Set Percolation of Gaussian Random Fields on Complex Networks","abstract":"We provide an explicit solution of the problem of level-set percolation for multivariate Gaussians defined in terms of weighted graph Laplacians on complex networks. The solution requires an analysis of the heterogeneous micro-structure of the percolation problem, i.e., a self-consistent determination of locally varying percolation probabilities. This is achieved using a cavity or message passing approach. It can be evaluated, both for single large instances of locally tree-like graphs, and in the thermodynamic limit of random graphs of finite mean degree in the configuration model class.","sentences":["We provide an explicit solution of the problem of level-set percolation for multivariate Gaussians defined in terms of weighted graph Laplacians on complex networks.","The solution requires an analysis of the heterogeneous micro-structure of the percolation problem, i.e., a self-consistent determination of locally varying percolation probabilities.","This is achieved using a cavity or message passing approach.","It can be evaluated, both for single large instances of locally tree-like graphs, and in the thermodynamic limit of random graphs of finite mean degree in the configuration model class."],"url":"http://arxiv.org/abs/2404.05503v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-08 13:20:48","title":"Stability Mechanisms for Predictive Safety Filters","abstract":"Predictive safety filters enable the integration of potentially unsafe learning-based control approaches and humans into safety-critical systems. In addition to simple constraint satisfaction, many control problems involve additional stability requirements that may vary depending on the specific use case or environmental context. In this work, we address this problem by augmenting predictive safety filters with stability guarantees, ranging from bounded convergence to uniform asymptotic stability. The proposed framework extends well-known stability results from model predictive control (MPC) theory while supporting commonly used design techniques. As a result, straightforward extensions to dynamic trajectory tracking problems can be easily adapted, as outlined in this article. The practicality of the framework is demonstrated using an automotive advanced driver assistance scenario, involving a reference trajectory stabilization problem.","sentences":["Predictive safety filters enable the integration of potentially unsafe learning-based control approaches and humans into safety-critical systems.","In addition to simple constraint satisfaction, many control problems involve additional stability requirements that may vary depending on the specific use case or environmental context.","In this work, we address this problem by augmenting predictive safety filters with stability guarantees, ranging from bounded convergence to uniform asymptotic stability.","The proposed framework extends well-known stability results from model predictive control (MPC) theory while supporting commonly used design techniques.","As a result, straightforward extensions to dynamic trajectory tracking problems can be easily adapted, as outlined in this article.","The practicality of the framework is demonstrated using an automotive advanced driver assistance scenario, involving a reference trajectory stabilization problem."],"url":"http://arxiv.org/abs/2404.05496v1","category":"cs.SY"}
{"created":"2024-04-08 13:15:18","title":"Implementation of the bilayer Hubbard model in a moir\u00e9 heterostructure","abstract":"Moir\\'e materials provide a unique platform for studies of correlated many-body physics of the Fermi-Hubbard model on triangular spin-charge lattices. Bilayer Hubbard models are of particular significance with regard to the physics of Mott insulating states and their relation to unconventional superconductivity, yet their experimental implementation in moir\\'e systems has so far remained elusive. Here, we demonstrate the realization of a staggered bilayer triangular lattice of electrons in an antiparallel MoSe$_{2}$/WS$_{2}$ heterostructure. The bilayer lattice emerges due to strong electron confinement in the moir\\'e potential minima and the near-resonant alignment of conduction band edges in MoSe$_{2}$ and WS$_{2}$. As a result, charge filling proceeds layer-by-layer, with the first and second electron per moir\\'e cell consecutively occupying first the MoSe$_{2}$ and then the WS$_{2}$ layer. We describe the observed charging sequence by an electrostatic model and provide experimental evidence of spin correlations on the vertically offset and laterally staggered bilayer lattice, yielding absolute exciton Land\\'e factors as high as $600$ at lowest temperatures. The bilayer character of the implemented spin-charge lattice allows for electrostatic tunability of Ruderman-Kittel-Kasuya-Yosida magnetism, and establishes antiparallel MoSe$_{2}$/WS$_{2}$ heterostructures as a viable platform for studies of bilayer Hubbard model physics with exotic magnetic phases on frustrated lattices.","sentences":["Moir\\'e materials provide a unique platform for studies of correlated many-body physics of the Fermi-Hubbard model on triangular spin-charge lattices.","Bilayer Hubbard models are of particular significance with regard to the physics of Mott insulating states and their relation to unconventional superconductivity, yet their experimental implementation in moir\\'e systems has so far remained elusive.","Here, we demonstrate the realization of a staggered bilayer triangular lattice of electrons in an antiparallel MoSe$_{2}$/WS$_{2}$ heterostructure.","The bilayer lattice emerges due to strong electron confinement in the moir\\'e potential minima and the near-resonant alignment of conduction band edges in MoSe$_{2}$ and WS$_{2}$.","As a result, charge filling proceeds layer-by-layer, with the first and second electron per moir\\'e cell consecutively occupying first the MoSe$_{2}$ and then the WS$_{2}$ layer.","We describe the observed charging sequence by an electrostatic model and provide experimental evidence of spin correlations on the vertically offset and laterally staggered bilayer lattice, yielding absolute exciton Land\\'e factors as high as $600$ at lowest temperatures.","The bilayer character of the implemented spin-charge lattice allows for electrostatic tunability of Ruderman-Kittel-Kasuya-Yosida magnetism, and establishes antiparallel MoSe$_{2}$/WS$_{2}$ heterostructures as a viable platform for studies of bilayer Hubbard model physics with exotic magnetic phases on frustrated lattices."],"url":"http://arxiv.org/abs/2404.05494v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 13:10:49","title":"PHL 5038AB: Is the brown dwarf causing pollution of its white dwarf host star?","abstract":"We present new results on PHL 5038AB, a widely separated binary system composed of a white dwarf and a brown dwarf, refining the white and brown dwarf parameters and determining the binary separation to be $66^{+12}_{-24}$~AU. New spectra of the white dwarf show calcium absorption lines suggesting the hydrogen-rich atmosphere is weakly polluted, inferring the presence of planetesimals in the system, which we determine are in an S-type orbit around the white dwarf in orbits closer than 17-32 AU. We do not detect any infrared excess that would indicate the presence of a disc, suggesting all dust present has either been totally accreted or is optically thin. In this system, we suggest the metal pollution in the white dwarf atmosphere can be directly attributed to the presence of the brown dwarf companion disrupting the orbits of planetesimals within the system.","sentences":["We present new results on PHL 5038AB, a widely separated binary system composed of a white dwarf and a brown dwarf, refining the white and brown dwarf parameters and determining the binary separation to be $66^{+12}_{-24}$~AU.","New spectra of the white dwarf show calcium absorption lines suggesting the hydrogen-rich atmosphere is weakly polluted, inferring the presence of planetesimals in the system, which we determine are in an S-type orbit around the white dwarf in orbits closer than 17-32 AU.","We do not detect any infrared excess that would indicate the presence of a disc, suggesting all dust present has either been totally accreted or is optically thin.","In this system, we suggest the metal pollution in the white dwarf atmosphere can be directly attributed to the presence of the brown dwarf companion disrupting the orbits of planetesimals within the system."],"url":"http://arxiv.org/abs/2404.05488v1","category":"astro-ph.SR"}
{"created":"2024-04-08 13:01:25","title":"WaveCatBoost for Probabilistic Forecasting of Regional Air Quality Data","abstract":"Accurate and reliable air quality forecasting is essential for protecting public health, sustainable development, pollution control, and enhanced urban planning. This letter presents a novel WaveCatBoost architecture designed to forecast the real-time concentrations of air pollutants by combining the maximal overlapping discrete wavelet transform (MODWT) with the CatBoost model. This hybrid approach efficiently transforms time series into high-frequency and low-frequency components, thereby extracting signal from noise and improving prediction accuracy and robustness. Evaluation of two distinct regional datasets, from the Central Air Pollution Control Board (CPCB) sensor network and a low-cost air quality sensor system (LAQS), underscores the superior performance of our proposed methodology in real-time forecasting compared to the state-of-the-art statistical and deep learning architectures. Moreover, we employ a conformal prediction strategy to provide probabilistic bands with our forecasts.","sentences":["Accurate and reliable air quality forecasting is essential for protecting public health, sustainable development, pollution control, and enhanced urban planning.","This letter presents a novel WaveCatBoost architecture designed to forecast the real-time concentrations of air pollutants by combining the maximal overlapping discrete wavelet transform (MODWT) with the CatBoost model.","This hybrid approach efficiently transforms time series into high-frequency and low-frequency components, thereby extracting signal from noise and improving prediction accuracy and robustness.","Evaluation of two distinct regional datasets, from the Central Air Pollution Control Board (CPCB) sensor network and a low-cost air quality sensor system (LAQS), underscores the superior performance of our proposed methodology in real-time forecasting compared to the state-of-the-art statistical and deep learning architectures.","Moreover, we employ a conformal prediction strategy to provide probabilistic bands with our forecasts."],"url":"http://arxiv.org/abs/2404.05482v1","category":"cs.LG"}
{"created":"2024-04-08 12:55:17","title":"Three Subtyping Algorithms for Binary Session Types and their Complexity Analyses","abstract":"Session types are a type discipline for describing and specifying communication behaviours of concurrent processes. Session subtyping, firstly introduced by Gay and Hole, is widely used for enlarging typability of session programs. This paper gives the complexity analysis of three algorithms for subtyping of synchronous binary session types. First, we analyse the complexity of the algorithm from the original paper, which is based on an inductive tree search. We then introduce its optimised version, which improves the complexity, but is still exponential against the size of the two types. Finally, we propose a new quadratic algorithm based on a graph search using the concept of XYZW-simulation, recently introduced by Silva et al.","sentences":["Session types are a type discipline for describing and specifying communication behaviours of concurrent processes.","Session subtyping, firstly introduced by Gay and Hole, is widely used for enlarging typability of session programs.","This paper gives the complexity analysis of three algorithms for subtyping of synchronous binary session types.","First, we analyse the complexity of the algorithm from the original paper, which is based on an inductive tree search.","We then introduce its optimised version, which improves the complexity, but is still exponential against the size of the two types.","Finally, we propose a new quadratic algorithm based on a graph search using the concept of XYZW-simulation, recently introduced by Silva et al."],"url":"http://arxiv.org/abs/2404.05480v1","category":"cs.PL"}
{"created":"2024-04-08 12:54:09","title":"Linear Contextual Metaprogramming and Session Types","abstract":"We explore the integration of metaprogramming in a call-by-value linear lambda-calculus and sketch its extension to a session type system. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may then be boxed and transmitted in messages. Once received, one such value may then be unboxed (with a let-box construct) and locally applied before being run. We present a series of examples where servers prepare and ship code on demand via session typed messages.","sentences":["We explore the integration of metaprogramming in a call-by-value linear lambda-calculus and sketch its extension to a session type system.","We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may then be boxed and transmitted in messages.","Once received, one such value may then be unboxed (with a let-box construct) and locally applied before being run.","We present a series of examples where servers prepare and ship code on demand via session typed messages."],"url":"http://arxiv.org/abs/2404.05475v1","category":"cs.LO"}
{"created":"2024-04-08 12:44:24","title":"Enhancing Lip Reading with Multi-Scale Video and Multi-Encoder","abstract":"Automatic lip-reading (ALR) aims to automatically transcribe spoken content from a speaker's silent lip motion captured in video. Current mainstream lip-reading approaches only use a single visual encoder to model input videos of a single scale. In this paper, we propose to enhance lipreading by incorporating multi-scale video data and multi-encoder. Specifically, we first propose a novel multi-scale lip extraction algorithm based on the size of the speaker's face and an enhanced ResNet3D visual front-end (VFE) to extract lip features at different scales. For the multi-encoder, in addition to the mainstream Transformer and Conformer, we also incorporate the recently proposed Branchformer and EBranchformer as visual encoders. In the experiments, we explore the influence of different video data scales and encoders on ALR system performance and fuse the texts transcribed by all ALR systems using recognizer output voting error reduction (ROVER). Finally, our proposed approach placed second in the ICME 2024 ChatCLR Challenge Task 2, with a 21.52% reduction in character error rate (CER) compared to the official baseline on the evaluation set.","sentences":["Automatic lip-reading (ALR) aims to automatically transcribe spoken content from a speaker's silent lip motion captured in video.","Current mainstream lip-reading approaches only use a single visual encoder to model input videos of a single scale.","In this paper, we propose to enhance lipreading by incorporating multi-scale video data and multi-encoder.","Specifically, we first propose a novel multi-scale lip extraction algorithm based on the size of the speaker's face and an enhanced ResNet3D visual front-end (VFE) to extract lip features at different scales.","For the multi-encoder, in addition to the mainstream Transformer and Conformer, we also incorporate the recently proposed Branchformer and EBranchformer as visual encoders.","In the experiments, we explore the influence of different video data scales and encoders on ALR system performance and fuse the texts transcribed by all ALR systems using recognizer output voting error reduction (ROVER).","Finally, our proposed approach placed second in the ICME 2024 ChatCLR Challenge Task 2, with a 21.52% reduction in character error rate (CER) compared to the official baseline on the evaluation set."],"url":"http://arxiv.org/abs/2404.05466v1","category":"cs.CV"}
{"created":"2024-04-08 12:40:46","title":"A Coq Library of Sets for Teaching Denotational Semantics","abstract":"Sets and relations are very useful concepts for defining denotational semantics. In the Coq proof assistant, curried functions to Prop are used to represent sets and relations, e.g. A -> Prop, A -> B -> Prop, A -> B -> C -> Prop, etc. Further, the membership relation can be encoded by function applications, e.g. X a represents a in X if X: A -> Prop. This is very convenient for developing formal definitions and proofs for professional users, but it makes propositions more difficult to read for non-professional users, e.g. students of a program semantics course. We develop a small Coq library of sets and relations so that standard math notations can be used when teaching denotational semantics of simple imperative languages. This library is developed using Coq's type class system. It brings about zero proof-term overhead comparing with the existing formalization of sets.","sentences":["Sets and relations are very useful concepts for defining denotational semantics.","In the Coq proof assistant, curried functions to Prop are used to represent sets and relations, e.g. A -> Prop, A -> B -> Prop, A -> B -> C -> Prop, etc.","Further, the membership relation can be encoded by function applications, e.g. X a represents a in X if X: A -> Prop.","This is very convenient for developing formal definitions and proofs for professional users, but it makes propositions more difficult to read for non-professional users, e.g. students of a program semantics course.","We develop a small Coq library of sets and relations so that standard math notations can be used when teaching denotational semantics of simple imperative languages.","This library is developed using Coq's type class system.","It brings about zero proof-term overhead comparing with the existing formalization of sets."],"url":"http://arxiv.org/abs/2404.05459v1","category":"cs.PL"}
{"created":"2024-04-08 12:37:20","title":"Complex network approach to the turbulent velocity gradient dynamics: High- and low-probability Lagrangian paths","abstract":"Understanding the dynamics of the turbulent velocity gradient tensor (VGT) is essential to gain insights into the Navier-Stokes equations and improve small-scale turbulence modeling. However, characterizing the VGT dynamics conditional on all its relevant invariants in a continuous fashion is extremely difficult. In this paper, we represent the VGT Lagrangian dynamics using a network where each node represents a unique flow state. This approach enables us to discern how the VGT transitions from one state to another in a simplified fashion. Our analysis reveals intriguing features of the resulting network, such as the clustering of the commonly visited nodes where the eigenvalues of the VGT are real, in the proximity of the Vieillefosse tail. We then relate our complex network approach to the well-established VGT discretization based on the sign of its principal invariants, $Q$ and $R$, and its discriminant, $\\Delta$. To this end, we separate the shortest paths on the network (geodesics) based on the $Q$-$R$ region to which their starting and arrival nodes belong. The distribution of the length of intra-region geodesics, with starting and arrival nodes belonging to the same $Q$-$R$ region, exhibits a distinct bimodality in two regions of the $Q$-$R$ plane, those in which the deviatoric part of the pressure Hessian introduces complexity to the VGT dynamics. Such bimodality is associated with infrequently visited nodes having to follow a long, low probability path to drastically change the state of the VGT compared to other flow states that can acquire the necessary characteristics without changing their sign for $Q$ or $R$. We complement the geodesics approach by examining random walks on the network, showing how the VGT non-normality and the associated production terms distinguish the shortest commuting paths between different $Q$-$R$ regions.","sentences":["Understanding the dynamics of the turbulent velocity gradient tensor (VGT) is essential to gain insights into the Navier-Stokes equations and improve small-scale turbulence modeling.","However, characterizing the VGT dynamics conditional on all its relevant invariants in a continuous fashion is extremely difficult.","In this paper, we represent the VGT Lagrangian dynamics using a network where each node represents a unique flow state.","This approach enables us to discern how the VGT transitions from one state to another in a simplified fashion.","Our analysis reveals intriguing features of the resulting network, such as the clustering of the commonly visited nodes where the eigenvalues of the VGT are real, in the proximity of the Vieillefosse tail.","We then relate our complex network approach to the well-established VGT discretization based on the sign of its principal invariants, $Q$ and $R$, and its discriminant, $\\Delta$. To this end, we separate the shortest paths on the network (geodesics) based on the $Q$-$R$ region to which their starting and arrival nodes belong.","The distribution of the length of intra-region geodesics, with starting and arrival nodes belonging to the same $Q$-$R$ region, exhibits a distinct bimodality in two regions of the $Q$-$R$ plane, those in which the deviatoric part of the pressure Hessian introduces complexity to the VGT dynamics.","Such bimodality is associated with infrequently visited nodes having to follow a long, low probability path to drastically change the state of the VGT compared to other flow states that can acquire the necessary characteristics without changing their sign for $Q$ or $R$. We complement the geodesics approach by examining random walks on the network, showing how the VGT non-normality and the associated production terms distinguish the shortest commuting paths between different $Q$-$R$ regions."],"url":"http://arxiv.org/abs/2404.05453v1","category":"physics.flu-dyn"}
{"created":"2024-04-08 12:26:06","title":"The Open Autonomy Safety Case Framework","abstract":"A system safety case is a compelling, comprehensible, and valid argument about the satisfaction of the safety goals of a given system operating in a given environment supported by convincing evidence. Since the publication of UL 4600 in 2020, safety cases have become a best practice for measuring, managing, and communicating the safety of autonomous vehicles (AVs). Although UL 4600 provides guidance on how to build the safety case for an AV, the complexity of AVs and their operating environments, the novelty of the used technology, the need for complying with various regulations and technical standards, and for addressing cybersecurity concerns and ethical considerations make the development of safety cases for AVs challenging. To this end, safety case frameworks have been proposed that bring strategies, argument templates, and other guidance together to support the development of a safety case. This paper introduces the Open Autonomy Safety Case Framework, developed over years of work with the autonomous vehicle industry, as a roadmap for how AVs can be deployed safely and responsibly.","sentences":["A system safety case is a compelling, comprehensible, and valid argument about the satisfaction of the safety goals of a given system operating in a given environment supported by convincing evidence.","Since the publication of UL 4600 in 2020, safety cases have become a best practice for measuring, managing, and communicating the safety of autonomous vehicles (AVs).","Although UL 4600 provides guidance on how to build the safety case for an AV, the complexity of AVs and their operating environments, the novelty of the used technology, the need for complying with various regulations and technical standards, and for addressing cybersecurity concerns and ethical considerations make the development of safety cases for AVs challenging.","To this end, safety case frameworks have been proposed that bring strategies, argument templates, and other guidance together to support the development of a safety case.","This paper introduces the Open Autonomy Safety Case Framework, developed over years of work with the autonomous vehicle industry, as a roadmap for how AVs can be deployed safely and responsibly."],"url":"http://arxiv.org/abs/2404.05444v1","category":"cs.SE"}
{"created":"2024-04-08 12:24:03","title":"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All","abstract":"Quantum annealers (QA), such as D-Wave systems, become increasingly efficient and competitive at solving combinatorial optimization problems. However, solving problems that do not directly map the chip topology remains challenging for this type of quantum computer. The creation of logical qubits as sets of interconnected physical qubits overcomes limitations imposed by the sparsity of the chip at the expense of increasing the problem size and adding new parameters to optimize. This paper explores the advantages and drawbacks provided by the structure of the logical qubits and the impact of the rescaling of coupler strength on the minimum spectral gap of Ising models. We show that densely connected logical qubits require a lower chain strength to maintain the ferromagnetic coupling. We also analyze the optimal chain strength variations considering different minor embeddings of the same instance. This experimental study suggests that the chain strength can be optimized for each instance. We design a heuristic that optimizes the chain strength using a very low number of shots during the pre-processing step. This heuristic outperforms the default method used to initialize the chain strength on D-Wave systems, increasing the quality of the best solution by up to 17.2% for tested instances on the max-cut problem.","sentences":["Quantum annealers (QA), such as D-Wave systems, become increasingly efficient and competitive at solving combinatorial optimization problems.","However, solving problems that do not directly map the chip topology remains challenging for this type of quantum computer.","The creation of logical qubits as sets of interconnected physical qubits overcomes limitations imposed by the sparsity of the chip at the expense of increasing the problem size and adding new parameters to optimize.","This paper explores the advantages and drawbacks provided by the structure of the logical qubits and the impact of the rescaling of coupler strength on the minimum spectral gap of Ising models.","We show that densely connected logical qubits require a lower chain strength to maintain the ferromagnetic coupling.","We also analyze the optimal chain strength variations considering different minor embeddings of the same instance.","This experimental study suggests that the chain strength can be optimized for each instance.","We design a heuristic that optimizes the chain strength using a very low number of shots during the pre-processing step.","This heuristic outperforms the default method used to initialize the chain strength on D-Wave systems, increasing the quality of the best solution by up to 17.2% for tested instances on the max-cut problem."],"url":"http://arxiv.org/abs/2404.05443v1","category":"quant-ph"}
{"created":"2024-04-08 12:15:48","title":"An intriguing connection between the Bardeen-Moshe-Bander phenomenon and 2+p spin glasses","abstract":"This paper aims to establish a close connection between the Bardeen-Moshe-Bander phenomenon and a p=2+3 spin-glass model with sextic confinement potential. This is made possible by the unconventional power-counting induced by the effective kinetics provided by the disorder coupling in the large $N$-limit. Because of the absence of epsilon expansion, our approach is more attractive than the previous one and plays a relevant role in the signal detection issue in nearly continuous spectra.","sentences":["This paper aims to establish a close connection between the Bardeen-Moshe-Bander phenomenon and a p=2+3 spin-glass model with sextic confinement potential.","This is made possible by the unconventional power-counting induced by the effective kinetics provided by the disorder coupling in the large $N$-limit.","Because of the absence of epsilon expansion, our approach is more attractive than the previous one and plays a relevant role in the signal detection issue in nearly continuous spectra."],"url":"http://arxiv.org/abs/2404.05436v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-08 12:14:23","title":"Paired and Toeplitz + Hankel operators","abstract":"We present complete classifications of paired operators on the Hilbert space $L^2(\\mathbb{T})$ and Toeplitz + Hankel operators on vector-valued Hardy spaces. We introduce the notion of inner-paired operators defined on the Hardy space that use the classical model spaces. We fully classify inner-paired operators.","sentences":["We present complete classifications of paired operators on the Hilbert space $L^2(\\mathbb{T})$ and Toeplitz + Hankel operators on vector-valued Hardy spaces.","We introduce the notion of inner-paired operators defined on the Hardy space that use the classical model spaces.","We fully classify inner-paired operators."],"url":"http://arxiv.org/abs/2404.05435v1","category":"math.FA"}
{"created":"2024-04-08 12:04:43","title":"Nonadiabatic Field with Triangle Window Functions on Quantum Phase Space","abstract":"The constraint coordinate-momentum phase space (CPS) formulation of finite-state quantum systems has recently revealed that the triangle window function approach is an isomorphic representation of the exact population-population correlation function of the two-state system. We use the triangle window (TW) function and the CPS mapping kernel element to formulate a novel useful representation of discrete electronic degrees of freedom (DOFs). When it is employed with nonadiabatic field (NaF) dynamics, a new variant of the NaF approach (i.e., NaF-TW) is proposed. Extensive benchmark tests of model systems in both the condensed phase and gas phase demonstrate that the NaF-TW approach is competent in faithfully capturing the dynamical interplay between electronic and nuclear DOFs. In comparison to the symmetrical quasi-classical (SQC) method where triangle window functions were originally proposed, the performance of NaF-TW is significantly better when the bifurcation characteristic of nuclear motion in the asymptotic region is important.","sentences":["The constraint coordinate-momentum phase space (CPS) formulation of finite-state quantum systems has recently revealed that the triangle window function approach is an isomorphic representation of the exact population-population correlation function of the two-state system.","We use the triangle window (TW) function and the CPS mapping kernel element to formulate a novel useful representation of discrete electronic degrees of freedom (DOFs).","When it is employed with nonadiabatic field (NaF) dynamics, a new variant of the NaF approach (i.e., NaF-TW) is proposed.","Extensive benchmark tests of model systems in both the condensed phase and gas phase demonstrate that the NaF-TW approach is competent in faithfully capturing the dynamical interplay between electronic and nuclear DOFs.","In comparison to the symmetrical quasi-classical (SQC) method where triangle window functions were originally proposed, the performance of NaF-TW is significantly better when the bifurcation characteristic of nuclear motion in the asymptotic region is important."],"url":"http://arxiv.org/abs/2404.05432v1","category":"quant-ph"}
{"created":"2024-04-08 11:56:22","title":"Re-Ranking News Comments by Constructiveness and Curiosity Significantly Increases Perceived Respect, Trustworthiness, and Interest","abstract":"Online commenting platforms have commonly developed systems to address online harms by removing and down-ranking content. An alternative, under-explored approach is to focus on up-ranking content to proactively prioritize prosocial commentary and set better conversational norms. We present a study with 460 English-speaking US-based news readers to understand the effects of re-ranking comments by constructiveness, curiosity, and personal stories on a variety of outcomes related to willingness to participate and engage, as well as perceived credibility and polarization in a comment section. In our rich-media survey experiment, participants across these four ranking conditions and a control group reviewed prototypes of comment sections of a Politics op-ed and Dining article. We found that outcomes varied significantly by article type. Up-ranking curiosity and constructiveness improved a number of measures for the Politics article, including perceived \\textit{Respect}, \\textit{Trustworthiness}, and \\textit{Interestingness} of the comment section. Constructiveness also increased perceptions that the comments were favorable to Republicans, with no condition worsening perceptions of partisans. Additionally, in the Dining article, personal stories and constructiveness rankings significantly improved the perceived informativeness of the comments. Overall, these findings indicate that incorporating prosocial qualities of speech into ranking could be a promising approach to promote healthier, less polarized dialogue in online comment sections.","sentences":["Online commenting platforms have commonly developed systems to address online harms by removing and down-ranking content.","An alternative, under-explored approach is to focus on up-ranking content to proactively prioritize prosocial commentary and set better conversational norms.","We present a study with 460 English-speaking US-based news readers to understand the effects of re-ranking comments by constructiveness, curiosity, and personal stories on a variety of outcomes related to willingness to participate and engage, as well as perceived credibility and polarization in a comment section.","In our rich-media survey experiment, participants across these four ranking conditions and a control group reviewed prototypes of comment sections of a Politics op-ed and Dining article.","We found that outcomes varied significantly by article type.","Up-ranking curiosity and constructiveness improved a number of measures for the Politics article, including perceived \\textit{Respect}, \\textit{Trustworthiness}, and \\textit{Interestingness} of the comment section.","Constructiveness also increased perceptions that the comments were favorable to Republicans, with no condition worsening perceptions of partisans.","Additionally, in the Dining article, personal stories and constructiveness rankings significantly improved the perceived informativeness of the comments.","Overall, these findings indicate that incorporating prosocial qualities of speech into ranking could be a promising approach to promote healthier, less polarized dialogue in online comment sections."],"url":"http://arxiv.org/abs/2404.05429v1","category":"cs.HC"}
{"created":"2024-04-08 11:50:40","title":"Requirements Elicitation in Government Projects: A Preliminary Empirical Study","abstract":"Government development projects vary significantly from private sector initiatives in scope, stakeholder complexity, and regulatory requirements. There is a lack of empirical studies focusing on requirements engineering (RE) activities specifically for government projects. We addressed this gap by conducting a series of semi-structured interviews with 12 professional software practitioners working on government projects. These interviewees are employed by two types of companies, each serving different government departments. Our findings uncover differences in the requirements elicitation phase between government projects, particularly for data visualization aspects, and other software projects, such as stakeholders and policy requirements. Additionally, we explore the coverage of human and social aspects in requirements elicitation, finding that culture, team dynamics, and policy implications are critical considerations. Our findings also pinpoint the main challenges encountered during the requirements elicitation phase for government projects. Our findings highlight future research work that is important to bridge the gap in RE activities for government software projects.","sentences":["Government development projects vary significantly from private sector initiatives in scope, stakeholder complexity, and regulatory requirements.","There is a lack of empirical studies focusing on requirements engineering (RE) activities specifically for government projects.","We addressed this gap by conducting a series of semi-structured interviews with 12 professional software practitioners working on government projects.","These interviewees are employed by two types of companies, each serving different government departments.","Our findings uncover differences in the requirements elicitation phase between government projects, particularly for data visualization aspects, and other software projects, such as stakeholders and policy requirements.","Additionally, we explore the coverage of human and social aspects in requirements elicitation, finding that culture, team dynamics, and policy implications are critical considerations.","Our findings also pinpoint the main challenges encountered during the requirements elicitation phase for government projects.","Our findings highlight future research work that is important to bridge the gap in RE activities for government software projects."],"url":"http://arxiv.org/abs/2404.05425v1","category":"cs.SE"}
{"created":"2024-04-08 11:42:44","title":"Selecting active matter according to motility in an acoustofluidic setup: Self-propelled particles and sperm cells","abstract":"Active systems -- including sperm cells, living organisms like bacteria, fish, birds, or active soft matter systems like synthetic ''microswimmers'' -- are characterized by motility, i.e., the ability to propel using their own ''engine''. Motility is the key feature that distinguishes active systems from passive or externally driven systems. In a large ensemble, motility of individual species can vary in a wide range. Selecting active species according to their motility represents an exciting and challenging problem. We propose a new method for selecting active species based on their motility using an acoustofluidic setup where highly motile species escape from the acoustic trap. This is demonstrated in simulations and in experiments with self-propelled Janus particles and human sperm. The immediate application of this method is selecting highly motile sperm for medically assisted reproduction (MAR). Due to the tunable acoustic trap, the proposed method is more flexible than the existing passive microfluidic methods. The proposed selection method based on motility can also be applied to other active systems that require selecting highly motile species or removing immotile species.","sentences":["Active systems -- including sperm cells, living organisms like bacteria, fish, birds, or active soft matter systems like synthetic ''microswimmers'' -- are characterized by motility, i.e., the ability to propel using their own ''engine''.","Motility is the key feature that distinguishes active systems from passive or externally driven systems.","In a large ensemble, motility of individual species can vary in a wide range.","Selecting active species according to their motility represents an exciting and challenging problem.","We propose a new method for selecting active species based on their motility using an acoustofluidic setup where highly motile species escape from the acoustic trap.","This is demonstrated in simulations and in experiments with self-propelled Janus particles and human sperm.","The immediate application of this method is selecting highly motile sperm for medically assisted reproduction (MAR).","Due to the tunable acoustic trap, the proposed method is more flexible than the existing passive microfluidic methods.","The proposed selection method based on motility can also be applied to other active systems that require selecting highly motile species or removing immotile species."],"url":"http://arxiv.org/abs/2404.05422v1","category":"cond-mat.soft"}
{"created":"2024-04-08 11:32:26","title":"Two Hands Are Better Than One: Resolving Hand to Hand Intersections via Occupancy Networks","abstract":"3D hand pose estimation from images has seen considerable interest from the literature, with new methods improving overall 3D accuracy. One current challenge is to address hand-to-hand interaction where self-occlusions and finger articulation pose a significant problem to estimation. Little work has applied physical constraints that minimize the hand intersections that occur as a result of noisy estimation. This work addresses the intersection of hands by exploiting an occupancy network that represents the hand's volume as a continuous manifold. This allows us to model the probability distribution of points being inside a hand. We designed an intersection loss function to minimize the likelihood of hand-to-point intersections. Moreover, we propose a new hand mesh parameterization that is superior to the commonly used MANO model in many respects including lower mesh complexity, underlying 3D skeleton extraction, watertightness, etc. On the benchmark InterHand2.6M dataset, the models trained using our intersection loss achieve better results than the state-of-the-art by significantly decreasing the number of hand intersections while lowering the mean per-joint positional error. Additionally, we demonstrate superior performance for 3D hand uplift on Re:InterHand and SMILE datasets and show reduced hand-to-hand intersections for complex domains such as sign-language pose estimation.","sentences":["3D hand pose estimation from images has seen considerable interest from the literature, with new methods improving overall 3D accuracy.","One current challenge is to address hand-to-hand interaction where self-occlusions and finger articulation pose a significant problem to estimation.","Little work has applied physical constraints that minimize the hand intersections that occur as a result of noisy estimation.","This work addresses the intersection of hands by exploiting an occupancy network that represents the hand's volume as a continuous manifold.","This allows us to model the probability distribution of points being inside a hand.","We designed an intersection loss function to minimize the likelihood of hand-to-point intersections.","Moreover, we propose a new hand mesh parameterization that is superior to the commonly used MANO model in many respects including lower mesh complexity, underlying 3D skeleton extraction, watertightness, etc.","On the benchmark InterHand2.6M dataset, the models trained using our intersection loss achieve better results than the state-of-the-art by significantly decreasing the number of hand intersections while lowering the mean per-joint positional error.","Additionally, we demonstrate superior performance for 3D hand uplift on Re:InterHand and SMILE datasets and show reduced hand-to-hand intersections for complex domains such as sign-language pose estimation."],"url":"http://arxiv.org/abs/2404.05414v1","category":"cs.CV"}
{"created":"2024-04-08 11:08:05","title":"Contouring Error Bounded Control for Biaxial Switched Linear Systems","abstract":"Biaxial motion control systems are used extensively in manufacturing and printing industries. To improve throughput and reduce machine cost, lightweight materials are being proposed in structural components but may result in higher flexibility in the machine links. This flexibility is often position dependent and compromises precision of the end effector of the machine. To address the need for improved contouring accuracy in industrial machines with position-dependent structural flexibility, this paper introduces a novel contouring error-bounded control algorithm for biaxial switched linear systems. The proposed algorithm utilizes model predictive control to guarantee the satisfaction of state, input, and contouring error constraints for any admissible mode switching. In this paper, the switching signal remains unknown to the controller, although information about the minimum time the system is expected to stay in a specific mode is considered to be available. The proposed algorithm has the property of recursive feasibility and ensures the stability of the closed-loop system. The effectiveness of the proposed method is demonstrated by applying it to a high-fidelity simulation of a dual-drive industrial laser machine. The results show that the contouring error is successfully bounded within the given tolerance.","sentences":["Biaxial motion control systems are used extensively in manufacturing and printing industries.","To improve throughput and reduce machine cost, lightweight materials are being proposed in structural components but may result in higher flexibility in the machine links.","This flexibility is often position dependent and compromises precision of the end effector of the machine.","To address the need for improved contouring accuracy in industrial machines with position-dependent structural flexibility, this paper introduces a novel contouring error-bounded control algorithm for biaxial switched linear systems.","The proposed algorithm utilizes model predictive control to guarantee the satisfaction of state, input, and contouring error constraints for any admissible mode switching.","In this paper, the switching signal remains unknown to the controller, although information about the minimum time the system is expected to stay in a specific mode is considered to be available.","The proposed algorithm has the property of recursive feasibility and ensures the stability of the closed-loop system.","The effectiveness of the proposed method is demonstrated by applying it to a high-fidelity simulation of a dual-drive industrial laser machine.","The results show that the contouring error is successfully bounded within the given tolerance."],"url":"http://arxiv.org/abs/2404.05404v1","category":"eess.SY"}
{"created":"2024-04-08 10:49:59","title":"Towards AI Safety: A Taxonomy for AI System Evaluation","abstract":"The advent of advanced AI brings to the forefront the need for comprehensive safety evaluation. However, divergent practices and terminologies across different communities (i.e., AI, software engineering, and governance), combined with the complexity of AI systems and environmental affordances (e.g., access to tools), call for a holistic evaluation approach. This paper proposes a framework for comprehensive AI system evaluation comprising three components: 1) harmonised terminology to facilitate communication across disciplines involved in AI safety evaluation; 2) a taxonomy identifying essential elements for AI system evaluation; 3) a mapping between AI lifecycle, stakeholders, and requisite evaluations for accountable AI supply chain. This framework catalyses a deeper discourse on AI system evaluation beyond model-centric approaches.","sentences":["The advent of advanced AI brings to the forefront the need for comprehensive safety evaluation.","However, divergent practices and terminologies across different communities (i.e., AI, software engineering, and governance), combined with the complexity of AI systems and environmental affordances (e.g., access to tools), call for a holistic evaluation approach.","This paper proposes a framework for comprehensive AI system evaluation comprising three components: 1) harmonised terminology to facilitate communication across disciplines involved in AI safety evaluation; 2) a taxonomy identifying essential elements for AI system evaluation; 3) a mapping between AI lifecycle, stakeholders, and requisite evaluations for accountable AI supply chain.","This framework catalyses a deeper discourse on AI system evaluation beyond model-centric approaches."],"url":"http://arxiv.org/abs/2404.05388v1","category":"cs.SE"}
{"created":"2024-04-08 10:47:32","title":"Multiple Floquet Chern insulator phases in the spin-charge coupled triangular-lattice ferrimagnet: Crucial roles of higher-order terms in the high-frequency expansion","abstract":"We study the effects of photoirradiation with circularly polarized light on the Dirac half-metal state induced by the ferrimagnetic order in a triangular Kondo-lattice model. Our analysis based on the Floquet theory reveals that two types of Floquet Chern insulator phases appear as photoinduced nonequilibrium steady states and that these two phases can be experimentally detected and distinguished by measurements of the Hall conductivity. It is elucidated that these rich nonequilibrium topological phases come from higher-order terms in the high-frequency expansion called Brillouin-Wigner expansion, which is in striking contrast to usually discussed Floquet Chern insulator phases originating from the lowest-order terms of the expansion. So far, the lattice electron models on simple non-multipartite lattices such as triangular lattices and square lattices have not been regarded as targets of the Floquet engineering because the lowest-order terms of the high-frequency expansion for Floquet effective Hamiltonians cancel each other to vanish in these systems. Our findings of the Floquet Chern insulator phases in a triangular Kondo-lattice model are expected to expand the range of potential models and even materials targeted by the Floquet engineering.","sentences":["We study the effects of photoirradiation with circularly polarized light on the Dirac half-metal state induced by the ferrimagnetic order in a triangular Kondo-lattice model.","Our analysis based on the Floquet theory reveals that two types of Floquet Chern insulator phases appear as photoinduced nonequilibrium steady states and that these two phases can be experimentally detected and distinguished by measurements of the Hall conductivity.","It is elucidated that these rich nonequilibrium topological phases come from higher-order terms in the high-frequency expansion called Brillouin-Wigner expansion, which is in striking contrast to usually discussed Floquet Chern insulator phases originating from the lowest-order terms of the expansion.","So far, the lattice electron models on simple non-multipartite lattices such as triangular lattices and square lattices have not been regarded as targets of the Floquet engineering because the lowest-order terms of the high-frequency expansion for Floquet effective Hamiltonians cancel each other to vanish in these systems.","Our findings of the Floquet Chern insulator phases in a triangular Kondo-lattice model are expected to expand the range of potential models and even materials targeted by the Floquet engineering."],"url":"http://arxiv.org/abs/2404.05385v1","category":"cond-mat.str-el"}
{"created":"2024-04-08 10:34:48","title":"Logic-dependent emergence of multistability, hysteresis, and biphasic dynamics in a minimal positive feedback network with an autoloop","abstract":"Cellular decision-making (CDM) is a dynamic phenomenon often controlled by regulatory networks defining interactions between genes and transcription factor proteins. Traditional studies have focussed on molecular switches such as positive feedback circuits that exhibit at most bistability. However, higher-order dynamics such as tristability is also prominent in many biological processes. It is thus imperative to identify a minimal circuit that can alone explain mono, bi, and tristable dynamics. In this work, we consider a two-component positive feedback network with an autoloop and explore these regimes of stability for different degrees of multimerization and the choice of Boolean logic functions. We report that this network can exhibit numerous dynamical scenarios such as bi-and tristability, hysteresis, and biphasic kinetics, explaining the possibilities of abrupt cell state transitions and the smooth state swap without a step-like switch. Specifically, while with monomeric regulation and competitive OR logic, the circuit exhibits mono-and bistability and biphasic dynamics, with non-competitive AND and OR logics only monostability can be achieved. To obtain bistability in the latter cases, we show that the autoloop must have (at least) dimeric regulation. In pursuit of higher-order stability, we show that tristability occurs with higher degrees of multimerization and with non-competitive OR logic only. Our results, backed by rigorous analytical calculations and numerical examples, thus explain the association between multistability, multimerization, and logic in this minimal circuit. Since this circuit underlies various biological processes, including epithelial-mesenchymal transition which often drives carcinoma metastasis, these results can thus offer crucial inputs to control cell state transition by manipulating multimerization and the logic of regulation in cells.","sentences":["Cellular decision-making (CDM) is a dynamic phenomenon often controlled by regulatory networks defining interactions between genes and transcription factor proteins.","Traditional studies have focussed on molecular switches such as positive feedback circuits that exhibit at most bistability.","However, higher-order dynamics such as tristability is also prominent in many biological processes.","It is thus imperative to identify a minimal circuit that can alone explain mono, bi, and tristable dynamics.","In this work, we consider a two-component positive feedback network with an autoloop and explore these regimes of stability for different degrees of multimerization and the choice of Boolean logic functions.","We report that this network can exhibit numerous dynamical scenarios such as bi-and tristability, hysteresis, and biphasic kinetics, explaining the possibilities of abrupt cell state transitions and the smooth state swap without a step-like switch.","Specifically, while with monomeric regulation and competitive OR logic, the circuit exhibits mono-and bistability and biphasic dynamics, with non-competitive AND and OR logics only monostability can be achieved.","To obtain bistability in the latter cases, we show that the autoloop must have (at least) dimeric regulation.","In pursuit of higher-order stability, we show that tristability occurs with higher degrees of multimerization and with non-competitive OR logic only.","Our results, backed by rigorous analytical calculations and numerical examples, thus explain the association between multistability, multimerization, and logic in this minimal circuit.","Since this circuit underlies various biological processes, including epithelial-mesenchymal transition which often drives carcinoma metastasis, these results can thus offer crucial inputs to control cell state transition by manipulating multimerization and the logic of regulation in cells."],"url":"http://arxiv.org/abs/2404.05379v1","category":"q-bio.MN"}
{"created":"2024-04-08 10:32:30","title":"A Max-Min-Max Algorithm for Large-Scale Robust Optimization","abstract":"Robust optimization (RO) is a powerful paradigm for decision making under uncertainty. Existing algorithms for solving RO, including the reformulation approach and the cutting-plane method, do not scale well, hindering the application of RO to large-scale decision problems. In this paper, we devise a first-order algorithm for solving RO based on a novel max-min-max perspective. Our algorithm operates directly on the model functions and sets through the subgradient and projection oracles, which enables the exploitation of problem structures and is especially suitable for large-scale RO. Theoretically, we prove that the oracle complexity of our algorithm for attaining an $\\varepsilon$-approximate optimal solution is $\\mathcal{O}(\\varepsilon^{-3})$ or $\\mathcal{O}(\\varepsilon^{-2})$, depending on the smoothness of the model functions. The algorithm and its theoretical results are then extended to RO with projection-unfriendly uncertainty sets. We also show via extensive numerical experiments that the proposed algorithm outperforms the reformulation approach, the cutting-plane method and two other recent first-order algorithms.","sentences":["Robust optimization (RO) is a powerful paradigm for decision making under uncertainty.","Existing algorithms for solving RO, including the reformulation approach and the cutting-plane method, do not scale well, hindering the application of RO to large-scale decision problems.","In this paper, we devise a first-order algorithm for solving RO based on a novel max-min-max perspective.","Our algorithm operates directly on the model functions and sets through the subgradient and projection oracles, which enables the exploitation of problem structures and is especially suitable for large-scale RO.","Theoretically, we prove that the oracle complexity of our algorithm for attaining an $\\varepsilon$-approximate optimal solution is $\\mathcal{O}(\\varepsilon^{-3})$ or $\\mathcal{O}(\\varepsilon^{-2})$, depending on the smoothness of the model functions.","The algorithm and its theoretical results are then extended to RO with projection-unfriendly uncertainty sets.","We also show via extensive numerical experiments that the proposed algorithm outperforms the reformulation approach, the cutting-plane method and two other recent first-order algorithms."],"url":"http://arxiv.org/abs/2404.05377v1","category":"math.OC"}
{"created":"2024-04-08 10:09:15","title":"Finite Elements with Switch Detection for Numerical Optimal Control of Projected Dynamical Systems","abstract":"The Finite Elements with Switch Detection (FESD) method is a highly accurate direct transcription method for optimal control of several classes of nonsmooth dynamical systems. This paper extends the FESD method to Projected Dynamical Systems (PDS) and first-order sweeping processes with time-independent sets. This method discretizes an equivalent dynamic complementarity system and exploits the particular structure of the discontinuities present in these systems. In the FESD method, allowing integration step sizes to be degrees of freedom, and introducing additional complementarity constraints, enables the exact detection of nonsmooth events. In contrast to the standard fixed-step Runge-Kutta methods, this approach allows for the recovery of full-order integration accuracy and the correct computation of numerical sensitivities. Numerical examples illustrate the effectiveness of the proposed method in an optimal control context. This method and the examples are included in the open-source software package nosnoc.","sentences":["The Finite Elements with Switch Detection (FESD) method is a highly accurate direct transcription method for optimal control of several classes of nonsmooth dynamical systems.","This paper extends the FESD method to Projected Dynamical Systems (PDS) and first-order sweeping processes with time-independent sets.","This method discretizes an equivalent dynamic complementarity system and exploits the particular structure of the discontinuities present in these systems.","In the FESD method, allowing integration step sizes to be degrees of freedom, and introducing additional complementarity constraints, enables the exact detection of nonsmooth events.","In contrast to the standard fixed-step Runge-Kutta methods, this approach allows for the recovery of full-order integration accuracy and the correct computation of numerical sensitivities.","Numerical examples illustrate the effectiveness of the proposed method in an optimal control context.","This method and the examples are included in the open-source software package nosnoc."],"url":"http://arxiv.org/abs/2404.05367v1","category":"math.OC"}
{"created":"2024-04-08 09:54:28","title":"Multi-head Attention-based Deep Multiple Instance Learning","abstract":"This paper introduces MAD-MIL, a Multi-head Attention-based Deep Multiple Instance Learning model, designed for weakly supervised Whole Slide Images (WSIs) classification in digital pathology. Inspired by the multi-head attention mechanism of the Transformer, MAD-MIL simplifies model complexity while achieving competitive results against advanced models like CLAM and DS-MIL. Evaluated on the MNIST-BAGS and public datasets, including TUPAC16, TCGA BRCA, TCGA LUNG, and TCGA KIDNEY, MAD-MIL consistently outperforms ABMIL. This demonstrates enhanced information diversity, interpretability, and efficiency in slide representation. The model's effectiveness, coupled with fewer trainable parameters and lower computational complexity makes it a promising solution for automated pathology workflows. Our code is available at https://github.com/tueimage/MAD-MIL.","sentences":["This paper introduces MAD-MIL, a Multi-head Attention-based Deep Multiple Instance Learning model, designed for weakly supervised Whole Slide Images (WSIs) classification in digital pathology.","Inspired by the multi-head attention mechanism of the Transformer, MAD-MIL simplifies model complexity while achieving competitive results against advanced models like CLAM and DS-MIL.","Evaluated on the MNIST-BAGS and public datasets, including TUPAC16, TCGA BRCA, TCGA LUNG, and TCGA KIDNEY, MAD-MIL consistently outperforms ABMIL.","This demonstrates enhanced information diversity, interpretability, and efficiency in slide representation.","The model's effectiveness, coupled with fewer trainable parameters and lower computational complexity makes it a promising solution for automated pathology workflows.","Our code is available at https://github.com/tueimage/MAD-MIL."],"url":"http://arxiv.org/abs/2404.05362v1","category":"cs.CV"}
{"created":"2024-04-08 09:53:42","title":"Optimal Controller Realizations against False Data Injections in Cooperative Driving","abstract":"To enhance the robustness of cooperative driving to cyberattacks, we study a controller-oriented approach to mitigate the effect of a class of False-Data Injection (FDI) attacks. By reformulating a given dynamic Cooperative Adaptive Cruise Control (CACC) scheme (the base controller), we recognize that the base controller can be represented by a class of new but equivalent controllers (base controller realizations) that exhibits the same platooning behavior with varying robustness in the presence of attacks. We propose a prescriptive synthesis framework where the base controller and the system dynamics are written in new coordinates via an invertible coordinate transformation on the controller state. Because the input-output behavior is invariant under coordinate transformations, the input-output behavior is unaffected (so controller realizations do not change the system's closed-loop performance). However, each base controller realization may require a different combination of sensors. To this end, we obtain the optimal combination of sensors that minimizes the effect of FDI attacks by solving a Linear Matrix Inequality (LMI), while quantifying the FDI's attack impact through reachability analysis. Through simulation studies, we demonstrate that this approach enhances the robustness of cooperative driving, without relying on a detection scheme and maintaining all system properties.","sentences":["To enhance the robustness of cooperative driving to cyberattacks, we study a controller-oriented approach to mitigate the effect of a class of False-Data Injection (FDI) attacks.","By reformulating a given dynamic Cooperative Adaptive Cruise Control (CACC) scheme (the base controller), we recognize that the base controller can be represented by a class of new but equivalent controllers (base controller realizations) that exhibits the same platooning behavior with varying robustness in the presence of attacks.","We propose a prescriptive synthesis framework where the base controller and the system dynamics are written in new coordinates via an invertible coordinate transformation on the controller state.","Because the input-output behavior is invariant under coordinate transformations, the input-output behavior is unaffected (so controller realizations do not change the system's closed-loop performance).","However, each base controller realization may require a different combination of sensors.","To this end, we obtain the optimal combination of sensors that minimizes the effect of FDI attacks by solving a Linear Matrix Inequality (LMI), while quantifying the FDI's attack impact through reachability analysis.","Through simulation studies, we demonstrate that this approach enhances the robustness of cooperative driving, without relying on a detection scheme and maintaining all system properties."],"url":"http://arxiv.org/abs/2404.05361v1","category":"cs.SY"}
{"created":"2024-04-08 09:52:28","title":"Global solutions to quadratic systems of stochastic reaction-diffusion equations in space-dimension two","abstract":"We prove the existence of global-in-time regular solutions to a system of stochastic quadratic reaction-diffusion equations. Global-in-time existence is based on a $L^\\infty$-estimate obtained by an approach {\\`a} la De Giorgi, as in [GoudonVasseur10]. The adaptation of this technique to the stochastic case requires in its final step an $L^2\\ln(L^2)$-bound, furnished by an estimate by duality on the entropy inequality, as in [DesvillettesFellnerPierreVovelle07]. In our stochastic context, and similarly to [DebusscheRoselloVovelle2021], we need to solve a backward SPDE to exploit the duality technique","sentences":["We prove the existence of global-in-time regular solutions to a system of stochastic quadratic reaction-diffusion equations.","Global-in-time existence is based on a $L^\\infty$-estimate obtained by an approach {\\`a} la De Giorgi, as in [GoudonVasseur10].","The adaptation of this technique to the stochastic case requires in its final step an $L^2\\ln(L^2)$-bound, furnished by an estimate by duality on the entropy inequality, as in [DesvillettesFellnerPierreVovelle07].","In our stochastic context, and similarly to [DebusscheRoselloVovelle2021], we need to solve a backward SPDE to exploit the duality technique"],"url":"http://arxiv.org/abs/2404.05360v1","category":"math.AP"}
{"created":"2024-04-08 09:48:09","title":"On a port-Hamiltonian formulation and structure-preserving numerical approximations for thermodynamic compressible fluid flow","abstract":"The high volatility of renewable energies calls for more energy efficiency. Thus, different physical systems need to be coupled efficiently although they run on various time scales. Here, the port-Hamiltonian (pH) modeling framework comes into play as it has several advantages, e.g., physical properties are encoded in the system structure and systems running on different time scales can be coupled easily. Additionally, pH systems coupled by energy-preserving conditions are still pH. Furthermore, in the energy transition hydrogen becomes an important player and unlike in natural gas, its temperature-dependence is of importance. Thus, we introduce an infinite dimensional pH formulation of the compressible non-isothermal Euler equations to model flow with temperature-dependence. We set up the underlying Stokes-Dirac structure and deduce the boundary port variables. We introduce coupling conditions into our pH formulation, such that the whole network system is pH itself. This is achieved by using energy-preserving coupling conditions, i.e., mass conservation and equality of total enthalpy, at the coupling nodes. Furthermore, to close the system a third coupling condition is needed. Here, equality of the outgoing entropy at coupling nodes is used and included into our systems in a structure-preserving way. Following that, we adapt the structure-preserving aproximation methods from the isothermal to the non-isothermal case. Academic numerical examples will support our analytical findings.","sentences":["The high volatility of renewable energies calls for more energy efficiency.","Thus, different physical systems need to be coupled efficiently although they run on various time scales.","Here, the port-Hamiltonian (pH) modeling framework comes into play as it has several advantages, e.g., physical properties are encoded in the system structure and systems running on different time scales can be coupled easily.","Additionally, pH systems coupled by energy-preserving conditions are still pH. Furthermore, in the energy transition hydrogen becomes an important player and unlike in natural gas, its temperature-dependence is of importance.","Thus, we introduce an infinite dimensional pH formulation of the compressible non-isothermal Euler equations to model flow with temperature-dependence.","We set up the underlying Stokes-Dirac structure and deduce the boundary port variables.","We introduce coupling conditions into our pH formulation, such that the whole network system is pH itself.","This is achieved by using energy-preserving coupling conditions, i.e., mass conservation and equality of total enthalpy, at the coupling nodes.","Furthermore, to close the system a third coupling condition is needed.","Here, equality of the outgoing entropy at coupling nodes is used and included into our systems in a structure-preserving way.","Following that, we adapt the structure-preserving aproximation methods from the isothermal to the non-isothermal case.","Academic numerical examples will support our analytical findings."],"url":"http://arxiv.org/abs/2404.05358v1","category":"math.NA"}
{"created":"2024-04-08 09:48:02","title":"CNN-based Game State Detection for a Foosball Table","abstract":"The automation of games using Deep Reinforcement Learning Strategies (DRL) is a well-known challenge in AI research. While for feature extraction in a video game typically the whole image is used, this is hardly practical for many real world games. Instead, using a smaller game state reducing the dimension of the parameter space to include essential parameters only seems to be a promising approach. In the game of Foosball, a compact and comprehensive game state description consists of the positional shifts and rotations of the figures and the position of the ball over time. In particular, velocities and accelerations can be derived from consecutive time samples of the game state. In this paper, a figure detection system to determine the game state in Foosball is presented. We capture a dataset containing the rotations of the rods which were measured using accelerometers and the positional shifts were derived using traditional Computer Vision techniques (in a laboratory setting). This dataset is utilized to train Convolutional Neural Network (CNN) based end-to-end regression models to predict the rotations and shifts of each rod. We present an evaluation of our system using different state-of-the-art CNNs as base architectures for the regression model. We show that our system is able to predict the game state with high accuracy. By providing data for both black and white teams, the presented system is intended to provide the required data for future developments of Imitation Learning techniques w.r.t. to observing human players.","sentences":["The automation of games using Deep Reinforcement Learning Strategies (DRL) is a well-known challenge in AI research.","While for feature extraction in a video game typically the whole image is used, this is hardly practical for many real world games.","Instead, using a smaller game state reducing the dimension of the parameter space to include essential parameters only seems to be a promising approach.","In the game of Foosball, a compact and comprehensive game state description consists of the positional shifts and rotations of the figures and the position of the ball over time.","In particular, velocities and accelerations can be derived from consecutive time samples of the game state.","In this paper, a figure detection system to determine the game state in Foosball is presented.","We capture a dataset containing the rotations of the rods which were measured using accelerometers and the positional shifts were derived using traditional Computer Vision techniques (in a laboratory setting).","This dataset is utilized to train Convolutional Neural Network (CNN) based end-to-end regression models to predict the rotations and shifts of each rod.","We present an evaluation of our system using different state-of-the-art CNNs as base architectures for the regression model.","We show that our system is able to predict the game state with high accuracy.","By providing data for both black and white teams, the presented system is intended to provide the required data for future developments of Imitation Learning techniques w.r.t. to observing human players."],"url":"http://arxiv.org/abs/2404.05357v1","category":"cs.CV"}
{"created":"2024-04-08 09:33:25","title":"Cooling dynamics of a free ion in a Bose-Einstein condensate","abstract":"We investigate the dynamics of an ion moving through a homogeneous Bose-Einstein condensate (BEC) after an initial momentum is imparted. For this, we derive a master equation in the weak-coupling limit and Lamb-Dicke approximation for the reduced density matrix of the ion. We study the time evolution of the ion's kinetic energy and observe that its expectation value, identified as the ion temperature $T_\\mathrm{ion}$, is reduced by several orders of magnitude in a time on the order of microseconds for a condensate density in the experimentally relevant range between $10^{13}\\,\\mathrm{cm}^{-3}$ and $10^{14}\\,\\mathrm{cm}^{-3}$. We characterize this behavior by defining the duration at half maximum as the time required by $T_\\mathrm{ion}$ to reach half of its initial value, and study its dependence on the system parameters. Similarly, we find that the expectation value of the ion's momentum operator is reduced by nine orders of magnitude on the same timescale, making the ion's position converge to a final value. Based on these results, we conclude that the interaction with the bosonic bath allows for cooling and pinning of the ion by decreasing the expectation value of its kinetic energy and velocity, which constitutes a result of direct relevance for current atom-ion experiments.","sentences":["We investigate the dynamics of an ion moving through a homogeneous Bose-Einstein condensate (BEC) after an initial momentum is imparted.","For this, we derive a master equation in the weak-coupling limit and Lamb-Dicke approximation for the reduced density matrix of the ion.","We study the time evolution of the ion's kinetic energy and observe that its expectation value, identified as the ion temperature $T_\\mathrm{ion}$, is reduced by several orders of magnitude in a time on the order of microseconds for a condensate density in the experimentally relevant range between $10^{13}\\,\\mathrm{cm}^{-3}$ and $10^{14}\\,\\mathrm{cm}^{-3}$. We characterize this behavior by defining the duration at half maximum as the time required by $T_\\mathrm{ion}$ to reach half of its initial value, and study its dependence on the system parameters.","Similarly, we find that the expectation value of the ion's momentum operator is reduced by nine orders of magnitude on the same timescale, making the ion's position converge to a final value.","Based on these results, we conclude that the interaction with the bosonic bath allows for cooling and pinning of the ion by decreasing the expectation value of its kinetic energy and velocity, which constitutes a result of direct relevance for current atom-ion experiments."],"url":"http://arxiv.org/abs/2404.05347v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-08 09:30:17","title":"Phase Noise Detection via Expectation Propagation and Related Algorithms","abstract":"In the context of signal detection in the presence of an unknown time-varying channel parameter, receivers based on the Expectation Propagation (EP) framework appear to be very promising. EP is a message-passing algorithm based on factor graphs with an inherent ability to combine prior knowledge of system variables with channel observations. This suggests that an effective estimation of random channel parameters can be achieved even with a very limited number of pilot symbols, thus increasing the payload efficiency. However, achieving satisfactory performance often requires ad-hoc adjustments in the way the probability distributions of latent variables - both data and channel parameters - are combined and projected. Here, we apply EP to a classical problem of coded transmission on a strong Wiener phase noise channel, employing soft-input soft-output decoding. We identify its limitations and propose new strategies which reach the performance benchmark while maintaining low complexity, with a primary focus on challenging scenarios where the state-of-the-art algorithms fail.","sentences":["In the context of signal detection in the presence of an unknown time-varying channel parameter, receivers based on the Expectation Propagation (EP) framework appear to be very promising.","EP is a message-passing algorithm based on factor graphs with an inherent ability to combine prior knowledge of system variables with channel observations.","This suggests that an effective estimation of random channel parameters can be achieved even with a very limited number of pilot symbols, thus increasing the payload efficiency.","However, achieving satisfactory performance often requires ad-hoc adjustments in the way the probability distributions of latent variables - both data and channel parameters - are combined and projected.","Here, we apply EP to a classical problem of coded transmission on a strong Wiener phase noise channel, employing soft-input soft-output decoding.","We identify its limitations and propose new strategies which reach the performance benchmark while maintaining low complexity, with a primary focus on challenging scenarios where the state-of-the-art algorithms fail."],"url":"http://arxiv.org/abs/2404.05344v1","category":"eess.SP"}
{"created":"2024-04-08 09:29:34","title":"Non-linear Model Predictive Control for Multi-task GPS-free Autonomous Navigation in Vineyards","abstract":"Autonomous navigation is the foundation of agricultural robots. This paper focuses on developing an advanced autonomous navigation system for a rover operating within row-based crops. A position-agnostic system is proposed to address the challenging situation when standard localization methods, like GPS, fail due to unfavorable weather or obstructed signals. This breakthrough is especially vital in densely vegetated regions, including areas covered by thick tree canopies or pergola vineyards. This work proposed a novel system that leverages a single RGB-D camera and a Non-linear Model Predictive Control strategy to navigate through entire rows, adapting to various crop spacing. The presented solution demonstrates versatility in handling diverse crop densities, environmental factors, and multiple navigation tasks to support agricultural activities at an extremely cost-effective implementation. Experimental validation in simulated and real vineyards underscores the system's robustness and competitiveness in both standard row traversal and target objects approach.","sentences":["Autonomous navigation is the foundation of agricultural robots.","This paper focuses on developing an advanced autonomous navigation system for a rover operating within row-based crops.","A position-agnostic system is proposed to address the challenging situation when standard localization methods, like GPS, fail due to unfavorable weather or obstructed signals.","This breakthrough is especially vital in densely vegetated regions, including areas covered by thick tree canopies or pergola vineyards.","This work proposed a novel system that leverages a single RGB-D camera and a Non-linear Model Predictive Control strategy to navigate through entire rows, adapting to various crop spacing.","The presented solution demonstrates versatility in handling diverse crop densities, environmental factors, and multiple navigation tasks to support agricultural activities at an extremely cost-effective implementation.","Experimental validation in simulated and real vineyards underscores the system's robustness and competitiveness in both standard row traversal and target objects approach."],"url":"http://arxiv.org/abs/2404.05343v1","category":"cs.RO"}
{"created":"2024-04-08 09:22:49","title":"Modeling the Dynamic Process of Inventions for Reducing Knowledge Search Costs","abstract":"A knowledge search is a key process for inventions. However, there is inadequate quantitative modeling of dynamic knowledge search processes and associated search costs. In this study, agent-based and complex network methodologies were proposed to quantitatively describe the dynamic process of knowledge search for actual inventions. Prior knowledge networks (PKNs), the search space of historical patents, were constructed, representative search rules were formulated for R&D agents, and measures for knowledge search cost were designed to serve as search objectives. Simulation results in the field of photolithographic technology show that search costs differ significantly with different search rules. Familiarity and Degree rules significantly outperform BFS, DFS and Recency rules in terms of knowledge search costs, and are less affected by the size and density of PKNs. Interestingly, there is no significant correlation between the mean and variance of search costs and patent value, indicating that high-value patents are not particularly difficult to obtain. The implications for innovation theories and R&D practices are drawn from the models and results.","sentences":["A knowledge search is a key process for inventions.","However, there is inadequate quantitative modeling of dynamic knowledge search processes and associated search costs.","In this study, agent-based and complex network methodologies were proposed to quantitatively describe the dynamic process of knowledge search for actual inventions.","Prior knowledge networks (PKNs), the search space of historical patents, were constructed, representative search rules were formulated for R&D agents, and measures for knowledge search cost were designed to serve as search objectives.","Simulation results in the field of photolithographic technology show that search costs differ significantly with different search rules.","Familiarity and Degree rules significantly outperform BFS, DFS and Recency rules in terms of knowledge search costs, and are less affected by the size and density of PKNs.","Interestingly, there is no significant correlation between the mean and variance of search costs and patent value, indicating that high-value patents are not particularly difficult to obtain.","The implications for innovation theories and R&D practices are drawn from the models and results."],"url":"http://arxiv.org/abs/2404.05334v1","category":"cs.SI"}
{"created":"2024-04-08 09:13:20","title":"Widely-tunable and narrow-linewidth hybrid-integrated diode laser at 637 nm","abstract":"We present hybrid-integrated extended cavity diode lasers tunable around 637 nm, with a gain-wide spectral coverage of 8 nm. This tuning range allows addressing the zero-phonon line of nitrogen vacancy centers and includes the wavelength of HeNe lasers (633 nm). The lasers provide wide mode-hop free tuning up to 97 GHz and a narrow intrinsic linewidth down to 10 kHz. The maximum output power is 2.5 mW in a single-mode fiber, corresponding to an on-chip power of 4.0 mW. Full integration and packaging in a standard housing with fiber pigtails provide high intrinsic stability and will enable integration into complex optical systems.","sentences":["We present hybrid-integrated extended cavity diode lasers tunable around 637 nm, with a gain-wide spectral coverage of 8 nm.","This tuning range allows addressing the zero-phonon line of nitrogen vacancy centers and includes the wavelength of HeNe lasers (633 nm).","The lasers provide wide mode-hop free tuning up to 97 GHz and a narrow intrinsic linewidth down to 10 kHz.","The maximum output power is 2.5 mW in a single-mode fiber, corresponding to an on-chip power of 4.0 mW. Full integration and packaging in a standard housing with fiber pigtails provide high intrinsic stability and will enable integration into complex optical systems."],"url":"http://arxiv.org/abs/2404.05325v1","category":"physics.optics"}
{"created":"2024-04-08 09:10:20","title":"Unravelling the Power of Single-Pass Look-Ahead in Modern Codecs for Optimized Transcoding Deployment","abstract":"Modern video encoders have evolved into sophisticated pieces of software in which various coding tools interact with each other. In the past, singlepass encoding was not considered for Video-On-Demand (VOD) use cases. In this work, we evaluate production-ready encoders for H.264 (x264), H.265 (HEVC), AV1 (SVT-AV1) along with direct comparisons to the latest AV1 encoder inside NVIDIA GPUs (40 series), and AWS Mediaconvert's AV1 implementation. Our experimental results demonstrate single pass encoding inside modern encoder implementations can give us very good quality at a reasonable compute cost. The results are presented as three different scenarios targeting High, Medium, and Low complexity accounting quality/bitrate/compute load. Finally, a set of recommendations is presented for end-users to help decide which encoder/preset combination might be more suited to their use case.","sentences":["Modern video encoders have evolved into sophisticated pieces of software in which various coding tools interact with each other.","In the past, singlepass encoding was not considered for Video-On-Demand (VOD) use cases.","In this work, we evaluate production-ready encoders for H.264 (x264), H.265 (HEVC), AV1 (SVT-AV1) along with direct comparisons to the latest AV1 encoder inside NVIDIA GPUs (40 series), and AWS Mediaconvert's AV1 implementation.","Our experimental results demonstrate single pass encoding inside modern encoder implementations can give us very good quality at a reasonable compute cost.","The results are presented as three different scenarios targeting High, Medium, and Low complexity accounting quality/bitrate/compute load.","Finally, a set of recommendations is presented for end-users to help decide which encoder/preset combination might be more suited to their use case."],"url":"http://arxiv.org/abs/2404.05321v1","category":"eess.IV"}
{"created":"2024-04-08 09:06:16","title":"HOEG: A New Approach for Object-Centric Predictive Process Monitoring","abstract":"Predictive Process Monitoring focuses on predicting future states of ongoing process executions, such as forecasting the remaining time. Recent developments in Object-Centric Process Mining have enriched event data with objects and their explicit relations between events. To leverage this enriched data, we propose the Heterogeneous Object Event Graph encoding (HOEG), which integrates events and objects into a graph structure with diverse node types. It does so without aggregating object features, thus creating a more nuanced and informative representation. We then adopt a heterogeneous Graph Neural Network architecture, which incorporates these diverse object features in prediction tasks. We evaluate the performance and scalability of HOEG in predicting remaining time, benchmarking it against two established graph-based encodings and two baseline models. Our evaluation uses three Object-Centric Event Logs (OCELs), including one from a real-life process at a major Dutch financial institution. The results indicate that HOEG competes well with existing models and surpasses them when OCELs contain informative object attributes and event-object interactions.","sentences":["Predictive Process Monitoring focuses on predicting future states of ongoing process executions, such as forecasting the remaining time.","Recent developments in Object-Centric Process Mining have enriched event data with objects and their explicit relations between events.","To leverage this enriched data, we propose the Heterogeneous Object Event Graph encoding (HOEG), which integrates events and objects into a graph structure with diverse node types.","It does so without aggregating object features, thus creating a more nuanced and informative representation.","We then adopt a heterogeneous Graph Neural Network architecture, which incorporates these diverse object features in prediction tasks.","We evaluate the performance and scalability of HOEG in predicting remaining time, benchmarking it against two established graph-based encodings and two baseline models.","Our evaluation uses three Object-Centric Event Logs (OCELs), including one from a real-life process at a major Dutch financial institution.","The results indicate that HOEG competes well with existing models and surpasses them when OCELs contain informative object attributes and event-object interactions."],"url":"http://arxiv.org/abs/2404.05316v1","category":"cs.LG"}
{"created":"2024-04-08 09:05:50","title":"$c {\\bar c}$ and $b {\\bar b}$ suppression in Glasma","abstract":"This study investigates the evolution and dissociation dynamics of $c\\bar{c}$ and $b\\bar{b}$ pairs within the Glasma medium, resulting from the collision of ultra-relativistic heavy ions. An attractive potential is used to create the pairs, but the strong Glasma fields dominate over it, causing an increase in pair separation and subsequent dissociation. The observed finite probability of dissociation for these states reveals the intricate interplay between QCD dynamics and the suppression of $c\\bar{c}$ and $b\\bar{b}$ states during the Glasma phase. The research highlights differences between $c\\bar{c}$ and $b\\bar{b}$ pairs, revealing the role of quark flavor in the dissociation process. Dissociation spectra analysis indicates a peak shift towards higher momentum, reflecting a slight energy gain by the pairs. This investigation provides valuable insights into the complex dynamics of $c\\bar{c}$ and $b\\bar{b}$ pairs in the Glasma, which may help in better interpretation of experimental results on further integration with subsequent phases of the created matter.","sentences":["This study investigates the evolution and dissociation dynamics of $c\\bar{c}$ and $b\\bar{b}$ pairs within the Glasma medium, resulting from the collision of ultra-relativistic heavy ions.","An attractive potential is used to create the pairs, but the strong Glasma fields dominate over it, causing an increase in pair separation and subsequent dissociation.","The observed finite probability of dissociation for these states reveals the intricate interplay between QCD dynamics and the suppression of $c\\bar{c}$ and $b\\bar{b}$ states during the Glasma phase.","The research highlights differences between $c\\bar{c}$ and $b\\bar{b}$ pairs, revealing the role of quark flavor in the dissociation process.","Dissociation spectra analysis indicates a peak shift towards higher momentum, reflecting a slight energy gain by the pairs.","This investigation provides valuable insights into the complex dynamics of $c\\bar{c}$ and $b\\bar{b}$ pairs in the Glasma, which may help in better interpretation of experimental results on further integration with subsequent phases of the created matter."],"url":"http://arxiv.org/abs/2404.05315v1","category":"hep-ph"}
{"created":"2024-04-08 08:32:39","title":"Online Learning of Joint-Muscle Mapping Using Vision in Tendon-driven Musculoskeletal Humanoids","abstract":"The body structures of tendon-driven musculoskeletal humanoids are complex, and accurate modeling is difficult, because they are made by imitating the body structures of human beings. For this reason, we have not been able to move them accurately like ordinary humanoids driven by actuators in each axis, and large internal muscle tension and slack of tendon wires have emerged by the model error between its geometric model and the actual robot. Therefore, we construct a joint-muscle mapping (JMM) using a neural network (NN), which expresses a nonlinear relationship between joint angles and muscle lengths, and aim to move tendon-driven musculoskeletal humanoids accurately by updating the JMM online from data of the actual robot. In this study, the JMM is updated online by using the vision of the robot so that it moves to the correct position (Vision Updater). Also, we execute another update to modify muscle antagonisms correctly (Antagonism Updater). By using these two updaters, the error between the target and actual joint angles decrease to about 40% in 5 minutes, and we show through a manipulation experiment that the tendon-driven musculoskeletal humanoid Kengoro becomes able to move as intended. This novel system can adapt to the state change and growth of robots, because it updates the JMM online successively.","sentences":["The body structures of tendon-driven musculoskeletal humanoids are complex, and accurate modeling is difficult, because they are made by imitating the body structures of human beings.","For this reason, we have not been able to move them accurately like ordinary humanoids driven by actuators in each axis, and large internal muscle tension and slack of tendon wires have emerged by the model error between its geometric model and the actual robot.","Therefore, we construct a joint-muscle mapping (JMM) using a neural network (NN), which expresses a nonlinear relationship between joint angles and muscle lengths, and aim to move tendon-driven musculoskeletal humanoids accurately by updating the JMM online from data of the actual robot.","In this study, the JMM is updated online by using the vision of the robot so that it moves to the correct position (Vision Updater).","Also, we execute another update to modify muscle antagonisms correctly (Antagonism Updater).","By using these two updaters, the error between the target and actual joint angles decrease to about 40% in 5 minutes, and we show through a manipulation experiment that the tendon-driven musculoskeletal humanoid Kengoro becomes able to move as intended.","This novel system can adapt to the state change and growth of robots, because it updates the JMM online successively."],"url":"http://arxiv.org/abs/2404.05295v1","category":"cs.RO"}
{"created":"2024-04-08 08:31:28","title":"Long-time Self-body Image Acquisition and its Application to the Control of Musculoskeletal Structures","abstract":"The tendon-driven musculoskeletal humanoid has many benefits that human beings have, but the modeling of its complex muscle and bone structures is difficult and conventional model-based controls cannot realize intended movements. Therefore, a learning control mechanism that acquires nonlinear relationships between joint angles, muscle tensions, and muscle lengths from the actual robot is necessary. In this study, we propose a system which runs the learning control mechanism for a long time to keep the self-body image of the musculoskeletal humanoid correct at all times. Also, we show that the musculoskeletal humanoid can conduct position control, torque control, and variable stiffness control using this self-body image. We conduct a long-time self-body image acquisition experiment lasting 3 hours, evaluate variable stiffness control using the self-body image, etc., and discuss the superiority and practicality of the self-body image acquisition of musculoskeletal structures, comprehensively.","sentences":["The tendon-driven musculoskeletal humanoid has many benefits that human beings have, but the modeling of its complex muscle and bone structures is difficult and conventional model-based controls cannot realize intended movements.","Therefore, a learning control mechanism that acquires nonlinear relationships between joint angles, muscle tensions, and muscle lengths from the actual robot is necessary.","In this study, we propose a system which runs the learning control mechanism for a long time to keep the self-body image of the musculoskeletal humanoid correct at all times.","Also, we show that the musculoskeletal humanoid can conduct position control, torque control, and variable stiffness control using this self-body image.","We conduct a long-time self-body image acquisition experiment lasting 3 hours, evaluate variable stiffness control using the self-body image, etc., and discuss the superiority and practicality of the self-body image acquisition of musculoskeletal structures, comprehensively."],"url":"http://arxiv.org/abs/2404.05293v1","category":"cs.RO"}
{"created":"2024-04-08 08:31:13","title":"Well-posedness of the initial boundary value problem for degenerate hyperbolic systems with a localized term and its application to the linearized system for the motion of an inextensible hanging string","abstract":"Motivated by an analysis on the well-posedness of the initial boundary value problem for the motion of an inextensible hanging string, we first consider an initial boundary value problem for one-dimensional degenerate hyperbolic systems with a localized term and show its well-posedness in weighted Sobolev spaces. We then consider the linearized system for the motion of an inextensible hanging string. Well-posedness of its initial boundary value problem is demonstrated as an application of the result obtained in the first part.","sentences":["Motivated by an analysis on the well-posedness of the initial boundary value problem for the motion of an inextensible hanging string, we first consider an initial boundary value problem for one-dimensional degenerate hyperbolic systems with a localized term and show its well-posedness in weighted Sobolev spaces.","We then consider the linearized system for the motion of an inextensible hanging string.","Well-posedness of its initial boundary value problem is demonstrated as an application of the result obtained in the first part."],"url":"http://arxiv.org/abs/2404.05292v1","category":"math.AP"}
{"created":"2024-04-08 08:24:09","title":"Online Self-body Image Acquisition Considering Changes in Muscle Routes Caused by Softness of Body Tissue for Tendon-driven Musculoskeletal Humanoids","abstract":"Tendon-driven musculoskeletal humanoids have many benefits in terms of the flexible spine, multiple degrees of freedom, and variable stiffness. At the same time, because of its body complexity, there are problems in controllability. First, due to the large difference between the actual robot and its geometric model, it cannot move as intended and large internal muscle tension may emerge. Second, movements which do not appear as changes in muscle lengths may emerge, because of the muscle route changes caused by softness of body tissue. To solve these problems, we construct two models: ideal joint-muscle model and muscle-route change model, using a neural network. We initialize these models by a man-made geometric model and update them online using the sensor information of the actual robot. We validate that the tendon-driven musculoskeletal humanoid Kengoro is able to obtain a correct self-body image through several experiments.","sentences":["Tendon-driven musculoskeletal humanoids have many benefits in terms of the flexible spine, multiple degrees of freedom, and variable stiffness.","At the same time, because of its body complexity, there are problems in controllability.","First, due to the large difference between the actual robot and its geometric model, it cannot move as intended and large internal muscle tension may emerge.","Second, movements which do not appear as changes in muscle lengths may emerge, because of the muscle route changes caused by softness of body tissue.","To solve these problems, we construct two models: ideal joint-muscle model and muscle-route change model, using a neural network.","We initialize these models by a man-made geometric model and update them online using the sensor information of the actual robot.","We validate that the tendon-driven musculoskeletal humanoid Kengoro is able to obtain a correct self-body image through several experiments."],"url":"http://arxiv.org/abs/2404.05286v1","category":"cs.RO"}
{"created":"2024-04-08 08:18:40","title":"Characterization of the ESPRESSO Line-Spread Function and Improvement of the Wavelength Calibration Accuracy","abstract":"Achieving a truly accurate wavelength calibration of high-dispersion echelle spectrographs is a challenging task but crucially needed for certain science cases, e.g. to test for a possible variation of the fine-structure constant in quasar spectra. One of the spectrographs best suited for this mission is VLT/ESPRESSO. Nevertheless, previous studies have identified significant discrepancies between the classical wavelength solutions and the one derived independently from the laser frequency comb. The dominant parts of these systematics were intra-order distortions, most-likely related to a deviation of the instrumental line-spread function from the assumed Gaussian shape. Here, we therefore present a study focused on a detailed modeling of the ESPRESSO instrumental line-spread function. We demonstrate that it is strongly asymmetric, non-Gaussian, different for the two slices and fibers, and varies significantly along the spectral orders. Incorporating the determined non-parametric model in the wavelength calibration process drastically improves the wavelength calibration accuracy, reducing the discrepancies between the two independent wavelength solutions from 50m/s to about 10m/s. The most striking success is, however, that the different fibers and slices now provide fully consistent measurements with a scatter of just a couple m/s. This demonstrates that the instrument-related systematics can be nearly eliminated over most of the spectral range by properly taking into account the complex shape of the instrumental line-spread function and paves the way for further optimizations of the wavelength calibration process.","sentences":["Achieving a truly accurate wavelength calibration of high-dispersion echelle spectrographs is a challenging task but crucially needed for certain science cases, e.g. to test for a possible variation of the fine-structure constant in quasar spectra.","One of the spectrographs best suited for this mission is VLT/ESPRESSO.","Nevertheless, previous studies have identified significant discrepancies between the classical wavelength solutions and the one derived independently from the laser frequency comb.","The dominant parts of these systematics were intra-order distortions, most-likely related to a deviation of the instrumental line-spread function from the assumed Gaussian shape.","Here, we therefore present a study focused on a detailed modeling of the ESPRESSO instrumental line-spread function.","We demonstrate that it is strongly asymmetric, non-Gaussian, different for the two slices and fibers, and varies significantly along the spectral orders.","Incorporating the determined non-parametric model in the wavelength calibration process drastically improves the wavelength calibration accuracy, reducing the discrepancies between the two independent wavelength solutions from 50m/s to about 10m/s.","The most striking success is, however, that the different fibers and slices now provide fully consistent measurements with a scatter of just a couple m/s.","This demonstrates that the instrument-related systematics can be nearly eliminated over most of the spectral range by properly taking into account the complex shape of the instrumental line-spread function and paves the way for further optimizations of the wavelength calibration process."],"url":"http://arxiv.org/abs/2404.05283v1","category":"astro-ph.IM"}
{"created":"2024-04-08 08:11:26","title":"The Galactic bulge exploration II. Line-of-sight velocity templates for single-mode RR~Lyrae stars","abstract":"We present a new set of tools to derive systemic velocities for single-mode RR~Lyrae stars from visual and near-infrared spectra. We derived scaling relations and line-of-sight velocity templates using both APOGEE and {\\it Gaia} spectroscopic products combined with photometric $G$-band amplitudes. We provide a means to estimate systemic velocities for the RR~Lyrae subclasses, RRab and RRc. Our analysis indicates that the scaling relation between the photometric and line-of-sight velocity amplitudes is nonlinear, with a break in a linear relation occurring around 0.4mag in both the $V$-band and $G$-band amplitudes. We did not observe such a break in the relation for the first-overtone pulsators. Using stellar pulsation models, we further confirm and examine the nonlinearity in scaling relation for the RRab subclass. We observed little to no variation with stellar parameters (mass, metallicity, and luminosity) in the scaling relation between the photometric and line-of-sight velocity amplitudes for fundamental-mode pulsators. We observed an offset in the scaling relation between the observations and stellar pulsation models, mainly in the low-amplitude RR~Lyrae regime. This offset disappears when different sets of convective parameters are used. Thus, the Fourier amplitudes obtained from the photometry and line-of-sight velocity measurements can be utilized to constrain convective parameters of stellar pulsation models. The scaling relations and templates for APOGEE and {\\it Gaia} data accurately predict systemic velocities compared to literature values. In addition, our tools derived from the {\\it Gaia} spectra improve the precision of the derived systemic velocities by approximately 50 percent and provide a better description of the uncertainty distribution in comparison with previous studies. Our newly derived tools will be used for RR~Lyrae variables observed toward the Galactic bulge.","sentences":["We present a new set of tools to derive systemic velocities for single-mode RR~Lyrae stars from visual and near-infrared spectra.","We derived scaling relations and line-of-sight velocity templates using both APOGEE and {\\it Gaia} spectroscopic products combined with photometric $G$-band amplitudes.","We provide a means to estimate systemic velocities for the RR~Lyrae subclasses, RRab and RRc.","Our analysis indicates that the scaling relation between the photometric and line-of-sight velocity amplitudes is nonlinear, with a break in a linear relation occurring around 0.4mag in both the $V$-band and $G$-band amplitudes.","We did not observe such a break in the relation for the first-overtone pulsators.","Using stellar pulsation models, we further confirm and examine the nonlinearity in scaling relation for the RRab subclass.","We observed little to no variation with stellar parameters (mass, metallicity, and luminosity) in the scaling relation between the photometric and line-of-sight velocity amplitudes for fundamental-mode pulsators.","We observed an offset in the scaling relation between the observations and stellar pulsation models, mainly in the low-amplitude RR~Lyrae regime.","This offset disappears when different sets of convective parameters are used.","Thus, the Fourier amplitudes obtained from the photometry and line-of-sight velocity measurements can be utilized to constrain convective parameters of stellar pulsation models.","The scaling relations and templates for APOGEE and {\\it Gaia} data accurately predict systemic velocities compared to literature values.","In addition, our tools derived from the {\\it Gaia} spectra improve the precision of the derived systemic velocities by approximately 50 percent and provide a better description of the uncertainty distribution in comparison with previous studies.","Our newly derived tools will be used for RR~Lyrae variables observed toward the Galactic bulge."],"url":"http://arxiv.org/abs/2404.05279v1","category":"astro-ph.SR"}
{"created":"2024-04-08 08:11:07","title":"Coherent transceiver architecture enabling data transmission and optical identification","abstract":"We propose a coherent transceiver architecture able to transmit information and enhance the security of the optical network by identifying other optical systems and subsystems. Simulations show that identification is obtained with sufficient reliability in standard operating conditions.","sentences":["We propose a coherent transceiver architecture able to transmit information and enhance the security of the optical network by identifying other optical systems and subsystems.","Simulations show that identification is obtained with sufficient reliability in standard operating conditions."],"url":"http://arxiv.org/abs/2404.05278v1","category":"cs.IT"}
{"created":"2024-04-08 08:08:27","title":"On the complexity of normalization for the planar $\u03bb$-calculus","abstract":"We sketch a tentative proof of P-completeness for the $\\beta$-convertibility problem on untyped planar (a.k.a. ordered or non-commutative) $\\lambda$-terms.","sentences":["We sketch a tentative proof of P-completeness for the $\\beta$-convertibility problem on untyped planar (a.k.a. ordered or non-commutative) $\\lambda$-terms."],"url":"http://arxiv.org/abs/2404.05276v1","category":"cs.LO"}
{"created":"2024-04-08 08:04:44","title":"Deep Optics for Video Snapshot Compressive Imaging","abstract":"Video snapshot compressive imaging (SCI) aims to capture a sequence of video frames with only a single shot of a 2D detector, whose backbones rest in optical modulation patterns (also known as masks) and a computational reconstruction algorithm. Advanced deep learning algorithms and mature hardware are putting video SCI into practical applications. Yet, there are two clouds in the sunshine of SCI: i) low dynamic range as a victim of high temporal multiplexing, and ii) existing deep learning algorithms' degradation on real system. To address these challenges, this paper presents a deep optics framework to jointly optimize masks and a reconstruction network. Specifically, we first propose a new type of structural mask to realize motion-aware and full-dynamic-range measurement. Considering the motion awareness property in measurement domain, we develop an efficient network for video SCI reconstruction using Transformer to capture long-term temporal dependencies, dubbed Res2former. Moreover, sensor response is introduced into the forward model of video SCI to guarantee end-to-end model training close to real system. Finally, we implement the learned structural masks on a digital micro-mirror device. Experimental results on synthetic and real data validate the effectiveness of the proposed framework. We believe this is a milestone for real-world video SCI. The source code and data are available at https://github.com/pwangcs/DeepOpticsSCI.","sentences":["Video snapshot compressive imaging (SCI) aims to capture a sequence of video frames with only a single shot of a 2D detector, whose backbones rest in optical modulation patterns (also known as masks) and a computational reconstruction algorithm.","Advanced deep learning algorithms and mature hardware are putting video SCI into practical applications.","Yet, there are two clouds in the sunshine of SCI: i) low dynamic range as a victim of high temporal multiplexing, and ii) existing deep learning algorithms' degradation on real system.","To address these challenges, this paper presents a deep optics framework to jointly optimize masks and a reconstruction network.","Specifically, we first propose a new type of structural mask to realize motion-aware and full-dynamic-range measurement.","Considering the motion awareness property in measurement domain, we develop an efficient network for video SCI reconstruction using Transformer to capture long-term temporal dependencies, dubbed Res2former.","Moreover, sensor response is introduced into the forward model of video SCI to guarantee end-to-end model training close to real system.","Finally, we implement the learned structural masks on a digital micro-mirror device.","Experimental results on synthetic and real data validate the effectiveness of the proposed framework.","We believe this is a milestone for real-world video SCI.","The source code and data are available at https://github.com/pwangcs/DeepOpticsSCI."],"url":"http://arxiv.org/abs/2404.05274v1","category":"cs.CV"}
{"created":"2024-04-08 07:36:24","title":"Relativistic chaotic scattering: unveiling scaling laws for trapped trajectories","abstract":"In this paper, we study different types of phase space structures which appear in the context of relativistic chaotic scattering. By using the relativistic version of the H\\'{e}non-Heiles Hamiltonian, we numerically study the topology of different kind of exit basins and compare it with the case of low velocities in which the Newtonian version of the system is valid. Specifically, we numerically study the escapes in the phase space, in the energy plane and also in the $\\beta$ plane which richly characterize the dynamics of the system. In all cases, fractal structures are present, and the escaping dynamics is characterized. Besides, in every case a scaling law is numerically obtained in which the percentage of the trapped trajectories as a function of the relativistic parameter $\\beta$ and the energy is obtained. Our work could be useful in the context of charged particles which eventually can be trapped in the magnetosphere, where the analysis of these structures can be relevant.","sentences":["In this paper, we study different types of phase space structures which appear in the context of relativistic chaotic scattering.","By using the relativistic version of the H\\'{e}non-Heiles Hamiltonian, we numerically study the topology of different kind of exit basins and compare it with the case of low velocities in which the Newtonian version of the system is valid.","Specifically, we numerically study the escapes in the phase space, in the energy plane and also in the $\\beta$ plane which richly characterize the dynamics of the system.","In all cases, fractal structures are present, and the escaping dynamics is characterized.","Besides, in every case a scaling law is numerically obtained in which the percentage of the trapped trajectories as a function of the relativistic parameter $\\beta$ and the energy is obtained.","Our work could be useful in the context of charged particles which eventually can be trapped in the magnetosphere, where the analysis of these structures can be relevant."],"url":"http://arxiv.org/abs/2404.05254v1","category":"nlin.CD"}
{"created":"2024-04-08 07:24:45","title":"Some extensions of the Brouwer fixed point theorem","abstract":"We study the existence of fixed points for continuous maps $f$ from an $n$-ball $X$ in $\\mathbb R^n$ to $\\mathbb R^n$ with $n\\geq 1$. We show that $f$ has a fixed point if, for some absolute retract $Y\\subset\\partial X$, $f(Y)\\subset X$ and $\\partial X-Y$ is an $(f, X)$-blockading set. For $n\\geq 2$, let $D$ be an $n$-ball in $X$ and $Y$ be an $(n-1)$-ball in $\\partial X$. Relying on the result just mentioned, we show the existence of a fixed point of $f$, if $D$ and $Y$ are well placed and behave well under $f$, and ${\\rm deg}(f_D)=-{\\rm deg}(f_{\\partial Y})$, where $f_D=f|D: D \\rightarrow \\mathbb{R}^n$ and $f_{\\partial Y}=f|\\partial Y: \\partial Y \\rightarrow \\partial Y$. The degree ${\\rm deg}(f_D)$ of $f_D$ is explicitly defined and some elementary properties of which are investigated. These results extend the Brouwer fixed point theorem.","sentences":["We study the existence of fixed points for continuous maps $f$ from an $n$-ball $X$ in $\\mathbb R^n$ to $\\mathbb R^n$ with $n\\geq 1$.","We show that $f$ has a fixed point if, for some absolute retract $Y\\subset\\partial X$, $f(Y)\\subset X$ and $\\partial X-Y$ is an $(f, X)$-blockading set.","For $n\\geq 2$, let $D$ be an $n$-ball in $X$ and $Y$ be an $(n-1)$-ball in $\\partial X$. Relying on the result just mentioned, we show the existence of a fixed point of $f$, if $D$ and $Y$ are well placed and behave well under $f$, and ${\\rm deg}(f_D)=-{\\rm deg}(f_{\\partial Y})$, where $f_D=f|D: D \\rightarrow \\mathbb{R}^n$ and $f_{\\partial Y}=f|\\partial Y: \\partial Y \\rightarrow \\partial Y$.","The degree ${\\rm deg}(f_D)$ of $f_D$ is explicitly defined and some elementary properties of which are investigated.","These results extend the Brouwer fixed point theorem."],"url":"http://arxiv.org/abs/2404.05248v1","category":"math.DS"}
{"created":"2024-04-08 07:11:33","title":"Lightweight Inference for Forward-Forward Training Algorithm","abstract":"The human brain performs tasks with an outstanding energy-efficiency, i.e., with approximately 20 Watts. The state-of-the-art Artificial/Deep Neural Networks (ANN/DNN), on the other hand, have recently been shown to consume massive amounts of energy. The training of these ANNs/DNNs is done almost exclusively based on the back-propagation algorithm, which is known to be biologically implausible. This has led to a new generation of forward-only techniques, including the Forward-Forward algorithm. In this paper, we propose a lightweight inference scheme specifically designed for DNNs trained using the Forward-Forward algorithm. We have evaluated our proposed lightweight inference scheme in the case of the MNIST and CIFAR datasets, as well as two real-world applications, namely, epileptic seizure detection and cardiac arrhythmia classification using wearable technologies, where complexity overheads/energy consumption is a major constraint, and demonstrate its relevance.","sentences":["The human brain performs tasks with an outstanding energy-efficiency, i.e., with approximately 20 Watts.","The state-of-the-art Artificial/Deep Neural Networks (ANN/DNN), on the other hand, have recently been shown to consume massive amounts of energy.","The training of these ANNs/DNNs is done almost exclusively based on the back-propagation algorithm, which is known to be biologically implausible.","This has led to a new generation of forward-only techniques, including the Forward-Forward algorithm.","In this paper, we propose a lightweight inference scheme specifically designed for DNNs trained using the Forward-Forward algorithm.","We have evaluated our proposed lightweight inference scheme in the case of the MNIST and CIFAR datasets, as well as two real-world applications, namely, epileptic seizure detection and cardiac arrhythmia classification using wearable technologies, where complexity overheads/energy consumption is a major constraint, and demonstrate its relevance."],"url":"http://arxiv.org/abs/2404.05241v1","category":"cs.LG"}
{"created":"2024-04-08 06:57:37","title":"Local sparse limit of a particle coalescence model under environmental noise","abstract":"A system of diffusions subject to common (environmental) noise is investigated. Due to the scaling limit assumptions on the common noise, a deterministic PDE limit is obtained. Particles interact by coalescence and the sparse regime of Hammond and Rezakhanlou is imposed, so that a nontrivial cell equation comes into play to determine the average coalescence rate. The corrector to the mean coalescence rate provided by that theory explains the role of turbulence in the enhancement of coalescence. Notice that classical models of particle coalescence in turbulent fluids are of inertial type, so it is a striking fact that a non-inertial model allows one to get meaningful physical results, thanks to the information contained in the corrector.","sentences":["A system of diffusions subject to common (environmental) noise is investigated.","Due to the scaling limit assumptions on the common noise, a deterministic PDE limit is obtained.","Particles interact by coalescence and the sparse regime of Hammond and Rezakhanlou is imposed, so that a nontrivial cell equation comes into play to determine the average coalescence rate.","The corrector to the mean coalescence rate provided by that theory explains the role of turbulence in the enhancement of coalescence.","Notice that classical models of particle coalescence in turbulent fluids are of inertial type, so it is a striking fact that a non-inertial model allows one to get meaningful physical results, thanks to the information contained in the corrector."],"url":"http://arxiv.org/abs/2404.05233v1","category":"math.PR"}
{"created":"2024-04-08 06:45:32","title":"Fair Machine Guidance to Enhance Fair Decision Making in Biased People","abstract":"Teaching unbiased decision-making is crucial for addressing biased decision-making in daily life. Although both raising awareness of personal biases and providing guidance on unbiased decision-making are essential, the latter topics remains under-researched. In this study, we developed and evaluated an AI system aimed at educating individuals on making unbiased decisions using fairness-aware machine learning. In a between-subjects experimental design, 99 participants who were prone to bias performed personal assessment tasks. They were divided into two groups: a) those who received AI guidance for fair decision-making before the task and b) those who received no such guidance but were informed of their biases. The results suggest that although several participants doubted the fairness of the AI system, fair machine guidance prompted them to reassess their views regarding fairness, reflect on their biases, and modify their decision-making criteria. Our findings provide insights into the design of AI systems for guiding fair decision-making in humans.","sentences":["Teaching unbiased decision-making is crucial for addressing biased decision-making in daily life.","Although both raising awareness of personal biases and providing guidance on unbiased decision-making are essential, the latter topics remains under-researched.","In this study, we developed and evaluated an AI system aimed at educating individuals on making unbiased decisions using fairness-aware machine learning.","In a between-subjects experimental design, 99 participants who were prone to bias performed personal assessment tasks.","They were divided into two groups: a) those who received AI guidance for fair decision-making before the task and b) those who received no such guidance but were informed of their biases.","The results suggest that although several participants doubted the fairness of the AI system, fair machine guidance prompted them to reassess their views regarding fairness, reflect on their biases, and modify their decision-making criteria.","Our findings provide insights into the design of AI systems for guiding fair decision-making in humans."],"url":"http://arxiv.org/abs/2404.05228v1","category":"cs.HC"}
{"created":"2024-04-08 06:44:50","title":"A Note on the Common Haar State Model","abstract":"Common random string model is a popular model in classical cryptography with many constructions proposed in this model. We study a quantum analogue of this model called the common Haar state model, which was also studied in an independent work by Chen, Coladangelo and Sattath (arXiv 2024). In this model, every party in the cryptographic system receives many copies of one or more i.i.d Haar states.   Our main result is the construction of a statistically secure PRSG with: (a) the output length of the PRSG is strictly larger than the key size, (b) the security holds even if the adversary receives $O\\left(\\frac{\\lambda}{(\\log(\\lambda))^{1.01}} \\right)$ copies of the pseudorandom state. We show the optimality of our construction by showing a matching lower bound. Our construction is simple and its analysis uses elementary techniques.","sentences":["Common random string model is a popular model in classical cryptography with many constructions proposed in this model.","We study a quantum analogue of this model called the common Haar state model, which was also studied in an independent work by Chen, Coladangelo and Sattath (arXiv 2024).","In this model, every party in the cryptographic system receives many copies of one or more i.i.d Haar states.   ","Our main result is the construction of a statistically secure PRSG with: (a) the output length of the PRSG is strictly larger than the key size, (b) the security holds even if the adversary receives $O\\left(\\frac{\\lambda}{(\\log(\\lambda))^{1.01}} \\right)$ copies of the pseudorandom state.","We show the optimality of our construction by showing a matching lower bound.","Our construction is simple and its analysis uses elementary techniques."],"url":"http://arxiv.org/abs/2404.05227v1","category":"quant-ph"}
{"created":"2024-04-08 06:11:15","title":"Network-Constrained Unit Commitment with Flexible Temporal Resolution","abstract":"Modern network-constrained unit commitment (NCUC) bears a heavy computational burden due to the ever-growing model scale. This situation becomes more challenging when detailed operational characteristics, complicated constraints, and multiple objectives are considered. We propose a novel simplification method to determine the flexible temporal resolution for acceleration and near-optimal solutions. The flexible temporal resolution is determined by analyzing the impact on generators in each adaptive time period with awareness of congestion effects. Additionally, multiple improvements are employed on the existing NCUC model compatible with flexible temporal resolution to reduce the number of integer variables while preserving the original features. A case study using the IEEE 118-bus and the Polish 2736-bus systems verifies that the proposed method achieves substantial acceleration with low cost variation and high accuracy.","sentences":["Modern network-constrained unit commitment (NCUC) bears a heavy computational burden due to the ever-growing model scale.","This situation becomes more challenging when detailed operational characteristics, complicated constraints, and multiple objectives are considered.","We propose a novel simplification method to determine the flexible temporal resolution for acceleration and near-optimal solutions.","The flexible temporal resolution is determined by analyzing the impact on generators in each adaptive time period with awareness of congestion effects.","Additionally, multiple improvements are employed on the existing NCUC model compatible with flexible temporal resolution to reduce the number of integer variables while preserving the original features.","A case study using the IEEE 118-bus and the Polish 2736-bus systems verifies that the proposed method achieves substantial acceleration with low cost variation and high accuracy."],"url":"http://arxiv.org/abs/2404.05217v1","category":"eess.SY"}
{"created":"2024-04-08 05:58:07","title":"DiffCJK: Conditional Diffusion Model for High-Quality and Wide-coverage CJK Character Generation","abstract":"Chinese, Japanese, and Korean (CJK), with a vast number of native speakers, has profound influence on society and culture. The typesetting of CJK languages carries a wide range of requirements due to the complexity of their scripts and unique literary traditions. A critical aspect of this typesetting process is that CJK fonts need to provide a set of consistent-looking glyphs for approximately one hundred thousand characters. However, creating such a font is inherently labor-intensive and expensive, which significantly hampers the development of new CJK fonts for typesetting, historical, aesthetic, or artistic purposes.   To bridge this gap, we are motivated by recent advancements in diffusion-based generative models and propose a novel diffusion method for generating glyphs in a targeted style from a \\emph{single} conditioned, standard glyph form. Our experiments show that our method is capable of generating fonts of both printed and hand-written styles, the latter of which presents a greater challenge. Moreover, our approach shows remarkable zero-shot generalization capabilities for non-CJK but Chinese-inspired scripts. We also show our method facilitates smooth style interpolation and generates bitmap images suitable for vectorization, which is crucial in the font creation process. In summary, our proposed method opens the door to high-quality, generative model-assisted font creation for CJK characters, for both typesetting and artistic endeavors.","sentences":["Chinese, Japanese, and Korean (CJK), with a vast number of native speakers, has profound influence on society and culture.","The typesetting of CJK languages carries a wide range of requirements due to the complexity of their scripts and unique literary traditions.","A critical aspect of this typesetting process is that CJK fonts need to provide a set of consistent-looking glyphs for approximately one hundred thousand characters.","However, creating such a font is inherently labor-intensive and expensive, which significantly hampers the development of new CJK fonts for typesetting, historical, aesthetic, or artistic purposes.   ","To bridge this gap, we are motivated by recent advancements in diffusion-based generative models and propose a novel diffusion method for generating glyphs in a targeted style from a \\emph{single} conditioned, standard glyph form.","Our experiments show that our method is capable of generating fonts of both printed and hand-written styles, the latter of which presents a greater challenge.","Moreover, our approach shows remarkable zero-shot generalization capabilities for non-CJK but Chinese-inspired scripts.","We also show our method facilitates smooth style interpolation and generates bitmap images suitable for vectorization, which is crucial in the font creation process.","In summary, our proposed method opens the door to high-quality, generative model-assisted font creation for CJK characters, for both typesetting and artistic endeavors."],"url":"http://arxiv.org/abs/2404.05212v1","category":"cs.CV"}
{"created":"2024-04-08 05:50:46","title":"Multi-level Graph Subspace Contrastive Learning for Hyperspectral Image Clustering","abstract":"Hyperspectral image (HSI) clustering is a challenging task due to its high complexity. Despite subspace clustering shows impressive performance for HSI, traditional methods tend to ignore the global-local interaction in HSI data. In this study, we proposed a multi-level graph subspace contrastive learning (MLGSC) for HSI clustering. The model is divided into the following main parts. Graph convolution subspace construction: utilizing spectral and texture feautures to construct two graph convolution views. Local-global graph representation: local graph representations were obtained by step-by-step convolutions and a more representative global graph representation was obtained using an attention-based pooling strategy. Multi-level graph subspace contrastive learning: multi-level contrastive learning was conducted to obtain local-global joint graph representations, to improve the consistency of the positive samples between views, and to obtain more robust graph embeddings. Specifically, graph-level contrastive learning is used to better learn global representations of HSI data. Node-level intra-view and inter-view contrastive learning is designed to learn joint representations of local regions of HSI. The proposed model is evaluated on four popular HSI datasets: Indian Pines, Pavia University, Houston, and Xu Zhou. The overall accuracies are 97.75%, 99.96%, 92.28%, and 95.73%, which significantly outperforms the current state-of-the-art clustering methods.","sentences":["Hyperspectral image (HSI) clustering is a challenging task due to its high complexity.","Despite subspace clustering shows impressive performance for HSI, traditional methods tend to ignore the global-local interaction in HSI data.","In this study, we proposed a multi-level graph subspace contrastive learning (MLGSC) for HSI clustering.","The model is divided into the following main parts.","Graph convolution subspace construction: utilizing spectral and texture feautures to construct two graph convolution views.","Local-global graph representation: local graph representations were obtained by step-by-step convolutions and a more representative global graph representation was obtained using an attention-based pooling strategy.","Multi-level graph subspace contrastive learning: multi-level contrastive learning was conducted to obtain local-global joint graph representations, to improve the consistency of the positive samples between views, and to obtain more robust graph embeddings.","Specifically, graph-level contrastive learning is used to better learn global representations of HSI data.","Node-level intra-view and inter-view contrastive learning is designed to learn joint representations of local regions of HSI.","The proposed model is evaluated on four popular HSI datasets: Indian Pines, Pavia University, Houston, and Xu Zhou.","The overall accuracies are 97.75%, 99.96%, 92.28%, and 95.73%, which significantly outperforms the current state-of-the-art clustering methods."],"url":"http://arxiv.org/abs/2404.05211v1","category":"cs.CV"}
{"created":"2024-04-08 05:18:39","title":"A secure and private ensemble matcher using multi-vault obfuscated templates","abstract":"Given the irrevocability of biometric samples and mounting privacy concerns, biometric template security and secure matching are among the essential features of any well-designed modern biometric system. In this paper, we propose an obfuscation method that hides the biometric template information with just enough chaff. The main idea is to reduce the number of chaff points to a practical level by creating n sub-templates from the original template and hiding each sub-template with m chaff points. During verification, s closest vectors to the biometric query are retrieved from each vault and then combined to generate hash values that are compared with the stored hash value. We demonstrate the effectiveness of synthetic facial images, generated by a Generative Adversarial Network (GAN), as ``random chaff points'' within a secure-vault authorization system. This approach safeguards user identities during training and deployment. We tested our protocol using the AT&T, GT, and LFW face datasets, with the ROC areas under the curve being 0.99, 0.99, and 0.90, respectively. These numbers were close to those of the unprotected templates, showing that our method does not adversely affect accuracy.","sentences":["Given the irrevocability of biometric samples and mounting privacy concerns, biometric template security and secure matching are among the essential features of any well-designed modern biometric system.","In this paper, we propose an obfuscation method that hides the biometric template information with just enough chaff.","The main idea is to reduce the number of chaff points to a practical level by creating n sub-templates from the original template and hiding each sub-template with m chaff points.","During verification, s closest vectors to the biometric query are retrieved from each vault and then combined to generate hash values that are compared with the stored hash value.","We demonstrate the effectiveness of synthetic facial images, generated by a Generative Adversarial Network (GAN), as ``random chaff points'' within a secure-vault authorization system.","This approach safeguards user identities during training and deployment.","We tested our protocol using the AT&T, GT, and LFW face datasets, with the ROC areas under the curve being 0.99, 0.99, and 0.90, respectively.","These numbers were close to those of the unprotected templates, showing that our method does not adversely affect accuracy."],"url":"http://arxiv.org/abs/2404.05205v1","category":"cs.CV"}
{"created":"2024-04-08 05:10:35","title":"MeSA-DRL: Memory-Enhanced Deep Reinforcement Learning for Advanced Socially Aware Robot Navigation in Crowded Environments","abstract":"Autonomous navigation capabilities play a critical role in service robots operating in environments where human interactions are pivotal, due to the dynamic and unpredictable nature of these environments. However, the variability in human behavior presents a substantial challenge for robots in predicting and anticipating movements, particularly in crowded scenarios. To address this issue, a memory-enabled deep reinforcement learning framework is proposed for autonomous robot navigation in diverse pedestrian scenarios. The proposed framework leverages long-term memory to retain essential information about the surroundings and model sequential dependencies effectively. The importance of human-robot interactions is also encoded to assign higher attention to these interactions. A global planning mechanism is incorporated into the memory-enabled architecture. Additionally, a multi-term reward system is designed to prioritize and encourage long-sighted robot behaviors by incorporating dynamic warning zones. Simultaneously, it promotes smooth trajectories and minimizes the time taken to reach the robot's desired goal. Extensive simulation experiments show that the suggested approach outperforms representative state-of-the-art methods, showcasing its ability to a navigation efficiency and safety in real-world scenarios.","sentences":["Autonomous navigation capabilities play a critical role in service robots operating in environments where human interactions are pivotal, due to the dynamic and unpredictable nature of these environments.","However, the variability in human behavior presents a substantial challenge for robots in predicting and anticipating movements, particularly in crowded scenarios.","To address this issue, a memory-enabled deep reinforcement learning framework is proposed for autonomous robot navigation in diverse pedestrian scenarios.","The proposed framework leverages long-term memory to retain essential information about the surroundings and model sequential dependencies effectively.","The importance of human-robot interactions is also encoded to assign higher attention to these interactions.","A global planning mechanism is incorporated into the memory-enabled architecture.","Additionally, a multi-term reward system is designed to prioritize and encourage long-sighted robot behaviors by incorporating dynamic warning zones.","Simultaneously, it promotes smooth trajectories and minimizes the time taken to reach the robot's desired goal.","Extensive simulation experiments show that the suggested approach outperforms representative state-of-the-art methods, showcasing its ability to a navigation efficiency and safety in real-world scenarios."],"url":"http://arxiv.org/abs/2404.05203v1","category":"cs.RO"}
{"created":"2024-04-08 04:54:34","title":"The geometry of high-dimensional phase diagrams: II. The duality between closed and open chemical systems","abstract":"In our ambition to construct high-dimensional phase diagrams featuring any thermodynamic variable on its axes, here we examine the duality between extensive and intensive conjugate variables in equilibrium thermodynamics. This duality manifests from the distinction between closed and open boundary conditions of a thermodynamic system, to the relationship between the Internal Energy and its Legendre transformations, to the point-line duality in calculating convex hulls versus half-space intersections. Here we focus on the duality relationships of chemical work, with extensive composition variables, N, and intensive chemical potentials, {\\mu}. In particular, we explore mixed composition-chemical potential diagrams for oxynitride synthesis, lithium-ion cathode stability, and oxidation of high-entropy alloys. We further illustrate how chemical potential diagrams reveal the driving forces for non-equilibrium growth and dissolution kinetics.","sentences":["In our ambition to construct high-dimensional phase diagrams featuring any thermodynamic variable on its axes, here we examine the duality between extensive and intensive conjugate variables in equilibrium thermodynamics.","This duality manifests from the distinction between closed and open boundary conditions of a thermodynamic system, to the relationship between the Internal Energy and its Legendre transformations, to the point-line duality in calculating convex hulls versus half-space intersections.","Here we focus on the duality relationships of chemical work, with extensive composition variables, N, and intensive chemical potentials, {\\mu}.","In particular, we explore mixed composition-chemical potential diagrams for oxynitride synthesis, lithium-ion cathode stability, and oxidation of high-entropy alloys.","We further illustrate how chemical potential diagrams reveal the driving forces for non-equilibrium growth and dissolution kinetics."],"url":"http://arxiv.org/abs/2404.05197v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 04:51:17","title":"Conjugacy class fusion from four maximal subgroups of the Monster","abstract":"We determine the conjugacy class fusion from certain maximal subgroups of the Monster to the Monster, to justify the addition of these data to the Character Table Library in the computational algebra system GAP. The maximal subgroups in question are $(\\text{PSL}_2(11) {\\times} \\text{PSL}_2(11)){:}4$, $11^2{:}(5 {\\times} 2\\text{A}_5)$, $7^2{:}\\text{SL}_2(7)$, and $\\text{PSL}_2(19){:}2$. Our proofs are supported by reproducible calculations carried out using the Python package mmgroup, a computational construction of the Monster recently developed by Seysen.","sentences":["We determine the conjugacy class fusion from certain maximal subgroups of the Monster to the Monster, to justify the addition of these data to the Character Table Library in the computational algebra system GAP.","The maximal subgroups in question are $(\\text{PSL}_2(11) {\\times} \\text{PSL}_2(11)){:}4$, $11^2{:}(5 {\\times} 2\\text{A}_5)$, $7^2{:}\\text{SL}_2(7)$, and $\\text{PSL}_2(19){:}2$. Our proofs are supported by reproducible calculations carried out using the Python package mmgroup, a computational construction of the Monster recently developed by Seysen."],"url":"http://arxiv.org/abs/2404.05194v1","category":"math.GR"}
{"created":"2024-04-08 04:41:39","title":"ATFNet: Adaptive Time-Frequency Ensembled Network for Long-term Time Series Forecasting","abstract":"The intricate nature of time series data analysis benefits greatly from the distinct advantages offered by time and frequency domain representations. While the time domain is superior in representing local dependencies, particularly in non-periodic series, the frequency domain excels in capturing global dependencies, making it ideal for series with evident periodic patterns. To capitalize on both of these strengths, we propose ATFNet, an innovative framework that combines a time domain module and a frequency domain module to concurrently capture local and global dependencies in time series data. Specifically, we introduce Dominant Harmonic Series Energy Weighting, a novel mechanism for dynamically adjusting the weights between the two modules based on the periodicity of the input time series. In the frequency domain module, we enhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT, designed to address the challenge of discrete frequency misalignment. Additionally, our Complex-valued Spectrum Attention mechanism offers a novel approach to discern the intricate relationships between different frequency combinations. Extensive experiments across multiple real-world datasets demonstrate that our ATFNet framework outperforms current state-of-the-art methods in long-term time series forecasting.","sentences":["The intricate nature of time series data analysis benefits greatly from the distinct advantages offered by time and frequency domain representations.","While the time domain is superior in representing local dependencies, particularly in non-periodic series, the frequency domain excels in capturing global dependencies, making it ideal for series with evident periodic patterns.","To capitalize on both of these strengths, we propose ATFNet, an innovative framework that combines a time domain module and a frequency domain module to concurrently capture local and global dependencies in time series data.","Specifically, we introduce Dominant Harmonic Series Energy Weighting, a novel mechanism for dynamically adjusting the weights between the two modules based on the periodicity of the input time series.","In the frequency domain module, we enhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT, designed to address the challenge of discrete frequency misalignment.","Additionally, our Complex-valued Spectrum Attention mechanism offers a novel approach to discern the intricate relationships between different frequency combinations.","Extensive experiments across multiple real-world datasets demonstrate that our ATFNet framework outperforms current state-of-the-art methods in long-term time series forecasting."],"url":"http://arxiv.org/abs/2404.05192v1","category":"cs.LG"}
{"created":"2024-04-08 04:41:03","title":"Graph-based Untrained Neural Network Detector for OTFS Systems","abstract":"Inter-carrier interference (ICI) caused by mobile reflectors significantly degrades the conventional orthogonal frequency division multiplexing (OFDM) performance in high-mobility environments. The orthogonal time frequency space (OTFS) modulation system effectively represents ICI in the delay-Doppler domain, thus significantly outperforming OFDM. Existing iterative and neural network (NN) based OTFS detectors suffer from high complex matrix operations and performance degradation in untrained environments, where the real wireless channel does not match the one used in the training, which often happens in real wireless networks. In this paper, we propose to embed the prior knowledge of interference extracted from the estimated channel state information (CSI) as a directed graph into a decoder untrained neural network (DUNN), namely graph-based DUNN (GDUNN). We then combine it with Bayesian parallel interference cancellation (BPIC) for OTFS symbol detection, resulting in GDUNN-BPIC. Simulation results show that the proposed GDUNN-BPIC outperforms state-of-the-art OTFS detectors under imperfect CSI.","sentences":["Inter-carrier interference (ICI) caused by mobile reflectors significantly degrades the conventional orthogonal frequency division multiplexing (OFDM) performance in high-mobility environments.","The orthogonal time frequency space (OTFS) modulation system effectively represents ICI in the delay-Doppler domain, thus significantly outperforming OFDM.","Existing iterative and neural network (NN) based OTFS detectors suffer from high complex matrix operations and performance degradation in untrained environments, where the real wireless channel does not match the one used in the training, which often happens in real wireless networks.","In this paper, we propose to embed the prior knowledge of interference extracted from the estimated channel state information (CSI) as a directed graph into a decoder untrained neural network (DUNN), namely graph-based DUNN (GDUNN).","We then combine it with Bayesian parallel interference cancellation (BPIC) for OTFS symbol detection, resulting in GDUNN-BPIC.","Simulation results show that the proposed GDUNN-BPIC outperforms state-of-the-art OTFS detectors under imperfect CSI."],"url":"http://arxiv.org/abs/2404.05191v1","category":"eess.SP"}
{"created":"2024-04-08 04:25:05","title":"A user's guide to Beilinson-Kato's zeta elements","abstract":"In his ground-breaking work, K. Kato constructed the Euler system of Beilinson--Kato's zeta elements and proved spectacular results on the Iwasawa main conjecture for elliptic curves and the classical and $p$-adic Birch and Swinnerton-Dyer conjectures by using these elements. The goal of this expository lecture note is to explain how Kato's Euler systems fit into the framework of the arithmetic of elliptic curves and their Iwasawa theory, and we hope that this approach eventually helps the reader to read Kato's original paper more easily and with less pain. It is written for the proceedings of the program \"Elliptic curves and the special values of $L$-functions (ONLINE)\" in ICTS in August 2021.","sentences":["In his ground-breaking work, K. Kato constructed the Euler system of Beilinson--Kato's zeta elements and proved spectacular results on the Iwasawa main conjecture for elliptic curves and the classical and $p$-adic Birch and Swinnerton-Dyer conjectures by using these elements.","The goal of this expository lecture note is to explain how Kato's Euler systems fit into the framework of the arithmetic of elliptic curves and their Iwasawa theory, and we hope that this approach eventually helps the reader to read Kato's original paper more easily and with less pain.","It is written for the proceedings of the program \"Elliptic curves and the special values of $L$-functions (ONLINE)\" in ICTS in August 2021."],"url":"http://arxiv.org/abs/2404.05186v1","category":"math.NT"}
{"created":"2024-04-08 04:22:55","title":"Convergence analysis of controlled particle systems arising in deep learning: from finite to infinite sample size","abstract":"This paper deals with a class of neural SDEs and studies the limiting behavior of the associated sampled optimal control problems as the sample size grows to infinity. The neural SDEs with N samples can be linked to the N-particle systems with centralized control. We analyze the Hamilton--Jacobi--Bellman equation corresponding to the N-particle system and establish regularity results which are uniform in N. The uniform regularity estimates are obtained by the stochastic maximum principle and the analysis of a backward stochastic Riccati equation. Using these uniform regularity results, we show the convergence of the minima of objective functionals and optimal parameters of the neural SDEs as the sample size N tends to infinity. The limiting objects can be identified with suitable functions defined on the Wasserstein space of Borel probability measures. Furthermore, quantitative algebraic convergence rates are also obtained.","sentences":["This paper deals with a class of neural SDEs and studies the limiting behavior of the associated sampled optimal control problems as the sample size grows to infinity.","The neural SDEs with N samples can be linked to the N-particle systems with centralized control.","We analyze the Hamilton--Jacobi--Bellman equation corresponding to the N-particle system and establish regularity results which are uniform in N.","The uniform regularity estimates are obtained by the stochastic maximum principle and the analysis of a backward stochastic Riccati equation.","Using these uniform regularity results, we show the convergence of the minima of objective functionals and optimal parameters of the neural SDEs as the sample size","N tends to infinity.","The limiting objects can be identified with suitable functions defined on the Wasserstein space of Borel probability measures.","Furthermore, quantitative algebraic convergence rates are also obtained."],"url":"http://arxiv.org/abs/2404.05185v1","category":"math.OC"}
{"created":"2024-04-08 04:06:23","title":"Floer homology and square pegs","abstract":"We construct a version of Lagrangian Floer homology whose chain complex is generated by the inscriptions of a rectangle into a real analytic Jordan curve. By using its associated spectral invariants, we establish that a rectifiable Jordan curve admits inscriptions of a whole interval of rectangles. In particular, it inscribes a square if the area it encloses is more than half that of a circle of equal diameter.","sentences":["We construct a version of Lagrangian Floer homology whose chain complex is generated by the inscriptions of a rectangle into a real analytic Jordan curve.","By using its associated spectral invariants, we establish that a rectifiable Jordan curve admits inscriptions of a whole interval of rectangles.","In particular, it inscribes a square if the area it encloses is more than half that of a circle of equal diameter."],"url":"http://arxiv.org/abs/2404.05179v1","category":"math.SG"}
{"created":"2024-04-08 17:52:11","title":"Reconstructing the recombination history by combining early and late cosmological probes","abstract":"We develop and apply a new framework for reconstructing the ionization history during the epoch of recombination with combinations of cosmic microwave background (CMB), baryon acoustic oscillation (BAO) and supernova data. We find a wide range of ionization histories that are consistent with current CMB data, and also that cosmological parameter constraints are significantly weakened once freedom in recombination is introduced. BAO data partially break the degeneracy between cosmological parameters and the recombination model, and are therefore important in these reconstructions. The 95% confidence upper limits on H0 are 80.1 (70.7) km/s/Mpc given CMB (CMB+BAO) data, assuming no other changes are made to the standard cosmological model. Including Cepheid-calibrated supernova data in the analysis drives a preference for non-standard recombination histories with visibility functions that peak early and exhibit appreciable skewness. Forthcoming measurements from SPT-3G will reduce the uncertainties in our reconstructions by about a factor of two.","sentences":["We develop and apply a new framework for reconstructing the ionization history during the epoch of recombination with combinations of cosmic microwave background (CMB), baryon acoustic oscillation (BAO) and supernova data.","We find a wide range of ionization histories that are consistent with current CMB data, and also that cosmological parameter constraints are significantly weakened once freedom in recombination is introduced.","BAO data partially break the degeneracy between cosmological parameters and the recombination model, and are therefore important in these reconstructions.","The 95% confidence upper limits on H0 are 80.1 (70.7) km/s/Mpc given CMB (CMB+BAO) data, assuming no other changes are made to the standard cosmological model.","Including Cepheid-calibrated supernova data in the analysis drives a preference for non-standard recombination histories with visibility functions that peak early and exhibit appreciable skewness.","Forthcoming measurements from SPT-3G will reduce the uncertainties in our reconstructions by about a factor of two."],"url":"http://arxiv.org/abs/2404.05715v1","category":"astro-ph.CO"}
{"created":"2024-04-08 17:16:13","title":"The neutrino background from non-jetted active galactic nuclei","abstract":"Aims. We calculate the contribution to the neutrino background from the non-jetted active galactic nuclei (AGN) population following the recent IceCube association of TeV neutrinos with NGC 1068. Methods. We exploit our robust knowledge of the AGN X-ray luminosity function and evolution and convert it to the neutrino band by using NGC 1068 as a benchmark and a theoretically motivated neutrino spectrum. Results. The resulting neutrino background up to redshift 5 does not violate either the IceCube diffuse flux or the upper bounds for non-jetted AGN, although barely so. This is consistent with a scenario where the latter class makes a substantial contribution mostly below 1 PeV, while jetted AGN, i.e. blazars, dominate above this energy, in intriguing agreement with the dip in the neutrino data at ~ 300 TeV. More and better IceCube data on Seyfert galaxies will allow us to constrain the fraction of neutrino emitters among non-jetted AGN.","sentences":["Aims.","We calculate the contribution to the neutrino background from the non-jetted active galactic nuclei (AGN) population following the recent IceCube association of TeV neutrinos with NGC 1068.","Methods.","We exploit our robust knowledge of the AGN X-ray luminosity function and evolution and convert it to the neutrino band by using NGC 1068 as a benchmark and a theoretically motivated neutrino spectrum.","Results.","The resulting neutrino background up to redshift 5 does not violate either the IceCube diffuse flux or the upper bounds for non-jetted AGN, although barely so.","This is consistent with a scenario where the latter class makes a substantial contribution mostly below 1 PeV, while jetted AGN, i.e. blazars, dominate above this energy, in intriguing agreement with the dip in the neutrino data at ~ 300 TeV. More and better IceCube data on Seyfert galaxies will allow us to constrain the fraction of neutrino emitters among non-jetted AGN."],"url":"http://arxiv.org/abs/2404.05690v1","category":"astro-ph.HE"}
{"created":"2024-04-08 16:57:44","title":"Flexible Fairness Learning via Inverse Conditional Permutation","abstract":"Equalized odds, as a popular notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm prediction when conditioning on the true outcome. Despite rapid advancements, most of the current research focuses on the violation of equalized odds caused by one sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes under-addressed. We address this gap by introducing a fairness learning approach that integrates adversarial learning with a novel inverse conditional permutation. This approach effectively and flexibly handles multiple sensitive attributes, potentially of mixed data types. The efficacy and flexibility of our method are demonstrated through both simulation studies and empirical analysis of real-world datasets.","sentences":["Equalized odds, as a popular notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm prediction when conditioning on the true outcome.","Despite rapid advancements, most of the current research focuses on the violation of equalized odds caused by one sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes under-addressed.","We address this gap by introducing a fairness learning approach that integrates adversarial learning with a novel inverse conditional permutation.","This approach effectively and flexibly handles multiple sensitive attributes, potentially of mixed data types.","The efficacy and flexibility of our method are demonstrated through both simulation studies and empirical analysis of real-world datasets."],"url":"http://arxiv.org/abs/2404.05678v1","category":"stat.ML"}
{"created":"2024-04-08 16:56:07","title":"Dark matter free dwarf galaxy formation at the the tips of the tentacles of jellyfish galaxies","abstract":"When falling into a galaxy cluster, galaxies experience a loss of gas due to ram pressure stripping. In particular, disk galaxies lose gas from their disks and very large tentacles of gas can be formed. Because of the morphology of these stripped galaxies they have been referred to as Jellyfish galaxies. It has been found that star formation is triggered not only in the disk, but also in the tentacles of such Jellyfish galaxies. The observed star forming regions located in the tentacles of those galaxies have been found to be as massive as $3\\times10^7$ M$_{\\odot}$ and with sizes $> 100$ pc. Interestingly, these parameters in mass and size agree with those of dwarf galaxies. In this work we make use of the state of the art magneto-hydrodynamical cosmological simulation Illustris TNG-50, to study massive jellyfish galaxies with long tentacles. We find that, in the tentacles of TNG-50 Jellyfish galaxies, the star formation regions (gas+stars) formed could be as massive as $\\sim2\\times10^8$ M$_{\\odot}$. A particular star forming region was analyzed. This region has a star formation rate of $0.04$ M$_{\\odot}$/yr, it is metal rich, has an average age of $0.46$ Gyr, and has a half mass radius of $\\sim1$ kpc, typical of standard dwarf galaxies. Most importantly, this region is gravitationally self-bound. All and all, we identify a new type of dwarf galaxy being born from the gas tentacles of jellyfish galaxies, that by construction lacks a dark matter (hereafter DM) halo.","sentences":["When falling into a galaxy cluster, galaxies experience a loss of gas due to ram pressure stripping.","In particular, disk galaxies lose gas from their disks and very large tentacles of gas can be formed.","Because of the morphology of these stripped galaxies they have been referred to as Jellyfish galaxies.","It has been found that star formation is triggered not only in the disk, but also in the tentacles of such Jellyfish galaxies.","The observed star forming regions located in the tentacles of those galaxies have been found to be as massive as $3\\times10^7$ M$_{\\odot}$ and with sizes $> 100$ pc.","Interestingly, these parameters in mass and size agree with those of dwarf galaxies.","In this work we make use of the state of the art magneto-hydrodynamical cosmological simulation Illustris TNG-50, to study massive jellyfish galaxies with long tentacles.","We find that, in the tentacles of TNG-50 Jellyfish galaxies, the star formation regions (gas+stars) formed could be as massive as $\\sim2\\times10^8","$ M$_{\\odot}$. A particular star forming region was analyzed.","This region has a star formation rate of $0.04$ M$_{\\odot}$/yr, it is metal rich, has an average age of $0.46$ Gyr, and has a half mass radius of $\\sim1$ kpc, typical of standard dwarf galaxies.","Most importantly, this region is gravitationally self-bound.","All and all, we identify a new type of dwarf galaxy being born from the gas tentacles of jellyfish galaxies, that by construction lacks a dark matter (hereafter DM) halo."],"url":"http://arxiv.org/abs/2404.05676v1","category":"astro-ph.GA"}
{"created":"2024-04-08 16:40:15","title":"MLP Can Be A Good Transformer Learner","abstract":"Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.","sentences":["Self-attention mechanism is the key of the Transformer but often criticized for its computation demands.","Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs.","This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations.","We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity.","Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks.","Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks.","Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise.","Code is available at https://github.com/sihaoevery/lambda_vit."],"url":"http://arxiv.org/abs/2404.05657v1","category":"cs.CV"}
{"created":"2024-04-08 16:38:16","title":"Optimized Bandpasses for the Habitable Worlds Observatory's ExoEarth Survey","abstract":"A primary scientific goal of the future Habitable Worlds Observatory will be the direct detection and characterization of Earth-like planets. Estimates of the exoplanet yields for this concept will help guide mission design through detailed trade studies. It is therefore critical that yield estimation codes optimally adapt observations to the mission's performance parameters to ensure accurate trade studies. To aid in this, we implement wavelength optimization in yield calculations for the first time, allowing the yield code to determine the ideal detection and characterization bandpasses. We use this new capability to confirm the observational wavelength assumptions made for the LUVOIR-B study, namely that the optimum detection wavelength is 500 nm for the majority of targets and the optimum wavelength to detect water is near 1000 nm, given LUVOIR-B's assumed instrument performance parameters. We show that including the wavelength dependent albedo of an Earth twin as a prior provides no significant benefit to the yields of exoEarth candidates and caution against tuning observations to modern Earth twins. We also show that coronagraphs whose inner working angles are similar to step functions may benefit from wavelength optimization and demonstrate how wavelength-dependent instrument performance can impact the optimum wavelengths for detection and characterization. The optimization methods we implement automate wavelength selection and remove uncertainties regarding these choices, helping to adapt the observations to the instrument's performance parameters.","sentences":["A primary scientific goal of the future Habitable Worlds Observatory will be the direct detection and characterization of Earth-like planets.","Estimates of the exoplanet yields for this concept will help guide mission design through detailed trade studies.","It is therefore critical that yield estimation codes optimally adapt observations to the mission's performance parameters to ensure accurate trade studies.","To aid in this, we implement wavelength optimization in yield calculations for the first time, allowing the yield code to determine the ideal detection and characterization bandpasses.","We use this new capability to confirm the observational wavelength assumptions made for the LUVOIR-B study, namely that the optimum detection wavelength is 500 nm for the majority of targets and the optimum wavelength to detect water is near 1000 nm, given LUVOIR-B's assumed instrument performance parameters.","We show that including the wavelength dependent albedo of an Earth twin as a prior provides no significant benefit to the yields of exoEarth candidates and caution against tuning observations to modern Earth twins.","We also show that coronagraphs whose inner working angles are similar to step functions may benefit from wavelength optimization and demonstrate how wavelength-dependent instrument performance can impact the optimum wavelengths for detection and characterization.","The optimization methods we implement automate wavelength selection and remove uncertainties regarding these choices, helping to adapt the observations to the instrument's performance parameters."],"url":"http://arxiv.org/abs/2404.05654v1","category":"astro-ph.EP"}
{"created":"2024-04-08 15:53:46","title":"AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets","abstract":"Active learning for imbalanced classification tasks is challenging as the minority classes naturally occur rarely. Gathering a large pool of unlabelled data is thus essential to capture minority instances. Standard pool-based active learning is computationally expensive on large pools and often reaches low accuracy by overfitting the initial decision boundary, thus failing to explore the input space and find minority instances. To address these issues we propose AnchorAL. At each iteration, AnchorAL chooses class-specific instances from the labelled set, or anchors, and retrieves the most similar unlabelled instances from the pool. This resulting subpool is then used for active learning. Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools. By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances. Experiments across different classification tasks, active learning strategies, and model architectures AnchorAL is (i) faster, often reducing runtime from hours to minutes, (ii) trains more performant models, (iii) and returns more balanced datasets than competing methods.","sentences":["Active learning for imbalanced classification tasks is challenging as the minority classes naturally occur rarely.","Gathering a large pool of unlabelled data is thus essential to capture minority instances.","Standard pool-based active learning is computationally expensive on large pools and often reaches low accuracy by overfitting the initial decision boundary, thus failing to explore the input space and find minority instances.","To address these issues we propose AnchorAL.","At each iteration, AnchorAL chooses class-specific instances from the labelled set, or anchors, and retrieves the most similar unlabelled instances from the pool.","This resulting subpool is then used for active learning.","Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools.","By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances.","Experiments across different classification tasks, active learning strategies, and model architectures AnchorAL is (i) faster, often reducing runtime from hours to minutes, (ii) trains more performant models, (iii) and returns more balanced datasets than competing methods."],"url":"http://arxiv.org/abs/2404.05623v1","category":"cs.LG"}
{"created":"2024-04-08 15:46:58","title":"Mourre theory and spectral analysis of energy-momentum operators in relativistic quantum field theory","abstract":"A central task of theoretical physics is to analyse spectral properties of quantum mechanical observables. In this endeavour, Mourre's conjugate operator method emerged as an effective tool in the spectral theory of Schr\\\"odinger operators. This paper introduces a novel class of examples amenable to Mourre's method, focusing on the spectra of energy-momentum operators in relativistic quantum field theory. Under the assumption of Lorentz or dilation covariance and the spectrum condition, we provide new proofs of the absolute continuity of the energy-momentum spectra, showcasing the efficiency of Mourre's method in the relativistic setting.","sentences":["A central task of theoretical physics is to analyse spectral properties of quantum mechanical observables.","In this endeavour, Mourre's conjugate operator method emerged as an effective tool in the spectral theory of Schr\\\"odinger operators.","This paper introduces a novel class of examples amenable to Mourre's method, focusing on the spectra of energy-momentum operators in relativistic quantum field theory.","Under the assumption of Lorentz or dilation covariance and the spectrum condition, we provide new proofs of the absolute continuity of the energy-momentum spectra, showcasing the efficiency of Mourre's method in the relativistic setting."],"url":"http://arxiv.org/abs/2404.05619v1","category":"math-ph"}
{"created":"2024-04-08 15:00:28","title":"Horizontally and vertically polarized kink oscillations in curved solar coronal loops","abstract":"Kink oscillations are frequently observed in coronal loops. This work aims to numerically clarify the influence of loop curvature on horizontally and vertically polarized kink oscillations. Working within the framework of ideal MHD, we conduct 3D simulations of axial fundamental kink oscillations in curved density-enhanced loops embedded in a potential magnetic field. Both horizontal and vertical polarizations are examined, and their oscillation frequencies are compared with WKB expectations. We discriminate two different density specifications. In the first (dubbed\"uniform-density\"), the density is axially uniform and varies continuously in the transverse direction toward a uniform ambient corona. Some further stratification is implemented in the second specification (dubbed\"stratified\"), allowing us to address the effect of evanescent barriers. Examining the oscillating profiles of the initially perturbed uniform-density loops, we found that the frequencies for both polarizations deviate from the WKB expectation by $\\sim 10\\%$. In the stratified loop, however, the frequency of the horizontal polarization deviates to a larger extent ($\\sim 25\\%$). We illustrate the lateral leakage of kink modes through wave tunnelling in 3D simulations, for the first time. Despite this, in both loops, the damping time-to-period ratios are similar and close to the analytical predictions for straight configurations under the thin-tube-thin-boundary (TTTB) assumption. The WKB expectation for straight configurations can reasonably describe the eigenfrequency of kink oscillations only in loops without an asymmetrical cross-loop density profile perpendicular to the oscillating direction. Lateral leakage via wave tunnelling is found to be less efficient than resonant absorption, meaning that the latter remains a robust damping mechanism for kink motions even when loop curvature is included.","sentences":["Kink oscillations are frequently observed in coronal loops.","This work aims to numerically clarify the influence of loop curvature on horizontally and vertically polarized kink oscillations.","Working within the framework of ideal MHD, we conduct 3D simulations of axial fundamental kink oscillations in curved density-enhanced loops embedded in a potential magnetic field.","Both horizontal and vertical polarizations are examined, and their oscillation frequencies are compared with WKB expectations.","We discriminate two different density specifications.","In the first (dubbed\"uniform-density\"), the density is axially uniform and varies continuously in the transverse direction toward a uniform ambient corona.","Some further stratification is implemented in the second specification (dubbed\"stratified\"), allowing us to address the effect of evanescent barriers.","Examining the oscillating profiles of the initially perturbed uniform-density loops, we found that the frequencies for both polarizations deviate from the WKB expectation by $\\sim 10\\%$.","In the stratified loop, however, the frequency of the horizontal polarization deviates to a larger extent ($\\sim 25\\%$).","We illustrate the lateral leakage of kink modes through wave tunnelling in 3D simulations, for the first time.","Despite this, in both loops, the damping time-to-period ratios are similar and close to the analytical predictions for straight configurations under the thin-tube-thin-boundary (TTTB) assumption.","The WKB expectation for straight configurations can reasonably describe the eigenfrequency of kink oscillations only in loops without an asymmetrical cross-loop density profile perpendicular to the oscillating direction.","Lateral leakage via wave tunnelling is found to be less efficient than resonant absorption, meaning that the latter remains a robust damping mechanism for kink motions even when loop curvature is included."],"url":"http://arxiv.org/abs/2404.05586v1","category":"astro-ph.SR"}
{"created":"2024-04-08 14:59:53","title":"Neural Cellular Automata for Lightweight, Robust and Explainable Classification of White Blood Cell Images","abstract":"Diagnosis of hematological malignancies depends on accurate identification of white blood cells in peripheral blood smears. Deep learning techniques are emerging as a viable solution to scale and optimize this process by automatic identification of cells in laboratories. However, these techniques face several challenges such as limited generalizability, sensitivity to domain shifts and lack of explainability. Here, we are introducing a novel approach based on neural cellular automata (NCA) for white blood cell classification. We test our approach on three datasets of white blood cell images and show that we achieve competitive performance compared to conventional methods. Our NCA-based method is significantly smaller in terms of parameters and exhibits robustness to domain shifts. Furthermore, the architecture is inherently explainable, providing insights into the decision process for each classification, helping experts understand and validate model predictions. Results demonstrate that NCA not only can be used for image classification, but also address key challenges of conventional methods, indicating a high potential for applicability in clinical practice.","sentences":["Diagnosis of hematological malignancies depends on accurate identification of white blood cells in peripheral blood smears.","Deep learning techniques are emerging as a viable solution to scale and optimize this process by automatic identification of cells in laboratories.","However, these techniques face several challenges such as limited generalizability, sensitivity to domain shifts and lack of explainability.","Here, we are introducing a novel approach based on neural cellular automata (NCA) for white blood cell classification.","We test our approach on three datasets of white blood cell images and show that we achieve competitive performance compared to conventional methods.","Our NCA-based method is significantly smaller in terms of parameters and exhibits robustness to domain shifts.","Furthermore, the architecture is inherently explainable, providing insights into the decision process for each classification, helping experts understand and validate model predictions.","Results demonstrate that NCA not only can be used for image classification, but also address key challenges of conventional methods, indicating a high potential for applicability in clinical practice."],"url":"http://arxiv.org/abs/2404.05584v1","category":"cs.CV"}
{"created":"2024-04-08 14:49:26","title":"Eye Tracking on Text Reading with Visual Enhancements","abstract":"The interplay between text and visualization is gaining importance for media where traditional text is enriched by visual elements to improve readability and emphasize facts. In two controlled eye-tracking experiments ($N=12$), we approach answers to the question: How do visualization techniques influence reading behavior? We compare plain text to that marked with highlights, icons, and word-sized data visualizations. We assess quantitative metrics~(eye movement, completion time, error rate) and subjective feedback~(personal preference and ratings). The results indicate that visualization techniques, especially in the first experiment, show promising trends for improved reading behavior. The results also show the need for further research to make reading more effective and inform suggestions for future studies.","sentences":["The interplay between text and visualization is gaining importance for media where traditional text is enriched by visual elements to improve readability and emphasize facts.","In two controlled eye-tracking experiments ($N=12$), we approach answers to the question: How do visualization techniques influence reading behavior?","We compare plain text to that marked with highlights, icons, and word-sized data visualizations.","We assess quantitative metrics~(eye movement, completion time, error rate) and subjective feedback~(personal preference and ratings).","The results indicate that visualization techniques, especially in the first experiment, show promising trends for improved reading behavior.","The results also show the need for further research to make reading more effective and inform suggestions for future studies."],"url":"http://arxiv.org/abs/2404.05572v1","category":"cs.HC"}
{"created":"2024-04-08 13:59:56","title":"Observation of dichotomic field-tunable electronic structure in twisted monolayer-bilayer graphene","abstract":"Twisted bilayer graphene (tBLG) provides a fascinating platform for engineering flat bands and inducing correlated phenomena. By designing the stacking architecture of graphene layers, twisted multilayer graphene can exhibit different symmetries with rich tunability. For example, in twisted monolayer-bilayer graphene (tMBG) which breaks the C2z symmetry, transport measurements reveal an asymmetric phase diagram under an out-of-plane electric field, exhibiting correlated insulating state and ferromagnetic state respectively when reversing the field direction. Revealing how the electronic structure evolves with electric field is critical for providing a better understanding of such asymmetric field-tunable properties. Here we report the experimental observation of field-tunable dichotomic electronic structure of tMBG by nanospot angle-resolved photoemission spectroscopy (NanoARPES) with operando gating. Interestingly, selective enhancement of the relative spectral weight contributions from monolayer and bilayer graphene is observed when switching the polarity of the bias voltage. Combining experimental results with theoretical calculations, the origin of such field-tunable electronic structure, resembling either tBLG or twisted double-bilayer graphene (tDBG), is attributed to the selectively enhanced contribution from different stacking graphene layers with a strong electron-hole asymmetry. Our work provides electronic structure insights for understanding the rich field-tunable physics of tMBG.","sentences":["Twisted bilayer graphene (tBLG) provides a fascinating platform for engineering flat bands and inducing correlated phenomena.","By designing the stacking architecture of graphene layers, twisted multilayer graphene can exhibit different symmetries with rich tunability.","For example, in twisted monolayer-bilayer graphene (tMBG) which breaks the C2z symmetry, transport measurements reveal an asymmetric phase diagram under an out-of-plane electric field, exhibiting correlated insulating state and ferromagnetic state respectively when reversing the field direction.","Revealing how the electronic structure evolves with electric field is critical for providing a better understanding of such asymmetric field-tunable properties.","Here we report the experimental observation of field-tunable dichotomic electronic structure of tMBG by nanospot angle-resolved photoemission spectroscopy (NanoARPES) with operando gating.","Interestingly, selective enhancement of the relative spectral weight contributions from monolayer and bilayer graphene is observed when switching the polarity of the bias voltage.","Combining experimental results with theoretical calculations, the origin of such field-tunable electronic structure, resembling either tBLG or twisted double-bilayer graphene (tDBG), is attributed to the selectively enhanced contribution from different stacking graphene layers with a strong electron-hole asymmetry.","Our work provides electronic structure insights for understanding the rich field-tunable physics of tMBG."],"url":"http://arxiv.org/abs/2404.05533v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 13:42:24","title":"Double loop dc-SQUID as a tunable Josephson diode","abstract":"The development of superconducting electronics requires careful characterization of the components that make up electronic circuits. Superconducting weak links are the building blocks of most superconducting electronics components and are characterized by highly nonlinear current-to-phase relations (CPR), which are often not perfectly known. Recent research has found that the Josephson diode effect (JDE) can be related to the high harmonic content of the current-to-phase relation of the weak links embedded in superconducting interferometers. This makes the JDE a natural tool for exploring the harmonic content of weak links beyond single-harmonic CPR. In this study, we present the theoretical model and experimental characterization of a double-loop superconducting quantum interference device (DL-SQUID) that embeds all-metallic superconductor-normal metal-superconductor junctions. The proposed device exhibits the JDE due to the interference of the supercurrents of three weak links in parallel, and this feature can be adjusted through two magnetic fluxes, which act as experimental knobs. We carry out a theoretical study of the device in terms of the relative weight of the interferometer arms and the experimental characterization concerning flux tunability and temperature.","sentences":["The development of superconducting electronics requires careful characterization of the components that make up electronic circuits.","Superconducting weak links are the building blocks of most superconducting electronics components and are characterized by highly nonlinear current-to-phase relations (CPR), which are often not perfectly known.","Recent research has found that the Josephson diode effect (JDE) can be related to the high harmonic content of the current-to-phase relation of the weak links embedded in superconducting interferometers.","This makes the JDE a natural tool for exploring the harmonic content of weak links beyond single-harmonic CPR.","In this study, we present the theoretical model and experimental characterization of a double-loop superconducting quantum interference device (DL-SQUID) that embeds all-metallic superconductor-normal metal-superconductor junctions.","The proposed device exhibits the JDE due to the interference of the supercurrents of three weak links in parallel, and this feature can be adjusted through two magnetic fluxes, which act as experimental knobs.","We carry out a theoretical study of the device in terms of the relative weight of the interferometer arms and the experimental characterization concerning flux tunability and temperature."],"url":"http://arxiv.org/abs/2404.05521v1","category":"cond-mat.supr-con"}
{"created":"2024-04-08 13:34:18","title":"Hardy and Rellich identities and inequalities for Grushin operators via spherical vector fields and Bessel pairs","abstract":"For Grushin vector fields, we prove Hardy, Hardy-Rellich, and Rellich identities and inequalities with sharp constants. Our explicit remainder terms significantly improve those found in the literature. Our arguments build on abstract Hardy-Rellich identities involving the Bessel pair along with the use of spherical harmonics developed by Garofalo-Shen [Ann. Inst. Fourier (1994)]. Furthermore, in the spirit of Bez-Machihara-Ozawa [Math. Z (2023)], we construct spherical vector fields that correspond to the Grushin vector fields and prove identities that, in turn, establish optimal Rellich identities by comparing the Grushin operator with its radial and spherical components. We give alternate proofs of Hardy identities and inequalities with enhanced Hardy constants in some subspaces of the Sobolev space, among other things. Additionally, we compute the deficit involving $L^2$-norm of the Laplacian and radial components of the Laplacian for the Grushin operator with an explicit remainder term, which leads to a comparison of Laplacian with the radial components of the Laplacian. As a consequence of the main results, new second-order Heisenberg-Pauli-Weyl uncertainty principles and Hydrogen uncertainty principles are also derived. Furthermore, we also derive certain symmetrization principles on the Grushin space.","sentences":["For Grushin vector fields, we prove Hardy, Hardy-Rellich, and Rellich identities and inequalities with sharp constants.","Our explicit remainder terms significantly improve those found in the literature.","Our arguments build on abstract Hardy-Rellich identities involving the Bessel pair along with the use of spherical harmonics developed by Garofalo-Shen","[Ann.","Inst.","Fourier (1994)].","Furthermore, in the spirit of Bez-Machihara-Ozawa [Math. Z (2023)], we construct spherical vector fields that correspond to the Grushin vector fields and prove identities that, in turn, establish optimal Rellich identities by comparing the Grushin operator with its radial and spherical components.","We give alternate proofs of Hardy identities and inequalities with enhanced Hardy constants in some subspaces of the Sobolev space, among other things.","Additionally, we compute the deficit involving $L^2$-norm of the Laplacian and radial components of the Laplacian for the Grushin operator with an explicit remainder term, which leads to a comparison of Laplacian with the radial components of the Laplacian.","As a consequence of the main results, new second-order Heisenberg-Pauli-Weyl uncertainty principles and Hydrogen uncertainty principles are also derived.","Furthermore, we also derive certain symmetrization principles on the Grushin space."],"url":"http://arxiv.org/abs/2404.05510v1","category":"math.AP"}
{"created":"2024-04-08 12:54:19","title":"Experimental demonstration of improved quantum optimization with linear Ising penalties","abstract":"The standard approach to encoding constraints in quantum optimization is the quadratic penalty method. Quadratic penalties introduce additional couplings and energy scales, which can be detrimental to the performance of a quantum optimizer. In quantum annealing experiments performed on a D-Wave Advantage, we explore an alternative penalty method that only involves linear Ising terms and apply it to a customer data science problem. Our findings support our hypothesis that the linear Ising penalty method should improve the performance of quantum optimization compared to using the quadratic penalty method due to its more efficient use of physical resources. Although the linear Ising penalty method is not guaranteed to exactly implement the desired constraint in all cases, it is able to do so for the majority of problem instances we consider. For problems with many constraints, where making all penalties linear is unlikely to be feasible, we investigate strategies for combining linear Ising penalties with quadratic penalties to satisfy constraints for which the linear method is not well-suited. We find that this strategy is most effective when the penalties that contribute most to limiting the dynamic range are removed.","sentences":["The standard approach to encoding constraints in quantum optimization is the quadratic penalty method.","Quadratic penalties introduce additional couplings and energy scales, which can be detrimental to the performance of a quantum optimizer.","In quantum annealing experiments performed on a D-Wave Advantage, we explore an alternative penalty method that only involves linear Ising terms and apply it to a customer data science problem.","Our findings support our hypothesis that the linear Ising penalty method should improve the performance of quantum optimization compared to using the quadratic penalty method due to its more efficient use of physical resources.","Although the linear Ising penalty method is not guaranteed to exactly implement the desired constraint in all cases, it is able to do so for the majority of problem instances we consider.","For problems with many constraints, where making all penalties linear is unlikely to be feasible, we investigate strategies for combining linear Ising penalties with quadratic penalties to satisfy constraints for which the linear method is not well-suited.","We find that this strategy is most effective when the penalties that contribute most to limiting the dynamic range are removed."],"url":"http://arxiv.org/abs/2404.05476v1","category":"quant-ph"}
{"created":"2024-04-08 12:46:22","title":"Quantum optimization with linear Ising penalty functions for customer data science","abstract":"Constrained combinatorial optimization problems, which are ubiquitous in industry, can be solved by quantum algorithms such as quantum annealing (QA) and the quantum approximate optimization algorithm (QAOA). In these quantum algorithms, constraints are typically implemented with quadratic penalty functions. This penalty method can introduce large energy scales and make interaction graphs much more dense. These effects can result in worse performance of quantum optimization, particularly on near-term devices that have sparse hardware graphs and other physical limitations. In this work, we consider linear Ising penalty functions, which are applied with local fields in the Ising model, as an alternative method for implementing constraints that makes more efficient use of physical resources. We study the behaviour of the penalty method in the context of quantum optimization for customer data science problems. Our theoretical analysis and numerical simulations of QA and the QAOA indicate that this penalty method can lead to better performance in quantum optimization than the quadratic method. However, the linear Ising penalty method is not suitable for all problems as it cannot always exactly implement the desired constraint. In cases where the linear method is not successful in implementing all constraints, we propose that schemes involving both quadratic and linear Ising penalties can be effective.","sentences":["Constrained combinatorial optimization problems, which are ubiquitous in industry, can be solved by quantum algorithms such as quantum annealing (QA) and the quantum approximate optimization algorithm (QAOA).","In these quantum algorithms, constraints are typically implemented with quadratic penalty functions.","This penalty method can introduce large energy scales and make interaction graphs much more dense.","These effects can result in worse performance of quantum optimization, particularly on near-term devices that have sparse hardware graphs and other physical limitations.","In this work, we consider linear Ising penalty functions, which are applied with local fields in the Ising model, as an alternative method for implementing constraints that makes more efficient use of physical resources.","We study the behaviour of the penalty method in the context of quantum optimization for customer data science problems.","Our theoretical analysis and numerical simulations of QA and the QAOA indicate that this penalty method can lead to better performance in quantum optimization than the quadratic method.","However, the linear Ising penalty method is not suitable for all problems as it cannot always exactly implement the desired constraint.","In cases where the linear method is not successful in implementing all constraints, we propose that schemes involving both quadratic and linear Ising penalties can be effective."],"url":"http://arxiv.org/abs/2404.05467v1","category":"quant-ph"}
{"created":"2024-04-08 12:41:03","title":"Thermal melting of a vortex lattice in a quasi two-dimensional Bose gas","abstract":"We report the observation of the melting of a vortex lattice in a fast rotating quasi-two dimensional Bose gas, under the influence of thermal fluctuations. We image the vortex lattice after a time-of-flight expansion, for increasing rotation frequency at constant atom number and temperature. We detect the vortex positions and study the order of the lattice using the pair correlation function and the orientational correlation function. We evidence the melting transition by an abrupt change in the decay of orientational correlations, associated to a proliferation of dislocations. Our findings are consistent with the hexatic to liquid transition in the KTHNY scenario for two-dimensional melting.","sentences":["We report the observation of the melting of a vortex lattice in a fast rotating quasi-two dimensional Bose gas, under the influence of thermal fluctuations.","We image the vortex lattice after a time-of-flight expansion, for increasing rotation frequency at constant atom number and temperature.","We detect the vortex positions and study the order of the lattice using the pair correlation function and the orientational correlation function.","We evidence the melting transition by an abrupt change in the decay of orientational correlations, associated to a proliferation of dislocations.","Our findings are consistent with the hexatic to liquid transition in the KTHNY scenario for two-dimensional melting."],"url":"http://arxiv.org/abs/2404.05460v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-08 12:16:21","title":"Mapping finite-fault slip with spatial correlation between seismicity and point-source Coulomb failure stress change","abstract":"Most earthquake energy release arises during fault slip many kilometers below the Earth's surface. Understanding earthquakes and their hazard requires mapping the geometry and amount of this slip. Such finite-fault maps are typically derived from measures of surface phenomena, such as seismic and geodetic ground motions. Here we introduce a procedure for mapping finite-fault slip directly from seismicity and aftershocks-phenomena occurring at depth around an earthquake rupture. For specified source and receiver faults, we map source-fault slip in 3D by correlation of point-source Coulomb failure stress change ({\\Delta}CFS) kernels across the distribution of seismicity around an earthquake. These seismicity-stress maps show relative, static fault slip compatible with the surrounding seismicity under the physics of {\\Delta}CFS; they can aid other slip inversions and aftershock forecasting, and be obtained for early instrumental earthquakes. We verify the maps recover synthetic fault slip and match independent estimates of slip for the 2004 Mw 6.0 Parkfield and 2021 Mw 6.0 Antelope Valley California earthquakes. For the 2018 Mw 7.1 Anchorage Alaska intra-slab earthquake, seismicity-stress maps, combined with multi-scale precise hypocenter relocation, resolve the enigma of which mainshock faulting plane ruptured (the gently east-dipping plane), and clarify slab structures activated in the energetic aftershock sequence.","sentences":["Most earthquake energy release arises during fault slip many kilometers below the Earth's surface.","Understanding earthquakes and their hazard requires mapping the geometry and amount of this slip.","Such finite-fault maps are typically derived from measures of surface phenomena, such as seismic and geodetic ground motions.","Here we introduce a procedure for mapping finite-fault slip directly from seismicity and aftershocks-phenomena occurring at depth around an earthquake rupture.","For specified source and receiver faults, we map source-fault slip in 3D by correlation of point-source Coulomb failure stress change ({\\Delta}CFS) kernels across the distribution of seismicity around an earthquake.","These seismicity-stress maps show relative, static fault slip compatible with the surrounding seismicity under the physics of {\\Delta}CFS; they can aid other slip inversions and aftershock forecasting, and be obtained for early instrumental earthquakes.","We verify the maps recover synthetic fault slip and match independent estimates of slip for the 2004 Mw 6.0 Parkfield and 2021 Mw 6.0 Antelope Valley California earthquakes.","For the 2018 Mw 7.1 Anchorage Alaska intra-slab earthquake, seismicity-stress maps, combined with multi-scale precise hypocenter relocation, resolve the enigma of which mainshock faulting plane ruptured (the gently east-dipping plane), and clarify slab structures activated in the energetic aftershock sequence."],"url":"http://arxiv.org/abs/2404.05437v1","category":"physics.geo-ph"}
{"created":"2024-04-08 12:02:06","title":"Simplifying MBA Expression Using E-Graphs","abstract":"Code obfuscation involves the addition of meaningless code or the complication of existing code in order to make a program difficult to reverse engineer. In recent years, MBA (Mixed Boolean Arithmetic) obfuscation has been applied to virus and malware code to impede expert analysis. Among the various obfuscation techniques, Mixed Boolean Arithmetic (MBA) obfuscation is considered the most challenging to decipher using existing code deobfuscation techniques. In this paper, we have attempted to simplify the MBA expression. We use an e-graph data structure to efficiently hold multiple expressions of the same semantics to systematically rewrite terms and find simpler expressions. The preliminary experimental result shows that our e-graph based MBA deobfuscation approach works faster with reasonable performance than other approaches do.","sentences":["Code obfuscation involves the addition of meaningless code or the complication of existing code in order to make a program difficult to reverse engineer.","In recent years, MBA (Mixed Boolean Arithmetic) obfuscation has been applied to virus and malware code to impede expert analysis.","Among the various obfuscation techniques, Mixed Boolean Arithmetic (MBA) obfuscation is considered the most challenging to decipher using existing code deobfuscation techniques.","In this paper, we have attempted to simplify the MBA expression.","We use an e-graph data structure to efficiently hold multiple expressions of the same semantics to systematically rewrite terms and find simpler expressions.","The preliminary experimental result shows that our e-graph based MBA deobfuscation approach works faster with reasonable performance than other approaches do."],"url":"http://arxiv.org/abs/2404.05431v1","category":"cs.CR"}
{"created":"2024-04-08 11:42:19","title":"The $\u03ba$-model under test of the SPARC database","abstract":"Our main goal is here to make a comparative analysis between the well-known MOND theory and a more recent model called$\\kappa$-model. An additional connection, between the $\\kappa$-model and twoother novel MOND-type theories: Newtonian Fractional-DimensionGravity (NFDG) and Refracted Gravity (RG), is likewise presented.All these models are built to overtake the DM paradigm, or at leastto strongly reduce the dark matter content. Whereas they rely ondifferent formalisms, however, all four seem to suggest that the universal parameter, a0, appearing in MOND theory could intrinsicallybe correlated to either the sole baryonic mean mass density (RG and$\\kappa$-model) and/or to the dimension of the object under consideration(NFDG and $\\kappa$-model). We could then confer to the parameter a0 amore flexible status of multiscale parameter, as required to explainthe dynamics together in galaxies and in galaxy clusters. Eventually,the conformal gravity theory (CFT) also seems to have some remotelink with the $\\kappa$-model, even though the first one is an extension ofgeneral relativity, and the second one is Newtonian in essence. The$\\kappa$-model has been tested on a small sample of spiral galaxies and ingalaxy clusters. Now we test this model on a large sample of galaxiesissued from the SPARC database.","sentences":["Our main goal is here to make a comparative analysis between the well-known MOND theory and a more recent model called$\\kappa$-model.","An additional connection, between the $\\kappa$-model and twoother novel MOND-type theories: Newtonian Fractional-DimensionGravity (NFDG) and Refracted Gravity (RG), is likewise presented.","All these models are built to overtake the DM paradigm, or at leastto strongly reduce the dark matter content.","Whereas they rely ondifferent formalisms, however, all four seem to suggest that the universal parameter, a0, appearing in MOND theory could intrinsicallybe correlated to either the sole baryonic mean mass density (RG and$\\kappa$-model) and/or to the dimension of the object under consideration(NFDG and $\\kappa$-model).","We could then confer to the parameter a0 amore flexible status of multiscale parameter, as required to explainthe dynamics together in galaxies and in galaxy clusters.","Eventually,the conformal gravity theory (CFT) also seems to have some remotelink with the $\\kappa$-model, even though the first one is an extension ofgeneral relativity, and the second one is Newtonian in essence.","The$\\kappa$-model has been tested on a small sample of spiral galaxies and ingalaxy clusters.","Now we test this model on a large sample of galaxiesissued from the SPARC database."],"url":"http://arxiv.org/abs/2404.05421v1","category":"astro-ph.GA"}
{"created":"2024-04-08 11:31:59","title":"Search and study of young infrared stellar clusters","abstract":"The main aim of this paper is to study both the Interstellar Medium (ISM) and the young stellar population in the three star-forming regions, namely IRAS 05137+3919, 05168+3634, and 19110+1045. The study of the ISM includes determination of the hydrogen column density (N(H_2)) and dust temperature (T_d) in the regions using Modified blackbody fitting. The main parameters of identified and classified young stellar objects (YSOs) belonging to the regions were determined comparing with the radiation transfer models. We also constructed a colour-magnitude diagram to compare the parameters of the YSOs with the results of the radiative transfer models. The three stellar populations appear to have formed under different scenarios. In the cases of IRAS 05137+3919 and IRAS 05168+3634, the age spread is considerably wider, suggesting that the stellar population likely emerged from independent condensations. In contrast, the third region comprises a pair of ultra-compact HII regions (UCHIIs), G45.12+0.13 and G45.07+0.13, with a notably smaller age spread. This hints at the possibility that these clusters originated from a single triggering event.","sentences":["The main aim of this paper is to study both the Interstellar Medium (ISM) and the young stellar population in the three star-forming regions, namely IRAS 05137+3919, 05168+3634, and 19110+1045.","The study of the ISM includes determination of the hydrogen column density (N(H_2))","and dust temperature (T_d) in the regions using Modified blackbody fitting.","The main parameters of identified and classified young stellar objects (YSOs) belonging to the regions were determined comparing with the radiation transfer models.","We also constructed a colour-magnitude diagram to compare the parameters of the YSOs with the results of the radiative transfer models.","The three stellar populations appear to have formed under different scenarios.","In the cases of IRAS 05137+3919 and IRAS 05168+3634, the age spread is considerably wider, suggesting that the stellar population likely emerged from independent condensations.","In contrast, the third region comprises a pair of ultra-compact HII regions (UCHIIs), G45.12+0.13 and G45.07+0.13, with a notably smaller age spread.","This hints at the possibility that these clusters originated from a single triggering event."],"url":"http://arxiv.org/abs/2404.05413v1","category":"astro-ph.GA"}
{"created":"2024-04-08 11:16:14","title":"A new solution of the pulsar equation","abstract":"We present the first new type of solution of the pulsar equation since 1999. In it, the whole magnetosphere is confined inside the light cylinder and an electrically charged layer wraps around it and holds it together. The reason this new solution has never been obtained before is that all current time-dependent simulations are initialized with a vacuum dipole configuration that extends to infinity, thus their final steady-state solution also extends to infinity. Under special conditions, such a confined configuration may be attained when the neutron star first forms in the interior of a collapsing star during a supernova explosion, or when it accretes from an external wind or disk from a donor star. It is shown that this new maximally closed non-decelerating solution is the limit of a continuous sequence of standard magnetospheres with open and closed field lines when the amount of open field lines gradually drops to zero. The minimum energy solution in this sequence is a standard magnetosphere in which the closed field line region extends up to about 80% of the light cylinder. We estimate that the released energy when the new solution transitions to the minimum energy one is enough to power a fast radio burst.","sentences":["We present the first new type of solution of the pulsar equation since 1999.","In it, the whole magnetosphere is confined inside the light cylinder and an electrically charged layer wraps around it and holds it together.","The reason this new solution has never been obtained before is that all current time-dependent simulations are initialized with a vacuum dipole configuration that extends to infinity, thus their final steady-state solution also extends to infinity.","Under special conditions, such a confined configuration may be attained when the neutron star first forms in the interior of a collapsing star during a supernova explosion, or when it accretes from an external wind or disk from a donor star.","It is shown that this new maximally closed non-decelerating solution is the limit of a continuous sequence of standard magnetospheres with open and closed field lines when the amount of open field lines gradually drops to zero.","The minimum energy solution in this sequence is a standard magnetosphere in which the closed field line region extends up to about 80% of the light cylinder.","We estimate that the released energy when the new solution transitions to the minimum energy one is enough to power a fast radio burst."],"url":"http://arxiv.org/abs/2404.05408v1","category":"astro-ph.HE"}
{"created":"2024-04-08 11:01:12","title":"Air-Water Interface-Assisted Synthesis and Charge Transport Characterization of Quasi-2D Polyacetylene Films with Enhanced Electron Mobility via Ring-Opening Polymerization of Pyrrole","abstract":"Water surfaces catalyze some organic reactions more effectively, making them unique for 2D organic material synthesis. This report introduces a new synthesis method via surfactant-monolayer-assisted interfacial synthesis on water surfaces for ring-opening polymerization of pyrrole, producing distinct polypyrrole derivatives with polyacetylene backbones and ionic substitutions. The synthesis result in quasi 2D polyacetylene (q2DPA) film with enhanced charge transport behavior. We employed time-of-flight photoconductivity (TOFP) measurements using pulsed laser light of tunable wavelength for photoexcitation of the charge carriers within the q2DPA film. The charge transport was measured in the lateral direction as a function of external bias voltage ranging from 0 V to 200 V. We observed high electron mobility ({\\mu}) of q2DPA reaching values of 375 cm2 V-1 s-1 at bias voltage Vb = -20V and photon energy of 3.8 eV.","sentences":["Water surfaces catalyze some organic reactions more effectively, making them unique for 2D organic material synthesis.","This report introduces a new synthesis method via surfactant-monolayer-assisted interfacial synthesis on water surfaces for ring-opening polymerization of pyrrole, producing distinct polypyrrole derivatives with polyacetylene backbones and ionic substitutions.","The synthesis result in quasi 2D polyacetylene (q2DPA) film with enhanced charge transport behavior.","We employed time-of-flight photoconductivity (TOFP) measurements using pulsed laser light of tunable wavelength for photoexcitation of the charge carriers within the q2DPA film.","The charge transport was measured in the lateral direction as a function of external bias voltage ranging from 0 V to 200 V.","We observed high electron mobility ({\\mu}) of q2DPA reaching values of 375 cm2 V-1 s-1 at bias voltage Vb = -20V and photon energy of 3.8 eV."],"url":"http://arxiv.org/abs/2404.05402v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 10:50:45","title":"An Oedometer Test under Acid Injection with a Discrete Element Model: the case of Debonding","abstract":"Rock weathering is a common phenomenon in most engineering applications, such as underground storage or geothermal energy. This work offers a discrete element modelization of the problem considering cohesive granular material and debonding effect. Oedometer conditions are applied during the weathering and the evolution of the coefficient of lateral earth pressure, a proxy of the state of stress, is tracked. Especially, the influence of the degree of cementation, the confining pressure, the initial value of k0 and the history of load are investigated. It has been emphasized that the granular media aims to reach an attractor configuration. And the grain reorganization occurring is divided into two main phenomena: the collapse of the unstable chain forces (stable only thanks to the cementation) and the softening of the grains.","sentences":["Rock weathering is a common phenomenon in most engineering applications, such as underground storage or geothermal energy.","This work offers a discrete element modelization of the problem considering cohesive granular material and debonding effect.","Oedometer conditions are applied during the weathering and the evolution of the coefficient of lateral earth pressure, a proxy of the state of stress, is tracked.","Especially, the influence of the degree of cementation, the confining pressure, the initial value of k0 and the history of load are investigated.","It has been emphasized that the granular media aims to reach an attractor configuration.","And the grain reorganization occurring is divided into two main phenomena: the collapse of the unstable chain forces (stable only thanks to the cementation) and the softening of the grains."],"url":"http://arxiv.org/abs/2404.05390v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 10:31:37","title":"A theoretical perspective on the almost dark galaxy Nube: exploring the fuzzy dark matter model","abstract":"In recent astronomical observations, an almost dark galaxy, designated as Nube, has unveiled an intriguing anomaly in its stellar distribution. Specifically, Nube exhibits an exceptionally low central brightness, with the 2D half-light radius of its stars far exceeding the typical values found in dwarf galaxies, and even surpassing those observed in ultra-diffuse galaxies (UDGs). This phenomenon is difficult to explain within the framework of cold dark matter (CDM). Meanwhile, due to its ultralight particle mass, fuzzy dark matter (FDM) exhibits a de Broglie wavelength on the order of kiloparsecs under the typical velocities of galaxies. The interference between different modes of the FDM wave gives rise to fluctuations in the gravitational field, which can lead to the dynamical heating of stars within galaxies, resulting in an expansion of their spatial distribution. In this paper, we aim to interpret the anomalous stellar distribution observed in Nube as a consequence of the dynamical heating effect induced by FDM. Our findings suggest that a FDM particle mass around $1-2\\times 10^{-23}$ eV can effectively account for this anomaly. And we propose that the FDM dynamical heating effect provides a new insight into understanding the formation of field UDGs.","sentences":["In recent astronomical observations, an almost dark galaxy, designated as Nube, has unveiled an intriguing anomaly in its stellar distribution.","Specifically, Nube exhibits an exceptionally low central brightness, with the 2D half-light radius of its stars far exceeding the typical values found in dwarf galaxies, and even surpassing those observed in ultra-diffuse galaxies (UDGs).","This phenomenon is difficult to explain within the framework of cold dark matter (CDM).","Meanwhile, due to its ultralight particle mass, fuzzy dark matter (FDM) exhibits a de Broglie wavelength on the order of kiloparsecs under the typical velocities of galaxies.","The interference between different modes of the FDM wave gives rise to fluctuations in the gravitational field, which can lead to the dynamical heating of stars within galaxies, resulting in an expansion of their spatial distribution.","In this paper, we aim to interpret the anomalous stellar distribution observed in Nube as a consequence of the dynamical heating effect induced by FDM.","Our findings suggest that a FDM particle mass around $1-2\\times 10^{-23}$ eV can effectively account for this anomaly.","And we propose that the FDM dynamical heating effect provides a new insight into understanding the formation of field UDGs."],"url":"http://arxiv.org/abs/2404.05375v1","category":"astro-ph.CO"}
{"created":"2024-04-08 10:19:51","title":"Constraints on the dense matter equation of state from young and cold isolated neutron stars","abstract":"Neutron stars are the dense and highly magnetic relics of supernova explosions of massive stars. The quest to constrain the Equation of State (EoS) of ultra-dense matter and thereby probe the behavior of matter inside neutron stars, is one of the core goals of modern physics and astrophysics. A promising method involves investigating the long-term cooling of neutron stars, and comparing theoretical predictions with various sources at different ages. However, limited observational data, and uncertainties in source ages and distances, have hindered this approach. In this work, re-analyzing XMM-Newton and Chandra data from dozens of thermally emitting isolated neutron stars, we have identified three sources with unexpectedly cold surface temperatures for their young ages. To investigate these anomalies, we conducted magneto-thermal simulations across diverse mass and magnetic fields, considering three different EoS. We found that the \"minimal\" cooling model, failed to explain the observations, regardless the mass and the magnetic field, as validated by a machine learning classification method. The existence of these young cold neutron stars suggests that any dense matter EoS must be compatible with a fast cooling process at least in certain mass ranges, eliminating a significant portion of current EoS options according to recent meta-modelling analysis.","sentences":["Neutron stars are the dense and highly magnetic relics of supernova explosions of massive stars.","The quest to constrain the Equation of State (EoS) of ultra-dense matter and thereby probe the behavior of matter inside neutron stars, is one of the core goals of modern physics and astrophysics.","A promising method involves investigating the long-term cooling of neutron stars, and comparing theoretical predictions with various sources at different ages.","However, limited observational data, and uncertainties in source ages and distances, have hindered this approach.","In this work, re-analyzing XMM-Newton and Chandra data from dozens of thermally emitting isolated neutron stars, we have identified three sources with unexpectedly cold surface temperatures for their young ages.","To investigate these anomalies, we conducted magneto-thermal simulations across diverse mass and magnetic fields, considering three different EoS. We found that the \"minimal\" cooling model, failed to explain the observations, regardless the mass and the magnetic field, as validated by a machine learning classification method.","The existence of these young cold neutron stars suggests that any dense matter EoS must be compatible with a fast cooling process at least in certain mass ranges, eliminating a significant portion of current EoS options according to recent meta-modelling analysis."],"url":"http://arxiv.org/abs/2404.05371v1","category":"astro-ph.HE"}
{"created":"2024-04-08 09:57:02","title":"A parameter-free clustering algorithm for missing datasets","abstract":"Missing datasets, in which some objects have missing values in certain dimensions, are prevalent in the Real-world. Existing clustering algorithms for missing datasets first impute the missing values and then perform clustering. However, both the imputation and clustering processes require input parameters. Too many input parameters inevitably increase the difficulty of obtaining accurate clustering results. Although some studies have shown that decision graphs can replace the input parameters of clustering algorithms, current decision graphs require equivalent dimensions among objects and are therefore not suitable for missing datasets. To this end, we propose a Single-Dimensional Clustering algorithm, i.e., SDC. SDC, which removes the imputation process and adapts the decision graph to the missing datasets by splitting dimension and partition intersection fusion, can obtain valid clustering results on the missing datasets without input parameters. Experiments demonstrate that, across three evaluation metrics, SDC outperforms baseline algorithms by at least 13.7%(NMI), 23.8%(ARI), and 8.1%(Purity).","sentences":["Missing datasets, in which some objects have missing values in certain dimensions, are prevalent in the Real-world.","Existing clustering algorithms for missing datasets first impute the missing values and then perform clustering.","However, both the imputation and clustering processes require input parameters.","Too many input parameters inevitably increase the difficulty of obtaining accurate clustering results.","Although some studies have shown that decision graphs can replace the input parameters of clustering algorithms, current decision graphs require equivalent dimensions among objects and are therefore not suitable for missing datasets.","To this end, we propose a Single-Dimensional Clustering algorithm, i.e., SDC.","SDC, which removes the imputation process and adapts the decision graph to the missing datasets by splitting dimension and partition intersection fusion, can obtain valid clustering results on the missing datasets without input parameters.","Experiments demonstrate that, across three evaluation metrics, SDC outperforms baseline algorithms by at least 13.7%(NMI), 23.8%(ARI), and 8.1%(Purity)."],"url":"http://arxiv.org/abs/2404.05363v1","category":"cs.LG"}
{"created":"2024-04-08 09:33:10","title":"On the relevance of lift force modelling in turbulent wall flows with small inertial particles","abstract":"In particle-laden turbulent wall flows, lift forces can influence the near-wall turbulence. This has been recently observed in particle-resolved simulations, which, however, are too expensive to be used in upscaled models. Instead, point-particle simulations have been the method of choice to simulate the dynamics of these flows during the last decades. While this approach is simpler, cheaper, and physically sound for small inertial particles in turbulence, some issues remain. In the present work, we address challenges associated with lift force modelling in turbulent wall flows and the impact of lift forces in the near-wall flow. We performed direct numerical simulations (DNS) of small inertial point particles in turbulent channel flow for fixed Stokes number and mass loading while varying the particle size. Our results show that the particle dynamics in the buffer region, causing the apparent particle-to-fluid slip velocity to vanish, raise major challenges for accurately modelling lift forces. While our results confirm that lift forces have little influence on particle dynamics for sufficiently small particle sizes, for inner-scaled diameters of order one and beyond, lift forces become quite important near the wall. The different particle dynamics under lift forces result in the modulation of streamwise momentum transport in the near-wall region. We analyze this lift-induced turbulence modulation for different lift force models, and the results indicate that realistic models are critical for particle-modeled simulations to correctly predict turbulence modulation by particles in the near-wall region.","sentences":["In particle-laden turbulent wall flows, lift forces can influence the near-wall turbulence.","This has been recently observed in particle-resolved simulations, which, however, are too expensive to be used in upscaled models.","Instead, point-particle simulations have been the method of choice to simulate the dynamics of these flows during the last decades.","While this approach is simpler, cheaper, and physically sound for small inertial particles in turbulence, some issues remain.","In the present work, we address challenges associated with lift force modelling in turbulent wall flows and the impact of lift forces in the near-wall flow.","We performed direct numerical simulations (DNS) of small inertial point particles in turbulent channel flow for fixed Stokes number and mass loading while varying the particle size.","Our results show that the particle dynamics in the buffer region, causing the apparent particle-to-fluid slip velocity to vanish, raise major challenges for accurately modelling lift forces.","While our results confirm that lift forces have little influence on particle dynamics for sufficiently small particle sizes, for inner-scaled diameters of order one and beyond, lift forces become quite important near the wall.","The different particle dynamics under lift forces result in the modulation of streamwise momentum transport in the near-wall region.","We analyze this lift-induced turbulence modulation for different lift force models, and the results indicate that realistic models are critical for particle-modeled simulations to correctly predict turbulence modulation by particles in the near-wall region."],"url":"http://arxiv.org/abs/2404.05346v1","category":"physics.flu-dyn"}
{"created":"2024-04-08 09:23:52","title":"Jammer-Resilient Time Synchronization in the MIMO Uplink","abstract":"Spatial filtering based on multiple-input multiple-output (MIMO) processing is a promising approach to jammer mitigation. Effective MIMO data detectors that mitigate smart jammers have recently been proposed, but they all assume perfect time synchronization between transmitter(s) and receiver. However, to the best of our knowledge, there are no methods for resilient time synchronization in the presence of smart jammers. To remedy this situation, we propose JASS, the first method that enables reliable time synchronization for the single-user MIMO uplink while mitigating smart jamming attacks. JASS detects a randomized synchronization sequence based on a novel optimization problem that fits a spatial filter to the time-windowed receive signal in order to mitigate the jammer. We underscore the efficacy of the proposed optimization problem by proving that it ensures successful time synchronization under certain intuitive conditions. We then derive an efficient algorithm for approximately solving our optimization problem. Finally, we use simulations to demonstrate the effectiveness of JASS against a wide range of different jammer types.","sentences":["Spatial filtering based on multiple-input multiple-output (MIMO) processing is a promising approach to jammer mitigation.","Effective MIMO data detectors that mitigate smart jammers have recently been proposed, but they all assume perfect time synchronization between transmitter(s) and receiver.","However, to the best of our knowledge, there are no methods for resilient time synchronization in the presence of smart jammers.","To remedy this situation, we propose JASS, the first method that enables reliable time synchronization for the single-user MIMO uplink while mitigating smart jamming attacks.","JASS detects a randomized synchronization sequence based on a novel optimization problem that fits a spatial filter to the time-windowed receive signal in order to mitigate the jammer.","We underscore the efficacy of the proposed optimization problem by proving that it ensures successful time synchronization under certain intuitive conditions.","We then derive an efficient algorithm for approximately solving our optimization problem.","Finally, we use simulations to demonstrate the effectiveness of JASS against a wide range of different jammer types."],"url":"http://arxiv.org/abs/2404.05335v1","category":"eess.SP"}
{"created":"2024-04-08 09:14:04","title":"The spin, expansion and contraction of open star clusters","abstract":"Empirical constraints on the internal dynamics of open clusters are important for understanding their evolution and evaporation. High precision astrometry from Gaia DR3 are thus useful to observe aspects of the cluster dynamics. This work aims to identify dynamically peculiar clusters such as spinning and expanding clusters. We also quantify the spin frequency and expansion rate and compare them with $N$-body models to identify the origins of the peculiarities. We used the latest Gaia DR3 and archival spectroscopic surveys to analyse the radial velocities and proper motions of the cluster members in 1379 open clusters. A systematic analysis of synthetic clusters is performed to demonstrate the observability of the cluster spin along with effects of observational uncertainties. $N$-body simulations were used to understand the evolution of cluster spin and expansion for initially non-rotating clusters. We identified spin signatures in 10 clusters (and 16 candidates). Additionally, we detected expansion in 18 clusters and contraction in 3 clusters. The expansion rate is compatible with previous theoretical estimates based on expulsion of residual gas. The orientation of the spin axis is independent of the orbital angular momentum. The spin frequencies are much larger than what is expected from simulated initially non-rotating clusters. This indicates that >1% of the clusters are born rotating and/or they have undergone strong interactions. Higher precision observations are required to increase the sample of such dynamically peculiar clusters and to characterise them.","sentences":["Empirical constraints on the internal dynamics of open clusters are important for understanding their evolution and evaporation.","High precision astrometry from Gaia DR3 are thus useful to observe aspects of the cluster dynamics.","This work aims to identify dynamically peculiar clusters such as spinning and expanding clusters.","We also quantify the spin frequency and expansion rate and compare them with $N$-body models to identify the origins of the peculiarities.","We used the latest Gaia DR3 and archival spectroscopic surveys to analyse the radial velocities and proper motions of the cluster members in 1379 open clusters.","A systematic analysis of synthetic clusters is performed to demonstrate the observability of the cluster spin along with effects of observational uncertainties.","$N$-body simulations were used to understand the evolution of cluster spin and expansion for initially non-rotating clusters.","We identified spin signatures in 10 clusters (and 16 candidates).","Additionally, we detected expansion in 18 clusters and contraction in 3 clusters.","The expansion rate is compatible with previous theoretical estimates based on expulsion of residual gas.","The orientation of the spin axis is independent of the orbital angular momentum.","The spin frequencies are much larger than what is expected from simulated initially non-rotating clusters.","This indicates that >1% of the clusters are born rotating and/or they have undergone strong interactions.","Higher precision observations are required to increase the sample of such dynamically peculiar clusters and to characterise them."],"url":"http://arxiv.org/abs/2404.05327v1","category":"astro-ph.GA"}
{"created":"2024-04-08 08:34:04","title":"Can Edge Computing fulfill the requirements of automated vehicular services using 5G network ?","abstract":"Communication and computation services supporting Connected and Automated Vehicles (CAVs) are characterized by stringent requirements, in terms of response time and reliability. Fulfilling these requirements is crucial for ensuring road safety and traffic optimization. The conceptually simple solution of hosting these services in the vehicles increases their cost (mainly due to the installation and maintenance of computation infrastructure) and may drain their battery excessively. Such disadvantages can be tackled via Multi-Access Edge Computing (MEC), consisting in deploying computation capability in network nodes deployed close to the devices (vehicles in this case), such as to satisfy the stringent CAV requirements. However, it is not yet clear under which conditions MEC can support CAV requirements and for which services. To shed light on this question, we conduct a simulation campaign using well-known open-source simulation tools, namely OMNeT++, Simu5G, Veins, INET, and SUMO. We are thus able to provide a reality check on MEC for CAV, pinpointing what are the computation capacities that must be installed in the MEC, to support the different services, and the amount of vehicles that a single MEC node can support. We find that such parameters must vary a lot, depending on the service considered. This study can serve as a preliminary basis for network operators to plan future deployment of MEC to support CAV.","sentences":["Communication and computation services supporting Connected and Automated Vehicles (CAVs) are characterized by stringent requirements, in terms of response time and reliability.","Fulfilling these requirements is crucial for ensuring road safety and traffic optimization.","The conceptually simple solution of hosting these services in the vehicles increases their cost (mainly due to the installation and maintenance of computation infrastructure) and may drain their battery excessively.","Such disadvantages can be tackled via Multi-Access Edge Computing (MEC), consisting in deploying computation capability in network nodes deployed close to the devices (vehicles in this case), such as to satisfy the stringent CAV requirements.","However, it is not yet clear under which conditions MEC can support CAV requirements and for which services.","To shed light on this question, we conduct a simulation campaign using well-known open-source simulation tools, namely OMNeT++, Simu5G, Veins, INET, and SUMO.","We are thus able to provide a reality check on MEC for CAV, pinpointing what are the computation capacities that must be installed in the MEC, to support the different services, and the amount of vehicles that a single MEC node can support.","We find that such parameters must vary a lot, depending on the service considered.","This study can serve as a preliminary basis for network operators to plan future deployment of MEC to support CAV."],"url":"http://arxiv.org/abs/2404.05296v1","category":"cs.NI"}
{"created":"2024-04-08 08:32:24","title":"Reduction of (pseudo-)Critical Temperatures of Chiral Restoration and Deconfinement Phase Transitions in a Magnetized PNJL Model","abstract":"We investigate the chiral restoration and deconfinement phase transitions under external magnetic field in frame of a Pauli-Villars regularized PNJL model. A running Polyakov loop scale parameter $T_0(eB)$ is introduced to mimic the reaction of the gluon sector to the presence of magnetic fields. It is found that a decreasing $T_0(eB)$ with magnetic fields can realize the inverse magnetic catalysis phenomena of chiral condensates of $u$ and $d$ quarks, increase of Polyakov loop and the reduction of (pseudo-)critical temperatures of chiral restoration and deconfinement phase transitions.","sentences":["We investigate the chiral restoration and deconfinement phase transitions under external magnetic field in frame of a Pauli-Villars regularized PNJL model.","A running Polyakov loop scale parameter $T_0(eB)$ is introduced to mimic the reaction of the gluon sector to the presence of magnetic fields.","It is found that a decreasing $T_0(eB)$ with magnetic fields can realize the inverse magnetic catalysis phenomena of chiral condensates of $u$ and $d$ quarks, increase of Polyakov loop and the reduction of (pseudo-)critical temperatures of chiral restoration and deconfinement phase transitions."],"url":"http://arxiv.org/abs/2404.05294v1","category":"hep-ph"}
{"created":"2024-04-08 08:16:35","title":"Prediction intervals for overdispersed Poisson data and their application in medical and pre-clinical quality control","abstract":"In pre-clinical and medical quality control, it is of interest to assess the stability of the process under monitoring or to validate a current observation using historical control data. Classically, this is done by the application of historical control limits (HCL) graphically displayed in control charts. In many applications, HCL are applied to count data, e.g. the number of revertant colonies (Ames assay) or the number of relapses per multiple sclerosis patient. Count data may be overdispersed, can be heavily right-skewed and clusters may differ in cluster size or other baseline quantities (e.g. number of petri dishes per control group or different length of monitoring times per patient).   Based on the quasi-Poisson assumption or the negative-binomial distribution, we propose prediction intervals for overdispersed count data to be used as HCL. Variable baseline quantities are accounted for by offsets. Furthermore, we provide a bootstrap calibration algorithm that accounts for the skewed distribution and achieves equal tail probabilities.   Comprehensive Monte-Carlo simulations assessing the coverage probabilities of eight different methods for HCL calculation reveal, that the bootstrap calibrated prediction intervals control the type-1-error best. Heuristics traditionally used in control charts (e.g. the limits in Sheward c- or u-charts or the mean plus minus 2 SD) fail to control a pre-specified coverage probability.   The application of HCL is demonstrated based on data from the Ames assay and for numbers of relapses of multiple sclerosis patients. The proposed prediction intervals and the algorithm for bootstrap calibration are publicly available via the R package predint.","sentences":["In pre-clinical and medical quality control, it is of interest to assess the stability of the process under monitoring or to validate a current observation using historical control data.","Classically, this is done by the application of historical control limits (HCL) graphically displayed in control charts.","In many applications, HCL are applied to count data, e.g. the number of revertant colonies (Ames assay) or the number of relapses per multiple sclerosis patient.","Count data may be overdispersed, can be heavily right-skewed and clusters may differ in cluster size or other baseline quantities (e.g. number of petri dishes per control group or different length of monitoring times per patient).   ","Based on the quasi-Poisson assumption or the negative-binomial distribution, we propose prediction intervals for overdispersed count data to be used as HCL.","Variable baseline quantities are accounted for by offsets.","Furthermore, we provide a bootstrap calibration algorithm that accounts for the skewed distribution and achieves equal tail probabilities.   Comprehensive Monte-Carlo simulations assessing the coverage probabilities of eight different methods for HCL calculation reveal, that the bootstrap calibrated prediction intervals control the type-1-error best.","Heuristics traditionally used in control charts (e.g. the limits in Sheward c- or u-charts or the mean plus minus 2 SD) fail to control a pre-specified coverage probability.   ","The application of HCL is demonstrated based on data from the Ames assay and for numbers of relapses of multiple sclerosis patients.","The proposed prediction intervals and the algorithm for bootstrap calibration are publicly available via the R package predint."],"url":"http://arxiv.org/abs/2404.05282v1","category":"stat.AP"}
{"created":"2024-04-08 08:01:19","title":"Scheduling Multi-Server Jobs is Not Easy","abstract":"The problem of online scheduling of multi-server jobs is considered, where there are a total of $K$ servers, and each job requires concurrent service from multiple servers for it to be processed. Each job on its arrival reveals its processing time, the number of servers from which it needs concurrent service and an online algorithm has to make scheduling decisions using only causal information, with the goal of minimizing the response/flow time. The worst case input model is considered and the performance metric is the competitive ratio. For the case, when all job processing time (sizes) are the same, we show that the competitive ratio of any deterministic/randomized algorithm is at least $\\Omega(K)$ and propose an online algorithm whose competitive ratio is at most $K+1$. With equal job sizes, we also consider the resource augmentation regime where an online algorithm has access to more servers than an optimal offline algorithm. With resource augmentation, we propose a simple algorithm and show that it has a competitive ratio of $1$ when provided with $2K$ servers with respect to an optimal offline algorithm with $K$ servers. With unequal job sizes, we propose an online algorithm whose competitive ratio is at most $2K \\log (K w_{\\max})$, where $w_{\\max}$ is the maximum size of any job.","sentences":["The problem of online scheduling of multi-server jobs is considered, where there are a total of $K$ servers, and each job requires concurrent service from multiple servers for it to be processed.","Each job on its arrival reveals its processing time, the number of servers from which it needs concurrent service and an online algorithm has to make scheduling decisions using only causal information, with the goal of minimizing the response/flow time.","The worst case input model is considered and the performance metric is the competitive ratio.","For the case, when all job processing time (sizes) are the same, we show that the competitive ratio of any deterministic/randomized algorithm is at least $\\Omega(K)$ and propose an online algorithm whose competitive ratio is at most $K+1$. With equal job sizes, we also consider the resource augmentation regime where an online algorithm has access to more servers than an optimal offline algorithm.","With resource augmentation, we propose a simple algorithm and show that it has a competitive ratio of $1$ when provided with $2K$ servers with respect to an optimal offline algorithm with $K$ servers.","With unequal job sizes, we propose an online algorithm whose competitive ratio is at most $2K \\log (K w_{\\max})$, where $w_{\\max}$ is the maximum size of any job."],"url":"http://arxiv.org/abs/2404.05271v1","category":"cs.DS"}
{"created":"2024-04-08 08:00:05","title":"Exploiting Preference Elicitation in Interactive and User-centered Algorithmic Recourse: An Initial Exploration","abstract":"Algorithmic Recourse aims to provide actionable explanations, or recourse plans, to overturn potentially unfavourable decisions taken by automated machine learning models. In this paper, we propose an interaction paradigm based on a guided interaction pattern aimed at both eliciting the users' preferences and heading them toward effective recourse interventions. In a fictional task of money lending, we compare this approach with an exploratory interaction pattern based on a combination of alternative plans and the possibility of freely changing the configurations by the users themselves. Our results suggest that users may recognize that the guided interaction paradigm improves efficiency. However, they also feel less freedom to experiment with \"what-if\" scenarios. Nevertheless, the time spent on the purely exploratory interface tends to be perceived as a lack of efficiency, which reduces attractiveness, perspicuity, and dependability. Conversely, for the guided interface, more time on the interface seems to increase its attractiveness, perspicuity, and dependability while not impacting the perceived efficiency. That might suggest that this type of interfaces should combine these two approaches by trying to support exploratory behavior while gently pushing toward a guided effective solution.","sentences":["Algorithmic Recourse aims to provide actionable explanations, or recourse plans, to overturn potentially unfavourable decisions taken by automated machine learning models.","In this paper, we propose an interaction paradigm based on a guided interaction pattern aimed at both eliciting the users' preferences and heading them toward effective recourse interventions.","In a fictional task of money lending, we compare this approach with an exploratory interaction pattern based on a combination of alternative plans and the possibility of freely changing the configurations by the users themselves.","Our results suggest that users may recognize that the guided interaction paradigm improves efficiency.","However, they also feel less freedom to experiment with \"what-if\" scenarios.","Nevertheless, the time spent on the purely exploratory interface tends to be perceived as a lack of efficiency, which reduces attractiveness, perspicuity, and dependability.","Conversely, for the guided interface, more time on the interface seems to increase its attractiveness, perspicuity, and dependability while not impacting the perceived efficiency.","That might suggest that this type of interfaces should combine these two approaches by trying to support exploratory behavior while gently pushing toward a guided effective solution."],"url":"http://arxiv.org/abs/2404.05270v1","category":"cs.HC"}
{"created":"2024-04-08 07:34:39","title":"CodeEnhance: A Codebook-Driven Approach for Low-Light Image Enhancement","abstract":"Low-light image enhancement (LLIE) aims to improve low-illumination images. However, existing methods face two challenges: (1) uncertainty in restoration from diverse brightness degradations; (2) loss of texture and color information caused by noise suppression and light enhancement. In this paper, we propose a novel enhancement approach, CodeEnhance, by leveraging quantized priors and image refinement to address these challenges. In particular, we reframe LLIE as learning an image-to-code mapping from low-light images to discrete codebook, which has been learned from high-quality images. To enhance this process, a Semantic Embedding Module (SEM) is introduced to integrate semantic information with low-level features, and a Codebook Shift (CS) mechanism, designed to adapt the pre-learned codebook to better suit the distinct characteristics of our low-light dataset. Additionally, we present an Interactive Feature Transformation (IFT) module to refine texture and color information during image reconstruction, allowing for interactive enhancement based on user preferences. Extensive experiments on both real-world and synthetic benchmarks demonstrate that the incorporation of prior knowledge and controllable information transfer significantly enhances LLIE performance in terms of quality and fidelity. The proposed CodeEnhance exhibits superior robustness to various degradations, including uneven illumination, noise, and color distortion.","sentences":["Low-light image enhancement (LLIE) aims to improve low-illumination images.","However, existing methods face two challenges: (1) uncertainty in restoration from diverse brightness degradations; (2) loss of texture and color information caused by noise suppression and light enhancement.","In this paper, we propose a novel enhancement approach, CodeEnhance, by leveraging quantized priors and image refinement to address these challenges.","In particular, we reframe LLIE as learning an image-to-code mapping from low-light images to discrete codebook, which has been learned from high-quality images.","To enhance this process, a Semantic Embedding Module (SEM) is introduced to integrate semantic information with low-level features, and a Codebook Shift (CS) mechanism, designed to adapt the pre-learned codebook to better suit the distinct characteristics of our low-light dataset.","Additionally, we present an Interactive Feature Transformation (IFT) module to refine texture and color information during image reconstruction, allowing for interactive enhancement based on user preferences.","Extensive experiments on both real-world and synthetic benchmarks demonstrate that the incorporation of prior knowledge and controllable information transfer significantly enhances LLIE performance in terms of quality and fidelity.","The proposed CodeEnhance exhibits superior robustness to various degradations, including uneven illumination, noise, and color distortion."],"url":"http://arxiv.org/abs/2404.05253v1","category":"cs.CV"}
{"created":"2024-04-08 07:26:27","title":"Interpreting Themes from Educational Stories","abstract":"Reading comprehension continues to be a crucial research focus in the NLP community. Recent advances in Machine Reading Comprehension (MRC) have mostly centered on literal comprehension, referring to the surface-level understanding of content. In this work, we focus on the next level - interpretive comprehension, with a particular emphasis on inferring the themes of a narrative text. We introduce the first dataset specifically designed for interpretive comprehension of educational narratives, providing corresponding well-edited theme texts. The dataset spans a variety of genres and cultural origins and includes human-annotated theme keywords with varying levels of granularity. We further formulate NLP tasks under different abstractions of interpretive comprehension toward the main idea of a story. After conducting extensive experiments with state-of-the-art methods, we found the task to be both challenging and significant for NLP research. The dataset and source code have been made publicly available to the research community at https://github.com/RiTUAL-UH/EduStory.","sentences":["Reading comprehension continues to be a crucial research focus in the NLP community.","Recent advances in Machine Reading Comprehension (MRC) have mostly centered on literal comprehension, referring to the surface-level understanding of content.","In this work, we focus on the next level - interpretive comprehension, with a particular emphasis on inferring the themes of a narrative text.","We introduce the first dataset specifically designed for interpretive comprehension of educational narratives, providing corresponding well-edited theme texts.","The dataset spans a variety of genres and cultural origins and includes human-annotated theme keywords with varying levels of granularity.","We further formulate NLP tasks under different abstractions of interpretive comprehension toward the main idea of a story.","After conducting extensive experiments with state-of-the-art methods, we found the task to be both challenging and significant for NLP research.","The dataset and source code have been made publicly available to the research community at https://github.com/RiTUAL-UH/EduStory."],"url":"http://arxiv.org/abs/2404.05250v1","category":"cs.CL"}
{"created":"2024-04-08 07:21:56","title":"Assessing the causes of continuous effects by posterior effects of causes","abstract":"To evaluate a single cause of a binary effect, Dawid et al. (2014) defined the probability of causation, while Pearl (2015) defined the probabilities of necessity and sufficiency. For assessing the multiple correlated causes of a binary effect, Lu et al. (2023) defined the posterior causal effects based on post-treatment variables. In many scenarios, outcomes are continuous, simply binarizing them and applying previous methods may result in information loss or biased conclusions. To address this limitation, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous effect, including posterior intervention effects, posterior total causal effects, and posterior natural direct effects. Under the assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations. We also provide a simple but effective estimation procedure and establish the asymptotic properties of the proposed estimators. An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method.","sentences":["To evaluate a single cause of a binary effect, Dawid et al.","(2014) defined the probability of causation, while Pearl (2015) defined the probabilities of necessity and sufficiency.","For assessing the multiple correlated causes of a binary effect, Lu et al.","(2023) defined the posterior causal effects based on post-treatment variables.","In many scenarios, outcomes are continuous, simply binarizing them and applying previous methods may result in information loss or biased conclusions.","To address this limitation, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous effect, including posterior intervention effects, posterior total causal effects, and posterior natural direct effects.","Under the assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations.","We also provide a simple but effective estimation procedure and establish the asymptotic properties of the proposed estimators.","An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method."],"url":"http://arxiv.org/abs/2404.05246v1","category":"stat.ME"}
{"created":"2024-04-08 07:09:15","title":"Allowing humans to interactively guide machines where to look does not always improve a human-AI team's classification accuracy","abstract":"Via thousands of papers in Explainable AI (XAI), attention maps \\cite{vaswani2017attention} and feature attribution maps \\cite{bansal2020sam} have been established as a common means for explaining the input features that are important to AI's decisions. It is an interesting but unexplored question whether allowing users to edit the importance scores of input features at test time would improve the human-AI team's accuracy on downstream tasks. In this paper, we address this question by taking CHM-Corr, a state-of-the-art, ante-hoc explanation method \\cite{taesiri2022visual} that first predicts patch-wise correspondences between the input and the training-set images, and then uses them to make classification decisions. We build an interactive interface on top of CHM-Corr, enabling users to directly edit the initial feature attribution map provided by CHM-Corr. Via our CHM-Corr++ interface, users gain insights into if, when, and how the model changes its outputs, enhancing understanding beyond static explanations. Our user study with 18 machine learning researchers who performed $\\sim$1,400 decisions shows that our interactive approach does not improve user accuracy on CUB-200 bird image classification over static explanations. This challenges the belief that interactivity inherently boosts XAI effectiveness~\\cite{sokol2020one,sun2022exploring,shen2024towards,singh2024rethinking,mindlin2024beyond,lakkaraju2022rethinking,cheng2019explaining,liu2021understanding} and raises needs for future research. Our work contributes to the field by open-sourcing an interactive tool for manipulating model attention, and it lays the groundwork for future research to enable effective human-AI interaction in computer vision. We release code and data on \\href{https://anonymous.4open.science/r/CHMCorrPlusPlus/}{github}. Our interface are available \\href{http://137.184.82.109:7080/}{here}.","sentences":["Via thousands of papers in Explainable AI (XAI), attention maps \\cite{vaswani2017attention} and feature attribution maps \\cite{bansal2020sam} have been established as a common means for explaining the input features that are important to AI's decisions.","It is an interesting but unexplored question whether allowing users to edit the importance scores of input features at test time would improve the human-AI team's accuracy on downstream tasks.","In this paper, we address this question by taking CHM-Corr, a state-of-the-art, ante-hoc explanation method \\cite{taesiri2022visual} that first predicts patch-wise correspondences between the input and the training-set images, and then uses them to make classification decisions.","We build an interactive interface on top of CHM-Corr, enabling users to directly edit the initial feature attribution map provided by CHM-Corr.","Via our CHM-Corr++ interface, users gain insights into if, when, and how the model changes its outputs, enhancing understanding beyond static explanations.","Our user study with 18 machine learning researchers who performed $\\sim$1,400 decisions shows that our interactive approach does not improve user accuracy on CUB-200 bird image classification over static explanations.","This challenges the belief that interactivity inherently boosts XAI effectiveness~\\cite{sokol2020one,sun2022exploring,shen2024towards,singh2024rethinking,mindlin2024beyond,lakkaraju2022rethinking,cheng2019explaining,liu2021understanding} and raises needs for future research.","Our work contributes to the field by open-sourcing an interactive tool for manipulating model attention, and it lays the groundwork for future research to enable effective human-AI interaction in computer vision.","We release code and data on \\href{https://anonymous.4open.science/r/CHMCorrPlusPlus/}{github}.","Our interface are available \\href{http://137.184.82.109:7080/}{here}."],"url":"http://arxiv.org/abs/2404.05238v1","category":"cs.CV"}
{"created":"2024-04-08 07:01:24","title":"Bioinspired polymer-incorporating self-lubricating and antifouling hydrogels","abstract":"Healthy articular cartilage has excellent lubricating properties, with friction coefficients reaching extremely low values at physiological pressures. Such high-performing lubricating layer in joints is attributed to the surface hydration arising from the interplay between multiple hydrophilic biopolymers (such as hyaluronic acid, proteoglycans, and lubricin) and phospholipids in the cartilage matrix. Mimicking such molecular structure, hydrogels, composed of a hydrophilic polymer network, have the potential to replicate the lubricating feature and possibly replace natural cartilages. In this study, we have synthesized a poly(2-methacryloyloxyethyl phosphorylcholine-co-N-isopropylacrylamide) (PMPC-co-PNIPAM, PMN)random copolymer with highly-hydrated lubricious 2-methacryloyloxyethyl phosphorylcholine moieties and less hydrated N-isopropylacrylamide moieties. Incorporation of PMN copolymers within various hydrogels significantly reduces the gels sliding surface friction, resulting in low friction coefficients against different counter surfaces, including stainless steel (hard metal surface), polyethylene (hydrophobic surface), and polyHEMA (soft hydrogel surface). Additionally, hydrogels containing PMN are shown to be biocompatible and have excellent antifouling properties, making them an ideal coating for commercially available stents. With these qualities, hydrogels containing PMN stand out as a promising new material with numerous possible applications.","sentences":["Healthy articular cartilage has excellent lubricating properties, with friction coefficients reaching extremely low values at physiological pressures.","Such high-performing lubricating layer in joints is attributed to the surface hydration arising from the interplay between multiple hydrophilic biopolymers (such as hyaluronic acid, proteoglycans, and lubricin) and phospholipids in the cartilage matrix.","Mimicking such molecular structure, hydrogels, composed of a hydrophilic polymer network, have the potential to replicate the lubricating feature and possibly replace natural cartilages.","In this study, we have synthesized a poly(2-methacryloyloxyethyl phosphorylcholine-co-N-isopropylacrylamide) (PMPC-co-PNIPAM, PMN)random copolymer with highly-hydrated lubricious 2-methacryloyloxyethyl phosphorylcholine moieties and less hydrated N-isopropylacrylamide moieties.","Incorporation of PMN copolymers within various hydrogels significantly reduces the gels sliding surface friction, resulting in low friction coefficients against different counter surfaces, including stainless steel (hard metal surface), polyethylene (hydrophobic surface), and polyHEMA (soft hydrogel surface).","Additionally, hydrogels containing PMN are shown to be biocompatible and have excellent antifouling properties, making them an ideal coating for commercially available stents.","With these qualities, hydrogels containing PMN stand out as a promising new material with numerous possible applications."],"url":"http://arxiv.org/abs/2404.05234v1","category":"cond-mat.soft"}
{"created":"2024-04-08 06:56:18","title":"Invariant stability conditions on local $\\mathbb{P}^1\\times \\mathbb{P}^1$ (after Del Monte-Longhi)","abstract":"Let $X$ be the total space of canonical bundle of $\\pp$, we study an invariant subspace of stability conditions on $X$ under an autoequivalence of $D^b(X)$. We describe the complete set of stable objects with respect to the invariant stability conditions and characterize the space of invariant stability conditions.","sentences":["Let $X$ be the total space of canonical bundle of $\\pp$, we study an invariant subspace of stability conditions on $X$ under an autoequivalence of $D^b(X)$. We describe the complete set of stable objects with respect to the invariant stability conditions and characterize the space of invariant stability conditions."],"url":"http://arxiv.org/abs/2404.05232v1","category":"math.AG"}
{"created":"2024-04-08 06:53:05","title":"Non-concave distributionally robust stochastic control in a discrete time finite horizon setting","abstract":"In this article we present a general framework for non-concave distributionally robust stochastic control problems in a discrete time finite horizon setting. Our framework allows to consider a variety of different path-dependent ambiguity sets of probability measures comprising, as a natural example, the ambiguity set defined via Wasserstein-balls around path-dependent reference measures, as well as parametric classes of probability distributions. We establish a dynamic programming principle which allows to derive both optimal control and worst-case measure by solving recursively a sequence of one-step optimization problems. As a concrete application, we study the robust hedging problem of a financial derivative under an asymmetric (and non-convex) loss function accounting for different preferences of sell- and buy side when it comes to the hedging of financial derivatives. As our entirely data-driven ambiguity set of probability measures, we consider Wasserstein-balls around the empirical measure derived from real financial data. We demonstrate that during adverse scenarios such as a financial crisis, our robust approach outperforms typical model-based hedging strategies such as the classical Delta-hedging strategy as well as the hedging strategy obtained in the non-robust setting with respect to the empirical measure and therefore overcomes the problem of model misspecification in such critical periods.","sentences":["In this article we present a general framework for non-concave distributionally robust stochastic control problems in a discrete time finite horizon setting.","Our framework allows to consider a variety of different path-dependent ambiguity sets of probability measures comprising, as a natural example, the ambiguity set defined via Wasserstein-balls around path-dependent reference measures, as well as parametric classes of probability distributions.","We establish a dynamic programming principle which allows to derive both optimal control and worst-case measure by solving recursively a sequence of one-step optimization problems.","As a concrete application, we study the robust hedging problem of a financial derivative under an asymmetric (and non-convex) loss function accounting for different preferences of sell- and buy side when it comes to the hedging of financial derivatives.","As our entirely data-driven ambiguity set of probability measures, we consider Wasserstein-balls around the empirical measure derived from real financial data.","We demonstrate that during adverse scenarios such as a financial crisis, our robust approach outperforms typical model-based hedging strategies such as the classical Delta-hedging strategy as well as the hedging strategy obtained in the non-robust setting with respect to the empirical measure and therefore overcomes the problem of model misspecification in such critical periods."],"url":"http://arxiv.org/abs/2404.05230v1","category":"math.OC"}
{"created":"2024-04-08 06:27:38","title":"Out-of-Distribution Data: An Acquaintance of Adversarial Examples -- A Survey","abstract":"Deep neural networks (DNNs) deployed in real-world applications can encounter out-of-distribution (OOD) data and adversarial examples. These represent distinct forms of distributional shifts that can significantly impact DNNs' reliability and robustness. Traditionally, research has addressed OOD detection and adversarial robustness as separate challenges. This survey focuses on the intersection of these two areas, examining how the research community has investigated them together. Consequently, we identify two key research directions: robust OOD detection and unified robustness. Robust OOD detection aims to differentiate between in-distribution (ID) data and OOD data, even when they are adversarially manipulated to deceive the OOD detector. Unified robustness seeks a single approach to make DNNs robust against both adversarial attacks and OOD inputs. Accordingly, first, we establish a taxonomy based on the concept of distributional shifts. This framework clarifies how robust OOD detection and unified robustness relate to other research areas addressing distributional shifts, such as OOD detection, open set recognition, and anomaly detection. Subsequently, we review existing work on robust OOD detection and unified robustness. Finally, we highlight the limitations of the existing work and propose promising research directions that explore adversarial and OOD inputs within a unified framework.","sentences":["Deep neural networks (DNNs) deployed in real-world applications can encounter out-of-distribution (OOD) data and adversarial examples.","These represent distinct forms of distributional shifts that can significantly impact DNNs' reliability and robustness.","Traditionally, research has addressed OOD detection and adversarial robustness as separate challenges.","This survey focuses on the intersection of these two areas, examining how the research community has investigated them together.","Consequently, we identify two key research directions: robust OOD detection and unified robustness.","Robust OOD detection aims to differentiate between in-distribution (ID) data and OOD data, even when they are adversarially manipulated to deceive the OOD detector.","Unified robustness seeks a single approach to make DNNs robust against both adversarial attacks and OOD inputs.","Accordingly, first, we establish a taxonomy based on the concept of distributional shifts.","This framework clarifies how robust OOD detection and unified robustness relate to other research areas addressing distributional shifts, such as OOD detection, open set recognition, and anomaly detection.","Subsequently, we review existing work on robust OOD detection and unified robustness.","Finally, we highlight the limitations of the existing work and propose promising research directions that explore adversarial and OOD inputs within a unified framework."],"url":"http://arxiv.org/abs/2404.05219v1","category":"cs.LG"}
{"created":"2024-04-08 06:05:14","title":"Generalized measure Black-Scholes equation: Towards option self-similar pricing","abstract":"In this work, we give a generalized formulation of the Black-Scholes model. The novelty resides in considering the Black-Scholes model to be valid on 'average', but such that the pointwise option price dynamics depends on a measure representing the investors' 'uncertainty'. We make use of the theory of non-symmetric Dirichlet forms and the abstract theory of partial differential equations to establish well posedness of the problem. A detailed numerical analysis is given in the case of self-similar measures.","sentences":["In this work, we give a generalized formulation of the Black-Scholes model.","The novelty resides in considering the Black-Scholes model to be valid on 'average', but such that the pointwise option price dynamics depends on a measure representing the investors' 'uncertainty'.","We make use of the theory of non-symmetric Dirichlet forms and the abstract theory of partial differential equations to establish well posedness of the problem.","A detailed numerical analysis is given in the case of self-similar measures."],"url":"http://arxiv.org/abs/2404.05214v1","category":"q-fin.MF"}
{"created":"2024-04-08 05:39:41","title":"Maximally Forward-Looking Core Inflation","abstract":"Timely monetary policy decision-making requires timely core inflation measures. We create a new core inflation series that is explicitly designed to succeed at that goal. Precisely, we introduce the Assemblage Regression, a generalized nonnegative ridge regression problem that optimizes the price index's subcomponent weights such that the aggregate is maximally predictive of future headline inflation. Ordering subcomponents according to their rank in each period switches the algorithm to be learning supervised trimmed inflation - or, put differently, the maximally forward-looking summary statistic of the realized price changes distribution. In an extensive out-of-sample forecasting experiment for the US and the euro area, we find substantial improvements for signaling medium-term inflation developments in both the pre- and post-Covid years. Those coming from the supervised trimmed version are particularly striking, and are attributable to a highly asymmetric trimming which contrasts with conventional indicators. We also find that this metric was indicating first upward pressures on inflation as early as mid-2020 and quickly captured the turning point in 2022. We also consider extensions, like assembling inflation from geographical regions, trimmed temporal aggregation, and building core measures specialized for either upside or downside inflation risks.","sentences":["Timely monetary policy decision-making requires timely core inflation measures.","We create a new core inflation series that is explicitly designed to succeed at that goal.","Precisely, we introduce the Assemblage Regression, a generalized nonnegative ridge regression problem that optimizes the price index's subcomponent weights such that the aggregate is maximally predictive of future headline inflation.","Ordering subcomponents according to their rank in each period switches the algorithm to be learning supervised trimmed inflation - or, put differently, the maximally forward-looking summary statistic of the realized price changes distribution.","In an extensive out-of-sample forecasting experiment for the US and the euro area, we find substantial improvements for signaling medium-term inflation developments in both the pre- and post-Covid years.","Those coming from the supervised trimmed version are particularly striking, and are attributable to a highly asymmetric trimming which contrasts with conventional indicators.","We also find that this metric was indicating first upward pressures on inflation as early as mid-2020 and quickly captured the turning point in 2022.","We also consider extensions, like assembling inflation from geographical regions, trimmed temporal aggregation, and building core measures specialized for either upside or downside inflation risks."],"url":"http://arxiv.org/abs/2404.05209v1","category":"econ.EM"}
{"created":"2024-04-08 05:09:31","title":"Boundary shape reconstruction with Robin condition: existence result, stability analysis, and inversion via multiple measurements","abstract":"This study revisits the problem of identifying the unknown interior Robin boundary of a connected domain using Cauchy data from the exterior region of a harmonic function. It investigates two shape optimization reformulations employing least-squares boundary-data-tracking cost functionals. Firstly, it rigorously addresses the existence of optimal shape solutions, thus filling a gap in the literature. The argumentation utilized in the proof strategy is contingent upon the specific formulation under consideration. Secondly, it demonstrates the ill-posed nature of the two shape optimization formulations by establishing the compactness of the Riesz operator associated with the quadratic shape Hessian corresponding to each cost functional. Lastly, the study employs multiple sets of Cauchy data to address the difficulty of detecting concavities in the unknown boundary. Numerical experiments in two and three dimensions illustrate the numerical procedure relying on Sobolev gradients proposed herein.","sentences":["This study revisits the problem of identifying the unknown interior Robin boundary of a connected domain using Cauchy data from the exterior region of a harmonic function.","It investigates two shape optimization reformulations employing least-squares boundary-data-tracking cost functionals.","Firstly, it rigorously addresses the existence of optimal shape solutions, thus filling a gap in the literature.","The argumentation utilized in the proof strategy is contingent upon the specific formulation under consideration.","Secondly, it demonstrates the ill-posed nature of the two shape optimization formulations by establishing the compactness of the Riesz operator associated with the quadratic shape Hessian corresponding to each cost functional.","Lastly, the study employs multiple sets of Cauchy data to address the difficulty of detecting concavities in the unknown boundary.","Numerical experiments in two and three dimensions illustrate the numerical procedure relying on Sobolev gradients proposed herein."],"url":"http://arxiv.org/abs/2404.05202v1","category":"math.NA"}
{"created":"2024-04-08 03:57:29","title":"Emergence in String Theory and Fermi Gases","abstract":"The Emergence Proposal suggests that some Swampland criteria, in particular on large field distances, are a consequence of the emergent nature of dynamics for fields in the infrared. In the context of type II string theory compactified on Calabi-Yau manifolds, it proposes that the cubic tree-level piece of the genus-zero prepotential is emergent from integrating out massive non-perturbative states. For a certain special non-compact Calabi-Yau, the blown-up conifold, it is known that the full all-genus prepotential can be matched onto the Grand Canonical potential of a two-dimensional Fermi gas. We propose here that this should be understood in the context of emergence: the prepotential is induced by integrating out the Fermi gas degrees of freedom. To make contact with the Swampland we need dynamical gravity, so compact Calabi-Yau manifolds. We show that for specifically the cubic term, an integrating out calculation also works for compact cases. In particular, the exact cubic term coefficient can be recovered from integrating out a Fermi gas for any compact Calabi-Yau that is an elliptic fibration over a reflexive toric base. We also propose a general map, for any one-parameter Calabi-Yau, between the Grand Canonical potential of the ultraviolet non-perturbative system and the period. In particular, this map leads to an emergent cubic term in the genus-zero prepotential for any such one-parameter model.","sentences":["The Emergence Proposal suggests that some Swampland criteria, in particular on large field distances, are a consequence of the emergent nature of dynamics for fields in the infrared.","In the context of type II string theory compactified on Calabi-Yau manifolds, it proposes that the cubic tree-level piece of the genus-zero prepotential is emergent from integrating out massive non-perturbative states.","For a certain special non-compact Calabi-Yau, the blown-up conifold, it is known that the full all-genus prepotential can be matched onto the Grand Canonical potential of a two-dimensional Fermi gas.","We propose here that this should be understood in the context of emergence: the prepotential is induced by integrating out the Fermi gas degrees of freedom.","To make contact with the Swampland we need dynamical gravity, so compact Calabi-Yau manifolds.","We show that for specifically the cubic term, an integrating out calculation also works for compact cases.","In particular, the exact cubic term coefficient can be recovered from integrating out a Fermi gas for any compact Calabi-Yau that is an elliptic fibration over a reflexive toric base.","We also propose a general map, for any one-parameter Calabi-Yau, between the Grand Canonical potential of the ultraviolet non-perturbative system and the period.","In particular, this map leads to an emergent cubic term in the genus-zero prepotential for any such one-parameter model."],"url":"http://arxiv.org/abs/2404.05176v1","category":"hep-th"}
{"created":"2024-04-08 03:45:00","title":"Directed Buy-at-Bulk Spanners","abstract":"We present a framework that unifies directed buy-at-bulk network design and directed spanner problems, namely, \\emph{buy-at-bulk spanners}. The goal is to find a minimum-cost routing solution for network design problems that capture economies at scale, while satisfying demands and distance constraints for terminal pairs. A more restricted version of this problem was shown to be $O(2^{{\\log^{1-\\ep} n}})$-hard to approximate, where $n$ is the number of vertices, under a standard complexity assumption, due to Elkin and Peleg (Theory of Computing Systems, 2007).   To the best of our knowledge, our results are the first sublinear factor approximation algorithms for directed buy-at-bulk spanners. Furthermore, these results hold even when we allow the edge lengths to be \\emph{negative}, unlike the previous literature for spanners. Our approximation ratios match the state-of-the-art ratios in special cases, namely, buy-at-bulk network design by Antonakopoulos (WAOA, 2010) and weighted spanners by Grigorescu, Kumar, and Lin (APPROX 2023).   Our results are based on new approximation algorithms for the following two problems that are of independent interest: \\emph{minimum-density distance-constrained junction trees} and \\emph{resource-constrained shortest path with negative consumption}.","sentences":["We present a framework that unifies directed buy-at-bulk network design and directed spanner problems, namely, \\emph{buy-at-bulk spanners}.","The goal is to find a minimum-cost routing solution for network design problems that capture economies at scale, while satisfying demands and distance constraints for terminal pairs.","A more restricted version of this problem was shown to be $O(2^{{\\log^{1-\\ep} n}})$-hard to approximate, where $n$ is the number of vertices, under a standard complexity assumption, due to Elkin and Peleg (Theory of Computing Systems, 2007).   ","To the best of our knowledge, our results are the first sublinear factor approximation algorithms for directed buy-at-bulk spanners.","Furthermore, these results hold even when we allow the edge lengths to be \\emph{negative}, unlike the previous literature for spanners.","Our approximation ratios match the state-of-the-art ratios in special cases, namely, buy-at-bulk network design by Antonakopoulos (WAOA, 2010) and weighted spanners by Grigorescu, Kumar, and Lin (APPROX 2023).   ","Our results are based on new approximation algorithms for the following two problems that are of independent interest: \\emph{minimum-density distance-constrained junction trees} and \\emph{resource-constrained shortest path with negative consumption}."],"url":"http://arxiv.org/abs/2404.05172v1","category":"cs.DS"}
{"created":"2024-04-08 03:33:01","title":"QMix: Quality-aware Learning with Mixed Noise for Robust Retinal Disease Diagnosis","abstract":"Due to the complexity of medical image acquisition and the difficulty of annotation, medical image datasets inevitably contain noise. Noisy data with wrong labels affects the robustness and generalization ability of deep neural networks. Previous noise learning methods mainly considered noise arising from images being mislabeled, i.e. label noise, assuming that all mislabeled images are of high image quality. However, medical images are prone to suffering extreme quality issues, i.e. data noise, where discriminative visual features are missing for disease diagnosis. In this paper, we propose a noise learning framework, termed as QMix, that learns a robust disease diagnosis model under mixed noise. QMix alternates between sample separation and quality-aware semisupervised training in each training epoch. In the sample separation phase, we design a joint uncertainty-loss criterion to effectively separate (1) correctly labeled images; (2) mislabeled images with high quality and (3) mislabeled images with low quality. In the semi-supervised training phase, we train a disease diagnosis model to learn robust feature representation from the separated samples. Specifically, we devise a sample-reweighing loss to mitigate the effect of mislabeled images with low quality during training. Meanwhile, a contrastive enhancement loss is proposed to further distinguish mislabeled images with low quality from correctly labeled images. QMix achieved state-of-the-art disease diagnosis performance on five public retinal image datasets and exhibited substantial improvement on robustness against mixed noise.","sentences":["Due to the complexity of medical image acquisition and the difficulty of annotation, medical image datasets inevitably contain noise.","Noisy data with wrong labels affects the robustness and generalization ability of deep neural networks.","Previous noise learning methods mainly considered noise arising from images being mislabeled, i.e. label noise, assuming that all mislabeled images are of high image quality.","However, medical images are prone to suffering extreme quality issues, i.e. data noise, where discriminative visual features are missing for disease diagnosis.","In this paper, we propose a noise learning framework, termed as QMix, that learns a robust disease diagnosis model under mixed noise.","QMix alternates between sample separation and quality-aware semisupervised training in each training epoch.","In the sample separation phase, we design a joint uncertainty-loss criterion to effectively separate (1) correctly labeled images; (2) mislabeled images with high quality and (3) mislabeled images with low quality.","In the semi-supervised training phase, we train a disease diagnosis model to learn robust feature representation from the separated samples.","Specifically, we devise a sample-reweighing loss to mitigate the effect of mislabeled images with low quality during training.","Meanwhile, a contrastive enhancement loss is proposed to further distinguish mislabeled images with low quality from correctly labeled images.","QMix achieved state-of-the-art disease diagnosis performance on five public retinal image datasets and exhibited substantial improvement on robustness against mixed noise."],"url":"http://arxiv.org/abs/2404.05169v1","category":"cs.CV"}
{"created":"2024-04-08 02:55:01","title":"Semantic Stealth: Adversarial Text Attacks on NLP Using Several Methods","abstract":"In various real-world applications such as machine translation, sentiment analysis, and question answering, a pivotal role is played by NLP models, facilitating efficient communication and decision-making processes in domains ranging from healthcare to finance. However, a significant challenge is posed to the robustness of these natural language processing models by text adversarial attacks. These attacks involve the deliberate manipulation of input text to mislead the predictions of the model while maintaining human interpretability. Despite the remarkable performance achieved by state-of-the-art models like BERT in various natural language processing tasks, they are found to remain vulnerable to adversarial perturbations in the input text. In addressing the vulnerability of text classifiers to adversarial attacks, three distinct attack mechanisms are explored in this paper using the victim model BERT: BERT-on-BERT attack, PWWS attack, and Fraud Bargain's Attack (FBA). Leveraging the IMDB, AG News, and SST2 datasets, a thorough comparative analysis is conducted to assess the effectiveness of these attacks on the BERT classifier model. It is revealed by the analysis that PWWS emerges as the most potent adversary, consistently outperforming other methods across multiple evaluation scenarios, thereby emphasizing its efficacy in generating adversarial examples for text classification. Through comprehensive experimentation, the performance of these attacks is assessed and the findings indicate that the PWWS attack outperforms others, demonstrating lower runtime, higher accuracy, and favorable semantic similarity scores. The key insight of this paper lies in the assessment of the relative performances of three prevalent state-of-the-art attack mechanisms.","sentences":["In various real-world applications such as machine translation, sentiment analysis, and question answering, a pivotal role is played by NLP models, facilitating efficient communication and decision-making processes in domains ranging from healthcare to finance.","However, a significant challenge is posed to the robustness of these natural language processing models by text adversarial attacks.","These attacks involve the deliberate manipulation of input text to mislead the predictions of the model while maintaining human interpretability.","Despite the remarkable performance achieved by state-of-the-art models like BERT in various natural language processing tasks, they are found to remain vulnerable to adversarial perturbations in the input text.","In addressing the vulnerability of text classifiers to adversarial attacks, three distinct attack mechanisms are explored in this paper using the victim model BERT: BERT-on-BERT attack, PWWS attack, and Fraud Bargain's Attack (FBA).","Leveraging the IMDB, AG News, and SST2 datasets, a thorough comparative analysis is conducted to assess the effectiveness of these attacks on the BERT classifier model.","It is revealed by the analysis that PWWS emerges as the most potent adversary, consistently outperforming other methods across multiple evaluation scenarios, thereby emphasizing its efficacy in generating adversarial examples for text classification.","Through comprehensive experimentation, the performance of these attacks is assessed and the findings indicate that the PWWS attack outperforms others, demonstrating lower runtime, higher accuracy, and favorable semantic similarity scores.","The key insight of this paper lies in the assessment of the relative performances of three prevalent state-of-the-art attack mechanisms."],"url":"http://arxiv.org/abs/2404.05159v1","category":"cs.CL"}
{"created":"2024-04-08 02:48:18","title":"Longtime Asymptotic Behavior of Nonlinear Fokker-Planck Type Equations with Periodic Boundary Conditions","abstract":"In this paper, we study the asymptotic behavior of a class of nonlinear Fokker-Planck type equations in a bounded domain with periodic boundary conditions. The system is motivated by our study of grain boundary dynamics, especially under the non-isothermal environments. To obtain the long time behavior of the solutions, in particular, the exponential decay, the kinematic structures of the systems are investigated using novel reinterpretation of the classical entropy method.","sentences":["In this paper, we study the asymptotic behavior of a class of nonlinear Fokker-Planck type equations in a bounded domain with periodic boundary conditions.","The system is motivated by our study of grain boundary dynamics, especially under the non-isothermal environments.","To obtain the long time behavior of the solutions, in particular, the exponential decay, the kinematic structures of the systems are investigated using novel reinterpretation of the classical entropy method."],"url":"http://arxiv.org/abs/2404.05157v1","category":"math.AP"}
{"created":"2024-04-08 02:41:32","title":"On the price of exact truthfulness in incentive-compatible online learning with bandit feedback: A regret lower bound for WSU-UX","abstract":"In one view of the classical game of prediction with expert advice with binary outcomes, in each round, each expert maintains an adversarially chosen belief and honestly reports this belief. We consider a recently introduced, strategic variant of this problem with selfish (reputation-seeking) experts, where each expert strategically reports in order to maximize their expected future reputation based on their belief. In this work, our goal is to design an algorithm for the selfish experts problem that is incentive-compatible (IC, or \\emph{truthful}), meaning each expert's best strategy is to report truthfully, while also ensuring the algorithm enjoys sublinear regret with respect to the expert with the best belief. Freeman et al. (2020) recently studied this problem in the full information and bandit settings and obtained truthful, no-regret algorithms by leveraging prior work on wagering mechanisms. While their results under full information match the minimax rate for the classical (\"honest experts\") problem, the best-known regret for their bandit algorithm WSU-UX is $O(T^{2/3})$, which does not match the minimax rate for the classical (\"honest bandits\") setting. It was unclear whether the higher regret was an artifact of their analysis or a limitation of WSU-UX. We show, via explicit construction of loss sequences, that the algorithm suffers a worst-case $\\Omega(T^{2/3})$ lower bound. Left open is the possibility that a different IC algorithm obtains $O(\\sqrt{T})$ regret. Yet, WSU-UX was a natural choice for such an algorithm owing to the limited design room for IC algorithms in this setting.","sentences":["In one view of the classical game of prediction with expert advice with binary outcomes, in each round, each expert maintains an adversarially chosen belief and honestly reports this belief.","We consider a recently introduced, strategic variant of this problem with selfish (reputation-seeking) experts, where each expert strategically reports in order to maximize their expected future reputation based on their belief.","In this work, our goal is to design an algorithm for the selfish experts problem that is incentive-compatible (IC, or \\emph{truthful}), meaning each expert's best strategy is to report truthfully, while also ensuring the algorithm enjoys sublinear regret with respect to the expert with the best belief.","Freeman et al. (2020) recently studied this problem in the full information and bandit settings and obtained truthful, no-regret algorithms by leveraging prior work on wagering mechanisms.","While their results under full information match the minimax rate for the classical (\"honest experts\") problem, the best-known regret for their bandit algorithm WSU-UX is $O(T^{2/3})$, which does not match the minimax rate for the classical (\"honest bandits\") setting.","It was unclear whether the higher regret was an artifact of their analysis or a limitation of WSU-UX.","We show, via explicit construction of loss sequences, that the algorithm suffers a worst-case $\\Omega(T^{2/3})$ lower bound.","Left open is the possibility that a different IC algorithm obtains $O(\\sqrt{T})$ regret.","Yet, WSU-UX was a natural choice for such an algorithm owing to the limited design room for IC algorithms in this setting."],"url":"http://arxiv.org/abs/2404.05155v1","category":"cs.LG"}
{"created":"2024-04-08 02:36:53","title":"Newton polygons and B\u00f6ttcher coordinates near infinity for polynomial skew products","abstract":"Let $f(z,w)=(p(z),q(z,w))$ be a polynomial skew product such that the degrees of $p$ and $q$ are grater than or equal to $2$. Under one or two conditions, we prove that $f$ is conjugate to a monomial map on an invariant region near infinity. The monomial map and the region are determined by the degree of $p$ and a Newton polygon of $q$. Moreover, the region is included in the attracting basin of a superattracting fixed or indeterminacy point at infinity, or in the closure of the attracting basins of two point at infinity.","sentences":["Let $f(z,w)=(p(z),q(z,w))$ be a polynomial skew product such that the degrees of $p$ and $q$ are grater than or equal to $2$. Under one or two conditions, we prove that $f$ is conjugate to a monomial map on an invariant region near infinity.","The monomial map and the region are determined by the degree of $p$ and a Newton polygon of $q$. Moreover, the region is included in the attracting basin of a superattracting fixed or indeterminacy point at infinity, or in the closure of the attracting basins of two point at infinity."],"url":"http://arxiv.org/abs/2404.05154v1","category":"math.DS"}
{"created":"2024-04-08 02:30:38","title":"A Fast Analytical Model for Predicting Battery Performance Under Mixed Kinetic Control","abstract":"The prediction of battery rate performance traditionally relies on computation-intensive numerical simulations. While simplified analytical models have been developed to accelerate the calculation, they usually assume battery performance to be controlled by a single rate-limiting process, such as solid diffusion or electrolyte transport. Here, we propose an improved analytical model that could be applied to battery discharging under mixed control of mass transport in both solid and electrolyte phases. Compared to previous single-particle models extended to incorporate the electrolyte kinetics, our model is able to predict the effect of salt depletion on diminishing the discharge capacity, a phenomenon that becomes important in thick electrodes and/or at high rates. The model demonstrates good agreement with the full-order simulation over a wide range of cell parameters and offers a speedup of over 600 times at the same time. Furthermore, it could be combined with gradient-based optimization algorithms to very efficiently search for the optimal battery cell configurations while numerical simulation fails at the task due to its inability to accurately evaluate the derivatives of the objective function. The high efficiency and the analytical nature of the model render it a powerful tool for battery cell design and optimization.","sentences":["The prediction of battery rate performance traditionally relies on computation-intensive numerical simulations.","While simplified analytical models have been developed to accelerate the calculation, they usually assume battery performance to be controlled by a single rate-limiting process, such as solid diffusion or electrolyte transport.","Here, we propose an improved analytical model that could be applied to battery discharging under mixed control of mass transport in both solid and electrolyte phases.","Compared to previous single-particle models extended to incorporate the electrolyte kinetics, our model is able to predict the effect of salt depletion on diminishing the discharge capacity, a phenomenon that becomes important in thick electrodes and/or at high rates.","The model demonstrates good agreement with the full-order simulation over a wide range of cell parameters and offers a speedup of over 600 times at the same time.","Furthermore, it could be combined with gradient-based optimization algorithms to very efficiently search for the optimal battery cell configurations while numerical simulation fails at the task due to its inability to accurately evaluate the derivatives of the objective function.","The high efficiency and the analytical nature of the model render it a powerful tool for battery cell design and optimization."],"url":"http://arxiv.org/abs/2404.05152v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 02:18:57","title":"Generalized Criterion for Identifiability of Additive Noise Models Using Majorization","abstract":"The discovery of causal relationships from observational data is very challenging. Many recent approaches rely on complexity or uncertainty concepts to impose constraints on probability distributions, aiming to identify specific classes of directed acyclic graph (DAG) models. In this paper, we introduce a novel identifiability criterion for DAGs that places constraints on the conditional variances of additive noise models. We demonstrate that this criterion extends and generalizes existing identifiability criteria in the literature that employ (conditional) variances as measures of uncertainty in (conditional) distributions. For linear Structural Equation Models, we present a new algorithm that leverages the concept of weak majorization applied to the diagonal elements of the Cholesky factor of the covariance matrix to learn a topological ordering of variables. Through extensive simulations and the analysis of bank connectivity data, we provide evidence of the effectiveness of our approach in successfully recovering DAGs. The code for reproducing the results in this paper is available in Supplementary Materials.","sentences":["The discovery of causal relationships from observational data is very challenging.","Many recent approaches rely on complexity or uncertainty concepts to impose constraints on probability distributions, aiming to identify specific classes of directed acyclic graph (DAG) models.","In this paper, we introduce a novel identifiability criterion for DAGs that places constraints on the conditional variances of additive noise models.","We demonstrate that this criterion extends and generalizes existing identifiability criteria in the literature that employ (conditional) variances as measures of uncertainty in (conditional) distributions.","For linear Structural Equation Models, we present a new algorithm that leverages the concept of weak majorization applied to the diagonal elements of the Cholesky factor of the covariance matrix to learn a topological ordering of variables.","Through extensive simulations and the analysis of bank connectivity data, we provide evidence of the effectiveness of our approach in successfully recovering DAGs.","The code for reproducing the results in this paper is available in Supplementary Materials."],"url":"http://arxiv.org/abs/2404.05148v1","category":"stat.ME"}
{"created":"2024-04-08 01:38:43","title":"Better Monocular 3D Detectors with LiDAR from the Past","abstract":"Accurate 3D object detection is crucial to autonomous driving. Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles. Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images. In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data. Specifically, at inference time, we assume that the camera-based detectors have access to multiple unlabeled LiDAR scans from past traversals at locations of interest (potentially from other high-end vehicles equipped with LiDAR sensors). Under this setup, we proposed a novel, simple, and end-to-end trainable framework, termed AsyncDepth, to effectively extract relevant features from asynchronous LiDAR traversals of the same location for monocular 3D detectors. We show consistent and significant performance gain (up to 9 AP) across multiple state-of-the-art models and datasets with a negligible additional latency of 9.66 ms and a small storage cost.","sentences":["Accurate 3D object detection is crucial to autonomous driving.","Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles.","Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images.","In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data.","Specifically, at inference time, we assume that the camera-based detectors have access to multiple unlabeled LiDAR scans from past traversals at locations of interest (potentially from other high-end vehicles equipped with LiDAR sensors).","Under this setup, we proposed a novel, simple, and end-to-end trainable framework, termed AsyncDepth, to effectively extract relevant features from asynchronous LiDAR traversals of the same location for monocular 3D detectors.","We show consistent and significant performance gain (up to 9 AP) across multiple state-of-the-art models and datasets with a negligible additional latency of 9.66 ms and a small storage cost."],"url":"http://arxiv.org/abs/2404.05139v1","category":"cs.CV"}
{"created":"2024-04-08 01:36:36","title":"Out-of-plane orientated self-trapped excitons enabled polarized light guiding in 2D perovskites","abstract":"Active optical waveguides combine light source and waveguides together in an individual component, which are essential for the integrated photonic chips. Although 1D luminescent materials based optical waveguides were extensively investigated, 2D waveguides allow photons to flow within a plane and serve as an ideal component for the ultracompact photonic circuits. Nevertheless, light guiding in 2D planar structures normally relies on the precise control of molecular orientation, which is complicated and low yield. Here, we report a strategy to guide polarized light in 2D microflakes by making use of the out-of-plane (OP) orientation of self-trapped excitons in as-synthesized 2D perovskite microplates. A space confined crystallization method is developed to synthesize 2D perovskite microflakes with dominated broad self-trapped excitons emission at room temperature, which are highly OP orientated with a percentage of the OP component over 85%. Taking advantages of the negligible absorption coefficient and improved coupling efficiency of OP orientated self-trapped exciton emission to the planar waveguide mode of the as-synthesized perovskite microflakes, we have achieved a broadband polarized light guiding with a full width at half maximum over 120 nm. Our findings provide a promising platform for the development of ultracompact photonic circuits.","sentences":["Active optical waveguides combine light source and waveguides together in an individual component, which are essential for the integrated photonic chips.","Although 1D luminescent materials based optical waveguides were extensively investigated, 2D waveguides allow photons to flow within a plane and serve as an ideal component for the ultracompact photonic circuits.","Nevertheless, light guiding in 2D planar structures normally relies on the precise control of molecular orientation, which is complicated and low yield.","Here, we report a strategy to guide polarized light in 2D microflakes by making use of the out-of-plane (OP) orientation of self-trapped excitons in as-synthesized 2D perovskite microplates.","A space confined crystallization method is developed to synthesize 2D perovskite microflakes with dominated broad self-trapped excitons emission at room temperature, which are highly OP orientated with a percentage of the OP component over 85%.","Taking advantages of the negligible absorption coefficient and improved coupling efficiency of OP orientated self-trapped exciton emission to the planar waveguide mode of the as-synthesized perovskite microflakes, we have achieved a broadband polarized light guiding with a full width at half maximum over 120 nm.","Our findings provide a promising platform for the development of ultracompact photonic circuits."],"url":"http://arxiv.org/abs/2404.05138v1","category":"physics.optics"}
{"created":"2024-04-08 01:19:53","title":"On Euler-Dierkes-Huisken variational problem","abstract":"In this paper, we study the stability and minimizing properties of higher codimensional surfaces in Euclidean space associated with the $f$-weighted area-functional $$\\mathcal{E}_f(M)=\\int_M f(x)\\; d \\mathcal{H}_k$$ with the density function $f(x)=g(|x|)$ and $g(t)$ is non-negative, which develop the recent works by U. Dierkes and G. Huisken (Math. Ann., 20 October 2023) on hypersurfaces with the density function $|x|^\\alpha$. Under suitable assumptions on $g(t)$, we prove that minimal cones with globally flat normal bundles are $f$-stable, and we also prove that the regular minimal cones satisfying Lawlor curvature criterion, the highly singular determinantal varieties and Pfaffian varieties without some exceptional cases are $f$-minimizing. As an application, we show that $k$-dimensional minimal cones over product of spheres are $|x|^\\alpha$-stable for $\\alpha\\geq-k+2\\sqrt{2(k-1)}$, the oriented stable minimal hypercones are $|x|^\\alpha$-stable for $\\alpha\\geq 0$, and we also show that the minimal cones over product of spheres $\\mathcal{C}=C \\left(S^{k_1} \\times \\cdots \\times S^{k_{m}}\\right)$ are $|x|^\\alpha$-minimizing for $\\dim \\mathcal{C} \\geq 7$, $k_i>1$ and $\\alpha \\geq 0$, the Simons cones $C(S^{p} \\times S^{p})(p\\geq 1)$ are $|x|^\\alpha$-minimizing for any $\\alpha \\geq 1$, which relaxes the assumption $1\\leq\\alpha \\leq 2p$ in \\cite{DH23}.","sentences":["In this paper, we study the stability and minimizing properties of higher codimensional surfaces in Euclidean space associated with the $f$-weighted area-functional $$\\mathcal{E}_f(M)=\\int_M f(x)\\; d \\mathcal{H}_k$$ with the density function $f(x)=g(|x|)$ and $g(t)$ is non-negative, which develop the recent works by U. Dierkes and G. Huisken (Math.","Ann., 20 October 2023) on hypersurfaces with the density function $|x|^\\alpha$. Under suitable assumptions on $g(t)$, we prove that minimal cones with globally flat normal bundles are $f$-stable, and we also prove that the regular minimal cones satisfying Lawlor curvature criterion, the highly singular determinantal varieties and Pfaffian varieties without some exceptional cases are $f$-minimizing.","As an application, we show that $k$-dimensional minimal cones over product of spheres are $|x|^\\alpha$-stable for $\\alpha\\geq-k+2\\sqrt{2(k-1)}$, the oriented stable minimal hypercones are $|x|^\\alpha$-stable for $\\alpha\\geq 0$, and we also show that the minimal cones over product of spheres $\\mathcal{C}=C \\left(S^{k_1} \\times \\cdots \\times S^{k_{m}}\\right)$ are $|x|^\\alpha$-minimizing for $\\dim \\mathcal{C} \\geq 7$, $k_i>1$ and $\\alpha \\geq 0$, the Simons cones $C(S^{p} \\times S^{p})(p\\geq 1)$ are $|x|^\\alpha$-minimizing for any $\\alpha \\geq 1$, which relaxes the assumption $1\\leq\\alpha \\leq 2p$ in \\cite{DH23}."],"url":"http://arxiv.org/abs/2404.05132v1","category":"math.DG"}
{"created":"2024-04-08 00:54:29","title":"Optimized LinDistFlow for High-Fidelity Power Flow Modeling of Distribution Networks","abstract":"The DistFlow model accurately represents power flows in distribution systems, but the model's nonlinearities result in computational challenges for many optimization applications. Accordingly, a linear approximation known as LinDistFlow is commonly employed. This paper introduces an algorithm for enhancing the accuracy of the LinDistFlow approximation, with the goal of aligning the outputs more closely with those from the nonlinear DistFlow model. Using sensitivity information, our algorithm optimizes the LinDistFlow approximation's coefficient and bias parameters to minimize discrepancies in predictions of voltage magnitudes relative to the nonlinear DistFlow model. The algorithm employs the Truncated Newton Conjugate-Gradient (TNC) optimization method to fine-tune coefficients and bias parameters during an offline training phase in order to improve the LinDistFlow approximation's accuracy in optimization applications. This improves the model's effectiveness across various system scenarios, leading to a marked improvement in predictive accuracy. Numerical results underscore the algorithm's efficacy, showcasing accuracy improvements in $L_{1}$-norm and $L_{\\infty}$-norm losses of up to $92\\%$ and $88\\%$, respectively, relative to the traditional LinDistFlow model. We assess how the optimized parameters perform under changes in the network topology and also validate the optimized LinDistFlow approximation's efficacy in a hosting capacity optimization problem.","sentences":["The DistFlow model accurately represents power flows in distribution systems, but the model's nonlinearities result in computational challenges for many optimization applications.","Accordingly, a linear approximation known as LinDistFlow is commonly employed.","This paper introduces an algorithm for enhancing the accuracy of the LinDistFlow approximation, with the goal of aligning the outputs more closely with those from the nonlinear DistFlow model.","Using sensitivity information, our algorithm optimizes the LinDistFlow approximation's coefficient and bias parameters to minimize discrepancies in predictions of voltage magnitudes relative to the nonlinear DistFlow model.","The algorithm employs the Truncated Newton Conjugate-Gradient (TNC) optimization method to fine-tune coefficients and bias parameters during an offline training phase in order to improve the LinDistFlow approximation's accuracy in optimization applications.","This improves the model's effectiveness across various system scenarios, leading to a marked improvement in predictive accuracy.","Numerical results underscore the algorithm's efficacy, showcasing accuracy improvements in $L_{1}$-norm and $L_{\\infty}$-norm losses of up to $92\\%$ and $88\\%$, respectively, relative to the traditional LinDistFlow model.","We assess how the optimized parameters perform under changes in the network topology and also validate the optimized LinDistFlow approximation's efficacy in a hosting capacity optimization problem."],"url":"http://arxiv.org/abs/2404.05125v1","category":"eess.SY"}
{"created":"2024-04-08 00:28:57","title":"Time-dependent conserved operators for non-relativistic Schr\u00f6dinger equation with electromagnetic field and quantization of resistance","abstract":"Two systems are studied: the first one involves a charged particle under the influence of a constant electric field, and the second one involves a charged particle under the influence of a constant electromagnetic field. For both systems, it is possible to find time-dependent conserved operators that can be used to derive time-dependent solutions to the complete Schr\\\"odinger equation. These conserved operators are employed to define the symmetries of the system. An argument of invariance of the wave function under the action of a unitary operator leads to the quantization of resistance and resistivity, in integer multiples of the von Klitzing's constant, for the first and second cases respectively.","sentences":["Two systems are studied: the first one involves a charged particle under the influence of a constant electric field, and the second one involves a charged particle under the influence of a constant electromagnetic field.","For both systems, it is possible to find time-dependent conserved operators that can be used to derive time-dependent solutions to the complete Schr\\\"odinger equation.","These conserved operators are employed to define the symmetries of the system.","An argument of invariance of the wave function under the action of a unitary operator leads to the quantization of resistance and resistivity, in integer multiples of the von Klitzing's constant, for the first and second cases respectively."],"url":"http://arxiv.org/abs/2404.05115v1","category":"quant-ph"}
{"created":"2024-04-07 22:06:19","title":"How much reliable is ChatGPT's prediction on Information Extraction under Input Perturbations?","abstract":"In this paper, we assess the robustness (reliability) of ChatGPT under input perturbations for one of the most fundamental tasks of Information Extraction (IE) i.e. Named Entity Recognition (NER). Despite the hype, the majority of the researchers have vouched for its language understanding and generation capabilities; a little attention has been paid to understand its robustness: How the input-perturbations affect 1) the predictions, 2) the confidence of predictions and 3) the quality of rationale behind its prediction. We perform a systematic analysis of ChatGPT's robustness (under both zero-shot and few-shot setup) on two NER datasets using both automatic and human evaluation. Based on automatic evaluation metrics, we find that 1) ChatGPT is more brittle on Drug or Disease replacements (rare entities) compared to the perturbations on widely known Person or Location entities, 2) the quality of explanations for the same entity considerably differ under different types of \"Entity-Specific\" and \"Context-Specific\" perturbations and the quality can be significantly improved using in-context learning, and 3) it is overconfident for majority of the incorrect predictions, and hence it could lead to misguidance of the end-users.","sentences":["In this paper, we assess the robustness (reliability) of ChatGPT under input perturbations for one of the most fundamental tasks of Information Extraction (IE) i.e. Named Entity Recognition (NER).","Despite the hype, the majority of the researchers have vouched for its language understanding and generation capabilities; a little attention has been paid to understand its robustness: How the input-perturbations affect 1) the predictions, 2) the confidence of predictions and 3) the quality of rationale behind its prediction.","We perform a systematic analysis of ChatGPT's robustness (under both zero-shot and few-shot setup) on two NER datasets using both automatic and human evaluation.","Based on automatic evaluation metrics, we find that 1) ChatGPT is more brittle on Drug or Disease replacements (rare entities) compared to the perturbations on widely known Person or Location entities, 2) the quality of explanations for the same entity considerably differ under different types of \"Entity-Specific\" and \"Context-Specific\" perturbations and the quality can be significantly improved using in-context learning, and 3) it is overconfident for majority of the incorrect predictions, and hence it could lead to misguidance of the end-users."],"url":"http://arxiv.org/abs/2404.05088v1","category":"cs.CL"}
{"created":"2024-04-07 22:02:53","title":"PCBot: a Minimalist Robot Designed for Swarm Applications","abstract":"Complexity, cost, and power requirements for the actuation of individual robots can play a large factor in limiting the size of robotic swarms. Here we present PCBot, a minimalist robot that can precisely move on an orbital shake table using a bi-stable solenoid actuator built directly into its PCB. This allows the actuator to be built as part of the automated PCB manufacturing process, greatly reducing the impact it has on manual assembly. Thanks to this novel actuator design, PCBot has merely five major components and can be assembled in under 20 seconds, potentially enabling them to be easily mass-manufactured. Here we present the electro-magnetic and mechanical design of PCBot. Additionally, a prototype robot is used to demonstrate its ability to move in a straight line as well as follow given paths.","sentences":["Complexity, cost, and power requirements for the actuation of individual robots can play a large factor in limiting the size of robotic swarms.","Here we present PCBot, a minimalist robot that can precisely move on an orbital shake table using a bi-stable solenoid actuator built directly into its PCB.","This allows the actuator to be built as part of the automated PCB manufacturing process, greatly reducing the impact it has on manual assembly.","Thanks to this novel actuator design, PCBot has merely five major components and can be assembled in under 20 seconds, potentially enabling them to be easily mass-manufactured.","Here we present the electro-magnetic and mechanical design of PCBot.","Additionally, a prototype robot is used to demonstrate its ability to move in a straight line as well as follow given paths."],"url":"http://arxiv.org/abs/2404.05087v1","category":"cs.RO"}
{"created":"2024-04-07 21:52:35","title":"Robustness of Quantum Random Walk Search with multi-phase matching","abstract":"In our previous works, we have studied quantum random walk search algorithm on hypercube, with traversing coin constructed by using generalized Householder reflection and a phase multiplier. When the same phases are used each iteration, the algorithm is robust (stable against errors in the phases) if a certain connection between the phases in the traversing coin is preserved, otherwise small errors lead to poor algorithm performance. Here we investigate how the robustness changes if different phases are used, depending on the current iteration number. We numerically study six different examples with different phase sequences. We show that usage of a particular sequence of phases can make the algorithm more robust even if there is no preserved connection between the phases in the traversing coin.","sentences":["In our previous works, we have studied quantum random walk search algorithm on hypercube, with traversing coin constructed by using generalized Householder reflection and a phase multiplier.","When the same phases are used each iteration, the algorithm is robust (stable against errors in the phases) if a certain connection between the phases in the traversing coin is preserved, otherwise small errors lead to poor algorithm performance.","Here we investigate how the robustness changes if different phases are used, depending on the current iteration number.","We numerically study six different examples with different phase sequences.","We show that usage of a particular sequence of phases can make the algorithm more robust even if there is no preserved connection between the phases in the traversing coin."],"url":"http://arxiv.org/abs/2404.05084v1","category":"quant-ph"}
{"created":"2024-04-07 21:43:23","title":"Ratios conjecture of quadratic Hecke $L$-functions of prime-related moduli","abstract":"Using the method of multiple Dirichlet series, we develop L-functions ratios conjecture with one shift in both the numerator and denominator in certain ranges for quadratic families of Dirichlet and Hecke L-functions of primerelated moduli of imaginary quadratic number fields of class number one under the generalized Riemann hypothesis. As corollaries, we evaluate asymptotically the first moment of central values as well as the one-level density of the families of L-functions under consideration.","sentences":["Using the method of multiple Dirichlet series, we develop L-functions ratios conjecture with one shift in both the numerator and denominator in certain ranges for quadratic families of Dirichlet and Hecke L-functions of primerelated moduli of imaginary quadratic number fields of class number one under the generalized Riemann hypothesis.","As corollaries, we evaluate asymptotically the first moment of central values as well as the one-level density of the families of L-functions under consideration."],"url":"http://arxiv.org/abs/2404.05081v1","category":"math.NT"}
{"created":"2024-04-07 21:16:15","title":"Entropy Engineered Middle-In Synthesis of Dual Single-Atom Compounds for Nitrate Reduction Reaction","abstract":"Despite the immense potential of Dual Single-Atom Compounds (DSACs), the challenges in their synthesis process, including complexity, stability, purity, and scalability, remain primary concerns in current research. Here, we present a general strategy, termed \"Entropy-Engineered Middle-In Synthesis of Dual Single-Atom Compounds\" (EEMIS-DSAC), which is meticulously crafted to produce a diverse range of DSACs, effectively addressing the aforementioned issues. Our strategy integrates the advantages of both bottom-up and top-down paradigms, proposing a new insight to optimize the catalyst structure. The as-fabricated DSACs exhibited excellent activity and stability in the nitrate reduction reaction (NO3RR). In a significant advancement, our prototypical CuNi DSACs demonstrated outstanding performance under conditions reminiscent of industrial wastewater. Specifically, under a NO3- concentration of 2000 ppm, it yielded a Faradaic efficiency (FE) for NH3 of 96.97 %, coupled with a mass productivity of 131.47 mg h-1 mg-1 and an area productivity of 10.06 mg h-1 cm-2. Impressively, even under a heightened NO3- concentration of 0.5 M, the FE for NH3 peaked at 90.61 %, with mass productivity reaching 1024.50 mg h-1 mg-1 and an area productivity of 78.41 mg h-1 cm-2. This work underpins the potential of the EEMIS-DSAC approach, signaling a promising frontier for high-performing DSACs.","sentences":["Despite the immense potential of Dual Single-Atom Compounds (DSACs), the challenges in their synthesis process, including complexity, stability, purity, and scalability, remain primary concerns in current research.","Here, we present a general strategy, termed \"Entropy-Engineered Middle-In Synthesis of Dual Single-Atom Compounds\" (EEMIS-DSAC), which is meticulously crafted to produce a diverse range of DSACs, effectively addressing the aforementioned issues.","Our strategy integrates the advantages of both bottom-up and top-down paradigms, proposing a new insight to optimize the catalyst structure.","The as-fabricated DSACs exhibited excellent activity and stability in the nitrate reduction reaction (NO3RR).","In a significant advancement, our prototypical CuNi DSACs demonstrated outstanding performance under conditions reminiscent of industrial wastewater.","Specifically, under a NO3- concentration of 2000 ppm, it yielded a Faradaic efficiency (FE) for NH3 of 96.97 %, coupled with a mass productivity of 131.47 mg h-1 mg-1 and an area productivity of 10.06 mg h-1 cm-2.","Impressively, even under a heightened NO3- concentration of 0.5 M, the FE for NH3 peaked at 90.61 %, with mass productivity reaching 1024.50 mg h-1 mg-1 and an area productivity of 78.41 mg h-1 cm-2.","This work underpins the potential of the EEMIS-DSAC approach, signaling a promising frontier for high-performing DSACs."],"url":"http://arxiv.org/abs/2404.05075v1","category":"physics.app-ph"}
{"created":"2024-04-07 21:02:55","title":"QRscript: Embedding a Programming Language in QR codes to support Decision and Management","abstract":"Embedding a programming language in a QR code is a new and extremely promising opportunity, as it makes devices and objects smarter without necessarily requiring an Internet connection. In this paper, all the steps needed to translate a program written in a high-level programming language to its binary representation encoded in a QR code, and the opposite process that, starting from the QR code, executes it by means of a virtual machine, have been carefully detailed. The proposed programming language was named QRscript, and can be easily extended so as to integrate new features. One of the main design goals was to produce a very compact target binary code. In particular, in this work we propose a specific sub-language (a dialect) that is aimed at encoding decision trees. Besides industrial scenarios, this is useful in many other application fields. The reported example, related to the configuration of an industrial networked device, highlights the potential of the proposed technology, and permits to better understand all the translation steps.","sentences":["Embedding a programming language in a QR code is a new and extremely promising opportunity, as it makes devices and objects smarter without necessarily requiring an Internet connection.","In this paper, all the steps needed to translate a program written in a high-level programming language to its binary representation encoded in a QR code, and the opposite process that, starting from the QR code, executes it by means of a virtual machine, have been carefully detailed.","The proposed programming language was named QRscript, and can be easily extended so as to integrate new features.","One of the main design goals was to produce a very compact target binary code.","In particular, in this work we propose a specific sub-language (a dialect) that is aimed at encoding decision trees.","Besides industrial scenarios, this is useful in many other application fields.","The reported example, related to the configuration of an industrial networked device, highlights the potential of the proposed technology, and permits to better understand all the translation steps."],"url":"http://arxiv.org/abs/2404.05073v1","category":"cs.NI"}
{"created":"2024-04-07 20:49:26","title":"STAIC regularization for spatio-temporal image reconstruction","abstract":"We propose a regularization-based image restoration scheme for 2D images recorded over time (2D+t). We design an infimal convolution-based regularization function which we call spatio-temporal Adaptive Infimal Convolution (STAIC) regularization. We formulate the infimal convolution in the form of an additive decomposition of the 2D+t image such that the extent of spatial and temporal smoothing is controlled in a spatially and temporally varying manner. This makes the regularization adaptable to the local characteristics of the motion leading to an improved ability to handle noise. We also develop a minimization method for image reconstruction by using the proposed form of regularization. We demonstrate the effectiveness of the proposed regularization using TIRF images recorded over time and compare with some selected existing regularizations.","sentences":["We propose a regularization-based image restoration scheme for 2D images recorded over time (2D+t).","We design an infimal convolution-based regularization function which we call spatio-temporal Adaptive Infimal Convolution (STAIC) regularization.","We formulate the infimal convolution in the form of an additive decomposition of the 2D+t image such that the extent of spatial and temporal smoothing is controlled in a spatially and temporally varying manner.","This makes the regularization adaptable to the local characteristics of the motion leading to an improved ability to handle noise.","We also develop a minimization method for image reconstruction by using the proposed form of regularization.","We demonstrate the effectiveness of the proposed regularization using TIRF images recorded over time and compare with some selected existing regularizations."],"url":"http://arxiv.org/abs/2404.05070v1","category":"eess.IV"}
{"created":"2024-04-07 20:39:31","title":"AirShot: Efficient Few-Shot Detection for Autonomous Exploration","abstract":"Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable features and exhaustive class loops. In this work, we propose a new paradigm AirShot, and discover that, by fully exploiting the valuable correlation map, AirShot can result in a more robust and faster few-shot object detection system, which is more applicable to robotics community. The core module Top Prediction Filter (TPF) can operate on multi-scale correlation maps in both the training and inference stages. During training, TPF supervises the generation of a more representative correlation map, while during inference, it reduces looping iterations by selecting top-ranked classes, thus cutting down on computational costs with better performance. Surprisingly, this dual functionality exhibits general effectiveness and efficiency on various off-the-shelf models. Exhaustive experiments on COCO2017, VOC2014, and SubT datasets demonstrate that TPF can significantly boost the efficacy and efficiency of most off-the-shelf models, achieving up to 36.4% precision improvements along with 56.3% faster inference speed. Code and Data are at: https://github.com/ImNotPrepared/AirShot.","sentences":["Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples.","Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration.","Existing methods still face performance and efficiency challenges, mainly due to unreliable features and exhaustive class loops.","In this work, we propose a new paradigm AirShot, and discover that, by fully exploiting the valuable correlation map, AirShot can result in a more robust and faster few-shot object detection system, which is more applicable to robotics community.","The core module Top Prediction Filter (TPF) can operate on multi-scale correlation maps in both the training and inference stages.","During training, TPF supervises the generation of a more representative correlation map, while during inference, it reduces looping iterations by selecting top-ranked classes, thus cutting down on computational costs with better performance.","Surprisingly, this dual functionality exhibits general effectiveness and efficiency on various off-the-shelf models.","Exhaustive experiments on COCO2017, VOC2014, and SubT datasets demonstrate that TPF can significantly boost the efficacy and efficiency of most off-the-shelf models, achieving up to 36.4% precision improvements along with 56.3% faster inference speed.","Code and Data are at: https://github.com/ImNotPrepared/AirShot."],"url":"http://arxiv.org/abs/2404.05069v1","category":"cs.CV"}
{"created":"2024-04-07 20:38:39","title":"Data Conditioning for Subsurface Models with Single-Image Generative Adversarial Network (SinGAN)","abstract":"The characterization of subsurface models relies on the accuracy of subsurface models which request integrating a large number of information across different sources through model conditioning, such as data conditioning and geological concepts conditioning. Conventional geostatistical models have a trade-off between honoring geological conditioning (i.e., qualitative geological concepts) and data conditioning (i.e., quantitative static data and dynamic data). To resolve this limit, generative AI methods, such as Generative adversarial network (GAN), have been widely applied for subsurface modeling due to their ability to reproduce complex geological patterns. However, the current practices of data conditioning in GANs conduct quality assessment through ocular inspection to check model plausibility or some preliminary quantitative analysis of the distribution of property of interests. We propose the generative AI realization minimum acceptance criteria for data conditioning, demonstrated with single image GAN. Our conditioning checks include static small-scale local and large-scale exhaustive data conditioning checks, local uncertainty, and spatial nonstationarity reproduction checks. We also check conditioning to geological concepts through multiscale spatial distribution, the number of connected geobodies, the spatial continuity check, and the model facies proportion reproduction check. Our proposed workflow provides guidance on the conditioning of deep learning methods for subsurface modeling and enhanced model conditioning checking essential for applying these models to support uncertainty characterization and decision making.","sentences":["The characterization of subsurface models relies on the accuracy of subsurface models which request integrating a large number of information across different sources through model conditioning, such as data conditioning and geological concepts conditioning.","Conventional geostatistical models have a trade-off between honoring geological conditioning (i.e., qualitative geological concepts) and data conditioning (i.e., quantitative static data and dynamic data).","To resolve this limit, generative AI methods, such as Generative adversarial network (GAN), have been widely applied for subsurface modeling due to their ability to reproduce complex geological patterns.","However, the current practices of data conditioning in GANs conduct quality assessment through ocular inspection to check model plausibility or some preliminary quantitative analysis of the distribution of property of interests.","We propose the generative AI realization minimum acceptance criteria for data conditioning, demonstrated with single image GAN.","Our conditioning checks include static small-scale local and large-scale exhaustive data conditioning checks, local uncertainty, and spatial nonstationarity reproduction checks.","We also check conditioning to geological concepts through multiscale spatial distribution, the number of connected geobodies, the spatial continuity check, and the model facies proportion reproduction check.","Our proposed workflow provides guidance on the conditioning of deep learning methods for subsurface modeling and enhanced model conditioning checking essential for applying these models to support uncertainty characterization and decision making."],"url":"http://arxiv.org/abs/2404.05068v1","category":"stat.AP"}
{"created":"2024-04-07 20:24:44","title":"A Structure-Guided Gauss-Newton Method for Shallow ReLU Neural Network","abstract":"In this paper, we propose a structure-guided Gauss-Newton (SgGN) method for solving least squares problems using a shallow ReLU neural network. The method effectively takes advantage of both the least squares structure and the neural network structure of the objective function. By categorizing the weights and biases of the hidden and output layers of the network as nonlinear and linear parameters, respectively, the method iterates back and forth between the nonlinear and linear parameters. The nonlinear parameters are updated by a damped Gauss-Newton method and the linear ones are updated by a linear solver. Moreover, at the Gauss-Newton step, a special form of the Gauss-Newton matrix is derived for the shallow ReLU neural network and is used for efficient iterations. It is shown that the corresponding mass and Gauss-Newton matrices in the respective linear and nonlinear steps are symmetric and positive definite under reasonable assumptions. Thus, the SgGN method naturally produces an effective search direction without the need of additional techniques like shifting in the Levenberg-Marquardt method to achieve invertibility of the Gauss-Newton matrix. The convergence and accuracy of the method are demonstrated numerically for several challenging function approximation problems, especially those with discontinuities or sharp transition layers that pose significant challenges for commonly used training algorithms in machine learning.","sentences":["In this paper, we propose a structure-guided Gauss-Newton (SgGN) method for solving least squares problems using a shallow ReLU neural network.","The method effectively takes advantage of both the least squares structure and the neural network structure of the objective function.","By categorizing the weights and biases of the hidden and output layers of the network as nonlinear and linear parameters, respectively, the method iterates back and forth between the nonlinear and linear parameters.","The nonlinear parameters are updated by a damped Gauss-Newton method and the linear ones are updated by a linear solver.","Moreover, at the Gauss-Newton step, a special form of the Gauss-Newton matrix is derived for the shallow ReLU neural network and is used for efficient iterations.","It is shown that the corresponding mass and Gauss-Newton matrices in the respective linear and nonlinear steps are symmetric and positive definite under reasonable assumptions.","Thus, the SgGN method naturally produces an effective search direction without the need of additional techniques like shifting in the Levenberg-Marquardt method to achieve invertibility of the Gauss-Newton matrix.","The convergence and accuracy of the method are demonstrated numerically for several challenging function approximation problems, especially those with discontinuities or sharp transition layers that pose significant challenges for commonly used training algorithms in machine learning."],"url":"http://arxiv.org/abs/2404.05064v1","category":"cs.LG"}
{"created":"2024-04-07 20:12:32","title":"Dir-SPGLM: A Bayesian semiparametric GLM with data-driven reference distribution","abstract":"The recently developed semi-parametric generalized linear model (SPGLM) offers more flexibility as compared to the classical GLM by including the baseline or reference distribution of the response as an additional parameter in the model. However, some inference summaries are not easily generated under existing maximum-likelihood based inference (ML-SPGLM). This includes uncertainty in estimation for model-derived functionals such as exceedance probabilities. The latter are critical in a clinical diagnostic or decision-making setting. In this article, by placing a Dirichlet prior on the baseline distribution, we propose a Bayesian model-based approach for inference to address these important gaps. We establish consistency and asymptotic normality results for the implied canonical parameter. Simulation studies and an illustration with data from an aging research study confirm that the proposed method performs comparably or better in comparison with ML-SPGLM. The proposed Bayesian framework is most attractive for inference with small sample training data or in sparse-data scenarios.","sentences":["The recently developed semi-parametric generalized linear model (SPGLM) offers more flexibility as compared to the classical GLM by including the baseline or reference distribution of the response as an additional parameter in the model.","However, some inference summaries are not easily generated under existing maximum-likelihood based inference (ML-SPGLM).","This includes uncertainty in estimation for model-derived functionals such as exceedance probabilities.","The latter are critical in a clinical diagnostic or decision-making setting.","In this article, by placing a Dirichlet prior on the baseline distribution, we propose a Bayesian model-based approach for inference to address these important gaps.","We establish consistency and asymptotic normality results for the implied canonical parameter.","Simulation studies and an illustration with data from an aging research study confirm that the proposed method performs comparably or better in comparison with ML-SPGLM.","The proposed Bayesian framework is most attractive for inference with small sample training data or in sparse-data scenarios."],"url":"http://arxiv.org/abs/2404.05060v1","category":"stat.ME"}
{"created":"2024-04-07 20:05:49","title":"A robust assessment for invariant representations","abstract":"The performance of machine learning models can be impacted by changes in data over time. A promising approach to address this challenge is invariant learning, with a particular focus on a method known as invariant risk minimization (IRM). This technique aims to identify a stable data representation that remains effective with out-of-distribution (OOD) data. While numerous studies have developed IRM-based methods adaptive to data augmentation scenarios, there has been limited attention on directly assessing how well these representations preserve their invariant performance under varying conditions. In our paper, we propose a novel method to evaluate invariant performance, specifically tailored for IRM-based methods. We establish a bridge between the conditional expectation of an invariant predictor across different environments through the likelihood ratio. Our proposed criterion offers a robust basis for evaluating invariant performance. We validate our approach with theoretical support and demonstrate its effectiveness through extensive numerical studies.These experiments illustrate how our method can assess the invariant performance of various representation techniques.","sentences":["The performance of machine learning models can be impacted by changes in data over time.","A promising approach to address this challenge is invariant learning, with a particular focus on a method known as invariant risk minimization (IRM).","This technique aims to identify a stable data representation that remains effective with out-of-distribution (OOD) data.","While numerous studies have developed IRM-based methods adaptive to data augmentation scenarios, there has been limited attention on directly assessing how well these representations preserve their invariant performance under varying conditions.","In our paper, we propose a novel method to evaluate invariant performance, specifically tailored for IRM-based methods.","We establish a bridge between the conditional expectation of an invariant predictor across different environments through the likelihood ratio.","Our proposed criterion offers a robust basis for evaluating invariant performance.","We validate our approach with theoretical support and demonstrate its effectiveness through extensive numerical studies.","These experiments illustrate how our method can assess the invariant performance of various representation techniques."],"url":"http://arxiv.org/abs/2404.05058v1","category":"cs.LG"}
{"created":"2024-04-07 19:39:14","title":"TimeCSL: Unsupervised Contrastive Learning of General Shapelets for Explorable Time Series Analysis","abstract":"Unsupervised (a.k.a. Self-supervised) representation learning (URL) has emerged as a new paradigm for time series analysis, because it has the ability to learn generalizable time series representation beneficial for many downstream tasks without using labels that are usually difficult to obtain. Considering that existing approaches have limitations in the design of the representation encoder and the learning objective, we have proposed Contrastive Shapelet Learning (CSL), the first URL method that learns the general-purpose shapelet-based representation through unsupervised contrastive learning, and shown its superior performance in several analysis tasks, such as time series classification, clustering, and anomaly detection. In this paper, we develop TimeCSL, an end-to-end system that makes full use of the general and interpretable shapelets learned by CSL to achieve explorable time series analysis in a unified pipeline. We introduce the system components and demonstrate how users interact with TimeCSL to solve different analysis tasks in the unified pipeline, and gain insight into their time series by exploring the learned shapelets and representation.","sentences":["Unsupervised (a.k.a. Self-supervised) representation learning (URL) has emerged as a new paradigm for time series analysis, because it has the ability to learn generalizable time series representation beneficial for many downstream tasks without using labels that are usually difficult to obtain.","Considering that existing approaches have limitations in the design of the representation encoder and the learning objective, we have proposed Contrastive Shapelet Learning (CSL), the first URL method that learns the general-purpose shapelet-based representation through unsupervised contrastive learning, and shown its superior performance in several analysis tasks, such as time series classification, clustering, and anomaly detection.","In this paper, we develop TimeCSL, an end-to-end system that makes full use of the general and interpretable shapelets learned by CSL to achieve explorable time series analysis in a unified pipeline.","We introduce the system components and demonstrate how users interact with TimeCSL to solve different analysis tasks in the unified pipeline, and gain insight into their time series by exploring the learned shapelets and representation."],"url":"http://arxiv.org/abs/2404.05057v1","category":"cs.LG"}
{"created":"2024-04-07 19:22:51","title":"Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint","abstract":"We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning. We draw inspiration from spectral decomposition of Markov decision processes. The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills. The skill representations are transferable across arbitrary tasks with the same transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data. We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets. We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking. Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%.","sentences":["We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning.","We draw inspiration from spectral decomposition of Markov decision processes.","The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills.","The skill representations are transferable across arbitrary tasks with the same transition dynamics.","Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data.","We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets.","We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors.","We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking.","Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%."],"url":"http://arxiv.org/abs/2404.05051v1","category":"cs.LG"}
{"created":"2024-04-07 18:59:03","title":"Clinical Trials Protocol Authoring using LLMs","abstract":"This report embarks on a mission to revolutionize clinical trial protocol development through the integration of advanced AI technologies. With a focus on leveraging the capabilities of generative AI, specifically GPT-4, this initiative aimed to streamline and enhance the efficiency and accuracy of clinical trial protocols. The methodology encompassed a detailed analysis and preparation of comprehensive drug and study level metadata, followed by the deployment of GPT-4 for automated protocol section generation. Results demonstrated a significant improvement in protocol authoring, highlighted by increases in efficiency, accuracy, and the customization of protocols to specific trial requirements. Challenges encountered during model selection and prompt engineering were systematically addressed, leading to refined methodologies that capitalized on the advanced text generation capabilities of GPT-4. This project not only showcases the practical applications and benefits of generative AI in clinical trial design but also sets a foundation for future innovations in the field.","sentences":["This report embarks on a mission to revolutionize clinical trial protocol development through the integration of advanced AI technologies.","With a focus on leveraging the capabilities of generative AI, specifically GPT-4, this initiative aimed to streamline and enhance the efficiency and accuracy of clinical trial protocols.","The methodology encompassed a detailed analysis and preparation of comprehensive drug and study level metadata, followed by the deployment of GPT-4 for automated protocol section generation.","Results demonstrated a significant improvement in protocol authoring, highlighted by increases in efficiency, accuracy, and the customization of protocols to specific trial requirements.","Challenges encountered during model selection and prompt engineering were systematically addressed, leading to refined methodologies that capitalized on the advanced text generation capabilities of GPT-4.","This project not only showcases the practical applications and benefits of generative AI in clinical trial design but also sets a foundation for future innovations in the field."],"url":"http://arxiv.org/abs/2404.05044v1","category":"cs.CE"}
{"created":"2024-04-07 18:21:55","title":"Pressure-dependent adhesion between solid-supported PC-lipid bilayers and vesicles under electric fields","abstract":"Fusion of lipid bilayers in membranes is important in processes from vesicle-cell interactions (as in drug delivery) to exosome-cell signaling, while transient transmembrane electric fields are known to occur spontaneously. Two contacting phosphatidylcholine (PC) lipid membranes are known to fuse into one under external electric fields, suggesting that the interaction between them is modified by the field as they approach, prior to the fusion event. Here we measure directly the adhesion energy between dimyristoylphosphatidylcholine (DMPC) and between distearoylphosphatidylcholine (DSPC) surface layers attached to solid substrates both without and with a transmembrane electric field. We find a marked pressure-dependent adhesion behavior in the electric field, which we attribute to fusion intermediates that are formed, shedding new light on membrane electro-fusion.","sentences":["Fusion of lipid bilayers in membranes is important in processes from vesicle-cell interactions (as in drug delivery) to exosome-cell signaling, while transient transmembrane electric fields are known to occur spontaneously.","Two contacting phosphatidylcholine (PC) lipid membranes are known to fuse into one under external electric fields, suggesting that the interaction between them is modified by the field as they approach, prior to the fusion event.","Here we measure directly the adhesion energy between dimyristoylphosphatidylcholine (DMPC) and between distearoylphosphatidylcholine (DSPC) surface layers attached to solid substrates both without and with a transmembrane electric field.","We find a marked pressure-dependent adhesion behavior in the electric field, which we attribute to fusion intermediates that are formed, shedding new light on membrane electro-fusion."],"url":"http://arxiv.org/abs/2404.05035v1","category":"cond-mat.soft"}
{"created":"2024-04-07 18:00:41","title":"Magic Boundaries of 3D Color Codes","abstract":"We investigate boundaries of 3D color codes and provide a systematic classification into 101 distinct boundary types. The elementary types of boundaries are codimension-1 (2D) boundaries that condense electric particle ($Z$-type) or magnetic flux ($X$-type) excitations in the 3D color code, including the $Z$-boundary condensing only electric particles, the $X$-boundary condensing only the magnetic flux, and other boundaries condensing both electric and magnetic excitations. Two novel types of boundaries can be generated based on certain elementary types. The first type is generated by applying transversal-$T$ gate on the entire code in the presence of the $X$-boundary, which effectively sweeps the codimension-1 (2D) $T$-domain wall across the system and attaches it to the $X$-boundary. Since the $T$-domain wall cannot condense on the $X$-boundary, a new \\textit{magic boundary} is produced, where the boundary stabilizers contain $XS$-stabilizers going beyond the conventional Pauli stabilizer formalism and hence contains `magic'. Neither electric nor magnetic excitations can condense on such a magic boundary, and only the composite of the magnetic flux and codimension-2 (1D) $S$-domain wall can condense on it, which makes the magic boundary going beyond the classification of the Lagrangian subgroup. The second type is generated by applying transversal-$S$ gate on a codimension-1 (2D) submanifold in the presence of certain codimension-1 (2D) boundaries, which effectively sweeps the $S$-domain wall across this submanifold and attaches it onto the boundary. This generates a codimension-2 (1D) \\textit{nested boundary} at the intersection. We also connect these novel boundaries to their previously discovered counterpart in the $\\mathbb{Z}_2^3$ gauge theory equivalent to three copies of 3D toric codes...","sentences":["We investigate boundaries of 3D color codes and provide a systematic classification into 101 distinct boundary types.","The elementary types of boundaries are codimension-1 (2D) boundaries that condense electric particle ($Z$-type) or magnetic flux ($X$-type) excitations in the 3D color code, including the $Z$-boundary condensing only electric particles, the $X$-boundary condensing only the magnetic flux, and other boundaries condensing both electric and magnetic excitations.","Two novel types of boundaries can be generated based on certain elementary types.","The first type is generated by applying transversal-$T$ gate on the entire code in the presence of the $X$-boundary, which effectively sweeps the codimension-1 (2D) $T$-domain wall across the system and attaches it to the $X$-boundary.","Since the $T$-domain wall cannot condense on the $X$-boundary, a new \\textit{magic boundary} is produced, where the boundary stabilizers contain $XS$-stabilizers going beyond the conventional Pauli stabilizer formalism and hence contains `magic'.","Neither electric nor magnetic excitations can condense on such a magic boundary, and only the composite of the magnetic flux and codimension-2 (1D) $S$-domain wall can condense on it, which makes the magic boundary going beyond the classification of the Lagrangian subgroup.","The second type is generated by applying transversal-$S$ gate on a codimension-1 (2D) submanifold in the presence of certain codimension-1 (2D) boundaries, which effectively sweeps the $S$-domain wall across this submanifold and attaches it onto the boundary.","This generates a codimension-2 (1D) \\textit{nested boundary} at the intersection.","We also connect these novel boundaries to their previously discovered counterpart in the $\\mathbb{Z}_2^3$ gauge theory equivalent to three copies of 3D toric codes..."],"url":"http://arxiv.org/abs/2404.05033v1","category":"quant-ph"}
{"created":"2024-04-07 17:31:53","title":"PathFinder: Attention-Driven Dynamic Non-Line-of-Sight Tracking with a Mobile Robot","abstract":"The study of non-line-of-sight (NLOS) imaging is growing due to its many potential applications, including rescue operations and pedestrian detection by self-driving cars. However, implementing NLOS imaging on a moving camera remains an open area of research. Existing NLOS imaging methods rely on time-resolved detectors and laser configurations that require precise optical alignment, making it difficult to deploy them in dynamic environments. This work proposes a data-driven approach to NLOS imaging, PathFinder, that can be used with a standard RGB camera mounted on a small, power-constrained mobile robot, such as an aerial drone. Our experimental pipeline is designed to accurately estimate the 2D trajectory of a person who moves in a Manhattan-world environment while remaining hidden from the camera's field-of-view. We introduce a novel approach to process a sequence of dynamic successive frames in a line-of-sight (LOS) video using an attention-based neural network that performs inference in real-time. The method also includes a preprocessing selection metric that analyzes images from a moving camera which contain multiple vertical planar surfaces, such as walls and building facades, and extracts planes that return maximum NLOS information. We validate the approach on in-the-wild scenes using a drone for video capture, thus demonstrating low-cost NLOS imaging in dynamic capture environments.","sentences":["The study of non-line-of-sight (NLOS) imaging is growing due to its many potential applications, including rescue operations and pedestrian detection by self-driving cars.","However, implementing NLOS imaging on a moving camera remains an open area of research.","Existing NLOS imaging methods rely on time-resolved detectors and laser configurations that require precise optical alignment, making it difficult to deploy them in dynamic environments.","This work proposes a data-driven approach to NLOS imaging, PathFinder, that can be used with a standard RGB camera mounted on a small, power-constrained mobile robot, such as an aerial drone.","Our experimental pipeline is designed to accurately estimate the 2D trajectory of a person who moves in a Manhattan-world environment while remaining hidden from the camera's field-of-view.","We introduce a novel approach to process a sequence of dynamic successive frames in a line-of-sight (LOS) video using an attention-based neural network that performs inference in real-time.","The method also includes a preprocessing selection metric that analyzes images from a moving camera which contain multiple vertical planar surfaces, such as walls and building facades, and extracts planes that return maximum NLOS information.","We validate the approach on in-the-wild scenes using a drone for video capture, thus demonstrating low-cost NLOS imaging in dynamic capture environments."],"url":"http://arxiv.org/abs/2404.05024v1","category":"cs.CV"}
{"created":"2024-04-07 16:32:37","title":"A Hybrid Execution Environment for Computer-Interpretable Guidelines in PROforma","abstract":"In this paper, we share our experience of developing a hybrid execution environment for computer-interpretable guidelines (CIGs) in PROforma. The proposed environment is part of the CAPABLE system which provides coaching for cancer patients and decision support for physicians. It extends a standard PROforma execution engine - Deontics Engine (DE) - with additional components that act as wrappers around DE, allow handling of non-standard tasks, and facilitate integration with the rest of the CAPABLE system. This yields a hybrid environment in which the standard engine and specialized components must be interfaced together by some intervening layer. In the CAPABLE system this has been achieved by defining a set of specialized meta-properties which are attached to data and tasks in the PROforma CIGs to specify the interface between engine and components.","sentences":["In this paper, we share our experience of developing a hybrid execution environment for computer-interpretable guidelines (CIGs) in PROforma.","The proposed environment is part of the CAPABLE system which provides coaching for cancer patients and decision support for physicians.","It extends a standard PROforma execution engine - Deontics Engine (DE) - with additional components that act as wrappers around DE, allow handling of non-standard tasks, and facilitate integration with the rest of the CAPABLE system.","This yields a hybrid environment in which the standard engine and specialized components must be interfaced together by some intervening layer.","In the CAPABLE system this has been achieved by defining a set of specialized meta-properties which are attached to data and tasks in the PROforma CIGs to specify the interface between engine and components."],"url":"http://arxiv.org/abs/2404.05011v1","category":"cs.SE"}
{"created":"2024-04-07 16:16:44","title":"Minimax Least-Square Policy Iteration for Cost-Aware Defense of Traffic Routing against Unknown Threats","abstract":"Dynamic routing is one of the representative control scheme in transportation, production lines, and data transmission. In the modern context of connectivity and autonomy, routing decisions are potentially vulnerable to malicious attacks. In this paper, we consider the dynamic routing problem over parallel traffic links in the face of such threats. An attacker is capable of increasing or destabilizing traffic queues by strategic manipulating the nominally optimal routing decisions. A defender is capable of securing the correct routing decision. Attacking and defensive actions induce technological costs. The defender has no prior information about the attacker's strategy. We develop an least-square policy iteration algorithm for the defender to compute a cost-aware and threat-adaptive defensive strategy. The policy evaluation step computes a weight vector that minimizes the sampled temporal-difference error. We derive a concrete theoretical upper bound on the evaluation error based on the theory of value function approximation. The policy improvement step solves a minimax problem and thus iteratively computes the Markov perfect equilibrium of the security game. We also discuss the training error of the entire policy iteration process.","sentences":["Dynamic routing is one of the representative control scheme in transportation, production lines, and data transmission.","In the modern context of connectivity and autonomy, routing decisions are potentially vulnerable to malicious attacks.","In this paper, we consider the dynamic routing problem over parallel traffic links in the face of such threats.","An attacker is capable of increasing or destabilizing traffic queues by strategic manipulating the nominally optimal routing decisions.","A defender is capable of securing the correct routing decision.","Attacking and defensive actions induce technological costs.","The defender has no prior information about the attacker's strategy.","We develop an least-square policy iteration algorithm for the defender to compute a cost-aware and threat-adaptive defensive strategy.","The policy evaluation step computes a weight vector that minimizes the sampled temporal-difference error.","We derive a concrete theoretical upper bound on the evaluation error based on the theory of value function approximation.","The policy improvement step solves a minimax problem and thus iteratively computes the Markov perfect equilibrium of the security game.","We also discuss the training error of the entire policy iteration process."],"url":"http://arxiv.org/abs/2404.05008v1","category":"eess.SY"}
{"created":"2024-04-07 15:10:25","title":"Chromatic number in $1.9999^n$ time? Fast deterministic set partitioning under the asymptotic rank conjecture","abstract":"In this paper we further explore the recently discovered connection by Bj\\\"{o}rklund and Kaski [STOC 2024] and Pratt [STOC 2024] between the asymptotic rank conjecture of Strassen [Progr. Math. 1994] and the three-way partitioning problem. We show that under the asymptotic rank conjecture, the chromatic number of an $n$-vertex graph can be computed deterministically in $O(1.99982^n)$ time, thus giving a conditional answer to a question of Zamir [ICALP 2021], and questioning the optimality of the $2^n\\operatorname{poly}(n)$ time algorithm for chromatic number by Bj\\\"{o}rklund, Husfeldt, and Koivisto [SICOMP 2009].   Our technique is a combination of earlier algorithms for detecting $k$-colorings for small $k$ and enumerating $k$-colorable subgraphs, with an extension and derandomisation of Pratt's tensor-based algorithm for balanced three-way partitioning to the unbalanced case.","sentences":["In this paper we further explore the recently discovered connection by Bj\\\"{o}rklund and Kaski [STOC 2024] and Pratt","[STOC 2024] between the asymptotic rank conjecture of Strassen [Progr.","Math. 1994] and the three-way partitioning problem.","We show that under the asymptotic rank conjecture, the chromatic number of an $n$-vertex graph can be computed deterministically in $O(1.99982^n)$ time, thus giving a conditional answer to a question of Zamir [ICALP 2021], and questioning the optimality of the $2^n\\operatorname{poly}(n)$ time algorithm for chromatic number by Bj\\\"{o}rklund, Husfeldt, and","Koivisto [SICOMP 2009].   ","Our technique is a combination of earlier algorithms for detecting $k$-colorings for small $k$ and enumerating $k$-colorable subgraphs, with an extension and derandomisation of Pratt's tensor-based algorithm for balanced three-way partitioning to the unbalanced case."],"url":"http://arxiv.org/abs/2404.04987v1","category":"cs.DS"}
{"created":"2024-04-07 15:02:21","title":"Quasinormal Modes of Analog Rotating Black Holes in 2-Dimensional Photon-Fluid Model","abstract":"It was recently found that the optical field fluctuations in self-defocusing media can be described by sound waves propagating in a two-dimensional photon-fluid. This photon-fluid is controlled by the driving beam and serves as the background in which the sound waves can experience an effective curved spacetime, such that it provides a new platform of studying analog black holes. In this paper, we are interested in investigating the quasinormal modes (QNMs) of this analog black hole in the photon-fluid model. Based on the master equation of motion of the optical field fluctuations, we calculate the frequencies of quasinormal modes (QNF) with three different numerical methods to make sure the QNF we get are reliable. Besides fundamental modes, we also try to calculate the overtones up to $n=3$ aiming to uncover more properties of QNF. The effects of angular velocity $\\Omega_H$ of the black hole, the overtone number $n$ and the winding number $m$ on the QNF are investigated. Under the $m$ with opposite sign, we find that both the real and imaginary part of the QNF will show strikingly contrasting behaviors when the QNF is plotted against $\\Omega_H$, and the similar contrast effects are also found when comparing the influences from winding number and overtone number. We hope that this work may potentially contribute to the future detections of QNMs in experimental settings of photon-fluid.","sentences":["It was recently found that the optical field fluctuations in self-defocusing media can be described by sound waves propagating in a two-dimensional photon-fluid.","This photon-fluid is controlled by the driving beam and serves as the background in which the sound waves can experience an effective curved spacetime, such that it provides a new platform of studying analog black holes.","In this paper, we are interested in investigating the quasinormal modes (QNMs) of this analog black hole in the photon-fluid model.","Based on the master equation of motion of the optical field fluctuations, we calculate the frequencies of quasinormal modes (QNF) with three different numerical methods to make sure the QNF we get are reliable.","Besides fundamental modes, we also try to calculate the overtones up to $n=3$ aiming to uncover more properties of QNF.","The effects of angular velocity $\\Omega_H$ of the black hole, the overtone number $n$ and the winding number $m$ on the QNF are investigated.","Under the $m$ with opposite sign, we find that both the real and imaginary part of the QNF will show strikingly contrasting behaviors when the QNF is plotted against $\\Omega_H$, and the similar contrast effects are also found when comparing the influences from winding number and overtone number.","We hope that this work may potentially contribute to the future detections of QNMs in experimental settings of photon-fluid."],"url":"http://arxiv.org/abs/2404.04982v1","category":"gr-qc"}
{"created":"2024-04-07 14:52:32","title":"On Completely multiplicative $\\pm1$ sequences that omit many consecutive $+1$ values","abstract":"We investigate the construction of $\\pm1$-valued completely multiplicative functions that take the value $+1$ at at most $k$ consecutive integers, which we call length-$k$ functions. We introduce a way to extend the length based on the idea of the \"rotation trick\" and such an extension can be quantified by the number of modified primes. Under the assumption of Elliott's conjecture, this method allows us to construct length-$k$ functions systematically for $k\\geq 4$ which generalizes the work of I. Schur for $k = 2$ and R. Hudson for $k =3$.","sentences":["We investigate the construction of $\\pm1$-valued completely multiplicative functions that take the value $+1$ at at most $k$ consecutive integers, which we call length-$k$ functions.","We introduce a way to extend the length based on the idea of the \"rotation trick\" and such an extension can be quantified by the number of modified primes.","Under the assumption of Elliott's conjecture, this method allows us to construct length-$k$ functions systematically for $k\\geq 4$ which generalizes the work of I. Schur for $k = 2$ and R. Hudson for $k =3$."],"url":"http://arxiv.org/abs/2404.04981v1","category":"math.NT"}
{"created":"2024-04-07 14:21:37","title":"FPL+: Filtered Pseudo Label-based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation","abstract":"Adapting a medical image segmentation model to a new domain is important for improving its cross-domain transferability, and due to the expensive annotation process, Unsupervised Domain Adaptation (UDA) is appealing where only unlabeled images are needed for the adaptation. Existing UDA methods are mainly based on image or feature alignment with adversarial training for regularization, and they are limited by insufficient supervision in the target domain. In this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation. It first uses cross-domain data augmentation to translate labeled images in the source domain to a dual-domain training set consisting of a pseudo source-domain set and a pseudo target-domain set. To leverage the dual-domain augmented images to train a pseudo label generator, domain-specific batch normalization layers are used to deal with the domain shift while learning the domain-invariant structure features, generating high-quality pseudo labels for target-domain images. We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels. Experiments on three public multi-modal datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully supervised learning in the target domain in some cases.","sentences":["Adapting a medical image segmentation model to a new domain is important for improving its cross-domain transferability, and due to the expensive annotation process, Unsupervised Domain Adaptation (UDA) is appealing where only unlabeled images are needed for the adaptation.","Existing UDA methods are mainly based on image or feature alignment with adversarial training for regularization, and they are limited by insufficient supervision in the target domain.","In this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation.","It first uses cross-domain data augmentation to translate labeled images in the source domain to a dual-domain training set consisting of a pseudo source-domain set and a pseudo target-domain set.","To leverage the dual-domain augmented images to train a pseudo label generator, domain-specific batch normalization layers are used to deal with the domain shift while learning the domain-invariant structure features, generating high-quality pseudo labels for target-domain images.","We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels.","Experiments on three public multi-modal datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully supervised learning in the target domain in some cases."],"url":"http://arxiv.org/abs/2404.04971v1","category":"cs.CV"}
{"created":"2024-04-07 14:20:51","title":"How to characterize imprecision in multi-view clustering?","abstract":"It is still challenging to cluster multi-view data since existing methods can only assign an object to a specific (singleton) cluster when combining different view information. As a result, it fails to characterize imprecision of objects in overlapping regions of different clusters, thus leading to a high risk of errors. In this paper, we thereby want to answer the question: how to characterize imprecision in multi-view clustering? Correspondingly, we propose a multi-view low-rank evidential c-means based on entropy constraint (MvLRECM). The proposed MvLRECM can be considered as a multi-view version of evidential c-means based on the theory of belief functions. In MvLRECM, each object is allowed to belong to different clusters with various degrees of support (masses of belief) to characterize uncertainty when decision-making. Moreover, if an object is in the overlapping region of several singleton clusters, it can be assigned to a meta-cluster, defined as the union of these singleton clusters, to characterize the local imprecision in the result. In addition, entropy-weighting and low-rank constraints are employed to reduce imprecision and improve accuracy. Compared to state-of-the-art methods, the effectiveness of MvLRECM is demonstrated based on several toy and UCI real datasets.","sentences":["It is still challenging to cluster multi-view data since existing methods can only assign an object to a specific (singleton) cluster when combining different view information.","As a result, it fails to characterize imprecision of objects in overlapping regions of different clusters, thus leading to a high risk of errors.","In this paper, we thereby want to answer the question: how to characterize imprecision in multi-view clustering?","Correspondingly, we propose a multi-view low-rank evidential c-means based on entropy constraint (MvLRECM).","The proposed MvLRECM can be considered as a multi-view version of evidential c-means based on the theory of belief functions.","In MvLRECM, each object is allowed to belong to different clusters with various degrees of support (masses of belief) to characterize uncertainty when decision-making.","Moreover, if an object is in the overlapping region of several singleton clusters, it can be assigned to a meta-cluster, defined as the union of these singleton clusters, to characterize the local imprecision in the result.","In addition, entropy-weighting and low-rank constraints are employed to reduce imprecision and improve accuracy.","Compared to state-of-the-art methods, the effectiveness of MvLRECM is demonstrated based on several toy and UCI real datasets."],"url":"http://arxiv.org/abs/2404.04970v1","category":"cs.LG"}
{"created":"2024-04-07 14:08:28","title":"Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis","abstract":"Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage.","sentences":["Automatic test generation plays a critical role in software quality assurance.","While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches.","Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques.","In this work, we propose TELPA, a novel technique aimed at addressing these challenges.","Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints.","To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples.","Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches.","Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage."],"url":"http://arxiv.org/abs/2404.04966v1","category":"cs.SE"}
{"created":"2024-04-07 13:37:30","title":"A Two Dimensional Feature Engineering Method for Relation Extraction","abstract":"Transforming a sentence into a two-dimensional (2D) representation (e.g., the table filling) has the ability to unfold a semantic plane, where an element of the plane is a word-pair representation of a sentence which may denote a possible relation representation composed of two named entities. The 2D representation is effective in resolving overlapped relation instances. However, in related works, the representation is directly transformed from a raw input. It is weak to utilize prior knowledge, which is important to support the relation extraction task. In this paper, we propose a two-dimensional feature engineering method in the 2D sentence representation for relation extraction. Our proposed method is evaluated on three public datasets (ACE05 Chinese, ACE05 English, and SanWen) and achieves the state-of-the-art performance. The results indicate that two-dimensional feature engineering can take advantage of a two-dimensional sentence representation and make full use of prior knowledge in traditional feature engineering. Our code is publicly available at https://github.com/Wang-ck123/A-Two-Dimensional-Feature-Engineering-Method-for-Entity-Relation-Extraction","sentences":["Transforming a sentence into a two-dimensional (2D) representation (e.g., the table filling) has the ability to unfold a semantic plane, where an element of the plane is a word-pair representation of a sentence which may denote a possible relation representation composed of two named entities.","The 2D representation is effective in resolving overlapped relation instances.","However, in related works, the representation is directly transformed from a raw input.","It is weak to utilize prior knowledge, which is important to support the relation extraction task.","In this paper, we propose a two-dimensional feature engineering method in the 2D sentence representation for relation extraction.","Our proposed method is evaluated on three public datasets (ACE05 Chinese, ACE05 English, and SanWen) and achieves the state-of-the-art performance.","The results indicate that two-dimensional feature engineering can take advantage of a two-dimensional sentence representation and make full use of prior knowledge in traditional feature engineering.","Our code is publicly available at https://github.com/Wang-ck123/A-Two-Dimensional-Feature-Engineering-Method-for-Entity-Relation-Extraction"],"url":"http://arxiv.org/abs/2404.04959v1","category":"cs.CL"}
{"created":"2024-04-07 13:32:55","title":"Optimality of Decentralized Symmetric Policies for Stochastic Teams with Mean-Field Information Sharing","abstract":"We study a class of stochastic exchangeable teams comprising a finite number of decision makers (DMs) as well as their mean-field limits involving infinite numbers of DMs. In the finite population regime, we study exchangeable teams under the centralized information structure (IS). For the infinite population setting, we study exchangeable teams under the decentralized mean-field information sharing. The paper makes the following main contributions: i) For finite population exchangeable teams, we establish the existence of a randomized optimal policy that is exchangeable (permutation invariant) and Markovian. This optimal policy is obtained via value iterations for an equivalent measure-valued controlled Markov decision problem (MDP); ii) We show that a sequence of exchangeable optimal policies for a finite population setting converges to a conditionally symmetric (identical), independent, and decentralized randomized policy for the infinite population problem. This result establishes the existence of a symmetric, independent, decentralized optimal randomized policy for the infinite population problem. Additionally, it proves the optimality of the limiting measure-valued MDP for the representative DM; iii) Finally, we show that symmetric, independent, decentralized optimal randomized policies are approximately optimal for the corresponding finite-population team with a large number of DMs under the centralized IS.","sentences":["We study a class of stochastic exchangeable teams comprising a finite number of decision makers (DMs) as well as their mean-field limits involving infinite numbers of DMs.","In the finite population regime, we study exchangeable teams under the centralized information structure (IS).","For the infinite population setting, we study exchangeable teams under the decentralized mean-field information sharing.","The paper makes the following main contributions: i) For finite population exchangeable teams, we establish the existence of a randomized optimal policy that is exchangeable (permutation invariant) and Markovian.","This optimal policy is obtained via value iterations for an equivalent measure-valued controlled Markov decision problem (MDP); ii) We show that a sequence of exchangeable optimal policies for a finite population setting converges to a conditionally symmetric (identical), independent, and decentralized randomized policy for the infinite population problem.","This result establishes the existence of a symmetric, independent, decentralized optimal randomized policy for the infinite population problem.","Additionally, it proves the optimality of the limiting measure-valued MDP for the representative DM; iii)","Finally, we show that symmetric, independent, decentralized optimal randomized policies are approximately optimal for the corresponding finite-population team with a large number of DMs under the centralized IS."],"url":"http://arxiv.org/abs/2404.04957v1","category":"math.OC"}
{"created":"2024-04-07 13:07:54","title":"Axialgravisolitons at infinite corners","abstract":"Gravitational solitons (gravisolitons) are particular exact solutions of Einstein field equation in vacuum build on a given background solution. Their interpretation and importance is not yet fully clear and in this work we investigate the asymptotically behaviour of axially symmetric gravisolitons given minimal assumptions on the background metric. We develop an explicit systematic asymptotically expansion for the $N$-axialsoliton solution; we compute the leading order of the asymptotic killing vectors and we make a link with the corner symmetry proposal.","sentences":["Gravitational solitons (gravisolitons) are particular exact solutions of Einstein field equation in vacuum build on a given background solution.","Their interpretation and importance is not yet fully clear and in this work we investigate the asymptotically behaviour of axially symmetric gravisolitons given minimal assumptions on the background metric.","We develop an explicit systematic asymptotically expansion for the $N$-axialsoliton solution; we compute the leading order of the asymptotic killing vectors and we make a link with the corner symmetry proposal."],"url":"http://arxiv.org/abs/2404.04951v1","category":"gr-qc"}
{"created":"2024-04-07 13:05:26","title":"A Thermodynamically Consistent Phase-Field Model and an Entropy Stable Numerical Method for Simulating Two-Phase Flows with Thermocapillary Effects","abstract":"In this study, we have derived a thermodynamically consistent phase-field model for two-phase flows with thermocapillary effects. This model accommodates variations in physical properties such as density, viscosity, heat capacity, and thermal conductivity between the two components. The model equations encompass a Cahn-Hilliard equation with the volume fraction as the phase variable, a Navier-Stokes equation, and a heat equation, and meanwhile maintains mass conservation, energy conservation, and entropy increase simultaneously. Given the highly coupled and nonlinear nature of the model equations, we developed a semi-decoupled, mass-preserving, and entropy-stable time-discrete numerical method. We conducted several numerical tests to validate both our model and numerical method. Additionally, we have investigated the merging process of two bubbles under non-isothermal conditions and compared the results with those under isothermal conditions. Our findings reveal that temperature gradients influence bubble morphology and lead to earlier merging. Moreover, we have observed that the merging of bubbles slows down with increasing heat Peclect number PeT when the initial temperature field increases linearly along the channel, while bubbles merge faster with heat Peclect number PeT when the initial temperature field decreases linearly along the channel.","sentences":["In this study, we have derived a thermodynamically consistent phase-field model for two-phase flows with thermocapillary effects.","This model accommodates variations in physical properties such as density, viscosity, heat capacity, and thermal conductivity between the two components.","The model equations encompass a Cahn-Hilliard equation with the volume fraction as the phase variable, a Navier-Stokes equation, and a heat equation, and meanwhile maintains mass conservation, energy conservation, and entropy increase simultaneously.","Given the highly coupled and nonlinear nature of the model equations, we developed a semi-decoupled, mass-preserving, and entropy-stable time-discrete numerical method.","We conducted several numerical tests to validate both our model and numerical method.","Additionally, we have investigated the merging process of two bubbles under non-isothermal conditions and compared the results with those under isothermal conditions.","Our findings reveal that temperature gradients influence bubble morphology and lead to earlier merging.","Moreover, we have observed that the merging of bubbles slows down with increasing heat Peclect number PeT when the initial temperature field increases linearly along the channel, while bubbles merge faster with heat Peclect number PeT when the initial temperature field decreases linearly along the channel."],"url":"http://arxiv.org/abs/2404.04950v1","category":"physics.flu-dyn"}
{"created":"2024-04-07 12:59:40","title":"Iniva: Inclusive and Incentive-compatible Vote Aggregation","abstract":"Many blockchain platforms use committee-based consensus for scalability, finality, and security. In this consensus scheme, a committee decides which blocks get appended to the chain, typically through several voting phases. Platforms typically leverage the committee members' recorded votes to reward, punish, or detect failures. A common approach is to let the block proposer decide which votes to include, opening the door to possible attacks. For example, a malicious proposer can omit votes from targeted committee members, resulting in lost profits and, ultimately, their departure from the system.   This paper presents Iniva, an inclusive and incentive-compatible vote aggregation scheme that prevents such vote omission attacks. Iniva relies on a tree overlay with carefully selected fallback paths, making it robust against process failures without needing reconfiguration or additional redundancy. Our analysis shows that Iniva significantly reduces the chance to omit individual votes while ensuring that omitting many votes incurs a significant cost. In addition, our experimental results show that Iniva enjoys robustness, scalability, and reasonable throughput.","sentences":["Many blockchain platforms use committee-based consensus for scalability, finality, and security.","In this consensus scheme, a committee decides which blocks get appended to the chain, typically through several voting phases.","Platforms typically leverage the committee members' recorded votes to reward, punish, or detect failures.","A common approach is to let the block proposer decide which votes to include, opening the door to possible attacks.","For example, a malicious proposer can omit votes from targeted committee members, resulting in lost profits and, ultimately, their departure from the system.   ","This paper presents Iniva, an inclusive and incentive-compatible vote aggregation scheme that prevents such vote omission attacks.","Iniva relies on a tree overlay with carefully selected fallback paths, making it robust against process failures without needing reconfiguration or additional redundancy.","Our analysis shows that Iniva significantly reduces the chance to omit individual votes while ensuring that omitting many votes incurs a significant cost.","In addition, our experimental results show that Iniva enjoys robustness, scalability, and reasonable throughput."],"url":"http://arxiv.org/abs/2404.04948v1","category":"cs.CR"}
{"created":"2024-04-07 12:24:37","title":"The field of iterates of a rational function","abstract":"We study how the field of definition of a rational function changes under iteration. We provide a complete classification of polynomials with the property that the field of definition of one of their iterates drops in degree (over a given base field). We show with families of examples that this characterization does not hold for rational functions. Finally, we also classify fractional linear transformations with this property.","sentences":["We study how the field of definition of a rational function changes under iteration.","We provide a complete classification of polynomials with the property that the field of definition of one of their iterates drops in degree (over a given base field).","We show with families of examples that this characterization does not hold for rational functions.","Finally, we also classify fractional linear transformations with this property."],"url":"http://arxiv.org/abs/2404.04939v1","category":"math.NT"}
{"created":"2024-04-07 12:23:09","title":"Integer Optimal Control with Fractional Perimeter Regularization","abstract":"Motivated by many applications, optimal control problems with integer controls have recently received a significant attention. Some state-of-the-art work uses perimeter-regularization to derive stationarity conditions and trust-region algorithms. However, the discretization is difficult in this case because the perimeter is concentrated on a set of dimension $d - 1$ for a domain of dimension $d$.   This article proposes a potential way to overcome this challenge by using the fractional nonlocal perimeter with fractional exponent $0<\\alpha<1$. In this way, the boundary integrals in the perimeter regularization are replaced by volume integrals. Besides establishing some non-trivial properties associated with this perimeter, a $\\Gamma$-convergence result is derived. This result establishes convergence of minimizers of fractional perimeter-regularized problem, to the standard one, as the exponent $\\alpha$ tends to 1. In addition, the stationarity results are derived and algorithmic convergence analysis is carried out for $\\alpha \\in (0.5,1)$ under an additional assumption on the gradient of the reduced objective.   The theoretical results are supplemented by a preliminary computational experiment. We observe that the isotropy of the total variation may be approximated by means of the fractional perimeter functional.","sentences":["Motivated by many applications, optimal control problems with integer controls have recently received a significant attention.","Some state-of-the-art work uses perimeter-regularization to derive stationarity conditions and trust-region algorithms.","However, the discretization is difficult in this case because the perimeter is concentrated on a set of dimension $d - 1$ for a domain of dimension $d$.   This article proposes a potential way to overcome this challenge by using the fractional nonlocal perimeter with fractional exponent $0<\\alpha<1$.","In this way, the boundary integrals in the perimeter regularization are replaced by volume integrals.","Besides establishing some non-trivial properties associated with this perimeter, a $\\Gamma$-convergence result is derived.","This result establishes convergence of minimizers of fractional perimeter-regularized problem, to the standard one, as the exponent $\\alpha$ tends to 1.","In addition, the stationarity results are derived and algorithmic convergence analysis is carried out for $\\alpha \\in (0.5,1)$ under an additional assumption on the gradient of the reduced objective.   ","The theoretical results are supplemented by a preliminary computational experiment.","We observe that the isotropy of the total variation may be approximated by means of the fractional perimeter functional."],"url":"http://arxiv.org/abs/2404.04938v1","category":"math.OC"}
{"created":"2024-04-08 15:37:56","title":"Local behaviour of non-local hypoelliptic equations: divergence form","abstract":"We derive the Strong Harnack inequality for a class of hypoelliptic integro-differential equations in divergence form. The proof is based on a priori estimates, and as such extends the first non-stochastic approach of the non-local parabolic Strong Harnack inequality by Kassmann-Weidner [arXiv:2303.05975] to hypoelliptic equations. In a first step, we derive a local bound on the non-local tail on upper level sets by exploiting the coercivity of the cross terms. In a second step, we perform a De Giorgi argument in $L^1$, since we control the tail term only in $L^1$. This yields a linear $L^1$ to $L^\\infty$ bound. Consequentially, we prove polynomial upper and exponential lower bounds on the fundamental solution by adapting Aronson's method to non-local hypoelliptic equations.","sentences":["We derive the Strong Harnack inequality for a class of hypoelliptic integro-differential equations in divergence form.","The proof is based on a priori estimates, and as such extends the first non-stochastic approach of the non-local parabolic Strong Harnack inequality by Kassmann-Weidner [arXiv:2303.05975] to hypoelliptic equations.","In a first step, we derive a local bound on the non-local tail on upper level sets by exploiting the coercivity of the cross terms.","In a second step, we perform a De Giorgi argument in $L^1$, since we control the tail term only in $L^1$. This yields a linear $L^1$ to $L^\\infty$ bound.","Consequentially, we prove polynomial upper and exponential lower bounds on the fundamental solution by adapting Aronson's method to non-local hypoelliptic equations."],"url":"http://arxiv.org/abs/2404.05612v1","category":"math.AP"}
{"created":"2024-04-08 14:30:42","title":"TIM: A Time Interval Machine for Audio-Visual Action Recognition","abstract":"Diverse actions give rise to rich audio-visual signals in long videos. Recent works showcase that the two modalities of audio and video exhibit different temporal extents of events and distinct labels. We address the interplay between the two modalities in long videos by explicitly modelling the temporal extents of audio and visual events. We propose the Time Interval Machine (TIM) where a modality-specific time interval poses as a query to a transformer encoder that ingests a long video input. The encoder then attends to the specified interval, as well as the surrounding context in both modalities, in order to recognise the ongoing action.   We test TIM on three long audio-visual video datasets: EPIC-KITCHENS, Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. On EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly larger pre-training by 2.9% top-1 action recognition accuracy. Additionally, we show that TIM can be adapted for action detection, using dense multi-scale interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and showing strong performance on the Perception Test. Our ablations show the critical role of integrating the two modalities and modelling their time intervals in achieving this performance. Code and models at: https://github.com/JacobChalk/TIM","sentences":["Diverse actions give rise to rich audio-visual signals in long videos.","Recent works showcase that the two modalities of audio and video exhibit different temporal extents of events and distinct labels.","We address the interplay between the two modalities in long videos by explicitly modelling the temporal extents of audio and visual events.","We propose the Time Interval Machine (TIM) where a modality-specific time interval poses as a query to a transformer encoder that ingests a long video input.","The encoder then attends to the specified interval, as well as the surrounding context in both modalities, in order to recognise the ongoing action.   ","We test TIM on three long audio-visual video datasets: EPIC-KITCHENS, Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition.","On EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly larger pre-training by 2.9% top-1 action recognition accuracy.","Additionally, we show that TIM can be adapted for action detection, using dense multi-scale interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and showing strong performance on the Perception Test.","Our ablations show the critical role of integrating the two modalities and modelling their time intervals in achieving this performance.","Code and models at: https://github.com/JacobChalk/TIM"],"url":"http://arxiv.org/abs/2404.05559v1","category":"cs.CV"}
{"created":"2024-04-08 14:19:25","title":"Improving Quantum and Classical Decomposition Methods for Vehicle Routing","abstract":"Quantum computing is a promising technology to address combinatorial optimization problems, for example via the quantum approximate optimization algorithm (QAOA). Its potential, however, hinges on scaling toy problems to sizes relevant for industry. In this study, we address this challenge by an elaborate combination of two decomposition methods, namely graph shrinking and circuit cutting. Graph shrinking reduces the problem size before encoding into QAOA circuits, while circuit cutting decomposes quantum circuits into fragments for execution on medium-scale quantum computers. Our shrinking method adaptively reduces the problem such that the resulting QAOA circuits are particularly well-suited for circuit cutting. Moreover, we integrate two cutting techniques which allows us to run the resulting circuit fragments sequentially on the same device. We demonstrate the utility of our method by successfully applying it to the archetypical traveling salesperson problem (TSP) which often occurs as a sub-problem in practically relevant vehicle routing applications. For a TSP with seven cities, we are able to retrieve an optimum solution by consecutively running two 7-qubit QAOA circuits. Without decomposition methods, we would require five times as many qubits. Our results offer insights into the performance of algorithms for combinatorial optimization problems within the constraints of current quantum technology.","sentences":["Quantum computing is a promising technology to address combinatorial optimization problems, for example via the quantum approximate optimization algorithm (QAOA).","Its potential, however, hinges on scaling toy problems to sizes relevant for industry.","In this study, we address this challenge by an elaborate combination of two decomposition methods, namely graph shrinking and circuit cutting.","Graph shrinking reduces the problem size before encoding into QAOA circuits, while circuit cutting decomposes quantum circuits into fragments for execution on medium-scale quantum computers.","Our shrinking method adaptively reduces the problem such that the resulting QAOA circuits are particularly well-suited for circuit cutting.","Moreover, we integrate two cutting techniques which allows us to run the resulting circuit fragments sequentially on the same device.","We demonstrate the utility of our method by successfully applying it to the archetypical traveling salesperson problem (TSP) which often occurs as a sub-problem in practically relevant vehicle routing applications.","For a TSP with seven cities, we are able to retrieve an optimum solution by consecutively running two 7-qubit QAOA circuits.","Without decomposition methods, we would require five times as many qubits.","Our results offer insights into the performance of algorithms for combinatorial optimization problems within the constraints of current quantum technology."],"url":"http://arxiv.org/abs/2404.05551v1","category":"quant-ph"}
{"created":"2024-04-08 13:13:12","title":"Design, fabrication and test of a 5 GHz klystron based on the kladistron principle","abstract":"A new bunching method, named \"kladistron\" has been developed at CEA in order to provide high efficiency klystrons. A first \"kladistron\" prototype was designed and realized. It was adapted from the 4.9 GHz TH2166 from Thales, where the interaction line was transformed from 6 to 16 cavities. The design and fabrication phases of this prototype are developed in this paper. The kladistron prototype was tested in Thales facility. Its efficiency is finally lower (41 %) than expected (55 %), moreover it presents a spurious oscillation at 4.96 GHz. After analysis of the experimental results, it is concluded that the discrepancy between design and real frequencies is the cause for the low efficiency while the spurious oscillation results from a high gain peak at 4.96 GHz.","sentences":["A new bunching method, named \"kladistron\" has been developed at CEA in order to provide high efficiency klystrons.","A first \"kladistron\" prototype was designed and realized.","It was adapted from the 4.9 GHz TH2166 from Thales, where the interaction line was transformed from 6 to 16 cavities.","The design and fabrication phases of this prototype are developed in this paper.","The kladistron prototype was tested in Thales facility.","Its efficiency is finally lower (41 %) than expected (55 %), moreover it presents a spurious oscillation at 4.96 GHz.","After analysis of the experimental results, it is concluded that the discrepancy between design and real frequencies is the cause for the low efficiency while the spurious oscillation results from a high gain peak at 4.96 GHz."],"url":"http://arxiv.org/abs/2404.05493v1","category":"physics.acc-ph"}
{"created":"2024-04-08 11:20:28","title":"Anatomical Conditioning for Contrastive Unpaired Image-to-Image Translation of Optical Coherence Tomography Images","abstract":"For a unified analysis of medical images from different modalities, data harmonization using image-to-image (I2I) translation is desired. We study this problem employing an optical coherence tomography (OCT) data set of Spectralis-OCT and Home-OCT images. I2I translation is challenging because the images are unpaired, and a bijective mapping does not exist due to the information discrepancy between both domains. This problem has been addressed by the Contrastive Learning for Unpaired I2I Translation (CUT) approach, but it reduces semantic consistency. To restore the semantic consistency, we support the style decoder using an additional segmentation decoder. Our approach increases the similarity between the style-translated images and the target distribution. Importantly, we improve the segmentation of biomarkers in Home-OCT images in an unsupervised domain adaptation scenario. Our data harmonization approach provides potential for the monitoring of diseases, e.g., age related macular disease, using different OCT devices.","sentences":["For a unified analysis of medical images from different modalities, data harmonization using image-to-image (I2I) translation is desired.","We study this problem employing an optical coherence tomography (OCT) data set of Spectralis-OCT and Home-OCT images.","I2I translation is challenging because the images are unpaired, and a bijective mapping does not exist due to the information discrepancy between both domains.","This problem has been addressed by the Contrastive Learning for Unpaired I2I Translation (CUT) approach, but it reduces semantic consistency.","To restore the semantic consistency, we support the style decoder using an additional segmentation decoder.","Our approach increases the similarity between the style-translated images and the target distribution.","Importantly, we improve the segmentation of biomarkers in Home-OCT images in an unsupervised domain adaptation scenario.","Our data harmonization approach provides potential for the monitoring of diseases, e.g., age related macular disease, using different OCT devices."],"url":"http://arxiv.org/abs/2404.05409v1","category":"eess.IV"}
{"created":"2024-04-08 10:54:23","title":"On an optimal AFEM for elastoplasticity","abstract":"In this paper, optimal convergence for an adaptive finite element algorithm for elastoplasticity is considered. To this end, the proposed adaptive algorithm is established within the abstract framework of the axioms of adaptivity [Comput. Math. Appl., 67(6) (2014), 1195-1253], which provides a specific proceeding to prove the optimal convergence of the scheme. The proceeding is based on verifying four axioms, which ensure the optimal convergence. The verification is done by using results from [Numer. Math., 132(1) (2016), 131-154], which presents an alternative approach to optimality without explicitly relying on the axioms.","sentences":["In this paper, optimal convergence for an adaptive finite element algorithm for elastoplasticity is considered.","To this end, the proposed adaptive algorithm is established within the abstract framework of the axioms of adaptivity [Comput.","Math.","Appl., 67(6) (2014), 1195-1253], which provides a specific proceeding to prove the optimal convergence of the scheme.","The proceeding is based on verifying four axioms, which ensure the optimal convergence.","The verification is done by using results from [Numer.","Math., 132(1) (2016), 131-154], which presents an alternative approach to optimality without explicitly relying on the axioms."],"url":"http://arxiv.org/abs/2404.05395v1","category":"math.NA"}
{"created":"2024-04-08 09:38:22","title":"Certified PEFTSmoothing: Parameter-Efficient Fine-Tuning with Randomized Smoothing","abstract":"Randomized smoothing is the primary certified robustness method for accessing the robustness of deep learning models to adversarial perturbations in the l2-norm, by adding isotropic Gaussian noise to the input image and returning the majority votes over the base classifier. Theoretically, it provides a certified norm bound, ensuring predictions of adversarial examples are stable within this bound. A notable constraint limiting widespread adoption is the necessity to retrain base models entirely from scratch to attain a robust version. This is because the base model fails to learn the noise-augmented data distribution to give an accurate vote. One intuitive way to overcome this challenge is to involve a custom-trained denoiser to eliminate the noise. However, this approach is inefficient and sub-optimal. Inspired by recent large model training procedures, we explore an alternative way named PEFTSmoothing to adapt the base model to learn the Gaussian noise-augmented data with Parameter-Efficient Fine-Tuning (PEFT) methods in both white-box and black-box settings. Extensive results demonstrate the effectiveness and efficiency of PEFTSmoothing, which allow us to certify over 98% accuracy for ViT on CIFAR-10, 20% higher than SoTA denoised smoothing, and over 61% accuracy on ImageNet which is 30% higher than CNN-based denoiser and comparable to the Diffusion-based denoiser.","sentences":["Randomized smoothing is the primary certified robustness method for accessing the robustness of deep learning models to adversarial perturbations in the l2-norm, by adding isotropic Gaussian noise to the input image and returning the majority votes over the base classifier.","Theoretically, it provides a certified norm bound, ensuring predictions of adversarial examples are stable within this bound.","A notable constraint limiting widespread adoption is the necessity to retrain base models entirely from scratch to attain a robust version.","This is because the base model fails to learn the noise-augmented data distribution to give an accurate vote.","One intuitive way to overcome this challenge is to involve a custom-trained denoiser to eliminate the noise.","However, this approach is inefficient and sub-optimal.","Inspired by recent large model training procedures, we explore an alternative way named PEFTSmoothing to adapt the base model to learn the Gaussian noise-augmented data with Parameter-Efficient Fine-Tuning (PEFT) methods in both white-box and black-box settings.","Extensive results demonstrate the effectiveness and efficiency of PEFTSmoothing, which allow us to certify over 98% accuracy for ViT on CIFAR-10, 20% higher than SoTA denoised smoothing, and over 61% accuracy on ImageNet which is 30% higher than CNN-based denoiser and comparable to the Diffusion-based denoiser."],"url":"http://arxiv.org/abs/2404.05350v1","category":"cs.LG"}
{"created":"2024-04-08 09:27:42","title":"Comparative Analysis of Image Enhancement Techniques for Brain Tumor Segmentation: Contrast, Histogram, and Hybrid Approaches","abstract":"This study systematically investigates the impact of image enhancement techniques on Convolutional Neural Network (CNN)-based Brain Tumor Segmentation, focusing on Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE), and their hybrid variations. Employing the U-Net architecture on a dataset of 3064 Brain MRI images, the research delves into preprocessing steps, including resizing and enhancement, to optimize segmentation accuracy. A detailed analysis of the CNN-based U-Net architecture, training, and validation processes is provided. The comparative analysis, utilizing metrics such as Accuracy, Loss, MSE, IoU, and DSC, reveals that the hybrid approach CLAHE-HE consistently outperforms others. Results highlight its superior accuracy (0.9982, 0.9939, 0.9936 for training, testing, and validation, respectively) and robust segmentation overlap, with Jaccard values of 0.9862, 0.9847, and 0.9864, and Dice values of 0.993, 0.9923, and 0.9932 for the same phases, emphasizing its potential in neuro-oncological applications. The study concludes with a call for refinement in segmentation methodologies to further enhance diagnostic precision and treatment planning in neuro-oncology.","sentences":["This study systematically investigates the impact of image enhancement techniques on Convolutional Neural Network (CNN)-based Brain Tumor Segmentation, focusing on Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE), and their hybrid variations.","Employing the U-Net architecture on a dataset of 3064 Brain MRI images, the research delves into preprocessing steps, including resizing and enhancement, to optimize segmentation accuracy.","A detailed analysis of the CNN-based U-Net architecture, training, and validation processes is provided.","The comparative analysis, utilizing metrics such as Accuracy, Loss, MSE, IoU, and DSC, reveals that the hybrid approach CLAHE-HE consistently outperforms others.","Results highlight its superior accuracy (0.9982, 0.9939, 0.9936 for training, testing, and validation, respectively) and robust segmentation overlap, with Jaccard values of 0.9862, 0.9847, and 0.9864, and Dice values of 0.993, 0.9923, and 0.9932 for the same phases, emphasizing its potential in neuro-oncological applications.","The study concludes with a call for refinement in segmentation methodologies to further enhance diagnostic precision and treatment planning in neuro-oncology."],"url":"http://arxiv.org/abs/2404.05341v1","category":"eess.IV"}
{"created":"2024-04-08 09:26:31","title":"GPS-free Autonomous Navigation in Cluttered Tree Rows with Deep Semantic Segmentation","abstract":"Segmentation-based autonomous navigation has recently been presented as an appealing approach to guiding robotic platforms through crop rows without requiring perfect GPS localization. Nevertheless, current techniques are restricted to situations where the distinct separation between the plants and the sky allows for the identification of the row's center. However, tall, dense vegetation, such as high tree rows and orchards, is the primary cause of GPS signal blockage. In this study, we increase the overall robustness and adaptability of the control algorithm by extending the segmentation-based robotic guiding to those cases where canopies and branches occlude the sky and prevent the utilization of GPS and earlier approaches. An efficient Deep Neural Network architecture has been used to address semantic segmentation, performing the training with synthetic data only. Numerous vineyards and tree fields have undergone extensive testing in both simulation and real-world to show the solution's competitive benefits.","sentences":["Segmentation-based autonomous navigation has recently been presented as an appealing approach to guiding robotic platforms through crop rows without requiring perfect GPS localization.","Nevertheless, current techniques are restricted to situations where the distinct separation between the plants and the sky allows for the identification of the row's center.","However, tall, dense vegetation, such as high tree rows and orchards, is the primary cause of GPS signal blockage.","In this study, we increase the overall robustness and adaptability of the control algorithm by extending the segmentation-based robotic guiding to those cases where canopies and branches occlude the sky and prevent the utilization of GPS and earlier approaches.","An efficient Deep Neural Network architecture has been used to address semantic segmentation, performing the training with synthetic data only.","Numerous vineyards and tree fields have undergone extensive testing in both simulation and real-world to show the solution's competitive benefits."],"url":"http://arxiv.org/abs/2404.05338v1","category":"cs.RO"}
{"created":"2024-04-08 08:47:46","title":"Liquid Neural Network-based Adaptive Learning vs. Incremental Learning for Link Load Prediction amid Concept Drift due to Network Failures","abstract":"Adapting to concept drift is a challenging task in machine learning, which is usually tackled using incremental learning techniques that periodically re-fit a learning model leveraging newly available data. A primary limitation of these techniques is their reliance on substantial amounts of data for retraining. The necessity of acquiring fresh data introduces temporal delays prior to retraining, potentially rendering the models inaccurate if a sudden concept drift occurs in-between two consecutive retrainings. In communication networks, such issue emerges when performing traffic forecasting following a~failure event: post-failure re-routing may induce a drastic shift in distribution and pattern of traffic data, thus requiring a timely model adaptation. In this work, we address this challenge for the problem of traffic forecasting and propose an approach that exploits adaptive learning algorithms, namely, liquid neural networks, which are capable of self-adaptation to abrupt changes in data patterns without requiring any retraining. Through extensive simulations of failure scenarios, we compare the predictive performance of our proposed approach to that of a reference method based on incremental learning. Experimental results show that our proposed approach outperforms incremental learning-based methods in situations where the shifts in traffic patterns are drastic.","sentences":["Adapting to concept drift is a challenging task in machine learning, which is usually tackled using incremental learning techniques that periodically re-fit a learning model leveraging newly available data.","A primary limitation of these techniques is their reliance on substantial amounts of data for retraining.","The necessity of acquiring fresh data introduces temporal delays prior to retraining, potentially rendering the models inaccurate if a sudden concept drift occurs in-between two consecutive retrainings.","In communication networks, such issue emerges when performing traffic forecasting following a~failure event: post-failure re-routing may induce a drastic shift in distribution and pattern of traffic data, thus requiring a timely model adaptation.","In this work, we address this challenge for the problem of traffic forecasting and propose an approach that exploits adaptive learning algorithms, namely, liquid neural networks, which are capable of self-adaptation to abrupt changes in data patterns without requiring any retraining.","Through extensive simulations of failure scenarios, we compare the predictive performance of our proposed approach to that of a reference method based on incremental learning.","Experimental results show that our proposed approach outperforms incremental learning-based methods in situations where the shifts in traffic patterns are drastic."],"url":"http://arxiv.org/abs/2404.05304v1","category":"cs.NI"}
{"created":"2024-04-08 07:51:20","title":"Robust Anthropomorphic Robotic Manipulation through Biomimetic Distributed Compliance","abstract":"The impressive capabilities of humans to robustly perform manipulation relies on compliant interactions, enabled through the structure and materials spatially distributed in our hands. We propose by mimicking this distributed compliance in an anthropomorphic robotic hand, the open-loop manipulation robustness increases and observe the emergence of human-like behaviours. To achieve this, we introduce the ADAPT Hand equipped with tunable compliance throughout the skin, fingers, and the wrist. Through extensive automated pick-and-place tests, we show the grasping robustness closely mirrors an estimated geometric theoretical limit, while `stress-testing' the robot hand to perform 800+ grasps. Finally, 24 items with largely varying geometries are grasped in a constrained environment with a success rate of 93\\%. We demonstrate the hand-object self-organization behavior underlines this extreme robustness, where the hand automatically exhibits different grasp types depending on object geometries. Furthermore, the robot grasp type mimics a natural human grasp with a direct similarity of 68\\%.","sentences":["The impressive capabilities of humans to robustly perform manipulation relies on compliant interactions, enabled through the structure and materials spatially distributed in our hands.","We propose by mimicking this distributed compliance in an anthropomorphic robotic hand, the open-loop manipulation robustness increases and observe the emergence of human-like behaviours.","To achieve this, we introduce the ADAPT Hand equipped with tunable compliance throughout the skin, fingers, and the wrist.","Through extensive automated pick-and-place tests, we show the grasping robustness closely mirrors an estimated geometric theoretical limit, while `stress-testing' the robot hand to perform 800+ grasps.","Finally, 24 items with largely varying geometries are grasped in a constrained environment with a success rate of 93\\%.","We demonstrate the hand-object self-organization behavior underlines this extreme robustness, where the hand automatically exhibits different grasp types depending on object geometries.","Furthermore, the robot grasp type mimics a natural human grasp with a direct similarity of 68\\%."],"url":"http://arxiv.org/abs/2404.05262v1","category":"cs.RO"}
{"created":"2024-04-08 06:32:11","title":"StylizedGS: Controllable Stylization for 3D Gaussian Splatting","abstract":"With the rapid development of XR, 3D generation and editing are becoming more and more important, among which, stylization is an important tool of 3D appearance editing. It can achieve consistent 3D artistic stylization given a single reference style image and thus is a user-friendly editing way. However, recent NeRF-based 3D stylization methods face efficiency issues that affect the actual user experience and the implicit nature limits its ability to transfer the geometric pattern styles. Additionally, the ability for artists to exert flexible control over stylized scenes is considered highly desirable, fostering an environment conducive to creative exploration. In this paper, we introduce StylizedGS, a 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The 3DGS brings the benefits of high efficiency. We propose a GS filter to eliminate floaters in the reconstruction which affects the stylization effects before stylization. Then the nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content. Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale and regions during the stylization to possess customized capabilities. Our method can attain high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls. Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference FPS.","sentences":["With the rapid development of XR, 3D generation and editing are becoming more and more important, among which, stylization is an important tool of 3D appearance editing.","It can achieve consistent 3D artistic stylization given a single reference style image and thus is a user-friendly editing way.","However, recent NeRF-based 3D stylization methods face efficiency issues that affect the actual user experience and the implicit nature limits its ability to transfer the geometric pattern styles.","Additionally, the ability for artists to exert flexible control over stylized scenes is considered highly desirable, fostering an environment conducive to creative exploration.","In this paper, we introduce StylizedGS, a 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation.","The 3DGS brings the benefits of high efficiency.","We propose a GS filter to eliminate floaters in the reconstruction which affects the stylization effects before stylization.","Then the nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content.","Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale and regions during the stylization to possess customized capabilities.","Our method can attain high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls.","Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference FPS."],"url":"http://arxiv.org/abs/2404.05220v1","category":"cs.CV"}
{"created":"2024-04-08 06:07:32","title":"Spatio-Temporal Attention and Gaussian Processes for Personalized Video Gaze Estimation","abstract":"Gaze is an essential prompt for analyzing human behavior and attention. Recently, there has been an increasing interest in determining gaze direction from facial videos. However, video gaze estimation faces significant challenges, such as understanding the dynamic evolution of gaze in video sequences, dealing with static backgrounds, and adapting to variations in illumination. To address these challenges, we propose a simple and novel deep learning model designed to estimate gaze from videos, incorporating a specialized attention module. Our method employs a spatial attention mechanism that tracks spatial dynamics within videos. This technique enables accurate gaze direction prediction through a temporal sequence model, adeptly transforming spatial observations into temporal insights, thereby significantly improving gaze estimation accuracy. Additionally, our approach integrates Gaussian processes to include individual-specific traits, facilitating the personalization of our model with just a few labeled samples. Experimental results confirm the efficacy of the proposed approach, demonstrating its success in both within-dataset and cross-dataset settings. Specifically, our proposed approach achieves state-of-the-art performance on the Gaze360 dataset, improving by $2.5^\\circ$ without personalization. Further, by personalizing the model with just three samples, we achieved an additional improvement of $0.8^\\circ$. The code and pre-trained models are available at \\url{https://github.com/jswati31/stage}.","sentences":["Gaze is an essential prompt for analyzing human behavior and attention.","Recently, there has been an increasing interest in determining gaze direction from facial videos.","However, video gaze estimation faces significant challenges, such as understanding the dynamic evolution of gaze in video sequences, dealing with static backgrounds, and adapting to variations in illumination.","To address these challenges, we propose a simple and novel deep learning model designed to estimate gaze from videos, incorporating a specialized attention module.","Our method employs a spatial attention mechanism that tracks spatial dynamics within videos.","This technique enables accurate gaze direction prediction through a temporal sequence model, adeptly transforming spatial observations into temporal insights, thereby significantly improving gaze estimation accuracy.","Additionally, our approach integrates Gaussian processes to include individual-specific traits, facilitating the personalization of our model with just a few labeled samples.","Experimental results confirm the efficacy of the proposed approach, demonstrating its success in both within-dataset and cross-dataset settings.","Specifically, our proposed approach achieves state-of-the-art performance on the Gaze360 dataset, improving by $2.5^\\circ$ without personalization.","Further, by personalizing the model with just three samples, we achieved an additional improvement of $0.8^\\circ$. The code and pre-trained models are available at \\url{https://github.com/jswati31/stage}."],"url":"http://arxiv.org/abs/2404.05215v1","category":"cs.CV"}
{"created":"2024-04-08 05:23:12","title":"iVPT: Improving Task-relevant Information Sharing in Visual Prompt Tuning by Cross-layer Dynamic Connection","abstract":"Recent progress has shown great potential of visual prompt tuning (VPT) when adapting pre-trained vision transformers to various downstream tasks. However, most existing solutions independently optimize prompts at each layer, thereby neglecting the usage of task-relevant information encoded in prompt tokens across layers. Additionally, existing prompt structures are prone to interference from task-irrelevant noise in input images, which can do harm to the sharing of task-relevant information. In this paper, we propose a novel VPT approach, \\textbf{iVPT}. It innovatively incorporates a cross-layer dynamic connection (CDC) for input prompt tokens from adjacent layers, enabling effective sharing of task-relevant information. Furthermore, we design a dynamic aggregation (DA) module that facilitates selective sharing of information between layers. The combination of CDC and DA enhances the flexibility of the attention process within the VPT framework. Building upon these foundations, iVPT introduces an attentive reinforcement (AR) mechanism, by automatically identifying salient image tokens, which are further enhanced by prompt tokens in an additive manner. Extensive experiments on 24 image classification and semantic segmentation benchmarks clearly demonstrate the advantage of the proposed iVPT, compared to the state-of-the-art counterparts.","sentences":["Recent progress has shown great potential of visual prompt tuning (VPT) when adapting pre-trained vision transformers to various downstream tasks.","However, most existing solutions independently optimize prompts at each layer, thereby neglecting the usage of task-relevant information encoded in prompt tokens across layers.","Additionally, existing prompt structures are prone to interference from task-irrelevant noise in input images, which can do harm to the sharing of task-relevant information.","In this paper, we propose a novel VPT approach, \\textbf{iVPT}.","It innovatively incorporates a cross-layer dynamic connection (CDC) for input prompt tokens from adjacent layers, enabling effective sharing of task-relevant information.","Furthermore, we design a dynamic aggregation (DA) module that facilitates selective sharing of information between layers.","The combination of CDC and DA enhances the flexibility of the attention process within the VPT framework.","Building upon these foundations, iVPT introduces an attentive reinforcement (AR) mechanism, by automatically identifying salient image tokens, which are further enhanced by prompt tokens in an additive manner.","Extensive experiments on 24 image classification and semantic segmentation benchmarks clearly demonstrate the advantage of the proposed iVPT, compared to the state-of-the-art counterparts."],"url":"http://arxiv.org/abs/2404.05207v1","category":"cs.CV"}
{"created":"2024-04-08 04:43:40","title":"Foundations for operator algebraic tricategories","abstract":"An operator algebraic tricategory is a higher categorical analogue of an operator algebra. For algebraic tricategories, Gordon, Power, and Street proved that every algebraic tricategory is equivalent to a Gray-category, a result later refined by Gurski. We adapt this result to the context of functional analysis, showing that every operator algebraic tricategory is equivalent to an operator Gray-category. We then categorify the Gelfand-Naimark theorem for operator algebras, inductively proving that every (small) operator algebraic tricategory is equivalent to a concrete operator Gray-category. We also provide several examples of interest for operator algebraic tricategories.","sentences":["An operator algebraic tricategory is a higher categorical analogue of an operator algebra.","For algebraic tricategories, Gordon, Power, and Street proved that every algebraic tricategory is equivalent to a Gray-category, a result later refined by Gurski.","We adapt this result to the context of functional analysis, showing that every operator algebraic tricategory is equivalent to an operator Gray-category.","We then categorify the Gelfand-Naimark theorem for operator algebras, inductively proving that every (small) operator algebraic tricategory is equivalent to a concrete operator Gray-category.","We also provide several examples of interest for operator algebraic tricategories."],"url":"http://arxiv.org/abs/2404.05193v1","category":"math.OA"}
{"created":"2024-04-08 04:13:35","title":"Adaptive Learning for Multi-view Stereo Reconstruction","abstract":"Deep learning has recently demonstrated its excellent performance on the task of multi-view stereo (MVS). However, loss functions applied for deep MVS are rarely studied. In this paper, we first analyze existing loss functions' properties for deep depth based MVS approaches. Regression based loss leads to inaccurate continuous results by computing mathematical expectation, while classification based loss outputs discretized depth values. To this end, we then propose a novel loss function, named adaptive Wasserstein loss, which is able to narrow down the difference between the true and predicted probability distributions of depth. Besides, a simple but effective offset module is introduced to better achieve sub-pixel prediction accuracy. Extensive experiments on different benchmarks, including DTU, Tanks and Temples and BlendedMVS, show that the proposed method with the adaptive Wasserstein loss and the offset module achieves state-of-the-art performance.","sentences":["Deep learning has recently demonstrated its excellent performance on the task of multi-view stereo (MVS).","However, loss functions applied for deep MVS are rarely studied.","In this paper, we first analyze existing loss functions' properties for deep depth based MVS approaches.","Regression based loss leads to inaccurate continuous results by computing mathematical expectation, while classification based loss outputs discretized depth values.","To this end, we then propose a novel loss function, named adaptive Wasserstein loss, which is able to narrow down the difference between the true and predicted probability distributions of depth.","Besides, a simple but effective offset module is introduced to better achieve sub-pixel prediction accuracy.","Extensive experiments on different benchmarks, including DTU, Tanks and Temples and BlendedMVS, show that the proposed method with the adaptive Wasserstein loss and the offset module achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.05181v1","category":"cs.CV"}
{"created":"2024-04-08 03:29:58","title":"Adapting to Covariate Shift in Real-time by Encoding Trees with Motion Equations","abstract":"Input distribution shift presents a significant problem in many real-world systems. Here we present Xenovert, an adaptive algorithm that can dynamically adapt to changes in input distribution. It is a perfect binary tree that adaptively divides a continuous input space into several intervals of uniform density while receiving a continuous stream of input. This process indirectly maps the source distribution to the shifted target distribution, preserving the data's relationship with the downstream decoder/operation, even after the shift occurs. In this paper, we demonstrated how a neural network integrated with Xenovert achieved better results in 4 out of 5 shifted datasets, saving the hurdle of retraining a machine learning model. We anticipate that Xenovert can be applied to many more applications that require adaptation to unforeseen input distribution shifts, even when the distribution shift is drastic.","sentences":["Input distribution shift presents a significant problem in many real-world systems.","Here we present Xenovert, an adaptive algorithm that can dynamically adapt to changes in input distribution.","It is a perfect binary tree that adaptively divides a continuous input space into several intervals of uniform density while receiving a continuous stream of input.","This process indirectly maps the source distribution to the shifted target distribution, preserving the data's relationship with the downstream decoder/operation, even after the shift occurs.","In this paper, we demonstrated how a neural network integrated with Xenovert achieved better results in 4 out of 5 shifted datasets, saving the hurdle of retraining a machine learning model.","We anticipate that Xenovert can be applied to many more applications that require adaptation to unforeseen input distribution shifts, even when the distribution shift is drastic."],"url":"http://arxiv.org/abs/2404.05168v1","category":"cs.LG"}
{"created":"2024-04-08 03:06:19","title":"Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular Videos","abstract":"In this work, we pioneer Semantic Flow, a neural semantic representation of dynamic scenes from monocular videos. In contrast to previous NeRF methods that reconstruct dynamic scenes from the colors and volume densities of individual points, Semantic Flow learns semantics from continuous flows that contain rich 3D motion information. As there is 2D-to-3D ambiguity problem in the viewing direction when extracting 3D flow features from 2D video frames, we consider the volume densities as opacity priors that describe the contributions of flow features to the semantics on the frames. More specifically, we first learn a flow network to predict flows in the dynamic scene, and propose a flow feature aggregation module to extract flow features from video frames. Then, we propose a flow attention module to extract motion information from flow features, which is followed by a semantic network to output semantic logits of flows. We integrate the logits with volume densities in the viewing direction to supervise the flow features with semantic labels on video frames. Experimental results show that our model is able to learn from multiple dynamic scenes and supports a series of new tasks such as instance-level scene editing, semantic completions, dynamic scene tracking and semantic adaption on novel scenes. Codes are available at https://github.com/tianfr/Semantic-Flow/.","sentences":["In this work, we pioneer Semantic Flow, a neural semantic representation of dynamic scenes from monocular videos.","In contrast to previous NeRF methods that reconstruct dynamic scenes from the colors and volume densities of individual points, Semantic Flow learns semantics from continuous flows that contain rich 3D motion information.","As there is 2D-to-3D ambiguity problem in the viewing direction when extracting 3D flow features from 2D video frames, we consider the volume densities as opacity priors that describe the contributions of flow features to the semantics on the frames.","More specifically, we first learn a flow network to predict flows in the dynamic scene, and propose a flow feature aggregation module to extract flow features from video frames.","Then, we propose a flow attention module to extract motion information from flow features, which is followed by a semantic network to output semantic logits of flows.","We integrate the logits with volume densities in the viewing direction to supervise the flow features with semantic labels on video frames.","Experimental results show that our model is able to learn from multiple dynamic scenes and supports a series of new tasks such as instance-level scene editing, semantic completions, dynamic scene tracking and semantic adaption on novel scenes.","Codes are available at https://github.com/tianfr/Semantic-Flow/."],"url":"http://arxiv.org/abs/2404.05163v1","category":"cs.CV"}
{"created":"2024-04-08 02:02:15","title":"UniMix: Towards Domain Adaptive and Generalizable LiDAR Semantic Segmentation in Adverse Weather","abstract":"LiDAR semantic segmentation (LSS) is a critical task in autonomous driving and has achieved promising progress. However, prior LSS methods are conventionally investigated and evaluated on datasets within the same domain in clear weather. The robustness of LSS models in unseen scenes and all weather conditions is crucial for ensuring safety and reliability in real applications. To this end, we propose UniMix, a universal method that enhances the adaptability and generalizability of LSS models. UniMix first leverages physically valid adverse weather simulation to construct a Bridge Domain, which serves to bridge the domain gap between the clear weather scenes and the adverse weather scenes. Then, a Universal Mixing operator is defined regarding spatial, intensity, and semantic distributions to create the intermediate domain with mixed samples from given domains. Integrating the proposed two techniques into a teacher-student framework, UniMix efficiently mitigates the domain gap and enables LSS models to learn weather-robust and domain-invariant representations. We devote UniMix to two main setups: 1) unsupervised domain adaption, adapting the model from the clear weather source domain to the adverse weather target domain; 2) domain generalization, learning a model that generalizes well to unseen scenes in adverse weather. Extensive experiments validate the effectiveness of UniMix across different tasks and datasets, all achieving superior performance over state-of-the-art methods. The code will be released.","sentences":["LiDAR semantic segmentation (LSS) is a critical task in autonomous driving and has achieved promising progress.","However, prior LSS methods are conventionally investigated and evaluated on datasets within the same domain in clear weather.","The robustness of LSS models in unseen scenes and all weather conditions is crucial for ensuring safety and reliability in real applications.","To this end, we propose UniMix, a universal method that enhances the adaptability and generalizability of LSS models.","UniMix first leverages physically valid adverse weather simulation to construct a Bridge Domain, which serves to bridge the domain gap between the clear weather scenes and the adverse weather scenes.","Then, a Universal Mixing operator is defined regarding spatial, intensity, and semantic distributions to create the intermediate domain with mixed samples from given domains.","Integrating the proposed two techniques into a teacher-student framework, UniMix efficiently mitigates the domain gap and enables LSS models to learn weather-robust and domain-invariant representations.","We devote UniMix to two main setups: 1) unsupervised domain adaption, adapting the model from the clear weather source domain to the adverse weather target domain; 2) domain generalization, learning a model that generalizes well to unseen scenes in adverse weather.","Extensive experiments validate the effectiveness of UniMix across different tasks and datasets, all achieving superior performance over state-of-the-art methods.","The code will be released."],"url":"http://arxiv.org/abs/2404.05145v1","category":"cs.CV"}
{"created":"2024-04-08 01:25:38","title":"LLM-BT: Performing Robotic Adaptive Tasks based on Large Language Models and Behavior Trees","abstract":"Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks. However, handling external disturbances during tasks is still an open challenge. This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs). It utilizes ChatGPT to reason the descriptive steps of tasks. In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm. Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs. Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks. Different from other LLM-based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances. Our method is validated with simulation in different practical scenarios.","sentences":["Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks.","However, handling external disturbances during tasks is still an open challenge.","This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs).","It utilizes ChatGPT to reason the descriptive steps of tasks.","In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm.","Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs.","Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks.","Different from other LLM-based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances.","Our method is validated with simulation in different practical scenarios."],"url":"http://arxiv.org/abs/2404.05134v1","category":"cs.RO"}
{"created":"2024-04-08 01:00:37","title":"Well-posedness of the 2D surface quasi-geostrophic equation in variable Lebesgue spaces","abstract":"In this paper, we are mainly concerned with the well-posedness of the dissipative surface quasi-geostrophic equation in the framework of variable Lebesgue spaces. Based on some analytical results developed in the variable Lebesgue spaces and the $L^{p}$-$L^{q}$ decay estimates of the fractional heat kernel, we establish, for the 2D dissipative surface quasi-geostrophic equation, the global well-posedness in the space $\\mathcal{L}^{p(\\cdot)}_{\\frac{2}{\\alpha-1}}(\\mathbb{R}^{2},L^{\\infty}(0,\\infty))$ and the local well-posedness in the space $L^{q(\\cdot)}(0,T; L^{p}(\\mathbb{R}^{2}))$.","sentences":["In this paper, we are mainly concerned with the well-posedness of the dissipative surface quasi-geostrophic equation in the framework of variable Lebesgue spaces.","Based on some analytical results developed in the variable Lebesgue spaces and the $L^{p}$-$L^{q}$ decay estimates of the fractional heat kernel, we establish, for the 2D dissipative surface quasi-geostrophic equation, the global well-posedness in the space $\\mathcal{L}^{p(\\cdot)}_{\\frac{2}{\\alpha-1}}(\\mathbb{R}^{2},L^{\\infty}(0,\\infty))$ and the local well-posedness in the space $L^{q(\\cdot)}(0,T; L^{p}(\\mathbb{R}^{2}))$."],"url":"http://arxiv.org/abs/2404.05127v1","category":"math.AP"}
{"created":"2024-04-08 00:36:19","title":"Nanouniverse: Virtual Instancing of Structural Detail and Adaptive Shell Mapping","abstract":"Rendering huge biological scenes with atomistic detail presents a significant challenge in molecular visualization due to the memory limitations inherent in traditional rendering approaches. In this paper, we propose a novel method for the interactive rendering of massive molecular scenes based on hardware-accelerated ray tracing. Our approach circumvents GPU memory constraints by introducing virtual instantiation of full-detail scene elements. Using instancing significantly reduces memory consumption while preserving the full atomistic detail of scenes comprising trillions of atoms, with interactive rendering performance and completely free user exploration. We utilize coarse meshes as proxy geometries to approximate the overall shape of biological compartments, and access all atomistic detail dynamically during ray tracing. We do this via a novel adaptive technique utilizing a volumetric shell layer of prisms extruded around proxy geometry triangles, and a virtual volume grid for the interior of each compartment. Our algorithm scales to enormous molecular scenes with minimal memory consumption and the potential to accommodate even larger scenes. Our method also supports advanced effects such as clipping planes and animations. We demonstrate the efficiency and scalability of our approach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models theoretically containing more than 20 trillion atoms.","sentences":["Rendering huge biological scenes with atomistic detail presents a significant challenge in molecular visualization due to the memory limitations inherent in traditional rendering approaches.","In this paper, we propose a novel method for the interactive rendering of massive molecular scenes based on hardware-accelerated ray tracing.","Our approach circumvents GPU memory constraints by introducing virtual instantiation of full-detail scene elements.","Using instancing significantly reduces memory consumption while preserving the full atomistic detail of scenes comprising trillions of atoms, with interactive rendering performance and completely free user exploration.","We utilize coarse meshes as proxy geometries to approximate the overall shape of biological compartments, and access all atomistic detail dynamically during ray tracing.","We do this via a novel adaptive technique utilizing a volumetric shell layer of prisms extruded around proxy geometry triangles, and a virtual volume grid for the interior of each compartment.","Our algorithm scales to enormous molecular scenes with minimal memory consumption and the potential to accommodate even larger scenes.","Our method also supports advanced effects such as clipping planes and animations.","We demonstrate the efficiency and scalability of our approach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models theoretically containing more than 20 trillion atoms."],"url":"http://arxiv.org/abs/2404.05116v1","category":"cs.GR"}
{"created":"2024-04-08 00:13:05","title":"Class Similarity Transition: Decoupling Class Similarities and Imbalance from Generalized Few-shot Segmentation","abstract":"In Generalized Few-shot Segmentation (GFSS), a model is trained with a large corpus of base class samples and then adapted on limited samples of novel classes. This paper focuses on the relevance between base and novel classes, and improves GFSS in two aspects: 1) mining the similarity between base and novel classes to promote the learning of novel classes, and 2) mitigating the class imbalance issue caused by the volume difference between the support set and the training set. Specifically, we first propose a similarity transition matrix to guide the learning of novel classes with base class knowledge. Then, we leverage the Label-Distribution-Aware Margin (LDAM) loss and Transductive Inference to the GFSS task to address the problem of class imbalance as well as overfitting the support set. In addition, by extending the probability transition matrix, the proposed method can mitigate the catastrophic forgetting of base classes when learning novel classes. With a simple training phase, our proposed method can be applied to any segmentation network trained on base classes. We validated our methods on the adapted version of OpenEarthMap. Compared to existing GFSS baselines, our method excels them all from 3% to 7% and ranks second in the OpenEarthMap Land Cover Mapping Few-Shot Challenge at the completion of this paper. Code: https://github.com/earth-insights/ClassTrans","sentences":["In Generalized Few-shot Segmentation (GFSS), a model is trained with a large corpus of base class samples and then adapted on limited samples of novel classes.","This paper focuses on the relevance between base and novel classes, and improves GFSS in two aspects: 1) mining the similarity between base and novel classes to promote the learning of novel classes, and 2) mitigating the class imbalance issue caused by the volume difference between the support set and the training set.","Specifically, we first propose a similarity transition matrix to guide the learning of novel classes with base class knowledge.","Then, we leverage the Label-Distribution-Aware Margin (LDAM) loss and Transductive Inference to the GFSS task to address the problem of class imbalance as well as overfitting the support set.","In addition, by extending the probability transition matrix, the proposed method can mitigate the catastrophic forgetting of base classes when learning novel classes.","With a simple training phase, our proposed method can be applied to any segmentation network trained on base classes.","We validated our methods on the adapted version of OpenEarthMap.","Compared to existing GFSS baselines, our method excels them all from 3% to 7% and ranks second in the OpenEarthMap Land Cover Mapping Few-Shot Challenge at the completion of this paper.","Code: https://github.com/earth-insights/ClassTrans"],"url":"http://arxiv.org/abs/2404.05111v1","category":"cs.CV"}
{"created":"2024-04-07 20:37:08","title":"Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization","abstract":"The following paper presents an adaptive anchor pairs selection method for ultra-wideband (UWB) Time Difference of Arrival (TDOA) based positioning systems. The method divides the area covered by the system into several zones and assigns them anchor pair sets. The pair sets are determined during calibration based on localization root mean square error (RMSE). The calibration assumes driving a mobile platform equipped with a LiDAR sensor and a UWB tag through the specified zones. The robot is localized separately based on a large set of different TDOA pairs and using a LiDAR, which acts as the reference. For each zone, the TDOA pairs set for which the registered RMSE is lowest is selected and used for localization in the routine system work. The proposed method has been tested with simulations and experiments. The results for both simulated static and experimental dynamic scenarios have proven that the adaptive selection of the anchor nodes leads to an increase in localization accuracy. In the experiment, the median trajectory error for a moving person localization was at a level of 25 cm.","sentences":["The following paper presents an adaptive anchor pairs selection method for ultra-wideband (UWB) Time Difference of Arrival (TDOA) based positioning systems.","The method divides the area covered by the system into several zones and assigns them anchor pair sets.","The pair sets are determined during calibration based on localization root mean square error (RMSE).","The calibration assumes driving a mobile platform equipped with a LiDAR sensor and a UWB tag through the specified zones.","The robot is localized separately based on a large set of different TDOA pairs and using a LiDAR, which acts as the reference.","For each zone, the TDOA pairs set for which the registered RMSE is lowest is selected and used for localization in the routine system work.","The proposed method has been tested with simulations and experiments.","The results for both simulated static and experimental dynamic scenarios have proven that the adaptive selection of the anchor nodes leads to an increase in localization accuracy.","In the experiment, the median trajectory error for a moving person localization was at a level of 25 cm."],"url":"http://arxiv.org/abs/2404.05067v1","category":"cs.RO"}
{"created":"2024-04-07 19:23:28","title":"Facial Affective Behavior Analysis with Instruction Tuning","abstract":"Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images. However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and reasoning capability for complex facial behaviors. The advent of Multi-modal Large Language Models (MLLMs) has been proven successful in general visual understanding tasks. However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and benchmarks, neglecting facial prior knowledge, and low training efficiency. To address these challenges, we introduce (i) an instruction-following dataset for two FABA tasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Bench with a new metric considering both recognition and generation ability, and (iii) a new MLLM \"EmoLA\" as a strong baseline to the community. Our initiative on the dataset and benchmarks reveal the nature and rationale of facial affective behaviors, i.e., fine-grained facial movement, interpretability, and reasoning. Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM. We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets. The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench. On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models.","sentences":["Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images.","However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and reasoning capability for complex facial behaviors.","The advent of Multi-modal Large Language Models (MLLMs) has been proven successful in general visual understanding tasks.","However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and benchmarks, neglecting facial prior knowledge, and low training efficiency.","To address these challenges, we introduce (i) an instruction-following dataset for two FABA tasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Bench with a new metric considering both recognition and generation ability, and (iii) a new MLLM \"EmoLA\" as a strong baseline to the community.","Our initiative on the dataset and benchmarks reveal the nature and rationale of facial affective behaviors, i.e., fine-grained facial movement, interpretability, and reasoning.","Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM.","We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets.","The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench.","On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models."],"url":"http://arxiv.org/abs/2404.05052v1","category":"cs.CV"}
{"created":"2024-04-07 18:53:30","title":"How Do OSS Developers Utilize Architectural Solutions from Q&A Sites: An Empirical Study","abstract":"Developers utilize programming-related knowledge (e.g., code snippets) on Q&A sites (e.g., Stack Overflow) that functionally matches the programming problems they encounter in their development. Despite extensive research on Q&A sites, being a high-level and important type of development-related knowledge, architectural solutions (e.g., architecture tactics) and their utilization are rarely explored. To fill this gap, we conducted a mixed-methods study that includes a mining study and a survey study. For the mining study, we mined 984 commits and issues (i.e., 821 commits and 163 issues) from 893 Open-Source Software (OSS) projects on GitHub that explicitly referenced architectural solutions from Stack Overflow (SO) and Software Engineering Stack Exchange (SWESE). For the survey study, we identified practitioners involved in the utilization of these architectural solutions and surveyed 227 of them to further understand how practitioners utilize architectural solutions from Q&A sites in their OSS development. Our main findings are that: (1) OSS practitioners use architectural solutions from Q&A sites to solve a large variety (15 categories) of architectural problems, wherein Component design issue, Architectural anti-pattern, and Security issue are dominant; (2) Seven categories of architectural solutions from Q&A sites have been utilized to solve those problems, among which Architectural refactoring, Use of frameworks, and Architectural tactic are the three most utilized architectural solutions; (3) Using architectural solutions from SO comes with a variety of challenges, e.g., OSS practitioners complain that they need to spend significant time to adapt such architectural solutions to address design concerns raised in their OSS development, and it is challenging to use architectural solutions that are not tailored to the design context of their OSS projects.","sentences":["Developers utilize programming-related knowledge (e.g., code snippets) on Q&A sites (e.g., Stack Overflow) that functionally matches the programming problems they encounter in their development.","Despite extensive research on Q&A sites, being a high-level and important type of development-related knowledge, architectural solutions (e.g., architecture tactics) and their utilization are rarely explored.","To fill this gap, we conducted a mixed-methods study that includes a mining study and a survey study.","For the mining study, we mined 984 commits and issues (i.e., 821 commits and 163 issues) from 893 Open-Source Software (OSS) projects on GitHub that explicitly referenced architectural solutions from Stack Overflow (SO) and Software Engineering Stack Exchange (SWESE).","For the survey study, we identified practitioners involved in the utilization of these architectural solutions and surveyed 227 of them to further understand how practitioners utilize architectural solutions from Q&A sites in their OSS development.","Our main findings are that: (1) OSS practitioners use architectural solutions from Q&A sites to solve a large variety (15 categories) of architectural problems, wherein Component design issue, Architectural anti-pattern, and Security issue are dominant; (2) Seven categories of architectural solutions from Q&A sites have been utilized to solve those problems, among which Architectural refactoring, Use of frameworks, and Architectural tactic are the three most utilized architectural solutions; (3) Using architectural solutions from SO comes with a variety of challenges, e.g., OSS practitioners complain that they need to spend significant time to adapt such architectural solutions to address design concerns raised in their OSS development, and it is challenging to use architectural solutions that are not tailored to the design context of their OSS projects."],"url":"http://arxiv.org/abs/2404.05041v1","category":"cs.SE"}
{"created":"2024-04-07 18:52:10","title":"Lagrangian operator inference enhanced with structure-preserving machine learning for nonintrusive model reduction of mechanical systems","abstract":"Complex mechanical systems often exhibit strongly nonlinear behavior due to the presence of nonlinearities in the energy dissipation mechanisms, material constitutive relationships, or geometric/connectivity mechanics. Numerical modeling of these systems leads to nonlinear full-order models that possess an underlying Lagrangian structure. This work proposes a Lagrangian operator inference method enhanced with structure-preserving machine learning to learn nonlinear reduced-order models (ROMs) of nonlinear mechanical systems. This two-step approach first learns the best-fit linear Lagrangian ROM via Lagrangian operator inference and then presents a structure-preserving machine learning method to learn nonlinearities in the reduced space. The proposed approach can learn a structure-preserving nonlinear ROM purely from data, unlike the existing operator inference approaches that require knowledge about the mathematical form of nonlinear terms. From a machine learning perspective, it accelerates the training of the structure-preserving neural network by providing an informed prior, and it reduces the computational cost of the network training by operating on the reduced space. The method is first demonstrated on two simulated examples: a conservative nonlinear rod model and a two-dimensional nonlinear membrane with nonlinear internal damping. Finally, the method is demonstrated on an experimental dataset consisting of digital image correlation measurements taken from a lap-joint beam structure from which a predictive model is learned that captures amplitude-dependent frequency and damping characteristics accurately. The numerical results demonstrate that the proposed approach yields generalizable nonlinear ROMs that exhibit bounded energy error, capture the nonlinear characteristics reliably, and provide accurate long-time predictions outside the training data regime.","sentences":["Complex mechanical systems often exhibit strongly nonlinear behavior due to the presence of nonlinearities in the energy dissipation mechanisms, material constitutive relationships, or geometric/connectivity mechanics.","Numerical modeling of these systems leads to nonlinear full-order models that possess an underlying Lagrangian structure.","This work proposes a Lagrangian operator inference method enhanced with structure-preserving machine learning to learn nonlinear reduced-order models (ROMs) of nonlinear mechanical systems.","This two-step approach first learns the best-fit linear Lagrangian ROM via Lagrangian operator inference and then presents a structure-preserving machine learning method to learn nonlinearities in the reduced space.","The proposed approach can learn a structure-preserving nonlinear ROM purely from data, unlike the existing operator inference approaches that require knowledge about the mathematical form of nonlinear terms.","From a machine learning perspective, it accelerates the training of the structure-preserving neural network by providing an informed prior, and it reduces the computational cost of the network training by operating on the reduced space.","The method is first demonstrated on two simulated examples: a conservative nonlinear rod model and a two-dimensional nonlinear membrane with nonlinear internal damping.","Finally, the method is demonstrated on an experimental dataset consisting of digital image correlation measurements taken from a lap-joint beam structure from which a predictive model is learned that captures amplitude-dependent frequency and damping characteristics accurately.","The numerical results demonstrate that the proposed approach yields generalizable nonlinear ROMs that exhibit bounded energy error, capture the nonlinear characteristics reliably, and provide accurate long-time predictions outside the training data regime."],"url":"http://arxiv.org/abs/2404.05040v1","category":"cs.CE"}
{"created":"2024-04-07 16:49:07","title":"MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators","abstract":"Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions. A largely overlooked problem in T2V is that existing models have not adequately encoded physical knowledge of the real world, thus generated videos tend to have limited motion and poor variations. In this paper, we propose \\textbf{MagicTime}, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic generation. First, we design a MagicAdapter scheme to decouple spatial and temporal training, encode more physical knowledge from metamorphic videos, and transform pre-trained T2V models to generate metamorphic videos. Second, we introduce a Dynamic Frames Extraction strategy to adapt to metamorphic time-lapse videos, which have a wider variation range and cover dramatic object metamorphic processes, thus embodying more physical knowledge than general videos. Finally, we introduce a Magic Text-Encoder to improve the understanding of metamorphic video prompts. Furthermore, we create a time-lapse video-text dataset called \\textbf{ChronoMagic}, specifically curated to unlock the metamorphic video generation ability. Extensive experiments demonstrate the superiority and effectiveness of MagicTime for generating high-quality and dynamic metamorphic videos, suggesting time-lapse video generation is a promising path toward building metamorphic simulators of the physical world.","sentences":["Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions.","A largely overlooked problem in T2V is that existing models have not adequately encoded physical knowledge of the real world, thus generated videos tend to have limited motion and poor variations.","In this paper, we propose \\textbf{MagicTime}, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic generation.","First, we design a MagicAdapter scheme to decouple spatial and temporal training, encode more physical knowledge from metamorphic videos, and transform pre-trained T2V models to generate metamorphic videos.","Second, we introduce a Dynamic Frames Extraction strategy to adapt to metamorphic time-lapse videos, which have a wider variation range and cover dramatic object metamorphic processes, thus embodying more physical knowledge than general videos.","Finally, we introduce a Magic Text-Encoder to improve the understanding of metamorphic video prompts.","Furthermore, we create a time-lapse video-text dataset called \\textbf{ChronoMagic}, specifically curated to unlock the metamorphic video generation ability.","Extensive experiments demonstrate the superiority and effectiveness of MagicTime for generating high-quality and dynamic metamorphic videos, suggesting time-lapse video generation is a promising path toward building metamorphic simulators of the physical world."],"url":"http://arxiv.org/abs/2404.05014v1","category":"cs.CV"}
{"created":"2024-04-07 16:47:18","title":"Inducing a Metal-Insulator Transition through Systematic Alterations of Local Rewriting Rules in a Quantum Graph","abstract":"The Anderson localization transition in quantum graphs has garnered significant recent attention due to its relevance to many-body localization studies. Typically, graphs are constructed using top-down methods. Here, we explore a bottom-up approach, employing a simple local rewriting rule to construct the graph. Through the use of ratio statistics for the energy spectrum and Kullback-Leibler divergence correlations for the eigenstates, numerical analysis demonstrates that slight adjustments to the rewriting rule can induce a transition from a localized to an extended quantum phase. This extended state exhibits non-ergodic behavior, akin to the non-ergodic extended phase observed in the Porter-Rosenzweig model and suggested for many-body localization. Thus, by adapting straightforward local rewriting rules, it becomes feasible to assemble complex graphs from which desired global quantum phases emerge. This approach holds promise for numerical investigations and could be implemented in building optical realizations of complex networks using optical fibers and beam splitters.","sentences":["The Anderson localization transition in quantum graphs has garnered significant recent attention due to its relevance to many-body localization studies.","Typically, graphs are constructed using top-down methods.","Here, we explore a bottom-up approach, employing a simple local rewriting rule to construct the graph.","Through the use of ratio statistics for the energy spectrum and Kullback-Leibler divergence correlations for the eigenstates, numerical analysis demonstrates that slight adjustments to the rewriting rule can induce a transition from a localized to an extended quantum phase.","This extended state exhibits non-ergodic behavior, akin to the non-ergodic extended phase observed in the Porter-Rosenzweig model and suggested for many-body localization.","Thus, by adapting straightforward local rewriting rules, it becomes feasible to assemble complex graphs from which desired global quantum phases emerge.","This approach holds promise for numerical investigations and could be implemented in building optical realizations of complex networks using optical fibers and beam splitters."],"url":"http://arxiv.org/abs/2404.05013v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-07 15:23:28","title":"MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models","abstract":"The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters. Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning. To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German. MLaKE aggregates fact chains from Wikipedia across languages and utilizes LLMs to generate questions in both free-form and multiple-choice. We evaluate the multilingual knowledge editing generalization capabilities of existing methods on MLaKE. Existing knowledge editing methods demonstrate higher success rates in English samples compared to other languages. However, their generalization capabilities are limited in multi-language experiments. Notably, existing knowledge editing methods often show relatively high generalization for languages within the same language family compared to languages from different language families. These results underscore the imperative need for advancements in multilingual knowledge editing and we hope MLaKE can serve as a valuable resource for benchmarking and solution development.","sentences":["The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters.","Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning.","To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German.","MLaKE aggregates fact chains from Wikipedia across languages and utilizes LLMs to generate questions in both free-form and multiple-choice.","We evaluate the multilingual knowledge editing generalization capabilities of existing methods on MLaKE.","Existing knowledge editing methods demonstrate higher success rates in English samples compared to other languages.","However, their generalization capabilities are limited in multi-language experiments.","Notably, existing knowledge editing methods often show relatively high generalization for languages within the same language family compared to languages from different language families.","These results underscore the imperative need for advancements in multilingual knowledge editing and we hope MLaKE can serve as a valuable resource for benchmarking and solution development."],"url":"http://arxiv.org/abs/2404.04990v1","category":"cs.CL"}
{"created":"2024-04-07 13:02:21","title":"SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning","abstract":"Large language models (LLMs) are increasingly being applied across various specialized fields, leveraging their extensive knowledge to empower a multitude of scenarios within these domains. However, each field encompasses a variety of specific tasks that require learning, and the diverse, heterogeneous data across these domains can lead to conflicts during model task transfer. In response to this challenge, our study introduces an Adaptive Semantic Space Learning (ASSL) framework, which utilizes the adaptive reorganization of data distributions within the semantic space to enhance the performance and selection efficacy of multi-expert models. Utilizing this framework, we trained a financial multi-task LLM named \"SilverSight\". Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities.","sentences":["Large language models (LLMs) are increasingly being applied across various specialized fields, leveraging their extensive knowledge to empower a multitude of scenarios within these domains.","However, each field encompasses a variety of specific tasks that require learning, and the diverse, heterogeneous data across these domains can lead to conflicts during model task transfer.","In response to this challenge, our study introduces an Adaptive Semantic Space Learning (ASSL) framework, which utilizes the adaptive reorganization of data distributions within the semantic space to enhance the performance and selection efficacy of multi-expert models.","Utilizing this framework, we trained a financial multi-task LLM named \"SilverSight\".","Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities."],"url":"http://arxiv.org/abs/2404.04949v1","category":"cs.CL"}
{"created":"2024-04-07 12:57:41","title":"AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment","abstract":"Recent video editing advancements rely on accurate pose sequences to animate subjects. However, these efforts are not suitable for cross-species animation due to pose misalignment between species (for example, the poses of a cat differs greatly from that of a pig due to differences in body structure). In this paper, we present AnimateZoo, a zero-shot diffusion-based video generator to address this challenging cross-species animation issue, aiming to accurately produce animal animations while preserving the background. The key technique used in our AnimateZoo is subject alignment, which includes two steps. First, we improve appearance feature extraction by integrating a Laplacian detail booster and a prompt-tuning identity extractor. These components are specifically designed to capture essential appearance information, including identity and fine details. Second, we align shape features and address conflicts from differing subjects by introducing a scale-information remover. This ensures accurate cross-species animation. Moreover, we introduce two high-quality animal video datasets featuring a wide variety of species. Trained on these extensive datasets, our model is capable of generating videos characterized by accurate movements, consistent appearance, and high-fidelity frames, without the need for the pre-inference fine-tuning that prior arts required. Extensive experiments showcase the outstanding performance of our method in cross-species action following tasks, demonstrating exceptional shape adaptation capability. The project page is available at https://justinxu0.github.io/AnimateZoo/.","sentences":["Recent video editing advancements rely on accurate pose sequences to animate subjects.","However, these efforts are not suitable for cross-species animation due to pose misalignment between species (for example, the poses of a cat differs greatly from that of a pig due to differences in body structure).","In this paper, we present AnimateZoo, a zero-shot diffusion-based video generator to address this challenging cross-species animation issue, aiming to accurately produce animal animations while preserving the background.","The key technique used in our AnimateZoo is subject alignment, which includes two steps.","First, we improve appearance feature extraction by integrating a Laplacian detail booster and a prompt-tuning identity extractor.","These components are specifically designed to capture essential appearance information, including identity and fine details.","Second, we align shape features and address conflicts from differing subjects by introducing a scale-information remover.","This ensures accurate cross-species animation.","Moreover, we introduce two high-quality animal video datasets featuring a wide variety of species.","Trained on these extensive datasets, our model is capable of generating videos characterized by accurate movements, consistent appearance, and high-fidelity frames, without the need for the pre-inference fine-tuning that prior arts required.","Extensive experiments showcase the outstanding performance of our method in cross-species action following tasks, demonstrating exceptional shape adaptation capability.","The project page is available at https://justinxu0.github.io/AnimateZoo/."],"url":"http://arxiv.org/abs/2404.04946v1","category":"cs.CV"}
{"created":"2024-04-07 10:49:59","title":"CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis","abstract":"Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, several factors have impeded its further proliferation as next-generation 3D media. To establish a ubiquitous presence in everyday media formats, such as images and videos, it is imperative to devise a solution that effectively fulfills three key objectives: fast encoding and decoding time, compact model sizes, and high-quality renderings. Despite significant advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of a novel encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we develop a novel finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 150x and 20x reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets, such as ShapeNet and Objaverse.","sentences":["Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes.","However, several factors have impeded its further proliferation as next-generation 3D media.","To establish a ubiquitous presence in everyday media formats, such as images and videos, it is imperative to devise a solution that effectively fulfills three key objectives: fast encoding and decoding time, compact model sizes, and high-quality renderings.","Despite significant advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized.","In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of a novel encoder and decoder architecture that can generate a NeRF representation in a single forward pass.","Furthermore, inspired by the recent parameter-efficient finetuning approaches, we develop a novel finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes.","The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 150x and 20x reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets, such as ShapeNet and Objaverse."],"url":"http://arxiv.org/abs/2404.04913v1","category":"cs.CV"}
{"created":"2024-04-07 09:59:10","title":"Cracking and complexity of self-gravitating dissipative compact objects","abstract":"The concept of cracking refers to the tendency of a fluid distribution to \"split'', once it abandons the equilibrium. In this manuscript we develop a general formalism to describe the occurrence of cracking within a dissipative fluid distribution, in comoving coordinates. The role of dissipative processes in the occurrence of cracking is brought out. Next, we relate the occurrence of cracking with the concept of complexity for self-gravitating objects defined in [1-3]. More specifically we relate the occurrence of cracking with the condition of the vanishing of the scalar function intended to measure the complexity of the fluid distribution (the complexity factor). We also relate the occurrence of cracking with the specific mode of leaving the equilibrium. Thus, we prove that leaving the equilibrium in either, the homologous (H), or the quasi--homologous regime (QH), prevents the occurrence of cracking. Also, it is shown that imposing the condition of vanishing complexity factor alone, (independently of the mode of leaving the equilibrium) prevents the occurrence of cracking in the non-dissipative geodesic case, and in the non-dissipative isotropic case. These results bring out further the relevance of the complexity factor and its related definition of complexity, in the study of self-gravitating systems.","sentences":["The concept of cracking refers to the tendency of a fluid distribution to \"split'', once it abandons the equilibrium.","In this manuscript we develop a general formalism to describe the occurrence of cracking within a dissipative fluid distribution, in comoving coordinates.","The role of dissipative processes in the occurrence of cracking is brought out.","Next, we relate the occurrence of cracking with the concept of complexity for self-gravitating objects defined in [1-3].","More specifically we relate the occurrence of cracking with the condition of the vanishing of the scalar function intended to measure the complexity of the fluid distribution (the complexity factor).","We also relate the occurrence of cracking with the specific mode of leaving the equilibrium.","Thus, we prove that leaving the equilibrium in either, the homologous (H), or the quasi--homologous regime (QH), prevents the occurrence of cracking.","Also, it is shown that imposing the condition of vanishing complexity factor alone, (independently of the mode of leaving the equilibrium) prevents the occurrence of cracking in the non-dissipative geodesic case, and in the non-dissipative isotropic case.","These results bring out further the relevance of the complexity factor and its related definition of complexity, in the study of self-gravitating systems."],"url":"http://arxiv.org/abs/2404.04901v1","category":"gr-qc"}
{"created":"2024-04-07 09:32:14","title":"Tensorized Ant Colony Optimization for GPU Acceleration","abstract":"Ant Colony Optimization (ACO) is renowned for its effectiveness in solving Traveling Salesman Problems, yet it faces computational challenges in CPU-based environments, particularly with large-scale instances. In response, we introduce a Tensorized Ant Colony Optimization (TensorACO) to utilize the advancements of GPU acceleration. As the core, TensorACO fully transforms ant system and ant path into tensor forms, a process we refer to as tensorization. For the tensorization of ant system, we propose a preprocessing method to reduce the computational overhead by calculating the probability transition matrix. In the tensorization of ant path, we propose an index mapping method to accelerate the update of pheromone matrix by replacing the mechanism of sequential path update with parallel matrix operations. Additionally, we introduce an Adaptive Independent Roulette (AdaIR) method to overcome the challenges of parallelizing ACO's selection mechanism on GPUs. Comprehensive experiments demonstrate the superior performance of TensorACO achieving up to 1921$\\times$ speedup over standard ACO. Moreover, the AdaIR method further improves TensorACO's convergence speed by 80% and solution quality by 2%. Source codes are available at https://github.com/EMI-Group/tensoraco.","sentences":["Ant Colony Optimization (ACO) is renowned for its effectiveness in solving Traveling Salesman Problems, yet it faces computational challenges in CPU-based environments, particularly with large-scale instances.","In response, we introduce a Tensorized Ant Colony Optimization (TensorACO) to utilize the advancements of GPU acceleration.","As the core, TensorACO fully transforms ant system and ant path into tensor forms, a process we refer to as tensorization.","For the tensorization of ant system, we propose a preprocessing method to reduce the computational overhead by calculating the probability transition matrix.","In the tensorization of ant path, we propose an index mapping method to accelerate the update of pheromone matrix by replacing the mechanism of sequential path update with parallel matrix operations.","Additionally, we introduce an Adaptive Independent Roulette (AdaIR) method to overcome the challenges of parallelizing ACO's selection mechanism on GPUs.","Comprehensive experiments demonstrate the superior performance of TensorACO achieving up to 1921$\\times$ speedup over standard ACO.","Moreover, the AdaIR method further improves TensorACO's convergence speed by 80% and solution quality by 2%.","Source codes are available at https://github.com/EMI-Group/tensoraco."],"url":"http://arxiv.org/abs/2404.04895v1","category":"cs.NE"}
{"created":"2024-04-07 09:05:09","title":"TimeGPT in Load Forecasting: A Large Time Series Model Perspective","abstract":"Machine learning models have made significant progress in load forecasting, but their forecast accuracy is limited in cases where historical load data is scarce. Inspired by the outstanding performance of large language models (LLMs) in computer vision and natural language processing, this paper aims to discuss the potential of large time series models in load forecasting with scarce historical data. Specifically, the large time series model is constructed as a time series generative pre-trained transformer (TimeGPT), which is trained on massive and diverse time series datasets consisting of 100 billion data points (e.g., finance, transportation, banking, web traffic, weather, energy, healthcare, etc.). Then, the scarce historical load data is used to fine-tune the TimeGPT, which helps it to adapt to the data distribution and characteristics associated with load forecasting. Simulation results show that TimeGPT outperforms the benchmarks (e.g., popular machine learning models and statistical models) for load forecasting on several real datasets with scarce training samples, particularly for short look-ahead times. However, it cannot be guaranteed that TimeGPT is always superior to benchmarks for load forecasting with scarce data, since the performance of TimeGPT may be affected by the distribution differences between the load data and the training data. In practical applications, we can divide the historical data into a training set and a validation set, and then use the validation set loss to decide whether TimeGPT is the best choice for a specific dataset.","sentences":["Machine learning models have made significant progress in load forecasting, but their forecast accuracy is limited in cases where historical load data is scarce.","Inspired by the outstanding performance of large language models (LLMs) in computer vision and natural language processing, this paper aims to discuss the potential of large time series models in load forecasting with scarce historical data.","Specifically, the large time series model is constructed as a time series generative pre-trained transformer (TimeGPT), which is trained on massive and diverse time series datasets consisting of 100 billion data points (e.g., finance, transportation, banking, web traffic, weather, energy, healthcare, etc.).","Then, the scarce historical load data is used to fine-tune the TimeGPT, which helps it to adapt to the data distribution and characteristics associated with load forecasting.","Simulation results show that TimeGPT outperforms the benchmarks (e.g., popular machine learning models and statistical models) for load forecasting on several real datasets with scarce training samples, particularly for short look-ahead times.","However, it cannot be guaranteed that TimeGPT is always superior to benchmarks for load forecasting with scarce data, since the performance of TimeGPT may be affected by the distribution differences between the load data and the training data.","In practical applications, we can divide the historical data into a training set and a validation set, and then use the validation set loss to decide whether TimeGPT is the best choice for a specific dataset."],"url":"http://arxiv.org/abs/2404.04885v1","category":"cs.LG"}
{"created":"2024-04-07 09:01:50","title":"Mixture of Low-rank Experts for Transferable AI-Generated Image Detection","abstract":"Generative models have shown a giant leap in synthesizing photo-realistic images with minimal expertise, sparking concerns about the authenticity of online information. This study aims to develop a universal AI-generated image detector capable of identifying images from diverse sources. Existing methods struggle to generalize across unseen generative models when provided with limited sample sources. Inspired by the zero-shot transferability of pre-trained vision-language models, we seek to harness the nontrivial visual-world knowledge and descriptive proficiency of CLIP-ViT to generalize over unknown domains. This paper presents a novel parameter-efficient fine-tuning approach, mixture of low-rank experts, to fully exploit CLIP-ViT's potential while preserving knowledge and expanding capacity for transferable detection. We adapt only the MLP layers of deeper ViT blocks via an integration of shared and separate LoRAs within an MoE-based structure. Extensive experiments on public benchmarks show that our method achieves superiority over state-of-the-art approaches in cross-generator generalization and robustness to perturbations. Remarkably, our best-performing ViT-L/14 variant requires training only 0.08% of its parameters to surpass the leading baseline by +3.64% mAP and +12.72% avg.Acc across unseen diffusion and autoregressive models. This even outperforms the baseline with just 0.28% of the training data. Our code and pre-trained models will be available at https://github.com/zhliuworks/CLIPMoLE.","sentences":["Generative models have shown a giant leap in synthesizing photo-realistic images with minimal expertise, sparking concerns about the authenticity of online information.","This study aims to develop a universal AI-generated image detector capable of identifying images from diverse sources.","Existing methods struggle to generalize across unseen generative models when provided with limited sample sources.","Inspired by the zero-shot transferability of pre-trained vision-language models, we seek to harness the nontrivial visual-world knowledge and descriptive proficiency of CLIP-ViT to generalize over unknown domains.","This paper presents a novel parameter-efficient fine-tuning approach, mixture of low-rank experts, to fully exploit CLIP-ViT's potential while preserving knowledge and expanding capacity for transferable detection.","We adapt only the MLP layers of deeper ViT blocks via an integration of shared and separate LoRAs within an MoE-based structure.","Extensive experiments on public benchmarks show that our method achieves superiority over state-of-the-art approaches in cross-generator generalization and robustness to perturbations.","Remarkably, our best-performing ViT-L/14 variant requires training only 0.08% of its parameters to surpass the leading baseline by +3.64% mAP and +12.72% avg.","Acc across unseen diffusion and autoregressive models.","This even outperforms the baseline with just 0.28% of the training data.","Our code and pre-trained models will be available at https://github.com/zhliuworks/CLIPMoLE."],"url":"http://arxiv.org/abs/2404.04883v1","category":"cs.CV"}
{"created":"2024-04-07 08:04:33","title":"Learning Adaptive Multi-Objective Robot Navigation with Demonstrations","abstract":"Preference-aligned robot navigation in human environments is typically achieved through learning-based approaches, utilizing demonstrations and user feedback for personalization. However, personal preferences are subject to change and might even be context-dependent. Yet traditional reinforcement learning (RL) approaches with a static reward function often fall short in adapting to these varying user preferences. This paper introduces a framework that combines multi-objective reinforcement learning (MORL) with demonstration-based learning. Our approach allows for dynamic adaptation to changing user preferences without retraining. Through rigorous evaluations, including sim-to-real and robot-to-robot transfers, we demonstrate our framework's capability to reflect user preferences accurately while achieving high navigational performance in terms of collision avoidance and goal pursuance.","sentences":["Preference-aligned robot navigation in human environments is typically achieved through learning-based approaches, utilizing demonstrations and user feedback for personalization.","However, personal preferences are subject to change and might even be context-dependent.","Yet traditional reinforcement learning (RL) approaches with a static reward function often fall short in adapting to these varying user preferences.","This paper introduces a framework that combines multi-objective reinforcement learning (MORL) with demonstration-based learning.","Our approach allows for dynamic adaptation to changing user preferences without retraining.","Through rigorous evaluations, including sim-to-real and robot-to-robot transfers, we demonstrate our framework's capability to reflect user preferences accurately while achieving high navigational performance in terms of collision avoidance and goal pursuance."],"url":"http://arxiv.org/abs/2404.04857v1","category":"cs.RO"}
{"created":"2024-04-07 06:52:20","title":"Robotic Sorting Systems: Robot Management and Layout Design Optimization","abstract":"In the contemporary logistics industry, automation plays a pivotal role in enhancing production efficiency and expanding industrial scale. Autonomous mobile robots, in particular, have become integral to the modernization efforts in warehouses. One noteworthy application in robotic warehousing is the robotic sorting system (RSS), distinguished by its characteristics such as cost-effectiveness, simplicity, scalability, and adaptable throughput control. While previous research has focused on analyzing the efficiency of RSS, it often assumed an ideal robot management system ignoring potential queuing delays by assuming constant travel times. This study relaxes this assumption and explores the quantitative relationship between RSS configuration parameters and system throughput. We introduce a novel robot traffic management method, named the rhythmic control for sorting scenario (RC-S), for RSS operations, equipped with an estimation formula establishing the relationship between system performance and configurations. Simulations validate that RC-S reduces average service time by 10.3\\% compared to the classical cooperative A* algorithm, while also improving throughput and runtime. Based on the performance analysis of RC-S, we further develop a layout optimization model for RSS, considering RSS configuration, desired throughput, and costs, to minimize expenses and determine the best layout. Numerical studies show that at lower throughput levels, facility costs dominate, while at higher throughput levels, labor costs prevail. Additionally, due to traffic efficiency limitations, RSS is well-suited for small-scale operations like end-of-supply-chain distribution centers.","sentences":["In the contemporary logistics industry, automation plays a pivotal role in enhancing production efficiency and expanding industrial scale.","Autonomous mobile robots, in particular, have become integral to the modernization efforts in warehouses.","One noteworthy application in robotic warehousing is the robotic sorting system (RSS), distinguished by its characteristics such as cost-effectiveness, simplicity, scalability, and adaptable throughput control.","While previous research has focused on analyzing the efficiency of RSS, it often assumed an ideal robot management system ignoring potential queuing delays by assuming constant travel times.","This study relaxes this assumption and explores the quantitative relationship between RSS configuration parameters and system throughput.","We introduce a novel robot traffic management method, named the rhythmic control for sorting scenario (RC-S), for RSS operations, equipped with an estimation formula establishing the relationship between system performance and configurations.","Simulations validate that RC-S reduces average service time by 10.3\\% compared to the classical cooperative A* algorithm, while also improving throughput and runtime.","Based on the performance analysis of RC-S, we further develop a layout optimization model for RSS, considering RSS configuration, desired throughput, and costs, to minimize expenses and determine the best layout.","Numerical studies show that at lower throughput levels, facility costs dominate, while at higher throughput levels, labor costs prevail.","Additionally, due to traffic efficiency limitations, RSS is well-suited for small-scale operations like end-of-supply-chain distribution centers."],"url":"http://arxiv.org/abs/2404.04832v1","category":"math.OC"}
{"created":"2024-04-07 04:56:58","title":"MemFlow: Optical Flow Estimation and Prediction with Memory","abstract":"Optical flow is a classical task that is important to the vision community. Classical optical flow estimation uses two frames as input, whilst some recent methods consider multiple frames to explicitly model long-range information. The former ones limit their ability to fully leverage temporal coherence along the video sequence; and the latter ones incur heavy computational overhead, typically not possible for real-time flow estimation. Some multi-frame-based approaches even necessitate unseen future frames for current estimation, compromising real-time applicability in safety-critical scenarios. To this end, we present MemFlow, a real-time method for optical flow estimation and prediction with memory. Our method enables memory read-out and update modules for aggregating historical motion information in real-time. Furthermore, we integrate resolution-adaptive re-scaling to accommodate diverse video resolutions. Besides, our approach seamlessly extends to the future prediction of optical flow based on past observations. Leveraging effective historical motion aggregation, our method outperforms VideoFlow with fewer parameters and faster inference speed on Sintel and KITTI-15 datasets in terms of generalization performance. At the time of submission, MemFlow also leads in performance on the 1080p Spring dataset. Codes and models will be available at: https://dqiaole.github.io/MemFlow/.","sentences":["Optical flow is a classical task that is important to the vision community.","Classical optical flow estimation uses two frames as input, whilst some recent methods consider multiple frames to explicitly model long-range information.","The former ones limit their ability to fully leverage temporal coherence along the video sequence; and the latter ones incur heavy computational overhead, typically not possible for real-time flow estimation.","Some multi-frame-based approaches even necessitate unseen future frames for current estimation, compromising real-time applicability in safety-critical scenarios.","To this end, we present MemFlow, a real-time method for optical flow estimation and prediction with memory.","Our method enables memory read-out and update modules for aggregating historical motion information in real-time.","Furthermore, we integrate resolution-adaptive re-scaling to accommodate diverse video resolutions.","Besides, our approach seamlessly extends to the future prediction of optical flow based on past observations.","Leveraging effective historical motion aggregation, our method outperforms VideoFlow with fewer parameters and faster inference speed on Sintel and KITTI-15 datasets in terms of generalization performance.","At the time of submission, MemFlow also leads in performance on the 1080p Spring dataset.","Codes and models will be available at: https://dqiaole.github.io/MemFlow/."],"url":"http://arxiv.org/abs/2404.04808v1","category":"cs.CV"}
{"created":"2024-04-07 04:55:58","title":"D2SL: Decouple Defogging and Semantic Learning for Foggy Domain-Adaptive Segmentation","abstract":"We investigated domain adaptive semantic segmentation in foggy weather scenarios, which aims to enhance the utilization of unlabeled foggy data and improve the model's adaptability to foggy conditions. Current methods rely on clear images as references, jointly learning defogging and segmentation for foggy images. Despite making some progress, there are still two main drawbacks: (1) the coupling of segmentation and defogging feature representations, resulting in a decrease in semantic representation capability, and (2) the failure to leverage real fog priors in unlabeled foggy data, leading to insufficient model generalization ability. To address these issues, we propose a novel training framework, Decouple Defogging and Semantic learning, called D2SL, aiming to alleviate the adverse impact of defogging tasks on the final segmentation task. In this framework, we introduce a domain-consistent transfer strategy to establish a connection between defogging and segmentation tasks. Furthermore, we design a real fog transfer strategy to improve defogging effects by fully leveraging the fog priors from real foggy images. Our approach enhances the semantic representations required for segmentation during the defogging learning process and maximizes the representation capability of fog invariance by effectively utilizing real fog data. Comprehensive experiments validate the effectiveness of the proposed method.","sentences":["We investigated domain adaptive semantic segmentation in foggy weather scenarios, which aims to enhance the utilization of unlabeled foggy data and improve the model's adaptability to foggy conditions.","Current methods rely on clear images as references, jointly learning defogging and segmentation for foggy images.","Despite making some progress, there are still two main drawbacks: (1) the coupling of segmentation and defogging feature representations, resulting in a decrease in semantic representation capability, and (2) the failure to leverage real fog priors in unlabeled foggy data, leading to insufficient model generalization ability.","To address these issues, we propose a novel training framework, Decouple Defogging and Semantic learning, called D2SL, aiming to alleviate the adverse impact of defogging tasks on the final segmentation task.","In this framework, we introduce a domain-consistent transfer strategy to establish a connection between defogging and segmentation tasks.","Furthermore, we design a real fog transfer strategy to improve defogging effects by fully leveraging the fog priors from real foggy images.","Our approach enhances the semantic representations required for segmentation during the defogging learning process and maximizes the representation capability of fog invariance by effectively utilizing real fog data.","Comprehensive experiments validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.04807v1","category":"cs.CV"}
{"created":"2024-04-07 03:37:29","title":"Few-Shot Object Detection: Research Advances and Challenges","abstract":"Object detection as a subfield within computer vision has achieved remarkable progress, which aims to accurately identify and locate a specific object from images or videos. Such methods rely on large-scale labeled training samples for each object category to ensure accurate detection, but obtaining extensive annotated data is a labor-intensive and expensive process in many real-world scenarios. To tackle this challenge, researchers have explored few-shot object detection (FSOD) that combines few-shot learning and object detection techniques to rapidly adapt to novel objects with limited annotated samples. This paper presents a comprehensive survey to review the significant advancements in the field of FSOD in recent years and summarize the existing challenges and solutions. Specifically, we first introduce the background and definition of FSOD to emphasize potential value in advancing the field of computer vision. We then propose a novel FSOD taxonomy method and survey the plentifully remarkable FSOD algorithms based on this fact to report a comprehensive overview that facilitates a deeper understanding of the FSOD problem and the development of innovative solutions. Finally, we discuss the advantages and limitations of these algorithms to summarize the challenges, potential research direction, and development trend of object detection in the data scarcity scenario.","sentences":["Object detection as a subfield within computer vision has achieved remarkable progress, which aims to accurately identify and locate a specific object from images or videos.","Such methods rely on large-scale labeled training samples for each object category to ensure accurate detection, but obtaining extensive annotated data is a labor-intensive and expensive process in many real-world scenarios.","To tackle this challenge, researchers have explored few-shot object detection (FSOD) that combines few-shot learning and object detection techniques to rapidly adapt to novel objects with limited annotated samples.","This paper presents a comprehensive survey to review the significant advancements in the field of FSOD in recent years and summarize the existing challenges and solutions.","Specifically, we first introduce the background and definition of FSOD to emphasize potential value in advancing the field of computer vision.","We then propose a novel FSOD taxonomy method and survey the plentifully remarkable FSOD algorithms based on this fact to report a comprehensive overview that facilitates a deeper understanding of the FSOD problem and the development of innovative solutions.","Finally, we discuss the advantages and limitations of these algorithms to summarize the challenges, potential research direction, and development trend of object detection in the data scarcity scenario."],"url":"http://arxiv.org/abs/2404.04799v1","category":"cs.CV"}
{"created":"2024-04-07 01:37:00","title":"Soft-in Soft-out Decoding of Spherical Codes from Cartesian Powers of PAM Constellations","abstract":"For applications in concatenated coding for optical communications systems, we examine the encoding and soft-decoding of short spherical codes constructed as constant-energy shells of the Cartesian power of pulse amplitude modulation constellations. These are unions of permutation codes having the same average power. We construct a list decoder for permutation codes by adapting Murty's algorithm, which is then used to determine mutual information curves for these permutation codes. In the process, we discover a straightforward expression for determining the likelihood of large subcodes of permutation codes. We refer to these subcodes, obtained by all possible sign flips of a given permutation codeword, as orbits. We introduce a simple process, which we call orbit decoding with frozen symbols, that allows us to extract soft information from noisy permutation codewords. In a sample communication system with probabilistic amplitude shaping protected by a standard low-density parity-check code that employs short permutation codes, we demonstrate that orbit decoding with frozen symbols provides a gain of about 0.3 dB in signal-to-noise ratio compared to the traditional symbol-by-symbol decoding. By using spherical codes composed of unions of permutation codes, we can increase the input entropy compared to using permutation codes alone. In one scheme, we consider a union of a small number of permutation codes. In this case, orbit decoding with frozen symbols provides about 0.2 dB gain compared to the traditional method. In another scheme, we use all possible permutations to form a spherical code that exhibits a computationally feasible trellis representation. The soft information obtained using the BCJR algorithm outperforms the traditional symbol-by-symbol method by 0.1 dB.","sentences":["For applications in concatenated coding for optical communications systems, we examine the encoding and soft-decoding of short spherical codes constructed as constant-energy shells of the Cartesian power of pulse amplitude modulation constellations.","These are unions of permutation codes having the same average power.","We construct a list decoder for permutation codes by adapting Murty's algorithm, which is then used to determine mutual information curves for these permutation codes.","In the process, we discover a straightforward expression for determining the likelihood of large subcodes of permutation codes.","We refer to these subcodes, obtained by all possible sign flips of a given permutation codeword, as orbits.","We introduce a simple process, which we call orbit decoding with frozen symbols, that allows us to extract soft information from noisy permutation codewords.","In a sample communication system with probabilistic amplitude shaping protected by a standard low-density parity-check code that employs short permutation codes, we demonstrate that orbit decoding with frozen symbols provides a gain of about 0.3 dB in signal-to-noise ratio compared to the traditional symbol-by-symbol decoding.","By using spherical codes composed of unions of permutation codes, we can increase the input entropy compared to using permutation codes alone.","In one scheme, we consider a union of a small number of permutation codes.","In this case, orbit decoding with frozen symbols provides about 0.2 dB gain compared to the traditional method.","In another scheme, we use all possible permutations to form a spherical code that exhibits a computationally feasible trellis representation.","The soft information obtained using the BCJR algorithm outperforms the traditional symbol-by-symbol method by 0.1 dB."],"url":"http://arxiv.org/abs/2404.04776v1","category":"cs.IT"}
{"created":"2024-04-06 22:08:20","title":"Collaborative Feedback Discriminative Propagation for Video Super-Resolution","abstract":"The key success of existing video super-resolution (VSR) methods stems mainly from exploring spatial and temporal information, which is usually achieved by a recurrent propagation module with an alignment module. However, inaccurate alignment usually leads to aligned features with significant artifacts, which will be accumulated during propagation and thus affect video restoration. Moreover, propagation modules only propagate the same timestep features forward or backward that may fail in case of complex motion or occlusion, limiting their performance for high-quality frame restoration. To address these issues, we propose a collaborative feedback discriminative (CFD) method to correct inaccurate aligned features and model long -range spatial and temporal information for better video reconstruction. In detail, we develop a discriminative alignment correction (DAC) method to adaptively explore information and reduce the influences of the artifacts caused by inaccurate alignment. Then, we propose a collaborative feedback propagation (CFP) module that employs feedback and gating mechanisms to better explore spatial and temporal information of different timestep features from forward and backward propagation simultaneously. Finally, we embed the proposed DAC and CFP into commonly used VSR networks to verify the effectiveness of our method. Quantitative and qualitative experiments on several benchmarks demonstrate that our method can improve the performance of existing VSR models while maintaining a lower model complexity. The source code and pre-trained models will be available at \\url{https://github.com/House-Leo/CFDVSR}.","sentences":["The key success of existing video super-resolution (VSR) methods stems mainly from exploring spatial and temporal information, which is usually achieved by a recurrent propagation module with an alignment module.","However, inaccurate alignment usually leads to aligned features with significant artifacts, which will be accumulated during propagation and thus affect video restoration.","Moreover, propagation modules only propagate the same timestep features forward or backward that may fail in case of complex motion or occlusion, limiting their performance for high-quality frame restoration.","To address these issues, we propose a collaborative feedback discriminative (CFD) method to correct inaccurate aligned features and model long -range spatial and temporal information for better video reconstruction.","In detail, we develop a discriminative alignment correction (DAC) method to adaptively explore information and reduce the influences of the artifacts caused by inaccurate alignment.","Then, we propose a collaborative feedback propagation (CFP) module that employs feedback and gating mechanisms to better explore spatial and temporal information of different timestep features from forward and backward propagation simultaneously.","Finally, we embed the proposed DAC and CFP into commonly used VSR networks to verify the effectiveness of our method.","Quantitative and qualitative experiments on several benchmarks demonstrate that our method can improve the performance of existing VSR models while maintaining a lower model complexity.","The source code and pre-trained models will be available at \\url{https://github.com/House-Leo/CFDVSR}."],"url":"http://arxiv.org/abs/2404.04745v1","category":"cs.CV"}
{"created":"2024-04-06 22:05:09","title":"From Batch to Stream: Automatic Generation of Online Algorithms","abstract":"Online streaming algorithms, tailored for continuous data processing, offer substantial benefits but are often more intricate to design than their offline counterparts. This paper introduces a novel approach for automatically synthesizing online streaming algorithms from their offline versions. In particular, we propose a novel methodology, based on the notion of relational function signature (RFS), for deriving an online algorithm given its offline version. Then, we propose a concrete synthesis algorithm that is an instantiation of the proposed methodology. Our algorithm uses the RFS to decompose the synthesis problem into a set of independent subtasks and uses a combination of symbolic reasoning and search to solve each subproblem. We implement the proposed technique in a new tool called Opera and evaluate it on over 50 tasks spanning two domains: statistical computations and online auctions. Our results show that Opera can automatically derive the online version of the original algorithm for 98% of the tasks. Our experiments also demonstrate that Opera significantly outperforms alternative approaches, including adaptations of SyGuS solvers to this problem as well as two of Opera's own ablations.","sentences":["Online streaming algorithms, tailored for continuous data processing, offer substantial benefits but are often more intricate to design than their offline counterparts.","This paper introduces a novel approach for automatically synthesizing online streaming algorithms from their offline versions.","In particular, we propose a novel methodology, based on the notion of relational function signature (RFS), for deriving an online algorithm given its offline version.","Then, we propose a concrete synthesis algorithm that is an instantiation of the proposed methodology.","Our algorithm uses the RFS to decompose the synthesis problem into a set of independent subtasks and uses a combination of symbolic reasoning and search to solve each subproblem.","We implement the proposed technique in a new tool called Opera and evaluate it on over 50 tasks spanning two domains: statistical computations and online auctions.","Our results show that Opera can automatically derive the online version of the original algorithm for 98% of the tasks.","Our experiments also demonstrate that Opera significantly outperforms alternative approaches, including adaptations of SyGuS solvers to this problem as well as two of Opera's own ablations."],"url":"http://arxiv.org/abs/2404.04743v1","category":"cs.PL"}
{"created":"2024-04-06 21:25:39","title":"Ultrafast dynamic beam steering with optical frequency comb arrays","abstract":"Efficient spatiotemporal control of optical beams is of paramount importance in diverse technological domains. Conventional systems focusing on quasi-static beam control demand precise phase or wavelength tuning for steering. This work presents a time-efficient solution for dynamic beam steering, emphasizing high-duty-cycle operation with fast scan rates, and eliminating the need for active tuning of the beam direction. We achieve 100%-duty-cycle scans at a rate of $\\sim$9.8 GHz within an angular range of $\\sim$1$^\\circ$. Furthermore, leveraging the dispersion characteristics of a virtually imaged phased array (VIPA), we devise a broadband source array that seamlessly transitions from continuous-angular steering to pulsed discrete-angular operation, unlocking possibilities for high-sensitivity angle-, range-, and time-resolved imaging. We also elucidate the adaptability of integrated photonic designs incorporating wavelength-selective switches and spectral dispersers, for enabling a versatile on-chip realization of the proposed beam steering schemes.","sentences":["Efficient spatiotemporal control of optical beams is of paramount importance in diverse technological domains.","Conventional systems focusing on quasi-static beam control demand precise phase or wavelength tuning for steering.","This work presents a time-efficient solution for dynamic beam steering, emphasizing high-duty-cycle operation with fast scan rates, and eliminating the need for active tuning of the beam direction.","We achieve 100%-duty-cycle scans at a rate of $\\sim$9.8 GHz within an angular range of $\\sim$1$^\\circ$. Furthermore, leveraging the dispersion characteristics of a virtually imaged phased array (VIPA), we devise a broadband source array that seamlessly transitions from continuous-angular steering to pulsed discrete-angular operation, unlocking possibilities for high-sensitivity angle-, range-, and time-resolved imaging.","We also elucidate the adaptability of integrated photonic designs incorporating wavelength-selective switches and spectral dispersers, for enabling a versatile on-chip realization of the proposed beam steering schemes."],"url":"http://arxiv.org/abs/2404.04732v1","category":"physics.optics"}
{"created":"2024-04-06 20:34:14","title":"Convolutional Neural Network Transformer (CNNT) for Fluorescence Microscopy image Denoising with Improved Generalization and Fast Adaptation","abstract":"Deep neural networks have been applied to improve the image quality of fluorescence microscopy imaging. Previous methods are based on convolutional neural networks (CNNs) which generally require more time-consuming training of separate models for each new imaging experiment, impairing the applicability and generalization. Once the model is trained (typically with tens to hundreds of image pairs) it can then be used to enhance new images that are like the training data. In this study, we proposed a novel imaging-transformer based model, Convolutional Neural Network Transformer (CNNT), to outperform the CNN networks for image denoising. In our scheme we have trained a single CNNT based backbone model from pairwise high-low SNR images for one type of fluorescence microscope (instance structured illumination, iSim). Fast adaption to new applications was achieved by fine-tuning the backbone on only 5-10 sample pairs per new experiment. Results show the CNNT backbone and fine-tuning scheme significantly reduces the training time and improves the image quality, outperformed training separate models using CNN approaches such as - RCAN and Noise2Fast. Here we show three examples of the efficacy of this approach on denoising wide-field, two-photon and confocal fluorescence data. In the confocal experiment, which is a 5 by 5 tiled acquisition, the fine-tuned CNNT model reduces the scan time form one hour to eight minutes, with improved quality.","sentences":["Deep neural networks have been applied to improve the image quality of fluorescence microscopy imaging.","Previous methods are based on convolutional neural networks (CNNs) which generally require more time-consuming training of separate models for each new imaging experiment, impairing the applicability and generalization.","Once the model is trained (typically with tens to hundreds of image pairs)","it can then be used to enhance new images that are like the training data.","In this study, we proposed a novel imaging-transformer based model, Convolutional Neural Network Transformer (CNNT), to outperform the CNN networks for image denoising.","In our scheme we have trained a single CNNT based backbone model from pairwise high-low SNR images for one type of fluorescence microscope (instance structured illumination, iSim).","Fast adaption to new applications was achieved by fine-tuning the backbone on only 5-10 sample pairs per new experiment.","Results show the CNNT backbone and fine-tuning scheme significantly reduces the training time and improves the image quality, outperformed training separate models using CNN approaches such as - RCAN and Noise2Fast.","Here we show three examples of the efficacy of this approach on denoising wide-field, two-photon and confocal fluorescence data.","In the confocal experiment, which is a 5 by 5 tiled acquisition, the fine-tuned CNNT model reduces the scan time form one hour to eight minutes, with improved quality."],"url":"http://arxiv.org/abs/2404.04726v1","category":"q-bio.QM"}
{"created":"2024-04-06 16:48:08","title":"Salient Sparse Visual Odometry With Pose-Only Supervision","abstract":"Visual Odometry (VO) is vital for the navigation of autonomous systems, providing accurate position and orientation estimates at reasonable costs. While traditional VO methods excel in some conditions, they struggle with challenges like variable lighting and motion blur. Deep learning-based VO, though more adaptable, can face generalization problems in new environments. Addressing these drawbacks, this paper presents a novel hybrid visual odometry (VO) framework that leverages pose-only supervision, offering a balanced solution between robustness and the need for extensive labeling. We propose two cost-effective and innovative designs: a self-supervised homographic pre-training for enhancing optical flow learning from pose-only labels and a random patch-based salient point detection strategy for more accurate optical flow patch extraction. These designs eliminate the need for dense optical flow labels for training and significantly improve the generalization capability of the system in diverse and challenging environments. Our pose-only supervised method achieves competitive performance on standard datasets and greater robustness and generalization ability in extreme and unseen scenarios, even compared to dense optical flow-supervised state-of-the-art methods.","sentences":["Visual Odometry (VO) is vital for the navigation of autonomous systems, providing accurate position and orientation estimates at reasonable costs.","While traditional VO methods excel in some conditions, they struggle with challenges like variable lighting and motion blur.","Deep learning-based VO, though more adaptable, can face generalization problems in new environments.","Addressing these drawbacks, this paper presents a novel hybrid visual odometry (VO) framework that leverages pose-only supervision, offering a balanced solution between robustness and the need for extensive labeling.","We propose two cost-effective and innovative designs: a self-supervised homographic pre-training for enhancing optical flow learning from pose-only labels and a random patch-based salient point detection strategy for more accurate optical flow patch extraction.","These designs eliminate the need for dense optical flow labels for training and significantly improve the generalization capability of the system in diverse and challenging environments.","Our pose-only supervised method achieves competitive performance on standard datasets and greater robustness and generalization ability in extreme and unseen scenarios, even compared to dense optical flow-supervised state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.04677v1","category":"cs.CV"}
{"created":"2024-04-06 16:30:36","title":"Study of Adaptive Reweighted Sparse Belief Propagation Decoders for Polar Codes","abstract":"In this paper, we present an adaptive reweighted sparse belief propagation (AR-SBP) decoder for polar codes. The AR-SBP technique is inspired by decoders that employ the sum-product algorithm for low-density parity-check codes. In particular, the AR-SBP decoding strategy introduces reweighting of the exchanged log-likelihood-ratio in order to refine the message passing, improving the performance of the decoder and reducing the number of required iterations. An analysis of the convergence of AR-SBP is carried out along with a study of the complexity of the analyzed decoders. Numerical examples show that the AR-SBP decoder outperforms existing decoding algorithms for a reduced number of iterations, enabling low-latency applications.","sentences":["In this paper, we present an adaptive reweighted sparse belief propagation (AR-SBP) decoder for polar codes.","The AR-SBP technique is inspired by decoders that employ the sum-product algorithm for low-density parity-check codes.","In particular, the AR-SBP decoding strategy introduces reweighting of the exchanged log-likelihood-ratio in order to refine the message passing, improving the performance of the decoder and reducing the number of required iterations.","An analysis of the convergence of AR-SBP is carried out along with a study of the complexity of the analyzed decoders.","Numerical examples show that the AR-SBP decoder outperforms existing decoding algorithms for a reduced number of iterations, enabling low-latency applications."],"url":"http://arxiv.org/abs/2404.04674v1","category":"cs.IT"}
{"created":"2024-04-06 15:30:42","title":"The study of periphery uniqueness and balance in ecological networks","abstract":"The study of ecological networks is crucial for modern conservation biology, addressing habitat fragmentation and biodiversity loss, especially in complex regions. These networks, including corridors, sources, and nodes, are key for species movement and ecosystem functioning. The Periphery Analysis Model (PAM) is introduced as a new approach to study the periphery of these networks, focusing on peripheral nodes' role in environmental change response and network resilience. PAM, drawing from graph theory, complex network analysis, and landscape ecology, uses the Periphery Uniqueness Index (PuI) and the Periphery Balance Index (PbI) to measure peripheral nodes' attributes and balance. It also offers derived indices for a detailed understanding of the periphery's influence. By revealing the periphery's defining characteristics, PAM enhances knowledge of ecological networks' structural features, providing insights for biodiversity, connectivity, and ecosystem health. The research encourages integrating PAM into conservation strategies to inform policy for ecosystem preservation amid environmental challenges.","sentences":["The study of ecological networks is crucial for modern conservation biology, addressing habitat fragmentation and biodiversity loss, especially in complex regions.","These networks, including corridors, sources, and nodes, are key for species movement and ecosystem functioning.","The Periphery Analysis Model (PAM) is introduced as a new approach to study the periphery of these networks, focusing on peripheral nodes' role in environmental change response and network resilience.","PAM, drawing from graph theory, complex network analysis, and landscape ecology, uses the Periphery Uniqueness Index (PuI) and the Periphery Balance Index (PbI) to measure peripheral nodes' attributes and balance.","It also offers derived indices for a detailed understanding of the periphery's influence.","By revealing the periphery's defining characteristics, PAM enhances knowledge of ecological networks' structural features, providing insights for biodiversity, connectivity, and ecosystem health.","The research encourages integrating PAM into conservation strategies to inform policy for ecosystem preservation amid environmental challenges."],"url":"http://arxiv.org/abs/2404.04660v1","category":"nlin.AO"}
{"created":"2024-04-06 14:54:11","title":"CANEDERLI: On The Impact of Adversarial Training and Transferability on CAN Intrusion Detection Systems","abstract":"The growing integration of vehicles with external networks has led to a surge in attacks targeting their Controller Area Network (CAN) internal bus. As a countermeasure, various Intrusion Detection Systems (IDSs) have been suggested in the literature to prevent and mitigate these threats. With the increasing volume of data facilitated by the integration of Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication networks, most of these systems rely on data-driven approaches such as Machine Learning (ML) and Deep Learning (DL) models. However, these systems are susceptible to adversarial evasion attacks. While many researchers have explored this vulnerability, their studies often involve unrealistic assumptions, lack consideration for a realistic threat model, and fail to provide effective solutions.   In this paper, we present CANEDERLI (CAN Evasion Detection ResiLIence), a novel framework for securing CAN-based IDSs. Our system considers a realistic threat model and addresses the impact of adversarial attacks on DL-based detection systems. Our findings highlight strong transferability properties among diverse attack methodologies by considering multiple state-of-the-art attacks and model architectures. We analyze the impact of adversarial training in addressing this threat and propose an adaptive online adversarial training technique outclassing traditional fine-tuning methodologies with F1 scores up to 0.941. By making our framework publicly available, we aid practitioners and researchers in assessing the resilience of IDSs to a varied adversarial landscape.","sentences":["The growing integration of vehicles with external networks has led to a surge in attacks targeting their Controller Area Network (CAN) internal bus.","As a countermeasure, various Intrusion Detection Systems (IDSs) have been suggested in the literature to prevent and mitigate these threats.","With the increasing volume of data facilitated by the integration of Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication networks, most of these systems rely on data-driven approaches such as Machine Learning (ML) and Deep Learning (DL) models.","However, these systems are susceptible to adversarial evasion attacks.","While many researchers have explored this vulnerability, their studies often involve unrealistic assumptions, lack consideration for a realistic threat model, and fail to provide effective solutions.   ","In this paper, we present CANEDERLI (CAN Evasion Detection ResiLIence), a novel framework for securing CAN-based IDSs.","Our system considers a realistic threat model and addresses the impact of adversarial attacks on DL-based detection systems.","Our findings highlight strong transferability properties among diverse attack methodologies by considering multiple state-of-the-art attacks and model architectures.","We analyze the impact of adversarial training in addressing this threat and propose an adaptive online adversarial training technique outclassing traditional fine-tuning methodologies with F1 scores up to 0.941.","By making our framework publicly available, we aid practitioners and researchers in assessing the resilience of IDSs to a varied adversarial landscape."],"url":"http://arxiv.org/abs/2404.04648v1","category":"cs.CR"}
{"created":"2024-04-06 14:34:46","title":"HyperTTS: Parameter Efficient Adaptation in Text to Speech using Hypernetworks","abstract":"Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal from the text domain to the speech domain. While developing TTS architectures that train and test on the same set of speakers has seen significant improvements, out-of-domain speaker performance still faces enormous limitations. Domain adaptation on a new set of speakers can be achieved by fine-tuning the whole model for each new domain, thus making it parameter-inefficient. This problem can be solved by Adapters that provide a parameter-efficient alternative to domain adaptation. Although famous in NLP, speech synthesis has not seen much improvement from Adapters. In this work, we present HyperTTS, which comprises a small learnable network, \"hypernetwork\", that generates parameters of the Adapter blocks, allowing us to condition Adapters on speaker representations and making them dynamic. Extensive evaluations of two domain adaptation settings demonstrate its effectiveness in achieving state-of-the-art performance in the parameter-efficient regime. We also compare different variants of HyperTTS, comparing them with baselines in different studies. Promising results on the dynamic adaptation of adapter parameters using hypernetworks open up new avenues for domain-generic multi-speaker TTS systems. The audio samples and code are available at https://github.com/declare-lab/HyperTTS.","sentences":["Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal from the text domain to the speech domain.","While developing TTS architectures that train and test on the same set of speakers has seen significant improvements, out-of-domain speaker performance still faces enormous limitations.","Domain adaptation on a new set of speakers can be achieved by fine-tuning the whole model for each new domain, thus making it parameter-inefficient.","This problem can be solved by Adapters that provide a parameter-efficient alternative to domain adaptation.","Although famous in NLP, speech synthesis has not seen much improvement from Adapters.","In this work, we present HyperTTS, which comprises a small learnable network, \"hypernetwork\", that generates parameters of the Adapter blocks, allowing us to condition Adapters on speaker representations and making them dynamic.","Extensive evaluations of two domain adaptation settings demonstrate its effectiveness in achieving state-of-the-art performance in the parameter-efficient regime.","We also compare different variants of HyperTTS, comparing them with baselines in different studies.","Promising results on the dynamic adaptation of adapter parameters using hypernetworks open up new avenues for domain-generic multi-speaker TTS systems.","The audio samples and code are available at https://github.com/declare-lab/HyperTTS."],"url":"http://arxiv.org/abs/2404.04645v1","category":"cs.CL"}
{"created":"2024-04-06 13:14:04","title":"Bridging the Gap Between End-to-End and Two-Step Text Spotting","abstract":"Modularity plays a crucial role in the development and maintenance of complex systems. While end-to-end text spotting efficiently mitigates the issues of error accumulation and sub-optimal performance seen in traditional two-step methodologies, the two-step methods continue to be favored in many competitions and practical settings due to their superior modularity. In this paper, we introduce Bridging Text Spotting, a novel approach that resolves the error accumulation and suboptimal performance issues in two-step methods while retaining modularity. To achieve this, we adopt a well-trained detector and recognizer that are developed and trained independently and then lock their parameters to preserve their already acquired capabilities. Subsequently, we introduce a Bridge that connects the locked detector and recognizer through a zero-initialized neural network. This zero-initialized neural network, initialized with weights set to zeros, ensures seamless integration of the large receptive field features in detection into the locked recognizer. Furthermore, since the fixed detector and recognizer cannot naturally acquire end-to-end optimization features, we adopt the Adapter to facilitate their efficient learning of these features. We demonstrate the effectiveness of the proposed method through extensive experiments: Connecting the latest detector and recognizer through Bridging Text Spotting, we achieved an accuracy of 83.3% on Total-Text, 69.8% on CTW1500, and 89.5% on ICDAR 2015. The code is available at https://github.com/mxin262/Bridging-Text-Spotting.","sentences":["Modularity plays a crucial role in the development and maintenance of complex systems.","While end-to-end text spotting efficiently mitigates the issues of error accumulation and sub-optimal performance seen in traditional two-step methodologies, the two-step methods continue to be favored in many competitions and practical settings due to their superior modularity.","In this paper, we introduce Bridging Text Spotting, a novel approach that resolves the error accumulation and suboptimal performance issues in two-step methods while retaining modularity.","To achieve this, we adopt a well-trained detector and recognizer that are developed and trained independently and then lock their parameters to preserve their already acquired capabilities.","Subsequently, we introduce a Bridge that connects the locked detector and recognizer through a zero-initialized neural network.","This zero-initialized neural network, initialized with weights set to zeros, ensures seamless integration of the large receptive field features in detection into the locked recognizer.","Furthermore, since the fixed detector and recognizer cannot naturally acquire end-to-end optimization features, we adopt the Adapter to facilitate their efficient learning of these features.","We demonstrate the effectiveness of the proposed method through extensive experiments: Connecting the latest detector and recognizer through Bridging Text Spotting, we achieved an accuracy of 83.3% on Total-Text, 69.8% on CTW1500, and 89.5% on ICDAR 2015.","The code is available at https://github.com/mxin262/Bridging-Text-Spotting."],"url":"http://arxiv.org/abs/2404.04624v1","category":"cs.CV"}
{"created":"2024-04-06 11:31:33","title":"Normalized solutions for Sobolev critical Schr\u00f6dinger equations on bounded domains","abstract":"We study the existence and multiplicity of positive solutions with prescribed $L^2$-norm for the Sobolev critical Schr\\\"odinger equation on a bounded domain $\\Omega\\subset\\mathbb{R}^N$, $N\\ge3$: \\[ -\\Delta U = \\lambda U + U^{2^{*}-1},\\qquad U\\in H^1_0(\\Omega),\\qquad \\int_\\Omega U^2\\,dx = \\rho^{2}, \\] where $2^*=\\frac{2N}{N-2}$.   First, we consider a general bounded domain $\\Omega$ in dimension $N\\ge3$, with a restriction, only in dimension $N=3$, involving its inradius and first Dirichlet eigenvalue. In this general case we show the existence of a mountain pass solution on the $L^2$-sphere, for $\\rho$ belonging to a subset of positive measure of the interval $(0,\\rho^{**})$, for a suitable threshold $\\rho^{**}>0$. Next, assuming that $\\Omega$ is star-shaped, we extend the previous result to all values $\\rho\\in(0,\\rho^{**})$.   With respect to that of local minimizers, already known in the literature, the existence of mountain pass solutions in the Sobolev critical case is much more elusive. In particular, our proofs are based on the sharp analysis of the bounded Palais-Smale sequences, provided by a nonstandard adaptation of the Struwe monotonicity trick, that we develop.","sentences":["We study the existence and multiplicity of positive solutions with prescribed $L^2$-norm for the Sobolev critical Schr\\\"odinger equation on a bounded domain $\\Omega\\subset\\mathbb{R}^N$, $N\\ge3$: \\[ -\\Delta U = \\lambda U","+","U^{2^{*}-1},\\qquad U\\in H^1_0(\\Omega),\\qquad \\int_\\Omega U^2\\,dx = \\rho^{2}, \\] where $2^*=\\frac{2N}{N-2}$.   First, we consider a general bounded domain $\\Omega$ in dimension $N\\ge3$, with a restriction, only in dimension $N=3$, involving its inradius and first Dirichlet eigenvalue.","In this general case we show the existence of a mountain pass solution on the $L^2$-sphere, for $\\rho$ belonging to a subset of positive measure of the interval $(0,\\rho^{**})$, for a suitable threshold $\\rho^{**}>0$. Next, assuming that $\\Omega$ is star-shaped, we extend the previous result to all values $\\rho\\in(0,\\rho^{**})$.   With respect to that of local minimizers, already known in the literature, the existence of mountain pass solutions in the Sobolev critical case is much more elusive.","In particular, our proofs are based on the sharp analysis of the bounded Palais-Smale sequences, provided by a nonstandard adaptation of the Struwe monotonicity trick, that we develop."],"url":"http://arxiv.org/abs/2404.04594v1","category":"math.AP"}
{"created":"2024-04-06 10:31:42","title":"Entropic curvature not comparable to other curvatures -- or is it?","abstract":"In this paper we consider global $\\theta$-curvatures of finite Markov chains with associated means $\\theta$ in the spirit of the entropic curvature (based on the logarithmic mean) by Erbar-Maas and Mielke. As in the case of Bakry-\\'Emery curvature, we also allow for a finite dimension parameter by making use of an adapted $\\Gamma$ calculus for $\\theta$-curvatures. We prove explicit positive lower curvature bounds (both finite- and infinite-dimensional) for finite abelian Cayley graphs. In the case of cycles, we provide also an upper curvature bound which shows that our lower bounds are asymptotically sharp (up to a logarithmic factor). Moreover, we prove new universal lower curvature bounds for finite Markov chains as well as curvature perturbation results (allowing, in particular, to compare entropic and Bakry-\\'Emery curvatures). Finally, we present examples where entropic curvature differs significantly from other curvature notions like Bakry-\\'Emery curvature or Ollivier Ricci and sectional curvatures.","sentences":["In this paper we consider global $\\theta$-curvatures of finite Markov chains with associated means $\\theta$ in the spirit of the entropic curvature (based on the logarithmic mean) by Erbar-Maas and Mielke.","As in the case of Bakry-\\'Emery curvature, we also allow for a finite dimension parameter by making use of an adapted $\\Gamma$ calculus for $\\theta$-curvatures.","We prove explicit positive lower curvature bounds (both finite- and infinite-dimensional) for finite abelian Cayley graphs.","In the case of cycles, we provide also an upper curvature bound which shows that our lower bounds are asymptotically sharp (up to a logarithmic factor).","Moreover, we prove new universal lower curvature bounds for finite Markov chains as well as curvature perturbation results (allowing, in particular, to compare entropic and Bakry-\\'Emery curvatures).","Finally, we present examples where entropic curvature differs significantly from other curvature notions like Bakry-\\'Emery curvature or Ollivier Ricci and sectional curvatures."],"url":"http://arxiv.org/abs/2404.04581v1","category":"math.DG"}
{"created":"2024-04-06 09:38:04","title":"Towards Architecting Sustainable MLOps: A Self-Adaptation Approach","abstract":"In today's dynamic technological landscape, sustainability has emerged as a pivotal concern, especially with respect to architecting Machine Learning enabled Systems (MLS). Many ML models fail in transitioning to production, primarily hindered by uncertainties due to data variations, evolving requirements, and model instabilities. Machine Learning Operations (MLOps) offers a promising solution by enhancing adaptability and technical sustainability in MLS. However, MLOps itself faces challenges related to environmental impact, technical maintenance, and economic concerns. Over the years, self-adaptation has emerged as a potential solution to handle uncertainties. This paper introduces a novel approach employing self-adaptive principles integrated into the MLOps architecture through a MAPE-K loop to bolster MLOps sustainability. By autonomously responding to uncertainties, including data, model dynamics, and environmental variations, our approach aims to address the sustainability concerns of a given MLOps pipeline identified by an architect at design time. Further, we implement the method for a Smart City use case to display the capabilities of our approach.","sentences":["In today's dynamic technological landscape, sustainability has emerged as a pivotal concern, especially with respect to architecting Machine Learning enabled Systems (MLS).","Many ML models fail in transitioning to production, primarily hindered by uncertainties due to data variations, evolving requirements, and model instabilities.","Machine Learning Operations (MLOps) offers a promising solution by enhancing adaptability and technical sustainability in MLS.","However, MLOps itself faces challenges related to environmental impact, technical maintenance, and economic concerns.","Over the years, self-adaptation has emerged as a potential solution to handle uncertainties.","This paper introduces a novel approach employing self-adaptive principles integrated into the MLOps architecture through a MAPE-K loop to bolster MLOps sustainability.","By autonomously responding to uncertainties, including data, model dynamics, and environmental variations, our approach aims to address the sustainability concerns of a given MLOps pipeline identified by an architect at design time.","Further, we implement the method for a Smart City use case to display the capabilities of our approach."],"url":"http://arxiv.org/abs/2404.04572v1","category":"cs.SE"}
{"created":"2024-04-06 08:52:22","title":"EVT-enriched Radio Maps for URLLC","abstract":"This paper introduces a sophisticated and adaptable framework combining extreme value theory with radio maps to spatially model extreme channel conditions accurately. Utilising existing signal-to-noise ratio (SNR) measurements and leveraging Gaussian processes, our approach predicts the tail of the SNR distribution, which entails estimating the parameters of a generalised Pareto distribution, at unobserved locations. This innovative method offers a versatile solution adaptable to various resource allocation challenges in ultra-reliable low-latency communications. We evaluate the performance of this method in a rate maximisation problem with defined outage constraints and compare it with a benchmark in the literature. Notably, the proposed approach meets the outage demands in a larger percentage of the coverage area and reaches higher transmission rates.","sentences":["This paper introduces a sophisticated and adaptable framework combining extreme value theory with radio maps to spatially model extreme channel conditions accurately.","Utilising existing signal-to-noise ratio (SNR) measurements and leveraging Gaussian processes, our approach predicts the tail of the SNR distribution, which entails estimating the parameters of a generalised Pareto distribution, at unobserved locations.","This innovative method offers a versatile solution adaptable to various resource allocation challenges in ultra-reliable low-latency communications.","We evaluate the performance of this method in a rate maximisation problem with defined outage constraints and compare it with a benchmark in the literature.","Notably, the proposed approach meets the outage demands in a larger percentage of the coverage area and reaches higher transmission rates."],"url":"http://arxiv.org/abs/2404.04558v1","category":"cs.NI"}
{"created":"2024-04-08 17:50:24","title":"A de Sitter S-matrix from amputated cosmological correlators","abstract":"Extending scattering to states with unphysical mass values (particles ``off their mass shell'') has been instrumental in developing modern amplitude technology for Minkowski spacetime. Here, we study the off-shell correlators which underpin the recently proposed S-matrix for scattering on de Sitter spacetime. By labelling each particle with both a spatial momentum and an independent ``energy'' variable (the de Sitter analogue of a 4-momentum), we find that the practical computation of these correlators is greatly simplified. This allows us to derive compact expressions for all 3- and 4-particle S-matrices at tree-level for scalar fields coupled through any derivative interactions. As on Minkowski, we find that the 3-particle and exchange part of the 4-particle S-matrices are unique (up to crossing). The remaining contact part of the 4-particle S-matrix is an analytic function of just two differential operators, which become the usual Mandelstam variables in the Minkowski limit. Finally, we introduce a spectral decomposition for the tree-level exchange of a heavy field responsible for a cosmological collider signal. Once projected onto physical mass eigenstates, these S-matrix elements encode the statistical properties of the early inflationary perturbations.","sentences":["Extending scattering to states with unphysical mass values (particles ``off their mass shell'') has been instrumental in developing modern amplitude technology for Minkowski spacetime.","Here, we study the off-shell correlators which underpin the recently proposed S-matrix for scattering on de Sitter spacetime.","By labelling each particle with both a spatial momentum and an independent ``energy'' variable (the de Sitter analogue of a 4-momentum), we find that the practical computation of these correlators is greatly simplified.","This allows us to derive compact expressions for all 3- and 4-particle S-matrices at tree-level for scalar fields coupled through any derivative interactions.","As on Minkowski, we find that the 3-particle and exchange part of the 4-particle S-matrices are unique (up to crossing).","The remaining contact part of the 4-particle S-matrix is an analytic function of just two differential operators, which become the usual Mandelstam variables in the Minkowski limit.","Finally, we introduce a spectral decomposition for the tree-level exchange of a heavy field responsible for a cosmological collider signal.","Once projected onto physical mass eigenstates, these S-matrix elements encode the statistical properties of the early inflationary perturbations."],"url":"http://arxiv.org/abs/2404.05712v1","category":"hep-th"}
{"created":"2024-04-08 16:41:11","title":"Error estimates for the discretization of bilinear control problems governed by semilinear elliptic PDEs","abstract":"This paper studies an optimal control problem governed by a semilinear elliptic equation, in which the control acts in a multiplicative or bilinear way as the reaction coefficient of the equation. We focus on the numerical discretization of the problem. The discretization is carried out by using the finite element method, with piecewise constant functions for the control and continuous piecewise linear functions for the state and the adjoint state. We first prove convergence of the solutions of the discrete problems to solutions of the continuous problem. We also demonstrate that strict local solutions of the continuous problem can be approximated by local solutions of the discrete problems. Next we obtain an error estimate of order $O(h)$ for the difference between continuous and discrete locally optimal controls. To obtain this result we assume no-gap second order sufficient optimality conditions. As it is usual in this kind of discretization, a superconvergence phenomenon of order $O(h^2)$ is observed in numerical experiments for the error estimates of the state and adjoint state. The last part of the paper is dedicated to explain this behaviour. A numerical experiment confirming these results is included.","sentences":["This paper studies an optimal control problem governed by a semilinear elliptic equation, in which the control acts in a multiplicative or bilinear way as the reaction coefficient of the equation.","We focus on the numerical discretization of the problem.","The discretization is carried out by using the finite element method, with piecewise constant functions for the control and continuous piecewise linear functions for the state and the adjoint state.","We first prove convergence of the solutions of the discrete problems to solutions of the continuous problem.","We also demonstrate that strict local solutions of the continuous problem can be approximated by local solutions of the discrete problems.","Next we obtain an error estimate of order $O(h)$ for the difference between continuous and discrete locally optimal controls.","To obtain this result we assume no-gap second order sufficient optimality conditions.","As it is usual in this kind of discretization, a superconvergence phenomenon of order $O(h^2)$ is observed in numerical experiments for the error estimates of the state and adjoint state.","The last part of the paper is dedicated to explain this behaviour.","A numerical experiment confirming these results is included."],"url":"http://arxiv.org/abs/2404.05658v1","category":"math.OC"}
{"created":"2024-04-08 16:38:50","title":"Convergence rates for the finite volume scheme of the stochastic heat equation","abstract":"In this contribution, we provide convergence rates for the finite volume scheme of the stochastic heat equation with multiplicative Lipschitz noise and homogeneous Neumann boundary conditions (SHE). More precisely, we give an error estimate for the $L^2$-norm of the space-time discretization of SHE by a semi-implicit Euler scheme with respect to time and a TPFA scheme with respect to space and the variational solution of SHE. The only regularity assumptions additionally needed is spatial regularity of the initial datum and smoothness of the diffusive term.","sentences":["In this contribution, we provide convergence rates for the finite volume scheme of the stochastic heat equation with multiplicative Lipschitz noise and homogeneous Neumann boundary conditions (SHE).","More precisely, we give an error estimate for the $L^2$-norm of the space-time discretization of SHE by a semi-implicit Euler scheme with respect to time and a TPFA scheme with respect to space and the variational solution of SHE.","The only regularity assumptions additionally needed is spatial regularity of the initial datum and smoothness of the diffusive term."],"url":"http://arxiv.org/abs/2404.05655v1","category":"math.NA"}
{"created":"2024-04-08 15:43:56","title":"Quantum tomography of structured light patterns from simple intensity measurements","abstract":"We study the tomography of spatial qudits encoded on structured light photons. While direct position measurements with cameras do not provide an informationally complete Positive Operator Valued Measure (POVM) in the space of fixed order modes, we complement this POVM with an astigmatic transformation. The enlarged POVM is informationally complete, allowing full characterization of the spatial quantum state from simple intensity measurements in both the intense and in the low photocount regimes. For intense light, the standard technique of linear inversion is used. For the low photocount regime, we employ Bayesian mean inference, and study how the quality of the tomographic reconstruction behaves as we increase the photocounts. In both cases, we also perform the tomography using a convolutional neural network, which displays an increased flexibility in exchange for a slightly lower quality reconstruction in some of the cases. These methods will be useful for classical and quantum communication with structured light.","sentences":["We study the tomography of spatial qudits encoded on structured light photons.","While direct position measurements with cameras do not provide an informationally complete Positive Operator Valued Measure (POVM) in the space of fixed order modes, we complement this POVM with an astigmatic transformation.","The enlarged POVM is informationally complete, allowing full characterization of the spatial quantum state from simple intensity measurements in both the intense and in the low photocount regimes.","For intense light, the standard technique of linear inversion is used.","For the low photocount regime, we employ Bayesian mean inference, and study how the quality of the tomographic reconstruction behaves as we increase the photocounts.","In both cases, we also perform the tomography using a convolutional neural network, which displays an increased flexibility in exchange for a slightly lower quality reconstruction in some of the cases.","These methods will be useful for classical and quantum communication with structured light."],"url":"http://arxiv.org/abs/2404.05616v1","category":"quant-ph"}
{"created":"2024-04-08 15:43:29","title":"Tensor neural networks for high-dimensional Fokker-Planck equations","abstract":"We solve high-dimensional steady-state Fokker-Planck equations on the whole space by applying tensor neural networks. The tensor networks are a tensor product of one-dimensional feedforward networks or a linear combination of several selected radial basis functions. The use of tensor feedforward networks allows us to efficiently exploit auto-differentiation in major Python packages while using radial basis functions can fully avoid auto-differentiation, which is rather expensive in high dimensions. We then use the physics-informed neural networks and stochastic gradient descent methods to learn the tensor networks. One essential step is to determine a proper truncated bounded domain or numerical support for the Fokker-Planck equation. To better train the tensor radial basis function networks, we impose some constraints on parameters, which lead to relatively high accuracy. We demonstrate numerically that the tensor neural networks in physics-informed machine learning are efficient for steady-state Fokker-Planck equations from two to ten dimensions.","sentences":["We solve high-dimensional steady-state Fokker-Planck equations on the whole space by applying tensor neural networks.","The tensor networks are a tensor product of one-dimensional feedforward networks or a linear combination of several selected radial basis functions.","The use of tensor feedforward networks allows us to efficiently exploit auto-differentiation in major Python packages while using radial basis functions can fully avoid auto-differentiation, which is rather expensive in high dimensions.","We then use the physics-informed neural networks and stochastic gradient descent methods to learn the tensor networks.","One essential step is to determine a proper truncated bounded domain or numerical support for the Fokker-Planck equation.","To better train the tensor radial basis function networks, we impose some constraints on parameters, which lead to relatively high accuracy.","We demonstrate numerically that the tensor neural networks in physics-informed machine learning are efficient for steady-state Fokker-Planck equations from two to ten dimensions."],"url":"http://arxiv.org/abs/2404.05615v1","category":"math.NA"}
{"created":"2024-04-08 15:24:20","title":"Technical Report: The Graph Spectral Token -- Enhancing Graph Transformers with Spectral Information","abstract":"Graph Transformers have emerged as a powerful alternative to Message-Passing Graph Neural Networks (MP-GNNs) to address limitations such as over-squashing of information exchange. However, incorporating graph inductive bias into transformer architectures remains a significant challenge. In this report, we propose the Graph Spectral Token, a novel approach to directly encode graph spectral information, which captures the global structure of the graph, into the transformer architecture. By parameterizing the auxiliary [CLS] token and leaving other tokens representing graph nodes, our method seamlessly integrates spectral information into the learning process. We benchmark the effectiveness of our approach by enhancing two existing graph transformers, GraphTrans and SubFormer. The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10% improvements on large graph benchmark datasets while maintaining efficiency comparable to MP-GNNs. SubFormer-Spec demonstrates strong performance across various datasets.","sentences":["Graph Transformers have emerged as a powerful alternative to Message-Passing Graph Neural Networks (MP-GNNs) to address limitations such as over-squashing of information exchange.","However, incorporating graph inductive bias into transformer architectures remains a significant challenge.","In this report, we propose the Graph Spectral Token, a novel approach to directly encode graph spectral information, which captures the global structure of the graph, into the transformer architecture.","By parameterizing the auxiliary [CLS] token and leaving other tokens representing graph nodes, our method seamlessly integrates spectral information into the learning process.","We benchmark the effectiveness of our approach by enhancing two existing graph transformers, GraphTrans and SubFormer.","The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10% improvements on large graph benchmark datasets while maintaining efficiency comparable to MP-GNNs.","SubFormer-Spec demonstrates strong performance across various datasets."],"url":"http://arxiv.org/abs/2404.05604v1","category":"cs.LG"}
{"created":"2024-04-08 14:59:55","title":"Examples of Atoms Absorbing Photon via Schr\u00f6dinger Equation and Vacuum Fluctuations","abstract":"The absorption of photons by atoms encompasses fundamental quantum mechanical aspects, particularly the emergence of randomness to account for the inherent unpredictability in absorption outcomes. We demonstrate that vacuum fluctuations can be the origin of this randomness. An illustrative example of this is the absorption of a single photon by two symmetrically arranged atoms. In the absence of a mechanism to introduce randomness, the Schr\\\"odinger equation alone governs the time evolution of the process until an entangled state of the two atoms emerges. This entangled state consists of two components: one in which the first atom is excited by the photon while the second remains in the ground state, and another in which the first atom remains in the ground state while the second is excited by the photon. These components form a superposition state characterized by an unbreakable symmetry in the absence of external influences. Consequently, the absorption process remains incomplete. When vacuum fluctuations come into play, they can induce fluctuations in the weights of these components, akin to Brownian motion. Over time, one component diminishes, thereby breaking the entanglement between the two atoms and allowing the photon absorption process to conclude. The remaining component ultimately determines which atom completes the photon absorption. Similar studies involving different numbers of atoms can be conducted. Vacuum fluctuations not only introduce randomness but also have the potential to give rise to the Born rule in this context. Furthermore, the Casimir effect, which is closely tied to vacuum fluctuations, presents a promising experimental avenue for validating this mechanism.","sentences":["The absorption of photons by atoms encompasses fundamental quantum mechanical aspects, particularly the emergence of randomness to account for the inherent unpredictability in absorption outcomes.","We demonstrate that vacuum fluctuations can be the origin of this randomness.","An illustrative example of this is the absorption of a single photon by two symmetrically arranged atoms.","In the absence of a mechanism to introduce randomness, the Schr\\\"odinger equation alone governs the time evolution of the process until an entangled state of the two atoms emerges.","This entangled state consists of two components: one in which the first atom is excited by the photon while the second remains in the ground state, and another in which the first atom remains in the ground state while the second is excited by the photon.","These components form a superposition state characterized by an unbreakable symmetry in the absence of external influences.","Consequently, the absorption process remains incomplete.","When vacuum fluctuations come into play, they can induce fluctuations in the weights of these components, akin to Brownian motion.","Over time, one component diminishes, thereby breaking the entanglement between the two atoms and allowing the photon absorption process to conclude.","The remaining component ultimately determines which atom completes the photon absorption.","Similar studies involving different numbers of atoms can be conducted.","Vacuum fluctuations not only introduce randomness but also have the potential to give rise to the Born rule in this context.","Furthermore, the Casimir effect, which is closely tied to vacuum fluctuations, presents a promising experimental avenue for validating this mechanism."],"url":"http://arxiv.org/abs/2404.05585v1","category":"quant-ph"}
{"created":"2024-04-08 14:55:35","title":"Robust Data Pruning: Uncovering and Overcoming Implicit Bias","abstract":"In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. At the same time, we argue that random data pruning with appropriate class ratios has potential to improve the worst-class performance. We propose a \"fairness-aware\" approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving robustness at a tolerable drop of average performance as we prune more from the datasets. We present theoretical analysis of the classification risk in a mixture of Gaussians to further motivate our algorithm and support our findings.","sentences":["In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning.","Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws.","However, little is known about its impact on classification bias of the trained models.","We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers.","At the same time, we argue that random data pruning with appropriate class ratios has potential to improve the worst-class performance.","We propose a \"fairness-aware\" approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks.","In sharp contrast to existing algorithms, our proposed method continues improving robustness at a tolerable drop of average performance as we prune more from the datasets.","We present theoretical analysis of the classification risk in a mixture of Gaussians to further motivate our algorithm and support our findings."],"url":"http://arxiv.org/abs/2404.05579v1","category":"cs.LG"}
{"created":"2024-04-08 14:21:28","title":"Partial balayage for the Helmholtz equation","abstract":"Kow, Larson, Salo and Shahgholian recently initiated the study of quadrature domains for the Helmholtz equation and developed an associated theory of partial balayage of measures. The present paper offers an alternative approach to partial balayage in this context that yields stronger results. Applications are given to quadrature domains and to a domain evolution question that is analogous to Hele-Shaw flow.","sentences":["Kow, Larson, Salo and Shahgholian recently initiated the study of quadrature domains for the Helmholtz equation and developed an associated theory of partial balayage of measures.","The present paper offers an alternative approach to partial balayage in this context that yields stronger results.","Applications are given to quadrature domains and to a domain evolution question that is analogous to Hele-Shaw flow."],"url":"http://arxiv.org/abs/2404.05552v1","category":"math.AP"}
{"created":"2024-04-08 14:16:46","title":"Differential reddening in 48 globular clusters: An end to the quest for the intracluster medium","abstract":"For decades, it has been theorized that a tenuous but detectable intracluster medium should be present in globular clusters, which is continuously replenished by the gas and dust ejected by bright giants and periodically cleared by interactions with the Galactic disk. However, dedicated searches, especially in infrared and radio wavelengths, have returned mostly upper limits, which are lower than theoretical expectations by several orders of magnitude. We profited from recent wide-field photometry for 48 Galactic globular clusters to compute high-resolution maps of differential reddening, which can be used to correct any photometric catalog in these areas for reddening variations. Using 3D reddening maps from the literature, we evaluated the amount of foreground extinction. This allowed us to estimate the masses of the intracluster medium in our sample clusters, with an accuracy of one order of magnitude. Our estimates agree with the few available literature detections and with theoretical expectations. Because the discrepancy between observations and expectations only concerns literature upper limits, we explored possible reasons why they could be underestimated and we show that two recent discoveries can explain the discrepancy. The first is the recent discovery that the intracluster medium in 47 Tuc is not centrally concentrated. This is also supported by our maps, which in the majority of cases do not show a central reddening concentration. The second is the discovery that the dust in metal-poor ([Fe/H] less than about -1 dex) globular clusters is dominated by iron grains rather than silicates, which undermines previous dust mass estimates from observed upper limits. We conclude that current evidence, including our maps, does not contradict theoretical expectations and the problem of the missing intracluster medium is no longer an issue.","sentences":["For decades, it has been theorized that a tenuous but detectable intracluster medium should be present in globular clusters, which is continuously replenished by the gas and dust ejected by bright giants and periodically cleared by interactions with the Galactic disk.","However, dedicated searches, especially in infrared and radio wavelengths, have returned mostly upper limits, which are lower than theoretical expectations by several orders of magnitude.","We profited from recent wide-field photometry for 48 Galactic globular clusters to compute high-resolution maps of differential reddening, which can be used to correct any photometric catalog in these areas for reddening variations.","Using 3D reddening maps from the literature, we evaluated the amount of foreground extinction.","This allowed us to estimate the masses of the intracluster medium in our sample clusters, with an accuracy of one order of magnitude.","Our estimates agree with the few available literature detections and with theoretical expectations.","Because the discrepancy between observations and expectations only concerns literature upper limits, we explored possible reasons why they could be underestimated and we show that two recent discoveries can explain the discrepancy.","The first is the recent discovery that the intracluster medium in 47 Tuc is not centrally concentrated.","This is also supported by our maps, which in the majority of cases do not show a central reddening concentration.","The second is the discovery that the dust in metal-poor ([Fe/H] less than about -1 dex) globular clusters is dominated by iron grains rather than silicates, which undermines previous dust mass estimates from observed upper limits.","We conclude that current evidence, including our maps, does not contradict theoretical expectations and the problem of the missing intracluster medium is no longer an issue."],"url":"http://arxiv.org/abs/2404.05548v1","category":"astro-ph.GA"}
{"created":"2024-04-08 13:39:12","title":"DepthMOT: Depth Cues Lead to a Strong Multi-Object Tracker","abstract":"Accurately distinguishing each object is a fundamental goal of Multi-object tracking (MOT) algorithms. However, achieving this goal still remains challenging, primarily due to: (i) For crowded scenes with occluded objects, the high overlap of object bounding boxes leads to confusion among closely located objects. Nevertheless, humans naturally perceive the depth of elements in a scene when observing 2D videos. Inspired by this, even though the bounding boxes of objects are close on the camera plane, we can differentiate them in the depth dimension, thereby establishing a 3D perception of the objects. (ii) For videos with rapidly irregular camera motion, abrupt changes in object positions can result in ID switches. However, if the camera pose are known, we can compensate for the errors in linear motion models. In this paper, we propose \\textit{DepthMOT}, which achieves: (i) detecting and estimating scene depth map \\textit{end-to-end}, (ii) compensating the irregular camera motion by camera pose estimation. Extensive experiments demonstrate the superior performance of DepthMOT in VisDrone-MOT and UAVDT datasets. The code will be available at \\url{https://github.com/JackWoo0831/DepthMOT}.","sentences":["Accurately distinguishing each object is a fundamental goal of Multi-object tracking (MOT) algorithms.","However, achieving this goal still remains challenging, primarily due to: (i) For crowded scenes with occluded objects, the high overlap of object bounding boxes leads to confusion among closely located objects.","Nevertheless, humans naturally perceive the depth of elements in a scene when observing 2D videos.","Inspired by this, even though the bounding boxes of objects are close on the camera plane, we can differentiate them in the depth dimension, thereby establishing a 3D perception of the objects.","(ii) For videos with rapidly irregular camera motion, abrupt changes in object positions can result in ID switches.","However, if the camera pose are known, we can compensate for the errors in linear motion models.","In this paper, we propose \\textit{DepthMOT}, which achieves: (i) detecting and estimating scene depth map \\textit{end-to-end}, (ii) compensating the irregular camera motion by camera pose estimation.","Extensive experiments demonstrate the superior performance of DepthMOT in VisDrone-MOT and UAVDT datasets.","The code will be available at \\url{https://github.com/JackWoo0831/DepthMOT}."],"url":"http://arxiv.org/abs/2404.05518v1","category":"cs.CV"}
{"created":"2024-04-08 13:38:38","title":"The $L^p$ estimate for the gain term of the Boltzmann collision operator and its application","abstract":"We prove the Hardy-Littlewood-Sobolev type $L^p$ estimates for the gain term of the Boltzmann collision operator including Maxwellian molecule, hard potential and hard sphere models. Combining with the results of Alonso et al. [2] for the soft potential and Maxwellian molecule models, we provide an unified form of $L^p$ estimates for all cutoff models which are sharp in the sense of scaling. The most striking feature of our new estimates for the hard potential and hard sphere models is that they do not increase the moment, the same as Maxwellian molecule and soft potential models. Based on these novelties, we prove the global existence and scattering of the non-negative unique mild solution for the Cauchy problem of the Boltzmann equation when the positive initial data is small in the weighted $L^3_{x,v}$ space.","sentences":["We prove the Hardy-Littlewood-Sobolev type $L^p$ estimates for the gain term of the Boltzmann collision operator including Maxwellian molecule, hard potential and hard sphere models.","Combining with the results of Alonso et al.","[2] for the soft potential and Maxwellian molecule models, we provide an unified form of $L^p$ estimates for all cutoff models which are sharp in the sense of scaling.","The most striking feature of our new estimates for the hard potential and hard sphere models is that they do not increase the moment, the same as Maxwellian molecule and soft potential models.","Based on these novelties, we prove the global existence and scattering of the non-negative unique mild solution for the Cauchy problem of the Boltzmann equation when the positive initial data is small in the weighted $L^3_{x,v}$ space."],"url":"http://arxiv.org/abs/2404.05517v1","category":"math.AP"}
{"created":"2024-04-08 13:12:04","title":"Rewording theoretical predictions at colliders with vacuum amplitudes","abstract":"We propose multiloop vacuum amplitudes as the optimal building blocks for efficiently assembling theoretical predictions at high-energy colliders. This hypothesis is strongly supported by the manifestly causal properties of the loop-tree duality (LTD) representation of a vacuum amplitude. The vacuum amplitude, acting as a kernel, encodes all the final states contributing to a given scattering or decay process through residues in the on-shell energies of the internal propagators. It also naturally implements gauge invariance and the wave function renormalisation of the external legs. This methodological approach, dubbed LTD causal unitary, leads to a novel representation of differential cross sections and decay rates that is locally free of ultraviolet and infrared singularities at all orders in perturbation theory. Unitary threshold singularities also match between different phase-space residues. Most notably, it allows us to conjecture for the first time the local functional form of initial-state collinear singularities. The fulfillment of all these properties provides a theoretical description of differential observables at colliders that is well defined in the four physical dimensions of the space-time.","sentences":["We propose multiloop vacuum amplitudes as the optimal building blocks for efficiently assembling theoretical predictions at high-energy colliders.","This hypothesis is strongly supported by the manifestly causal properties of the loop-tree duality (LTD) representation of a vacuum amplitude.","The vacuum amplitude, acting as a kernel, encodes all the final states contributing to a given scattering or decay process through residues in the on-shell energies of the internal propagators.","It also naturally implements gauge invariance and the wave function renormalisation of the external legs.","This methodological approach, dubbed LTD causal unitary, leads to a novel representation of differential cross sections and decay rates that is locally free of ultraviolet and infrared singularities at all orders in perturbation theory.","Unitary threshold singularities also match between different phase-space residues.","Most notably, it allows us to conjecture for the first time the local functional form of initial-state collinear singularities.","The fulfillment of all these properties provides a theoretical description of differential observables at colliders that is well defined in the four physical dimensions of the space-time."],"url":"http://arxiv.org/abs/2404.05491v1","category":"hep-ph"}
{"created":"2024-04-08 13:06:36","title":"Many-body quantum dynamics of spin-orbit coupled Andreev states in a Zeeman field","abstract":"We provide a theoretical framework to describe the quantum many-body dynamics of Andreev states in Josephson junctions with spin-orbit coupling and a magnetic Zeeman field. In such cases, employing a doubled Nambu spinor description is technically advantageous but one then has to be careful to avoid double-counting problems. By deriving the Lindblad master equation in the socalled excitation picture, we show that a physically consistent many-body theory free from doublecounting problems follows. We apply our formalism to a study of dynamical parity stabilization of the Andreev sector at intermediate times after an initial microwave pulse, in particular addressing the combined effects of spin-orbit coupling and Zeeman field.","sentences":["We provide a theoretical framework to describe the quantum many-body dynamics of Andreev states in Josephson junctions with spin-orbit coupling and a magnetic Zeeman field.","In such cases, employing a doubled Nambu spinor description is technically advantageous but one then has to be careful to avoid double-counting problems.","By deriving the Lindblad master equation in the socalled excitation picture, we show that a physically consistent many-body theory free from doublecounting problems follows.","We apply our formalism to a study of dynamical parity stabilization of the Andreev sector at intermediate times after an initial microwave pulse, in particular addressing the combined effects of spin-orbit coupling and Zeeman field."],"url":"http://arxiv.org/abs/2404.05485v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 12:41:40","title":"Curvature of quaternionic skew-Hermitian manifolds and bundle constructions","abstract":"This articles is devoted to a description of the second-order differential geometry of torsion-free almost quaternionic skew-Hermitian manifolds, that is, of quaternionic skew-Hermitian manifolds $(M, Q, \\omega)$. We provide a curvature characterization of such integrable geometric structures, based on the holonomy theory of symplectic connections and we study qualitative properties of the induced Ricci tensor. Then we proceed with bundle constructions over such a manifold $(M, Q, \\omega)$. In particular, we prove the existence of almost hypercomplex skew-Hermitian structures on the Swann bundle over $M$ and investigate their integrability.","sentences":["This articles is devoted to a description of the second-order differential geometry of torsion-free almost quaternionic skew-Hermitian manifolds, that is, of quaternionic skew-Hermitian manifolds $(M, Q, \\omega)$. We provide a curvature characterization of such integrable geometric structures, based on the holonomy theory of symplectic connections and we study qualitative properties of the induced Ricci tensor.","Then we proceed with bundle constructions over such a manifold $(M, Q, \\omega)$. In particular, we prove the existence of almost hypercomplex skew-Hermitian structures on the Swann bundle over $M$ and investigate their integrability."],"url":"http://arxiv.org/abs/2404.05463v1","category":"math.DG"}
{"created":"2024-04-08 12:27:00","title":"Unsupervised Training of Convex Regularizers using Maximum Likelihood Estimation","abstract":"Unsupervised learning is a training approach in the situation where ground truth data is unavailable, such as inverse imaging problems. We present an unsupervised Bayesian training approach to learning convex neural network regularizers using a fixed noisy dataset, based on a dual Markov chain estimation method. Compared to classical supervised adversarial regularization methods, where there is access to both clean images as well as unlimited to noisy copies, we demonstrate close performance on natural image Gaussian deconvolution and Poisson denoising tasks.","sentences":["Unsupervised learning is a training approach in the situation where ground truth data is unavailable, such as inverse imaging problems.","We present an unsupervised Bayesian training approach to learning convex neural network regularizers using a fixed noisy dataset, based on a dual Markov chain estimation method.","Compared to classical supervised adversarial regularization methods, where there is access to both clean images as well as unlimited to noisy copies, we demonstrate close performance on natural image Gaussian deconvolution and Poisson denoising tasks."],"url":"http://arxiv.org/abs/2404.05445v1","category":"stat.ME"}
{"created":"2024-04-08 10:11:05","title":"Optical spin orientation of localized electrons and holes interacting with nuclei in an FA0.9Cs0.1PbI2.8Br0.2 perovskite crystal","abstract":"Optical orientation of carrier spins by circularly polarized light is the basic concept and tool of spin physics in semiconductors. We study the optical orientation of electrons and holes in a crystal of the FA$_{0.9}$Cs$_{0.1}$PbI$_{2.8}$Br$_{0.2}$ lead halide perovskite by means of polarized photoluminescence, time-resolved differential reflectivity, and time-resolved Kerr rotation. At the cryogenic temperature of 1.6 K the optical orientation degree measured for continuous-wave excitaton reaches 6\\% for localized electrons and 2\\% for localized holes. Their contributions are distinguished from each other and from exciton optical orientation through the pronounced Hanle effect in transverse magnetic fields and the polarization recovery effect in longitudinal magnetic fields. The optical orientation degree is highly stable against detuning of the laser photon energy from the band gap by up to 0.25 eV, showing then a gradual decrease for detunings up to 0.9 eV. This evidences the inefficiency of spin relaxation mechanisms for free carriers during their energy relaxation. Spin relaxation for localized electrons and holes is provided by the hyperfine interaction with the nuclear spins. Dynamic polarization of nuclear spins is demonstrated by the Overhauser field reaching 4 mT acting on the electrons and $-76$ mT acting on the holes. This confirms the specifics of lead halide perovskite semiconductors, where the hole hyperfine interaction with the nuclei considerably exceeds that of the electron.","sentences":["Optical orientation of carrier spins by circularly polarized light is the basic concept and tool of spin physics in semiconductors.","We study the optical orientation of electrons and holes in a crystal of the FA$_{0.9}$Cs$_{0.1}$PbI$_{2.8}$Br$_{0.2}$ lead halide perovskite by means of polarized photoluminescence, time-resolved differential reflectivity, and time-resolved Kerr rotation.","At the cryogenic temperature of 1.6 K the optical orientation degree measured for continuous-wave excitaton reaches 6\\% for localized electrons and 2\\% for localized holes.","Their contributions are distinguished from each other and from exciton optical orientation through the pronounced Hanle effect in transverse magnetic fields and the polarization recovery effect in longitudinal magnetic fields.","The optical orientation degree is highly stable against detuning of the laser photon energy from the band gap by up to 0.25 eV, showing then a gradual decrease for detunings up to 0.9 eV.","This evidences the inefficiency of spin relaxation mechanisms for free carriers during their energy relaxation.","Spin relaxation for localized electrons and holes is provided by the hyperfine interaction with the nuclear spins.","Dynamic polarization of nuclear spins is demonstrated by the Overhauser field reaching 4 mT acting on the electrons and $-76$ mT acting on the holes.","This confirms the specifics of lead halide perovskite semiconductors, where the hole hyperfine interaction with the nuclei considerably exceeds that of the electron."],"url":"http://arxiv.org/abs/2404.05369v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 07:56:34","title":"Deforming Locally Convex Curves into Curves of Constant $k$-order Width","abstract":"A nonlocal curvature flow is introduced to evolve locally convex curves in the plane. It is proved that this flow with any initial locally convex curve has a global solution, keeping the local convexity and the elastic energy of the evolving curve, and that, as the time goes to infinity, the curve converges to a smooth, locally convex curve of constant $k$-order width. In particular, the limiting curve is a multiple circle if and only if the initial locally convex curve is $k$-symmetric.","sentences":["A nonlocal curvature flow is introduced to evolve locally convex curves in the plane.","It is proved that this flow with any initial locally convex curve has a global solution, keeping the local convexity and the elastic energy of the evolving curve, and that, as the time goes to infinity, the curve converges to a smooth, locally convex curve of constant $k$-order width.","In particular, the limiting curve is a multiple circle if and only if the initial locally convex curve is $k$-symmetric."],"url":"http://arxiv.org/abs/2404.05267v1","category":"math.DG"}
{"created":"2024-04-08 07:21:46","title":"Supervised Gradual Machine Learning for Aspect Category Detection","abstract":"Aspect Category Detection (ACD) aims to identify implicit and explicit aspects in a given review sentence. The state-of-the-art approaches for ACD use Deep Neural Networks (DNNs) to address the problem as a multi-label classification task. However, learning category-specific representations heavily rely on the amount of labeled examples, which may not readily available in real-world scenarios. In this paper, we propose a novel approach to tackle the ACD task by combining DNNs with Gradual Machine Learning (GML) in a supervised setting. we aim to leverage the strength of DNN in semantic relation modeling, which can facilitate effective knowledge transfer between labeled and unlabeled instances during the gradual inference of GML. To achieve this, we first analyze the learned latent space of the DNN to model the relations, i.e., similar or opposite, between instances. We then represent these relations as binary features in a factor graph to efficiently convey knowledge. Finally, we conduct a comparative study of our proposed solution on real benchmark datasets and demonstrate that the GML approach, in collaboration with DNNs for feature extraction, consistently outperforms pure DNN solutions.","sentences":["Aspect Category Detection (ACD) aims to identify implicit and explicit aspects in a given review sentence.","The state-of-the-art approaches for ACD use Deep Neural Networks (DNNs) to address the problem as a multi-label classification task.","However, learning category-specific representations heavily rely on the amount of labeled examples, which may not readily available in real-world scenarios.","In this paper, we propose a novel approach to tackle the ACD task by combining DNNs with Gradual Machine Learning (GML) in a supervised setting.","we aim to leverage the strength of DNN in semantic relation modeling, which can facilitate effective knowledge transfer between labeled and unlabeled instances during the gradual inference of GML.","To achieve this, we first analyze the learned latent space of the DNN to model the relations, i.e., similar or opposite, between instances.","We then represent these relations as binary features in a factor graph to efficiently convey knowledge.","Finally, we conduct a comparative study of our proposed solution on real benchmark datasets and demonstrate that the GML approach, in collaboration with DNNs for feature extraction, consistently outperforms pure DNN solutions."],"url":"http://arxiv.org/abs/2404.05245v1","category":"cs.CL"}
{"created":"2024-04-08 04:53:29","title":"HSViT: Horizontally Scalable Vision Transformer","abstract":"While the Vision Transformer (ViT) architecture gains prominence in computer vision and attracts significant attention from multimedia communities, its deficiency in prior knowledge (inductive bias) regarding shift, scale, and rotational invariance necessitates pre-training on large-scale datasets. Furthermore, the growing layers and parameters in both ViT and convolutional neural networks (CNNs) impede their applicability to mobile multimedia services, primarily owing to the constrained computational resources on edge devices. To mitigate the aforementioned challenges, this paper introduces a novel horizontally scalable vision transformer (HSViT). Specifically, a novel image-level feature embedding allows ViT to better leverage the inductive bias inherent in the convolutional layers. Based on this, an innovative horizontally scalable architecture is designed, which reduces the number of layers and parameters of the models while facilitating collaborative training and inference of ViT models across multiple nodes. The experimental results depict that, without pre-training on large-scale datasets, HSViT achieves up to 10% higher top-1 accuracy than state-of-the-art schemes, ascertaining its superior preservation of inductive bias. The code is available at https://github.com/xuchenhao001/HSViT.","sentences":["While the Vision Transformer (ViT) architecture gains prominence in computer vision and attracts significant attention from multimedia communities, its deficiency in prior knowledge (inductive bias) regarding shift, scale, and rotational invariance necessitates pre-training on large-scale datasets.","Furthermore, the growing layers and parameters in both ViT and convolutional neural networks (CNNs) impede their applicability to mobile multimedia services, primarily owing to the constrained computational resources on edge devices.","To mitigate the aforementioned challenges, this paper introduces a novel horizontally scalable vision transformer (HSViT).","Specifically, a novel image-level feature embedding allows ViT to better leverage the inductive bias inherent in the convolutional layers.","Based on this, an innovative horizontally scalable architecture is designed, which reduces the number of layers and parameters of the models while facilitating collaborative training and inference of ViT models across multiple nodes.","The experimental results depict that, without pre-training on large-scale datasets, HSViT achieves up to 10% higher top-1 accuracy than state-of-the-art schemes, ascertaining its superior preservation of inductive bias.","The code is available at https://github.com/xuchenhao001/HSViT."],"url":"http://arxiv.org/abs/2404.05196v1","category":"cs.CV"}
{"created":"2024-04-08 04:27:36","title":"LGSDF: Continual Global Learning of Signed Distance Fields Aided by Local Updating","abstract":"Implicit reconstruction of ESDF (Euclidean Signed Distance Field) involves training a neural network to regress the signed distance from any point to the nearest obstacle, which has the advantages of lightweight storage and continuous querying. However, existing algorithms usually rely on conflicting raw observations as training data, resulting in poor map performance. In this paper, we propose LGSDF, an ESDF continual Global learning algorithm aided by Local updating. At the front end, axis-aligned grids are dynamically updated by pre-processed sensor observations, where incremental fusion alleviates estimation error caused by limited viewing directions. At the back end, a randomly initialized implicit ESDF neural network performs continual self-supervised learning guided by these grids to generate smooth and continuous maps. The results on multiple scenes show that LGSDF can construct more accurate ESDF maps and meshes compared with SOTA (State Of The Art) explicit and implicit mapping algorithms. The source code of LGSDF is publicly available at https://github.com/BIT-DYN/LGSDF.","sentences":["Implicit reconstruction of ESDF (Euclidean Signed Distance Field) involves training a neural network to regress the signed distance from any point to the nearest obstacle, which has the advantages of lightweight storage and continuous querying.","However, existing algorithms usually rely on conflicting raw observations as training data, resulting in poor map performance.","In this paper, we propose LGSDF, an ESDF continual Global learning algorithm aided by Local updating.","At the front end, axis-aligned grids are dynamically updated by pre-processed sensor observations, where incremental fusion alleviates estimation error caused by limited viewing directions.","At the back end, a randomly initialized implicit ESDF neural network performs continual self-supervised learning guided by these grids to generate smooth and continuous maps.","The results on multiple scenes show that LGSDF can construct more accurate ESDF maps and meshes compared with SOTA (State Of The Art) explicit and implicit mapping algorithms.","The source code of LGSDF is publicly available at https://github.com/BIT-DYN/LGSDF."],"url":"http://arxiv.org/abs/2404.05187v1","category":"cs.CV"}
{"created":"2024-04-08 03:14:47","title":"Direct Approach of Indefinite Linear-Quadratic Mean Field Games","abstract":"This paper is concerned with an indefinite linear-quadratic (LQ) mean field games of stochastic large-polulation system, where the individual diffusion coefficients can depend on both the state and the control of the agents and the population state average. Moreover, the control weights in the cost functionals could be indefinite. We use a direct approach to derive the $\\epsilon$-Nash equilibrium strategy. First, we formally solving an $N$-player game problem within a vast and finite population setting. Subsequently, decoupling or reducing high-dimensional systems by introducing two Riccati equations explicitly yields centralized strategies, contingent on the state of a specific player and the average state of the population. As the population size $N$ approaches infinity, the construction of decentralized strategies becomes feasible. Then, we demonstrated they are an $\\epsilon$-Nash equilibrium.","sentences":["This paper is concerned with an indefinite linear-quadratic (LQ) mean field games of stochastic large-polulation system, where the individual diffusion coefficients can depend on both the state and the control of the agents and the population state average.","Moreover, the control weights in the cost functionals could be indefinite.","We use a direct approach to derive the $\\epsilon$-Nash equilibrium strategy.","First, we formally solving an $N$-player game problem within a vast and finite population setting.","Subsequently, decoupling or reducing high-dimensional systems by introducing two Riccati equations explicitly yields centralized strategies, contingent on the state of a specific player and the average state of the population.","As the population size $N$ approaches infinity, the construction of decentralized strategies becomes feasible.","Then, we demonstrated they are an $\\epsilon$-Nash equilibrium."],"url":"http://arxiv.org/abs/2404.05166v1","category":"math.OC"}
{"created":"2024-04-08 02:33:55","title":"Gromov-Hausdorff distances from simply connected geodesic spaces to the circle","abstract":"We prove that the Gromov-Hausdorff distance from the circle with its geodesic metric to any simply connected geodesic space is never smaller than $\\frac{\\pi}{4}$. We also prove that this bound is tight through the construction of a simply connected geodesic space $\\mathrm{E}$ which attains the lower bound $\\frac{\\pi}{4}$. We deduce the first statement from a general result that we also establish which gives conditions on how small the Gromov-Hausdorff distance between two geodesic metric spaces $(X, d_X)$ and $(Y, d_Y )$ has to be in order for $\\pi_1(X)$ and $\\pi_1(Y)$ to be isomorphic.","sentences":["We prove that the Gromov-Hausdorff distance from the circle with its geodesic metric to any simply connected geodesic space is never smaller than $\\frac{\\pi}{4}$. We also prove that this bound is tight through the construction of a simply connected geodesic space $\\mathrm{E}$ which attains the lower bound $\\frac{\\pi}{4}$. We deduce the first statement from a general result that we also establish which gives conditions on how small the Gromov-Hausdorff distance between two geodesic metric spaces $(X, d_X)$ and $(Y, d_Y )$ has to be in order for $\\pi_1(X)$ and $\\pi_1(Y)$ to be isomorphic."],"url":"http://arxiv.org/abs/2404.05153v1","category":"math.MG"}
{"created":"2024-04-08 02:13:12","title":"Katachi: Decoding the Imprints of Past Star Formation on Present Day Morphology in Galaxies with Interpretable CNNs","abstract":"The physical processes responsible for shaping how galaxies form and quench over time leave imprints on both the spatial (galaxy morphology) and temporal (star formation history; SFH) tracers that we use to study galaxies. While the morphology-SFR connection is well studied, the correlation with past star formation activity is not as well understood. To quantify this we present Katachi, an interpretable convolutional neural network (CNN) framework that learns the connection between the factors regulating star formation in galaxies on different spatial and temporal scales. Katachi is trained on 9904 galaxies at 0.02$<$z$<$0.1 in the SDSS-IV MaNGA DR17 sample to predict stellar mass (M$_*$; RMSE 0.22 dex), current star formation rate (SFR; RMSE 0.31 dex) and half-mass time (t$_{50}$; RMSE 0.23 dex). This information allows us to reconstruct non-parametric SFHs for each galaxy from \\textit{gri} imaging alone. To quantify the morphological features informing the SFH predictions we use SHAP (SHapley Additive exPlanations). We recover the expected trends of M$_*$ governed by the growth of galaxy bulges, and SFR correlating with spiral arms and other star-forming regions. We also find the SHAP maps of D4000 are more complex than those of M$_*$ and SFR, and that morphology is correlated with t$_{50}$ even at fixed mass and SFR. Katachi serves as a scalable public framework to predict galaxy properties from large imaging surveys including Rubin, Roman, and Euclid, with large datasets of high SNR imaging across limited photometric bands.","sentences":["The physical processes responsible for shaping how galaxies form and quench over time leave imprints on both the spatial (galaxy morphology) and temporal (star formation history; SFH) tracers that we use to study galaxies.","While the morphology-SFR connection is well studied, the correlation with past star formation activity is not as well understood.","To quantify this we present Katachi, an interpretable convolutional neural network (CNN) framework that learns the connection between the factors regulating star formation in galaxies on different spatial and temporal scales.","Katachi is trained on 9904 galaxies at 0.02$<$z$<$0.1 in the SDSS-IV MaNGA DR17 sample to predict stellar mass (M$_*$; RMSE 0.22 dex), current star formation rate (SFR; RMSE 0.31 dex) and half-mass time (t$_{50}$; RMSE 0.23 dex).","This information allows us to reconstruct non-parametric SFHs for each galaxy from \\textit{gri} imaging alone.","To quantify the morphological features informing the SFH predictions we use SHAP (SHapley Additive exPlanations).","We recover the expected trends of M$_*$ governed by the growth of galaxy bulges, and SFR correlating with spiral arms and other star-forming regions.","We also find the SHAP maps of D4000 are more complex than those of M$_*$ and SFR, and that morphology is correlated with t$_{50}$ even at fixed mass and SFR.","Katachi serves as a scalable public framework to predict galaxy properties from large imaging surveys including Rubin, Roman, and Euclid, with large datasets of high SNR imaging across limited photometric bands."],"url":"http://arxiv.org/abs/2404.05146v1","category":"astro-ph.GA"}
{"created":"2024-04-08 01:49:50","title":"Note on Warped Compactification -- Finite Brane Potentials and Non-Hermiticity --","abstract":"We study radius stabilization in the Randall-Sundrum model without assuming any unnaturally large stabilizing scalar potential parameter at the boundary branes ($\\gamma$) by the frequently used superpotential method. Employing a perturbative expansion in $1/\\gamma^2$ and the backreaction parameter, we obtain approximate analytical expressions for the radion mass and wavefunction. We validate them through a dedicated numerical analysis, which solves the linearized coupled scalar and metric field equations exactly. It is observed that the radion mass decreases with decreasing $\\gamma$. Below a critical value of $\\gamma$, the radion becomes tachyonic, suggesting destabilization of the extra dimension. We also address the issue of non-Hermiticity of the differential operator that determines the radion and Kaluza-Klein (KK) mode wavefunctions in the finite $\\gamma$ limit. It is accomplished by finding an explicit form of the general scalar product that re-establishes the orthogonality in the KK decomposition.","sentences":["We study radius stabilization in the Randall-Sundrum model without assuming any unnaturally large stabilizing scalar potential parameter at the boundary branes ($\\gamma$) by the frequently used superpotential method.","Employing a perturbative expansion in $1/\\gamma^2$ and the backreaction parameter, we obtain approximate analytical expressions for the radion mass and wavefunction.","We validate them through a dedicated numerical analysis, which solves the linearized coupled scalar and metric field equations exactly.","It is observed that the radion mass decreases with decreasing $\\gamma$. Below a critical value of $\\gamma$, the radion becomes tachyonic, suggesting destabilization of the extra dimension.","We also address the issue of non-Hermiticity of the differential operator that determines the radion and Kaluza-Klein (KK) mode wavefunctions in the finite $\\gamma$ limit.","It is accomplished by finding an explicit form of the general scalar product that re-establishes the orthogonality in the KK decomposition."],"url":"http://arxiv.org/abs/2404.05141v1","category":"hep-th"}
{"created":"2024-04-08 01:08:41","title":"Improving Deep Learning Predictions with Simulated Images, and Vice Versa","abstract":"Artificial neural networks are often used to identify features of crop plants. However, training their models requires many annotated images, which can be expensive and time-consuming to acquire. Procedural models of plants, such as those developed with Lindenmayer-systems (L-systems) can be created to produce visually realistic simulations, and hence images of plant simulations, where annotations are implicitly known. These synthetic images can either augment or completely replace real images in training neural networks for phenotyping tasks. In this paper, we systematically vary amounts of real and synthetic images used for training in both maize and canola to better understand situations where synthetic images generated from L-systems can help prediction on real images. This work also explores the degree to which realism in the synthetic images improves prediction. Furthermore, we see how neural network predictions can be used to help calibrate L-systems themselves, creating a feedback loop.","sentences":["Artificial neural networks are often used to identify features of crop plants.","However, training their models requires many annotated images, which can be expensive and time-consuming to acquire.","Procedural models of plants, such as those developed with Lindenmayer-systems (L-systems) can be created to produce visually realistic simulations, and hence images of plant simulations, where annotations are implicitly known.","These synthetic images can either augment or completely replace real images in training neural networks for phenotyping tasks.","In this paper, we systematically vary amounts of real and synthetic images used for training in both maize and canola to better understand situations where synthetic images generated from L-systems can help prediction on real images.","This work also explores the degree to which realism in the synthetic images improves prediction.","Furthermore, we see how neural network predictions can be used to help calibrate L-systems themselves, creating a feedback loop."],"url":"http://arxiv.org/abs/2404.05128v1","category":"cs.CV"}
{"created":"2024-04-08 00:37:19","title":"Computing the interaction of light pulses with objects moving at relativistic speeds","abstract":"The interaction of light with short light pulses is relevant in optical traps, optical tweezers, and many other applications. The theoretical description of such polychromatic light-matter interaction is challenging, and more so when the object is moving with respect to the light source, albeit with constant speed. Light sails are futuristic examples where such speed should reach the relativistic regime. In here, we provide a methodology for the theoretical and numerical analysis of the interaction of light pulses with objects moving with constant speed. The methodology allows one, in particular, to readily compute the transfer of fundamental quantities such as energy and momentum from the light pulse to the object. As an example, we compute the transfer of energy and momentum between a given pulse and a silicon sphere moving at relativistic speeds. The methodology, however, is valid for generic pulses and objects. Particularizing the equations to the case of zero speed allows one to treat static or quasi-static objects. The method is based on the polychromatic T-matrix formalism, which leverages the many publicly available resources for computing T-matrices.","sentences":["The interaction of light with short light pulses is relevant in optical traps, optical tweezers, and many other applications.","The theoretical description of such polychromatic light-matter interaction is challenging, and more so when the object is moving with respect to the light source, albeit with constant speed.","Light sails are futuristic examples where such speed should reach the relativistic regime.","In here, we provide a methodology for the theoretical and numerical analysis of the interaction of light pulses with objects moving with constant speed.","The methodology allows one, in particular, to readily compute the transfer of fundamental quantities such as energy and momentum from the light pulse to the object.","As an example, we compute the transfer of energy and momentum between a given pulse and a silicon sphere moving at relativistic speeds.","The methodology, however, is valid for generic pulses and objects.","Particularizing the equations to the case of zero speed allows one to treat static or quasi-static objects.","The method is based on the polychromatic T-matrix formalism, which leverages the many publicly available resources for computing T-matrices."],"url":"http://arxiv.org/abs/2404.05117v1","category":"physics.optics"}
{"created":"2024-04-07 23:10:26","title":"VMambaMorph: a Visual Mamba-based Framework with Cross-Scan Module for Deformable 3D Image Registration","abstract":"Image registration, a critical process in medical imaging, involves aligning different sets of medical imaging data into a single unified coordinate system. Deep learning networks, such as the Convolutional Neural Network (CNN)-based VoxelMorph, Vision Transformer (ViT)-based TransMorph, and State Space Model (SSM)-based MambaMorph, have demonstrated effective performance in this domain. The recent Visual State Space Model (VMamba), which incorporates a cross-scan module with SSM, has exhibited promising improvements in modeling global-range dependencies with efficient computational cost in computer vision tasks. This paper hereby introduces an exploration of VMamba with image registration, named VMambaMorph. This novel hybrid VMamba-CNN network is designed specifically for 3D image registration. Utilizing a U-shaped network architecture, VMambaMorph computes the deformation field based on target and source volumes. The VMamba-based block with 2D cross-scan module is redesigned for 3D volumetric feature processing, and a fine-grained feature extraction module is proposed for high-dimensional feature learning. We validate VMambaMorph using a public benchmark brain MR-CT registration dataset, comparing its performance against current state-of-the-art methods. The results indicate that VMambaMorph achieves competitive registration quality. The code for VMambaMorph is available on GitHub.","sentences":["Image registration, a critical process in medical imaging, involves aligning different sets of medical imaging data into a single unified coordinate system.","Deep learning networks, such as the Convolutional Neural Network (CNN)-based VoxelMorph, Vision Transformer (ViT)-based TransMorph, and State Space Model (SSM)-based MambaMorph, have demonstrated effective performance in this domain.","The recent Visual State Space Model (VMamba), which incorporates a cross-scan module with SSM, has exhibited promising improvements in modeling global-range dependencies with efficient computational cost in computer vision tasks.","This paper hereby introduces an exploration of VMamba with image registration, named VMambaMorph.","This novel hybrid VMamba-CNN network is designed specifically for 3D image registration.","Utilizing a U-shaped network architecture, VMambaMorph computes the deformation field based on target and source volumes.","The VMamba-based block with 2D cross-scan module is redesigned for 3D volumetric feature processing, and a fine-grained feature extraction module is proposed for high-dimensional feature learning.","We validate VMambaMorph using a public benchmark brain MR-CT registration dataset, comparing its performance against current state-of-the-art methods.","The results indicate that VMambaMorph achieves competitive registration quality.","The code for VMambaMorph is available on GitHub."],"url":"http://arxiv.org/abs/2404.05105v1","category":"cs.CV"}
{"created":"2024-04-07 22:58:18","title":"LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation","abstract":"As a result of the rise of Transformer architectures in medical image analysis, specifically in the domain of medical image segmentation, a multitude of hybrid models have been created that merge the advantages of Convolutional Neural Networks (CNNs) and Transformers. These hybrid models have achieved notable success by significantly improving segmentation accuracy. Yet, this progress often comes at the cost of increased model complexity, both in terms of parameters and computational demand. Moreover, many of these models fail to consider the crucial interplay between spatial and channel features, which could further refine and improve segmentation outcomes. To address this, we introduce LHU-Net, a Light Hybrid U-Net architecture optimized for volumetric medical image segmentation. LHU-Net is meticulously designed to prioritize spatial feature analysis in its initial layers before shifting focus to channel-based features in its deeper layers, ensuring a comprehensive feature extraction process. Rigorous evaluation across five benchmark datasets - Synapse, LA, Pancreas, ACDC, and BRaTS 2018 - underscores LHU-Net's superior performance, showcasing its dual capacity for efficiency and accuracy. Notably, LHU-Net sets new performance benchmarks, such as attaining a Dice score of 92.66 on the ACDC dataset, while simultaneously reducing parameters by 85% and quartering the computational load compared to existing state-of-the-art models. Achieved without any reliance on pre-training, additional data, or model ensemble, LHU-Net's effectiveness is further evidenced by its state-of-the-art performance across all evaluated datasets, utilizing fewer than 11 million parameters. This achievement highlights that balancing computational efficiency with high accuracy in medical image segmentation is feasible. Our implementation of LHU-Net is freely accessible to the research community on GitHub.","sentences":["As a result of the rise of Transformer architectures in medical image analysis, specifically in the domain of medical image segmentation, a multitude of hybrid models have been created that merge the advantages of Convolutional Neural Networks (CNNs) and Transformers.","These hybrid models have achieved notable success by significantly improving segmentation accuracy.","Yet, this progress often comes at the cost of increased model complexity, both in terms of parameters and computational demand.","Moreover, many of these models fail to consider the crucial interplay between spatial and channel features, which could further refine and improve segmentation outcomes.","To address this, we introduce LHU-Net, a Light Hybrid U-Net architecture optimized for volumetric medical image segmentation.","LHU-Net is meticulously designed to prioritize spatial feature analysis in its initial layers before shifting focus to channel-based features in its deeper layers, ensuring a comprehensive feature extraction process.","Rigorous evaluation across five benchmark datasets - Synapse, LA, Pancreas, ACDC, and BRaTS 2018 - underscores LHU-Net's superior performance, showcasing its dual capacity for efficiency and accuracy.","Notably, LHU-Net sets new performance benchmarks, such as attaining a Dice score of 92.66 on the ACDC dataset, while simultaneously reducing parameters by 85% and quartering the computational load compared to existing state-of-the-art models.","Achieved without any reliance on pre-training, additional data, or model ensemble, LHU-Net's effectiveness is further evidenced by its state-of-the-art performance across all evaluated datasets, utilizing fewer than 11 million parameters.","This achievement highlights that balancing computational efficiency with high accuracy in medical image segmentation is feasible.","Our implementation of LHU-Net is freely accessible to the research community on GitHub."],"url":"http://arxiv.org/abs/2404.05102v1","category":"eess.IV"}
{"created":"2024-04-07 21:37:24","title":"Random Sequential Adsorption with Correlated Defects: A Series Expansion Approach","abstract":"The Random Sequential Adsorption (RSA) problem holds crucial theoretical and practical significance, serving as a pivotal framework for understanding and optimizing particle packing in various scientific and technological applications. Here the problem of the one-dimensional RSA of k-mers onto a substrate with correlated defects controlled by uniform and power-law distributions is theoretically investigated: the coverage fraction is obtained as a function of the density of defects and several scaling laws are examined. The results are compared with extensive Monte Carlo simulations and more traditional methods based on master equations. Emphasis is given in elucidating the scaling behavior of the fluctuations of the coverage fraction. The phenomenon of universality breaking and the issues of conventional gaussian fluctuations and the L\\'evy type fluctuations from a simple perspective, relying on the Central Limit Theorem, are also addressed.","sentences":["The Random Sequential Adsorption (RSA) problem holds crucial theoretical and practical significance, serving as a pivotal framework for understanding and optimizing particle packing in various scientific and technological applications.","Here the problem of the one-dimensional RSA of k-mers onto a substrate with correlated defects controlled by uniform and power-law distributions is theoretically investigated: the coverage fraction is obtained as a function of the density of defects and several scaling laws are examined.","The results are compared with extensive Monte Carlo simulations and more traditional methods based on master equations.","Emphasis is given in elucidating the scaling behavior of the fluctuations of the coverage fraction.","The phenomenon of universality breaking and the issues of conventional gaussian fluctuations and the L\\'evy type fluctuations from a simple perspective, relying on the Central Limit Theorem, are also addressed."],"url":"http://arxiv.org/abs/2404.05080v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-07 20:33:12","title":"Entire solutions to the Swift--Hohenberg equation via variational approach","abstract":"We study the stationary Swift--Hohenberg equation $(\\Delta + 1)^2 u - \\alpha u - \\beta u^2 + u^3=0$ in the whole space $\\mathbb R^n$, $2\\le n \\le 7$. We develop and modify the variational approach introduced by Lerman, Naryshkin and Nazarov (2020) and obtain a series of periodic solutions with certain additional symmetries.","sentences":["We study the stationary Swift--Hohenberg equation $(\\Delta + 1)^2 u - \\alpha u - \\beta u^2 + u^3=0$ in the whole space $\\mathbb R^n$, $2\\le n \\le 7$.","We develop and modify the variational approach introduced by Lerman, Naryshkin and Nazarov (2020) and obtain a series of periodic solutions with certain additional symmetries."],"url":"http://arxiv.org/abs/2404.05066v1","category":"math.AP"}
{"created":"2024-04-07 20:09:37","title":"Detailed balance in non-equilibrium dynamics of granular matter: derivation and implications","abstract":"Discovering fundamental principles governing the dynamics of granular media has been a long-standing challenge. Recent predictions of detailed balance steady states (DBSS), supported by experimental observations in cyclic shear experiments of planar granular systems, called into question the common belief that the detailed balance principle is only a feature of equilibrium. Here, we first show analytically that DBSS in planar granular dynamics arise when a certain conditional cell order distribution is independent of the condition. We then demonstrate that this condition is met in rotational shear experiments, which indeed also give rise to robust DBSS. This suggests that DBSS not only exist but are also quite common. We also show that, when the unconditional cell order distribution maximises the entropy, as has been found recently, then this distribution is determined by a single parameter - the ratio of splitting and merging rates of cells of any arbitrary order. These results simplify the modelling of the complex dynamics of planar granular systems to the solution of recently proposed evolution equations, demonstrating their predictive power.","sentences":["Discovering fundamental principles governing the dynamics of granular media has been a long-standing challenge.","Recent predictions of detailed balance steady states (DBSS), supported by experimental observations in cyclic shear experiments of planar granular systems, called into question the common belief that the detailed balance principle is only a feature of equilibrium.","Here, we first show analytically that DBSS in planar granular dynamics arise when a certain conditional cell order distribution is independent of the condition.","We then demonstrate that this condition is met in rotational shear experiments, which indeed also give rise to robust DBSS.","This suggests that DBSS not only exist but are also quite common.","We also show that, when the unconditional cell order distribution maximises the entropy, as has been found recently, then this distribution is determined by a single parameter - the ratio of splitting and merging rates of cells of any arbitrary order.","These results simplify the modelling of the complex dynamics of planar granular systems to the solution of recently proposed evolution equations, demonstrating their predictive power."],"url":"http://arxiv.org/abs/2404.05059v1","category":"cond-mat.soft"}
{"created":"2024-04-07 19:29:06","title":"Mean field equations arising from random vortex dynamics","abstract":"We consider Mckean-Vlasov type stochastic differential equations with multiplicative noise arising from the random vortex method. Such an equation can be viewed as the mean-field limit of interacting particle systems with singular interacting kernels such as the Biot-Savart kernel. A new estimate for the transition probability density of diffusion processes will be formulated to handle the singularity of the interacting kernel. The existence and uniqueness of the weak solution of such SDEs will be established as the main result.","sentences":["We consider Mckean-Vlasov type stochastic differential equations with multiplicative noise arising from the random vortex method.","Such an equation can be viewed as the mean-field limit of interacting particle systems with singular interacting kernels such as the Biot-Savart kernel.","A new estimate for the transition probability density of diffusion processes will be formulated to handle the singularity of the interacting kernel.","The existence and uniqueness of the weak solution of such SDEs will be established as the main result."],"url":"http://arxiv.org/abs/2404.05054v1","category":"math.PR"}
{"created":"2024-04-07 19:02:50","title":"Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular Data Using GPT-4","abstract":"We investigate the application of large language models (LLMs), specifically GPT-4, to scenarios involving the tradeoff between privacy and utility in tabular data. Our approach entails prompting GPT-4 by transforming tabular data points into textual format, followed by the inclusion of precise sanitization instructions in a zero-shot manner. The primary objective is to sanitize the tabular data in such a way that it hinders existing machine learning models from accurately inferring private features while allowing models to accurately infer utility-related attributes. We explore various sanitization instructions. Notably, we discover that this relatively simple approach yields performance comparable to more complex adversarial optimization methods used for managing privacy-utility tradeoffs. Furthermore, while the prompts successfully obscure private features from the detection capabilities of existing machine learning models, we observe that this obscuration alone does not necessarily meet a range of fairness metrics. Nevertheless, our research indicates the potential effectiveness of LLMs in adhering to these fairness metrics, with some of our experimental results aligning with those achieved by well-established adversarial optimization techniques.","sentences":["We investigate the application of large language models (LLMs), specifically GPT-4, to scenarios involving the tradeoff between privacy and utility in tabular data.","Our approach entails prompting GPT-4 by transforming tabular data points into textual format, followed by the inclusion of precise sanitization instructions in a zero-shot manner.","The primary objective is to sanitize the tabular data in such a way that it hinders existing machine learning models from accurately inferring private features while allowing models to accurately infer utility-related attributes.","We explore various sanitization instructions.","Notably, we discover that this relatively simple approach yields performance comparable to more complex adversarial optimization methods used for managing privacy-utility tradeoffs.","Furthermore, while the prompts successfully obscure private features from the detection capabilities of existing machine learning models, we observe that this obscuration alone does not necessarily meet a range of fairness metrics.","Nevertheless, our research indicates the potential effectiveness of LLMs in adhering to these fairness metrics, with some of our experimental results aligning with those achieved by well-established adversarial optimization techniques."],"url":"http://arxiv.org/abs/2404.05047v1","category":"cs.LG"}
{"created":"2024-04-07 18:55:33","title":"Optimizing Privacy and Utility Tradeoffs for Group Interests Through Harmonization","abstract":"We propose a novel problem formulation to address the privacy-utility tradeoff, specifically when dealing with two distinct user groups characterized by unique sets of private and utility attributes. Unlike previous studies that primarily focus on scenarios where all users share identical private and utility attributes and often rely on auxiliary datasets or manual annotations, we introduce a collaborative data-sharing mechanism between two user groups through a trusted third party. This third party uses adversarial privacy techniques with our proposed data-sharing mechanism to internally sanitize data for both groups and eliminates the need for manual annotation or auxiliary datasets. Our methodology ensures that private attributes cannot be accurately inferred while enabling highly accurate predictions of utility features. Importantly, even if analysts or adversaries possess auxiliary datasets containing raw data, they are unable to accurately deduce private features. Additionally, our data-sharing mechanism is compatible with various existing adversarially trained privacy techniques. We empirically demonstrate the effectiveness of our approach using synthetic and real-world datasets, showcasing its ability to balance the conflicting goals of privacy and utility.","sentences":["We propose a novel problem formulation to address the privacy-utility tradeoff, specifically when dealing with two distinct user groups characterized by unique sets of private and utility attributes.","Unlike previous studies that primarily focus on scenarios where all users share identical private and utility attributes and often rely on auxiliary datasets or manual annotations, we introduce a collaborative data-sharing mechanism between two user groups through a trusted third party.","This third party uses adversarial privacy techniques with our proposed data-sharing mechanism to internally sanitize data for both groups and eliminates the need for manual annotation or auxiliary datasets.","Our methodology ensures that private attributes cannot be accurately inferred while enabling highly accurate predictions of utility features.","Importantly, even if analysts or adversaries possess auxiliary datasets containing raw data, they are unable to accurately deduce private features.","Additionally, our data-sharing mechanism is compatible with various existing adversarially trained privacy techniques.","We empirically demonstrate the effectiveness of our approach using synthetic and real-world datasets, showcasing its ability to balance the conflicting goals of privacy and utility."],"url":"http://arxiv.org/abs/2404.05043v1","category":"cs.LG"}
{"created":"2024-04-07 17:25:24","title":"Context-dependent Causality (the Non-Nonotonic Case)","abstract":"We develop a novel identification strategy as well as a new estimator for context-dependent causal inference in non-parametric triangular models with non-separable disturbances. Departing from the common practice, our analysis does not rely on the strict monotonicity assumption. Our key contribution lies in leveraging on diffusion models to formulate the structural equations as a system evolving from noise accumulation to account for the influence of the latent context (confounder) on the outcome. Our identifiability strategy involves a system of Fredholm integral equations expressing the distributional relationship between a latent context variable and a vector of observables. These integral equations involve an unknown kernel and are governed by a set of structural form functions, inducing a non-monotonic inverse problem. We prove that if the kernel density can be represented as an infinite mixture of Gaussians, then there exists a unique solution for the unknown function. This is a significant result, as it shows that it is possible to solve a non-monotonic inverse problem even when the kernel is unknown. On the methodological front we leverage on a novel and enriched Contaminated Generative Adversarial (Neural) Networks (CONGAN) which we provide as a solution to the non-monotonic inverse problem.","sentences":["We develop a novel identification strategy as well as a new estimator for context-dependent causal inference in non-parametric triangular models with non-separable disturbances.","Departing from the common practice, our analysis does not rely on the strict monotonicity assumption.","Our key contribution lies in leveraging on diffusion models to formulate the structural equations as a system evolving from noise accumulation to account for the influence of the latent context (confounder) on the outcome.","Our identifiability strategy involves a system of Fredholm integral equations expressing the distributional relationship between a latent context variable and a vector of observables.","These integral equations involve an unknown kernel and are governed by a set of structural form functions, inducing a non-monotonic inverse problem.","We prove that if the kernel density can be represented as an infinite mixture of Gaussians, then there exists a unique solution for the unknown function.","This is a significant result, as it shows that it is possible to solve a non-monotonic inverse problem even when the kernel is unknown.","On the methodological front we leverage on a novel and enriched Contaminated Generative Adversarial (Neural) Networks (CONGAN) which we provide as a solution to the non-monotonic inverse problem."],"url":"http://arxiv.org/abs/2404.05021v1","category":"econ.EM"}
{"created":"2024-04-07 16:18:24","title":"Generative downscaling of PDE solvers with physics-guided diffusion models","abstract":"Solving partial differential equations (PDEs) on fine spatio-temporal scales for high-fidelity solutions is critical for numerous scientific breakthroughs. Yet, this process can be prohibitively expensive, owing to the inherent complexities of the problems, including nonlinearity and multiscale phenomena. To speed up large-scale computations, a process known as downscaling is employed, which generates high-fidelity approximate solutions from their low-fidelity counterparts. In this paper, we propose a novel Physics-Guided Diffusion Model (PGDM) for downscaling. Our model, initially trained on a dataset comprising low-and-high-fidelity paired solutions across coarse and fine scales, generates new high-fidelity approximations from any new low-fidelity inputs. These outputs are subsequently refined through fine-tuning, aimed at minimizing the physical discrepancies as defined by the discretized PDEs at the finer scale. We evaluate and benchmark our model's performance against other downscaling baselines in three categories of nonlinear PDEs. Our numerical experiments demonstrate that our model not only outperforms the baselines but also achieves a computational acceleration exceeding tenfold, while maintaining the same level of accuracy as the conventional fine-scale solvers.","sentences":["Solving partial differential equations (PDEs) on fine spatio-temporal scales for high-fidelity solutions is critical for numerous scientific breakthroughs.","Yet, this process can be prohibitively expensive, owing to the inherent complexities of the problems, including nonlinearity and multiscale phenomena.","To speed up large-scale computations, a process known as downscaling is employed, which generates high-fidelity approximate solutions from their low-fidelity counterparts.","In this paper, we propose a novel Physics-Guided Diffusion Model (PGDM) for downscaling.","Our model, initially trained on a dataset comprising low-and-high-fidelity paired solutions across coarse and fine scales, generates new high-fidelity approximations from any new low-fidelity inputs.","These outputs are subsequently refined through fine-tuning, aimed at minimizing the physical discrepancies as defined by the discretized PDEs at the finer scale.","We evaluate and benchmark our model's performance against other downscaling baselines in three categories of nonlinear PDEs.","Our numerical experiments demonstrate that our model not only outperforms the baselines but also achieves a computational acceleration exceeding tenfold, while maintaining the same level of accuracy as the conventional fine-scale solvers."],"url":"http://arxiv.org/abs/2404.05009v1","category":"math.NA"}
{"created":"2024-04-07 15:59:32","title":"An Energy Conserving, Implicit and Higher Order in Time Discretization of Maxwell's Equations","abstract":"This is a sequel to our earlier work in which we described an implicit leapfrog scheme in conjunction with a higher order mixed finite element discretization of a system of Maxwell's equations. In our earlier work, we focussed on providing the error analysis for both the semidiscretization of the Maxwell's equations using an implicit leapfrog scheme that we invented as well as providing the error analysis for the full discretization using this time domain scheme in conjunction with higher order mixed finite elements from finite element exterior calculus. In this work, we record our initial results with extending our implicit leapfrog scheme from being a discretization that is second order accurate in time to an arbitrary (even) order accurate in time method. Towards this end, we provide here the complete error analysis for the semidiscretization in time and full discretization of the Maxwell's equations for the fourth order scheme. We leave the completion of our efforts in providing all the necessary proofs for the general scheme to an immediate future update of this work.","sentences":["This is a sequel to our earlier work in which we described an implicit leapfrog scheme in conjunction with a higher order mixed finite element discretization of a system of Maxwell's equations.","In our earlier work, we focussed on providing the error analysis for both the semidiscretization of the Maxwell's equations using an implicit leapfrog scheme that we invented as well as providing the error analysis for the full discretization using this time domain scheme in conjunction with higher order mixed finite elements from finite element exterior calculus.","In this work, we record our initial results with extending our implicit leapfrog scheme from being a discretization that is second order accurate in time to an arbitrary (even) order accurate in time method.","Towards this end, we provide here the complete error analysis for the semidiscretization in time and full discretization of the Maxwell's equations for the fourth order scheme.","We leave the completion of our efforts in providing all the necessary proofs for the general scheme to an immediate future update of this work."],"url":"http://arxiv.org/abs/2404.05004v1","category":"math.NA"}
{"created":"2024-04-07 15:48:45","title":"Long-time asymptotics of the Tzitz\u00e9ica equation on the line","abstract":"In this paper, the renowned Riemann-Hilbert method is employed to investigate the initial value problem of Tzitz\\'eica equation on the line. Initially, our analysis focuses on elucidating the properties of two reflection coefficients, which are determined by the initial values. Subsequently, leveraging these reflection coefficients, we construct a Riemann-Hilbert problem that is a powerful tool to articulate the solution of the Tzitz\\'eica equation. Finally, the nonlinear steepest descent method is applied to the oscillatory Riemann-Hilbert problem, which enables us to delineate the long-time asymptotic behaviors of solutions to the Tzitz\\'eica equation across various regions. Moreover, it is shown that the leading-order terms of asymptotic formulas match well with direct numerical simulations.","sentences":["In this paper, the renowned Riemann-Hilbert method is employed to investigate the initial value problem of Tzitz\\'eica equation on the line.","Initially, our analysis focuses on elucidating the properties of two reflection coefficients, which are determined by the initial values.","Subsequently, leveraging these reflection coefficients, we construct a Riemann-Hilbert problem that is a powerful tool to articulate the solution of the Tzitz\\'eica equation.","Finally, the nonlinear steepest descent method is applied to the oscillatory Riemann-Hilbert problem, which enables us to delineate the long-time asymptotic behaviors of solutions to the Tzitz\\'eica equation across various regions.","Moreover, it is shown that the leading-order terms of asymptotic formulas match well with direct numerical simulations."],"url":"http://arxiv.org/abs/2404.04999v1","category":"math-ph"}
{"created":"2024-04-07 15:19:12","title":"Darboux, Moser and Weinstein theorems for prequantum systems","abstract":"We establish analogs of the Darboux, Moser and Weinstein theorems for prequantum systems. We show that two prequantum systems on a manifold with vanishing first cohomology, with symplectic forms defining the same cohomology class and homotopic to each other within that class, differ only by a symplectomorphism and a gauge transformation. As an application, we show that the Bohr-Sommerfeld quantization of prequantum system on a manifold with trivial first cohomology is independent of the choice of the connection.","sentences":["We establish analogs of the Darboux, Moser and Weinstein theorems for prequantum systems.","We show that two prequantum systems on a manifold with vanishing first cohomology, with symplectic forms defining the same cohomology class and homotopic to each other within that class, differ only by a symplectomorphism and a gauge transformation.","As an application, we show that the Bohr-Sommerfeld quantization of prequantum system on a manifold with trivial first cohomology is independent of the choice of the connection."],"url":"http://arxiv.org/abs/2404.04988v1","category":"math.SG"}
{"created":"2024-04-07 14:37:04","title":"Quantum electrodynamics of lossy samples in vacuum: modified Langevin noise formalism","abstract":"Quantum behavior of the electromagnetic field in unbounded macroscopic media displaying absorption is properly described by the Langevin noise formalism (macroscopic quantum electrodynamics) where the field is assumed to be entirely produced by medium fluctuating sources via the dyadic Green's function. On the other hand, such formalism leads to inconsistencies when applied to finite-size lossy objects placed in vacuum since it does not take into account the scattering modes, whose homomeneous plane wave asymptotic behavior is ruled out in unbounded lossy media. Accordingly a modified Langevin noise formalism has been proposed to encompass the scattering modes and its consistency has been numerically validated in few specific geometries. In this paper we analytically derive the modified Langevin noise formalism from the enstablished canonical quantization of the electromagnetic field in macroscopic media, thus proving that it models any possible scenario involving linear, inhomegeneous and magnetodielectric samples. The derivation starts from quantum Maxwell equations in the Heisenberg picture together with their formal solution as the superposition of the medium assisted field and the scattering modes. We analytically prove that each of the two field parts can be expressed in term of particular bosonic operators, which in turn diagonalize the electromagnetic Hamiltonian and whose associated quasi-particles are medium assisted and scattering polaritons, respectively. The key ingredient underpinning our reasoning is a peculiar integral relation linking the far field amplitude of the dyadic Green's function and the scattering modes, relation we rigorously derive and physically explain by identifying the scattering modes as fields generated by infinitely far dipole point sources.","sentences":["Quantum behavior of the electromagnetic field in unbounded macroscopic media displaying absorption is properly described by the Langevin noise formalism (macroscopic quantum electrodynamics) where the field is assumed to be entirely produced by medium fluctuating sources via the dyadic Green's function.","On the other hand, such formalism leads to inconsistencies when applied to finite-size lossy objects placed in vacuum since it does not take into account the scattering modes, whose homomeneous plane wave asymptotic behavior is ruled out in unbounded lossy media.","Accordingly a modified Langevin noise formalism has been proposed to encompass the scattering modes and its consistency has been numerically validated in few specific geometries.","In this paper we analytically derive the modified Langevin noise formalism from the enstablished canonical quantization of the electromagnetic field in macroscopic media, thus proving that it models any possible scenario involving linear, inhomegeneous and magnetodielectric samples.","The derivation starts from quantum Maxwell equations in the Heisenberg picture together with their formal solution as the superposition of the medium assisted field and the scattering modes.","We analytically prove that each of the two field parts can be expressed in term of particular bosonic operators, which in turn diagonalize the electromagnetic Hamiltonian and whose associated quasi-particles are medium assisted and scattering polaritons, respectively.","The key ingredient underpinning our reasoning is a peculiar integral relation linking the far field amplitude of the dyadic Green's function and the scattering modes, relation we rigorously derive and physically explain by identifying the scattering modes as fields generated by infinitely far dipole point sources."],"url":"http://arxiv.org/abs/2404.04977v1","category":"quant-ph"}
{"created":"2024-04-07 14:30:11","title":"Nanometer Scanning with Micrometer Sensing: Beating Quantization Constraints in Lissajous Trajectory Tracking","abstract":"This paper addresses the task of tracking Lissajous trajectories in the presence of quantized positioning sensors. To do so, theoretical results on tracking of continuous time periodic signals in the presence of output quantization are provided. With these results in hand, the application to Lissajous tracking is explored. The method proposed relies on the internal model principle and dispenses perfect knowledge of the system equations. Numerical results show that an arbitrary small scanning resolution is achievable despite large sensor quantization intervals.","sentences":["This paper addresses the task of tracking Lissajous trajectories in the presence of quantized positioning sensors.","To do so, theoretical results on tracking of continuous time periodic signals in the presence of output quantization are provided.","With these results in hand, the application to Lissajous tracking is explored.","The method proposed relies on the internal model principle and dispenses perfect knowledge of the system equations.","Numerical results show that an arbitrary small scanning resolution is achievable despite large sensor quantization intervals."],"url":"http://arxiv.org/abs/2404.04973v1","category":"eess.SY"}
{"created":"2024-04-07 12:46:36","title":"A Fast Observability for Diffusion Equations in $\\mathbb R^N$","abstract":"Given an equidistributed set in the whole Euclidean space, we have established in [1] that there exists a constant positive $C$ such that the observability inequality of diffusion equations holds for all $T\\in]0,1[$, with an observability cost being of the form $Ce^{C/T}$. In this paper, for any small constant $\\varepsilon>0$, we prove that there exists a nontrivial equidistributed set (in the sense that whose complementary set is unbounded), so that the above observability cost can be improved to a fast form of $Ce^{\\varepsilon/T}$ for certain constant $C>0$. The proof is based on the strategy used in [1], as well as an interpolation inequality for gradients of solutions to elliptic equations obtained recently in [2].","sentences":["Given an equidistributed set in the whole Euclidean space, we have established in [1] that there exists a constant positive $C$ such that the observability inequality of diffusion equations holds for all $T\\in]0,1[$, with an observability cost being of the form $Ce^{C/T}$.","In this paper, for any small constant $\\varepsilon>0$, we prove that there exists a nontrivial equidistributed set (in the sense that whose complementary set is unbounded), so that the above observability cost can be improved to a fast form of $Ce^{\\varepsilon/T}$ for certain constant $C>0$. The proof is based on the strategy used in [1], as well as an interpolation inequality for gradients of solutions to elliptic equations obtained recently in [2]."],"url":"http://arxiv.org/abs/2404.04945v1","category":"math.AP"}
{"created":"2024-04-07 11:22:18","title":"Linear differential equation approach to the Loschmidt amplitude","abstract":"The Loschmidt echo is a popular quantity that allows making predictions about the stability of quantum states under time evolution. In our work, we present an approach that allows us to find a differential equation that can be used to compute the Loschmidt echo. This approach, while in essence perturbative, has the advantage that it converges at finite order. We demonstrate that the approach for generically chosen matrix Hamiltonians often offers advantages over Taylor and cumulant expansions even when we truncate at finite order. We then apply the approach to two ordinary band Hamiltonians (multi-Weyl semimetals and AB bilayer graphene) to obtain the Loschmidt echo after a quench for an arbitrary starting state and find that the results readily generalize to find transmission amplitudes and specific contributions to the partition function, too. We then test our methods on many body spin and fermionic Hamiltonians and find that while the approach still offers advantages, more care has to be taken than in a generic case.","sentences":["The Loschmidt echo is a popular quantity that allows making predictions about the stability of quantum states under time evolution.","In our work, we present an approach that allows us to find a differential equation that can be used to compute the Loschmidt echo.","This approach, while in essence perturbative, has the advantage that it converges at finite order.","We demonstrate that the approach for generically chosen matrix Hamiltonians often offers advantages over Taylor and cumulant expansions even when we truncate at finite order.","We then apply the approach to two ordinary band Hamiltonians (multi-Weyl semimetals and AB bilayer graphene) to obtain the Loschmidt echo after a quench for an arbitrary starting state and find that the results readily generalize to find transmission amplitudes and specific contributions to the partition function, too.","We then test our methods on many body spin and fermionic Hamiltonians and find that while the approach still offers advantages, more care has to be taken than in a generic case."],"url":"http://arxiv.org/abs/2404.04921v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-07 09:48:12","title":"Photonic time crystals: Theory and applications","abstract":"This tutorial offers a comprehensive overview of photonic time crystals - artificial materials whose electromagnetic properties are periodically modulated in time at scales comparable to the oscillation period of light while remaining spatially uniform. Being the temporal analogs to traditional photonic crystals, photonic time crystals differ in that they exhibit momentum bandgaps instead of energy bandgaps. The energy is not conserved within momentum bandgaps, and eigenmodes with exponentially growing amplitudes exist in the momentum bandgap. Such properties make photonic time crystals a fascinating novel class of artificial materials from a basic science and applied perspective. This tutorial overviews the fundamental electromagnetic equations governing photonic time crystals and explores the groundbreaking physical phenomena they support. Based on these properties, we also oversee a diverse range of applications they unlock. Different material platforms suitable for creating photonic time crystals are discussed and compared. Furthermore, we elaborate on the connections between wave amplification in photonic time crystals and parametric amplification mechanisms in electrical circuits and nonlinear optics. The tutorial will be helpful for readers with physics or engineering backgrounds. It is designed to serve as an introductory guide for beginners and to establish a reference baseline reflecting the current understanding for researchers in the field.","sentences":["This tutorial offers a comprehensive overview of photonic time crystals - artificial materials whose electromagnetic properties are periodically modulated in time at scales comparable to the oscillation period of light while remaining spatially uniform.","Being the temporal analogs to traditional photonic crystals, photonic time crystals differ in that they exhibit momentum bandgaps instead of energy bandgaps.","The energy is not conserved within momentum bandgaps, and eigenmodes with exponentially growing amplitudes exist in the momentum bandgap.","Such properties make photonic time crystals a fascinating novel class of artificial materials from a basic science and applied perspective.","This tutorial overviews the fundamental electromagnetic equations governing photonic time crystals and explores the groundbreaking physical phenomena they support.","Based on these properties, we also oversee a diverse range of applications they unlock.","Different material platforms suitable for creating photonic time crystals are discussed and compared.","Furthermore, we elaborate on the connections between wave amplification in photonic time crystals and parametric amplification mechanisms in electrical circuits and nonlinear optics.","The tutorial will be helpful for readers with physics or engineering backgrounds.","It is designed to serve as an introductory guide for beginners and to establish a reference baseline reflecting the current understanding for researchers in the field."],"url":"http://arxiv.org/abs/2404.04899v1","category":"physics.optics"}
{"created":"2024-04-07 09:48:05","title":"Graph Neural Network Meets Multi-Agent Reinforcement Learning: Fundamentals, Applications, and Future Directions","abstract":"Multi-agent reinforcement learning (MARL) has become a fundamental component of next-generation wireless communication systems. Theoretically, although MARL has the advantages of low computational complexity and fast convergence rate, there exist several challenges including partial observability, non-stationary, and scalability. In this article, we investigate a novel MARL with graph neural network-aided communication (GNNComm-MARL) to address the aforementioned challenges by making use of graph attention networks to effectively sample neighborhoods and selectively aggregate messages. Furthermore, we thoroughly study the architecture of GNNComm-MARL and present a systematic design solution. We then present the typical applications of GNNComm-MARL from two aspects: resource allocation and mobility management. The results obtained unveil that GNNComm-MARL can achieve better performance with lower communication overhead compared to conventional communication schemes. Finally, several important research directions regarding GNNComm-MARL are presented to facilitate further investigation.","sentences":["Multi-agent reinforcement learning (MARL) has become a fundamental component of next-generation wireless communication systems.","Theoretically, although MARL has the advantages of low computational complexity and fast convergence rate, there exist several challenges including partial observability, non-stationary, and scalability.","In this article, we investigate a novel MARL with graph neural network-aided communication (GNNComm-MARL) to address the aforementioned challenges by making use of graph attention networks to effectively sample neighborhoods and selectively aggregate messages.","Furthermore, we thoroughly study the architecture of GNNComm-MARL and present a systematic design solution.","We then present the typical applications of GNNComm-MARL from two aspects: resource allocation and mobility management.","The results obtained unveil that GNNComm-MARL can achieve better performance with lower communication overhead compared to conventional communication schemes.","Finally, several important research directions regarding GNNComm-MARL are presented to facilitate further investigation."],"url":"http://arxiv.org/abs/2404.04898v1","category":"cs.IT"}
{"created":"2024-04-07 09:17:00","title":"DL-EWF: Deep Learning Empowering Women's Fashion with Grounded-Segment-Anything Segmentation for Body Shape Classification","abstract":"The global fashion industry plays a pivotal role in the global economy, and addressing fundamental issues within the industry is crucial for developing innovative solutions. One of the most pressing challenges in the fashion industry is the mismatch between body shapes and the garments of individuals they purchase. This issue is particularly prevalent among individuals with non-ideal body shapes, exacerbating the challenges faced. Considering inter-individual variability in body shapes is essential for designing and producing garments that are widely accepted by consumers. Traditional methods for determining human body shape are limited due to their low accuracy, high costs, and time-consuming nature. New approaches, utilizing digital imaging and deep neural networks (DNN), have been introduced to identify human body shape. In this study, the Style4BodyShape dataset is used for classifying body shapes into five categories: Rectangle, Triangle, Inverted Triangle, Hourglass, and Apple. In this paper, the body shape segmentation of a person is extracted from the image, disregarding the surroundings and background. Then, Various pre-trained models, such as ResNet18, ResNet34, ResNet50, VGG16, VGG19, and Inception v3, are used to classify the segmentation results. Among these pre-trained models, the Inception V3 model demonstrates superior performance regarding f1-score evaluation metric and accuracy compared to the other models.","sentences":["The global fashion industry plays a pivotal role in the global economy, and addressing fundamental issues within the industry is crucial for developing innovative solutions.","One of the most pressing challenges in the fashion industry is the mismatch between body shapes and the garments of individuals they purchase.","This issue is particularly prevalent among individuals with non-ideal body shapes, exacerbating the challenges faced.","Considering inter-individual variability in body shapes is essential for designing and producing garments that are widely accepted by consumers.","Traditional methods for determining human body shape are limited due to their low accuracy, high costs, and time-consuming nature.","New approaches, utilizing digital imaging and deep neural networks (DNN), have been introduced to identify human body shape.","In this study, the Style4BodyShape dataset is used for classifying body shapes into five categories: Rectangle, Triangle, Inverted Triangle, Hourglass, and Apple.","In this paper, the body shape segmentation of a person is extracted from the image, disregarding the surroundings and background.","Then, Various pre-trained models, such as ResNet18, ResNet34, ResNet50, VGG16, VGG19, and Inception v3, are used to classify the segmentation results.","Among these pre-trained models, the Inception V3 model demonstrates superior performance regarding f1-score evaluation metric and accuracy compared to the other models."],"url":"http://arxiv.org/abs/2404.04891v1","category":"cs.CV"}
{"created":"2024-04-07 09:10:29","title":"Optical absorption window in Na$_3$Bi based three-dimensional Dirac electronic system","abstract":"We present a detailed theoretical study of the optoelectronic properties of a Na$_3$Bi-based three-dimensional Dirac electronic system (3DDES). The optical conductivity is evaluated using the energy-balance equation derived from a Boltzmann equation, where the electron Hamiltonian is taken from a simplified $\\mathbf{k}\\cdotp \\mathbf{p}$ approach. We find that for short-wavelength irradiation, the optical absorption in Na$_3$Bi is mainly due to inter-band electronic transitions. In contrast to the universal optical conductance observed for graphene, the optical conductivity for Na$_3$Bi based 3DDES depends on the radiation frequency but not on temperature, carrier density and electronic relaxation time. In the radiation wavelength regime of about 5 $\\mu m<\\lambda<$ 200 $\\mu m$, an optical absorption window is found. This is similar to what is observed in graphene. The position and width of the absorption window depend on the direction of the light polarization and sensitively on temperature, carrier density, and electronic relaxation time. Particularly, we demonstrate that the inter-band optical absorption channel can be switched on and off by applying the gate voltage. This implies that similar to graphene, Na$_3$Bi based 3DDES can also be applied in infrared electro-optical modulators. Our theoretical findings are helpful in gaining an in-depth understanding of the basic optoelectronic properties of recently discovered 3DDESs.","sentences":["We present a detailed theoretical study of the optoelectronic properties of a Na$_3$Bi-based three-dimensional Dirac electronic system (3DDES).","The optical conductivity is evaluated using the energy-balance equation derived from a Boltzmann equation, where the electron Hamiltonian is taken from a simplified $\\mathbf{k}\\cdotp \\mathbf{p}$ approach.","We find that for short-wavelength irradiation, the optical absorption in Na$_3$Bi is mainly due to inter-band electronic transitions.","In contrast to the universal optical conductance observed for graphene, the optical conductivity for Na$_3$Bi based 3DDES depends on the radiation frequency but not on temperature, carrier density and electronic relaxation time.","In the radiation wavelength regime of about 5 $\\mu m<\\lambda<$ 200 $\\mu m$, an optical absorption window is found.","This is similar to what is observed in graphene.","The position and width of the absorption window depend on the direction of the light polarization and sensitively on temperature, carrier density, and electronic relaxation time.","Particularly, we demonstrate that the inter-band optical absorption channel can be switched on and off by applying the gate voltage.","This implies that similar to graphene, Na$_3$Bi based 3DDES can also be applied in infrared electro-optical modulators.","Our theoretical findings are helpful in gaining an in-depth understanding of the basic optoelectronic properties of recently discovered 3DDESs."],"url":"http://arxiv.org/abs/2404.04888v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-07 09:05:04","title":"LRNet: Change detection of high-resolution remote sensing imagery via strategy of localization-then-refinement","abstract":"Change detection, as a research hotspot in the field of remote sensing, has witnessed continuous development and progress. However, the discrimination of boundary details remains a significant bottleneck due to the complexity of surrounding elements between change areas and backgrounds. Discriminating the boundaries of large change areas results in misalignment, while connecting boundaries occurs for small change targets. To address the above issues, a novel network based on the localization-then-refinement strategy is proposed in this paper, namely LRNet. LRNet consists of two stages: localization and refinement. In the localization stage, a three-branch encoder simultaneously extracts original image features and their differential features for interactive localization of the position of each change area. To minimize information loss during feature extraction, learnable optimal pooling (LOP) is proposed to replace the widely used max-pooling. Additionally, this process is trainable and contributes to the overall optimization of the network. To effectively interact features from different branches and accurately locate change areas of various sizes, change alignment attention (C2A) and hierarchical change alignment module (HCA) are proposed. In the refinement stage, the localization results from the localization stage are corrected by constraining the change areas and change edges through the edge-area alignment module (E2A). Subsequently, the decoder, combined with the difference features strengthened by C2A in the localization phase, refines change areas of different sizes, ultimately achieving accurate boundary discrimination of change areas. The proposed LRNet outperforms 13 other state-of-the-art methods in terms of comprehensive evaluation metrics and provides the most precise boundary discrimination results on the LEVIR-CD and WHU-CD datasets.","sentences":["Change detection, as a research hotspot in the field of remote sensing, has witnessed continuous development and progress.","However, the discrimination of boundary details remains a significant bottleneck due to the complexity of surrounding elements between change areas and backgrounds.","Discriminating the boundaries of large change areas results in misalignment, while connecting boundaries occurs for small change targets.","To address the above issues, a novel network based on the localization-then-refinement strategy is proposed in this paper, namely LRNet.","LRNet consists of two stages: localization and refinement.","In the localization stage, a three-branch encoder simultaneously extracts original image features and their differential features for interactive localization of the position of each change area.","To minimize information loss during feature extraction, learnable optimal pooling (LOP) is proposed to replace the widely used max-pooling.","Additionally, this process is trainable and contributes to the overall optimization of the network.","To effectively interact features from different branches and accurately locate change areas of various sizes, change alignment attention (C2A) and hierarchical change alignment module (HCA) are proposed.","In the refinement stage, the localization results from the localization stage are corrected by constraining the change areas and change edges through the edge-area alignment module (E2A).","Subsequently, the decoder, combined with the difference features strengthened by C2A in the localization phase, refines change areas of different sizes, ultimately achieving accurate boundary discrimination of change areas.","The proposed LRNet outperforms 13 other state-of-the-art methods in terms of comprehensive evaluation metrics and provides the most precise boundary discrimination results on the LEVIR-CD and WHU-CD datasets."],"url":"http://arxiv.org/abs/2404.04884v1","category":"cs.CV"}
{"created":"2024-04-07 08:48:01","title":"CycleINR: Cycle Implicit Neural Representation for Arbitrary-Scale Volumetric Super-Resolution of Medical Data","abstract":"In the realm of medical 3D data, such as CT and MRI images, prevalent anisotropic resolution is characterized by high intra-slice but diminished inter-slice resolution. The lowered resolution between adjacent slices poses challenges, hindering optimal viewing experiences and impeding the development of robust downstream analysis algorithms. Various volumetric super-resolution algorithms aim to surmount these challenges, enhancing inter-slice resolution and overall 3D medical imaging quality. However, existing approaches confront inherent challenges: 1) often tailored to specific upsampling factors, lacking flexibility for diverse clinical scenarios; 2) newly generated slices frequently suffer from over-smoothing, degrading fine details, and leading to inter-slice inconsistency. In response, this study presents CycleINR, a novel enhanced Implicit Neural Representation model for 3D medical data volumetric super-resolution. Leveraging the continuity of the learned implicit function, the CycleINR model can achieve results with arbitrary up-sampling rates, eliminating the need for separate training. Additionally, we enhance the grid sampling in CycleINR with a local attention mechanism and mitigate over-smoothing by integrating cycle-consistent loss. We introduce a new metric, Slice-wise Noise Level Inconsistency (SNLI), to quantitatively assess inter-slice noise level inconsistency. The effectiveness of our approach is demonstrated through image quality evaluations on an in-house dataset and a downstream task analysis on the Medical Segmentation Decathlon liver tumor dataset.","sentences":["In the realm of medical 3D data, such as CT and MRI images, prevalent anisotropic resolution is characterized by high intra-slice but diminished inter-slice resolution.","The lowered resolution between adjacent slices poses challenges, hindering optimal viewing experiences and impeding the development of robust downstream analysis algorithms.","Various volumetric super-resolution algorithms aim to surmount these challenges, enhancing inter-slice resolution and overall 3D medical imaging quality.","However, existing approaches confront inherent challenges: 1) often tailored to specific upsampling factors, lacking flexibility for diverse clinical scenarios; 2) newly generated slices frequently suffer from over-smoothing, degrading fine details, and leading to inter-slice inconsistency.","In response, this study presents CycleINR, a novel enhanced Implicit Neural Representation model for 3D medical data volumetric super-resolution.","Leveraging the continuity of the learned implicit function, the CycleINR model can achieve results with arbitrary up-sampling rates, eliminating the need for separate training.","Additionally, we enhance the grid sampling in CycleINR with a local attention mechanism and mitigate over-smoothing by integrating cycle-consistent loss.","We introduce a new metric, Slice-wise Noise Level Inconsistency (SNLI), to quantitatively assess inter-slice noise level inconsistency.","The effectiveness of our approach is demonstrated through image quality evaluations on an in-house dataset and a downstream task analysis on the Medical Segmentation Decathlon liver tumor dataset."],"url":"http://arxiv.org/abs/2404.04878v1","category":"eess.IV"}
{"created":"2024-04-07 08:42:38","title":"NeRF2Points: Large-Scale Point Cloud Generation From Street Views' Radiance Field Optimization","abstract":"Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology for the photorealistic rendering of objects and environments, enabling the synthesis of novel viewpoints with remarkable fidelity. This is accomplished through the strategic utilization of object-centric camera poses characterized by significant inter-frame overlap. This paper explores a compelling, alternative utility of NeRF: the derivation of point clouds from aggregated urban landscape imagery. The transmutation of street-view data into point clouds is fraught with complexities, attributable to a nexus of interdependent variables. First, high-quality point cloud generation hinges on precise camera poses, yet many datasets suffer from inaccuracies in pose metadata. Also, the standard approach of NeRF is ill-suited for the distinct characteristics of street-view data from autonomous vehicles in vast, open settings. Autonomous vehicle cameras often record with limited overlap, leading to blurring, artifacts, and compromised pavement representation in NeRF-based point clouds. In this paper, we present NeRF2Points, a tailored NeRF variant for urban point cloud synthesis, notable for its high-quality output from RGB inputs alone. Our paper is supported by a bespoke, high-resolution 20-kilometer urban street dataset, designed for point cloud generation and evaluation. NeRF2Points adeptly navigates the inherent challenges of NeRF-based point cloud synthesis through the implementation of the following strategic innovations: (1) Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data precision. (2) Layered Perception and Integrated Modeling (LPiM) is designed for distinct radiance field modeling in urban environments, resulting in coherent point cloud representations.","sentences":["Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology for the photorealistic rendering of objects and environments, enabling the synthesis of novel viewpoints with remarkable fidelity.","This is accomplished through the strategic utilization of object-centric camera poses characterized by significant inter-frame overlap.","This paper explores a compelling, alternative utility of NeRF: the derivation of point clouds from aggregated urban landscape imagery.","The transmutation of street-view data into point clouds is fraught with complexities, attributable to a nexus of interdependent variables.","First, high-quality point cloud generation hinges on precise camera poses, yet many datasets suffer from inaccuracies in pose metadata.","Also, the standard approach of NeRF is ill-suited for the distinct characteristics of street-view data from autonomous vehicles in vast, open settings.","Autonomous vehicle cameras often record with limited overlap, leading to blurring, artifacts, and compromised pavement representation in NeRF-based point clouds.","In this paper, we present NeRF2Points, a tailored NeRF variant for urban point cloud synthesis, notable for its high-quality output from RGB inputs alone.","Our paper is supported by a bespoke, high-resolution 20-kilometer urban street dataset, designed for point cloud generation and evaluation.","NeRF2Points adeptly navigates the inherent challenges of NeRF-based point cloud synthesis through the implementation of the following strategic innovations: (1) Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data precision.","(2) Layered Perception and Integrated Modeling (LPiM) is designed for distinct radiance field modeling in urban environments, resulting in coherent point cloud representations."],"url":"http://arxiv.org/abs/2404.04875v1","category":"cs.CV"}
{"created":"2024-04-07 08:28:10","title":"A Novel Class of Phase Space Representations for the Exact Population Dynamics of Two-State Quantum Systems and the Relation to Triangle Window Functions","abstract":"Isomorphism of the two-state system is heuristic in understanding the dynamical or statistical behavior of the simplest yet most quantum system that has no classical counterpart. We use constraint phase space [developed in J. Chem. Phys. 2016, 145, 204105; 2019, 151, 024105 and J. Phys. Chem. Lett. 2021, 12, 2496-2501], non-covariant phase space functions, time-dependent weight functions, and time-dependent normalization factors to construct a novel class of phase space representations of the exact population dynamics of the two-state quantum system. The equations of motion of the trajectory on constraint phase space are isomorphic to the time-dependent Schr\\\"odinger equation. The contribution of each trajectory to the integral expression for the population dynamics is always positive semi-definite. We also prove that the triangle window function approach, albeit empirically proposed in J. Chem. Phys. 2016, 145, 144108, is related to a special case of the novel class and leads to an isomorphic representation of the exact population dynamics of the two-state quantum system.","sentences":["Isomorphism of the two-state system is heuristic in understanding the dynamical or statistical behavior of the simplest yet most quantum system that has no classical counterpart.","We use constraint phase space [developed in J. Chem.","Phys. 2016, 145, 204105; 2019, 151, 024105 and J. Phys.","Chem.","Lett.","2021, 12, 2496-2501], non-covariant phase space functions, time-dependent weight functions, and time-dependent normalization factors to construct a novel class of phase space representations of the exact population dynamics of the two-state quantum system.","The equations of motion of the trajectory on constraint phase space are isomorphic to the time-dependent Schr\\\"odinger equation.","The contribution of each trajectory to the integral expression for the population dynamics is always positive semi-definite.","We also prove that the triangle window function approach, albeit empirically proposed in J. Chem.","Phys. 2016, 145, 144108, is related to a special case of the novel class and leads to an isomorphic representation of the exact population dynamics of the two-state quantum system."],"url":"http://arxiv.org/abs/2404.04868v1","category":"quant-ph"}
{"created":"2024-04-07 08:19:47","title":"Nonadiabatic Field on Quantum Phase Space: A Century after Ehrenfest","abstract":"Nonadiabatic transition dynamics lies at the core of many electron/hole transfer, photoactivated, and vacuum field-coupled processes. About a century after Ehrenfest proposed \"Phasenraum\" and the Ehrenfest theorem, we report a conceptually novel trajectory-based nonadiabatic dynamics approach, nonadiabatic field (NaF), based on a generalized exact coordinate-momentum phase space formulation of quantum mechanics. It does not employ the conventional Born-Oppenheimer or Ehrenfest trajectory in the nonadiabatic coupling region. Instead, in NaF the equations of motion of the independent trajectory involve a nonadiabatic nuclear force term in addition to an adiabatic nuclear force term of a single electronic state. A few benchmark tests for gas phase and condensed phase systems indicate that NaF offers a practical tool to capture the correct correlation of electronic and nuclear dynamics for processes where the states remain coupled all the time as well as for the asymptotic region where the coupling of electronic states vanishes.","sentences":["Nonadiabatic transition dynamics lies at the core of many electron/hole transfer, photoactivated, and vacuum field-coupled processes.","About a century after Ehrenfest proposed \"Phasenraum\" and the Ehrenfest theorem, we report a conceptually novel trajectory-based nonadiabatic dynamics approach, nonadiabatic field (NaF), based on a generalized exact coordinate-momentum phase space formulation of quantum mechanics.","It does not employ the conventional Born-Oppenheimer or Ehrenfest trajectory in the nonadiabatic coupling region.","Instead, in NaF the equations of motion of the independent trajectory involve a nonadiabatic nuclear force term in addition to an adiabatic nuclear force term of a single electronic state.","A few benchmark tests for gas phase and condensed phase systems indicate that NaF offers a practical tool to capture the correct correlation of electronic and nuclear dynamics for processes where the states remain coupled all the time as well as for the asymptotic region where the coupling of electronic states vanishes."],"url":"http://arxiv.org/abs/2404.04866v1","category":"quant-ph"}
{"created":"2024-04-07 08:07:02","title":"Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint","abstract":"In this paper, we advance the understanding of neural network training dynamics by examining the intricate interplay of various factors introduced by weight parameters in the initialization process. Motivated by the foundational work of Luo et al. (J. Mach. Learn. Res., Vol. 22, Iss. 1, No. 71, pp 3327-3373), we explore the gradient descent dynamics of neural networks through the lens of macroscopic limits, where we analyze its behavior as width $m$ tends to infinity. Our study presents a unified approach with refined techniques designed for multi-layer fully connected neural networks, which can be readily extended to other neural network architectures. Our investigation reveals that gradient descent can rapidly drive deep neural networks to zero training loss, irrespective of the specific initialization schemes employed by weight parameters, provided that the initial scale of the output function $\\kappa$ surpasses a certain threshold. This regime, characterized as the theta-lazy area, accentuates the predominant influence of the initial scale $\\kappa$ over other factors on the training behavior of neural networks. Furthermore, our approach draws inspiration from the Neural Tangent Kernel (NTK) paradigm, and we expand its applicability. While NTK typically assumes that $\\lim_{m\\to\\infty}\\frac{\\log \\kappa}{\\log m}=\\frac{1}{2}$, and imposes each weight parameters to scale by the factor $\\frac{1}{\\sqrt{m}}$, in our theta-lazy regime, we discard the factor and relax the conditions to $\\lim_{m\\to\\infty}\\frac{\\log \\kappa}{\\log m}>0$. Similar to NTK, the behavior of overparameterized neural networks within the theta-lazy regime trained by gradient descent can be effectively described by a specific kernel. Through rigorous analysis, our investigation illuminates the pivotal role of $\\kappa$ in governing the training dynamics of neural networks.","sentences":["In this paper, we advance the understanding of neural network training dynamics by examining the intricate interplay of various factors introduced by weight parameters in the initialization process.","Motivated by the foundational work of Luo et al.","(J. Mach.","Learn.","Res., Vol. 22, Iss. 1, No. 71, pp 3327-3373), we explore the gradient descent dynamics of neural networks through the lens of macroscopic limits, where we analyze its behavior as width $m$ tends to infinity.","Our study presents a unified approach with refined techniques designed for multi-layer fully connected neural networks, which can be readily extended to other neural network architectures.","Our investigation reveals that gradient descent can rapidly drive deep neural networks to zero training loss, irrespective of the specific initialization schemes employed by weight parameters, provided that the initial scale of the output function $\\kappa$ surpasses a certain threshold.","This regime, characterized as the theta-lazy area, accentuates the predominant influence of the initial scale $\\kappa$ over other factors on the training behavior of neural networks.","Furthermore, our approach draws inspiration from the Neural Tangent Kernel (NTK) paradigm, and we expand its applicability.","While NTK typically assumes that $\\lim_{m\\to\\infty}\\frac{\\log \\kappa}{\\log m}=\\frac{1}{2}$, and imposes each weight parameters to scale by the factor $\\frac{1}{\\sqrt{m}}$, in our theta-lazy regime, we discard the factor and relax the conditions to $\\lim_{m\\to\\infty}\\frac{\\log \\kappa}{\\log m}>0$. Similar to NTK, the behavior of overparameterized neural networks within the theta-lazy regime trained by gradient descent can be effectively described by a specific kernel.","Through rigorous analysis, our investigation illuminates the pivotal role of $\\kappa$ in governing the training dynamics of neural networks."],"url":"http://arxiv.org/abs/2404.04859v1","category":"cs.LG"}
{"created":"2024-04-07 07:39:45","title":"F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation","abstract":"In the evolving landscape of Neural Machine Translation (NMT), the pretrain-then-finetune paradigm has yielded impressive results. However, the persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While previous work has introduced Continual Learning (CL) methods to address CF, these approaches grapple with the delicate balance between avoiding forgetting and maintaining system extensibility. To address this, we propose a CL method, named $\\textbf{F-MALLOC}$ ($\\textbf{F}$eed-forward $\\textbf{M}$emory $\\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting that feed-forward layers emulate neural memories and encapsulate crucial translation knowledge. It decomposes feed-forward layers into discrete memory cells and allocates these memories to different tasks. By learning to allocate and safeguard these memories, our method effectively alleviates CF while ensuring robust extendability. Besides, we propose a comprehensive assessment protocol for multi-stage CL of NMT systems. Experiments conducted following this new protocol showcase the superior performance of F-MALLOC, evidenced by higher BLEU scores and almost zero forgetting.","sentences":["In the evolving landscape of Neural Machine Translation (NMT), the pretrain-then-finetune paradigm has yielded impressive results.","However, the persistent challenge of Catastrophic Forgetting (CF) remains a hurdle.","While previous work has introduced Continual Learning (CL) methods to address CF, these approaches grapple with the delicate balance between avoiding forgetting and maintaining system extensibility.","To address this, we propose a CL method, named $\\textbf{F-MALLOC}$ ($\\textbf{F}$eed-forward $\\textbf{M}$emory $\\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting that feed-forward layers emulate neural memories and encapsulate crucial translation knowledge.","It decomposes feed-forward layers into discrete memory cells and allocates these memories to different tasks.","By learning to allocate and safeguard these memories, our method effectively alleviates CF while ensuring robust extendability.","Besides, we propose a comprehensive assessment protocol for multi-stage CL of NMT systems.","Experiments conducted following this new protocol showcase the superior performance of F-MALLOC, evidenced by higher BLEU scores and almost zero forgetting."],"url":"http://arxiv.org/abs/2404.04846v1","category":"cs.CL"}
{"created":"2024-04-07 05:44:19","title":"A Covariant and Manifestly Projective Invariant Formulation of Thomas-Whitehead Gravity","abstract":"Thomas-Whitehead (TW) gravity is a recently formulated projectively invariant extension of Einstein-Hilbert gravity. Projective geometry was used long ago by Thomas et. al. to succinctly package equivalent paths encoded by the geodesic equation. Projective invariance in gravity has further origins in string theory through a geometric action constructed from the method of coadjoint orbits using the Virasoro Algebra. A projectively invariant connection arises from this construction, a part of which is known as the diffeomorphism field. TW Gravity exploits projective Gauss-Bonnet terms in the action functional to endow the diffeomorphism field with dynamics, while allowing the theory to collapse to General Relativity in the limit that the diffeomorphism field vanishes and the connection becomes Levi-Civita. In the original formulation of TW gravity, the diffeomorphism field is projectively invariant but not tensorial and the connection is projectively invariant but not affine. In this paper we reformulate TW gravity in terms of projectively invariant tensor fields and a projectively invariant covariant derivative, derive field equations respecting these symmetries, and show that the field equations obtained are classically equivalent across formulations. This provides a 'Rosetta Stone' between this newly constructed covariant and projective invariant formulation of TW gravity and the original formulation that was manifestly projectively invariant, but not covariant.","sentences":["Thomas-Whitehead (TW) gravity is a recently formulated projectively invariant extension of Einstein-Hilbert gravity.","Projective geometry was used long ago by Thomas et.","al. to succinctly package equivalent paths encoded by the geodesic equation.","Projective invariance in gravity has further origins in string theory through a geometric action constructed from the method of coadjoint orbits using the Virasoro Algebra.","A projectively invariant connection arises from this construction, a part of which is known as the diffeomorphism field.","TW Gravity exploits projective Gauss-Bonnet terms in the action functional to endow the diffeomorphism field with dynamics, while allowing the theory to collapse to General Relativity in the limit that the diffeomorphism field vanishes and the connection becomes Levi-Civita.","In the original formulation of TW gravity, the diffeomorphism field is projectively invariant but not tensorial and the connection is projectively invariant but not affine.","In this paper we reformulate TW gravity in terms of projectively invariant tensor fields and a projectively invariant covariant derivative, derive field equations respecting these symmetries, and show that the field equations obtained are classically equivalent across formulations.","This provides a 'Rosetta Stone' between this newly constructed covariant and projective invariant formulation of TW gravity and the original formulation that was manifestly projectively invariant, but not covariant."],"url":"http://arxiv.org/abs/2404.04813v1","category":"hep-th"}
{"created":"2024-04-07 04:09:09","title":"Spin-lattice relaxation with non-linear couplings: Comparison between Fermi's golden rule and extended dissipaton equation of motion","abstract":"Fermi's golden rule (FGR) offers an empirical framework for understanding the dynamics of spin-lattice relaxation in magnetic molecules, encompassing mechanisms like direct (one-phonon) and Raman (two-phonon) processes. These principles effectively model experimental longitudinal relaxation rates, denoted as $T_1^{-1}$. However, under scenarios of increased coupling strength and nonlinear spin-lattice interactions, FGR's applicability may diminish. This paper numerically evaluates the exact spin-lattice relaxation rate kernels, employing the extended dissipaton equation of motion (DEOM) formalism. Our calculations reveal that when quadratic spin-lattice coupling is considered, the rate kernels exhibit a free induction decay-like feature, and the damping rates depend on the interaction strength. We observe that the temperature dependence predicted by FGR significantly deviates from the exact results since FGR ignores the non-Markovian nature of spin-lattice relaxation. Our methods can be readily applied to other systems with nonlinear spin-lattice interactions and provide valuable insights into the temperature dependence of $T_1$ in molecular qubits.","sentences":["Fermi's golden rule (FGR) offers an empirical framework for understanding the dynamics of spin-lattice relaxation in magnetic molecules, encompassing mechanisms like direct (one-phonon) and Raman (two-phonon) processes.","These principles effectively model experimental longitudinal relaxation rates, denoted as $T_1^{-1}$. However, under scenarios of increased coupling strength and nonlinear spin-lattice interactions, FGR's applicability may diminish.","This paper numerically evaluates the exact spin-lattice relaxation rate kernels, employing the extended dissipaton equation of motion (DEOM) formalism.","Our calculations reveal that when quadratic spin-lattice coupling is considered, the rate kernels exhibit a free induction decay-like feature, and the damping rates depend on the interaction strength.","We observe that the temperature dependence predicted by FGR significantly deviates from the exact results since FGR ignores the non-Markovian nature of spin-lattice relaxation.","Our methods can be readily applied to other systems with nonlinear spin-lattice interactions and provide valuable insights into the temperature dependence of $T_1$ in molecular qubits."],"url":"http://arxiv.org/abs/2404.04803v1","category":"physics.chem-ph"}
{"created":"2024-04-07 03:11:15","title":"A Deep Learning Approach to Nonparametric Propensity Score Estimation with Optimized Covariate Balance","abstract":"This paper proposes a novel propensity score weighting analysis. We define two sufficient and necessary conditions for a function of the covariates to be the propensity score. The first is \"local balance\", which ensures the conditional independence of covariates and treatment assignment across a dense grid of propensity score values. The second condition, \"local calibration\", guarantees that a balancing score is a propensity score. Using three-layer feed-forward neural networks, we develop a nonparametric propensity score model that satisfies these conditions, effectively circumventing the issue of model misspecification and optimizing covariate balance to minimize bias and stabilize the inverse probability weights. Our proposed method performed substantially better than existing methods in extensive numerical studies of both real and simulated benchmark datasets.","sentences":["This paper proposes a novel propensity score weighting analysis.","We define two sufficient and necessary conditions for a function of the covariates to be the propensity score.","The first is \"local balance\", which ensures the conditional independence of covariates and treatment assignment across a dense grid of propensity score values.","The second condition, \"local calibration\", guarantees that a balancing score is a propensity score.","Using three-layer feed-forward neural networks, we develop a nonparametric propensity score model that satisfies these conditions, effectively circumventing the issue of model misspecification and optimizing covariate balance to minimize bias and stabilize the inverse probability weights.","Our proposed method performed substantially better than existing methods in extensive numerical studies of both real and simulated benchmark datasets."],"url":"http://arxiv.org/abs/2404.04794v1","category":"stat.ME"}
{"created":"2024-04-07 02:44:33","title":"GDR-HGNN: A Heterogeneous Graph Neural Networks Accelerator Frontend with Graph Decoupling and Recoupling","abstract":"Heterogeneous Graph Neural Networks (HGNNs) have broadened the applicability of graph representation learning to heterogeneous graphs. However, the irregular memory access pattern of HGNNs leads to the buffer thrashing issue in HGNN accelerators. In this work, we identify an opportunity to address buffer thrashing in HGNN acceleration through an analysis of the topology of heterogeneous graphs. To harvest this opportunity, we propose a graph restructuring method and map it into a hardware frontend named GDR-HGNN. GDR-HGNN dynamically restructures the graph on the fly to enhance data locality for HGNN accelerators. Experimental results demonstrate that, with the assistance of GDR-HGNN, a leading HGNN accelerator achieves an average speedup of 14.6 times and 1.78 times compared to the state-of-the-art software framework running on A100 GPU and itself, respectively.","sentences":["Heterogeneous Graph Neural Networks (HGNNs) have broadened the applicability of graph representation learning to heterogeneous graphs.","However, the irregular memory access pattern of HGNNs leads to the buffer thrashing issue in HGNN accelerators.","In this work, we identify an opportunity to address buffer thrashing in HGNN acceleration through an analysis of the topology of heterogeneous graphs.","To harvest this opportunity, we propose a graph restructuring method and map it into a hardware frontend named GDR-HGNN.","GDR-HGNN dynamically restructures the graph on the fly to enhance data locality for HGNN accelerators.","Experimental results demonstrate that, with the assistance of GDR-HGNN, a leading HGNN accelerator achieves an average speedup of 14.6 times and 1.78 times compared to the state-of-the-art software framework running on A100 GPU and itself, respectively."],"url":"http://arxiv.org/abs/2404.04792v1","category":"cs.AR"}
{"created":"2024-04-07 02:21:25","title":"Reconciling the HESS J1731-347 constraints with Parity doublet model","abstract":"The recent discovery of a central compact object (CCO) within the supernova remnant HESS J1731-347, characterized by a mass of approximately $0.77^{+0.20}_{-0.17} M_{\\odot}$ and a radius of about $10.4^{+0.86}_{-0.78}$ km, has opened up a new window for the study of compact objects. This CCO is particularly intriguing because it is the lightest and smallest compact object ever observed, raising questions and challenging the existing theories. To account for this light compact star, a mean-field model within the framework of parity doublet structure is applied to describe the hadron matter. Inside the model, part of the nucleon mass is associated with the chiral symmetry breaking while the other part is from the chiral invariant mass $m_0$ which is insensitive to the temperature/density. The value of $m_0$ affects the nuclear equation of state for uniform nuclear matter at low density and exhibits strong correlations with the radii of neutron stars.   We point out that HESS J1731-347 can be explained as the lightest neutron star for $m_0 \\simeq 850$\\,MeV.","sentences":["The recent discovery of a central compact object (CCO) within the supernova remnant HESS J1731-347, characterized by a mass of approximately $0.77^{+0.20}_{-0.17} M_{\\odot}$ and a radius of about $10.4^{+0.86}_{-0.78}$ km, has opened up a new window for the study of compact objects.","This CCO is particularly intriguing because it is the lightest and smallest compact object ever observed, raising questions and challenging the existing theories.","To account for this light compact star, a mean-field model within the framework of parity doublet structure is applied to describe the hadron matter.","Inside the model, part of the nucleon mass is associated with the chiral symmetry breaking while the other part is from the chiral invariant mass $m_0$ which is insensitive to the temperature/density.","The value of $m_0$ affects the nuclear equation of state for uniform nuclear matter at low density and exhibits strong correlations with the radii of neutron stars.   ","We point out that HESS J1731-347 can be explained as the lightest neutron star for $m_0 \\simeq 850$\\,MeV."],"url":"http://arxiv.org/abs/2404.04786v1","category":"nucl-th"}
{"created":"2024-04-07 01:54:16","title":"The convergence of the EM scheme in empirical approximation of invariant probability measure for McKean-Vlasov SDEs","abstract":"Based on the assumption of the existence and uniqueness of the invariant measure for McKean-Vlasov stochastic differential equations (MV-SDEs), a self-interacting process that depends only on the current and historical information of the solution is constructed for MV-SDEs. The convergence rate of the weighted empirical measure of the self-interacting process and the invariant measure of MV-SDEs is obtained in the W2-Wasserstein metric. Furthermore, under the condition of linear growth, an EM scheme whose uniformly 1/2-order convergence rate with respect to time is obtained is constructed for the self-interacting process. Then, the convergence rate between the weighted empirical measure of the EM numerical solution of the self-interacting process and the invariant measure of MV-SDEs is derived. Moreover, the convergence rate between the averaged weighted empirical measure of the EM numerical solution of the corresponding multi-particle system and the invariant measure of MV-SDEs in the W2-Wasserstein metric is also given. In addition, the computational cost of the two approximation methods is compared, which shows that the averaged weighted empirical approximation of the particle system has a lower cost. Finally, the theoretical results are validated through numerical experiments.","sentences":["Based on the assumption of the existence and uniqueness of the invariant measure for McKean-Vlasov stochastic differential equations (MV-SDEs), a self-interacting process that depends only on the current and historical information of the solution is constructed for MV-SDEs.","The convergence rate of the weighted empirical measure of the self-interacting process and the invariant measure of MV-SDEs is obtained in the W2-Wasserstein metric.","Furthermore, under the condition of linear growth, an EM scheme whose uniformly 1/2-order convergence rate with respect to time is obtained is constructed for the self-interacting process.","Then, the convergence rate between the weighted empirical measure of the EM numerical solution of the self-interacting process and the invariant measure of MV-SDEs is derived.","Moreover, the convergence rate between the averaged weighted empirical measure of the EM numerical solution of the corresponding multi-particle system and the invariant measure of MV-SDEs in the W2-Wasserstein metric is also given.","In addition, the computational cost of the two approximation methods is compared, which shows that the averaged weighted empirical approximation of the particle system has a lower cost.","Finally, the theoretical results are validated through numerical experiments."],"url":"http://arxiv.org/abs/2404.04781v1","category":"math.PR"}
{"created":"2024-04-07 00:42:22","title":"The intersection cohomology Hodge module of toric varieties","abstract":"We study the Hodge filtration of the intersection cohomology Hodge module for toric varieties. More precisely, we study the cohomology sheaves of the graded de Rham complex of the intersection cohomology Hodge module and give a precise formula relating it with the stalks of the intersection cohomology as a constructible complex. The main idea is to use the Ishida complex in order to compute the higher direct images of the sheaf of reflexive differentials.","sentences":["We study the Hodge filtration of the intersection cohomology Hodge module for toric varieties.","More precisely, we study the cohomology sheaves of the graded de Rham complex of the intersection cohomology Hodge module and give a precise formula relating it with the stalks of the intersection cohomology as a constructible complex.","The main idea is to use the Ishida complex in order to compute the higher direct images of the sheaf of reflexive differentials."],"url":"http://arxiv.org/abs/2404.04767v1","category":"math.AG"}
{"created":"2024-04-06 23:37:32","title":"On Schr\u00f6dinger equation with square and inverse-square potentials","abstract":"In this paper, we study the linear and nonlinear Schr\\\"odinger equations with a time-decaying harmonic oscillator and inverse-square potential. This model retains a form of scale invariance, and utilizing this property, we demonstrate the asymptotic completeness of wave operators and Strichartz estimates for linear propagators.","sentences":["In this paper, we study the linear and nonlinear Schr\\\"odinger equations with a time-decaying harmonic oscillator and inverse-square potential.","This model retains a form of scale invariance, and utilizing this property, we demonstrate the asymptotic completeness of wave operators and Strichartz estimates for linear propagators."],"url":"http://arxiv.org/abs/2404.04756v1","category":"math.AP"}
{"created":"2024-04-06 21:47:00","title":"A free boundary problem for an immersed filament in 3D Stokes flow","abstract":"We consider a simplified extensible version of a dynamic free boundary problem for a thin filament with radius $\\epsilon>0$ immersed in 3D Stokes flow. The 3D fluid is coupled to the quasi-1D filament dynamics via a novel type of angle-averaged Neumann-to-Dirichlet operator for the Stokes equations, and much of the difficulty in the analysis lies in understanding this operator. Here we show that the main part of this angle-averaged NtD map about a closed, curved filament is the corresponding operator about a straight filament, for which we can derive an explicit symbol. Remainder terms due to curvature are lower order with respect to regularity or size in $\\epsilon$. Using this operator decomposition, it is then possible to show that the simplified free boundary evolution is a third-order parabolic equation and is locally well posed. This establishes a more complete mathematical foundation for the myriad computational results based on slender body approximations for thin immersed elastic structures.","sentences":["We consider a simplified extensible version of a dynamic free boundary problem for a thin filament with radius $\\epsilon>0$ immersed in 3D Stokes flow.","The 3D fluid is coupled to the quasi-1D filament dynamics via a novel type of angle-averaged Neumann-to-Dirichlet operator for the Stokes equations, and much of the difficulty in the analysis lies in understanding this operator.","Here we show that the main part of this angle-averaged NtD map about a closed, curved filament is the corresponding operator about a straight filament, for which we can derive an explicit symbol.","Remainder terms due to curvature are lower order with respect to regularity or size in $\\epsilon$. Using this operator decomposition, it is then possible to show that the simplified free boundary evolution is a third-order parabolic equation and is locally well posed.","This establishes a more complete mathematical foundation for the myriad computational results based on slender body approximations for thin immersed elastic structures."],"url":"http://arxiv.org/abs/2404.04737v1","category":"math.AP"}
{"created":"2024-04-06 21:33:39","title":"Towards Generalized Entropic Sparsification for Convolutional Neural Networks","abstract":"Convolutional neural networks (CNNs) are reported to be overparametrized. The search for optimal (minimal) and sufficient architecture is an NP-hard problem as the hyperparameter space for possible network configurations is vast. Here, we introduce a layer-by-layer data-driven pruning method based on the mathematical idea aiming at a computationally-scalable entropic relaxation of the pruning problem. The sparse subnetwork is found from the pre-trained (full) CNN using the network entropy minimization as a sparsity constraint. This allows deploying a numerically scalable algorithm with a sublinear scaling cost. The method is validated on several benchmarks (architectures): (i) MNIST (LeNet) with sparsity 55%-84% and loss in accuracy 0.1%-0.5%, and (ii) CIFAR-10 (VGG-16, ResNet18) with sparsity 73-89% and loss in accuracy 0.1%-0.5%.","sentences":["Convolutional neural networks (CNNs) are reported to be overparametrized.","The search for optimal (minimal) and sufficient architecture is an NP-hard problem as the hyperparameter space for possible network configurations is vast.","Here, we introduce a layer-by-layer data-driven pruning method based on the mathematical idea aiming at a computationally-scalable entropic relaxation of the pruning problem.","The sparse subnetwork is found from the pre-trained (full) CNN using the network entropy minimization as a sparsity constraint.","This allows deploying a numerically scalable algorithm with a sublinear scaling cost.","The method is validated on several benchmarks (architectures): (i) MNIST (LeNet) with sparsity 55%-84% and loss in accuracy 0.1%-0.5%, and (ii) CIFAR-10 (VGG-16, ResNet18) with sparsity 73-89% and loss in accuracy 0.1%-0.5%."],"url":"http://arxiv.org/abs/2404.04734v1","category":"cs.CV"}
{"created":"2024-04-06 19:50:48","title":"On Exploring PDE Modeling for Point Cloud Video Representation Learning","abstract":"Point cloud video representation learning is challenging due to complex structures and unordered spatial arrangement. Traditional methods struggle with frame-to-frame correlations and point-wise correspondence tracking. Recently, partial differential equations (PDE) have provided a new perspective in uniformly solving spatial-temporal data information within certain constraints. While tracking tangible point correspondence remains challenging, we propose to formalize point cloud video representation learning as a PDE-solving problem. Inspired by fluid analysis, where PDEs are used to solve the deformation of spatial shape over time, we employ PDE to solve the variations of spatial points affected by temporal information. By modeling spatial-temporal correlations, we aim to regularize spatial variations with temporal features, thereby enhancing representation learning in point cloud videos. We introduce Motion PointNet composed of a PointNet-like encoder and a PDE-solving module. Initially, we construct a lightweight yet effective encoder to model an initial state of the spatial variations. Subsequently, we develop our PDE-solving module in a parameterized latent space, tailored to address the spatio-temporal correlations inherent in point cloud video. The process of solving PDE is guided and refined by a contrastive learning structure, which is pivotal in reshaping the feature distribution, thereby optimizing the feature representation within point cloud video data. Remarkably, our Motion PointNet achieves an impressive accuracy of 97.52% on the MSRAction-3D dataset, surpassing the current state-of-the-art in all aspects while consuming minimal resources (only 0.72M parameters and 0.82G FLOPs).","sentences":["Point cloud video representation learning is challenging due to complex structures and unordered spatial arrangement.","Traditional methods struggle with frame-to-frame correlations and point-wise correspondence tracking.","Recently, partial differential equations (PDE) have provided a new perspective in uniformly solving spatial-temporal data information within certain constraints.","While tracking tangible point correspondence remains challenging, we propose to formalize point cloud video representation learning as a PDE-solving problem.","Inspired by fluid analysis, where PDEs are used to solve the deformation of spatial shape over time, we employ PDE to solve the variations of spatial points affected by temporal information.","By modeling spatial-temporal correlations, we aim to regularize spatial variations with temporal features, thereby enhancing representation learning in point cloud videos.","We introduce Motion PointNet composed of a PointNet-like encoder and a PDE-solving module.","Initially, we construct a lightweight yet effective encoder to model an initial state of the spatial variations.","Subsequently, we develop our PDE-solving module in a parameterized latent space, tailored to address the spatio-temporal correlations inherent in point cloud video.","The process of solving PDE is guided and refined by a contrastive learning structure, which is pivotal in reshaping the feature distribution, thereby optimizing the feature representation within point cloud video data.","Remarkably, our Motion PointNet achieves an impressive accuracy of 97.52% on the MSRAction-3D dataset, surpassing the current state-of-the-art in all aspects while consuming minimal resources (only 0.72M parameters and 0.82G FLOPs)."],"url":"http://arxiv.org/abs/2404.04720v1","category":"cs.CV"}
{"created":"2024-04-06 19:22:11","title":"Helicity-dependent parton distribution functions at next-to-next-to-leading order accuracy from inclusive and semi-inclusive deep-inelastic scattering data","abstract":"We present MAPPDFpol1.0, a new determination of the helicity-dependent parton distribution functions (PDFs) of the proton from a set of longitudinally polarised inclusive and semi-inclusive deep-inelastic scattering data. The determination includes, for the first time, next-to-next-to-leading order QCD corrections to both processes, and is carried out in a framework that combines a neural-network parametrisation of PDFs with a Monte Carlo representation of their uncertainties. We discuss the quality of the determination, in particular its dependence on higher-order corrections, on the choice of data set, and on theoretical constraints.","sentences":["We present MAPPDFpol1.0, a new determination of the helicity-dependent parton distribution functions (PDFs) of the proton from a set of longitudinally polarised inclusive and semi-inclusive deep-inelastic scattering data.","The determination includes, for the first time, next-to-next-to-leading order QCD corrections to both processes, and is carried out in a framework that combines a neural-network parametrisation of PDFs with a Monte Carlo representation of their uncertainties.","We discuss the quality of the determination, in particular its dependence on higher-order corrections, on the choice of data set, and on theoretical constraints."],"url":"http://arxiv.org/abs/2404.04712v1","category":"hep-ph"}
{"created":"2024-04-06 19:08:28","title":"On the Uniqueness and Orbital Stability of Slow and Fast Solitary Wave Solutions of the Benjamin Equation","abstract":"This paper is devoted to the study of existence and properties of solitary waves of the Benjamin equation. The studied equation includes a parameter $ \\gamma $ in front of the Benjamin-Ono term. % whose change in sign changes in turn certain properties of the energy. We show the existence, uniqueness, decay and orbital stability of solitary wave solutions obtained as a solution to a certain minimization problem, associated either with high speeds without sign condition on the parameter $\\gamma$ or with small speeds for the appropriate sign. For any $ \\gamma\\in \\mathbb{R}^* $, we prove the uniqueness and orbital stability of fast even ground state solitary waves. We also prove the uniqueness and orbital stability of slow ground state solitary waves for $ \\gamma>0 $.","sentences":["This paper is devoted to the study of existence and properties of solitary waves of the Benjamin equation.","The studied equation includes a parameter $ \\gamma $ in front of the Benjamin-Ono term.","% whose change in sign changes in turn certain properties of the energy.","We show the existence, uniqueness, decay and orbital stability of solitary wave solutions obtained as a solution to a certain minimization problem, associated either with high speeds without sign condition on the parameter $\\gamma$ or with small speeds for the appropriate sign.","For any $ \\gamma\\in \\mathbb{R}^* $, we prove the uniqueness and orbital stability of fast even ground state solitary waves.","We also prove the uniqueness and orbital stability of slow ground state solitary waves for $ \\gamma>0 $."],"url":"http://arxiv.org/abs/2404.04711v1","category":"math.AP"}
{"created":"2024-04-06 18:49:24","title":"Advances in Differential Privacy and Differentially Private Machine Learning","abstract":"There has been an explosion of research on differential privacy (DP) and its various applications in recent years, ranging from novel variants and accounting techniques in differential privacy to the thriving field of differentially private machine learning (DPML) to newer implementations in practice, like those by various companies and organisations such as census bureaus. Most recent surveys focus on the applications of differential privacy in particular contexts like data publishing, specific machine learning tasks, analysis of unstructured data, location privacy, etc. This work thus seeks to fill the gap for a survey that primarily discusses recent developments in the theory of differential privacy along with newer DP variants, viz. Renyi DP and Concentrated DP, novel mechanisms and techniques, and the theoretical developments in differentially private machine learning in proper detail. In addition, this survey discusses its applications to privacy-preserving machine learning in practice and a few practical implementations of DP.","sentences":["There has been an explosion of research on differential privacy (DP) and its various applications in recent years, ranging from novel variants and accounting techniques in differential privacy to the thriving field of differentially private machine learning (DPML) to newer implementations in practice, like those by various companies and organisations such as census bureaus.","Most recent surveys focus on the applications of differential privacy in particular contexts like data publishing, specific machine learning tasks, analysis of unstructured data, location privacy, etc.","This work thus seeks to fill the gap for a survey that primarily discusses recent developments in the theory of differential privacy along with newer DP variants, viz.","Renyi DP and Concentrated DP, novel mechanisms and techniques, and the theoretical developments in differentially private machine learning in proper detail.","In addition, this survey discusses its applications to privacy-preserving machine learning in practice and a few practical implementations of DP."],"url":"http://arxiv.org/abs/2404.04706v1","category":"cs.CR"}
{"created":"2024-04-06 17:37:45","title":"The Identification and Categorization of Anemia Through Artificial Neural Networks: A Comparative Analysis of Three Models","abstract":"This paper presents different neural network-based classifier algorithms for diagnosing and classifying Anemia. The study compares these classifiers with established models such as Feed Forward Neural Network (FFNN), Elman network, and Non-linear Auto-Regressive Exogenous model (NARX). Experimental evaluations were conducted using data from clinical laboratory test results for 230 patients. The proposed neural network features nine inputs (age, gender, RBC, HGB, HCT, MCV, MCH, MCHC, WBCs) and one output. The simulation outcomes for diverse patients demonstrate that the suggested artificial neural network rapidly and accurately detects the presence of the disease. Consequently, the network could be seamlessly integrated into clinical laboratories for automatic generation of Anemia patients' reports Additionally, the suggested method is affordable and can be deployed on hardware at low costs.","sentences":["This paper presents different neural network-based classifier algorithms for diagnosing and classifying Anemia.","The study compares these classifiers with established models such as Feed Forward Neural Network (FFNN), Elman network, and Non-linear Auto-Regressive Exogenous model (NARX).","Experimental evaluations were conducted using data from clinical laboratory test results for 230 patients.","The proposed neural network features nine inputs (age, gender, RBC, HGB, HCT, MCV, MCH, MCHC, WBCs) and one output.","The simulation outcomes for diverse patients demonstrate that the suggested artificial neural network rapidly and accurately detects the presence of the disease.","Consequently, the network could be seamlessly integrated into clinical laboratories for automatic generation of Anemia patients' reports Additionally, the suggested method is affordable and can be deployed on hardware at low costs."],"url":"http://arxiv.org/abs/2404.04690v1","category":"cs.LG"}
{"created":"2024-04-06 17:23:43","title":"Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion","abstract":"Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent technique in computer vision and graphics for reconstructing 3D scenes. GS represents a scene as a set of 3D Gaussians with varying opacities and employs a computationally efficient splatting operation along with analytical derivatives to compute the 3D Gaussian parameters given scene images captured from various viewpoints. Unfortunately, capturing surround view ($360^{\\circ}$ viewpoint) images is impossible or impractical in many real-world imaging scenarios, including underwater imaging, rooms inside a building, and autonomous navigation. In these restricted baseline imaging scenarios, the GS algorithm suffers from a well-known 'missing cone' problem, which results in poor reconstruction along the depth axis. In this manuscript, we demonstrate that using transient data (from sonars) allows us to address the missing cone problem by sampling high-frequency data along the depth axis. We extend the Gaussian splatting algorithms for two commonly used sonars and propose fusion algorithms that simultaneously utilize RGB camera data and sonar data. Through simulations, emulations, and hardware experiments across various imaging scenarios, we show that the proposed fusion algorithms lead to significantly better novel view synthesis (5 dB improvement in PSNR) and 3D geometry reconstruction (60% lower Chamfer distance).","sentences":["Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent technique in computer vision and graphics for reconstructing 3D scenes.","GS represents a scene as a set of 3D Gaussians with varying opacities and employs a computationally efficient splatting operation along with analytical derivatives to compute the 3D Gaussian parameters given scene images captured from various viewpoints.","Unfortunately, capturing surround view ($360^{\\circ}$ viewpoint) images is impossible or impractical in many real-world imaging scenarios, including underwater imaging, rooms inside a building, and autonomous navigation.","In these restricted baseline imaging scenarios, the GS algorithm suffers from a well-known 'missing cone' problem, which results in poor reconstruction along the depth axis.","In this manuscript, we demonstrate that using transient data (from sonars) allows us to address the missing cone problem by sampling high-frequency data along the depth axis.","We extend the Gaussian splatting algorithms for two commonly used sonars and propose fusion algorithms that simultaneously utilize RGB camera data and sonar data.","Through simulations, emulations, and hardware experiments across various imaging scenarios, we show that the proposed fusion algorithms lead to significantly better novel view synthesis (5 dB improvement in PSNR) and 3D geometry reconstruction (60% lower Chamfer distance)."],"url":"http://arxiv.org/abs/2404.04687v1","category":"cs.CV"}
{"created":"2024-04-06 16:50:43","title":"Branch-cut in the shear-stress response function of massless $\u03bb \\varphi^4$ with Boltzmann statistics","abstract":"Using an analytical result for the eigensystem of the linearized collision term for a classical system of massless scalar particles with quartic self-interactions, we show that the shear-stress linear response function possesses a branch-cut singularity that covers the whole positive imaginary semi-axis. This is demonstrated in two ways: (1) by truncating the exact, infinite linear system of linear equations for the rank-two tensor modes, which reveals the cut touching the origin; and (2) by employing the Trotterization techniques to invert the linear response problem. The former shows that the first pole tends towards the origin and the average separation between consecutive poles tends towards zero as power laws in the dimension of the basis. The latter allows one to obtain the response function in closed form in terms of Tricomi hypergeometrical functions, which possess a branch-cut on the above-mentioned semi-axis. This suggests that the presence of a cut along the imaginary frequency axis of the shear stress correlator, inferred from previous numerical analyses of weakly coupled scalar $\\lambda \\varphi^4$ theories, does not arise due to quantum statistics but instead emerges from the fundamental properties of this system's interactions.","sentences":["Using an analytical result for the eigensystem of the linearized collision term for a classical system of massless scalar particles with quartic self-interactions, we show that the shear-stress linear response function possesses a branch-cut singularity that covers the whole positive imaginary semi-axis.","This is demonstrated in two ways: (1) by truncating the exact, infinite linear system of linear equations for the rank-two tensor modes, which reveals the cut touching the origin; and (2) by employing the Trotterization techniques to invert the linear response problem.","The former shows that the first pole tends towards the origin and the average separation between consecutive poles tends towards zero as power laws in the dimension of the basis.","The latter allows one to obtain the response function in closed form in terms of Tricomi hypergeometrical functions, which possess a branch-cut on the above-mentioned semi-axis.","This suggests that the presence of a cut along the imaginary frequency axis of the shear stress correlator, inferred from previous numerical analyses of weakly coupled scalar $\\lambda \\varphi^4$ theories, does not arise due to quantum statistics but instead emerges from the fundamental properties of this system's interactions."],"url":"http://arxiv.org/abs/2404.04679v1","category":"nucl-th"}
{"created":"2024-04-06 16:48:12","title":"Automatic Gradient Estimation for Calibrating Crowd Models with Discrete Decision Making","abstract":"Recently proposed gradient estimators enable gradient descent over stochastic programs with discrete jumps in the response surface, which are not covered by automatic differentiation (AD) alone. Although these estimators' capability to guide a swift local search has been shown for certain problems, their applicability to models relevant to real-world applications remains largely unexplored. As the gradients governing the choice in candidate solutions are calculated from sampled simulation trajectories, the optimization procedure bears similarities to metaheuristics such as particle swarm optimization, which puts the focus on the different methods' calibration progress per function evaluation. Here, we consider the calibration of force-based crowd evacuation models based on the popular Social Force model augmented by discrete decision making. After studying the ability of an AD-based estimator for branching programs to capture the simulation's rugged response surface, calibration problems are tackled using gradient descent and two metaheuristics. As our main insights, we find 1) that the estimation's fidelity benefits from disregarding jumps of large magnitude inherent to the Social Force model, and 2) that the common problem of calibration by adjusting a simulation input distribution obviates the need for AD across the Social Force calculations, allowing gradient descent to excel.","sentences":["Recently proposed gradient estimators enable gradient descent over stochastic programs with discrete jumps in the response surface, which are not covered by automatic differentiation (AD) alone.","Although these estimators' capability to guide a swift local search has been shown for certain problems, their applicability to models relevant to real-world applications remains largely unexplored.","As the gradients governing the choice in candidate solutions are calculated from sampled simulation trajectories, the optimization procedure bears similarities to metaheuristics such as particle swarm optimization, which puts the focus on the different methods' calibration progress per function evaluation.","Here, we consider the calibration of force-based crowd evacuation models based on the popular Social Force model augmented by discrete decision making.","After studying the ability of an AD-based estimator for branching programs to capture the simulation's rugged response surface, calibration problems are tackled using gradient descent and two metaheuristics.","As our main insights, we find 1) that the estimation's fidelity benefits from disregarding jumps of large magnitude inherent to the Social Force model, and 2) that the common problem of calibration by adjusting a simulation input distribution obviates the need for AD across the Social Force calculations, allowing gradient descent to excel."],"url":"http://arxiv.org/abs/2404.04678v1","category":"cs.LG"}
{"created":"2024-04-06 16:29:10","title":"Neural-ABC: Neural Parametric Models for Articulated Body with Clothes","abstract":"In this paper, we introduce Neural-ABC, a novel parametric model based on neural implicit functions that can represent clothed human bodies with disentangled latent spaces for identity, clothing, shape, and pose. Traditional mesh-based representations struggle to represent articulated bodies with clothes due to the diversity of human body shapes and clothing styles, as well as the complexity of poses. Our proposed model provides a unified framework for parametric modeling, which can represent the identity, clothing, shape and pose of the clothed human body. Our proposed approach utilizes the power of neural implicit functions as the underlying representation and integrates well-designed structures to meet the necessary requirements. Specifically, we represent the underlying body as a signed distance function and clothing as an unsigned distance function, and they can be uniformly represented as unsigned distance fields. Different types of clothing do not require predefined topological structures or classifications, and can follow changes in the underlying body to fit the body. Additionally, we construct poses using a controllable articulated structure. The model is trained on both open and newly constructed datasets, and our decoupling strategy is carefully designed to ensure optimal performance. Our model excels at disentangling clothing and identity in different shape and poses while preserving the style of the clothing. We demonstrate that Neural-ABC fits new observations of different types of clothing. Compared to other state-of-the-art parametric models, Neural-ABC demonstrates powerful advantages in the reconstruction of clothed human bodies, as evidenced by fitting raw scans, depth maps and images. We show that the attributes of the fitted results can be further edited by adjusting their identities, clothing, shape and pose codes.","sentences":["In this paper, we introduce Neural-ABC, a novel parametric model based on neural implicit functions that can represent clothed human bodies with disentangled latent spaces for identity, clothing, shape, and pose.","Traditional mesh-based representations struggle to represent articulated bodies with clothes due to the diversity of human body shapes and clothing styles, as well as the complexity of poses.","Our proposed model provides a unified framework for parametric modeling, which can represent the identity, clothing, shape and pose of the clothed human body.","Our proposed approach utilizes the power of neural implicit functions as the underlying representation and integrates well-designed structures to meet the necessary requirements.","Specifically, we represent the underlying body as a signed distance function and clothing as an unsigned distance function, and they can be uniformly represented as unsigned distance fields.","Different types of clothing do not require predefined topological structures or classifications, and can follow changes in the underlying body to fit the body.","Additionally, we construct poses using a controllable articulated structure.","The model is trained on both open and newly constructed datasets, and our decoupling strategy is carefully designed to ensure optimal performance.","Our model excels at disentangling clothing and identity in different shape and poses while preserving the style of the clothing.","We demonstrate that Neural-ABC fits new observations of different types of clothing.","Compared to other state-of-the-art parametric models, Neural-ABC demonstrates powerful advantages in the reconstruction of clothed human bodies, as evidenced by fitting raw scans, depth maps and images.","We show that the attributes of the fitted results can be further edited by adjusting their identities, clothing, shape and pose codes."],"url":"http://arxiv.org/abs/2404.04673v1","category":"cs.CV"}
{"created":"2024-04-06 16:29:00","title":"Hidden order in dielectrics: string condensation, solitons, and the charge-vortex duality","abstract":"Description of electrons as solitons of the polarization field implies that an ordinary dielectric has a hidden order, associated with the invariance with respect to adding loops of quantized electric flux. We describe the mechanism by which the finite polarizability of the medium renders the interaction between the solitons short-ranged (prior to their coupling to electromagnetism) and argue that the structure of the solitons allows them to be quantized as either fermions or bosons. At the quantum level, the theory has, in addition to the solitonic electric, elementary magnetic excitations, suggesting that small dielectrics may host quantized magnetic vortices carrying circular polarization currents.","sentences":["Description of electrons as solitons of the polarization field implies that an ordinary dielectric has a hidden order, associated with the invariance with respect to adding loops of quantized electric flux.","We describe the mechanism by which the finite polarizability of the medium renders the interaction between the solitons short-ranged (prior to their coupling to electromagnetism) and argue that the structure of the solitons allows them to be quantized as either fermions or bosons.","At the quantum level, the theory has, in addition to the solitonic electric, elementary magnetic excitations, suggesting that small dielectrics may host quantized magnetic vortices carrying circular polarization currents."],"url":"http://arxiv.org/abs/2404.04672v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-06 15:31:20","title":"Learning Minimal NAP Specifications for Neural Network Verification","abstract":"Specifications play a crucial role in neural network verification. They define the precise input regions we aim to verify, typically represented as L-infinity norm balls. While recent research suggests using neural activation patterns (NAPs) as specifications for verifying unseen test set data, it focuses on computing the most refined NAPs, often limited to very small regions in the input space. In this paper, we study the following problem: Given a neural network, find a minimal (coarsest) NAP that is sufficient for formal verification of the network's robustness. Finding the minimal NAP specification not only expands verifiable bounds but also provides insights into which neurons contribute to the model's robustness. To address this problem, we propose several exact and approximate approaches. Our exact approaches leverage the verification tool to find minimal NAP specifications in either a deterministic or statistical manner. Whereas the approximate methods efficiently estimate minimal NAPs using adversarial examples and local gradients, without making calls to the verification tool. This allows us to inspect potential causal links between neurons and the robustness of state-of-the-art neural networks, a task for which existing verification frameworks fail to scale. Our experimental results suggest that minimal NAP specifications require much smaller fractions of neurons compared to the most refined NAP specifications, yet they can significantly expand the verifiable boundaries to several orders of magnitude larger.","sentences":["Specifications play a crucial role in neural network verification.","They define the precise input regions we aim to verify, typically represented as L-infinity norm balls.","While recent research suggests using neural activation patterns (NAPs) as specifications for verifying unseen test set data, it focuses on computing the most refined NAPs, often limited to very small regions in the input space.","In this paper, we study the following problem: Given a neural network, find a minimal (coarsest) NAP that is sufficient for formal verification of the network's robustness.","Finding the minimal NAP specification not only expands verifiable bounds but also provides insights into which neurons contribute to the model's robustness.","To address this problem, we propose several exact and approximate approaches.","Our exact approaches leverage the verification tool to find minimal NAP specifications in either a deterministic or statistical manner.","Whereas the approximate methods efficiently estimate minimal NAPs using adversarial examples and local gradients, without making calls to the verification tool.","This allows us to inspect potential causal links between neurons and the robustness of state-of-the-art neural networks, a task for which existing verification frameworks fail to scale.","Our experimental results suggest that minimal NAP specifications require much smaller fractions of neurons compared to the most refined NAP specifications, yet they can significantly expand the verifiable boundaries to several orders of magnitude larger."],"url":"http://arxiv.org/abs/2404.04662v1","category":"cs.LG"}
{"created":"2024-04-06 15:21:28","title":"Characterization of the weighted Sobolev space $H_\u03b2^{s}(\u03a9)$ in $\\mathbb{R}^{2}$ in terms of the decay rate of Fourier-Jacobi coefficients","abstract":"In this paper, motivated by the analysis of the fractional Laplace equation on the unit disk in $\\mathbb{R}^{2}$, we establish a characterization of the weighted Sobolev space $H_{\\beta}^{s}(\\Omega)$ in terms of the decay rate of Fourier-Jacobi coefficients. This framework is then used to give a precise analysis of the solution to the fractional Laplace equation on the unit disk.","sentences":["In this paper, motivated by the analysis of the fractional Laplace equation on the unit disk in $\\mathbb{R}^{2}$, we establish a characterization of the weighted Sobolev space $H_{\\beta}^{s}(\\Omega)$ in terms of the decay rate of Fourier-Jacobi coefficients.","This framework is then used to give a precise analysis of the solution to the fractional Laplace equation on the unit disk."],"url":"http://arxiv.org/abs/2404.04658v1","category":"math.AP"}
{"created":"2024-04-06 15:10:29","title":"HawkDrive: A Transformer-driven Visual Perception System for Autonomous Driving in Night Scene","abstract":"Many established vision perception systems for autonomous driving scenarios ignore the influence of light conditions, one of the key elements for driving safety. To address this problem, we present HawkDrive, a novel perception system with hardware and software solutions. Hardware that utilizes stereo vision perception, which has been demonstrated to be a more reliable way of estimating depth information than monocular vision, is partnered with the edge computing device Nvidia Jetson Xavier AGX. Our software for low light enhancement, depth estimation, and semantic segmentation tasks, is a transformer-based neural network. Our software stack, which enables fast inference and noise reduction, is packaged into system modules in Robot Operating System 2 (ROS2). Our experimental results have shown that the proposed end-to-end system is effective in improving the depth estimation and semantic segmentation performance. Our dataset and codes will be released at https://github.com/ZionGo6/HawkDrive.","sentences":["Many established vision perception systems for autonomous driving scenarios ignore the influence of light conditions, one of the key elements for driving safety.","To address this problem, we present HawkDrive, a novel perception system with hardware and software solutions.","Hardware that utilizes stereo vision perception, which has been demonstrated to be a more reliable way of estimating depth information than monocular vision, is partnered with the edge computing device Nvidia Jetson Xavier AGX.","Our software for low light enhancement, depth estimation, and semantic segmentation tasks, is a transformer-based neural network.","Our software stack, which enables fast inference and noise reduction, is packaged into system modules in Robot Operating System 2 (ROS2).","Our experimental results have shown that the proposed end-to-end system is effective in improving the depth estimation and semantic segmentation performance.","Our dataset and codes will be released at https://github.com/ZionGo6/HawkDrive."],"url":"http://arxiv.org/abs/2404.04653v1","category":"cs.CV"}
{"created":"2024-04-06 14:49:36","title":"Structured Gradient-based Interpretations via Norm-Regularized Adversarial Training","abstract":"Gradient-based saliency maps have been widely used to explain the decisions of deep neural network classifiers. However, standard gradient-based interpretation maps, including the simple gradient and integrated gradient algorithms, often lack desired structures such as sparsity and connectedness in their application to real-world computer vision models. A frequently used approach to inducing sparsity structures into gradient-based saliency maps is to alter the simple gradient scheme using sparsification or norm-based regularization. A drawback with such post-processing methods is their frequently-observed significant loss in fidelity to the original simple gradient map. In this work, we propose to apply adversarial training as an in-processing scheme to train neural networks with structured simple gradient maps. We show a duality relation between the regularized norms of the adversarial perturbations and gradient-based maps, based on which we design adversarial training loss functions promoting sparsity and group-sparsity properties in simple gradient maps. We present several numerical results to show the influence of our proposed norm-based adversarial training methods on the standard gradient-based maps of standard neural network architectures on benchmark image datasets.","sentences":["Gradient-based saliency maps have been widely used to explain the decisions of deep neural network classifiers.","However, standard gradient-based interpretation maps, including the simple gradient and integrated gradient algorithms, often lack desired structures such as sparsity and connectedness in their application to real-world computer vision models.","A frequently used approach to inducing sparsity structures into gradient-based saliency maps is to alter the simple gradient scheme using sparsification or norm-based regularization.","A drawback with such post-processing methods is their frequently-observed significant loss in fidelity to the original simple gradient map.","In this work, we propose to apply adversarial training as an in-processing scheme to train neural networks with structured simple gradient maps.","We show a duality relation between the regularized norms of the adversarial perturbations and gradient-based maps, based on which we design adversarial training loss functions promoting sparsity and group-sparsity properties in simple gradient maps.","We present several numerical results to show the influence of our proposed norm-based adversarial training methods on the standard gradient-based maps of standard neural network architectures on benchmark image datasets."],"url":"http://arxiv.org/abs/2404.04647v1","category":"cs.CV"}
{"created":"2024-04-06 14:34:35","title":"Solve arbitrary one-loop reduction with generating function","abstract":"Recently, the concept of generating function has been employed in one-loop reduction. For one-loop integrals encompassing arbitrary tensor ranks and higher-pole contributions, the generating function can be decomposed into a tensor part and a higher-pole part. While the tensor component has been thoroughly addressed in recent studies, there remains a lack of satisfactory investigations regarding the higher-pole part. In this work, we completely solve the problem. We first establish the partial differential equations governing the higher-pole generating function. Based on these equations, we derive an integration recursion relation and solve it iteratively. This approach enables us to explore the analytical structure of higher-pole reduction and provides a valuable tool for generating reduction coefficients efficiently.","sentences":["Recently, the concept of generating function has been employed in one-loop reduction.","For one-loop integrals encompassing arbitrary tensor ranks and higher-pole contributions, the generating function can be decomposed into a tensor part and a higher-pole part.","While the tensor component has been thoroughly addressed in recent studies, there remains a lack of satisfactory investigations regarding the higher-pole part.","In this work, we completely solve the problem.","We first establish the partial differential equations governing the higher-pole generating function.","Based on these equations, we derive an integration recursion relation and solve it iteratively.","This approach enables us to explore the analytical structure of higher-pole reduction and provides a valuable tool for generating reduction coefficients efficiently."],"url":"http://arxiv.org/abs/2404.04644v1","category":"hep-ph"}
{"created":"2024-04-06 14:24:52","title":"On blow-up conditions for solutions of systems of quasilinear second-order elliptic inequalities","abstract":"We study systems of the differential inequalities $$   \\left\\{   \\begin{aligned}   &   - \\operatorname{div} A_1 (x, \\nabla u_1)   \\ge   F_1 (x, u_2)   &   \\mbox{in } {\\mathbb R}^n,   &   - \\operatorname{div} A_2 (x, \\nabla u_2)   \\ge   F_2 (x, u_1)   &   \\mbox{in } {\\mathbb R}^n,   \\end{aligned}   \\right. $$ where $n \\ge 2$ and $A_i$ are Caratheodory functions such that $$   C_1   |\\xi|^{p_i}   \\le   \\xi   A_i (x, \\xi),   \\quad   |A_i (x, \\xi)|   \\le   C_2   |\\xi|^{p_i - 1},   \\quad i = 1,2, $$ with some constants $C_1, C_2 > 0$ and $p_1, p_2 > 1$ for almost all $x \\in {\\mathbb R}^n$ and for all $\\xi \\in {\\mathbb R}^n$, $n \\ge 2$. For non-negative solutions of these systems we obtain exact blow-up conditions.","sentences":["We study systems of the differential inequalities $$   \\left\\{   \\begin{aligned}   &   - \\operatorname{div} A_1 (x, \\nabla u_1)   \\ge   F_1 (x, u_2)   &   \\mbox{in } {\\mathbb R}^n,   &   - \\operatorname{div} A_2 (x, \\nabla u_2)   \\ge   F_2 (x, u_1)   &   \\mbox{in } {\\mathbb R}^n,   \\end{aligned}   \\right.","$$ where $n \\ge 2$ and $A_i$ are Caratheodory functions such that $$   C_1   |\\xi|^{p_i}   \\le   \\xi   A_i (x, \\xi),   \\quad   |A_i (x, \\xi)|   \\le   C_2   |\\xi|^{p_i - 1},   \\quad i = 1,2, $$ with some constants $C_1, C_2 > 0$ and $p_1, p_2 > 1$ for almost all $x \\in {\\mathbb R}^n$ and for all $\\xi \\in {\\mathbb R}^n$, $n \\ge 2$. For non-negative solutions of these systems we obtain exact blow-up conditions."],"url":"http://arxiv.org/abs/2404.04641v1","category":"math.AP"}
{"created":"2024-04-06 14:17:08","title":"Uncertainty quantification analysis of bifurcations of the Allen--Cahn equation with random coefficients","abstract":"In this work we consider the Allen--Cahn equation, a prototypical model problem in nonlinear dynamics that exhibits bifurcations corresponding to variations of a deterministic bifurcation parameter. Going beyond the state-of-the-art, we introduce a random coefficient function in the linear reaction part of the equation, thereby accounting for random, spatially-heterogeneous effects. Importantly, we assume a spatially constant, deterministic mean value of the random coefficient. We show that this mean value is in fact a bifurcation parameter in the Allen--Cahn equation with random coefficients. Moreover, we show that the bifurcation points and bifurcation curves become random objects. We consider two distinct modelling situations: (i) for a spatially homogeneous coefficient we derive analytical expressions for the distribution of the bifurcation points and show that the bifurcation curves are random shifts of a fixed reference curve; (ii) for a spatially heterogeneous coefficient we employ a generalized polynomial chaos expansion to approximate the statistical properties of the random bifurcation points and bifurcation curves. We present numerical examples in 1D physical space, where we combine the popular software package Continuation Core and Toolboxes (CoCo) for numerical continuation and the Sparse Grids Matlab Kit for the polynomial chaos expansion. Our exposition addresses both, dynamical systems and uncertainty quantification, highlighting how analytical and numerical tools from both areas can be combined efficiently for the challenging uncertainty quantification analysis of bifurcations in random differential equations.","sentences":["In this work we consider the Allen--Cahn equation, a prototypical model problem in nonlinear dynamics that exhibits bifurcations corresponding to variations of a deterministic bifurcation parameter.","Going beyond the state-of-the-art, we introduce a random coefficient function in the linear reaction part of the equation, thereby accounting for random, spatially-heterogeneous effects.","Importantly, we assume a spatially constant, deterministic mean value of the random coefficient.","We show that this mean value is in fact a bifurcation parameter in the Allen--Cahn equation with random coefficients.","Moreover, we show that the bifurcation points and bifurcation curves become random objects.","We consider two distinct modelling situations: (i) for a spatially homogeneous coefficient we derive analytical expressions for the distribution of the bifurcation points and show that the bifurcation curves are random shifts of a fixed reference curve; (ii) for a spatially heterogeneous coefficient we employ a generalized polynomial chaos expansion to approximate the statistical properties of the random bifurcation points and bifurcation curves.","We present numerical examples in 1D physical space, where we combine the popular software package Continuation Core and Toolboxes (CoCo) for numerical continuation and the Sparse Grids Matlab Kit for the polynomial chaos expansion.","Our exposition addresses both, dynamical systems and uncertainty quantification, highlighting how analytical and numerical tools from both areas can be combined efficiently for the challenging uncertainty quantification analysis of bifurcations in random differential equations."],"url":"http://arxiv.org/abs/2404.04639v1","category":"math.NA"}
{"created":"2024-04-06 13:40:18","title":"Deciphering Radio Emissions from Accretion Disk Winds in Radio-Quiet Active Galactic Nuclei","abstract":"Unraveling the origins of radio emissions from radio-quiet active galactic nuclei (RQ AGNs) remains a pivotal challenge in astrophysics. One potential source of this radiation is the shock interaction between AGN disk winds and the interstellar medium (ISM). To understand this phenomenon, we construct a spherical, one-zone, and self-similar expansion model of shock structure between ultra-fast outflows (UFOs) and the ISM. We then calculate the energy density distribution of non-thermal electrons by solving the transport equation, considering diffusive shock acceleration as the acceleration mechanism and synchrotron and inverse Compton cooling as the cooling mechanisms. Based on the derived energy distribution of non-thermal electrons, we model the radio synchrotron spectrum of shocked ISM. For the 15 nearby RQ AGNs hosting UFOs, we investigate shocked ISM parameters required to model their observed radio spectra, based on X-ray observations and measured UFO velocities. Radio spectra of 11 out of 15 nearby RQ AGNs would be explained by the AGN disk wind model. This is a compelling indication that shock interactions between AGN disk winds and the ISM could indeed be the source of their radio emissions. The typical predicted source size and magnetic field strength are several $100$ pc and $0.1$ mG, respectively. We also discuss whether our prediction can be tested by future radio observations.","sentences":["Unraveling the origins of radio emissions from radio-quiet active galactic nuclei (RQ AGNs) remains a pivotal challenge in astrophysics.","One potential source of this radiation is the shock interaction between AGN disk winds and the interstellar medium (ISM).","To understand this phenomenon, we construct a spherical, one-zone, and self-similar expansion model of shock structure between ultra-fast outflows (UFOs) and the ISM.","We then calculate the energy density distribution of non-thermal electrons by solving the transport equation, considering diffusive shock acceleration as the acceleration mechanism and synchrotron and inverse Compton cooling as the cooling mechanisms.","Based on the derived energy distribution of non-thermal electrons, we model the radio synchrotron spectrum of shocked ISM.","For the 15 nearby RQ AGNs hosting UFOs, we investigate shocked ISM parameters required to model their observed radio spectra, based on X-ray observations and measured UFO velocities.","Radio spectra of 11 out of 15 nearby RQ AGNs would be explained by the AGN disk wind model.","This is a compelling indication that shock interactions between AGN disk winds and the ISM could indeed be the source of their radio emissions.","The typical predicted source size and magnetic field strength are several $100$ pc and $0.1$ mG, respectively.","We also discuss whether our prediction can be tested by future radio observations."],"url":"http://arxiv.org/abs/2404.04632v1","category":"astro-ph.HE"}
{"created":"2024-04-06 13:25:18","title":"A refined convergence estimate for a fourth order finite difference numerical scheme to the Cahn-Hilliard equation","abstract":"In this article we present a refined convergence analysis for a second order accurate in time, fourth order finite difference numerical scheme for the 3-D Cahn-Hilliard equation, with an improved convergence constant. A modified backward differentiation formula temporal discretization is applied, and a Douglas-Dupont artificial regularization is included to ensure the energy stability. In fact, a standard application of discrete Gronwall inequality leads to a convergence constant dependent on the interface width parameter in an exponential singular form. We aim to obtain an improved estimate, with such a singular dependence only in a polynomial order. A uniform in time functional bounds of the numerical solution, including the higher order Sobolev norms, as well as the associated bounds for the first and second order temporal difference stencil, have to be carefully established. Certain recursive analysis has to be applied in the analysis for the BDF-style temporal stencil. As a result, we are able to apply a spectrum estimate for the linearized Cahn-Hilliard operator, and this technique leads to the refined error estimate. A three-dimensional numerical example of accuracy check is presented as well.","sentences":["In this article we present a refined convergence analysis for a second order accurate in time, fourth order finite difference numerical scheme for the 3-D Cahn-Hilliard equation, with an improved convergence constant.","A modified backward differentiation formula temporal discretization is applied, and a Douglas-Dupont artificial regularization is included to ensure the energy stability.","In fact, a standard application of discrete Gronwall inequality leads to a convergence constant dependent on the interface width parameter in an exponential singular form.","We aim to obtain an improved estimate, with such a singular dependence only in a polynomial order.","A uniform in time functional bounds of the numerical solution, including the higher order Sobolev norms, as well as the associated bounds for the first and second order temporal difference stencil, have to be carefully established.","Certain recursive analysis has to be applied in the analysis for the BDF-style temporal stencil.","As a result, we are able to apply a spectrum estimate for the linearized Cahn-Hilliard operator, and this technique leads to the refined error estimate.","A three-dimensional numerical example of accuracy check is presented as well."],"url":"http://arxiv.org/abs/2404.04628v1","category":"math.NA"}
{"created":"2024-04-06 12:49:20","title":"Vanishing Variance Problem in Fully Decentralized Neural-Network Systems","abstract":"Federated learning and gossip learning are emerging methodologies designed to mitigate data privacy concerns by retaining training data on client devices and exclusively sharing locally-trained machine learning (ML) models with others. The primary distinction between the two lies in their approach to model aggregation: federated learning employs a centralized parameter server, whereas gossip learning adopts a fully decentralized mechanism, enabling direct model exchanges among nodes. This decentralized nature often positions gossip learning as less efficient compared to federated learning. Both methodologies involve a critical step: computing a representation of received ML models and integrating this representation into the existing model. Conventionally, this representation is derived by averaging the received models, exemplified by the FedAVG algorithm. Our findings suggest that this averaging approach inherently introduces a potential delay in model convergence. We identify the underlying cause and refer to it as the \"vanishing variance\" problem, where averaging across uncorrelated ML models undermines the optimal variance established by the Xavier weight initialization. Unlike federated learning where the central server ensures model correlation, and unlike traditional gossip learning which circumvents this problem through model partitioning and sampling, our research introduces a variance-corrected model averaging algorithm. This novel algorithm preserves the optimal variance needed during model averaging, irrespective of network topology or non-IID data distributions. Our extensive simulation results demonstrate that our approach enables gossip learning to achieve convergence efficiency comparable to that of federated learning.","sentences":["Federated learning and gossip learning are emerging methodologies designed to mitigate data privacy concerns by retaining training data on client devices and exclusively sharing locally-trained machine learning (ML) models with others.","The primary distinction between the two lies in their approach to model aggregation: federated learning employs a centralized parameter server, whereas gossip learning adopts a fully decentralized mechanism, enabling direct model exchanges among nodes.","This decentralized nature often positions gossip learning as less efficient compared to federated learning.","Both methodologies involve a critical step: computing a representation of received ML models and integrating this representation into the existing model.","Conventionally, this representation is derived by averaging the received models, exemplified by the FedAVG algorithm.","Our findings suggest that this averaging approach inherently introduces a potential delay in model convergence.","We identify the underlying cause and refer to it as the \"vanishing variance\" problem, where averaging across uncorrelated ML models undermines the optimal variance established by the Xavier weight initialization.","Unlike federated learning where the central server ensures model correlation, and unlike traditional gossip learning which circumvents this problem through model partitioning and sampling, our research introduces a variance-corrected model averaging algorithm.","This novel algorithm preserves the optimal variance needed during model averaging, irrespective of network topology or non-IID data distributions.","Our extensive simulation results demonstrate that our approach enables gossip learning to achieve convergence efficiency comparable to that of federated learning."],"url":"http://arxiv.org/abs/2404.04616v1","category":"cs.LG"}
{"created":"2024-04-06 12:40:21","title":"Spectral Graph Pruning Against Over-Squashing and Over-Smoothing","abstract":"Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a more effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on large heterophilic datasets.","sentences":["Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing.","The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions.","However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable.","Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously.","This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets.","To this end, we propose a more effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on large heterophilic datasets."],"url":"http://arxiv.org/abs/2404.04612v1","category":"cs.LG"}
{"created":"2024-04-06 12:32:44","title":"The particle acceleration study in blazar jet","abstract":"The particle acceleration of blazar jets is crucial to high-energy astrophysics, yet the acceleration mechanism division in blazar subclasses and the underlying nature of these mechanisms remain elusive. In this work, we utilized the synchrotron spectral information (synchrotron peak frequency, $\\log \\nu_{\\rm sy}$, and corresponding curvature, $b_{\\rm sy}$) of 2705 blazars from the literature and studied the subject of particle acceleration in blazar jets by analysing the correlation between $\\log \\nu_{\\rm sy}$ and $1/b_{\\rm sy}$. Our results suggested that the entire sample follows an energy-dependent probability acceleration (EDPA). Specifically, the low inverse Compton peak sources (LCPs) follow the mechanism that fluctuations of fractional gain acceleration (FFGA), while the high inverse Compton peak sources (HCPs) follow an acceleration mechanism of EDPA. Our results indicated that the separation between LCPs and HCPs results from the electron peak Lorentz factor ($\\gamma_{\\rm p}$), and the differentiation should originate from different acceleration mechanisms. Moreover, our study revealed a transition in the acceleration mechanism from FFGA to EDPA around $\\log \\nu_{\\rm sy} \\sim 15$ through a detailed analysis of binned-$\\log \\nu_{\\rm sy}$. The mechanism of FFGA dominates the particle acceleration in LCP jets because of stronger jets and the EDPA dominates the particle energy gain in the HCPs due to a more efficient acceleration process.","sentences":["The particle acceleration of blazar jets is crucial to high-energy astrophysics, yet the acceleration mechanism division in blazar subclasses and the underlying nature of these mechanisms remain elusive.","In this work, we utilized the synchrotron spectral information (synchrotron peak frequency, $\\log \\nu_{\\rm sy}$, and corresponding curvature, $b_{\\rm sy}$) of 2705 blazars from the literature and studied the subject of particle acceleration in blazar jets by analysing the correlation between $\\log \\nu_{\\rm sy}$ and $1/b_{\\rm sy}$. Our results suggested that the entire sample follows an energy-dependent probability acceleration (EDPA).","Specifically, the low inverse Compton peak sources (LCPs) follow the mechanism that fluctuations of fractional gain acceleration (FFGA), while the high inverse Compton peak sources (HCPs) follow an acceleration mechanism of EDPA.","Our results indicated that the separation between LCPs and HCPs results from the electron peak Lorentz factor ($\\gamma_{\\rm p}$), and the differentiation should originate from different acceleration mechanisms.","Moreover, our study revealed a transition in the acceleration mechanism from FFGA to EDPA around $\\log \\nu_{\\rm sy} \\sim 15$ through a detailed analysis of binned-$\\log \\nu_{\\rm sy}$. The mechanism of FFGA dominates the particle acceleration in LCP jets because of stronger jets and the EDPA dominates the particle energy gain in the HCPs due to a more efficient acceleration process."],"url":"http://arxiv.org/abs/2404.04609v1","category":"astro-ph.HE"}
{"created":"2024-04-06 12:25:45","title":"Quantized perfect transmission in graphene nanoribbons with random hollow adsorbates","abstract":"Impurities exist inevitably in two-dimensional materials as they spontaneously adsorb onto the surface during fabrication, usually exerting detrimental effects on electronic transport. Here, we focus on a special type of impurities that preferentially adsorb onto the hollow regions of graphene nanoribbons (GNRs), and study how they affect the quantum transport in GNRs. Contrary to previous knowledge that random adatoms should localize electrons, the so-called Anderson localization, noteworthy quantized conductance peaks (QCPs) are observed at specific electron energies. These QCPs are remarkably robust against variations in system size, GNR edge, and adatom properties, and they can reappear at identical energies following an arithmetic sequence of device width. Further investigation of wavefunction reveals a unique transport mode at each QCP energy which transmits through disordered GNRs reflectionlessly, while all the others become fully Anderson localized, indicating the survival of quantum ballistic transport in the localized regime. Our findings highlight the potential utility of hollow adatoms as a powerful tool to manipulate the conductivity of GNRs, and deepen the understanding of the interplay between impurities and graphene.","sentences":["Impurities exist inevitably in two-dimensional materials as they spontaneously adsorb onto the surface during fabrication, usually exerting detrimental effects on electronic transport.","Here, we focus on a special type of impurities that preferentially adsorb onto the hollow regions of graphene nanoribbons (GNRs), and study how they affect the quantum transport in GNRs.","Contrary to previous knowledge that random adatoms should localize electrons, the so-called Anderson localization, noteworthy quantized conductance peaks (QCPs) are observed at specific electron energies.","These QCPs are remarkably robust against variations in system size, GNR edge, and adatom properties, and they can reappear at identical energies following an arithmetic sequence of device width.","Further investigation of wavefunction reveals a unique transport mode at each QCP energy which transmits through disordered GNRs reflectionlessly, while all the others become fully Anderson localized, indicating the survival of quantum ballistic transport in the localized regime.","Our findings highlight the potential utility of hollow adatoms as a powerful tool to manipulate the conductivity of GNRs, and deepen the understanding of the interplay between impurities and graphene."],"url":"http://arxiv.org/abs/2404.04607v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-06 11:50:08","title":"Theory of local $\\mathbb{Z}_{2}$ topological markers for finite and periodic two-dimensional systems","abstract":"The topological phases of two-dimensional time-reversal symmetric insulators are classified by a $\\mathbb{Z}_{2}$ topological invariant. Usually, the invariant is introduced and calculated by exploiting the way time-reversal symmetry acts in reciprocal space, hence implicitly assuming periodicity and homogeneity. Here, we introduce two space-resolved $\\mathbb{Z}_{2}$ topological markers that are able to probe the local topology of the ground-state electronic structure also in the case of inhomogeneous and finite systems. The first approach leads to a generalized local spin-Chern marker, that usually remains well-defined also when the perpendicular component of the spin, $S_{z}$, is not conserved. The second marker is solely based on time-reversal symmetry, hence being more general. We validate our markers on the Kane-Mele model both in periodic and open boundary conditions, also in presence of disorder and including topological/trivial heterojunctions.","sentences":["The topological phases of two-dimensional time-reversal symmetric insulators are classified by a $\\mathbb{Z}_{2}$ topological invariant.","Usually, the invariant is introduced and calculated by exploiting the way time-reversal symmetry acts in reciprocal space, hence implicitly assuming periodicity and homogeneity.","Here, we introduce two space-resolved $\\mathbb{Z}_{2}$ topological markers that are able to probe the local topology of the ground-state electronic structure also in the case of inhomogeneous and finite systems.","The first approach leads to a generalized local spin-Chern marker, that usually remains well-defined also when the perpendicular component of the spin, $S_{z}$, is not conserved.","The second marker is solely based on time-reversal symmetry, hence being more general.","We validate our markers on the Kane-Mele model both in periodic and open boundary conditions, also in presence of disorder and including topological/trivial heterojunctions."],"url":"http://arxiv.org/abs/2404.04598v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-06 10:54:35","title":"Neuroevolving Electronic Dynamical Networks","abstract":"Neuroevolution is a powerful method of applying an evolutionary algorithm to refine the performance of artificial neural networks through natural selection; however, the fitness evaluation of these networks can be time-consuming and computationally expensive, particularly for continuous time recurrent neural networks (CTRNNs) that necessitate the simulation of differential equations. To overcome this challenge, field programmable gate arrays (FPGAs) have emerged as an increasingly popular solution, due to their high performance and low power consumption. Further, their ability to undergo dynamic and partial reconfiguration enables the extremely rapid evaluation of the fitness of CTRNNs, effectively addressing the bottleneck associated with conventional methods. By incorporating fitness evaluation directly upon the programmable logic of the FPGA, hyper-parallel evaluation becomes feasible, dramatically reducing the time required for assessment. This inherent parallelism of FPGAs accelerates the entire neuroevolutionary process by several orders of magnitude, facilitating faster convergence to an optimal solution. The work presented in this study demonstrates the potential of utilizing dynamic and partial reconfiguration on capable FPGAs as a powerful platform for neuroevolving dynamic neural networks.","sentences":["Neuroevolution is a powerful method of applying an evolutionary algorithm to refine the performance of artificial neural networks through natural selection; however, the fitness evaluation of these networks can be time-consuming and computationally expensive, particularly for continuous time recurrent neural networks (CTRNNs) that necessitate the simulation of differential equations.","To overcome this challenge, field programmable gate arrays (FPGAs) have emerged as an increasingly popular solution, due to their high performance and low power consumption.","Further, their ability to undergo dynamic and partial reconfiguration enables the extremely rapid evaluation of the fitness of CTRNNs, effectively addressing the bottleneck associated with conventional methods.","By incorporating fitness evaluation directly upon the programmable logic of the FPGA, hyper-parallel evaluation becomes feasible, dramatically reducing the time required for assessment.","This inherent parallelism of FPGAs accelerates the entire neuroevolutionary process by several orders of magnitude, facilitating faster convergence to an optimal solution.","The work presented in this study demonstrates the potential of utilizing dynamic and partial reconfiguration on capable FPGAs as a powerful platform for neuroevolving dynamic neural networks."],"url":"http://arxiv.org/abs/2404.04587v1","category":"cs.NE"}
{"created":"2024-04-06 10:44:58","title":"The Michor-Mumford conjecture in Hilbertian H-type groups","abstract":"We introduce infinite dimensional Hilbertian H-type groups equipped with weak, graded, left invariant Riemannian metrics. For these Lie groups, we show that the vanishing of the geodesic distance and the local unboundedness of the sectional curvature coexist. The result validates a deep phenomenon conjectured in an influential 2005 paper by Michor and Mumford, namely, the vanishing of the geodesic distance is linked to the local unboundedness of the sectional curvature. We prove that degenerate geodesic distances appear for a large class of weak, left invariant Riemannian metrics. Their vanishing is rather surprisingly related to the infinite dimensional sub-Riemannian structure of Hilbertian H-type groups. The same class of weak Riemannian metrics yields the nonexistence of the Levi-Civita covariant derivative.","sentences":["We introduce infinite dimensional Hilbertian H-type groups equipped with weak, graded, left invariant Riemannian metrics.","For these Lie groups, we show that the vanishing of the geodesic distance and the local unboundedness of the sectional curvature coexist.","The result validates a deep phenomenon conjectured in an influential 2005 paper by Michor and Mumford, namely, the vanishing of the geodesic distance is linked to the local unboundedness of the sectional curvature.","We prove that degenerate geodesic distances appear for a large class of weak, left invariant Riemannian metrics.","Their vanishing is rather surprisingly related to the infinite dimensional sub-Riemannian structure of Hilbertian H-type groups.","The same class of weak Riemannian metrics yields the nonexistence of the Levi-Civita covariant derivative."],"url":"http://arxiv.org/abs/2404.04583v1","category":"math.DG"}
{"created":"2024-04-08 17:59:46","title":"Finding Visual Task Vectors","abstract":"Visual Prompting is a technique for teaching models to perform a visual task via in-context examples, without any additional training. In this work, we analyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find task vectors, activations that encode task-specific information. Equipped with this insight, we demonstrate that it is possible to identify the task vectors and use them to guide the network towards performing different tasks without providing any input-output examples. To find task vectors, we compute the average intermediate activations per task and use the REINFORCE algorithm to search for the subset of task vectors. The resulting task vectors guide the model towards performing a task better than the original model without the need for input-output examples.","sentences":["Visual Prompting is a technique for teaching models to perform a visual task via in-context examples, without any additional training.","In this work, we analyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find task vectors, activations that encode task-specific information.","Equipped with this insight, we demonstrate that it is possible to identify the task vectors and use them to guide the network towards performing different tasks without providing any input-output examples.","To find task vectors, we compute the average intermediate activations per task and use the REINFORCE algorithm to search for the subset of task vectors.","The resulting task vectors guide the model towards performing a task better than the original model without the need for input-output examples."],"url":"http://arxiv.org/abs/2404.05729v1","category":"cs.CV"}
{"created":"2024-04-08 16:56:05","title":"Normalizing Flows on the Product Space of SO(3) Manifolds for Probabilistic Human Pose Modeling","abstract":"Normalizing flows have proven their efficacy for density estimation in Euclidean space, but their application to rotational representations, crucial in various domains such as robotics or human pose modeling, remains underexplored. Probabilistic models of the human pose can benefit from approaches that rigorously consider the rotational nature of human joints. For this purpose, we introduce HuProSO3, a normalizing flow model that operates on a high-dimensional product space of SO(3) manifolds, modeling the joint distribution for human joints with three degrees of freedom. HuProSO3's advantage over state-of-the-art approaches is demonstrated through its superior modeling accuracy in three different applications and its capability to evaluate the exact likelihood. This work not only addresses the technical challenge of learning densities on SO(3) manifolds, but it also has broader implications for domains where the probabilistic regression of correlated 3D rotations is of importance.","sentences":["Normalizing flows have proven their efficacy for density estimation in Euclidean space, but their application to rotational representations, crucial in various domains such as robotics or human pose modeling, remains underexplored.","Probabilistic models of the human pose can benefit from approaches that rigorously consider the rotational nature of human joints.","For this purpose, we introduce HuProSO3, a normalizing flow model that operates on a high-dimensional product space of SO(3) manifolds, modeling the joint distribution for human joints with three degrees of freedom.","HuProSO3's advantage over state-of-the-art approaches is demonstrated through its superior modeling accuracy in three different applications and its capability to evaluate the exact likelihood.","This work not only addresses the technical challenge of learning densities on SO(3) manifolds, but it also has broader implications for domains where the probabilistic regression of correlated 3D rotations is of importance."],"url":"http://arxiv.org/abs/2404.05675v1","category":"cs.CV"}
{"created":"2024-04-08 16:28:02","title":"Size dependent solid-solid crystallization of halide perovskites","abstract":"The efficiency and stability of halide perovskite-based solar cells and light-emitting diodes directly depend on the intricate dynamics of solid-solid crystallization[1-23]. In this study, we employ a multi-scale approach using random phase approximation, density functional theory, machine learning potentials, reduced charge force fields, and both enhanced sampling biased and brute-force unbiased molecular dynamics simulations to understand the solid-solid phase transitions in cesium lead iodide perovskite. Our simulations uncover that the direct phase transition from the non-perovskite to the perovskite involves the formation of stacked-faulted and low-dimensional intermediate structures. Through extensive large-scale all-atom simulations encompassing up to 650,000 atoms, we observe that solid-solid crystallization may require the formation of a sufficiently large critical nucleus to grow into a faceted perovskite crystal. Based on simulations, we determine that utilizing (100)-faceted seeded crystallization could offer a promising path for manufacturing high-performance and stable perovskite solar cells.","sentences":["The efficiency and stability of halide perovskite-based solar cells and light-emitting diodes directly depend on the intricate dynamics of solid-solid crystallization[1-23].","In this study, we employ a multi-scale approach using random phase approximation, density functional theory, machine learning potentials, reduced charge force fields, and both enhanced sampling biased and brute-force unbiased molecular dynamics simulations to understand the solid-solid phase transitions in cesium lead iodide perovskite.","Our simulations uncover that the direct phase transition from the non-perovskite to the perovskite involves the formation of stacked-faulted and low-dimensional intermediate structures.","Through extensive large-scale all-atom simulations encompassing up to 650,000 atoms, we observe that solid-solid crystallization may require the formation of a sufficiently large critical nucleus to grow into a faceted perovskite crystal.","Based on simulations, we determine that utilizing (100)-faceted seeded crystallization could offer a promising path for manufacturing high-performance and stable perovskite solar cells."],"url":"http://arxiv.org/abs/2404.05644v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 15:51:21","title":"MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning","abstract":"While excellent in transfer learning, Vision-Language models (VLMs) come with high computational costs due to their large number of parameters. To address this issue, removing parameters via model pruning is a viable solution. However, existing techniques for VLMs are task-specific, and thus require pruning the network from scratch for each new task of interest. In this work, we explore a new direction: Task-Agnostic Vision-Language Pruning (TA-VLP). Given a pretrained VLM, the goal is to find a unique pruned counterpart transferable to multiple unknown downstream tasks. In this challenging setting, the transferable representations already encoded in the pretrained model are a key aspect to preserve. Thus, we propose Multimodal Flow Pruning (MULTIFLOW), a first, gradient-free, pruning framework for TA-VLP where: (i) the importance of a parameter is expressed in terms of its magnitude and its information flow, by incorporating the saliency of the neurons it connects; and (ii) pruning is driven by the emergent (multimodal) distribution of the VLM parameters after pretraining. We benchmark eight state-of-the-art pruning algorithms in the context of TA-VLP, experimenting with two VLMs, three vision-language tasks, and three pruning ratios. Our experimental results show that MULTIFLOW outperforms recent sophisticated, combinatorial competitors in the vast majority of the cases, paving the way towards addressing TA-VLP. The code is publicly available at https://github.com/FarinaMatteo/multiflow.","sentences":["While excellent in transfer learning, Vision-Language models (VLMs) come with high computational costs due to their large number of parameters.","To address this issue, removing parameters via model pruning is a viable solution.","However, existing techniques for VLMs are task-specific, and thus require pruning the network from scratch for each new task of interest.","In this work, we explore a new direction: Task-Agnostic Vision-Language Pruning (TA-VLP).","Given a pretrained VLM, the goal is to find a unique pruned counterpart transferable to multiple unknown downstream tasks.","In this challenging setting, the transferable representations already encoded in the pretrained model are a key aspect to preserve.","Thus, we propose Multimodal Flow Pruning (MULTIFLOW), a first, gradient-free, pruning framework for TA-VLP where: (i) the importance of a parameter is expressed in terms of its magnitude and its information flow, by incorporating the saliency of the neurons it connects; and (ii) pruning is driven by the emergent (multimodal) distribution of the VLM parameters after pretraining.","We benchmark eight state-of-the-art pruning algorithms in the context of TA-VLP, experimenting with two VLMs, three vision-language tasks, and three pruning ratios.","Our experimental results show that MULTIFLOW outperforms recent sophisticated, combinatorial competitors in the vast majority of the cases, paving the way towards addressing TA-VLP.","The code is publicly available at https://github.com/FarinaMatteo/multiflow."],"url":"http://arxiv.org/abs/2404.05621v1","category":"cs.CV"}
{"created":"2024-04-08 14:54:54","title":"Social-MAE: Social Masked Autoencoder for Multi-person Motion Representation Learning","abstract":"For a complete comprehension of multi-person scenes, it is essential to go beyond basic tasks like detection and tracking. Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial. Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks. To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data. The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable and data efficient representations of motion in human crowded scenes. Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints' trajectory in the frequency domain. After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks. Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding. These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose.","sentences":["For a complete comprehension of multi-person scenes, it is essential to go beyond basic tasks like detection and tracking.","Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial.","Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks.","To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data.","The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable and data efficient representations of motion in human crowded scenes.","Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints' trajectory in the frequency domain.","After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks.","Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding.","These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose."],"url":"http://arxiv.org/abs/2404.05578v1","category":"cs.CV"}
{"created":"2024-04-08 14:32:52","title":"Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language Model Pre-training","abstract":"Chinese sequence labeling tasks are heavily reliant on accurate word boundary demarcation. Although current pre-trained language models (PLMs) have achieved substantial gains on these tasks, they rarely explicitly incorporate boundary information into the modeling process. An exception to this is BABERT, which incorporates unsupervised statistical boundary information into Chinese BERT's pre-training objectives. Building upon this approach, we input supervised high-quality boundary information to enhance BABERT's learning, developing a semi-supervised boundary-aware PLM. To assess PLMs' ability to encode boundaries, we introduce a novel ``Boundary Information Metric'' that is both simple and effective. This metric allows comparison of different PLMs without task-specific fine-tuning. Experimental results on Chinese sequence labeling datasets demonstrate that the improved BABERT variant outperforms the vanilla version, not only on these tasks but also more broadly across a range of Chinese natural language understanding tasks. Additionally, our proposed metric offers a convenient and accurate means of evaluating PLMs' boundary awareness.","sentences":["Chinese sequence labeling tasks are heavily reliant on accurate word boundary demarcation.","Although current pre-trained language models (PLMs) have achieved substantial gains on these tasks, they rarely explicitly incorporate boundary information into the modeling process.","An exception to this is BABERT, which incorporates unsupervised statistical boundary information into Chinese BERT's pre-training objectives.","Building upon this approach, we input supervised high-quality boundary information to enhance BABERT's learning, developing a semi-supervised boundary-aware PLM.","To assess PLMs' ability to encode boundaries, we introduce a novel ``Boundary Information Metric'' that is both simple and effective.","This metric allows comparison of different PLMs without task-specific fine-tuning.","Experimental results on Chinese sequence labeling datasets demonstrate that the improved BABERT variant outperforms the vanilla version, not only on these tasks but also more broadly across a range of Chinese natural language understanding tasks.","Additionally, our proposed metric offers a convenient and accurate means of evaluating PLMs' boundary awareness."],"url":"http://arxiv.org/abs/2404.05560v1","category":"cs.CL"}
{"created":"2024-04-08 13:35:14","title":"Impact of LiDAR visualisations on semantic segmentation of archaeological objects","abstract":"Deep learning methods in LiDAR-based archaeological research often leverage visualisation techniques derived from Digital Elevation Models to enhance characteristics of archaeological objects present in the images. This paper investigates the impact of visualisations on deep learning performance through a comprehensive testing framework. The study involves the use of eight semantic segmentation models to evaluate seven diverse visualisations across two study areas, encompassing five archaeological classes. Experimental results reveal that the choice of appropriate visualisations can influence performance by up to 8%. Yet, pinpointing one visualisation that outperforms the others in segmenting all archaeological classes proves challenging. The observed performance variation, reaching up to 25% across different model configurations, underscores the importance of thoughtfully selecting model configurations and LiDAR visualisations for successfully segmenting archaeological objects.","sentences":["Deep learning methods in LiDAR-based archaeological research often leverage visualisation techniques derived from Digital Elevation Models to enhance characteristics of archaeological objects present in the images.","This paper investigates the impact of visualisations on deep learning performance through a comprehensive testing framework.","The study involves the use of eight semantic segmentation models to evaluate seven diverse visualisations across two study areas, encompassing five archaeological classes.","Experimental results reveal that the choice of appropriate visualisations can influence performance by up to 8%.","Yet, pinpointing one visualisation that outperforms the others in segmenting all archaeological classes proves challenging.","The observed performance variation, reaching up to 25% across different model configurations, underscores the importance of thoughtfully selecting model configurations and LiDAR visualisations for successfully segmenting archaeological objects."],"url":"http://arxiv.org/abs/2404.05512v1","category":"cs.CV"}
{"created":"2024-04-08 13:31:48","title":"Extending the Continuum of Six-Colorings","abstract":"We present two novel six-colorings of the Euclidean plane that avoid monochromatic pairs of points at unit distance in five colors and monochromatic pairs at another specified distance $d$ in the sixth color. Such colorings have previously been known to exist for $0.41 < \\sqrt{2} - 1 \\le d \\le 1 / \\sqrt{5} < 0.45$. Our results significantly expand that range to $0.354 \\le d \\le 0.657$, the first improvement in 30 years. Notably, the constructions underlying this were derived by formalizing colorings suggested by a custom machine learning approach.","sentences":["We present two novel six-colorings of the Euclidean plane that avoid monochromatic pairs of points at unit distance in five colors and monochromatic pairs at another specified distance $d$ in the sixth color.","Such colorings have previously been known to exist for $0.41 < \\sqrt{2} - 1 \\le d \\le 1 / \\sqrt{5} < 0.45$.","Our results significantly expand that range to $0.354 \\le d \\le 0.657$, the first improvement in 30 years.","Notably, the constructions underlying this were derived by formalizing colorings suggested by a custom machine learning approach."],"url":"http://arxiv.org/abs/2404.05509v1","category":"math.CO"}
{"created":"2024-04-08 12:38:26","title":"B-ary Tree Push-Pull Method is Provably Efficient for Decentralized Learning on Heterogeneous Data","abstract":"This paper considers the distributed learning problem where a group of agents cooperatively minimizes the summation of their local cost functions based on peer-to-peer communication. Particularly, we propose a highly efficient algorithm, termed ``B-ary Tree Push-Pull'' (BTPP), that employs two B-ary spanning trees for distributing the information related to the parameters and stochastic gradients across the network. The simple method is efficient in communication since each agent interacts with at most $(B+1)$ neighbors per iteration. More importantly, BTPP achieves linear speedup for smooth nonconvex objective functions with only $\\tilde{O}(n)$ transient iterations, significantly outperforming the state-of-the-art results to the best of our knowledge.","sentences":["This paper considers the distributed learning problem where a group of agents cooperatively minimizes the summation of their local cost functions based on peer-to-peer communication.","Particularly, we propose a highly efficient algorithm, termed ``B-ary Tree Push-Pull'' (BTPP), that employs two B-ary spanning trees for distributing the information related to the parameters and stochastic gradients across the network.","The simple method is efficient in communication since each agent interacts with at most $(B+1)$ neighbors per iteration.","More importantly, BTPP achieves linear speedup for smooth nonconvex objective functions with only $\\tilde{O}(n)$ transient iterations, significantly outperforming the state-of-the-art results to the best of our knowledge."],"url":"http://arxiv.org/abs/2404.05454v1","category":"math.OC"}
{"created":"2024-04-08 12:06:30","title":"Combinatorial Correlation Clustering","abstract":"Correlation Clustering is a classic clustering objective arising in numerous machine learning and data mining applications. Given a graph $G=(V,E)$, the goal is to partition the vertex set into clusters so as to minimize the number of edges between clusters plus the number of edges missing within clusters.   The problem is APX-hard and the best known polynomial time approximation factor is 1.73 by Cohen-Addad, Lee, Li, and Newman [FOCS'23]. They use an LP with $|V|^{1/\\epsilon^{\\Theta(1)}}$ variables for some small $\\epsilon$. However, due to the practical relevance of correlation clustering, there has also been great interest in getting more efficient sequential and parallel algorithms.   The classic combinatorial pivot algorithm of Ailon, Charikar and Newman [JACM'08] provides a 3-approximation in linear time. Like most other algorithms discussed here, this uses randomization. Recently, Behnezhad, Charikar, Ma and Tan [FOCS'22] presented a $3+\\epsilon$-approximate solution for solving problem in a constant number of rounds in the Massively Parallel Computation (MPC) setting. Very recently, Cao, Huang, Su [SODA'24] provided a 2.4-approximation in a polylogarithmic number of rounds in the MPC model and in $\\tilde{O}(|E|^{1.5})$ time in the classic sequential setting. They asked whether it is possible to get a better than 3-approximation in near-linear time?   We resolve this problem with an efficient combinatorial algorithm providing a drastically better approximation factor. It achieves a $\\sim 2-2/13 < 1.847$-approximation in sub-linear ($\\tilde O(|V|)$) sequential time or in sub-linear ($\\tilde O(|V|)$) space in the streaming setting, and it uses only a constant number of rounds in the MPC model.","sentences":["Correlation Clustering is a classic clustering objective arising in numerous machine learning and data mining applications.","Given a graph $G=(V,E)$, the goal is to partition the vertex set into clusters so as to minimize the number of edges between clusters plus the number of edges missing within clusters.   ","The problem is APX-hard and the best known polynomial time approximation factor is 1.73 by Cohen-Addad, Lee, Li, and Newman","[FOCS'23].","They use an LP with $|V|^{1/\\epsilon^{\\Theta(1)}}$ variables for some small $\\epsilon$. However, due to the practical relevance of correlation clustering, there has also been great interest in getting more efficient sequential and parallel algorithms.   ","The classic combinatorial pivot algorithm of Ailon, Charikar and Newman [JACM'08] provides a 3-approximation in linear time.","Like most other algorithms discussed here, this uses randomization.","Recently, Behnezhad, Charikar, Ma and Tan [FOCS'22] presented a $3+\\epsilon$-approximate solution for solving problem in a constant number of rounds in the Massively Parallel Computation (MPC) setting.","Very recently, Cao, Huang, Su","[SODA'24] provided a 2.4-approximation in a polylogarithmic number of rounds in the MPC model and in $\\tilde{O}(|E|^{1.5})$ time in the classic sequential setting.","They asked whether it is possible to get a better than 3-approximation in near-linear time?   ","We resolve this problem with an efficient combinatorial algorithm providing a drastically better approximation factor.","It achieves a $\\sim 2-2/13 < 1.847$-approximation in sub-linear ($\\tilde O(|V|)$) sequential time or in sub-linear ($\\tilde O(|V|)$) space in the streaming setting, and it uses only a constant number of rounds in the MPC model."],"url":"http://arxiv.org/abs/2404.05433v1","category":"cs.DS"}
{"created":"2024-04-08 08:35:50","title":"In-Flight Estimation of Instrument Spectral Response Functions Using Sparse Representations","abstract":"Accurate estimates of Instrument Spectral Response Functions (ISRFs) are crucial in order to have a good characterization of high resolution spectrometers. Spectrometers are composed of different optical elements that can induce errors in the measurements and therefore need to be modeled as accurately as possible. Parametric models are currently used to estimate these response functions. However, these models cannot always take into account the diversity of ISRF shapes that are encountered in practical applications. This paper studies a new ISRF estimation method based on a sparse representation of atoms belonging to a dictionary. This method is applied to different high-resolution spectrometers in order to assess its reproducibility for multiple remote sensing missions. The proposed method is shown to be very competitive when compared to the more commonly used parametric models, and yields normalized ISRF estimation errors less than 1%.","sentences":["Accurate estimates of Instrument Spectral Response Functions (ISRFs) are crucial in order to have a good characterization of high resolution spectrometers.","Spectrometers are composed of different optical elements that can induce errors in the measurements and therefore need to be modeled as accurately as possible.","Parametric models are currently used to estimate these response functions.","However, these models cannot always take into account the diversity of ISRF shapes that are encountered in practical applications.","This paper studies a new ISRF estimation method based on a sparse representation of atoms belonging to a dictionary.","This method is applied to different high-resolution spectrometers in order to assess its reproducibility for multiple remote sensing missions.","The proposed method is shown to be very competitive when compared to the more commonly used parametric models, and yields normalized ISRF estimation errors less than 1%."],"url":"http://arxiv.org/abs/2404.05298v1","category":"math.NA"}
{"created":"2024-04-08 08:13:40","title":"Multi-Task Learning for Features Extraction in Financial Annual Reports","abstract":"For assessing various performance indicators of companies, the focus is shifting from strictly financial (quantitative) publicly disclosed information to qualitative (textual) information. This textual data can provide valuable weak signals, for example through stylistic features, which can complement the quantitative data on financial performance or on Environmental, Social and Governance (ESG) criteria. In this work, we use various multi-task learning methods for financial text classification with the focus on financial sentiment, objectivity, forward-looking sentence prediction and ESG-content detection. We propose different methods to combine the information extracted from training jointly on different tasks; our best-performing method highlights the positive effect of explicitly adding auxiliary task predictions as features for the final target task during the multi-task training. Next, we use these classifiers to extract textual features from annual reports of FTSE350 companies and investigate the link between ESG quantitative scores and these features.","sentences":["For assessing various performance indicators of companies, the focus is shifting from strictly financial (quantitative) publicly disclosed information to qualitative (textual) information.","This textual data can provide valuable weak signals, for example through stylistic features, which can complement the quantitative data on financial performance or on Environmental, Social and Governance (ESG) criteria.","In this work, we use various multi-task learning methods for financial text classification with the focus on financial sentiment, objectivity, forward-looking sentence prediction and ESG-content detection.","We propose different methods to combine the information extracted from training jointly on different tasks; our best-performing method highlights the positive effect of explicitly adding auxiliary task predictions as features for the final target task during the multi-task training.","Next, we use these classifiers to extract textual features from annual reports of FTSE350 companies and investigate the link between ESG quantitative scores and these features."],"url":"http://arxiv.org/abs/2404.05281v1","category":"cs.CL"}
{"created":"2024-04-08 07:56:30","title":"When climate variables improve the dengue forecasting: a machine learning approach","abstract":"Dengue is a viral vector-borne infectious disease that affects many countries worldwide, infecting around 390 million people per year. The main outbreaks occur in subtropical and tropical countries. We study here the influence of climate on dengue in Natal (2016-2019), Brazil, Iquitos (2001-2012), Peru, and Barranquilla (2011-2016), Colombia. For the analysis and simulations, we apply Machine Learning (ML) techniques, especially the Random Forest (RF) algorithm. In addition, regarding a feature in the ML technique, we analyze three possibilities: only dengue cases (D); climate and dengue cases (CD); humidity and dengue cases (HD). Depending on the city, our results show that the climate data can improve or not the forecast. For instance, for Natal, D induces a better forecast. For Iquitos, it is better to use CD. Nonetheless, for Barranquilla, the forecast is better, when we include cases and humidity data. For Natal, when we use more than 64\\% and less than 80\\% of the time series for training, we obtain results with correlation coefficients ($r$) among 0.917 and 0.949 and mean absolute errors (MAE) among 57.783 and 71.768 for the D case in forecasting. The optimal range for Iquitos is obtained when 79\\% up to 88\\% of the time series is considered for training. For this case, the best case is CD, having a minimum $r$ equal to 0.850 and maximum 0.887, while values of MAE oscillate among 2.780 and 4.156. For Barranquilla, the optimal range occurs between 72\\% until 82\\% of length training. In this case, the better approach is HD, where the measures exhibit a minimum $r$ equal to 0.942 and a maximum 0.953, while the minimum and maximum MAE vary between 6.085 and 6.669. We show that the forecast of dengue cases is a challenging problem and climate variables do not always help. However, when we include the mentioned climate variables, the most important one is humidity.","sentences":["Dengue is a viral vector-borne infectious disease that affects many countries worldwide, infecting around 390 million people per year.","The main outbreaks occur in subtropical and tropical countries.","We study here the influence of climate on dengue in Natal (2016-2019), Brazil, Iquitos (2001-2012), Peru, and Barranquilla (2011-2016), Colombia.","For the analysis and simulations, we apply Machine Learning (ML) techniques, especially the Random Forest (RF) algorithm.","In addition, regarding a feature in the ML technique, we analyze three possibilities: only dengue cases (D); climate and dengue cases (CD); humidity and dengue cases (HD).","Depending on the city, our results show that the climate data can improve or not the forecast.","For instance, for Natal, D induces a better forecast.","For Iquitos, it is better to use CD.","Nonetheless, for Barranquilla, the forecast is better, when we include cases and humidity data.","For Natal, when we use more than 64\\% and less than 80\\% of the time series for training, we obtain results with correlation coefficients ($r$) among 0.917 and 0.949 and mean absolute errors (MAE) among 57.783 and 71.768 for the D case in forecasting.","The optimal range for Iquitos is obtained when 79\\% up to 88\\% of the time series is considered for training.","For this case, the best case is CD, having a minimum $r$ equal to 0.850 and maximum 0.887, while values of MAE oscillate among 2.780 and 4.156.","For Barranquilla, the optimal range occurs between 72\\% until 82\\% of length training.","In this case, the better approach is HD, where the measures exhibit a minimum $r$ equal to 0.942 and a maximum 0.953, while the minimum and maximum MAE vary between 6.085 and 6.669.","We show that the forecast of dengue cases is a challenging problem and climate variables do not always help.","However, when we include the mentioned climate variables, the most important one is humidity."],"url":"http://arxiv.org/abs/2404.05266v1","category":"physics.bio-ph"}
{"created":"2024-04-08 06:53:30","title":"PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection","abstract":"The vision-language model has brought great improvement to few-shot industrial anomaly detection, which usually needs to design of hundreds of prompts through prompt engineering. For automated scenarios, we first use conventional prompt learning with many-class paradigm as the baseline to automatically learn prompts but found that it can not work well in one-class anomaly detection. To address the above problem, this paper proposes a one-class prompt learning method for few-shot anomaly detection, termed PromptAD. First, we propose semantic concatenation which can transpose normal prompts into anomaly prompts by concatenating normal prompts with anomaly suffixes, thus constructing a large number of negative samples used to guide prompt learning in one-class setting. Furthermore, to mitigate the training challenge caused by the absence of anomaly images, we introduce the concept of explicit anomaly margin, which is used to explicitly control the margin between normal prompt features and anomaly prompt features through a hyper-parameter. For image-level/pixel-level anomaly detection, PromptAD achieves first place in 11/12 few-shot settings on MVTec and VisA.","sentences":["The vision-language model has brought great improvement to few-shot industrial anomaly detection, which usually needs to design of hundreds of prompts through prompt engineering.","For automated scenarios, we first use conventional prompt learning with many-class paradigm as the baseline to automatically learn prompts but found that it can not work well in one-class anomaly detection.","To address the above problem, this paper proposes a one-class prompt learning method for few-shot anomaly detection, termed PromptAD.","First, we propose semantic concatenation which can transpose normal prompts into anomaly prompts by concatenating normal prompts with anomaly suffixes, thus constructing a large number of negative samples used to guide prompt learning in one-class setting.","Furthermore, to mitigate the training challenge caused by the absence of anomaly images, we introduce the concept of explicit anomaly margin, which is used to explicitly control the margin between normal prompt features and anomaly prompt features through a hyper-parameter.","For image-level/pixel-level anomaly detection, PromptAD achieves first place in 11/12 few-shot settings on MVTec and VisA."],"url":"http://arxiv.org/abs/2404.05231v1","category":"cs.CV"}
{"created":"2024-04-08 06:49:59","title":"Empirical Upscaling of Point-scale Soil Moisture Measurements for Spatial Evaluation of Model Simulations and Satellite Retrievals","abstract":"The evaluation of modelled or satellite-derived soil moisture (SM) estimates is usually dependent on comparisons against in-situ SM measurements. However, the inherent mismatch in spatial support (i.e., scale) necessitates a cautious interpretation of point-to-pixel comparisons. The upscaling of the in-situ measurements to a commensurate resolution to that of the modelled or retrieved SM will lead to a fairer comparison and statistically more defensible evaluation. In this study, we presented an upscaling approach that combines spatiotemporal fusion with machine learning to extrapolate point-scale SM measurements from 28 in-situ sites to a 100 m resolution for an agricultural area of 100 km by 100 km. We conducted a four-fold cross-validation, which consistently demonstrated comparable correlation performance across folds, ranging from 0.6 to 0.9. The proposed approach was further validated based on a cross-cluster strategy by using two spatial subsets within the study area, denoted as cluster A and B, each of which equally comprised of 12 in-situ sites. The cross-cluster validation underscored the capability of the upscaling approach to map the spatial variability of SM within areas that were not covered by in-situ sites, with correlation performance ranging between 0.6 and 0.8. In general, our proposed upscaling approach offers an avenue to extrapolate point measurements of SM to a spatial scale more akin to climatic model grids or remotely sensed observations. Future investigations should delve into a further evaluation of the upscaling approach using independent data, such as model simulations, satellite retrievals or field campaign data.","sentences":["The evaluation of modelled or satellite-derived soil moisture (SM) estimates is usually dependent on comparisons against in-situ SM measurements.","However, the inherent mismatch in spatial support (i.e., scale) necessitates a cautious interpretation of point-to-pixel comparisons.","The upscaling of the in-situ measurements to a commensurate resolution to that of the modelled or retrieved SM will lead to a fairer comparison and statistically more defensible evaluation.","In this study, we presented an upscaling approach that combines spatiotemporal fusion with machine learning to extrapolate point-scale SM measurements from 28 in-situ sites to a 100 m resolution for an agricultural area of 100 km by 100 km.","We conducted a four-fold cross-validation, which consistently demonstrated comparable correlation performance across folds, ranging from 0.6 to 0.9.","The proposed approach was further validated based on a cross-cluster strategy by using two spatial subsets within the study area, denoted as cluster A and B, each of which equally comprised of 12 in-situ sites.","The cross-cluster validation underscored the capability of the upscaling approach to map the spatial variability of SM within areas that were not covered by in-situ sites, with correlation performance ranging between 0.6 and 0.8.","In general, our proposed upscaling approach offers an avenue to extrapolate point measurements of SM to a spatial scale more akin to climatic model grids or remotely sensed observations.","Future investigations should delve into a further evaluation of the upscaling approach using independent data, such as model simulations, satellite retrievals or field campaign data."],"url":"http://arxiv.org/abs/2404.05229v1","category":"cs.LG"}
{"created":"2024-04-08 05:45:03","title":"Bidirectional Long-Range Parser for Sequential Data Understanding","abstract":"The transformer is a powerful data modelling framework responsible for remarkable performance on a wide range of tasks. However, they are limited in terms of scalability as it is suboptimal and inefficient to process long-sequence data. To this purpose we introduce BLRP (Bidirectional Long-Range Parser), a novel and versatile attention mechanism designed to increase performance and efficiency on long-sequence tasks. It leverages short and long range heuristics in the form of a local sliding window approach combined with a global bidirectional latent space synthesis technique. We show the benefits and versatility of our approach on vision and language domains by demonstrating competitive results against state-of-the-art methods on the Long-Range-Arena and CIFAR benchmarks together with ablations demonstrating the computational efficiency.","sentences":["The transformer is a powerful data modelling framework responsible for remarkable performance on a wide range of tasks.","However, they are limited in terms of scalability as it is suboptimal and inefficient to process long-sequence data.","To this purpose we introduce BLRP (Bidirectional Long-Range Parser), a novel and versatile attention mechanism designed to increase performance and efficiency on long-sequence tasks.","It leverages short and long range heuristics in the form of a local sliding window approach combined with a global bidirectional latent space synthesis technique.","We show the benefits and versatility of our approach on vision and language domains by demonstrating competitive results against state-of-the-art methods on the Long-Range-Arena and CIFAR benchmarks together with ablations demonstrating the computational efficiency."],"url":"http://arxiv.org/abs/2404.05210v1","category":"cs.CV"}
{"created":"2024-04-08 01:14:09","title":"Image-based Agarwood Resinous Area Segmentation using Deep Learning","abstract":"The manual extraction method of Agarwood resinous compound is laborious work, requires skilled workers, and is subject to human errors. Commercial Agarwood industries have been actively exploring using Computer Numerical Control (CNC) machines to replace human effort for this particular task. The CNC machine accepts a G-code script produced from a binary image in which the wood region that needs to be chiselled off is marked with (0, 0, 0) as its RGB value. Rather than requiring a human expert to perform the region marking, we propose using a Deep learning image segmentation method instead. Our setup involves a camera that captures the cross-section image and then passes the image file to a computer. The computer performs the automated image segmentation and feeds the CNC machine with a G-code script. In this article, we report the initial segmentation results achieved using a state-of-the-art Deep learning segmentation method and discuss potential improvements to refine the segmentation accuracy.","sentences":["The manual extraction method of Agarwood resinous compound is laborious work, requires skilled workers, and is subject to human errors.","Commercial Agarwood industries have been actively exploring using Computer Numerical Control (CNC) machines to replace human effort for this particular task.","The CNC machine accepts a G-code script produced from a binary image in which the wood region that needs to be chiselled off is marked with (0, 0, 0) as its RGB value.","Rather than requiring a human expert to perform the region marking, we propose using a Deep learning image segmentation method instead.","Our setup involves a camera that captures the cross-section image and then passes the image file to a computer.","The computer performs the automated image segmentation and feeds the CNC machine with a G-code script.","In this article, we report the initial segmentation results achieved using a state-of-the-art Deep learning segmentation method and discuss potential improvements to refine the segmentation accuracy."],"url":"http://arxiv.org/abs/2404.05129v1","category":"cs.CV"}
{"created":"2024-04-07 23:34:51","title":"Efficient Gradient Estimation of Variational Quantum Circuits with Lie Algebraic Symmetries","abstract":"Hybrid quantum-classical optimization and learning strategies are among the most promising approaches to harnessing quantum information or gaining a quantum advantage over classical methods. However, efficient estimation of the gradient of the objective function in such models remains a challenge due to several factors including the exponential dimensionality of the Hilbert spaces, and information loss of quantum measurements. In this work, we study generic parameterized circuits in the context of variational methods. We develop a framework for gradient estimation that exploits the algebraic symmetries of Hamiltonian characterized through Lie algebra or group theory. Particularly, we prove that when the dimension of the dynamical Lie algebra is polynomial in the number of qubits, one can estimate the gradient with polynomial classical and quantum resources. This is done by a series of Hadamard tests applied to the output of the ansatz with no change to its circuit. We show that this approach can be equipped with classical shadow tomography to further reduce the measurement shot complexity to scale logarithmically with the number of parameters.","sentences":["Hybrid quantum-classical optimization and learning strategies are among the most promising approaches to harnessing quantum information or gaining a quantum advantage over classical methods.","However, efficient estimation of the gradient of the objective function in such models remains a challenge due to several factors including the exponential dimensionality of the Hilbert spaces, and information loss of quantum measurements.","In this work, we study generic parameterized circuits in the context of variational methods.","We develop a framework for gradient estimation that exploits the algebraic symmetries of Hamiltonian characterized through Lie algebra or group theory.","Particularly, we prove that when the dimension of the dynamical Lie algebra is polynomial in the number of qubits, one can estimate the gradient with polynomial classical and quantum resources.","This is done by a series of Hadamard tests applied to the output of the ansatz with no change to its circuit.","We show that this approach can be equipped with classical shadow tomography to further reduce the measurement shot complexity to scale logarithmically with the number of parameters."],"url":"http://arxiv.org/abs/2404.05108v1","category":"quant-ph"}
{"created":"2024-04-07 23:31:37","title":"Reconstructing Retinal Visual Images from 3T fMRI Data Enhanced by Unsupervised Learning","abstract":"The reconstruction of human visual inputs from brain activity, particularly through functional Magnetic Resonance Imaging (fMRI), holds promising avenues for unraveling the mechanisms of the human visual system. Despite the significant strides made by deep learning methods in improving the quality and interpretability of visual reconstruction, there remains a substantial demand for high-quality, long-duration, subject-specific 7-Tesla fMRI experiments. The challenge arises in integrating diverse smaller 3-Tesla datasets or accommodating new subjects with brief and low-quality fMRI scans. In response to these constraints, we propose a novel framework that generates enhanced 3T fMRI data through an unsupervised Generative Adversarial Network (GAN), leveraging unpaired training across two distinct fMRI datasets in 7T and 3T, respectively. This approach aims to overcome the limitations of the scarcity of high-quality 7-Tesla data and the challenges associated with brief and low-quality scans in 3-Tesla experiments. In this paper, we demonstrate the reconstruction capabilities of the enhanced 3T fMRI data, highlighting its proficiency in generating superior input visual images compared to data-intensive methods trained and tested on a single subject.","sentences":["The reconstruction of human visual inputs from brain activity, particularly through functional Magnetic Resonance Imaging (fMRI), holds promising avenues for unraveling the mechanisms of the human visual system.","Despite the significant strides made by deep learning methods in improving the quality and interpretability of visual reconstruction, there remains a substantial demand for high-quality, long-duration, subject-specific 7-Tesla fMRI experiments.","The challenge arises in integrating diverse smaller 3-Tesla datasets or accommodating new subjects with brief and low-quality fMRI scans.","In response to these constraints, we propose a novel framework that generates enhanced 3T fMRI data through an unsupervised Generative Adversarial Network (GAN), leveraging unpaired training across two distinct fMRI datasets in 7T and 3T, respectively.","This approach aims to overcome the limitations of the scarcity of high-quality 7-Tesla data and the challenges associated with brief and low-quality scans in 3-Tesla experiments.","In this paper, we demonstrate the reconstruction capabilities of the enhanced 3T fMRI data, highlighting its proficiency in generating superior input visual images compared to data-intensive methods trained and tested on a single subject."],"url":"http://arxiv.org/abs/2404.05107v1","category":"cs.CV"}
{"created":"2024-04-07 22:13:43","title":"SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts","abstract":"The advancement of deep learning has led to the emergence of Mixture-of-Experts (MoEs) models, known for their dynamic allocation of computational resources based on input. Despite their promise, MoEs face challenges, particularly in terms of memory requirements. To address this, our work introduces SEER-MoE, a novel two-stage framework for reducing both the memory footprint and compute requirements of pre-trained MoE models. The first stage involves pruning the total number of experts using a heavy-hitters counting guidance, while the second stage employs a regularization-based fine-tuning strategy to recover accuracy loss and reduce the number of activated experts during inference. Our empirical studies demonstrate the effectiveness of our method, resulting in a sparse MoEs model optimized for inference efficiency with minimal accuracy trade-offs.","sentences":["The advancement of deep learning has led to the emergence of Mixture-of-Experts (MoEs) models, known for their dynamic allocation of computational resources based on input.","Despite their promise, MoEs face challenges, particularly in terms of memory requirements.","To address this, our work introduces SEER-MoE, a novel two-stage framework for reducing both the memory footprint and compute requirements of pre-trained MoE models.","The first stage involves pruning the total number of experts using a heavy-hitters counting guidance, while the second stage employs a regularization-based fine-tuning strategy to recover accuracy loss and reduce the number of activated experts during inference.","Our empirical studies demonstrate the effectiveness of our method, resulting in a sparse MoEs model optimized for inference efficiency with minimal accuracy trade-offs."],"url":"http://arxiv.org/abs/2404.05089v1","category":"cs.CL"}
{"created":"2024-04-07 21:57:45","title":"Fork is All You Needed in Heterogeneous Systems","abstract":"We present a unified programming model for heterogeneous computing systems. Such systems integrate multiple computing accelerators and memory units to deliver higher performance than CPU-centric systems. Although heterogeneous systems have been adopted by modern workloads such as machine learning, programming remains a critical limiting factor. Conventional heterogeneous programming techniques either impose heavy modifications to the code base or require rewriting the program in a different language. Such programming complexity stems from the lack of a unified abstraction layer for computing and data exchange, which forces each programming model to define its abstractions. However, with the emerging cache-coherent interconnections such as Compute Express Link, we see an opportunity to standardize such architecture heterogeneity and provide a unified programming model. We present CodeFlow, a language runtime system for heterogeneous computing. CodeFlow abstracts architecture computation in programming language runtime and utilizes CXL as a unified data exchange protocol. Workloads written in high-level languages such as C++ and Rust can be compiled to CodeFlow, which schedules different parts of the workload to suitable accelerators without requiring the developer to implement code or call APIs for specific accelerators. CodeFlow reduces programmers' effort in utilizing heterogeneous systems and improves workload performance.","sentences":["We present a unified programming model for heterogeneous computing systems.","Such systems integrate multiple computing accelerators and memory units to deliver higher performance than CPU-centric systems.","Although heterogeneous systems have been adopted by modern workloads such as machine learning, programming remains a critical limiting factor.","Conventional heterogeneous programming techniques either impose heavy modifications to the code base or require rewriting the program in a different language.","Such programming complexity stems from the lack of a unified abstraction layer for computing and data exchange, which forces each programming model to define its abstractions.","However, with the emerging cache-coherent interconnections such as Compute Express Link, we see an opportunity to standardize such architecture heterogeneity and provide a unified programming model.","We present CodeFlow, a language runtime system for heterogeneous computing.","CodeFlow abstracts architecture computation in programming language runtime and utilizes CXL as a unified data exchange protocol.","Workloads written in high-level languages such as C++ and Rust can be compiled to CodeFlow, which schedules different parts of the workload to suitable accelerators without requiring the developer to implement code or call APIs for specific accelerators.","CodeFlow reduces programmers' effort in utilizing heterogeneous systems and improves workload performance."],"url":"http://arxiv.org/abs/2404.05085v1","category":"cs.ET"}
{"created":"2024-04-07 21:46:47","title":"HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models","abstract":"While recent progress in video-text retrieval has been driven by the exploration of powerful model architectures and training strategies, the representation learning ability of video-text retrieval models is still limited due to low-quality and scarce training data annotations. To address this issue, we present a novel video-text learning paradigm, HaVTR, which augments video and text data to learn more generalized features. Specifically, we first adopt a simple augmentation method, which generates self-similar data by randomly duplicating or dropping subwords and frames. In addition, inspired by the recent advancement in visual and language generative models, we propose a more powerful augmentation method through textual paraphrasing and video stylization using large language models (LLMs) and visual generative models (VGMs). Further, to bring richer information into video and text, we propose a hallucination-based augmentation method, where we use LLMs and VGMs to generate and add new relevant information to the original data. Benefiting from the enriched data, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of HaVTR over existing methods.","sentences":["While recent progress in video-text retrieval has been driven by the exploration of powerful model architectures and training strategies, the representation learning ability of video-text retrieval models is still limited due to low-quality and scarce training data annotations.","To address this issue, we present a novel video-text learning paradigm, HaVTR, which augments video and text data to learn more generalized features.","Specifically, we first adopt a simple augmentation method, which generates self-similar data by randomly duplicating or dropping subwords and frames.","In addition, inspired by the recent advancement in visual and language generative models, we propose a more powerful augmentation method through textual paraphrasing and video stylization using large language models (LLMs) and visual generative models (VGMs).","Further, to bring richer information into video and text, we propose a hallucination-based augmentation method, where we use LLMs and VGMs to generate and add new relevant information to the original data.","Benefiting from the enriched data, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of HaVTR over existing methods."],"url":"http://arxiv.org/abs/2404.05083v1","category":"cs.CV"}
{"created":"2024-04-07 20:16:37","title":"New methods for computing the generalized chi-square distribution","abstract":"We present several exact and approximate mathematical methods and open-source software to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution, which appears in Bayesian classification problems. Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index $d'$ between multinormals. We compare the accuracy and speed of these methods against the best existing methods.","sentences":["We present several exact and approximate mathematical methods and open-source software to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution, which appears in Bayesian classification problems.","Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index $d'$ between multinormals.","We compare the accuracy and speed of these methods against the best existing methods."],"url":"http://arxiv.org/abs/2404.05062v1","category":"stat.CO"}
{"created":"2024-04-07 19:13:11","title":"Co-design Accessible Public Robots: Insights from People with Mobility Disability, Robotic Practitioners and Their Collaborations","abstract":"Sidewalk robots are increasingly common across the globe. Yet, their operation on public paths poses challenges for people with mobility disabilities (PwMD) who face barriers to accessibility, such as insufficient curb cuts. We interviewed 15 PwMD to understand how they perceive sidewalk robots. Findings indicated that PwMD feel they have to compete for space on the sidewalk when robots are introduced. We next interviewed eight robotics practitioners to learn about their attitudes towards accessibility. Practitioners described how issues often stem from robotic companies addressing accessibility only after problems arise. Both interview groups underscored the importance of integrating accessibility from the outset. Building on this finding, we held four co-design workshops with PwMD and practitioners in pairs. These convenings brought to bear accessibility needs around robots operating in public spaces and in the public interest. Our study aims to set the stage for a more inclusive future around public service robots.","sentences":["Sidewalk robots are increasingly common across the globe.","Yet, their operation on public paths poses challenges for people with mobility disabilities (PwMD) who face barriers to accessibility, such as insufficient curb cuts.","We interviewed 15 PwMD to understand how they perceive sidewalk robots.","Findings indicated that PwMD feel they have to compete for space on the sidewalk when robots are introduced.","We next interviewed eight robotics practitioners to learn about their attitudes towards accessibility.","Practitioners described how issues often stem from robotic companies addressing accessibility only after problems arise.","Both interview groups underscored the importance of integrating accessibility from the outset.","Building on this finding, we held four co-design workshops with PwMD and practitioners in pairs.","These convenings brought to bear accessibility needs around robots operating in public spaces and in the public interest.","Our study aims to set the stage for a more inclusive future around public service robots."],"url":"http://arxiv.org/abs/2404.05050v1","category":"cs.HC"}
{"created":"2024-04-07 17:30:57","title":"Scalable and Efficient Hierarchical Visual Topological Mapping","abstract":"Hierarchical topological representations can significantly reduce search times within mapping and localization algorithms. Although recent research has shown the potential for such approaches, limited consideration has been given to the suitability and comparative performance of different global feature representations within this context. In this work, we evaluate state-of-the-art hand-crafted and learned global descriptors using a hierarchical topological mapping technique on benchmark datasets and present results of a comprehensive evaluation of the impact of the global descriptor used. Although learned descriptors have been incorporated into place recognition methods to improve retrieval accuracy and enhance overall recall, the problem of scalability and efficiency when applied to longer trajectories has not been adequately addressed in a majority of research studies. Based on our empirical analysis of multiple runs, we identify that continuity and distinctiveness are crucial characteristics for an optimal global descriptor that enable efficient and scalable hierarchical mapping, and present a methodology for quantifying and contrasting these characteristics across different global descriptors. Our study demonstrates that the use of global descriptors based on an unsupervised learned Variational Autoencoder (VAE) excels in these characteristics and achieves significantly lower runtime. It runs on a consumer grade desktop, up to 2.3x faster than the second best global descriptor, NetVLAD, and up to 9.5x faster than the hand-crafted descriptor, PHOG, on the longest track evaluated (St Lucia, 17.6 km), without sacrificing overall recall performance.","sentences":["Hierarchical topological representations can significantly reduce search times within mapping and localization algorithms.","Although recent research has shown the potential for such approaches, limited consideration has been given to the suitability and comparative performance of different global feature representations within this context.","In this work, we evaluate state-of-the-art hand-crafted and learned global descriptors using a hierarchical topological mapping technique on benchmark datasets and present results of a comprehensive evaluation of the impact of the global descriptor used.","Although learned descriptors have been incorporated into place recognition methods to improve retrieval accuracy and enhance overall recall, the problem of scalability and efficiency when applied to longer trajectories has not been adequately addressed in a majority of research studies.","Based on our empirical analysis of multiple runs, we identify that continuity and distinctiveness are crucial characteristics for an optimal global descriptor that enable efficient and scalable hierarchical mapping, and present a methodology for quantifying and contrasting these characteristics across different global descriptors.","Our study demonstrates that the use of global descriptors based on an unsupervised learned Variational Autoencoder (VAE) excels in these characteristics and achieves significantly lower runtime.","It runs on a consumer grade desktop, up to 2.3x faster than the second best global descriptor, NetVLAD, and up to 9.5x faster than the hand-crafted descriptor, PHOG, on the longest track evaluated (St Lucia, 17.6 km), without sacrificing overall recall performance."],"url":"http://arxiv.org/abs/2404.05023v1","category":"cs.CV"}
{"created":"2024-04-07 17:17:23","title":"Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts","abstract":"Expert parallelism has been introduced as a strategy to distribute the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple computing devices, facilitating the execution of these increasingly large-scale models. However, the All-to-All communication intrinsic to expert parallelism constitutes a significant overhead, diminishing the MoE models' efficiency. Current optimization approaches offer some relief, yet they are constrained by the sequential interdependence of communication and computation operations. To address this limitation, we present a novel shortcut-connected MoE architecture with overlapping parallel strategy, designated as ScMoE, which effectively decouples communication from its conventional sequence, allowing for a substantial overlap of 70% to 100% with computation. When compared with the prevalent top-2 MoE architecture, ScMoE demonstrates training speed improvements of 30% and 11%, and inference improvements of 40% and 15%, in our PCIe and NVLink hardware environments, respectively, where communication constitutes 60% and 15% of the total MoE time consumption. On the other hand, extensive experiments and theoretical analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches in vision and language tasks.","sentences":["Expert parallelism has been introduced as a strategy to distribute the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple computing devices, facilitating the execution of these increasingly large-scale models.","However, the All-to-All communication intrinsic to expert parallelism constitutes a significant overhead, diminishing the MoE models' efficiency.","Current optimization approaches offer some relief, yet they are constrained by the sequential interdependence of communication and computation operations.","To address this limitation, we present a novel shortcut-connected MoE architecture with overlapping parallel strategy, designated as ScMoE, which effectively decouples communication from its conventional sequence, allowing for a substantial overlap of 70% to 100% with computation.","When compared with the prevalent top-2 MoE architecture, ScMoE demonstrates training speed improvements of 30% and 11%, and inference improvements of 40% and 15%, in our PCIe and NVLink hardware environments, respectively, where communication constitutes 60% and 15% of the total MoE time consumption.","On the other hand, extensive experiments and theoretical analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches in vision and language tasks."],"url":"http://arxiv.org/abs/2404.05019v1","category":"cs.LG"}
{"created":"2024-04-07 16:16:30","title":"Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments","abstract":"Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using VR. Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings.","sentences":["Recall impairment in a different environmental context from learning is called context-dependent forgetting.","Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization.","Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory.","Conventionally, these methods have been validated by switching between physical rooms.","However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation.","Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE.","Furthermore, we are the first to address decontextualization using VR.","Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs.","Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs.","These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings."],"url":"http://arxiv.org/abs/2404.05007v1","category":"cs.HC"}
{"created":"2024-04-07 15:53:21","title":"Dual-Scale Transformer for Large-Scale Single-Pixel Imaging","abstract":"Single-pixel imaging (SPI) is a potential computational imaging technique which produces image by solving an illposed reconstruction problem from few measurements captured by a single-pixel detector. Deep learning has achieved impressive success on SPI reconstruction. However, previous poor reconstruction performance and impractical imaging model limit its real-world applications. In this paper, we propose a deep unfolding network with hybrid-attention Transformer on Kronecker SPI model, dubbed HATNet, to improve the imaging quality of real SPI cameras. Specifically, we unfold the computation graph of the iterative shrinkagethresholding algorithm (ISTA) into two alternative modules: efficient tensor gradient descent and hybrid-attention multiscale denoising. By virtue of Kronecker SPI, the gradient descent module can avoid high computational overheads rooted in previous gradient descent modules based on vectorized SPI. The denoising module is an encoder-decoder architecture powered by dual-scale spatial attention for high- and low-frequency aggregation and channel attention for global information recalibration. Moreover, we build a SPI prototype to verify the effectiveness of the proposed method. Extensive experiments on synthetic and real data demonstrate that our method achieves the state-of-the-art performance. The source code and pre-trained models are available at https://github.com/Gang-Qu/HATNet-SPI.","sentences":["Single-pixel imaging (SPI) is a potential computational imaging technique which produces image by solving an illposed reconstruction problem from few measurements captured by a single-pixel detector.","Deep learning has achieved impressive success on SPI reconstruction.","However, previous poor reconstruction performance and impractical imaging model limit its real-world applications.","In this paper, we propose a deep unfolding network with hybrid-attention Transformer on Kronecker SPI model, dubbed HATNet, to improve the imaging quality of real SPI cameras.","Specifically, we unfold the computation graph of the iterative shrinkagethresholding algorithm (ISTA) into two alternative modules: efficient tensor gradient descent and hybrid-attention multiscale denoising.","By virtue of Kronecker SPI, the gradient descent module can avoid high computational overheads rooted in previous gradient descent modules based on vectorized SPI.","The denoising module is an encoder-decoder architecture powered by dual-scale spatial attention for high- and low-frequency aggregation and channel attention for global information recalibration.","Moreover, we build a SPI prototype to verify the effectiveness of the proposed method.","Extensive experiments on synthetic and real data demonstrate that our method achieves the state-of-the-art performance.","The source code and pre-trained models are available at https://github.com/Gang-Qu/HATNet-SPI."],"url":"http://arxiv.org/abs/2404.05001v1","category":"cs.CV"}
{"created":"2024-04-07 15:32:57","title":"QArray: a GPU-accelerated constant capacitance model simulator for large quantum dot arrays","abstract":"Semiconductor quantum dot arrays are a leading architecture for the development of quantum technologies. Over the years, the constant capacitance model has served as a fundamental framework for simulating, understanding, and navigating the charge stability diagrams of small quantum dot arrays. However, while the size of the arrays keeps growing, solving the constant capacitance model becomes computationally prohibitive. This paper presents an open-source software package able to compute a $100 \\times 100$ pixels charge stability diagram of a 16-dot array in less than a second. Smaller arrays can be simulated in milliseconds - faster than they could be measured experimentally, enabling the creation of diverse datasets for training machine learning models and the creation of digital twins that can interface with quantum dot devices in real-time. Our software package implements its core functionalities in the systems programming language Rust and the high-performance numerical computing library JAX. The Rust implementation benefits from advanced optimisations and parallelisation, enabling the users to take full advantage of multi-core processors. The JAX implementation allows for GPU acceleration.","sentences":["Semiconductor quantum dot arrays are a leading architecture for the development of quantum technologies.","Over the years, the constant capacitance model has served as a fundamental framework for simulating, understanding, and navigating the charge stability diagrams of small quantum dot arrays.","However, while the size of the arrays keeps growing, solving the constant capacitance model becomes computationally prohibitive.","This paper presents an open-source software package able to compute a $100 \\times 100$ pixels charge stability diagram of a 16-dot array in less than a second.","Smaller arrays can be simulated in milliseconds - faster than they could be measured experimentally, enabling the creation of diverse datasets for training machine learning models and the creation of digital twins that can interface with quantum dot devices in real-time.","Our software package implements its core functionalities in the systems programming language Rust and the high-performance numerical computing library JAX.","The Rust implementation benefits from advanced optimisations and parallelisation, enabling the users to take full advantage of multi-core processors.","The JAX implementation allows for GPU acceleration."],"url":"http://arxiv.org/abs/2404.04994v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-07 15:27:35","title":"Efficient Surgical Tool Recognition via HMM-Stabilized Deep Learning","abstract":"Recognizing various surgical tools, actions and phases from surgery videos is an important problem in computer vision with exciting clinical applications. Existing deep-learning-based methods for this problem either process each surgical video as a series of independent images without considering their dependence, or rely on complicated deep learning models to count for dependence of video frames. In this study, we revealed from exploratory data analysis that surgical videos enjoy relatively simple semantic structure, where the presence of surgical phases and tools can be well modeled by a compact hidden Markov model (HMM). Based on this observation, we propose an HMM-stabilized deep learning method for tool presence detection. A wide range of experiments confirm that the proposed approaches achieve better performance with lower training and running costs, and support more flexible ways to construct and utilize training data in scenarios where not all surgery videos of interest are extensively labelled. These results suggest that popular deep learning approaches with over-complicated model structures may suffer from inefficient utilization of data, and integrating ingredients of deep learning and statistical learning wisely may lead to more powerful algorithms that enjoy competitive performance, transparent interpretation and convenient model training simultaneously.","sentences":["Recognizing various surgical tools, actions and phases from surgery videos is an important problem in computer vision with exciting clinical applications.","Existing deep-learning-based methods for this problem either process each surgical video as a series of independent images without considering their dependence, or rely on complicated deep learning models to count for dependence of video frames.","In this study, we revealed from exploratory data analysis that surgical videos enjoy relatively simple semantic structure, where the presence of surgical phases and tools can be well modeled by a compact hidden Markov model (HMM).","Based on this observation, we propose an HMM-stabilized deep learning method for tool presence detection.","A wide range of experiments confirm that the proposed approaches achieve better performance with lower training and running costs, and support more flexible ways to construct and utilize training data in scenarios where not all surgery videos of interest are extensively labelled.","These results suggest that popular deep learning approaches with over-complicated model structures may suffer from inefficient utilization of data, and integrating ingredients of deep learning and statistical learning wisely may lead to more powerful algorithms that enjoy competitive performance, transparent interpretation and convenient model training simultaneously."],"url":"http://arxiv.org/abs/2404.04992v1","category":"cs.CV"}
{"created":"2024-04-07 14:47:07","title":"CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference","abstract":"Social science research often hinges on the relationship between categorical variables and outcomes. We introduce CAVIAR, a novel method for embedding categorical variables that assume values in a high-dimensional ambient space but are sampled from an underlying manifold. Our theoretical and numerical analyses outline challenges posed by such categorical variables in causal inference. Specifically, dynamically varying and sparse levels can lead to violations of the Donsker conditions and a failure of the estimation functionals to converge to a tight Gaussian process. Traditional approaches, including the exclusion of rare categorical levels and principled variable selection models like LASSO, fall short. CAVIAR embeds the data into a lower-dimensional global coordinate system. The mapping can be derived from both structured and unstructured data, and ensures stable and robust estimates through dimensionality reduction. In a dataset of direct-to-consumer apparel sales, we illustrate how high-dimensional categorical variables, such as zip codes, can be succinctly represented, facilitating inference and analysis.","sentences":["Social science research often hinges on the relationship between categorical variables and outcomes.","We introduce CAVIAR, a novel method for embedding categorical variables that assume values in a high-dimensional ambient space but are sampled from an underlying manifold.","Our theoretical and numerical analyses outline challenges posed by such categorical variables in causal inference.","Specifically, dynamically varying and sparse levels can lead to violations of the Donsker conditions and a failure of the estimation functionals to converge to a tight Gaussian process.","Traditional approaches, including the exclusion of rare categorical levels and principled variable selection models like LASSO, fall short.","CAVIAR embeds the data into a lower-dimensional global coordinate system.","The mapping can be derived from both structured and unstructured data, and ensures stable and robust estimates through dimensionality reduction.","In a dataset of direct-to-consumer apparel sales, we illustrate how high-dimensional categorical variables, such as zip codes, can be succinctly represented, facilitating inference and analysis."],"url":"http://arxiv.org/abs/2404.04979v1","category":"econ.EM"}
{"created":"2024-04-07 13:17:47","title":"High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning","abstract":"Zero-shot learning(ZSL) aims to recognize new classes without prior exposure to their samples, relying on semantic knowledge from observed classes. However, current attention-based models may overlook the transferability of visual features and the distinctiveness of attribute localization when learning regional features in images. Additionally, they often overlook shared attributes among different objects. Highly discriminative attribute features are crucial for identifying and distinguishing unseen classes. To address these issues, we propose an innovative approach called High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning (HDAFL). HDAFL optimizes visual features by learning attribute features to obtain discriminative visual embeddings. Specifically, HDAFL utilizes multiple convolutional kernels to automatically learn discriminative regions highly correlated with attributes in images, eliminating irrelevant interference in image features. Furthermore, we introduce a Transformer-based attribute discrimination encoder to enhance the discriminative capability among attributes. Simultaneously, the method employs contrastive loss to alleviate dataset biases and enhance the transferability of visual features, facilitating better semantic transfer between seen and unseen classes. Experimental results demonstrate the effectiveness of HDAFL across three widely used datasets.","sentences":["Zero-shot learning(ZSL) aims to recognize new classes without prior exposure to their samples, relying on semantic knowledge from observed classes.","However, current attention-based models may overlook the transferability of visual features and the distinctiveness of attribute localization when learning regional features in images.","Additionally, they often overlook shared attributes among different objects.","Highly discriminative attribute features are crucial for identifying and distinguishing unseen classes.","To address these issues, we propose an innovative approach called High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning (HDAFL).","HDAFL optimizes visual features by learning attribute features to obtain discriminative visual embeddings.","Specifically, HDAFL utilizes multiple convolutional kernels to automatically learn discriminative regions highly correlated with attributes in images, eliminating irrelevant interference in image features.","Furthermore, we introduce a Transformer-based attribute discrimination encoder to enhance the discriminative capability among attributes.","Simultaneously, the method employs contrastive loss to alleviate dataset biases and enhance the transferability of visual features, facilitating better semantic transfer between seen and unseen classes.","Experimental results demonstrate the effectiveness of HDAFL across three widely used datasets."],"url":"http://arxiv.org/abs/2404.04953v1","category":"cs.CV"}
{"created":"2024-04-07 13:16:37","title":"The Impact of Virtual Laboratories on Active Learning and Engagement in Cybersecurity Distance Education","abstract":"Virtual Laboratories (V Labs) have in the recent past become part and parcel of remote teaching in practical hands-on approaches, particularly in Cybersecurity distance courses. Their potential is meant to assist learners with hands-on practical laboratory exercises irrespective of geographical location. Nevertheless, adopting V Labs in didactic approaches in higher education has seen both merits and demerits. Based on this premise, this study investigates the impact of V Labs on Active Learning (AL) and engagement in cybersecurity distance education. A survey with a limited number of learners and educators who have had an experience with cybersecurity distance courses that leveraged V Labs in their practical Lab assignment, was conducted at Blekinge Tekniska H\\\"ogskola, Sweden, to assess the impact of V Labs on AL and engagement in Cybersecurity Distance Education. 29% and 73% of the learners and educators, respectively responded to the survey administered remotely and with good internal consistency of questionnaires based on the Cronbalch Alpha; the results showed that learners and educators had a positive perception of using V Labs to enhance AL in cybersecurity distance education. The key concentration of the study was on AL and engagement and problem-solving abilities when V Labs are used. Both the learners and educators found the V Labs to be engaging, interactive, and effective in improving their understanding of cybersecurity concepts.","sentences":["Virtual Laboratories (V Labs) have in the recent past become part and parcel of remote teaching in practical hands-on approaches, particularly in Cybersecurity distance courses.","Their potential is meant to assist learners with hands-on practical laboratory exercises irrespective of geographical location.","Nevertheless, adopting V Labs in didactic approaches in higher education has seen both merits and demerits.","Based on this premise, this study investigates the impact of V Labs on Active Learning (AL) and engagement in cybersecurity distance education.","A survey with a limited number of learners and educators who have had an experience with cybersecurity distance courses that leveraged V Labs in their practical Lab assignment, was conducted at Blekinge Tekniska H\\\"ogskola, Sweden, to assess the impact of V Labs on AL and engagement in Cybersecurity Distance Education.","29% and 73% of the learners and educators, respectively responded to the survey administered remotely and with good internal consistency of questionnaires based on the Cronbalch Alpha; the results showed that learners and educators had a positive perception of using V Labs to enhance AL in cybersecurity distance education.","The key concentration of the study was on AL and engagement and problem-solving abilities when V Labs are used.","Both the learners and educators found the V Labs to be engaging, interactive, and effective in improving their understanding of cybersecurity concepts."],"url":"http://arxiv.org/abs/2404.04952v1","category":"cs.CY"}
{"created":"2024-04-07 12:25:03","title":"Fuzzy K-Means Clustering without Cluster Centroids","abstract":"Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy K-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership matrices solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.","sentences":["Fuzzy K-Means clustering is a critical technique in unsupervised data analysis.","However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids.","To address these challenges, this paper proposes a novel Fuzzy K-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership matrices solely through distance matrix computation.","This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness.","The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques.","Experimental results on several real datasets demonstrate the effectiveness of the algorithm."],"url":"http://arxiv.org/abs/2404.04940v1","category":"cs.LG"}
{"created":"2024-04-07 12:17:40","title":"Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models","abstract":"Radiologists highly desire fully automated versatile AI for medical imaging interpretation. However, the lack of extensively annotated large-scale multi-disease datasets has hindered the achievement of this goal. In this paper, we explore the feasibility of leveraging language as a naturally high-quality supervision for chest CT imaging. In light of the limited availability of image-report pairs, we bootstrap the understanding of 3D chest CT images by distilling chest-related diagnostic knowledge from an extensively pre-trained 2D X-ray expert model. Specifically, we propose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation knowledge distillation. Subsequently, we use contrastive learning to align images and reports within the same patient while distinguishing them from the other patients. However, the challenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives. We introduce a robust contrastive learning that identifies and corrects these false negatives. We train our model with over 12,000 pairs of chest CT images and radiology reports. Extensive experiments across multiple scenarios, including zero-shot learning, report generation, and fine-tuning processes, demonstrate the model's feasibility in interpreting chest CT images.","sentences":["Radiologists highly desire fully automated versatile AI for medical imaging interpretation.","However, the lack of extensively annotated large-scale multi-disease datasets has hindered the achievement of this goal.","In this paper, we explore the feasibility of leveraging language as a naturally high-quality supervision for chest CT imaging.","In light of the limited availability of image-report pairs, we bootstrap the understanding of 3D chest CT images by distilling chest-related diagnostic knowledge from an extensively pre-trained 2D X-ray expert model.","Specifically, we propose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation knowledge distillation.","Subsequently, we use contrastive learning to align images and reports within the same patient while distinguishing them from the other patients.","However, the challenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives.","We introduce a robust contrastive learning that identifies and corrects these false negatives.","We train our model with over 12,000 pairs of chest CT images and radiology reports.","Extensive experiments across multiple scenarios, including zero-shot learning, report generation, and fine-tuning processes, demonstrate the model's feasibility in interpreting chest CT images."],"url":"http://arxiv.org/abs/2404.04936v1","category":"cs.CV"}
{"created":"2024-04-07 12:15:53","title":"Anomaly Detection in Electrocardiograms: Advancing Clinical Diagnosis Through Self-Supervised Learning","abstract":"The electrocardiogram (ECG) is an essential tool for diagnosing heart disease, with computer-aided systems improving diagnostic accuracy and reducing healthcare costs. Despite advancements, existing systems often miss rare cardiac anomalies that could be precursors to serious, life-threatening issues or alterations in the cardiac macro/microstructure. We address this gap by focusing on self-supervised anomaly detection (AD), training exclusively on normal ECGs to recognize deviations indicating anomalies. We introduce a novel self-supervised learning framework for ECG AD, utilizing a vast dataset of normal ECGs to autonomously detect and localize cardiac anomalies. It proposes a novel masking and restoration technique alongside a multi-scale cross-attention module, enhancing the model's ability to integrate global and local signal features. The framework emphasizes accurate localization of anomalies within ECG signals, ensuring the method's clinical relevance and reliability. To reduce the impact of individual variability, the approach further incorporates crucial patient-specific information from ECG reports, such as age and gender, thus enabling accurate identification of a broad spectrum of cardiac anomalies, including rare ones. Utilizing an extensive dataset of 478,803 ECG graphic reports from real-world clinical practice, our method has demonstrated exceptional effectiveness in AD across all tested conditions, regardless of their frequency of occurrence, significantly outperforming existing models. It achieved superior performance metrics, including an AUROC of 91.2%, an F1 score of 83.7%, a sensitivity rate of 84.2%, a specificity of 83.0%, and a precision of 75.6% with a fixed recall rate of 90%. It has also demonstrated robust localization capabilities, with an AUROC of 76.5% and a Dice coefficient of 65.3% for anomaly localization.","sentences":["The electrocardiogram (ECG) is an essential tool for diagnosing heart disease, with computer-aided systems improving diagnostic accuracy and reducing healthcare costs.","Despite advancements, existing systems often miss rare cardiac anomalies that could be precursors to serious, life-threatening issues or alterations in the cardiac macro/microstructure.","We address this gap by focusing on self-supervised anomaly detection (AD), training exclusively on normal ECGs to recognize deviations indicating anomalies.","We introduce a novel self-supervised learning framework for ECG AD, utilizing a vast dataset of normal ECGs to autonomously detect and localize cardiac anomalies.","It proposes a novel masking and restoration technique alongside a multi-scale cross-attention module, enhancing the model's ability to integrate global and local signal features.","The framework emphasizes accurate localization of anomalies within ECG signals, ensuring the method's clinical relevance and reliability.","To reduce the impact of individual variability, the approach further incorporates crucial patient-specific information from ECG reports, such as age and gender, thus enabling accurate identification of a broad spectrum of cardiac anomalies, including rare ones.","Utilizing an extensive dataset of 478,803 ECG graphic reports from real-world clinical practice, our method has demonstrated exceptional effectiveness in AD across all tested conditions, regardless of their frequency of occurrence, significantly outperforming existing models.","It achieved superior performance metrics, including an AUROC of 91.2%, an F1 score of 83.7%, a sensitivity rate of 84.2%, a specificity of 83.0%, and a precision of 75.6% with a fixed recall rate of 90%.","It has also demonstrated robust localization capabilities, with an AUROC of 76.5% and a Dice coefficient of 65.3% for anomaly localization."],"url":"http://arxiv.org/abs/2404.04935v1","category":"cs.CV"}
{"created":"2024-04-08 17:31:59","title":"Boundary regularity of the free interface in spectral optimal partition problems","abstract":"We consider the problem of optimal partition of a domain with respect to the sum of the principal eigenvalues and we prove for the first time regularity results for the free interface up to fixed boundary. All our results are quantitative and, in particular, we obtain fine estimates on the continuity of the solutions and the oscillation of the free interface (in terms of the modulus of continuity of the normal vector of the fixed boundary), even in the case of domains with low (Dini-type) regularity. Our analysis is based on an Almgren-type monotonicity formula at boundary points and an epiperimetric inequality at points of low frequency, which, together, yield an explicit rate of convergence for blow-up sequences and the boundary strong unique continuation property. Exploiting our quantitative blow-up analysis, we manage to prove clean-up results near one-phase and two-phase points. We define the notion of free interface inside the fixed boundary, and we prove that the subset of points of minimal frequency is regular and that the interior free interface is approaching the boundary orthogonally in a smooth way.","sentences":["We consider the problem of optimal partition of a domain with respect to the sum of the principal eigenvalues and we prove for the first time regularity results for the free interface up to fixed boundary.","All our results are quantitative and, in particular, we obtain fine estimates on the continuity of the solutions and the oscillation of the free interface (in terms of the modulus of continuity of the normal vector of the fixed boundary), even in the case of domains with low (Dini-type) regularity.","Our analysis is based on an Almgren-type monotonicity formula at boundary points and an epiperimetric inequality at points of low frequency, which, together, yield an explicit rate of convergence for blow-up sequences and the boundary strong unique continuation property.","Exploiting our quantitative blow-up analysis, we manage to prove clean-up results near one-phase and two-phase points.","We define the notion of free interface inside the fixed boundary, and we prove that the subset of points of minimal frequency is regular and that the interior free interface is approaching the boundary orthogonally in a smooth way."],"url":"http://arxiv.org/abs/2404.05698v1","category":"math.AP"}
{"created":"2024-04-08 16:59:51","title":"From enrollment to exams: Perceived stress dynamics among first-year physics students","abstract":"The current dropout rate in physics studies in Germany is about 60%, with the majority of dropouts occurring in the first year. Consequently, the physics study entry phase poses a significant challenge for many students. Students' stress perception can provide more profound insights into the processes and challenges during that period. In a panel study featuring 67 measuring points involving up to 128 participants at each point, we investigated the students' stress perception with the Perceived Stress Questionnaire (PSQ), identified underlying sources of stress, and assessed the self-estimated workload across two different cohorts. This examination occurred mostly every week during the first, and for one cohort also in the second semester, yielding a total of 3,206 PSQ data points and 5,823 stressors. The PSQ data indicate a consistent stress trajectory across all three groups studied that is characterized by significant dynamics between measuring points, spanning from $M=20.1, SD=15.9$ to $M=63.6, SD=13.4$ on the scale from 0 to 100. The stress level rises in the first lecture weeks, followed by a stable, elevated stress level until the exams and a relaxation phase afterward during the lecture-free time and Christmas vacation. In the first half of the lecture period, students primarily indicate the weekly exercise sheets, the physics lab course, and math courses as stressors; later on, preparation for exams and the exams themselves emerge as the most important stressors. Together with the students' self-estimated workload that correlates with the PSQ score, we can depict a coherent picture of stress perception among first-year physics students. This study enhances the understanding of stress perception and its potential management.","sentences":["The current dropout rate in physics studies in Germany is about 60%, with the majority of dropouts occurring in the first year.","Consequently, the physics study entry phase poses a significant challenge for many students.","Students' stress perception can provide more profound insights into the processes and challenges during that period.","In a panel study featuring 67 measuring points involving up to 128 participants at each point, we investigated the students' stress perception with the Perceived Stress Questionnaire (PSQ), identified underlying sources of stress, and assessed the self-estimated workload across two different cohorts.","This examination occurred mostly every week during the first, and for one cohort also in the second semester, yielding a total of 3,206 PSQ data points and 5,823 stressors.","The PSQ data indicate a consistent stress trajectory across all three groups studied that is characterized by significant dynamics between measuring points, spanning from $M=20.1, SD=15.9$ to $M=63.6, SD=13.4$ on the scale from 0 to 100.","The stress level rises in the first lecture weeks, followed by a stable, elevated stress level until the exams and a relaxation phase afterward during the lecture-free time and Christmas vacation.","In the first half of the lecture period, students primarily indicate the weekly exercise sheets, the physics lab course, and math courses as stressors; later on, preparation for exams and the exams themselves emerge as the most important stressors.","Together with the students' self-estimated workload that correlates with the PSQ score, we can depict a coherent picture of stress perception among first-year physics students.","This study enhances the understanding of stress perception and its potential management."],"url":"http://arxiv.org/abs/2404.05682v1","category":"physics.ed-ph"}
{"created":"2024-04-08 16:46:47","title":"Significant Photoluminescence Improvements from Bulk Germanium-Based Thin Films with Ultra-low Threading Dislocation Densities","abstract":"Bulk Ge crystals, characterized by significantly lower threading dislocation densities (TDD) than their epitaxial counterparts, emerge as optimal candidates for studying and improving Ge laser performance. Our study focused on the Ge thickness and TDD impacts on Ge's photoluminescence (PL). The PL peak intensity of a bulk Ge sample (TDD = 6000 cm^(-2), n-doping = 10^16 cm^(-3)) experiences a remarkable 32-fold increase as the thickness is reduced from 535 to 2 micron. This surpasses the PL peak intensity of a 0.75 micron thick epi-Ge on Si (biaxial tensile strain= 0.2 %, n-doping = 10^19 cm^(-3)) by a factor of 2.5. Furthermore, the PL peak intensity of a 405 micron thick zero-TDD bulk Ge sample (n-doping = 5 * 10^17 cm^(-3)) is ten times that of the 0.75 micron thick epi-Ge, rising to twelve times when thinned to 1 micron. The TDD reduction method is highly effective, which relaxes the requirements of high n-doping and stress in enhancing Ge laser performance and thus reduces the side effects of high optical absorption, high non-radiative recombination, bandgap narrowing, and large footprints associated with these two techniques.","sentences":["Bulk Ge crystals, characterized by significantly lower threading dislocation densities (TDD) than their epitaxial counterparts, emerge as optimal candidates for studying and improving Ge laser performance.","Our study focused on the Ge thickness and TDD impacts on Ge's photoluminescence (PL).","The PL peak intensity of a bulk Ge sample (TDD = 6000 cm^(-2), n-doping = 10^16 cm^(-3)) experiences a remarkable 32-fold increase as the thickness is reduced from 535 to 2 micron.","This surpasses the PL peak intensity of a 0.75 micron thick epi-Ge on Si (biaxial tensile strain= 0.2 %, n-doping = 10^19 cm^(-3))","by a factor of 2.5.","Furthermore, the PL peak intensity of a 405 micron thick zero-TDD bulk Ge sample (n-doping = 5 * 10^17 cm^(-3)) is ten times that of the 0.75 micron thick epi-Ge, rising to twelve times when thinned to 1 micron.","The TDD reduction method is highly effective, which relaxes the requirements of high n-doping and stress in enhancing Ge laser performance and thus reduces the side effects of high optical absorption, high non-radiative recombination, bandgap narrowing, and large footprints associated with these two techniques."],"url":"http://arxiv.org/abs/2404.05663v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 14:39:36","title":"Hausdorff Distance-Based Record Linkage for Improved Matching of Households and Individuals in Different Databases","abstract":"Matching households and individuals across different databases poses challenges due to the lack of unique identifiers, typographical errors, and changes in attributes over time. Record linkage tools play a crucial role in overcoming these difficulties. This paper presents a multi-step record linkage procedure that incorporates household information to enhance the entity-matching process across multiple databases. Our approach utilizes the Hausdorff distance to estimate the probability of a match between households in multiple files. Subsequently, probabilities of matching individuals within these households are computed using a logistic regression model based on attribute-level distances. These estimated probabilities are then employed in a linear programming optimization framework to infer one-to-one matches between individuals. To assess the efficacy of our method, we apply it to link data from the Italian Survey of Household Income and Wealth across different years. Through internal and external validation procedures, the proposed method is shown to provide a significant enhancement in the quality of the individual matching process, thanks to the incorporation of household information. A comparison with a standard record linkage approach based on direct matching of individuals, which neglects household information, underscores the advantages of accounting for such information.","sentences":["Matching households and individuals across different databases poses challenges due to the lack of unique identifiers, typographical errors, and changes in attributes over time.","Record linkage tools play a crucial role in overcoming these difficulties.","This paper presents a multi-step record linkage procedure that incorporates household information to enhance the entity-matching process across multiple databases.","Our approach utilizes the Hausdorff distance to estimate the probability of a match between households in multiple files.","Subsequently, probabilities of matching individuals within these households are computed using a logistic regression model based on attribute-level distances.","These estimated probabilities are then employed in a linear programming optimization framework to infer one-to-one matches between individuals.","To assess the efficacy of our method, we apply it to link data from the Italian Survey of Household Income and Wealth across different years.","Through internal and external validation procedures, the proposed method is shown to provide a significant enhancement in the quality of the individual matching process, thanks to the incorporation of household information.","A comparison with a standard record linkage approach based on direct matching of individuals, which neglects household information, underscores the advantages of accounting for such information."],"url":"http://arxiv.org/abs/2404.05566v1","category":"stat.AP"}
{"created":"2024-04-08 14:35:09","title":"Local analysis of the Kuznetsov formula and the density conjecture","abstract":"We prove Sarnak's spherical density conjecture for the principal congruence subgroup of SL(n, Z) of arbitrary level. Applications include a complete version of Sarnak's optimal lifting conjecture for principal congruence subgroups of SL(n, Z), as well as a transfer of the density theorem to certain co-compact situations. The main ingredients are new lower bounds for Whittaker functions and strong estimates for the cardinality of ramified Kloosterman sets.","sentences":["We prove Sarnak's spherical density conjecture for the principal congruence subgroup of SL(n, Z) of arbitrary level.","Applications include a complete version of Sarnak's optimal lifting conjecture for principal congruence subgroups of SL(n, Z), as well as a transfer of the density theorem to certain co-compact situations.","The main ingredients are new lower bounds for Whittaker functions and strong estimates for the cardinality of ramified Kloosterman sets."],"url":"http://arxiv.org/abs/2404.05561v1","category":"math.NT"}
{"created":"2024-04-08 14:16:05","title":"Information Sale on Network","abstract":"This paper studies a stylized model of a monopoly data seller when information-sharing network exists among data buyers. We show that, if the buyers' prior information is sufficiently noisy, the optimal selling strategy is characterized by a maximum independent set, which is the largest set of buyers who do not have information-sharing link at all. In addition, the precision of the seller's data decreases in the number of information-sharing links among buyers, but it is higher than the socially efficient level of precision.","sentences":["This paper studies a stylized model of a monopoly data seller when information-sharing network exists among data buyers.","We show that, if the buyers' prior information is sufficiently noisy, the optimal selling strategy is characterized by a maximum independent set, which is the largest set of buyers who do not have information-sharing link at all.","In addition, the precision of the seller's data decreases in the number of information-sharing links among buyers, but it is higher than the socially efficient level of precision."],"url":"http://arxiv.org/abs/2404.05546v1","category":"econ.TH"}
{"created":"2024-04-08 12:39:35","title":"On Optimal Transport Maps Between 1 /d-Concave Densities","abstract":"In this paper, we extend the scope of Caffarelli's contraction theorem, which provides a measure of the Lipschitz constant for optimal transport maps between log-concave probability densities in $\\R^d$. Our focus is on a broader category of densities, specifically those that are $\\nicefrac{1}{d}$-concave and can be represented as $V^{-d}$, where $V$ is convex. By setting appropriate conditions, we derive linear or sublinear limitations for the optimal transport map. This leads us to a comprehensive Lipschitz estimate that aligns with the principles established in Caffarelli's theorem.","sentences":["In this paper, we extend the scope of Caffarelli's contraction theorem, which provides a measure of the Lipschitz constant for optimal transport maps between log-concave probability densities in $\\R^d$. Our focus is on a broader category of densities, specifically those that are $\\nicefrac{1}{d}$-concave and can be represented as $V^{-d}$, where $V$ is convex.","By setting appropriate conditions, we derive linear or sublinear limitations for the optimal transport map.","This leads us to a comprehensive Lipschitz estimate that aligns with the principles established in Caffarelli's theorem."],"url":"http://arxiv.org/abs/2404.05456v1","category":"math.AP"}
{"created":"2024-04-08 12:30:07","title":"Efficient Encodings of the Travelling Salesperson Problem for Variational Quantum Algorithms","abstract":"Routing problems are a common optimization problem in industrial applications, which occur on a large scale in supply chain planning. Due to classical limitations for solving NP-hard problems, quantum computing hopes to improve upon speed or solution quality. Several suggestions have been made for encodings of routing problems to solve them with variational quantum algorithms. However, for an end user it is hard to decide a priori which encoding will give the best solutions according to their needs. In this work, we investigate different encodings for the Travelling Salesperson Problem. We compare their scaling and performance when using the Quantum Approximate Optimization Algorithm and the Variational Quantum Eigensolver and provide a clear guide for users when to choose which encoding. For small instances, we find evidence that the permutation encoding can yield good results since it does not suffer from feasibility issues.","sentences":["Routing problems are a common optimization problem in industrial applications, which occur on a large scale in supply chain planning.","Due to classical limitations for solving NP-hard problems, quantum computing hopes to improve upon speed or solution quality.","Several suggestions have been made for encodings of routing problems to solve them with variational quantum algorithms.","However, for an end user it is hard to decide a priori which encoding will give the best solutions according to their needs.","In this work, we investigate different encodings for the Travelling Salesperson Problem.","We compare their scaling and performance when using the Quantum Approximate Optimization Algorithm and the Variational Quantum Eigensolver and provide a clear guide for users when to choose which encoding.","For small instances, we find evidence that the permutation encoding can yield good results since it does not suffer from feasibility issues."],"url":"http://arxiv.org/abs/2404.05448v1","category":"quant-ph"}
{"created":"2024-04-08 09:10:02","title":"Reflected Search Poisoning for Illicit Promotion","abstract":"As an emerging black hat search engine optimization (SEO) technique, reflected search poisoning (RSP) allows a miscreant to free-ride the reputation of high-ranking websites, poisoning search engines with illicit promotion texts (IPTs) in an efficient and stealthy manner, while avoiding the burden of continuous website compromise as required by traditional promotion infections. However, little is known about the security implications of RSP, e.g., what illicit promotion campaigns are being distributed by RSP, and to what extent regular search users can be exposed to illicit promotion texts distributed by RSP. In this study, we conduct the first security study on RSP-based illicit promotion, which is made possible through an end-to-end methodology for capturing, analyzing, and infiltrating IPTs. As a result, IPTs distributed via RSP are found to be large-scale, continuously growing, and diverse in both illicit categories and natural languages. Particularly, we have identified over 11 million distinct IPTs belonging to 14 different illicit categories, with typical examples including drug trading, data theft, counterfeit goods, and hacking services. Also, the underlying RSP cases have abused tens of thousands of high-ranking websites, as well as extensively poisoning all four popular search engines we studied, especially Google Search and Bing. Furthermore, it is observed that benign search users are being exposed to IPTs at a concerning extent. To facilitate interaction with potential customers (victim search users), miscreants tend to embed various types of contacts in IPTs, especially instant messaging accounts. Further infiltration of these IPT contacts reveals that the underlying illicit campaigns are operated on a large scale. All these findings highlight the negative security implications of IPTs and RSPs, and thus call for more efforts to mitigate RSP-driven illicit promotion.","sentences":["As an emerging black hat search engine optimization (SEO) technique, reflected search poisoning (RSP) allows a miscreant to free-ride the reputation of high-ranking websites, poisoning search engines with illicit promotion texts (IPTs) in an efficient and stealthy manner, while avoiding the burden of continuous website compromise as required by traditional promotion infections.","However, little is known about the security implications of RSP, e.g., what illicit promotion campaigns are being distributed by RSP, and to what extent regular search users can be exposed to illicit promotion texts distributed by RSP.","In this study, we conduct the first security study on RSP-based illicit promotion, which is made possible through an end-to-end methodology for capturing, analyzing, and infiltrating IPTs.","As a result, IPTs distributed via RSP are found to be large-scale, continuously growing, and diverse in both illicit categories and natural languages.","Particularly, we have identified over 11 million distinct IPTs belonging to 14 different illicit categories, with typical examples including drug trading, data theft, counterfeit goods, and hacking services.","Also, the underlying RSP cases have abused tens of thousands of high-ranking websites, as well as extensively poisoning all four popular search engines we studied, especially Google Search and Bing.","Furthermore, it is observed that benign search users are being exposed to IPTs at a concerning extent.","To facilitate interaction with potential customers (victim search users), miscreants tend to embed various types of contacts in IPTs, especially instant messaging accounts.","Further infiltration of these IPT contacts reveals that the underlying illicit campaigns are operated on a large scale.","All these findings highlight the negative security implications of IPTs and RSPs, and thus call for more efforts to mitigate RSP-driven illicit promotion."],"url":"http://arxiv.org/abs/2404.05320v1","category":"cs.CR"}
{"created":"2024-04-08 09:04:40","title":"A measure for the stability of structures immersed in a 2D laminar flow","abstract":"We introduce a new measure for the stability of structures, such as the cross-section of the deck of a suspension bridge, subject to a 2D fluid force, such as the lift exerted by a laminar wind. We consider a wide class of possible flows, as well as a wide class of structural shapes. Within a suitable topological framework, we prove the existence of an optimal shape maximizing the stability. Applications to engineering problems are also discussed.","sentences":["We introduce a new measure for the stability of structures, such as the cross-section of the deck of a suspension bridge, subject to a 2D fluid force, such as the lift exerted by a laminar wind.","We consider a wide class of possible flows, as well as a wide class of structural shapes.","Within a suitable topological framework, we prove the existence of an optimal shape maximizing the stability.","Applications to engineering problems are also discussed."],"url":"http://arxiv.org/abs/2404.05314v1","category":"math.AP"}
{"created":"2024-04-08 07:45:10","title":"Sensing-Resistance-Oriented Beamforming for Privacy Protection from ISAC Devices","abstract":"With the evolution of integrated sensing and communication (ISAC) technology, a growing number of devices go beyond conventional communication functions with sensing abilities. Therefore, future networks are divinable to encounter new privacy concerns on sensing, such as the exposure of position information to unintended receivers. In contrast to traditional privacy preserving schemes aiming to prevent eavesdropping, this contribution conceives a novel beamforming design toward sensing resistance (SR). Specifically, we expect to guarantee the communication quality while masking the real direction of the SR transmitter during the communication. To evaluate the SR performance, a metric termed angular-domain peak-to-average ratio (ADPAR) is first defined and analyzed. Then, we resort to the null-space technique to conceal the real direction, hence to convert the optimization problem to a more tractable form. Moreover, semidefinite relaxation along with index optimization is further utilized to obtain the optimal beamformer. Finally, simulation results demonstrate the feasibility of the proposed SR-oriented beamforming design toward privacy protection from ISAC receivers.","sentences":["With the evolution of integrated sensing and communication (ISAC) technology, a growing number of devices go beyond conventional communication functions with sensing abilities.","Therefore, future networks are divinable to encounter new privacy concerns on sensing, such as the exposure of position information to unintended receivers.","In contrast to traditional privacy preserving schemes aiming to prevent eavesdropping, this contribution conceives a novel beamforming design toward sensing resistance (SR).","Specifically, we expect to guarantee the communication quality while masking the real direction of the SR transmitter during the communication.","To evaluate the SR performance, a metric termed angular-domain peak-to-average ratio (ADPAR) is first defined and analyzed.","Then, we resort to the null-space technique to conceal the real direction, hence to convert the optimization problem to a more tractable form.","Moreover, semidefinite relaxation along with index optimization is further utilized to obtain the optimal beamformer.","Finally, simulation results demonstrate the feasibility of the proposed SR-oriented beamforming design toward privacy protection from ISAC receivers."],"url":"http://arxiv.org/abs/2404.05257v1","category":"eess.SP"}
{"created":"2024-04-08 07:23:27","title":"Continuous-variable quantum key distribution with noisy squeezed states","abstract":"We address the role of noisy squeezing in security and performance of continuous-variable (CV) quantum key distribution (QKD) protocols. Squeezing has long been recognized for its numerous advantages in CV QKD, such as enhanced robustness against channel noise and loss, and improved secret key rates. However, the noise of the squeezed states, that unavoidably originates already from optical loss in the source, raises concerns about its potential exploitation by an eavesdropper. This is particularly relevant if this noise is pessimistically assumed untrusted. We address the allocation of untrusted noise within a squeezed state and show that anti-squeezing noise is typically more harmful for security of the protocols, as it potentially provides more information to an eavesdropper. Although the anti-squeezing noise may not directly contribute to the generated key data, it is involved in parameter estimation and can in fact be harmful even if considered trusted. Our study covers the effects of anti-squeezing noise in both the asymptotic and finite-size regimes. We highlight the positive effects and limitations of imposing trust assumption on anti-squeezing noise. Additionally, we emphasize the detrimental impact of untrusted noise in both fiber and free-space fading links. Our findings offer essential insights for practical implementations and optimization of squeezed-state CV QKD protocols in realistic scenarios.","sentences":["We address the role of noisy squeezing in security and performance of continuous-variable (CV) quantum key distribution (QKD) protocols.","Squeezing has long been recognized for its numerous advantages in CV QKD, such as enhanced robustness against channel noise and loss, and improved secret key rates.","However, the noise of the squeezed states, that unavoidably originates already from optical loss in the source, raises concerns about its potential exploitation by an eavesdropper.","This is particularly relevant if this noise is pessimistically assumed untrusted.","We address the allocation of untrusted noise within a squeezed state and show that anti-squeezing noise is typically more harmful for security of the protocols, as it potentially provides more information to an eavesdropper.","Although the anti-squeezing noise may not directly contribute to the generated key data, it is involved in parameter estimation and can in fact be harmful even if considered trusted.","Our study covers the effects of anti-squeezing noise in both the asymptotic and finite-size regimes.","We highlight the positive effects and limitations of imposing trust assumption on anti-squeezing noise.","Additionally, we emphasize the detrimental impact of untrusted noise in both fiber and free-space fading links.","Our findings offer essential insights for practical implementations and optimization of squeezed-state CV QKD protocols in realistic scenarios."],"url":"http://arxiv.org/abs/2404.05247v1","category":"quant-ph"}
{"created":"2024-04-08 07:11:43","title":"Collision-Free Trajectory Optimization in Cluttered Environments with Sums-of-Squares Programming","abstract":"In this work, we propose a trajectory optimization approach for robot navigation in cluttered 3D environments. We represent the robot's geometry as a semialgebraic set defined by polynomial inequalities such that robots with general shapes can be suitably characterized. To address the robot navigation task in obstacle-dense environments, we exploit the free space directly to construct a sequence of free regions, and allocate each waypoint on the trajectory to a specific region. Then, we incorporate a uniform scaling factor for each free region, and formulate a Sums-of-Squares (SOS) optimization problem that renders the containment relationship between the robot and the free space computationally tractable. The SOS optimization problem is further reformulated to a semidefinite program (SDP), and the collision-free constraints are shown to be equivalent to limiting the scaling factor along the entire trajectory. In this context, the robot at a specific configuration is tailored to stay within the free region. Next, to solve the trajectory optimization problem with the proposed safety constraints (which are implicitly dependent on the robot configurations), we derive the analytical solution to the gradient of the minimum scaling factor with respect to the robot configuration. As a result, this seamlessly facilitates the use of gradient-based methods in efficient solving of the trajectory optimization problem. Through a series of simulations and real-world experiments, the proposed trajectory optimization approach is validated in various challenging scenarios, and the results demonstrate its effectiveness in generating collision-free trajectories in dense and intricate environments populated with obstacles.","sentences":["In this work, we propose a trajectory optimization approach for robot navigation in cluttered 3D environments.","We represent the robot's geometry as a semialgebraic set defined by polynomial inequalities such that robots with general shapes can be suitably characterized.","To address the robot navigation task in obstacle-dense environments, we exploit the free space directly to construct a sequence of free regions, and allocate each waypoint on the trajectory to a specific region.","Then, we incorporate a uniform scaling factor for each free region, and formulate a Sums-of-Squares (SOS) optimization problem that renders the containment relationship between the robot and the free space computationally tractable.","The SOS optimization problem is further reformulated to a semidefinite program (SDP), and the collision-free constraints are shown to be equivalent to limiting the scaling factor along the entire trajectory.","In this context, the robot at a specific configuration is tailored to stay within the free region.","Next, to solve the trajectory optimization problem with the proposed safety constraints (which are implicitly dependent on the robot configurations), we derive the analytical solution to the gradient of the minimum scaling factor with respect to the robot configuration.","As a result, this seamlessly facilitates the use of gradient-based methods in efficient solving of the trajectory optimization problem.","Through a series of simulations and real-world experiments, the proposed trajectory optimization approach is validated in various challenging scenarios, and the results demonstrate its effectiveness in generating collision-free trajectories in dense and intricate environments populated with obstacles."],"url":"http://arxiv.org/abs/2404.05242v1","category":"cs.RO"}
{"created":"2024-04-08 03:46:23","title":"A Riemannian Manifold Approach to Constrained Resource Allocation in ISAC","abstract":"This paper introduces a new resource allocation framework for integrated sensing and communication (ISAC) systems, which are expected to be fundamental aspects of sixth-generation networks. In particular, we develop an augmented Lagrangian manifold optimization (ALMO) framework designed to maximize communication sum rate while satisfying sensing beampattern gain targets and base station (BS) transmit power limits. ALMO applies the principles of Riemannian manifold optimization (MO) to navigate the complex, non-convex landscape of the resource allocation problem. It efficiently leverages the augmented Lagrangian method to ensure adherence to constraints. We present comprehensive numerical results to validate our framework, which illustrates the ALMO method's superior capability to enhance the dual functionalities of communication and sensing in ISAC systems. For instance, with 12 antennas and 30 dBm BS transmit power, our proposed ALMO algorithm delivers a 10.1% sum rate gain over a benchmark optimization-based algorithm. This work demonstrates significant improvements in system performance and contributes a new algorithmic perspective to ISAC resource management.","sentences":["This paper introduces a new resource allocation framework for integrated sensing and communication (ISAC) systems, which are expected to be fundamental aspects of sixth-generation networks.","In particular, we develop an augmented Lagrangian manifold optimization (ALMO) framework designed to maximize communication sum rate while satisfying sensing beampattern gain targets and base station (BS) transmit power limits.","ALMO applies the principles of Riemannian manifold optimization (MO) to navigate the complex, non-convex landscape of the resource allocation problem.","It efficiently leverages the augmented Lagrangian method to ensure adherence to constraints.","We present comprehensive numerical results to validate our framework, which illustrates the ALMO method's superior capability to enhance the dual functionalities of communication and sensing in ISAC systems.","For instance, with 12 antennas and 30 dBm BS transmit power, our proposed ALMO algorithm delivers a 10.1% sum rate gain over a benchmark optimization-based algorithm.","This work demonstrates significant improvements in system performance and contributes a new algorithmic perspective to ISAC resource management."],"url":"http://arxiv.org/abs/2404.05173v1","category":"cs.IT"}
{"created":"2024-04-08 02:13:40","title":"Towards Optimal Circuit Size for Quantum Sparse State Preparation","abstract":"Compared to general quantum states, the sparse states arise more frequently in the field of quantum computation. In this work, we consider the preparation for $n$-qubit sparse quantum states with $s$ non-zero amplitudes and propose two algorithms. The first algorithm uses $O(ns/\\log n + n)$ gates, improving upon previous methods by $O(\\log n)$. We further establish a matching lower bound for any algorithm which is not amplitude-aware and employs at most $\\operatorname{poly}(n)$ ancillary qubits. The second algorithm is tailored for binary strings that exhibit a short Hamiltonian path. An application is the preparation of $U(1)$-invariant state with $k$ down-spins in a chain of length $n$, including Bethe states, for which our algorithm constructs a circuit of size $O\\left(\\binom{n}{k}\\log n\\right)$. This surpasses previous results by $O(n/\\log n)$ and is close to the lower bound $O\\left(\\binom{n}{k}\\right)$. Both the two algorithms shrink the existing gap theoretically and provide increasing advantages numerically.","sentences":["Compared to general quantum states, the sparse states arise more frequently in the field of quantum computation.","In this work, we consider the preparation for $n$-qubit sparse quantum states with $s$ non-zero amplitudes and propose two algorithms.","The first algorithm uses $O(ns/\\log n + n)$ gates, improving upon previous methods by $O(\\log n)$. We further establish a matching lower bound for any algorithm which is not amplitude-aware and employs at most $\\operatorname{poly}(n)$ ancillary qubits.","The second algorithm is tailored for binary strings that exhibit a short Hamiltonian path.","An application is the preparation of $U(1)$-invariant state with $k$ down-spins in a chain of length $n$, including Bethe states, for which our algorithm constructs a circuit of size $O\\left(\\binom{n}{k}\\log n\\right)$.","This surpasses previous results by $O(n/\\log n)$ and is close to the lower bound $O\\left(\\binom{n}{k}\\right)$. Both the two algorithms shrink the existing gap theoretically and provide increasing advantages numerically."],"url":"http://arxiv.org/abs/2404.05147v1","category":"quant-ph"}
{"created":"2024-04-08 00:46:59","title":"Oracle complexity of augmented Lagrangian methods for nonsmooth manifold optimization","abstract":"In this paper, we introduce a novel manifold inexact augmented Lagrangian method (\\textbf{ManIAL}) and provide an oracle complexity analysis. To the best of our knowledge, this is the first complexity result for the augmented Lagrangian method for solving nonsmooth problems on Riemannian manifolds. By using the Riemannian gradient method as a subroutine, we establish an oracle complexity result of $\\mathcal{O}(\\epsilon^{-3})$, matching the best-known complexity result. Our algorithm relies on the careful selection of penalty parameters and the precise control of termination criteria for subproblems. Furthermore, we also propose a stochastic manifold inexact augmented Lagrangian method (\\textbf{StoManIAL}) for cases where the smooth term follows an expectation form. By using a Riemannian recursive momentum method as a subroutine, we prove that \\textbf{StoManIAL} achieves an oracle complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$, which is better than the best-known $\\mathcal{O}(\\epsilon^{-4})$ result. Numerical experiments conducted on sparse principal component analysis and sparse canonical correlation analysis demonstrate that our proposed methods outperform an existing method with the previously best-known complexity result.","sentences":["In this paper, we introduce a novel manifold inexact augmented Lagrangian method (\\textbf{ManIAL}) and provide an oracle complexity analysis.","To the best of our knowledge, this is the first complexity result for the augmented Lagrangian method for solving nonsmooth problems on Riemannian manifolds.","By using the Riemannian gradient method as a subroutine, we establish an oracle complexity result of $\\mathcal{O}(\\epsilon^{-3})$, matching the best-known complexity result.","Our algorithm relies on the careful selection of penalty parameters and the precise control of termination criteria for subproblems.","Furthermore, we also propose a stochastic manifold inexact augmented Lagrangian method (\\textbf{StoManIAL}) for cases where the smooth term follows an expectation form.","By using a Riemannian recursive momentum method as a subroutine, we prove that \\textbf{StoManIAL} achieves an oracle complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$, which is better than the best-known $\\mathcal{O}(\\epsilon^{-4})$ result.","Numerical experiments conducted on sparse principal component analysis and sparse canonical correlation analysis demonstrate that our proposed methods outperform an existing method with the previously best-known complexity result."],"url":"http://arxiv.org/abs/2404.05121v1","category":"math.OC"}
{"created":"2024-04-08 00:45:35","title":"Rollbot: a Spherical Robot Driven by a Single Actuator","abstract":"Here we present Rollbot, the first spherical robot capable of controllably maneuvering on 2D plane with a single actuator. Rollbot rolls on the ground in circular pattern and controls its motion by changing the curvature of the trajectory through accelerating and decelerating its single motor and attached mass. We present the theoretical analysis, design, and control of Rollbot, and demonstrate its ability to move in a controllable circular pattern and follow waypoints.","sentences":["Here we present Rollbot, the first spherical robot capable of controllably maneuvering on 2D plane with a single actuator.","Rollbot rolls on the ground in circular pattern and controls its motion by changing the curvature of the trajectory through accelerating and decelerating its single motor and attached mass.","We present the theoretical analysis, design, and control of Rollbot, and demonstrate its ability to move in a controllable circular pattern and follow waypoints."],"url":"http://arxiv.org/abs/2404.05120v1","category":"cs.RO"}
{"created":"2024-04-07 21:22:42","title":"Performance Analysis of Wideband Near-Field Sensing (NISE)","abstract":"The impact of large bandwidth on near-filed sensing (NISE) is analyzed in multi-carrier systems. The fundamental Cramer-Rao bounds (CRBs) for wideband NISE are characterized. In particular, the closed-form CRBs are derived for both uniform linear arrays (ULAs) and uniform circular arrays (UCAs). Then, the asymptotic CRBs are analyzed. It is rigorously proved that: 1) as the number of antennas N increases, the maximum decay rates of asymptotic CRBs are 1/N for ULAs and 1/N^2 for UCAs; 2) as the number of subcarriers M increases, the asymptotic CRBs decay as 1/M^3 for both ULAs and UCAs; and 3) CRBs are inversely proportional to the beamforming gain. Based on the analytical results, two practical beamforming approaches are proposed for near-field wideband integrated sensing and communication (ISAC), namely independent and joint approaches. For the independent approach, the beamformer on each subcarrier is designed exclusively for either sensing or communication. For the joint approach, the beamformer on each subcarrier is jointly optimized for both functions through a low-complexity iterative algorithm. Finally, numerical results show that 1) large bandwidth sets an estimation error ceiling for NISE; 2) NISE performance converges to far-field sensing performance when the bandwidth is extremely large; 3) there is a tradeoff between array size and system bandwidth for achieving a given sensing performance; and 4) the simple independent beamforming approach achieves an ISAC performance close to the complex joint beamforming approach.","sentences":["The impact of large bandwidth on near-filed sensing (NISE) is analyzed in multi-carrier systems.","The fundamental Cramer-Rao bounds (CRBs) for wideband NISE are characterized.","In particular, the closed-form CRBs are derived for both uniform linear arrays (ULAs) and uniform circular arrays (UCAs).","Then, the asymptotic CRBs are analyzed.","It is rigorously proved that: 1) as the number of antennas N increases, the maximum decay rates of asymptotic CRBs are 1/N for ULAs and 1/N^2 for UCAs; 2) as the number of subcarriers M increases, the asymptotic CRBs decay as 1/M^3 for both ULAs and UCAs; and 3) CRBs are inversely proportional to the beamforming gain.","Based on the analytical results, two practical beamforming approaches are proposed for near-field wideband integrated sensing and communication (ISAC), namely independent and joint approaches.","For the independent approach, the beamformer on each subcarrier is designed exclusively for either sensing or communication.","For the joint approach, the beamformer on each subcarrier is jointly optimized for both functions through a low-complexity iterative algorithm.","Finally, numerical results show that 1) large bandwidth sets an estimation error ceiling for NISE; 2) NISE performance converges to far-field sensing performance when the bandwidth is extremely large; 3) there is a tradeoff between array size and system bandwidth for achieving a given sensing performance; and 4) the simple independent beamforming approach achieves an ISAC performance close to the complex joint beamforming approach."],"url":"http://arxiv.org/abs/2404.05076v1","category":"cs.IT"}
{"created":"2024-04-07 18:33:56","title":"Probing the Anisotropic Fermi Surface in Tetralayer Graphene via Transverse Magnetic Focusing","abstract":"Bernal-stacked tetralayer graphene (4LG) exhibits intriguing low-energy properties, featuring two massive subbands and showcasing diverse features of topologically distinct, anisotropic Fermi surfaces, including Lifshitz transitions and trigonal warping. Here, we study the influence of the band structure on electron dynamics within 4LG using transverse magnetic focusing. Our analysis reveals two distinct focusing peaks corresponding to the two subbands. Furthermore, we uncover a pronounced dependence of the focusing spectra on crystal orientations, indicative of an anisotropic Fermi surface. Utilizing the semiclassical model, we attribute this orientation-dependent behavior to the trigonal warping of the band structure. This phenomenon leads to variations in electron trajectories based on crystal orientation. Our findings not only enhance our understanding of the dynamics of electrons in 4LG, but also offer a promising method for probing anisotropic Fermi surfaces in other materials.","sentences":["Bernal-stacked tetralayer graphene (4LG) exhibits intriguing low-energy properties, featuring two massive subbands and showcasing diverse features of topologically distinct, anisotropic Fermi surfaces, including Lifshitz transitions and trigonal warping.","Here, we study the influence of the band structure on electron dynamics within 4LG using transverse magnetic focusing.","Our analysis reveals two distinct focusing peaks corresponding to the two subbands.","Furthermore, we uncover a pronounced dependence of the focusing spectra on crystal orientations, indicative of an anisotropic Fermi surface.","Utilizing the semiclassical model, we attribute this orientation-dependent behavior to the trigonal warping of the band structure.","This phenomenon leads to variations in electron trajectories based on crystal orientation.","Our findings not only enhance our understanding of the dynamics of electrons in 4LG, but also offer a promising method for probing anisotropic Fermi surfaces in other materials."],"url":"http://arxiv.org/abs/2404.05038v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-07 12:07:33","title":"The Sample Complexity of Gradient Descent in Stochastic Convex Optimization","abstract":"We analyze the sample complexity of full-batch Gradient Descent (GD) in the setup of non-smooth Stochastic Convex Optimization. We show that the generalization error of GD, with (minmax) optimal choice of hyper-parameters, can be $\\tilde \\Theta(d/m + 1/\\sqrt{m})$, where $d$ is the dimension and $m$ is the sample size. This matches the sample complexity of \\emph{worst-case} empirical risk minimizers. That means that, in contrast with other algorithms, GD has no advantage over naive ERMs. Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations. Our bound also shows that, for general hyper-parameters, when the dimension is strictly larger than number of samples, $T=\\Omega(1/\\epsilon^4)$ iterations are necessary to avoid overfitting. This resolves an open problem by \\citet*{schliserman2024dimension, amir2021sgd}, and improves over previous lower bounds that demonstrated that the sample size must be at least square root of the dimension.","sentences":["We analyze the sample complexity of full-batch Gradient Descent (GD) in the setup of non-smooth Stochastic Convex Optimization.","We show that the generalization error of GD, with (minmax) optimal choice of hyper-parameters, can be $\\tilde \\Theta(d/m + 1/\\sqrt{m})$, where $d$ is the dimension and $m$ is the sample size.","This matches the sample complexity of \\emph{worst-case} empirical risk minimizers.","That means that, in contrast with other algorithms, GD has no advantage over naive ERMs.","Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations.","Our bound also shows that, for general hyper-parameters, when the dimension is strictly larger than number of samples, $T=\\Omega(1/\\epsilon^4)$ iterations are necessary to avoid overfitting.","This resolves an open problem by \\citet*{schliserman2024dimension, amir2021sgd}, and improves over previous lower bounds that demonstrated that the sample size must be at least square root of the dimension."],"url":"http://arxiv.org/abs/2404.04931v1","category":"cs.LG"}
{"created":"2024-04-07 11:20:32","title":"Regularized Conditional Diffusion Model for Multi-Task Preference Alignment","abstract":"Sequential decision-making is desired to align with human intents and exhibit versatility across various tasks. Previous methods formulate it as a conditional generation process, utilizing return-conditioned diffusion models to directly model trajectory distributions. Nevertheless, the return-conditioned paradigm relies on pre-defined reward functions, facing challenges when applied in multi-task settings characterized by varying reward functions (versatility) and showing limited controllability concerning human preferences (alignment). In this work, we adopt multi-task preferences as a unified condition for both single- and multi-task decision-making, and propose preference representations aligned with preference labels. The learned representations are used to guide the conditional generation process of diffusion models, and we introduce an auxiliary objective to maximize the mutual information between representations and corresponding generated trajectories, improving alignment between trajectories and preferences. Extensive experiments in D4RL and Meta-World demonstrate that our method presents favorable performance in single- and multi-task scenarios, and exhibits superior alignment with preferences.","sentences":["Sequential decision-making is desired to align with human intents and exhibit versatility across various tasks.","Previous methods formulate it as a conditional generation process, utilizing return-conditioned diffusion models to directly model trajectory distributions.","Nevertheless, the return-conditioned paradigm relies on pre-defined reward functions, facing challenges when applied in multi-task settings characterized by varying reward functions (versatility) and showing limited controllability concerning human preferences (alignment).","In this work, we adopt multi-task preferences as a unified condition for both single- and multi-task decision-making, and propose preference representations aligned with preference labels.","The learned representations are used to guide the conditional generation process of diffusion models, and we introduce an auxiliary objective to maximize the mutual information between representations and corresponding generated trajectories, improving alignment between trajectories and preferences.","Extensive experiments in D4RL and Meta-World demonstrate that our method presents favorable performance in single- and multi-task scenarios, and exhibits superior alignment with preferences."],"url":"http://arxiv.org/abs/2404.04920v1","category":"cs.LG"}
{"created":"2024-04-07 10:26:45","title":"Convergence Analysis of Stochastic Saddle Point Mirror Descent Algorithm -- A Projected Dynamical View Point","abstract":"Saddle point problems, ubiquitous in optimization, extend beyond game theory to diverse domains like power networks and reinforcement learning. This paper presents novel approaches to tackle saddle point problem, with a focus on continuous-time contexts. In this paper we propose a continuous time dynamics to tackle saddle point problem utilizing projected dynamical system in non-Euclidean domain. This involves computing the (sub/super) gradient of the min-max function within a Riemannian metric. Additionally, we establish viable Caratheodory solutions also prove the Lyapunov stability and asymptotic set stability of the proposed continuous time dynamical system. Next, we present the Stochastic Saddle Point Mirror Descent (SSPMD) algorithm and establish its equivalence with the proposed continuous-time dynamics. Leveraging stability analysis of the continuous-time dynamics, we demonstrate the almost sure convergence of the algorithm's iterates. Furthermore, we introduce the Zeroth-Order Saddle Point Mirror Descent (SZSPMD) algorithm, which approximates gradients using Nesterov's Gaussian Approximation, showcasing convergence to a neighborhood around saddle points. The analysis in this paper provides geometric insights into the mirror descent algorithm and demonstrates how these insights offer theoretical foundations for various practical applications of the mirror descent algorithm in diverse scenarios.","sentences":["Saddle point problems, ubiquitous in optimization, extend beyond game theory to diverse domains like power networks and reinforcement learning.","This paper presents novel approaches to tackle saddle point problem, with a focus on continuous-time contexts.","In this paper we propose a continuous time dynamics to tackle saddle point problem utilizing projected dynamical system in non-Euclidean domain.","This involves computing the (sub/super) gradient of the min-max function within a Riemannian metric.","Additionally, we establish viable Caratheodory solutions also prove the Lyapunov stability and asymptotic set stability of the proposed continuous time dynamical system.","Next, we present the Stochastic Saddle Point Mirror Descent (SSPMD) algorithm and establish its equivalence with the proposed continuous-time dynamics.","Leveraging stability analysis of the continuous-time dynamics, we demonstrate the almost sure convergence of the algorithm's iterates.","Furthermore, we introduce the Zeroth-Order Saddle Point Mirror Descent (SZSPMD) algorithm, which approximates gradients using Nesterov's Gaussian Approximation, showcasing convergence to a neighborhood around saddle points.","The analysis in this paper provides geometric insights into the mirror descent algorithm and demonstrates how these insights offer theoretical foundations for various practical applications of the mirror descent algorithm in diverse scenarios."],"url":"http://arxiv.org/abs/2404.04907v1","category":"math.OC"}
{"created":"2024-04-07 08:49:09","title":"Multi-Type Map Construction via Semantics-Aware Autonomous Exploration in Unknown Indoor Environments","abstract":"This paper proposes a novel semantics-aware autonomous exploration model to handle the long-standing issue: the mainstream RRT (Rapid-exploration Random Tree) based exploration models usually make the mobile robot switch frequently between different regions, leading to the excessively-repeated explorations for the same region. Our proposed semantics-aware model encourages a mobile robot to fully explore the current region before moving to the next region, which is able to avoid excessively-repeated explorations and make the exploration faster. The core idea of semantics-aware autonomous exploration model is optimizing the sampling point selection mechanism and frontier point evaluation function by considering the semantic information of regions. In addition, compared with existing autonomous exploration methods that usually construct the single-type or 2-3 types of maps, our model allows to construct four kinds of maps including point cloud map, occupancy grid map, topological map, and semantic map. To test the performance of our model, we conducted experiments in three simulated environments. The experiment results demonstrate that compared to Improved RRT, our model achieved 33.0% exploration time reduction and 39.3% exploration trajectory length reduction when maintaining >98% exploration rate.","sentences":["This paper proposes a novel semantics-aware autonomous exploration model to handle the long-standing issue: the mainstream RRT (Rapid-exploration Random Tree) based exploration models usually make the mobile robot switch frequently between different regions, leading to the excessively-repeated explorations for the same region.","Our proposed semantics-aware model encourages a mobile robot to fully explore the current region before moving to the next region, which is able to avoid excessively-repeated explorations and make the exploration faster.","The core idea of semantics-aware autonomous exploration model is optimizing the sampling point selection mechanism and frontier point evaluation function by considering the semantic information of regions.","In addition, compared with existing autonomous exploration methods that usually construct the single-type or 2-3 types of maps, our model allows to construct four kinds of maps including point cloud map, occupancy grid map, topological map, and semantic map.","To test the performance of our model, we conducted experiments in three simulated environments.","The experiment results demonstrate that compared to Improved RRT, our model achieved 33.0% exploration time reduction and 39.3% exploration trajectory length reduction when maintaining >98% exploration rate."],"url":"http://arxiv.org/abs/2404.04879v1","category":"cs.RO"}
{"created":"2024-04-07 07:27:23","title":"Analog-Digital Beam Focusing for Line of Sight Wide-Aperture MIMO with Spherical Wavefronts","abstract":"Enhancing high-speed wireless communication in the future relies significantly on harnessing high frequency bands effectively. These bands predominantly operate in line-of-sight (LoS) paths, necessitating well-configured antenna arrays and beamforming techniques for optimal spectrum utilization. Maximizing the potential of LoS multiple-input multiple-output (MIMO) systems, which are crucial for achieving high spectral efficiency, heavily depends on this. As the costs and power demands of mixed-signal devices in high frequency bands make a fully-digital architecture impractical for large-scale MIMO setups, our focus shifts to a hybrid analog-digital hardware configuration. Yet, analog processors' limitations restrict flexibility within arrays, necessitating a nuanced understanding of hardware constraints for optimal antenna configuration design. We explore array design that optimizes the spectral efficiency of hybrid systems, considering hardware constraints. We propose an optimal antenna configuration, leveraging the prolate matrix structure of the LoS channel between two planar arrays. Building on the optimal array configuration, we introduce a low-complexity explicit analog-digital beam focusing scheme that exploits the asymptotic behavior of the LoS channel matrix in the near-field region. Simulation results validate that the proposed antenna configuration and beam focusing scheme achieves near-optimal performance across a range of signal-to-noise ratios with low computational complexity, even under arbitrary rotations relative to the communication link.","sentences":["Enhancing high-speed wireless communication in the future relies significantly on harnessing high frequency bands effectively.","These bands predominantly operate in line-of-sight (LoS) paths, necessitating well-configured antenna arrays and beamforming techniques for optimal spectrum utilization.","Maximizing the potential of LoS multiple-input multiple-output (MIMO) systems, which are crucial for achieving high spectral efficiency, heavily depends on this.","As the costs and power demands of mixed-signal devices in high frequency bands make a fully-digital architecture impractical for large-scale MIMO setups, our focus shifts to a hybrid analog-digital hardware configuration.","Yet, analog processors' limitations restrict flexibility within arrays, necessitating a nuanced understanding of hardware constraints for optimal antenna configuration design.","We explore array design that optimizes the spectral efficiency of hybrid systems, considering hardware constraints.","We propose an optimal antenna configuration, leveraging the prolate matrix structure of the LoS channel between two planar arrays.","Building on the optimal array configuration, we introduce a low-complexity explicit analog-digital beam focusing scheme that exploits the asymptotic behavior of the LoS channel matrix in the near-field region.","Simulation results validate that the proposed antenna configuration and beam focusing scheme achieves near-optimal performance across a range of signal-to-noise ratios with low computational complexity, even under arbitrary rotations relative to the communication link."],"url":"http://arxiv.org/abs/2404.04842v1","category":"cs.IT"}
{"created":"2024-04-07 06:50:38","title":"Dynamic Pricing for Air Cargo Revenue Management","abstract":"We address a dynamic pricing problem for airlines aiming to maximize expected revenue from selling cargo space on a single-leg flight. The cargo shipments' weight and volume are uncertain and their precise values remain unavailable at the booking time. We model this problem as a Markov decision process, and further derive a necessary condition for its optimal pricing strategy. To break the curse of dimensionality, we develop two categories of approximation methods and pricing strategies. One category is based on the quantity of accepted bookings, while the other is founded on the expected weight and volume of accepted bookings. We prove that the pricing strategy of the quantity-based method possesses several inherent structural properties, which are crucial for analytically validating the model and accelerating the computational process. For the weight-volume-based approximation method, we derive a theoretical upper bound for the optimality gap of total expected revenue. For both methods, we further develop augmented strategies to address the extreme pricing issues in scenarios with high product heterogeneity and incorporate the second moment to enhance performance in the scenarios of high uncertainty, respectively. We utilize realistic dataset to conduct extensive numerical tests, and the results show that the average performance gap between the optimal expected revenue and that of each proposed pricing strategy is less than 10%. The quantity-based method requires the least computation, and performs quite well in the scenarios with low product heterogeneity. The augmented quantity-based method and the weight-volume-based method further enhance the resilience to product heterogeneity. The augmented weight-volume-based method significantly improves the revenue when there are high penalties for overbooking and high uncertainty.","sentences":["We address a dynamic pricing problem for airlines aiming to maximize expected revenue from selling cargo space on a single-leg flight.","The cargo shipments' weight and volume are uncertain and their precise values remain unavailable at the booking time.","We model this problem as a Markov decision process, and further derive a necessary condition for its optimal pricing strategy.","To break the curse of dimensionality, we develop two categories of approximation methods and pricing strategies.","One category is based on the quantity of accepted bookings, while the other is founded on the expected weight and volume of accepted bookings.","We prove that the pricing strategy of the quantity-based method possesses several inherent structural properties, which are crucial for analytically validating the model and accelerating the computational process.","For the weight-volume-based approximation method, we derive a theoretical upper bound for the optimality gap of total expected revenue.","For both methods, we further develop augmented strategies to address the extreme pricing issues in scenarios with high product heterogeneity and incorporate the second moment to enhance performance in the scenarios of high uncertainty, respectively.","We utilize realistic dataset to conduct extensive numerical tests, and the results show that the average performance gap between the optimal expected revenue and that of each proposed pricing strategy is less than 10%.","The quantity-based method requires the least computation, and performs quite well in the scenarios with low product heterogeneity.","The augmented quantity-based method and the weight-volume-based method further enhance the resilience to product heterogeneity.","The augmented weight-volume-based method significantly improves the revenue when there are high penalties for overbooking and high uncertainty."],"url":"http://arxiv.org/abs/2404.04831v1","category":"math.OC"}
{"created":"2024-04-07 05:54:28","title":"FRACTAL: Fine-Grained Scoring from Aggregate Text Labels","abstract":"Large language models (LLMs) are being increasingly tuned to power complex generation tasks such as writing, fact-seeking, querying and reasoning. Traditionally, human or model feedback for evaluating and further tuning LLM performance has been provided at the response level, enabling faster and more cost-effective assessments. However, recent works (Amplayo et al. [2022], Wu et al. [2023]) indicate that sentence-level labels may provide more accurate and interpretable feedback for LLM optimization. In this work, we introduce methods to disaggregate response-level labels into sentence-level (pseudo-)labels. Our approach leverages multiple instance learning (MIL) and learning from label proportions (LLP) techniques in conjunction with prior information (e.g., document-sentence cosine similarity) to train a specialized model for sentence-level scoring. We also employ techniques which use model predictions to pseudo-label the train-set at the sentence-level for model training to further improve performance.   We conduct extensive evaluations of our methods across six datasets and four tasks: retrieval, question answering, summarization, and math reasoning. Our results demonstrate improved performance compared to multiple baselines across most of these tasks. Our work is the first to develop response-level feedback to sentence-level scoring techniques, leveraging sentence-level prior information, along with comprehensive evaluations on multiple tasks as well as end-to-end finetuning evaluation showing performance comparable to a model trained on fine-grained human annotated labels.","sentences":["Large language models (LLMs) are being increasingly tuned to power complex generation tasks such as writing, fact-seeking, querying and reasoning.","Traditionally, human or model feedback for evaluating and further tuning LLM performance has been provided at the response level, enabling faster and more cost-effective assessments.","However, recent works (Amplayo et al.","[2022], Wu et al.","[2023]) indicate that sentence-level labels may provide more accurate and interpretable feedback for LLM optimization.","In this work, we introduce methods to disaggregate response-level labels into sentence-level (pseudo-)labels.","Our approach leverages multiple instance learning (MIL) and learning from label proportions (LLP) techniques in conjunction with prior information (e.g., document-sentence cosine similarity) to train a specialized model for sentence-level scoring.","We also employ techniques which use model predictions to pseudo-label the train-set at the sentence-level for model training to further improve performance.   ","We conduct extensive evaluations of our methods across six datasets and four tasks: retrieval, question answering, summarization, and math reasoning.","Our results demonstrate improved performance compared to multiple baselines across most of these tasks.","Our work is the first to develop response-level feedback to sentence-level scoring techniques, leveraging sentence-level prior information, along with comprehensive evaluations on multiple tasks as well as end-to-end finetuning evaluation showing performance comparable to a model trained on fine-grained human annotated labels."],"url":"http://arxiv.org/abs/2404.04817v1","category":"cs.CL"}
{"created":"2024-04-07 05:47:54","title":"Allo: A Programming Model for Composable Accelerator Design","abstract":"Special-purpose hardware accelerators are increasingly pivotal for sustaining performance improvements in emerging applications, especially as the benefits of technology scaling continue to diminish. However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures in a productive manner. Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results. Despite the introduction of several new accelerator design languages (ADLs) aiming to enhance or replace HLS, their advantages are more evident in relatively simple applications with a single kernel. Existing ADLs prove less effective for realistic hierarchical designs with multiple kernels, even if the design hierarchy is flattened.   In this paper, we introduce Allo, a composable programming model for efficient spatial accelerator design. Allo decouples hardware customizations, including compute, memory, communication, and data type from algorithm specification, and encapsulates them as a set of customization primitives. Allo preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner. This approach facilitates holistic optimizations that span across function boundaries. We conduct comprehensive experiments on commonly-used HLS benchmarks and several realistic deep learning models. Our evaluation shows that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases in the PolyBench. For the GPT2 model, the inference latency of the Allo generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher energy efficiency, demonstrating the capability of Allo to handle large-scale designs.","sentences":["Special-purpose hardware accelerators are increasingly pivotal for sustaining performance improvements in emerging applications, especially as the benefits of technology scaling continue to diminish.","However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures in a productive manner.","Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results.","Despite the introduction of several new accelerator design languages (ADLs) aiming to enhance or replace HLS, their advantages are more evident in relatively simple applications with a single kernel.","Existing ADLs prove less effective for realistic hierarchical designs with multiple kernels, even if the design hierarchy is flattened.   ","In this paper, we introduce Allo, a composable programming model for efficient spatial accelerator design.","Allo decouples hardware customizations, including compute, memory, communication, and data type from algorithm specification, and encapsulates them as a set of customization primitives.","Allo preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner.","This approach facilitates holistic optimizations that span across function boundaries.","We conduct comprehensive experiments on commonly-used HLS benchmarks and several realistic deep learning models.","Our evaluation shows that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases in the PolyBench.","For the GPT2 model, the inference latency of the Allo generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher energy efficiency, demonstrating the capability of Allo to handle large-scale designs."],"url":"http://arxiv.org/abs/2404.04815v1","category":"cs.PL"}
{"created":"2024-04-07 04:05:41","title":"LHAASO-KM2A detector simulation using Geant4","abstract":"KM2A is one of the main sub-arrays of LHAASO, working on gamma ray astronomy and cosmic ray physics at energies above 10 TeV. Detector simulation is the important foundation for estimating detector performance and data analysis. It is a big challenge to simulate the KM2A detector in the framework of Geant4 due to the need to track numerous photons from a large number of detector units (>6000) with large altitude difference (30 m) and huge coverage (1.3 km^2). In this paper, the design of the KM2A simulation code G4KM2A based on Geant4 is introduced. The process of G4KM2A is optimized mainly in memory consumption to avoid memory overffow. Some simpliffcations are used to signiffcantly speed up the execution of G4KM2A. The running time is reduced by at least 30 times compared to full detector simulation. The particle distributions and the core/angle resolution comparison between simulation and experimental data of the full KM2A array are also presented, which show good agreement.","sentences":["KM2A is one of the main sub-arrays of LHAASO, working on gamma ray astronomy and cosmic ray physics at energies above 10 TeV. Detector simulation is the important foundation for estimating detector performance and data analysis.","It is a big challenge to simulate the KM2A detector in the framework of Geant4 due to the need to track numerous photons from a large number of detector units (>6000) with large altitude difference (30 m) and huge coverage (1.3 km^2).","In this paper, the design of the KM2A simulation code G4KM2A based on Geant4 is introduced.","The process of G4KM2A is optimized mainly in memory consumption to avoid memory overffow.","Some simpliffcations are used to signiffcantly speed up the execution of G4KM2A. The running time is reduced by at least 30 times compared to full detector simulation.","The particle distributions and the core/angle resolution comparison between simulation and experimental data of the full KM2A array are also presented, which show good agreement."],"url":"http://arxiv.org/abs/2404.04801v1","category":"astro-ph.IM"}
{"created":"2024-04-07 03:08:14","title":"SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget","abstract":"Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions. Based on our observations regarding layer-wise importance in inference, we propose SqueezeAttention to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative token sparsification algorithms to compress the KV-cache for each layer with its very own budget. By optimizing the KV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves around 30% to 70% of the memory reductions and up to 2.2 times of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention.","sentences":["Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference.","Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens.","In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions.","Based on our observations regarding layer-wise importance in inference, we propose SqueezeAttention to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative token sparsification algorithms to compress the KV-cache for each layer with its very own budget.","By optimizing the KV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves around 30% to 70% of the memory reductions and up to 2.2 times of throughput improvements in a wide range of LLMs and benchmarks.","The code is available at https://github.com/hetailang/SqueezeAttention."],"url":"http://arxiv.org/abs/2404.04793v1","category":"cs.LG"}
{"created":"2024-04-07 02:39:04","title":"Global $F$-regularity for weak del Pezzo surfaces","abstract":"Let $k$ be an algebraically closed field of characteristic $p>0$. Let $X$ be a normal projective surface over $k$ with canonical singularities whose anti-canonical divisor is nef and big. We prove that $X$ is globally $F$-regular except for the following cases: (1) $K_X^2=4$ and $p=2$, (2) $K_X^2=3$ and $p \\in \\{2, 3\\}$, (3) $K_X^2=2$ and $p \\in \\{2, 3\\}$, (4) $K_X^2=1$ and $p \\in \\{2, 3, 5\\}$. For each degree $K_X^2$, the assumption of $p$ is optimal.","sentences":["Let $k$ be an algebraically closed field of characteristic $p>0$. Let $X$ be a normal projective surface over $k$ with canonical singularities whose anti-canonical divisor is nef and big.","We prove that $X$ is globally $F$-regular except for the following cases: (1) $K_X^2=4$ and $p=2$, (2) $K_X^2=3$ and $p \\in \\{2, 3\\}$, (3) $K_X^2=2$ and $p \\in \\{2, 3\\}$, (4) $K_X^2=1$ and $p \\in \\{2, 3, 5\\}$.","For each degree $K_X^2$, the assumption of $p$ is optimal."],"url":"http://arxiv.org/abs/2404.04790v1","category":"math.AG"}
{"created":"2024-04-07 00:18:49","title":"WFC3 Infrared Spectroscopic Parallel (WISP) Survey: Photometric and Emission Line Data Release","abstract":"We present reduced images and catalogues of photometric and emission line data ($\\sim$230,000 and $\\sim$8,000 sources, respectively) for the WFC3 Infrared Spectroscopic Parallel (WISP) Survey. These data are made publicly available on the Mikulski Archive for Space Telescopes (MAST) and include reduced images from various facilities: ground-based $ugri$, HST WFC3, and Spitzer IRAC (Infrared Array Camera). Coverage in at least one additional filter beyond the WFC3/IR data are available for roughly half of the fields (227 out of 483), with $\\sim$20% (86) having coverage in six or more filters from $u$-band to IRAC 3.6$\\mu$m (0.35-3.6$\\mu$m). For the lower spatial resolution (and shallower) ground-based and IRAC data, we perform PSF-matched, prior-based, deconfusion photometry (i.e., forced-photometry) using the TPHOT software to optimally extract measurements or upper limits. We present the methodology and software used for the WISP emission line detection and visual inspection. The former adopts a continuous wavelet transformation that significantly reduces the number of spurious sources as candidates before the visual inspection stage. We combine both WISP catalogues and perform SED fitting on galaxies with reliable spectroscopic redshifts and multi-band photometry to measure their stellar masses. We stack WISP spectra as functions of stellar mass and redshift and measure average emission line fluxes and ratios. We find that WISP emission line sources are typically `normal' star-forming galaxies based on the Mass-Excitation diagram ([OIII]/H$\\beta$ vs. $M_\\star$; $0.74<z_\\mathrm{grism}<2.31$), the galaxy main sequence (SFR vs. $M_\\star$; $0.30<z_\\mathrm{grism}<1.45$), $S_{32}$ ratio vs. $M_\\star$ ($0.30<z_\\mathrm{grism}<0.73$), and $O_{32}$ and $R_{23}$ ratios vs. $M_\\star$ ($1.27<z_\\mathrm{grism}<1.45$).","sentences":["We present reduced images and catalogues of photometric and emission line data ($\\sim$230,000 and $\\sim$8,000 sources, respectively) for the WFC3 Infrared Spectroscopic Parallel (WISP) Survey.","These data are made publicly available on the Mikulski Archive for Space Telescopes (MAST) and include reduced images from various facilities: ground-based $ugri$, HST WFC3, and Spitzer IRAC (Infrared Array Camera).","Coverage in at least one additional filter beyond the WFC3/IR data are available for roughly half of the fields (227 out of 483), with $\\sim$20% (86) having coverage in six or more filters from $u$-band to IRAC 3.6$\\mu$m (0.35-3.6$\\mu$m).","For the lower spatial resolution (and shallower) ground-based and IRAC data, we perform PSF-matched, prior-based, deconfusion photometry (i.e., forced-photometry) using the TPHOT software to optimally extract measurements or upper limits.","We present the methodology and software used for the WISP emission line detection and visual inspection.","The former adopts a continuous wavelet transformation that significantly reduces the number of spurious sources as candidates before the visual inspection stage.","We combine both WISP catalogues and perform SED fitting on galaxies with reliable spectroscopic redshifts and multi-band photometry to measure their stellar masses.","We stack WISP spectra as functions of stellar mass and redshift and measure average emission line fluxes and ratios.","We find that WISP emission line sources are typically `normal' star-forming galaxies based on the Mass-Excitation diagram ([OIII]/H$\\beta$ vs. $M_\\star$; $0.74<z_\\mathrm{grism}<2.31$), the galaxy main sequence (SFR vs. $M_\\star$; $0.30<z_\\mathrm{grism}<1.45$), $S_{32}$ ratio vs. $M_\\star$ ($0.30<z_\\mathrm{grism}<0.73$), and $O_{32}$ and $R_{23}$ ratios vs. $M_\\star$ ($1.27<z_\\mathrm{grism}<1.45$)."],"url":"http://arxiv.org/abs/2404.04762v1","category":"astro-ph.GA"}
{"created":"2024-04-06 21:56:25","title":"Fifth Generation IMC: Expanding the scope to Profit, People, and the Planet","abstract":"This editorial outlines an expanded scope for the next (fifth) generation of integrated marketing communication. It identifies key market forces that gave rise to this evolution and describes a trajectory of where Integrated Marketing Communication (IMC) has been and where it is going. The central shift is moving from primarily focusing on one stakeholder to multiple ones, including people (employees and society), the planet (environment), and profits. It identifies examples from industry that exemplify multi-stakeholder decision-making and uses the examples to suggest research questions that academics and practitioners should address. Examples and research directions are organized around marketing strategy, communication media and messages, and measurement systems.","sentences":["This editorial outlines an expanded scope for the next (fifth) generation of integrated marketing communication.","It identifies key market forces that gave rise to this evolution and describes a trajectory of where Integrated Marketing Communication (IMC) has been and where it is going.","The central shift is moving from primarily focusing on one stakeholder to multiple ones, including people (employees and society), the planet (environment), and profits.","It identifies examples from industry that exemplify multi-stakeholder decision-making and uses the examples to suggest research questions that academics and practitioners should address.","Examples and research directions are organized around marketing strategy, communication media and messages, and measurement systems."],"url":"http://arxiv.org/abs/2404.04740v1","category":"cs.CY"}
{"created":"2024-04-06 19:50:37","title":"Change Point Detection in Dynamic Graphs with Generative Model","abstract":"This paper proposes a simple generative model to detect change points in time series of graphs. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that can generate dynamic graphs from the latent representations. The informative prior distributions in the latent spaces are learned from observed data as empirical Bayes, and the expressive power of a generative model is exploited to assist change point detection. Specifically, the model parameters are learned via maximum approximate likelihood, with a Group Fused Lasso regularization. The optimization problem is then solved via Alternating Direction Method of Multipliers (ADMM), and Langevin Dynamics are recruited for posterior inference. Experiments in simulated and real data demonstrate the ability of the generative model in supporting change point detection with good performance.","sentences":["This paper proposes a simple generative model to detect change points in time series of graphs.","The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that can generate dynamic graphs from the latent representations.","The informative prior distributions in the latent spaces are learned from observed data as empirical Bayes, and the expressive power of a generative model is exploited to assist change point detection.","Specifically, the model parameters are learned via maximum approximate likelihood, with a Group Fused Lasso regularization.","The optimization problem is then solved via Alternating Direction Method of Multipliers (ADMM), and Langevin Dynamics are recruited for posterior inference.","Experiments in simulated and real data demonstrate the ability of the generative model in supporting change point detection with good performance."],"url":"http://arxiv.org/abs/2404.04719v1","category":"stat.ME"}
{"created":"2024-04-06 19:04:44","title":"Two-Sided Flexibility in Platforms","abstract":"Flexibility is a cornerstone of operations management, crucial to hedge stochasticity in product demands, service requirements, and resource allocation. In two-sided platforms, flexibility is also two-sided and can be viewed as the compatibility of agents on one side with agents on the other side. Platform actions often influence the flexibility on either the demand or the supply side. But how should flexibility be jointly allocated across different sides? Whereas the literature has traditionally focused on only one side at a time, our work initiates the study of two-sided flexibility in matching platforms. We propose a parsimonious matching model in random graphs and identify the flexibility allocation that optimizes the expected size of a maximum matching. Our findings reveal that flexibility allocation is a first-order issue: for a given flexibility budget, the resulting matching size can vary greatly depending on how the budget is allocated. Moreover, even in the simple and symmetric settings we study, the quest for the optimal allocation is complicated. In particular, easy and costly mistakes can be made if the flexibility decisions on the demand and supply side are optimized independently (e.g., by two different teams in the company), rather than jointly. To guide the search for optimal flexibility allocation, we uncover two effects, flexibility cannibalization, and flexibility abundance, that govern when the optimal design places the flexibility budget only on one side or equally on both sides. In doing so we identify the study of two-sided flexibility as a significant aspect of platform efficiency.","sentences":["Flexibility is a cornerstone of operations management, crucial to hedge stochasticity in product demands, service requirements, and resource allocation.","In two-sided platforms, flexibility is also two-sided and can be viewed as the compatibility of agents on one side with agents on the other side.","Platform actions often influence the flexibility on either the demand or the supply side.","But how should flexibility be jointly allocated across different sides?","Whereas the literature has traditionally focused on only one side at a time, our work initiates the study of two-sided flexibility in matching platforms.","We propose a parsimonious matching model in random graphs and identify the flexibility allocation that optimizes the expected size of a maximum matching.","Our findings reveal that flexibility allocation is a first-order issue: for a given flexibility budget, the resulting matching size can vary greatly depending on how the budget is allocated.","Moreover, even in the simple and symmetric settings we study, the quest for the optimal allocation is complicated.","In particular, easy and costly mistakes can be made if the flexibility decisions on the demand and supply side are optimized independently (e.g., by two different teams in the company), rather than jointly.","To guide the search for optimal flexibility allocation, we uncover two effects, flexibility cannibalization, and flexibility abundance, that govern when the optimal design places the flexibility budget only on one side or equally on both sides.","In doing so we identify the study of two-sided flexibility as a significant aspect of platform efficiency."],"url":"http://arxiv.org/abs/2404.04709v1","category":"econ.GN"}
{"created":"2024-04-06 17:45:27","title":"Q-learning in Dynamic Treatment Regimes with Misclassified Binary Outcome","abstract":"The study of precision medicine involves dynamic treatment regimes (DTRs), which are sequences of treatment decision rules recommended by taking patient-level information as input. The primary goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that leads to the best expected clinical outcome. Statistical methods have been developed in recent years to estimate an optimal DTR, including Q-learning, a regression-based method in the DTR literature. Although there are many studies concerning Q-learning, little attention has been given in the presence of noisy data, such as misclassified outcomes. In this paper, we investigate the effect of outcome misclassification on Q-learning and propose a correction method to accommodate the misclassification effect. Simulation studies are conducted to demonstrate the satisfactory performance of the proposed method. We illustrate the proposed method in two examples from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study and the smoking cessation program.","sentences":["The study of precision medicine involves dynamic treatment regimes (DTRs), which are sequences of treatment decision rules recommended by taking patient-level information as input.","The primary goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that leads to the best expected clinical outcome.","Statistical methods have been developed in recent years to estimate an optimal DTR, including Q-learning, a regression-based method in the DTR literature.","Although there are many studies concerning Q-learning, little attention has been given in the presence of noisy data, such as misclassified outcomes.","In this paper, we investigate the effect of outcome misclassification on Q-learning and propose a correction method to accommodate the misclassification effect.","Simulation studies are conducted to demonstrate the satisfactory performance of the proposed method.","We illustrate the proposed method in two examples from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study and the smoking cessation program."],"url":"http://arxiv.org/abs/2404.04697v1","category":"stat.ME"}
{"created":"2024-04-06 17:42:48","title":"Dynamic Treatment Regimes with Replicated Observations Available for Error-prone Covariates: a Q-learning Approach","abstract":"Dynamic treatment regimes (DTRs) have received an increasing interest in recent years. DTRs are sequences of treatment decision rules tailored to patient-level information. The main goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that yields the best expected clinical outcome. Q-learning has been considered as one of the most popular regression-based methods to estimate the optimal DTR. However, it is rarely studied in an error-prone setting, where the patient information is contaminated with measurement error. In this paper, we study the effect of covariate measurement error on Q-learning and propose a correction method to correct the measurement error in Q-learning. Simulation studies are conducted to assess the performance of the proposed method in Q-learning. We illustrate the use of the proposed method in an application to the sequenced treatment alternatives to relieve depression data.","sentences":["Dynamic treatment regimes (DTRs) have received an increasing interest in recent years.","DTRs are sequences of treatment decision rules tailored to patient-level information.","The main goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that yields the best expected clinical outcome.","Q-learning has been considered as one of the most popular regression-based methods to estimate the optimal DTR.","However, it is rarely studied in an error-prone setting, where the patient information is contaminated with measurement error.","In this paper, we study the effect of covariate measurement error on Q-learning and propose a correction method to correct the measurement error in Q-learning.","Simulation studies are conducted to assess the performance of the proposed method in Q-learning.","We illustrate the use of the proposed method in an application to the sequenced treatment alternatives to relieve depression data."],"url":"http://arxiv.org/abs/2404.04696v1","category":"stat.ME"}
{"created":"2024-04-06 17:41:36","title":"OmniColor: A Global Camera Pose Optimization Approach of LiDAR-360Camera Fusion for Colorizing Point Clouds","abstract":"A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction. This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs. However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses. This paper presents OmniColor, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera. Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions. Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process. Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps. In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability. The code will be released at https://github.com/liubonan123/OmniColor/.","sentences":["A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction.","This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs.","However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses.","This paper presents OmniColor, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera.","Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions.","Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process.","Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps.","In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability.","The code will be released at https://github.com/liubonan123/OmniColor/."],"url":"http://arxiv.org/abs/2404.04693v1","category":"cs.CV"}
{"created":"2024-04-06 16:53:47","title":"Computation and Critical Transitions of Rate-Distortion-Perception Functions With Wasserstein Barycenter","abstract":"The information rate-distortion-perception (RDP) function characterizes the three-way trade-off between description rate, average distortion, and perceptual quality measured by discrepancy between probability distributions. We study several variants of the RDP functions through the lens of optimal transport. By transforming the information RDP function into a Wasserstein Barycenter problem, we identify the critical transitions when one of the constraints becomes inactive and demonstrate several critical transition properties of the RDP variants. Further, the non-strictly convexity brought by the perceptual constraint can be regularized by an entropy regularization term. We prove that the entropy regularized model converges to the original problem and propose an alternating iteration method based on the Sinkhorn algorithm to numerically solve the regularized optimization problem. Experimental results demonstrate the effectiveness and accuracy of the proposed algorithms. As a practical application of our theory, we incorporate our numerical method into a reverse data hiding problem, where a secret message is imperceptibly embedded into the image with guarantees of the perceptual fidelity.","sentences":["The information rate-distortion-perception (RDP) function characterizes the three-way trade-off between description rate, average distortion, and perceptual quality measured by discrepancy between probability distributions.","We study several variants of the RDP functions through the lens of optimal transport.","By transforming the information RDP function into a Wasserstein Barycenter problem, we identify the critical transitions when one of the constraints becomes inactive and demonstrate several critical transition properties of the RDP variants.","Further, the non-strictly convexity brought by the perceptual constraint can be regularized by an entropy regularization term.","We prove that the entropy regularized model converges to the original problem and propose an alternating iteration method based on the Sinkhorn algorithm to numerically solve the regularized optimization problem.","Experimental results demonstrate the effectiveness and accuracy of the proposed algorithms.","As a practical application of our theory, we incorporate our numerical method into a reverse data hiding problem, where a secret message is imperceptibly embedded into the image with guarantees of the perceptual fidelity."],"url":"http://arxiv.org/abs/2404.04681v1","category":"cs.IT"}
{"created":"2024-04-06 16:53:46","title":"On bipartite biregular large graphs","abstract":"A bipartite graph $G=(V,E)$ with $V=V_1\\cup V_2$ is biregular if all the vertices of each stable set, $V_1$ and $V_2$, have the same degree, $r$ and $s$, respectively. This paper studies difference sets derived from both Abelian and non-Abelian groups. From them, we propose some constructions of bipartite biregular graphs with diameter $d=3$ and asymptotically optimal order for given degrees $r$ and $s$. Moreover, we find some biMoore graphs, that is, bipartite biregular graphs that attain the Moore bound.","sentences":["A bipartite graph $G=(V,E)$ with $V=V_1\\cup V_2$ is biregular if all the vertices of each stable set, $V_1$ and $V_2$, have the same degree, $r$ and $s$, respectively.","This paper studies difference sets derived from both Abelian and non-Abelian groups.","From them, we propose some constructions of bipartite biregular graphs with diameter $d=3$ and asymptotically optimal order for given degrees $r$ and $s$. Moreover, we find some biMoore graphs, that is, bipartite biregular graphs that attain the Moore bound."],"url":"http://arxiv.org/abs/2404.04680v1","category":"math.CO"}
{"created":"2024-04-06 15:55:06","title":"Spectral Independence Beyond Total Influence on Trees and Related Graphs","abstract":"We study how to establish $\\textit{spectral independence}$, a key concept in sampling, without relying on total influence bounds, by applying an $\\textit{approximate inverse}$ of the influence matrix. Our method gives constant upper bounds on spectral independence for two foundational Gibbs distributions known to have unbounded total influences:   $\\bullet$ The monomer-dimer model on graphs with large girth (including trees). Prior to our work, such results were only known for graphs with constant maximum degrees or infinite regular trees, as shown by Chen, Liu, and Vigoda (STOC '21).   $\\bullet$ The hardcore model on trees with fugacity $\\lambda < \\mathrm{e}^2$. This remarkably surpasses the well-known $\\lambda_r>\\mathrm{e}-1$ lower bound for the reconstruction threshold on trees, significantly improving upon the current threshold $\\lambda < 1.3$, established in a prior work by Efthymiou, Hayes, \\v{S}tefankovi\\v{c}, and Vigoda (RANDOM '23).   Consequently, we establish optimal $\\Omega(n^{-1})$ spectral gaps of the Glauber dynamics for these models on arbitrary trees, regardless of the maximum degree $\\Delta$.","sentences":["We study how to establish $\\textit{spectral independence}$, a key concept in sampling, without relying on total influence bounds, by applying an $\\textit{approximate inverse}$ of the influence matrix.","Our method gives constant upper bounds on spectral independence for two foundational Gibbs distributions known to have unbounded total influences:   $\\bullet$ The monomer-dimer model on graphs with large girth (including trees).","Prior to our work, such results were only known for graphs with constant maximum degrees or infinite regular trees, as shown by Chen, Liu, and Vigoda (STOC '21).   ","$\\bullet$ The hardcore model on trees with fugacity $\\lambda < \\mathrm{e}^2$. This remarkably surpasses the well-known $\\lambda_r>\\mathrm{e}-1$ lower bound for the reconstruction threshold on trees, significantly improving upon the current threshold $\\lambda < 1.3$, established in a prior work by Efthymiou, Hayes, \\v{S}tefankovi\\v{c}, and Vigoda (RANDOM '23).   ","Consequently, we establish optimal $\\Omega(n^{-1})$ spectral gaps of the Glauber dynamics for these models on arbitrary trees, regardless of the maximum degree $\\Delta$."],"url":"http://arxiv.org/abs/2404.04668v1","category":"cs.DS"}
{"created":"2024-04-06 15:08:37","title":"Active control of road vehicle's drag for varying upstream flow conditions using a Recursive Subspace based Predictive Control methodology","abstract":"The growing focus on reducing energy consumption, particularly in electric vehicles with limited autonomy, has prompted innovative solutions. In this context, we propose a real-time flap-based control system aimed at improving aerodynamic drag in real driving conditions. Employing a Recursive Subspace based Predictive Control (RSPC) approach, we conducted wind tunnel tests on a representative model vehicle at reduced scale equipped with flaps. Comprehensive assessments using pressure measurements and Particle Image Velocimetry (PIV) were undertaken to evaluate the control efficiency. Static and dynamic perturbation tests were conducted, revealing the system's effectiveness in both scenarios. The closed-loop controlled system demonstrated a substantial gain, achieving a 5\\% base pressure recovery.","sentences":["The growing focus on reducing energy consumption, particularly in electric vehicles with limited autonomy, has prompted innovative solutions.","In this context, we propose a real-time flap-based control system aimed at improving aerodynamic drag in real driving conditions.","Employing a Recursive Subspace based Predictive Control (RSPC) approach, we conducted wind tunnel tests on a representative model vehicle at reduced scale equipped with flaps.","Comprehensive assessments using pressure measurements and Particle Image Velocimetry (PIV) were undertaken to evaluate the control efficiency.","Static and dynamic perturbation tests were conducted, revealing the system's effectiveness in both scenarios.","The closed-loop controlled system demonstrated a substantial gain, achieving a 5\\% base pressure recovery."],"url":"http://arxiv.org/abs/2404.04652v1","category":"math.DS"}
{"created":"2024-04-06 14:56:59","title":"InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization","abstract":"Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at https://github.com/xiefan-guo/initno.","sentences":["Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images.","However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge.","This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise.","Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images.","We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors.","A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions.","Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts.","Our code is available at https://github.com/xiefan-guo/initno."],"url":"http://arxiv.org/abs/2404.04650v1","category":"cs.CV"}
{"created":"2024-04-06 12:50:08","title":"Empowering Image Recovery_ A Multi-Attention Approach","abstract":"We propose Diverse Restormer (DART), a novel image restoration method that effectively integrates information from various sources (long sequences, local and global regions, feature dimensions, and positional dimensions) to address restoration challenges. While Transformer models have demonstrated excellent performance in image restoration due to their self-attention mechanism, they face limitations in complex scenarios. Leveraging recent advancements in Transformers and various attention mechanisms, our method utilizes customized attention mechanisms to enhance overall performance. DART, our novel network architecture, employs windowed attention to mimic the selective focusing mechanism of human eyes. By dynamically adjusting receptive fields, it optimally captures the fundamental features crucial for image resolution reconstruction. Efficiency and performance balance are achieved through the LongIR attention mechanism for long sequence image restoration. Integration of attention mechanisms across feature and positional dimensions further enhances the recovery of fine details. Evaluation across five restoration tasks consistently positions DART at the forefront. Upon acceptance, we commit to providing publicly accessible code and models to ensure reproducibility and facilitate further research.","sentences":["We propose Diverse Restormer (DART), a novel image restoration method that effectively integrates information from various sources (long sequences, local and global regions, feature dimensions, and positional dimensions) to address restoration challenges.","While Transformer models have demonstrated excellent performance in image restoration due to their self-attention mechanism, they face limitations in complex scenarios.","Leveraging recent advancements in Transformers and various attention mechanisms, our method utilizes customized attention mechanisms to enhance overall performance.","DART, our novel network architecture, employs windowed attention to mimic the selective focusing mechanism of human eyes.","By dynamically adjusting receptive fields, it optimally captures the fundamental features crucial for image resolution reconstruction.","Efficiency and performance balance are achieved through the LongIR attention mechanism for long sequence image restoration.","Integration of attention mechanisms across feature and positional dimensions further enhances the recovery of fine details.","Evaluation across five restoration tasks consistently positions DART at the forefront.","Upon acceptance, we commit to providing publicly accessible code and models to ensure reproducibility and facilitate further research."],"url":"http://arxiv.org/abs/2404.04617v1","category":"cs.CV"}
{"created":"2024-04-06 12:27:21","title":"Panoptic Perception: A Novel Task and Fine-grained Dataset for Universal Remote Sensing Image Interpretation","abstract":"Current remote-sensing interpretation models often focus on a single task such as detection, segmentation, or caption. However, the task-specific designed models are unattainable to achieve the comprehensive multi-level interpretation of images. The field also lacks support for multi-task joint interpretation datasets. In this paper, we propose Panoptic Perception, a novel task and a new fine-grained dataset (FineGrip) to achieve a more thorough and universal interpretation for RSIs. The new task, 1) integrates pixel-level, instance-level, and image-level information for universal image perception, 2) captures image information from coarse to fine granularity, achieving deeper scene understanding and description, and 3) enables various independent tasks to complement and enhance each other through multi-task learning. By emphasizing multi-task interactions and the consistency of perception results, this task enables the simultaneous processing of fine-grained foreground instance segmentation, background semantic segmentation, and global fine-grained image captioning. Concretely, the FineGrip dataset includes 2,649 remote sensing images, 12,054 fine-grained instance segmentation masks belonging to 20 foreground things categories, 7,599 background semantic masks for 5 stuff classes and 13,245 captioning sentences. Furthermore, we propose a joint optimization-based panoptic perception model. Experimental results on FineGrip demonstrate the feasibility of the panoptic perception task and the beneficial effect of multi-task joint optimization on individual tasks. The dataset will be publicly available.","sentences":["Current remote-sensing interpretation models often focus on a single task such as detection, segmentation, or caption.","However, the task-specific designed models are unattainable to achieve the comprehensive multi-level interpretation of images.","The field also lacks support for multi-task joint interpretation datasets.","In this paper, we propose Panoptic Perception, a novel task and a new fine-grained dataset (FineGrip) to achieve a more thorough and universal interpretation for RSIs.","The new task, 1) integrates pixel-level, instance-level, and image-level information for universal image perception, 2) captures image information from coarse to fine granularity, achieving deeper scene understanding and description, and 3) enables various independent tasks to complement and enhance each other through multi-task learning.","By emphasizing multi-task interactions and the consistency of perception results, this task enables the simultaneous processing of fine-grained foreground instance segmentation, background semantic segmentation, and global fine-grained image captioning.","Concretely, the FineGrip dataset includes 2,649 remote sensing images, 12,054 fine-grained instance segmentation masks belonging to 20 foreground things categories, 7,599 background semantic masks for 5 stuff classes and 13,245 captioning sentences.","Furthermore, we propose a joint optimization-based panoptic perception model.","Experimental results on FineGrip demonstrate the feasibility of the panoptic perception task and the beneficial effect of multi-task joint optimization on individual tasks.","The dataset will be publicly available."],"url":"http://arxiv.org/abs/2404.04608v1","category":"cs.CV"}
{"created":"2024-04-06 11:57:20","title":"Local Test for Unitarily Invariant Properties of Bipartite Quantum States","abstract":"We study the power of local test for bipartite quantum states. Our central result is that, for properties of bipartite pure states, unitary invariance on one part implies an optimal (over all global testers) local tester acting only on the other part. This suggests a canonical local tester for entanglement spectra (i.e., Schmidt coefficients), and reveals that purified samples offer no advantage in property testing of mixed states. As applications, we settle two open questions raised in the survey of Montanaro and de Wolf (2016) by providing:   1. A matching lower bound $\\Omega(1/\\varepsilon^2)$ for testing whether a multipartite pure state is product or $\\varepsilon$-far, showing that the algorithm of Harrow and Montanaro (2010) is optimal, even for bipartite states;   2. The first non-trivial lower bound $\\Omega(r/\\varepsilon)$ for testing whether the Schmidt rank of a bipartite pure state is at most $r$ or $\\varepsilon$-far.   We also show other new sample lower bounds, for example:   - A matching lower bound $\\Omega(d/\\varepsilon^2)$ for testing whether a $d$-dimensional bipartite pure state is maximally entangled or $\\varepsilon$-far, showing that the algorithm of O'Donnell and Wright (2015) is optimal for this task.   Beyond sample complexity, we also contribute new quantum query lower bounds:   - A query lower bound $\\widetilde \\Omega(\\sqrt{d/\\Delta})$ for the $d$-dimensional entanglement entropy problem with gap $\\Delta$, improving the prior best $\\Omega(\\sqrt[4]{d})$ by She and Yuen (2023) and $\\widetilde \\Omega(1/\\sqrt{\\Delta})$ by Wang and Zhang (2023) and Weggemans (2024).   Furthermore, our central result can be extended when the tested state is mixed: one-way LOCC is sufficient to realize the optimal tester.","sentences":["We study the power of local test for bipartite quantum states.","Our central result is that, for properties of bipartite pure states, unitary invariance on one part implies an optimal (over all global testers) local tester acting only on the other part.","This suggests a canonical local tester for entanglement spectra (i.e., Schmidt coefficients), and reveals that purified samples offer no advantage in property testing of mixed states.","As applications, we settle two open questions raised in the survey of Montanaro and de Wolf (2016) by providing:   1.","A matching lower bound $\\Omega(1/\\varepsilon^2)$ for testing whether a multipartite pure state is product or $\\varepsilon$-far, showing that the algorithm of Harrow and Montanaro (2010) is optimal, even for bipartite states;   2.","The first non-trivial lower bound $\\Omega(r/\\varepsilon)$ for testing whether the Schmidt rank of a bipartite pure state is at most $r$ or $\\varepsilon$-far.   ","We also show other new sample lower bounds, for example:   - A matching lower bound $\\Omega(d/\\varepsilon^2)$ for testing whether a $d$-dimensional bipartite pure state is maximally entangled or $\\varepsilon$-far, showing that the algorithm of O'Donnell and Wright (2015) is optimal for this task.   ","Beyond sample complexity, we also contribute new quantum query lower bounds:   - A query lower bound $\\widetilde \\Omega(\\sqrt{d/\\Delta})$ for the $d$-dimensional entanglement entropy problem with gap $\\Delta$, improving the prior best $\\Omega(\\sqrt[4]{d})$ by She and Yuen (2023) and $\\widetilde \\Omega(1/\\sqrt{\\Delta})$ by Wang and Zhang (2023) and Weggemans (2024).   ","Furthermore, our central result can be extended when the tested state is mixed: one-way LOCC is sufficient to realize the optimal tester."],"url":"http://arxiv.org/abs/2404.04599v1","category":"quant-ph"}
{"created":"2024-04-06 11:42:34","title":"A Two Time-Scale Joint Optimization Approach for UAV-assisted MEC","abstract":"Unmanned aerial vehicles (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services close to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply heterogeneity between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex mixed integer nonlinear programming (MINLP), we propose a two time-scale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach. In the short time scale, we propose a price-incentive method for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long time scale, we propose a convex optimization-based method for UAV trajectory control. Besides, we prove the stability, optimality, and polynomial complexity of TJCCT. Simulation results demonstrate that TJCCT outperforms the comparative algorithms in terms of the utility of the system, the QoE of MDs, and the revenue of MEC servers.","sentences":["Unmanned aerial vehicles (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services close to mobile devices (MDs).","However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply heterogeneity between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network.","To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility.","Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility.","Since the problem is a non-convex mixed integer nonlinear programming (MINLP), we propose a two time-scale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach.","In the short time scale, we propose a price-incentive method for on-demand computing resource allocation and a matching mechanism-based method for computation offloading.","In the long time scale, we propose a convex optimization-based method for UAV trajectory control.","Besides, we prove the stability, optimality, and polynomial complexity of TJCCT.","Simulation results demonstrate that TJCCT outperforms the comparative algorithms in terms of the utility of the system, the QoE of MDs, and the revenue of MEC servers."],"url":"http://arxiv.org/abs/2404.04597v1","category":"eess.SY"}
{"created":"2024-04-06 11:30:44","title":"Evaluation and Optimization of Positional Accuracy for Maritime Positioning Systems","abstract":"Navigation and trajectorial estimation of maritime vessels are contingent upon the context of positional accuracy. Even the smallest deviations in the estimation of a given vessel may result in detrimental consequences in terms of economic and ecologic quotients. To ensure an agile and precise environment for maritime vessel positional estimation, preexisting marine radar technologies can be utilized in a way that ensures a higher level of precision compared to GNSS-based identification and positioning. In this paper, we present a positional optimization for radarbased vessel navigation systems that utilize the installment of vessel detection sensors. The main objective of this research is to employ as fewer sensors as possible while preserving the attainable error threshold for positioning that is defined by International Maritime Organization (IMO). Our approach leads most of the time to a positioning error of up to 5 m along shorelines and rivers and up to 50 m along open coastal regions.","sentences":["Navigation and trajectorial estimation of maritime vessels are contingent upon the context of positional accuracy.","Even the smallest deviations in the estimation of a given vessel may result in detrimental consequences in terms of economic and ecologic quotients.","To ensure an agile and precise environment for maritime vessel positional estimation, preexisting marine radar technologies can be utilized in a way that ensures a higher level of precision compared to GNSS-based identification and positioning.","In this paper, we present a positional optimization for radarbased vessel navigation systems that utilize the installment of vessel detection sensors.","The main objective of this research is to employ as fewer sensors as possible while preserving the attainable error threshold for positioning that is defined by International Maritime Organization (IMO).","Our approach leads most of the time to a positioning error of up to 5 m along shorelines and rivers and up to 50 m along open coastal regions."],"url":"http://arxiv.org/abs/2404.04593v1","category":"eess.SY"}
{"created":"2024-04-06 11:29:53","title":"Quantum magnetic oscillations in the absence of closed electron trajectories","abstract":"Quantum magnetic oscillations in crystals are typically understood in terms of Bohr-Sommerfeld quantisation, the frequency of oscillation is given by the area of a closed electron trajectory. However, since the 1970s, oscillations have been observed with frequencies that do not correspond to closed electron trajectories and this effect has remained not fully understood. Previous theory has focused on explaining the effect using various kinetic mechanisms, however, frequencies without a closed electron orbit have been observed in equilibrium and so a kinetic mechanism cannot be the entire story. In this work we develop a theory which explains these frequencies in equilibrium and can thus be used to understand measurements of both Shubnikov-de Haas and de Haas-van Alphen oscillations. We show, analytically, that these frequencies arise due to multi-electron correlations. We then extend our theory to explain a recent experiment on artificial crystals in GaAs two-dimensional electron gases, which revealed for the first time magnetic oscillations having frequencies that are half of those previously observed. We show that the half-frequencies arise in equilibrium from single-particle dynamics with account of impurities. Our analytic results are reinforced by exact numerics, which we also use clarify prior works on the kinetic regime.","sentences":["Quantum magnetic oscillations in crystals are typically understood in terms of Bohr-Sommerfeld quantisation, the frequency of oscillation is given by the area of a closed electron trajectory.","However, since the 1970s, oscillations have been observed with frequencies that do not correspond to closed electron trajectories and this effect has remained not fully understood.","Previous theory has focused on explaining the effect using various kinetic mechanisms, however, frequencies without a closed electron orbit have been observed in equilibrium and so a kinetic mechanism cannot be the entire story.","In this work we develop a theory which explains these frequencies in equilibrium and can thus be used to understand measurements of both Shubnikov-de Haas and de Haas-van Alphen oscillations.","We show, analytically, that these frequencies arise due to multi-electron correlations.","We then extend our theory to explain a recent experiment on artificial crystals in GaAs two-dimensional electron gases, which revealed for the first time magnetic oscillations having frequencies that are half of those previously observed.","We show that the half-frequencies arise in equilibrium from single-particle dynamics with account of impurities.","Our analytic results are reinforced by exact numerics, which we also use clarify prior works on the kinetic regime."],"url":"http://arxiv.org/abs/2404.04592v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-06 10:16:33","title":"GLCM-Based Feature Combination for Extraction Model Optimization in Object Detection Using Machine Learning","abstract":"In the era of modern technology, object detection using the Gray Level Co-occurrence Matrix (GLCM) extraction method plays a crucial role in object recognition processes. It finds applications in real-time scenarios such as security surveillance and autonomous vehicle navigation, among others. Computational efficiency becomes a critical factor in achieving real-time object detection. Hence, there is a need for a detection model with low complexity and satisfactory accuracy. This research aims to enhance computational efficiency by selecting appropriate features within the GLCM framework. Two classification models, namely K-Nearest Neighbours (K-NN) and Support Vector Machine (SVM), were employed, with the results indicating that K-Nearest Neighbours (K-NN) outperforms SVM in terms of computational complexity. Specifically, K-NN, when utilizing a combination of Correlation, Energy, and Homogeneity features, achieves a 100% accuracy rate with low complexity. Moreover, when using a combination of Energy and Homogeneity features, K-NN attains an almost perfect accuracy level of 99.9889%, while maintaining low complexity. On the other hand, despite SVM achieving 100% accuracy in certain feature combinations, its high or very high complexity can pose challenges, particularly in real-time applications. Therefore, based on the trade-off between accuracy and complexity, the K-NN model with a combination of Correlation, Energy, and Homogeneity features emerges as a more suitable choice for real-time applications that demand high accuracy and low complexity. This research provides valuable insights for optimizing object detection in various applications requiring both high accuracy and rapid responsiveness.","sentences":["In the era of modern technology, object detection using the Gray Level Co-occurrence Matrix (GLCM) extraction method plays a crucial role in object recognition processes.","It finds applications in real-time scenarios such as security surveillance and autonomous vehicle navigation, among others.","Computational efficiency becomes a critical factor in achieving real-time object detection.","Hence, there is a need for a detection model with low complexity and satisfactory accuracy.","This research aims to enhance computational efficiency by selecting appropriate features within the GLCM framework.","Two classification models, namely K-Nearest Neighbours (K-NN) and Support Vector Machine (SVM), were employed, with the results indicating that K-Nearest Neighbours (K-NN) outperforms SVM in terms of computational complexity.","Specifically, K-NN, when utilizing a combination of Correlation, Energy, and Homogeneity features, achieves a 100% accuracy rate with low complexity.","Moreover, when using a combination of Energy and Homogeneity features, K-NN attains an almost perfect accuracy level of 99.9889%, while maintaining low complexity.","On the other hand, despite SVM achieving 100% accuracy in certain feature combinations, its high or very high complexity can pose challenges, particularly in real-time applications.","Therefore, based on the trade-off between accuracy and complexity, the K-NN model with a combination of Correlation, Energy, and Homogeneity features emerges as a more suitable choice for real-time applications that demand high accuracy and low complexity.","This research provides valuable insights for optimizing object detection in various applications requiring both high accuracy and rapid responsiveness."],"url":"http://arxiv.org/abs/2404.04578v1","category":"cs.CV"}
{"created":"2024-04-06 09:55:18","title":"Convex Reformulation of LMI-Based Distributed Controller Design with a Class of Non-Block-Diagonal Lyapunov Functions","abstract":"This study addresses a distributed state feedback controller design problem for continuous-time linear time-invariant systems by means of linear matrix inequalities (LMI). As the exact convexification is still open, the block-diagonal relaxation of Lyapunov functions has been prevalent despite its conservatism. In this work, we target a class of non-block-diagonal Lyapunov functions that has the same sparsity as distributed controllers. By leveraging a block-diagonal factorization of sparse matrices and Finsler's lemma, we first present a (nonlinear) matrix inequality for stabilizing distributed controllers with such Lyapunov functions, which boils down to a necessary and sufficient condition for such controllers if the sparsity pattern is chordal. As a relaxation of the inequality, we derive an LMI that completely covers the conventional relaxation and then provide analogous results for $H_\\infty$ control. Lastly, numerical examples underscore the efficacy of our results.","sentences":["This study addresses a distributed state feedback controller design problem for continuous-time linear time-invariant systems by means of linear matrix inequalities (LMI).","As the exact convexification is still open, the block-diagonal relaxation of Lyapunov functions has been prevalent despite its conservatism.","In this work, we target a class of non-block-diagonal Lyapunov functions that has the same sparsity as distributed controllers.","By leveraging a block-diagonal factorization of sparse matrices and Finsler's lemma, we first present a (nonlinear) matrix inequality for stabilizing distributed controllers with such Lyapunov functions, which boils down to a necessary and sufficient condition for such controllers if the sparsity pattern is chordal.","As a relaxation of the inequality, we derive an LMI that completely covers the conventional relaxation and then provide analogous results for $H_\\infty$ control.","Lastly, numerical examples underscore the efficacy of our results."],"url":"http://arxiv.org/abs/2404.04576v1","category":"math.OC"}
{"created":"2024-04-06 09:55:03","title":"To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO","abstract":"The temperature parameter plays a profound role during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models. A significant question remains: Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs\"? In this paper, we present a principled framework for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs. Our solution is composed of a novel learning framework with a robust loss underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration. TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model. It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models, e.g. Table 1. The code to reproduce the experimental results in this paper can be found at https://github.com/zhqiu/TempNet.","sentences":["The temperature parameter plays a profound role during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models.","Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models.","A significant question remains: Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs\"?","In this paper, we present a principled framework for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs.","Our solution is composed of a novel learning framework with a robust loss underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration.","TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model.","It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks.","Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models, e.g. Table 1.","The code to reproduce the experimental results in this paper can be found at https://github.com/zhqiu/TempNet."],"url":"http://arxiv.org/abs/2404.04575v1","category":"cs.LG"}
{"created":"2024-04-06 09:30:38","title":"Optimization of Lightweight Malware Detection Models For AIoT Devices","abstract":"Malware intrusion is problematic for Internet of Things (IoT) and Artificial Intelligence of Things (AIoT) devices as they often reside in an ecosystem of connected devices, such as a smart home. If any devices are infected, the whole ecosystem can be compromised. Although various Machine Learning (ML) models are deployed to detect malware and network intrusion, generally speaking, robust high-accuracy models tend to require resources not found in all IoT devices, compared to less robust models defined by weak learners. In order to combat this issue, Fadhilla proposed a meta-learner ensemble model comprised of less robust prediction results inherent with weak learner ML models to produce a highly robust meta-learning ensemble model. The main problem with the prior research is that it cannot be deployed in low-end AIoT devices due to the limited resources comprising processing power, storage, and memory (the required libraries quickly exhaust low-end AIoT devices' resources.) Hence, this research aims to optimize the proposed super learner meta-learning ensemble model to make it viable for low-end AIoT devices. We show the library and ML model memory requirements associated with each optimization stage and emphasize that optimization of current ML models is necessitated for low-end AIoT devices. Our results demonstrate that we can obtain similar accuracy and False Positive Rate (FPR) metrics from high-end AIoT devices running the derived ML model, with a lower inference duration and smaller memory footprint.","sentences":["Malware intrusion is problematic for Internet of Things (IoT) and Artificial Intelligence of Things (AIoT) devices as they often reside in an ecosystem of connected devices, such as a smart home.","If any devices are infected, the whole ecosystem can be compromised.","Although various Machine Learning (ML) models are deployed to detect malware and network intrusion, generally speaking, robust high-accuracy models tend to require resources not found in all IoT devices, compared to less robust models defined by weak learners.","In order to combat this issue, Fadhilla proposed a meta-learner ensemble model comprised of less robust prediction results inherent with weak learner ML models to produce a highly robust meta-learning ensemble model.","The main problem with the prior research is that it cannot be deployed in low-end AIoT devices due to the limited resources comprising processing power, storage, and memory (the required libraries quickly exhaust low-end AIoT devices' resources.)","Hence, this research aims to optimize the proposed super learner meta-learning ensemble model to make it viable for low-end AIoT devices.","We show the library and ML model memory requirements associated with each optimization stage and emphasize that optimization of current ML models is necessitated for low-end AIoT devices.","Our results demonstrate that we can obtain similar accuracy and False Positive Rate (FPR) metrics from high-end AIoT devices running the derived ML model, with a lower inference duration and smaller memory footprint."],"url":"http://arxiv.org/abs/2404.04567v1","category":"cs.CR"}
{"created":"2024-04-06 09:03:18","title":"Diffusion Time-step Curriculum for One Image to 3D Generation","abstract":"Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \\textbf{single} image. It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models. Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation. We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling. Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets. Codes and more generation demos will be released in https://github.com/yxymessi/DTC123.","sentences":["Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \\textbf{single} image.","It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models.","Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation.","We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling.","Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner.","Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets.","Codes and more generation demos will be released in https://github.com/yxymessi/DTC123."],"url":"http://arxiv.org/abs/2404.04562v1","category":"cs.CV"}
{"created":"2024-04-08 17:58:02","title":"Quintessential interpretation of the evolving dark energy in light of DESI","abstract":"The recent result of DESI in combination with other cosmological data shows evidence of the evolving dark energy parameterized by $w_0w_a$CDM model. We interpret this result in terms of a quintessential scalar field and demonstrate that it can explain the DESI result even though it becomes eventually phantom in the past. Relaxing the assumption on the functional form of the EoS parameter $w=w(a)$, we also discuss a more realistic quintessential model. The implications of the DESI result for Swampland conjectures, cosmic birefringence, and the fate of the Universe are discussed as well.","sentences":["The recent result of DESI in combination with other cosmological data shows evidence of the evolving dark energy parameterized by $w_0w_a$CDM model.","We interpret this result in terms of a quintessential scalar field and demonstrate that it can explain the DESI result even though it becomes eventually phantom in the past.","Relaxing the assumption on the functional form of the EoS parameter $w=w(a)$, we also discuss a more realistic quintessential model.","The implications of the DESI result for Swampland conjectures, cosmic birefringence, and the fate of the Universe are discussed as well."],"url":"http://arxiv.org/abs/2404.05722v1","category":"astro-ph.CO"}
{"created":"2024-04-08 17:46:16","title":"Walking Your Frog Fast in 4 LoC","abstract":"Given two polygonal curves, there are many ways to define a notion of similarity between them. One popular measure is the Fr\\'echet distance which has many desirable properties but is notoriously expensive to calculate, especially for non-trivial metrics. In 1994, Eiter and Mannila introduced the discrete Fr\\'echet distance which is much easier to implement and approximates the continuous Fr\\'echet distance with a quadratic runtime overhead. However, this algorithm relies on recursions and is not well suited for modern hardware. To that end, we introduce the Fast Fr\\'echet Distance algorithm, a recursion-free algorithm that calculates the discrete Fr\\'echet distance with a linear memory overhead and that can utilize modern hardware more effectively. We showcase an implementation with only four lines of code and present benchmarks of our algorithm running fast on modern CPUs and GPGPUs.","sentences":["Given two polygonal curves, there are many ways to define a notion of similarity between them.","One popular measure is the Fr\\'echet distance which has many desirable properties but is notoriously expensive to calculate, especially for non-trivial metrics.","In 1994, Eiter and Mannila introduced the discrete Fr\\'echet distance which is much easier to implement and approximates the continuous Fr\\'echet distance with a quadratic runtime overhead.","However, this algorithm relies on recursions and is not well suited for modern hardware.","To that end, we introduce the Fast Fr\\'echet Distance algorithm, a recursion-free algorithm that calculates the discrete Fr\\'echet distance with a linear memory overhead and that can utilize modern hardware more effectively.","We showcase an implementation with only four lines of code and present benchmarks of our algorithm running fast on modern CPUs and GPGPUs."],"url":"http://arxiv.org/abs/2404.05708v1","category":"cs.CG"}
{"created":"2024-04-08 17:38:02","title":"Novel collider signatures in the type-I 2HDM+$a$ model","abstract":"The 2HDM+$a$ model is one of the main models used in the interpretations of dark matter searches at the LHC. So far, all the 2HDM+$a$ benchmarks considered by the ATLAS and CMS experiments are limited to a type-II Yukawa sector, in which the Higgs bosons $A$, $H$, and $H^\\pm$ are all constrained to be mass-degenerate and heavier than around $600 \\, {\\rm GeV}$. In this work, we present the first detailed study of 2HDM+$a$ models with a type-I Yukawa sector, which, for moderate values of $\\tan\\beta$, lift the constraints from flavour physics, allowing the extra Higgs bosons to be even lighter than the $125 \\, {\\rm GeV}$ Higgs boson discovered at the LHC. We discuss several benchmarks where the $A$, $H$, and $H^\\pm$ states are not necessarily mass-degenerate and the signatures that arise in these models, some of which have not yet been explored at the LHC. We present the dominant channels in the studied benchmarks and the expected sensitivity in Run 2 data using truth-level analyses and discuss potential improvements in the experimental searches for Run 3.","sentences":["The 2HDM+$a$ model is one of the main models used in the interpretations of dark matter searches at the LHC.","So far, all the 2HDM+$a$ benchmarks considered by the ATLAS and CMS experiments are limited to a type-II Yukawa sector, in which the Higgs bosons $A$, $H$, and $H^\\pm$ are all constrained to be mass-degenerate and heavier than around $600 \\, {\\rm GeV}$.","In this work, we present the first detailed study of 2HDM+$a$ models with a type-I Yukawa sector, which, for moderate values of $\\tan\\beta$, lift the constraints from flavour physics, allowing the extra Higgs bosons to be even lighter than the $125 \\, {\\rm GeV}$ Higgs boson discovered at the LHC.","We discuss several benchmarks where the $A$, $H$, and $H^\\pm$ states are not necessarily mass-degenerate and the signatures that arise in these models, some of which have not yet been explored at the LHC.","We present the dominant channels in the studied benchmarks and the expected sensitivity in Run 2 data using truth-level analyses and discuss potential improvements in the experimental searches for Run 3."],"url":"http://arxiv.org/abs/2404.05704v1","category":"hep-ph"}
{"created":"2024-04-08 17:07:19","title":"Complete NLO QCD Corrections to $ZZ$ Production in Gluon Fusion","abstract":"We calculate the complete NLO QCD corrections to loop-induced $gg \\rightarrow ZZ$ production including full top-quark mass effects. The two-loop virtual corrections are obtained by combining analytic results for the massless, Higgs-mediated, and one-loop factorizable contributions with numerically computed amplitudes containing the top-quark mass. We show that the choice of subtraction scheme for the virtual contribution impacts the precision with which the virtual contribution must be evaluated in order to obtain sufficiently precise phenomenological predictions. For direct production through a massive top-quark loop, we observe that the relative NLO corrections are large. The direct massive and Higgs-mediated contributions individually increase relative to the massless production at high diboson invariant mass, but interfere destructively with each other. At the Large Hadron Collider, the NLO corrections to the gluon channel give a sizable contribution to the $pp \\to ZZ+X$ cross-section at N${}^3$LO.","sentences":["We calculate the complete NLO QCD corrections to loop-induced $gg \\rightarrow ZZ$ production including full top-quark mass effects.","The two-loop virtual corrections are obtained by combining analytic results for the massless, Higgs-mediated, and one-loop factorizable contributions with numerically computed amplitudes containing the top-quark mass.","We show that the choice of subtraction scheme for the virtual contribution impacts the precision with which the virtual contribution must be evaluated in order to obtain sufficiently precise phenomenological predictions.","For direct production through a massive top-quark loop, we observe that the relative NLO corrections are large.","The direct massive and Higgs-mediated contributions individually increase relative to the massless production at high diboson invariant mass, but interfere destructively with each other.","At the Large Hadron Collider, the NLO corrections to the gluon channel give a sizable contribution to the $pp \\to ZZ+X$ cross-section at N${}^3$LO."],"url":"http://arxiv.org/abs/2404.05684v1","category":"hep-ph"}
{"created":"2024-04-08 16:35:49","title":"The persistence of high altitude non-equilibrium diffuse ionized gas in simulations of star forming galaxies","abstract":"Widespread, high altitude, diffuse ionized gas with scale heights of around a kiloparsec is observed in the Milky Way and other star forming galaxies. Numerical radiation-magnetohydrodynamic simulations of a supernova-driven turbulent interstellar medium show that gas can be driven to high altitudes above the galactic midplane, but the degree of ionization is often less than inferred from observations. For computational expediency, ionizing radiation from massive stars is often included as a post-processing step assuming ionization equilibrium. We extend our simulations of a Milky Way-like interstellar medium to include the combined effect of supernovae and photoionization feedback from midplane OB stars and a population of hot evolved low mass stars. The diffuse ionized gas has densities below 0.1 ${\\rm cm^{-3}}$, so recombination timescales can exceed millions of years. Our simulations now follow the time-dependent ionization and recombination of low density gas. The long recombination timescales result in diffuse ionized gas that persists at large altitudes long after the deaths of massive stars that produce the vast majority of the ionized gas. The diffuse ionized gas does not exhibit the large variability inherent in simulations that adopt ionization equilibrium. The vertical distribution of neutral and ionized gas is close to what is observed in the Milky Way. The volume filling factor of ionised gas increases with altitude resulting in the scale height of free electrons being larger than that inferred from H$\\alpha$ emission, thus reconciling the observations of ionized gas made in H$\\alpha$ and from pulsar dispersion measurements.","sentences":["Widespread, high altitude, diffuse ionized gas with scale heights of around a kiloparsec is observed in the Milky Way and other star forming galaxies.","Numerical radiation-magnetohydrodynamic simulations of a supernova-driven turbulent interstellar medium show that gas can be driven to high altitudes above the galactic midplane, but the degree of ionization is often less than inferred from observations.","For computational expediency, ionizing radiation from massive stars is often included as a post-processing step assuming ionization equilibrium.","We extend our simulations of a Milky Way-like interstellar medium to include the combined effect of supernovae and photoionization feedback from midplane OB stars and a population of hot evolved low mass stars.","The diffuse ionized gas has densities below 0.1 ${\\rm cm^{-3}}$, so recombination timescales can exceed millions of years.","Our simulations now follow the time-dependent ionization and recombination of low density gas.","The long recombination timescales result in diffuse ionized gas that persists at large altitudes long after the deaths of massive stars that produce the vast majority of the ionized gas.","The diffuse ionized gas does not exhibit the large variability inherent in simulations that adopt ionization equilibrium.","The vertical distribution of neutral and ionized gas is close to what is observed in the Milky Way.","The volume filling factor of ionised gas increases with altitude resulting in the scale height of free electrons being larger than that inferred from H$\\alpha$ emission, thus reconciling the observations of ionized gas made in H$\\alpha$ and from pulsar dispersion measurements."],"url":"http://arxiv.org/abs/2404.05651v1","category":"astro-ph.GA"}
{"created":"2024-04-08 16:33:11","title":"Multi-wavelength pulse profiles from the force-free neutron star magnetosphere","abstract":"In this paper, we compute a full atlas of radio, X-ray and $\\gamma$-ray pulse profiles relying on the force-free magnetosphere model. Our goal is to use such data bank of multi-wavelength profiles to fit a substantial number of radio-loud $\\gamma$-ray pulsars also detected in non-thermal X-rays to decipher the X-ray radiation mechanism and sites. Using results from the third $\\gamma$-ray pulsar catalogue (3PC), we investigate the statistical properties of this population. We assume that radio emission emanates from field lines rooted to the polar caps, at varying height above the surface, close to the surface, at an altitude about 5-10% of the light-cylinder radius $r_{\\rm L}$. The X-ray photons are produced in the separatrix region within the magnetosphere, i.e. the current sheet formed by the jump from closed to open magnetic field lines. We allow for substantial variations in emission height. The $\\gamma$-ray are produced within the current sheet of the striped wind, outside the light-cylinder. A comprehensive set of light-curves in radio, X-ray and $\\gamma$-ray has been computed. Based on only geometric considerations about magnetic obliquity, line of sight inclination and radio beam cone opening angle, pulsars can be classified as radio-loud or quiet and $\\gamma$-ray loud or quiet. We found that the 3PC sample is compatible with an isotropic distribution of obliquity and line of sight. The atlases constructed in this work are the fundamental tools to explore individual pulsars and fit their multi-wavelength pulse profiles in order to constrain their magnetic topology, the emission sites as well as the observer line of sight.","sentences":["In this paper, we compute a full atlas of radio, X-ray and $\\gamma$-ray pulse profiles relying on the force-free magnetosphere model.","Our goal is to use such data bank of multi-wavelength profiles to fit a substantial number of radio-loud $\\gamma$-ray pulsars also detected in non-thermal X-rays to decipher the X-ray radiation mechanism and sites.","Using results from the third $\\gamma$-ray pulsar catalogue (3PC), we investigate the statistical properties of this population.","We assume that radio emission emanates from field lines rooted to the polar caps, at varying height above the surface, close to the surface, at an altitude about 5-10% of the light-cylinder radius $r_{\\rm L}$. The X-ray photons are produced in the separatrix region within the magnetosphere, i.e. the current sheet formed by the jump from closed to open magnetic field lines.","We allow for substantial variations in emission height.","The $\\gamma$-ray are produced within the current sheet of the striped wind, outside the light-cylinder.","A comprehensive set of light-curves in radio, X-ray and $\\gamma$-ray has been computed.","Based on only geometric considerations about magnetic obliquity, line of sight inclination and radio beam cone opening angle, pulsars can be classified as radio-loud or quiet and $\\gamma$-ray loud or quiet.","We found that the 3PC sample is compatible with an isotropic distribution of obliquity and line of sight.","The atlases constructed in this work are the fundamental tools to explore individual pulsars and fit their multi-wavelength pulse profiles in order to constrain their magnetic topology, the emission sites as well as the observer line of sight."],"url":"http://arxiv.org/abs/2404.05647v1","category":"astro-ph.HE"}
{"created":"2024-04-08 16:13:22","title":"Oblique photons, plasmons, and current-plasmons in relativistic plasmas and their topological implications","abstract":"Photons in vacuum are transverse in any inertial frame; longitudinal photons only exist virtually. By developing a manifestly covariant theory for electromagnetic excitations in relativistic plasmas and applying Wigner's little group method for elementary particle classifications, we show that photons in plasmas are neither transverse nor longitudinal; they are oblique. Plasmons are electromagnetic and oblique as well. The Lorentz invariant characteristics that distinguishes photons and plasmons is covariant compressibility. The manifestly covariant theory predicts the existence of the current-plasmon, a third oblique, electromagnetic eigenmode, and it also enables the study of photon topology in plasmas. Plasmas remove the photon's Dirac point in vacuum by giving it an effective mass, but create a tilted Dirac-Weyl point by reviving the virtual longitudinal photon.","sentences":["Photons in vacuum are transverse in any inertial frame; longitudinal photons only exist virtually.","By developing a manifestly covariant theory for electromagnetic excitations in relativistic plasmas and applying Wigner's little group method for elementary particle classifications, we show that photons in plasmas are neither transverse nor longitudinal; they are oblique.","Plasmons are electromagnetic and oblique as well.","The Lorentz invariant characteristics that distinguishes photons and plasmons is covariant compressibility.","The manifestly covariant theory predicts the existence of the current-plasmon, a third oblique, electromagnetic eigenmode, and it also enables the study of photon topology in plasmas.","Plasmas remove the photon's Dirac point in vacuum by giving it an effective mass, but create a tilted Dirac-Weyl point by reviving the virtual longitudinal photon."],"url":"http://arxiv.org/abs/2404.05636v1","category":"physics.plasm-ph"}
{"created":"2024-04-08 16:00:07","title":"eDIG-CHANGES II: Project Design and Initial Results on NGC 3556","abstract":"The extraplanar diffuse ionized gas (eDIG) represents ionized gases traced by optical/UV lines beyond the stellar extent of galaxies. We herein introduce a novel multi-slit narrow-band spectroscopy method to conduct spatially resolved spectroscopy of the eDIG around a sample of nearby edge-on disk galaxies (eDIG-CHANGES). In this paper, we introduce the project design and major scientific goals, as well as a pilot study of NGC 3556 (M108). The eDIG is detected to a vertical extent of a few kpc above the disk, comparable to the X-ray and radio images. We do not see significant vertical variation of the [N II]/H$\\alpha$ line ratio. A rough examination of the pressure balance between different circum-galactic medium (CGM) phases indicates the magnetic field is in a rough pressure balance with the X-ray emitting hot gas, and may play an important role in the global motion of both the eDIG and the hot gas in the lower halo. At the location of an HST/COS observed UV bright background AGN $\\sim29\\rm~kpc$ from the center of NGC 3556, the magnetic pressure is much lower than that of the hot gas and the ionized gas traced by UV absorption lines, although the extrapolation of the pressure profiles may cause some biases in this comparison. By comparing the position-velocity diagrams of the optical and CO lines, we also find the dynamics of the two gas phases are consistent with each other, with no evidence of a global inflow/outflow and a maximum rotation velocity of $\\sim150\\rm~km~s^{-1}$.","sentences":["The extraplanar diffuse ionized gas (eDIG) represents ionized gases traced by optical/UV lines beyond the stellar extent of galaxies.","We herein introduce a novel multi-slit narrow-band spectroscopy method to conduct spatially resolved spectroscopy of the eDIG around a sample of nearby edge-on disk galaxies (eDIG-CHANGES).","In this paper, we introduce the project design and major scientific goals, as well as a pilot study of NGC 3556 (M108).","The eDIG is detected to a vertical extent of a few kpc above the disk, comparable to the X-ray and radio images.","We do not see significant vertical variation of the [N II]/H$\\alpha$ line ratio.","A rough examination of the pressure balance between different circum-galactic medium (CGM) phases indicates the magnetic field is in a rough pressure balance with the X-ray emitting hot gas, and may play an important role in the global motion of both the eDIG and the hot gas in the lower halo.","At the location of an HST/COS observed UV bright background AGN $\\sim29\\rm~kpc$ from the center of NGC 3556, the magnetic pressure is much lower than that of the hot gas and the ionized gas traced by UV absorption lines, although the extrapolation of the pressure profiles may cause some biases in this comparison.","By comparing the position-velocity diagrams of the optical and CO lines, we also find the dynamics of the two gas phases are consistent with each other, with no evidence of a global inflow/outflow and a maximum rotation velocity of $\\sim150\\rm~km~s^{-1}$."],"url":"http://arxiv.org/abs/2404.05628v1","category":"astro-ph.GA"}
{"created":"2024-04-08 15:15:53","title":"A Comparative Study of the Ground State Transitions of CO and [C I] as Molecular Gas Tracers at High Redshift","abstract":"The CO(1--0) and [\\ion{C}{1}](1--0) emission lines are well-established tracers of cold molecular gas mass in local galaxies. At high redshift, where the interstellar medium (ISM) is likely to be denser, there have been limited direct comparisons of both ground state transitions. Here we present a study of CO(1--0) and [\\ion{C}{1}](1--0) emission in a sample of 20 unlensed dusty, star-forming galaxies at $z=2-5$. The CO(1--0)/[\\ion{C}{1}](1--0) ratio is constant up to at least $z=5$, supporting the use of [CI](1-0) as a gas mass tracer. PDR modelling of the available data indicates a median H$_2$ density of log$(n~[$cm$^{-3}])=4.7\\pm0.2$, and UV radiation field log$(G_{\\mathrm{UV}} [G$_0$])=3.2\\pm0.2$. We use the CO(1--0), [\\ion{C}{1}](1--0) and 3mm dust continuum measurements to cross--calibrate the respective gas mass conversion factors, finding no dependence of these factors on either redshift or infrared luminosity. Assuming a variable CO conversion factor then implies [\\ion{C}{1}] and dust conversion factors that differ from canonically assumed values but are consistent with the solar/super-solar metallicities expected for our sources. Radiative transfer modelling shows that the warmer CMB at high redshift can significantly affect the [\\ion{C}{1}] as well as CO emission, which can change the derived molecular gas masses by up to 70\\% for the coldest kinetic gas temperatures expected. Nevertheless, we show that the magnitude of the effect on the ratio of the tracers is within the known scatter of the $L'_\\mathrm{CO}-L'_\\mathrm{[CI]}$ relation. Further determining the absolute decrease of individual line intensities will require well-sampled spectral line energy distributions (SLEDs) to model the gas excitation conditions in more detail.","sentences":["The CO(1--0) and [\\ion{C}{1}](1--0) emission lines are well-established tracers of cold molecular gas mass in local galaxies.","At high redshift, where the interstellar medium (ISM) is likely to be denser, there have been limited direct comparisons of both ground state transitions.","Here we present a study of CO(1--0) and [\\ion{C}{1}](1--0) emission in a sample of 20 unlensed dusty, star-forming galaxies at $z=2-5$. The CO(1--0)/[\\ion{C}{1}](1--0) ratio is constant up to at least $z=5$, supporting the use of [CI](1-0) as a gas mass tracer.","PDR modelling of the available data indicates a median H$_2$ density of log$(n~[$cm$^{-3}])=4.7\\pm0.2$, and UV radiation field log$(G_{\\mathrm{UV}}","[G$_0$])=3.2\\pm0.2$. We use the CO(1--0), [\\ion{C}{1}](1--0) and 3mm dust continuum measurements to cross--calibrate the respective gas mass conversion factors, finding no dependence of these factors on either redshift or infrared luminosity.","Assuming a variable CO conversion factor then implies [\\ion{C}{1}] and dust conversion factors that differ from canonically assumed values but are consistent with the solar/super-solar metallicities expected for our sources.","Radiative transfer modelling shows that the warmer CMB at high redshift can significantly affect the [\\ion{C}{1}] as well as CO emission, which can change the derived molecular gas masses by up to 70\\% for the coldest kinetic gas temperatures expected.","Nevertheless, we show that the magnitude of the effect on the ratio of the tracers is within the known scatter of the $L'_\\mathrm{CO}-L'_\\mathrm{[CI]}$ relation.","Further determining the absolute decrease of individual line intensities will require well-sampled spectral line energy distributions (SLEDs) to model the gas excitation conditions in more detail."],"url":"http://arxiv.org/abs/2404.05596v1","category":"astro-ph.GA"}
{"created":"2024-04-08 15:03:56","title":"Low energy limit from high energy expansion in mass gapped theory","abstract":"We present a method to extract the low energy behavior of physical observables from their high energy expansions, systematically calculable via the operator product expansion (OPE), in asymptotically free and mass-gapped theories. By applying the inverse Laplace transform to correlation functions, their analytic structure is modified such that low-energy information connects with high energy expansions. Furthermore, this transformation alleviates the renormalon problem, enabling a more straightforward application of the OPE compared to the OPE before the transformation. We demonstrate that the low energy limit of correlation functions can be accurately extracted using the OPE in the two dimensional $O(N)$ nonlinear $\\sigma$ model, serving as a first testing ground.","sentences":["We present a method to extract the low energy behavior of physical observables from their high energy expansions, systematically calculable via the operator product expansion (OPE), in asymptotically free and mass-gapped theories.","By applying the inverse Laplace transform to correlation functions, their analytic structure is modified such that low-energy information connects with high energy expansions.","Furthermore, this transformation alleviates the renormalon problem, enabling a more straightforward application of the OPE compared to the OPE before the transformation.","We demonstrate that the low energy limit of correlation functions can be accurately extracted using the OPE in the two dimensional $O(N)$ nonlinear $\\sigma$ model, serving as a first testing ground."],"url":"http://arxiv.org/abs/2404.05589v1","category":"hep-th"}
{"created":"2024-04-08 14:36:44","title":"Giant photocaloric effects across a vast temperature range in ferroelectric perovskites","abstract":"Solid-state cooling presents an energy-efficient and environmentally friendly alternative to traditional refrigeration technologies that rely on thermodynamic cycles involving greenhouse gases. However, conventional caloric effects face several challenges that impede their practical application in refrigeration devices. Firstly, operational temperature conditions must align closely with zero-field phase transition points; otherwise, the required driving fields become excessively large. But phase transitions occur infrequently near room temperature. Additionally, caloric effects typically exhibit strong temperature dependence and are sizeable only within relatively narrow temperature ranges. In this study, we employ first-principles simulation methods to demonstrate that light-driven phase transitions in polar oxide perovskites have the potential to overcome such limitations. Specifically, for the prototypical ferroelectric KNbO$_{3}$ we illustrate the existence of giant photocaloric effects induced by light absorption ($\\Delta S_{\\rm PC} \\sim 100$~J~K$^{-1}$~kg$^{-1}$ and $\\Delta T_{\\rm PC} \\sim 10$~K) across a vast temperature range of several hundred Kelvin, encompassing room temperature. These findings are expected to be generalizable to other materials exhibiting similar polar behavior.","sentences":["Solid-state cooling presents an energy-efficient and environmentally friendly alternative to traditional refrigeration technologies that rely on thermodynamic cycles involving greenhouse gases.","However, conventional caloric effects face several challenges that impede their practical application in refrigeration devices.","Firstly, operational temperature conditions must align closely with zero-field phase transition points; otherwise, the required driving fields become excessively large.","But phase transitions occur infrequently near room temperature.","Additionally, caloric effects typically exhibit strong temperature dependence and are sizeable only within relatively narrow temperature ranges.","In this study, we employ first-principles simulation methods to demonstrate that light-driven phase transitions in polar oxide perovskites have the potential to overcome such limitations.","Specifically, for the prototypical ferroelectric KNbO$_{3}$ we illustrate the existence of giant photocaloric effects induced by light absorption ($\\Delta S_{\\rm PC} \\sim 100$~J~K$^{-1}$~kg$^{-1}$ and $\\Delta T_{\\rm PC} \\sim 10$~K) across a vast temperature range of several hundred Kelvin, encompassing room temperature.","These findings are expected to be generalizable to other materials exhibiting similar polar behavior."],"url":"http://arxiv.org/abs/2404.05562v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 14:22:12","title":"Maximum likelihood estimation in continuous affine Volterra processes in the ergodic regime","abstract":"We study statistical inference of the drift parameters for the Volterra Ornstein-Uhlenbeck process on R and the Volterra Cox-Ingersoll-Ross process on R+ in the ergodic regime. For continuous-time observations, we derive the corresponding maximum likelihood estimators and show that they are strongly consistent and asymptotically normal locally uniformly in the parameters. For the case of discrete high-frequency observations, we prove similar results by discretization of the continuous-time maximum likelihood estimator. Finally, for discrete low-frequency observations, we show that the method of moments is consistent. Our proofs are crucially based on the law of large numbers. To prove the latter, we introduce the notion of asymptotic independence which has the advantage that it can be effectively verified by the affine transformation formula and convergence of the characteristic function. As a side product of our results, we show that the stationary processes are ergodic.","sentences":["We study statistical inference of the drift parameters for the Volterra Ornstein-Uhlenbeck process on R and the Volterra Cox-Ingersoll-Ross process on R+ in the ergodic regime.","For continuous-time observations, we derive the corresponding maximum likelihood estimators and show that they are strongly consistent and asymptotically normal locally uniformly in the parameters.","For the case of discrete high-frequency observations, we prove similar results by discretization of the continuous-time maximum likelihood estimator.","Finally, for discrete low-frequency observations, we show that the method of moments is consistent.","Our proofs are crucially based on the law of large numbers.","To prove the latter, we introduce the notion of asymptotic independence which has the advantage that it can be effectively verified by the affine transformation formula and convergence of the characteristic function.","As a side product of our results, we show that the stationary processes are ergodic."],"url":"http://arxiv.org/abs/2404.05554v1","category":"math.ST"}
{"created":"2024-04-08 13:50:26","title":"ALMA Spectroscopy of Europa: A Search for Active Plumes","abstract":"The subsurface ocean of Europa is a high priority target in the search for extraterrestrial life, but direct investigations are hindered by the presence of a thick, exterior ice shell. Here we present spectral line and continuum maps of Europa obtained over four epochs in May-June 2021 using the Atacama Large Millimeter/submillimeter Array (ALMA), to search for molecular emission from atmospheric plumes, with the aim of investigating subsurface processes. Using a 3D physical model, we obtained upper limits for the plume abundances of HCN, H$_2$CO, SO$_2$ and CH$_3$OH. If active plume(s) were present, they contained very low abundances of these molecules. Assuming a total gas production rate of $10^{29}$ s$^{-1}$, our H$_2$CO abundance upper limit of $<0.016$\\% is more than an order of magnitude less than measured in the Enceladus plume by the Cassini spacecraft, implying a possible chemical difference between the plume source materials for these two icy moons.","sentences":["The subsurface ocean of Europa is a high priority target in the search for extraterrestrial life, but direct investigations are hindered by the presence of a thick, exterior ice shell.","Here we present spectral line and continuum maps of Europa obtained over four epochs in May-June 2021 using the Atacama Large Millimeter/submillimeter Array (ALMA), to search for molecular emission from atmospheric plumes, with the aim of investigating subsurface processes.","Using a 3D physical model, we obtained upper limits for the plume abundances of HCN, H$_2$CO, SO$_2$ and CH$_3$OH.","If active plume(s) were present, they contained very low abundances of these molecules.","Assuming a total gas production rate of $10^{29}$ s$^{-1}$, our H$_2$CO abundance upper limit of $<0.016$\\% is more than an order of magnitude less than measured in the Enceladus plume by the Cassini spacecraft, implying a possible chemical difference between the plume source materials for these two icy moons."],"url":"http://arxiv.org/abs/2404.05525v1","category":"astro-ph.EP"}
{"created":"2024-04-08 12:31:46","title":"Raman scattering by carbon nanotubes coupled to quantum dots via dipolar excitonic interaction","abstract":"The dipole-dipole interactions between excitons are of paramount importance in the nanoscale structures. When two excitons are placed together they can exchange the energy can manifest in the resonant Raman cross sections. We provide theoretical framework for such effects by combining the coupled oscillator model and perturbation theory. We apply this theory to a hybrid film comprising semiconducting quantum dots and metallic carbon nanotubes. The quantum dots exciton has a fixed energy, while the nanotube resonances span across a larger range from 1.7 to \\SI{1.93}{eV}. We acquire the resonant Raman profiles of the pristine nanotubes and hybrids and find a relative shift between them. The shift direction depends on the relative energies between the CNT and QD exciton energies, as predicted by our theory.","sentences":["The dipole-dipole interactions between excitons are of paramount importance in the nanoscale structures.","When two excitons are placed together they can exchange the energy can manifest in the resonant Raman cross sections.","We provide theoretical framework for such effects by combining the coupled oscillator model and perturbation theory.","We apply this theory to a hybrid film comprising semiconducting quantum dots and metallic carbon nanotubes.","The quantum dots exciton has a fixed energy, while the nanotube resonances span across a larger range from 1.7 to \\SI{1.93}{eV}.","We acquire the resonant Raman profiles of the pristine nanotubes and hybrids and find a relative shift between them.","The shift direction depends on the relative energies between the CNT and QD exciton energies, as predicted by our theory."],"url":"http://arxiv.org/abs/2404.05450v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 11:58:09","title":"New ideas on the formation and astrophysical detection of primordial black holes","abstract":"Recently, a number of novel scenarios for primordial black hole (PBH) formation have been discovered. Some of them require very minimal new physics, some others require no new ingredients besides those already present in commonly considered models, such as supersymmetry. At the same time, new strategies have emerged for detection of PBHs. For example, an observation of an orphan kilonova unaccompanied by the gravitational waves signal of merging neutron stars, but associated with a fast radio burst, could be a smoking gun of PBH dark matter. We review some new ideas for PBH formation and detection.","sentences":["Recently, a number of novel scenarios for primordial black hole (PBH) formation have been discovered.","Some of them require very minimal new physics, some others require no new ingredients besides those already present in commonly considered models, such as supersymmetry.","At the same time, new strategies have emerged for detection of PBHs.","For example, an observation of an orphan kilonova unaccompanied by the gravitational waves signal of merging neutron stars, but associated with a fast radio burst, could be a smoking gun of PBH dark matter.","We review some new ideas for PBH formation and detection."],"url":"http://arxiv.org/abs/2404.05430v1","category":"astro-ph.CO"}
{"created":"2024-04-08 11:31:17","title":"Valley edge states as bound states in the continuum","abstract":"Bound states in the continuum (BICs) are spatially localized states with energy embedded in the continuum spectrum of extended states. The combination of BICs physics and nontrivial band topology theory giving rise to topological BICs, which are robust against disorders and meanwhile of the merit of conventional BICs, is attracting wide attention recently. Here, we report valley edge states as topological BICs, which appear at domain wall between two distinct valley topological phases. The robustness of such BICs is demonstrated. The simulations and experiments show great agreement. Our findings of valley related topological BICs shed light on both BICs and valley physics, and may foster innovative applications of topological acoustic devices.","sentences":["Bound states in the continuum (BICs) are spatially localized states with energy embedded in the continuum spectrum of extended states.","The combination of BICs physics and nontrivial band topology theory giving rise to topological BICs, which are robust against disorders and meanwhile of the merit of conventional BICs, is attracting wide attention recently.","Here, we report valley edge states as topological BICs, which appear at domain wall between two distinct valley topological phases.","The robustness of such BICs is demonstrated.","The simulations and experiments show great agreement.","Our findings of valley related topological BICs shed light on both BICs and valley physics, and may foster innovative applications of topological acoustic devices."],"url":"http://arxiv.org/abs/2404.05412v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 09:43:53","title":"Search for Neutrino Emission from GRB 221009A using the KM3NeT ARCA and ORCA detectors","abstract":"Gamma-ray bursts are promising candidate sources of high-energy astrophysical neutrinos. The recent GRB 221009A event, identified as the brightest gamma-ray burst ever detected, provides a unique opportunity to investigate hadronic emissions involving neutrinos. The KM3NeT undersea neutrino detectors participated in the worldwide follow-up effort triggered by the event, searching for neutrino events. In this letter, we summarize subsequent searches, in a wide energy range from MeV up to a few PeVs. No neutrino events are found in any of the searches performed. Upper limits on the neutrino emission associated with GRB 221009A are computed.","sentences":["Gamma-ray bursts are promising candidate sources of high-energy astrophysical neutrinos.","The recent GRB 221009A event, identified as the brightest gamma-ray burst ever detected, provides a unique opportunity to investigate hadronic emissions involving neutrinos.","The KM3NeT undersea neutrino detectors participated in the worldwide follow-up effort triggered by the event, searching for neutrino events.","In this letter, we summarize subsequent searches, in a wide energy range from MeV up to a few PeVs.","No neutrino events are found in any of the searches performed.","Upper limits on the neutrino emission associated with GRB 221009A are computed."],"url":"http://arxiv.org/abs/2404.05354v1","category":"astro-ph.HE"}
{"created":"2024-04-08 09:25:11","title":"Nanomolecular OLED Pixelization Enabling Electroluminescent Metasurfaces","abstract":"Miniaturization of light-emitting diodes (LEDs) can enable high-resolution augmented and virtual reality displays and on-chip light sources for ultra-broadband chiplet communication. However, unlike silicon scaling in electronic integrated circuits, patterning of inorganic III-V semiconductors in LEDs considerably compromises device efficiencies at submicrometer scales. Here, we present the scalable fabrication of nanoscale organic LEDs (nano-OLEDs), with the highest array density (>84,000 pixels per inch) and the smallest pixel size (~100 nm) ever reported to date. Direct nanomolecular patterning of organic semiconductors is realized by self-aligned evaporation through nanoapertures fabricated on a free-standing silicon nitride film adhering to the substrate. The average external quantum efficiencies (EQEs) extracted from a nano-OLED device of more than 4 megapixels reach up to 10%. At the subwavelength scale, individual pixels act as electroluminescent meta-atoms forming metasurfaces that directly convert electricity into modulated light. The diffractive coupling between nano-pixels enables control over the far-field emission properties, including directionality and polarization. The results presented here lay the foundation for bright surface light sources of dimension smaller than the Abbe diffraction limit, offering new technological platforms for super-resolution imaging, spectroscopy, sensing, and hybrid integrated photonics.","sentences":["Miniaturization of light-emitting diodes (LEDs) can enable high-resolution augmented and virtual reality displays and on-chip light sources for ultra-broadband chiplet communication.","However, unlike silicon scaling in electronic integrated circuits, patterning of inorganic III-V semiconductors in LEDs considerably compromises device efficiencies at submicrometer scales.","Here, we present the scalable fabrication of nanoscale organic LEDs (nano-OLEDs), with the highest array density (>84,000 pixels per inch) and the smallest pixel size (~100 nm) ever reported to date.","Direct nanomolecular patterning of organic semiconductors is realized by self-aligned evaporation through nanoapertures fabricated on a free-standing silicon nitride film adhering to the substrate.","The average external quantum efficiencies (EQEs) extracted from a nano-OLED device of more than 4 megapixels reach up to 10%.","At the subwavelength scale, individual pixels act as electroluminescent meta-atoms forming metasurfaces that directly convert electricity into modulated light.","The diffractive coupling between nano-pixels enables control over the far-field emission properties, including directionality and polarization.","The results presented here lay the foundation for bright surface light sources of dimension smaller than the Abbe diffraction limit, offering new technological platforms for super-resolution imaging, spectroscopy, sensing, and hybrid integrated photonics."],"url":"http://arxiv.org/abs/2404.05336v1","category":"physics.optics"}
{"created":"2024-04-08 09:15:43","title":"In silico bioactivity prediction of proteins interacting with graphene-based nanomaterials guides rational design of biosensor","abstract":"Graphene based nanomaterials have attracted significant attention for their potentials in biomedical and biotechnology applications in recent years, owing to the outstanding physical and chemical properties. However, the interaction mechanism and impact on biological activity of macro and micro biomolecules still require more concerns and further research in order to enhance their applicability in biosensors, etc. Herein, an integrated method has been developed to predict the protein bioactivity performance when interacting with nanomaterials for protein based biosensor. Molecular dynamics simulation and molecular docking technique were consolidated to investigate several nanomaterials C60 fullerene, single walled carbon nanotube, pristine graphene and graphene oxide, and their effect when interacting with protein. The adsorption behavior, secondary structure changes and protein bioactivity changes were simulated, and the results of protein activity simulation were verified in combination with atomic force spectrum, circular dichroism spectrum fluorescence and electrochemical experiments. The best quantification alignment between bioactivity obtained by simulation and experiment measurements was further explored. The two proteins, RNase A and Exonuclease III, were regarded as analysis model for the proof of concept, and the prediction accuracy of protein bioactivty could reach up to 0.98.","sentences":["Graphene based nanomaterials have attracted significant attention for their potentials in biomedical and biotechnology applications in recent years, owing to the outstanding physical and chemical properties.","However, the interaction mechanism and impact on biological activity of macro and micro biomolecules still require more concerns and further research in order to enhance their applicability in biosensors, etc.","Herein, an integrated method has been developed to predict the protein bioactivity performance when interacting with nanomaterials for protein based biosensor.","Molecular dynamics simulation and molecular docking technique were consolidated to investigate several nanomaterials C60 fullerene, single walled carbon nanotube, pristine graphene and graphene oxide, and their effect when interacting with protein.","The adsorption behavior, secondary structure changes and protein bioactivity changes were simulated, and the results of protein activity simulation were verified in combination with atomic force spectrum, circular dichroism spectrum fluorescence and electrochemical experiments.","The best quantification alignment between bioactivity obtained by simulation and experiment measurements was further explored.","The two proteins, RNase A and Exonuclease III, were regarded as analysis model for the proof of concept, and the prediction accuracy of protein bioactivty could reach up to 0.98."],"url":"http://arxiv.org/abs/2404.05329v1","category":"q-bio.BM"}
{"created":"2024-04-08 09:15:18","title":"Strong decays of the vector tetraquark states with the masses about $4.5\\,\\rm{GeV}$ via the QCD sum rules","abstract":"We suppose that there exist three vector hidden-charm tetraquark states with the $J^{PC}=1^{--}$ at the energy about $4.5\\,\\rm{GeV}$, and investigate the two-body strong decays systematically. We obtain thirty QCD sum rules for the hadronic coupling constants based on rigorous quark-hadron duality, then obtain the partial decay widths, therefore the total widths approximately, which are compatible with the experimental data of the $Y(4500)$ from the BESIII collaboration. The $Y(4500)$ may be one vector tetraquark state having three main Fock components, or consists of three different vector tetraquark states. We can search for the typical decays $ Y \\to \\frac{\\bar{D}^0_1D^{0}-\\bar{D}^{0}D^{0}_1}{\\sqrt{2}}$,   $\\frac{\\bar{D}^-_1D^{+}-\\bar{D}^{-}D^{+}_1}{\\sqrt{2}}$, $\\frac{\\bar{D}^0_0D^{*0}-\\bar{D}^{*0}D^{0}_0}{\\sqrt{2}}$, $ \\frac{\\bar{D}^-_0D^{*+}-\\bar{D}^{*-}D^{+}_0}{\\sqrt{2}}$, $\\eta_c\\omega$, $J/\\psi\\omega$ to diagnose the nature of the $Y(4500)$.","sentences":["We suppose that there exist three vector hidden-charm tetraquark states with the $J^{PC}=1^{--}$ at the energy about $4.5\\,\\rm{GeV}$, and investigate the two-body strong decays systematically.","We obtain thirty QCD sum rules for the hadronic coupling constants based on rigorous quark-hadron duality, then obtain the partial decay widths, therefore the total widths approximately, which are compatible with the experimental data of the $Y(4500)$ from the BESIII collaboration.","The $Y(4500)$ may be one vector tetraquark state having three main Fock components, or consists of three different vector tetraquark states.","We can search for the typical decays $ Y \\to \\frac{\\bar{D}^0_1D^{0}-\\bar{D}^{0}D^{0}_1}{\\sqrt{2}}$,   $\\frac{\\bar{D}^-_1D^{+}-\\bar{D}^{-}D^{+}_1}{\\sqrt{2}}$, $\\frac{\\bar{D}^0_0D^{*0}-\\bar{D}^{*0}D^{0}_0}{\\sqrt{2}}$, $ \\frac{\\bar{D}^-_0D^{*+}-\\bar{D}^{*-}D^{+}_0}{\\sqrt{2}}$, $\\eta_c\\omega$, $J/\\psi\\omega$ to diagnose the nature of the $Y(4500)$."],"url":"http://arxiv.org/abs/2404.05328v1","category":"hep-ph"}
{"created":"2024-04-08 08:25:00","title":"T-odd Parton Distribution Functions and Azimuthal Anisotropy at High Transverse Momentum in $p$-$p$ and $p$-$A$ Collisions","abstract":"Various azimuthal anisotropies ($v_1, v_2, v_3, v_4$), at high transverse momentum (high-$p_T$), are shown to arise from the asymmetric scattering of transverse polarized quarks and gluons, arising from unpolarized nucleons (the Boer-Mulders' effect) and resulting in unpolarized hadrons (the Collins effect). Combined with the asymmetric scattering of partons from polarization independent but transverse momentum dependent (TMD) distributions, we obtain a possible mechanism to understand the azimuthal anisotropy of hadrons at large transverse momentum observed in $p$-$p$ collisions. Constraining the ratio of polarization dependent TMD distributions to polarization independent distributions by comparing with the data from $p$-$p$ collisions, we find that a simple $A^{1/3}$ enhancement of the intrinsic transverse momentum $(k_\\perp^2)$ distribution, of the initial state partons from the proton, straightforwardly yields the azimuthal anisotropy at high $p_T$ in $p$-$A$ collisions.","sentences":["Various azimuthal anisotropies ($v_1, v_2, v_3, v_4$), at high transverse momentum (high-$p_T$), are shown to arise from the asymmetric scattering of transverse polarized quarks and gluons, arising from unpolarized nucleons (the Boer-Mulders' effect) and resulting in unpolarized hadrons (the Collins effect).","Combined with the asymmetric scattering of partons from polarization independent but transverse momentum dependent (TMD) distributions, we obtain a possible mechanism to understand the azimuthal anisotropy of hadrons at large transverse momentum observed in $p$-$p$ collisions.","Constraining the ratio of polarization dependent TMD distributions to polarization independent distributions by comparing with the data from $p$-$p$ collisions, we find that a simple $A^{1/3}$ enhancement of the intrinsic transverse momentum $(k_\\perp^2)$ distribution, of the initial state partons from the proton, straightforwardly yields the azimuthal anisotropy at high $p_T$ in $p$-$A$ collisions."],"url":"http://arxiv.org/abs/2404.05287v1","category":"hep-ph"}
{"created":"2024-04-08 07:59:35","title":"Hybrid inflation from supersymmetry breaking","abstract":"We extend a recently proposed framework, dubbed inflation by supersymmetry breaking, to hybrid inflation by introducing a waterfall field that allows to decouple the supersymmetry breaking scale in the observable sector from the inflation scale, while keeping intact the inflation sector and its successful predictions: naturally small slow-roll parameters, small field initial conditions and absence of the pseudo-scalar companion of the inflaton, in terms of one free parameter which is the first order correction to the inflaton K\\\"ahler potential. During inflation, supersymmetry is spontaneously broken with the inflaton being the superpartner of the goldstino, together with a massive vector that gauges the R-symmetry. Inflation arises around the maximum of the scalar potential at the origin where R-symmetry is unbroken. Moreover, a nearby minimum with tuneable vacuum energy can be accommodated by introducing a second order correction to the K\\\"ahler potential. The inflaton sector can also play the role of the supersymmetry breaking 'hidden' sector when coupled to the (supersymmetric) Standard Model, predicting a superheavy superparticle spectrum near the inflation scale. Here we show that the introduction of a waterfall field provides a natural way to end inflation and allows for a scale separation between supersymmetry breaking and inflation. Moreover, the study of the global vacuum describing low energy Standard Model physics can be done in a perturbative way within a region of the parameter space of the model.","sentences":["We extend a recently proposed framework, dubbed inflation by supersymmetry breaking, to hybrid inflation by introducing a waterfall field that allows to decouple the supersymmetry breaking scale in the observable sector from the inflation scale, while keeping intact the inflation sector and its successful predictions: naturally small slow-roll parameters, small field initial conditions and absence of the pseudo-scalar companion of the inflaton, in terms of one free parameter which is the first order correction to the inflaton K\\\"ahler potential.","During inflation, supersymmetry is spontaneously broken with the inflaton being the superpartner of the goldstino, together with a massive vector that gauges the R-symmetry.","Inflation arises around the maximum of the scalar potential at the origin where R-symmetry is unbroken.","Moreover, a nearby minimum with tuneable vacuum energy can be accommodated by introducing a second order correction to the K\\\"ahler potential.","The inflaton sector can also play the role of the supersymmetry breaking 'hidden' sector when coupled to the (supersymmetric) Standard Model, predicting a superheavy superparticle spectrum near the inflation scale.","Here we show that the introduction of a waterfall field provides a natural way to end inflation and allows for a scale separation between supersymmetry breaking and inflation.","Moreover, the study of the global vacuum describing low energy Standard Model physics can be done in a perturbative way within a region of the parameter space of the model."],"url":"http://arxiv.org/abs/2404.05269v1","category":"hep-th"}
{"created":"2024-04-08 07:41:20","title":"Hidden charge density wave induced shadow bands and ultrafast dynamics of CuTe investigated using time-resolved ARPES","abstract":"Revealing the fine electronic structure is critical for understanding the underlying physics of low-dimensional materials. Angle-resolved photoemission spectroscopy (ARPES) is a powerful experimental technique for mapping out the experimental electronic structure. By reducing the photon energy (e.g. to 6 eV) using laser sources, a greatly improved momentum resolution can be achieved, thereby providing opportunities for ``zooming in'' the fine electronic structure and even revealing the previously unresolvable bands near the Brillouin zone center. Here, by using quasi-one-dimensional material CuTe as an example, we demonstrate the unique capability of laser-based ARPES in revealing the fine electronic structures of ``hidden'' charge density wave induced shadow bands near the Brillouin zone center, which are previously unresolvable using synchrotron sources. The observation of the shadow bands reveals the CDW phase from the aspect of band folding, and the unpredicted CDW band hybridization strongly modifies the electronic structure and Fermi surface, which suggests that such hybridization must be taken into account for studying the CDW transition. Moreover, the ultrafast non-equilibrium carrier dynamics are captured by time-resolved ARPES, revealing the relaxation dynamics through electron-phonon scattering. Our work demonstrates the advantages of laser-based ARPES in zooming in the fine electronic structures, as well as capturing the ultrafast dynamics of low-dimensional materials.","sentences":["Revealing the fine electronic structure is critical for understanding the underlying physics of low-dimensional materials.","Angle-resolved photoemission spectroscopy (ARPES) is a powerful experimental technique for mapping out the experimental electronic structure.","By reducing the photon energy (e.g. to 6 eV) using laser sources, a greatly improved momentum resolution can be achieved, thereby providing opportunities for ``zooming in'' the fine electronic structure and even revealing the previously unresolvable bands near the Brillouin zone center.","Here, by using quasi-one-dimensional material CuTe as an example, we demonstrate the unique capability of laser-based ARPES in revealing the fine electronic structures of ``hidden'' charge density wave induced shadow bands near the Brillouin zone center, which are previously unresolvable using synchrotron sources.","The observation of the shadow bands reveals the CDW phase from the aspect of band folding, and the unpredicted CDW band hybridization strongly modifies the electronic structure and Fermi surface, which suggests that such hybridization must be taken into account for studying the CDW transition.","Moreover, the ultrafast non-equilibrium carrier dynamics are captured by time-resolved ARPES, revealing the relaxation dynamics through electron-phonon scattering.","Our work demonstrates the advantages of laser-based ARPES in zooming in the fine electronic structures, as well as capturing the ultrafast dynamics of low-dimensional materials."],"url":"http://arxiv.org/abs/2404.05255v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 07:17:31","title":"On Finite Presentability of Subsemigroups of the Monogenic Free Inverse Semigroup","abstract":"The monogenic free inverse semigroup $FI_1$ is not finitely presented as a semigroup due to the classic result by Schein (1975). We extend this result and prove that a finitely generated subsemigroup of $FI_1$ is finitely presented if and only if it contains only finitely many idempotents. As a consequence, we derive that an inverse subsemigroup of $FI_1$ is finitely presented as a semigroup if and only if it is a finite semilattice.","sentences":["The monogenic free inverse semigroup $FI_1$ is not finitely presented as a semigroup due to the classic result by Schein (1975).","We extend this result and prove that a finitely generated subsemigroup of $FI_1$ is finitely presented if and only if it contains only finitely many idempotents.","As a consequence, we derive that an inverse subsemigroup of $FI_1$ is finitely presented as a semigroup if and only if it is a finite semilattice."],"url":"http://arxiv.org/abs/2404.05244v1","category":"math.GR"}
{"created":"2024-04-08 05:30:46","title":"Proximity-Induced Exchange Interaction: a New Pathway for Quantum Sensing using Spin Centers in Hexagonal Boron Nitride","abstract":"Defects in hexagonal boron nitride (hBN), a two-dimensional van der Waals material, have raised wide range interest for its potential in various quantum applications. Due to hBN's 2D nature, spin center in hBN can be engineered in close proximity to target material, providing advantages over their 3D counterparts, such as nitrogen-vacancy (NV) center in diamond. Here we propose a novel quantum sensing protocol driven by exchange interaction between spin center in hBN and the underlying magnetic substrate induced by magnetic proximity effect. By first-principle calculation, we demonstrate the induced exchange interaction dominates over dipole-dipole interaction by orders of magnitude when in proximity. The interaction remains antiferromagnetic across all stacking configuration between the spin center in hBN and the target van der Waals magnets. Additionally, we explored the scaling behavior of the exchange field as a function of the spatial separation between the spin center and the targets.","sentences":["Defects in hexagonal boron nitride (hBN), a two-dimensional van der Waals material, have raised wide range interest for its potential in various quantum applications.","Due to hBN's 2D nature, spin center in hBN can be engineered in close proximity to target material, providing advantages over their 3D counterparts, such as nitrogen-vacancy (NV) center in diamond.","Here we propose a novel quantum sensing protocol driven by exchange interaction between spin center in hBN and the underlying magnetic substrate induced by magnetic proximity effect.","By first-principle calculation, we demonstrate the induced exchange interaction dominates over dipole-dipole interaction by orders of magnitude when in proximity.","The interaction remains antiferromagnetic across all stacking configuration between the spin center in hBN and the target van der Waals magnets.","Additionally, we explored the scaling behavior of the exchange field as a function of the spatial separation between the spin center and the targets."],"url":"http://arxiv.org/abs/2404.05208v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-08 05:04:22","title":"Quasicrystal bulk and surface energies from density functional theory","abstract":"Are quasicrystals stable or metastable? Density functional theory (DFT) is often used to evaluate thermodynamic stability, but quasicrystals are long-range aperiodic and their energies cannot be calculated using conventional ab initio methods. Here, we perform first-principles calculations on quasicrystal nanoparticles of increasing sizes, from which we can directly extrapolate their bulk and surface energies. Using this technique, we determine with high confidence that the icosahedral quasicrystals ScZn7.33 and YbCd5.7 are ground-state phases--revealing that translational symmetry is not a necessary condition for the T = 0 K stability of inorganic solids. Although we find the ScZn7.33 quasicrystal to be thermodynamically stable, we show on a mixed thermodynamic and kinetic phase diagram that its solidification from the melt is nucleation-limited, which illustrates why even stable materials may be kinetically challenging to grow. Our techniques here broadly open the door to first-principles investigations into the structure-bonding-stability relationships of aperiodic materials.","sentences":["Are quasicrystals stable or metastable?","Density functional theory (DFT) is often used to evaluate thermodynamic stability, but quasicrystals are long-range aperiodic and their energies cannot be calculated using conventional ab initio methods.","Here, we perform first-principles calculations on quasicrystal nanoparticles of increasing sizes, from which we can directly extrapolate their bulk and surface energies.","Using this technique, we determine with high confidence that the icosahedral quasicrystals ScZn7.33 and YbCd5.7 are ground-state phases--revealing that translational symmetry is not a necessary condition for the T","= 0","K stability of inorganic solids.","Although we find the ScZn7.33 quasicrystal to be thermodynamically stable, we show on a mixed thermodynamic and kinetic phase diagram that its solidification from the melt is nucleation-limited, which illustrates why even stable materials may be kinetically challenging to grow.","Our techniques here broadly open the door to first-principles investigations into the structure-bonding-stability relationships of aperiodic materials."],"url":"http://arxiv.org/abs/2404.05200v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 04:31:40","title":"Measurement of the $CP$ violating phase $\u03c6_s$ and $\u03c6_s^{sq\\bar{q}}$ at LHCb","abstract":"Precise measurements of the $B_s^0 - \\overline{B}_s^0$ mixing parameters provide a powerful test of the Standard Model, offering potential hints to new physics. The LHCb collaboration has performed updated measurements of the $CP$-violating phases $\\phi_s^{c\\bar{c}s}$ and $\\phi_s^{s\\bar{s}s}$, which supersede previous results. Also, an alternative approach to determine $\\Delta \\Gamma_s$ is presented, bringing a new tool that may help to resolve the tension observed between measurements made in $B_s^0 \\to J/\\psi \\phi$ by LHC experiments.","sentences":["Precise measurements of the $B_s^0 - \\overline{B}_s^0$ mixing parameters provide a powerful test of the Standard Model, offering potential hints to new physics.","The LHCb collaboration has performed updated measurements of the $CP$-violating phases $\\phi_s^{c\\bar{c}s}$ and $\\phi_s^{s\\bar{s}s}$, which supersede previous results.","Also, an alternative approach to determine $\\Delta \\Gamma_s$ is presented, bringing a new tool that may help to resolve the tension observed between measurements made in $B_s^0 \\to J/\\psi \\phi$ by LHC experiments."],"url":"http://arxiv.org/abs/2404.05189v1","category":"hep-ex"}
{"created":"2024-04-08 03:56:55","title":"Solute strengthening and softening from screw dislocation in BCC tantalum: A first-principles study","abstract":"Improving the high-temperature performance and low-temperature plasticity of tantalum (Ta) alloys is a significant scientific challenge. We employed first-principles calculations to study the interaction between screw dislocations and solute atoms in the body centered cubic (BCC) structure of Ta, with a particular focus on solid solution softening and strengthening. We analyzed the impact of various solute elements on the generalized stacking fault energy (GSFE), energy barriers within the single-atom column displacement model, and their interaction with screw dislocations. The results indicate that Hf and Zr, either individually or in combination, exhibit notable solute softening effects in BCC Ta, significantly reducing GSFE, energy barriers, and interaction energies. In contrast, Nb shows relative insensitivity to solute effects, while Mo, W, and Ir demonstrate solute strengthening effects. The calculations suggest that the interaction energy between screw dislocations and solute atoms is a reliable indicator for predicting strengthening and softening effects. Additionally, we extend these predictions to ternary alloys, demonstrating that the strengthening and softening phenomena in these materials can be explained through the electronic work function at the electronic level.","sentences":["Improving the high-temperature performance and low-temperature plasticity of tantalum (Ta) alloys is a significant scientific challenge.","We employed first-principles calculations to study the interaction between screw dislocations and solute atoms in the body centered cubic (BCC) structure of Ta, with a particular focus on solid solution softening and strengthening.","We analyzed the impact of various solute elements on the generalized stacking fault energy (GSFE), energy barriers within the single-atom column displacement model, and their interaction with screw dislocations.","The results indicate that Hf and Zr, either individually or in combination, exhibit notable solute softening effects in BCC Ta, significantly reducing GSFE, energy barriers, and interaction energies.","In contrast, Nb shows relative insensitivity to solute effects, while Mo, W, and Ir demonstrate solute strengthening effects.","The calculations suggest that the interaction energy between screw dislocations and solute atoms is a reliable indicator for predicting strengthening and softening effects.","Additionally, we extend these predictions to ternary alloys, demonstrating that the strengthening and softening phenomena in these materials can be explained through the electronic work function at the electronic level."],"url":"http://arxiv.org/abs/2404.05175v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-08 03:40:32","title":"A Monte-Carlo Simulation on Resonant Scattering of X-ray Line Emission in Supernova Remnants","abstract":"Resonant scattering (RS) of X-ray line emission in supernova remnants (SNRs) may modify the observed line profiles and fluxes and has potential impact on estimating the physical properties of the hot gas and hence on understanding the SNR physics, but has not been theoretically modeled ever. Here we present our Monte-Carlo simulation of RS effect on X-ray resonant-line emission, typified by O VII He$\\alpha$ r line, from SNRs. We employ the physical conditions characterized by the Sedov-Taylor solution and some basic parameters similar to those in Cygnus Loop. We show that the impact of RS effect is most significant near the edge of the remnant. The line profiles are predicted to be asymmetric because of different temperatures and photon production efficiencies of the expanding gas at different radii. We also predict the surface brightness of the line emission would decrease in the outer projected region but is slightly enhanced in the inner. The G-ratio of the OVII He$\\alpha$ triplet can be effectively elevated by RS in the outer region. We show that RS effect of the O VII He$\\alpha$ r line in the southwestern boundary region of Cygnus Loop is non-negligible. The observed OVII G-ratio $\\sim$1.8 of the region could be achieved with RS taken into account for properly elevated O abundance from the previous estimates. Additional simulation performed for the SNRs in ejecta-dominated phase like Cas A shows that RS in the shocked ejecta may have some apparently effects on the observational properties of oxygen resonant lines.","sentences":["Resonant scattering (RS) of X-ray line emission in supernova remnants (SNRs) may modify the observed line profiles and fluxes and has potential impact on estimating the physical properties of the hot gas and hence on understanding the SNR physics, but has not been theoretically modeled ever.","Here we present our Monte-Carlo simulation of RS effect on X-ray resonant-line emission, typified by O VII He$\\alpha$ r line, from SNRs.","We employ the physical conditions characterized by the Sedov-Taylor solution and some basic parameters similar to those in Cygnus Loop.","We show that the impact of RS effect is most significant near the edge of the remnant.","The line profiles are predicted to be asymmetric because of different temperatures and photon production efficiencies of the expanding gas at different radii.","We also predict the surface brightness of the line emission would decrease in the outer projected region but is slightly enhanced in the inner.","The G-ratio of the OVII He$\\alpha$ triplet can be effectively elevated by RS in the outer region.","We show that RS effect of the O VII He$\\alpha$ r line in the southwestern boundary region of Cygnus Loop is non-negligible.","The observed OVII G-ratio $\\sim$1.8 of the region could be achieved with RS taken into account for properly elevated O abundance from the previous estimates.","Additional simulation performed for the SNRs in ejecta-dominated phase like Cas A shows that RS in the shocked ejecta may have some apparently effects on the observational properties of oxygen resonant lines."],"url":"http://arxiv.org/abs/2404.05171v1","category":"astro-ph.HE"}
{"created":"2024-04-08 03:05:37","title":"Quantum Circuit for High Order Perturbation Theory Corrections","abstract":"Perturbation theory (PT) might be one of the most powerful and fruitful tools for both physicists and chemists, which has led to a wide variety of applications. Over the past decades, advances in quantum computing provide opportunities for alternatives to classical methods. Recently, a general quantum circuit estimating the low order PT corrections has been proposed. In this article, we revisit the quantum circuits for PT calculations, and develop the methods for higher order PT corrections of eigenenergy, especially the 3rd and 4th order corrections. We present the feasible quantum circuit to estimate each term in these PT corrections. There are two the fundamental operations in the proposed circuit. One approximates the perturbation terms, the other approximates the inverse of unperturbed energy difference. The proposed method can be generalized to higher order PT corrections.","sentences":["Perturbation theory (PT) might be one of the most powerful and fruitful tools for both physicists and chemists, which has led to a wide variety of applications.","Over the past decades, advances in quantum computing provide opportunities for alternatives to classical methods.","Recently, a general quantum circuit estimating the low order PT corrections has been proposed.","In this article, we revisit the quantum circuits for PT calculations, and develop the methods for higher order PT corrections of eigenenergy, especially the 3rd and 4th order corrections.","We present the feasible quantum circuit to estimate each term in these PT corrections.","There are two the fundamental operations in the proposed circuit.","One approximates the perturbation terms, the other approximates the inverse of unperturbed energy difference.","The proposed method can be generalized to higher order PT corrections."],"url":"http://arxiv.org/abs/2404.05162v1","category":"quant-ph"}
{"created":"2024-04-08 03:00:10","title":"Linguistic Changes in Spontaneous Speech for Detecting Parkinsons Disease Using Large Language Models","abstract":"Parkinsons disease is the second most prevalent neurodegenerative disorder with over ten million active cases worldwide and one million new diagnoses per year. Detecting and subsequently diagnosing the disease is challenging because of symptom heterogeneity with respect to complexity, as well as the type and timing of phenotypic manifestations. Typically, language impairment can present in the prodromal phase and precede motor symptoms suggesting that a linguistic-based approach could serve as a diagnostic method for incipient Parkinsons disease. Additionally, improved linguistic models may enhance other approaches through ensemble techniques. The field of large language models is advancing rapidly, presenting the opportunity to explore the use of these new models for detecting Parkinsons disease and to improve on current linguistic approaches with high-dimensional representations of linguistics. We evaluate the application of state-of-the-art large language models to detect Parkinsons disease automatically from spontaneous speech with up to 73% accuracy.","sentences":["Parkinsons disease is the second most prevalent neurodegenerative disorder with over ten million active cases worldwide and one million new diagnoses per year.","Detecting and subsequently diagnosing the disease is challenging because of symptom heterogeneity with respect to complexity, as well as the type and timing of phenotypic manifestations.","Typically, language impairment can present in the prodromal phase and precede motor symptoms suggesting that a linguistic-based approach could serve as a diagnostic method for incipient Parkinsons disease.","Additionally, improved linguistic models may enhance other approaches through ensemble techniques.","The field of large language models is advancing rapidly, presenting the opportunity to explore the use of these new models for detecting Parkinsons disease and to improve on current linguistic approaches with high-dimensional representations of linguistics.","We evaluate the application of state-of-the-art large language models to detect Parkinsons disease automatically from spontaneous speech with up to 73% accuracy."],"url":"http://arxiv.org/abs/2404.05160v1","category":"cs.CL"}
{"created":"2024-04-08 01:32:49","title":"Massless limit and conformal soft limit for celestial massive amplitudes","abstract":"In celestial holography, the massive and massless scalars in 4d space-time are represented by the Fourier transform of the bulk-to-boundary propagators and the Mellin transform of plane waves respectively. Recently, the 3pt celestial amplitude of one massive scalar and two massless scalars was discussed in arXiv:2312.08597. In this paper, we compute the 3pt celestial amplitude of two massive scalars and one massless scalar. Then we take the massless limit $m\\to 0$ for one of the massive scalars, during which process the gamma function $\\Gamma(1-\\Delta)$ appears. By requiring the resulting amplitude to be well-defined, that is it goes to the 3pt amplitude of arXiv:2312.08597, the scaling dimension of this massive scalar has to be conformally soft $\\Delta \\to 1$. The pole $1/(1-\\Delta)$ coming from $\\Gamma(1-\\Delta)$ is crucial for this massless limit. Without it the resulting amplitude would be zero. This can be compared with the conformal soft limit in celestial gluon amplitudes, where a singularity $1/(\\Delta -1)$ arises and the leading contribution comes from the soft energy $\\omega\\to 0$. The phase factors in the massless limit of massive conformal primary wave functions, dicussed in arXiv:1705.01027, plays an import and consistent role in the celestial massive amplitudes. In this massless limit, the subleading terms $m^{2n}$ can also contribute poles when the scaling dimension is analytically continued to $\\Delta=1-n$.","sentences":["In celestial holography, the massive and massless scalars in 4d space-time are represented by the Fourier transform of the bulk-to-boundary propagators and the Mellin transform of plane waves respectively.","Recently, the 3pt celestial amplitude of one massive scalar and two massless scalars was discussed in arXiv:2312.08597.","In this paper, we compute the 3pt celestial amplitude of two massive scalars and one massless scalar.","Then we take the massless limit $m\\to 0$ for one of the massive scalars, during which process the gamma function $\\Gamma(1-\\Delta)$ appears.","By requiring the resulting amplitude to be well-defined, that is it goes to the 3pt amplitude of arXiv:2312.08597, the scaling dimension of this massive scalar has to be conformally soft $\\Delta \\to 1$.","The pole $1/(1-\\Delta)$ coming from $\\Gamma(1-\\Delta)$ is crucial for this massless limit.","Without it the resulting amplitude would be zero.","This can be compared with the conformal soft limit in celestial gluon amplitudes, where a singularity $1/(\\Delta -1)$ arises and the leading contribution comes from the soft energy $\\omega\\to 0$.","The phase factors in the massless limit of massive conformal primary wave functions, dicussed in arXiv:1705.01027, plays an import and consistent role in the celestial massive amplitudes.","In this massless limit, the subleading terms $m^{2n}$ can also contribute poles when the scaling dimension is analytically continued to $\\Delta=1-n$."],"url":"http://arxiv.org/abs/2404.05137v1","category":"hep-th"}
{"created":"2024-04-08 01:26:48","title":"Searching for Magnetar Binaries Disrupted by Core-Collapse Supernovae","abstract":"Core-collapse Supernovae (CCSNe) are considered to be the primary magnetar formation channel, with 15 magnetars associated with supernova remnants (SNRs). A large fraction of these should occur in massive stellar binaries that are disrupted by the explosion, meaning that $\\sim45\\%$ of magnetars should be nearby high-velocity stars. Here we conduct a multi-wavelength search for unbound stars, magnetar binaries, and SNR shells using public optical ($uvgrizy-$bands), infrared ($J-$, $H-$, $K-$, and $K_s-$bands), and radio ($888$ MHz, $1.4$ GHz, and $3$ GHz) catalogs. We use Monte Carlo analyses of candidates to estimate the probability of association with a given magnetar based on their proximity, distance, proper motion, and magnitude. In addition to recovering a proposed magnetar binary, a proposed unbound binary, and 13 of 15 magnetar SNRs, we identify two new candidate unbound systems: an OB star from the Gaia catalog we associate with SGR J1822.3-1606, and an X-ray pulsar we associate with 3XMM J185246.6+003317. Using a Markov-Chain Monte Carlo simulation that assumes all magnetars descend from CCSNe, we constrain the fraction of magnetars with unbound companions to $5\\lesssim f_u \\lesssim 24\\%$, which disagrees with population synthesis results. Alternate formation channels are unlikely to wholly account for the lack of unbound binaries as this would require $31\\lesssim f_{nc} \\lesssim 66\\%$ of magnetars to descend from such channels. Our results support a high fraction ($48\\lesssim f_m \\lesssim 86\\%$) of pre-CCSN mergers, which can amplify fossil magnetic fields to preferentially form magnetars.","sentences":["Core-collapse Supernovae (CCSNe) are considered to be the primary magnetar formation channel, with 15 magnetars associated with supernova remnants (SNRs).","A large fraction of these should occur in massive stellar binaries that are disrupted by the explosion, meaning that $\\sim45\\%$ of magnetars should be nearby high-velocity stars.","Here we conduct a multi-wavelength search for unbound stars, magnetar binaries, and SNR shells using public optical ($uvgrizy-$bands), infrared ($J-$, $H-$, $K-$, and $K_s-$bands), and radio ($888$ MHz, $1.4$ GHz, and $3$ GHz) catalogs.","We use Monte Carlo analyses of candidates to estimate the probability of association with a given magnetar based on their proximity, distance, proper motion, and magnitude.","In addition to recovering a proposed magnetar binary, a proposed unbound binary, and 13 of 15 magnetar SNRs, we identify two new candidate unbound systems: an OB star from the Gaia catalog we associate with SGR J1822.3-1606, and an X-ray pulsar we associate with 3XMM J185246.6+003317.","Using a Markov-Chain Monte Carlo simulation that assumes all magnetars descend from CCSNe, we constrain the fraction of magnetars with unbound companions to $5\\lesssim f_u","\\lesssim 24\\%$, which disagrees with population synthesis results.","Alternate formation channels are unlikely to wholly account for the lack of unbound binaries as this would require $31\\lesssim f_{nc} \\lesssim 66\\%$ of magnetars to descend from such channels.","Our results support a high fraction ($48\\lesssim f_m \\lesssim 86\\%$) of pre-CCSN mergers, which can amplify fossil magnetic fields to preferentially form magnetars."],"url":"http://arxiv.org/abs/2404.05135v1","category":"astro-ph.HE"}
{"created":"2024-04-08 00:40:06","title":"A 0.65-pJ/bit 3.6-TB/s/mm I/O Interface with XTalk Minimizing Affine Signaling for Next-Generation HBM with High Interconnect Density","abstract":"This paper presents an I/O interface with Xtalk Minimizing Affine Signaling (XMAS), which is designed to support high-speed data transmission in die-to-die communication over silicon interposers or similar high-density interconnects susceptible to crosstalk. The operating principles of XMAS are elucidated through rigorous analyses, and its advantages over existing signaling are validated through numerical experiments. XMAS not only demonstrates exceptional crosstalk removing capabilities but also exhibits robustness against noise, especially simultaneous switching noise. Fabricated in a 28-nm CMOS process, the prototype XMAS transceiver achieves an edge density of 3.6TB/s/mm and an energy efficiency of 0.65pJ/b. Compared to the single-ended signaling, the crosstalk-induced peak-to-peak jitter of the received eye with XMAS is reduced by 75% at 10GS/s/pin data rate, and the horizontal eye opening extends to 0.2UI at a bit error rate < 10$^{-12}$.","sentences":["This paper presents an I/O interface with Xtalk Minimizing Affine Signaling (XMAS), which is designed to support high-speed data transmission in die-to-die communication over silicon interposers or similar high-density interconnects susceptible to crosstalk.","The operating principles of XMAS are elucidated through rigorous analyses, and its advantages over existing signaling are validated through numerical experiments.","XMAS not only demonstrates exceptional crosstalk removing capabilities but also exhibits robustness against noise, especially simultaneous switching noise.","Fabricated in a 28-nm CMOS process, the prototype XMAS transceiver achieves an edge density of 3.6TB/s/mm and an energy efficiency of 0.65pJ/b. Compared to the single-ended signaling, the crosstalk-induced peak-to-peak jitter of the received eye with XMAS is reduced by 75% at 10GS/s/pin data rate, and the horizontal eye opening extends to 0.2UI at a bit error rate < 10$^{-12}$."],"url":"http://arxiv.org/abs/2404.05119v1","category":"eess.SP"}
{"created":"2024-04-08 00:20:12","title":"Analysis of $B_s^0 \\to X(3872) [\u03c8(2S)] \u03c0^+\u03c0^- (K^+ K^-)$ decays","abstract":"We have phenomenologically investigated the decays $B_s^0 \\to X(3872) \\pi^+\\pi^- (K^+ K^-)$ and $B_s^0 \\to \\psi(2S) \\pi^+ \\pi^- (K^+K^-)$. In our analysis, the scalar meson $f_0(980)$ is formed through the final state interactions of coupled channels $\\pi \\pi$ and $K\\bar{K}$. Our findings indicate that the $\\pi^+\\pi^-$ invariant mass distribution of the $B_s^0 \\to \\psi(2S)\\pi^+\\pi^-$ decay can be accurately reproduced. Furthermore, we have explored the $\\pi^+\\pi^- (K^+ K^-)$ invariant mass distribution of the $B_s^0 \\to X(3872) \\pi^+\\pi^- (K^+ K^-)$ decay, accounting for the different production mechanisms between $X(3872)$ and $\\psi(2S)$, up to a global factor. It is found that the production rates for $X(3872)$ and $\\psi(2S)$ are much different, which indicates that the structure of $X(3872)$ is more complicated than the $\\psi(2S)$, which is a conventional $c\\bar{c}$ state. Additionally, we have considered the contributions from $f_0(1500)$ to $\\pi^+\\pi^-$ and the $\\phi$ meson to $K^+ K^-$ in our analysis. Utilizing the model parameters, we have calculated the branching fraction of $B_s^0 \\to X(3872) K^+ K^-$, and anticipate that the findings of our study can be experimentally tested in the future.","sentences":["We have phenomenologically investigated the decays $B_s^0 \\to X(3872) \\pi^+\\pi^- (K^+ K^-)$ and $B_s^0 \\to \\psi(2S)","\\pi^+ \\pi^- (K^+K^-)$.","In our analysis, the scalar meson $f_0(980)$ is formed through the final state interactions of coupled channels $\\pi \\pi$ and $K\\bar{K}$. Our findings indicate that the $\\pi^+\\pi^-$ invariant mass distribution of the $B_s^0 \\to \\psi(2S)\\pi^+\\pi^-$ decay can be accurately reproduced.","Furthermore, we have explored the $\\pi^+\\pi^- (K^+ K^-)$ invariant mass distribution of the $B_s^0 \\to X(3872) \\pi^+\\pi^- (K^+ K^-)$ decay, accounting for the different production mechanisms between $X(3872)$ and $\\psi(2S)$, up to a global factor.","It is found that the production rates for $X(3872)$ and $\\psi(2S)$ are much different, which indicates that the structure of $X(3872)$ is more complicated than the $\\psi(2S)$, which is a conventional $c\\bar{c}$ state.","Additionally, we have considered the contributions from $f_0(1500)$ to $\\pi^+\\pi^-$ and the $\\phi$ meson to $K^+ K^-$ in our analysis.","Utilizing the model parameters, we have calculated the branching fraction of $B_s^0 \\to X(3872) K^+ K^-$, and anticipate that the findings of our study can be experimentally tested in the future."],"url":"http://arxiv.org/abs/2404.05114v1","category":"hep-ph"}
{"created":"2024-04-08 00:19:08","title":"Quasi-parton distributions in massive QED2: Towards quantum computation","abstract":"We analyze the quasi-parton distributions of the lightest $\\eta'$ meson in massive two-dimensional Quantum electrodynamics (QED2) by performing a digital quantum simulation on a classical computer (exact diagonalization). The Hamiltonian and boost operators are mapped onto spin qubits in a spatial lattice with open boundary conditions. The lowest excited state in the exact diagonalization is shown to interpolate continuously between an anomalous $\\eta'$ state at strong coupling,and a non-anomalous heavy meson at weak coupling, with a cusp at the critical point. The boosted $\\eta'$ state follows relativistic kinematics but with large deviations in the luminal limit. The spatial quasi-parton distribution function and amplitude for the $\\eta'$ state are computed numerically for increasing rapidity both at strong and weak coupling, and compared to the exact light front results. The numerical results from the boosted form of the spatial parton distributions, compare fairly with the inverse Fourier transformation of the luminal parton distributions, derived in the lowest Fock space approximation. Our analysis points out some of the limitations facing the current lattice program for the parton distributions.","sentences":["We analyze the quasi-parton distributions of the lightest $\\eta'$ meson in massive two-dimensional Quantum electrodynamics (QED2) by performing a digital quantum simulation on a classical computer (exact diagonalization).","The Hamiltonian and boost operators are mapped onto spin qubits in a spatial lattice with open boundary conditions.","The lowest excited state in the exact diagonalization is shown to interpolate continuously between an anomalous $\\eta'$ state at strong coupling,and a non-anomalous heavy meson at weak coupling, with a cusp at the critical point.","The boosted $\\eta'$ state follows relativistic kinematics but with large deviations in the luminal limit.","The spatial quasi-parton distribution function and amplitude for the $\\eta'$ state are computed numerically for increasing rapidity both at strong and weak coupling, and compared to the exact light front results.","The numerical results from the boosted form of the spatial parton distributions, compare fairly with the inverse Fourier transformation of the luminal parton distributions, derived in the lowest Fock space approximation.","Our analysis points out some of the limitations facing the current lattice program for the parton distributions."],"url":"http://arxiv.org/abs/2404.05112v1","category":"hep-ph"}
{"created":"2024-04-07 23:56:17","title":"Cosmic-ray confinement in radio bubbles by micromirrors","abstract":"Radio bubbles, ubiquitous features of the intracluster medium around active galactic nuclei, are known to rise buoyantly for multiple scale heights through the intracluster medium (ICM). It is an open question how the bubbles can retain their high-energy cosmic-ray content over such distances. We propose that the enhanced scattering of cosmic rays due to micromirrors generated in the ICM, as proposed recently by Reichherzer et al. (2023), is a viable mechanism for confining the cosmic rays within bubbles and can qualitatively reproduce their morphology. We discuss the observational implications of such a model of cosmic-ray confinement.","sentences":["Radio bubbles, ubiquitous features of the intracluster medium around active galactic nuclei, are known to rise buoyantly for multiple scale heights through the intracluster medium (ICM).","It is an open question how the bubbles can retain their high-energy cosmic-ray content over such distances.","We propose that the enhanced scattering of cosmic rays due to micromirrors generated in the ICM, as proposed recently by Reichherzer et al. (2023), is a viable mechanism for confining the cosmic rays within bubbles and can qualitatively reproduce their morphology.","We discuss the observational implications of such a model of cosmic-ray confinement."],"url":"http://arxiv.org/abs/2404.05110v1","category":"astro-ph.HE"}
{"created":"2024-04-07 23:10:03","title":"Uniform approximation of Betti numbers","abstract":"We prove that Lefschetz's principle of approximating the cohomology of a possibly singular affine scheme of finite type over a field by the cohomology of a suitable (thickening of a) hyperplane section can be made uniform: in the affine case, we can choose the hyperplane section independently of the cohomology. Using Jouanolou's trick, this gives a new way to bound the Betti numbers of quasi-projective schemes over a field, independently of the cohomology. This is achieved through a motivic version of Deligne's generic base change formula and an axiomatic presentation of the theory of perverse sheaves. These methods produce generating families of Voevodsky motivic sheaves that are realized in perverse sheaves over any base of equal characteristic.","sentences":["We prove that Lefschetz's principle of approximating the cohomology of a possibly singular affine scheme of finite type over a field by the cohomology of a suitable (thickening of a) hyperplane section can be made uniform: in the affine case, we can choose the hyperplane section independently of the cohomology.","Using Jouanolou's trick, this gives a new way to bound the Betti numbers of quasi-projective schemes over a field, independently of the cohomology.","This is achieved through a motivic version of Deligne's generic base change formula and an axiomatic presentation of the theory of perverse sheaves.","These methods produce generating families of Voevodsky motivic sheaves that are realized in perverse sheaves over any base of equal characteristic."],"url":"http://arxiv.org/abs/2404.05104v1","category":"math.AG"}
